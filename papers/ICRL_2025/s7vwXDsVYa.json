{
    "id": "s7vwXDsVYa",
    "title": "Conditional Diffusion on Web-Scale Image Pairs leads to Diverse Image Variations",
    "abstract": "Generating image variations, where a model produces variations of an input image while preserving the semantic context has gained increasing attention. Current image variation techniques involve adapting a text-to-image model to reconstruct an input image conditioned on the same image. We first demonstrate that a diffusion model trained to reconstruct an input image from frozen embeddings can reconstruct the image with minor variations. Second, inspired by how text-to-image models learn from web-scale text-image pairs, we explore a new pretraining strategy to generate image variations using a large collection of image pairs. Our diffusion model \\textit{Semantica} receives a random (encoded) image from a webpage as conditional input and denoises another noisy random image from the same webpage. We carefully examine various design choices for the image encoder, given its crucial role in extracting relevant context from the input image. Once trained, \\textit{Semantica} can adaptively generate new images from a dataset by simply using images from that dataset as input. Finally, we identify limitations in standard image consistency metrics for evaluating image variations and propose alternative metrics based on few-shot generation.",
    "keywords": [
        "diffusion",
        "image variations",
        "generative models"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-16",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=s7vwXDsVYa",
    "pdf_link": "https://openreview.net/pdf?id=s7vwXDsVYa",
    "comments": [
        {
            "summary": {
                "value": "This paper focuses on the task of generating image variations and proposes the Semantica model. Unlike traditional diffusion model training, Semantica is trained on web-scale image pairs. It receives a random (encoded) image from a webpage as conditional input and denoises another noisy random image from the same webpage. Additionally, the paper explores the impact of different image encoders on the results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper is easy to understand.\n2. The proposed method achieves good results on several benchmarks."
            },
            "weaknesses": {
                "value": "1. I have some doubts about the task of generating image variations. The model trained in this paper only supports image input and does not support text guidance, so it doesn\u2019t seem to have strong practical value for real-world image editing. Additionally, many AIGC models already support various tasks such as image editing and style transfer. Therefore, I don't quite understand the value of the generating image variations task.\n2. Emu2 [1] has already discovered that using CLIP image embeddings as conditional inputs for training a diffusion model can successfully reconstruct images, which is similar to the findings in this paper.\n3. From the visual results, it is difficult to see any advantage of the proposed method over IP-Adapter. Moreover, IP-Adapter has broader application scenarios, as it can adapt to various text-to-image models.\n\n\n\n[1] Generative Multimodal Models are In-Context Learners. CVPR, 2024."
            },
            "questions": {
                "value": "Refer to Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new model, Semantica, which generates image variations given contextual input images. The model is trained on web-scale image pairs. Specifically, it is trained with pairs of images from the same webpage. Multiple design choices are discussed in the paper, such as the selection of the image encoder, diffusion decoder, etc. The model is evaluated on a few-shot version of FID, recall, and precision for the generation of image variations and outperforms prior works such as SD-v2, IP-adapter, and Versatile Diffusion."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Generating diverse image variations by training on images from the same webpage is both simple and interesting.\n* The paper is well-written and easy to follow. The visualizations seem to outperform the prior works listed in the paper.\n* The designs and ablation studies of different model components are well explained."
            },
            "weaknesses": {
                "value": "* There is a missing comparison with some related work, such as RIVAL [1] which also targets the image variation task.\n* Few-shot FID, precision, and recall may not be sufficient to fully quantify performance. Some metrics proposed in the table 1 of RIVAL [1] could provide more insightful for evaluations.\n* There is a lack of a user study to compare different methods, which would be more convincing, as numerical metrics may not fully quantify performance.\n\n\n**Reference**\n\n[1] Real-World Image Variation by Aligning Diffusion Inversion Chain, NeurIPS 23"
            },
            "questions": {
                "value": "1. In eqn 1, why there is 2 variable t? One of them is context?\n2. In eqn 2, conditioning input is $y_c$, but target is x? which seems different with L301-303"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper works on image variations generation based on two main observations. Firstly, conditional diffusion models with frozen condition embeddings can reconstruct the image with minor variations. Secondly, web-scale image can be used to train image variation models. This paper then examine various design choices for condition representation generation given its crucial role in the proposed method, and find DINOV2 can provide good image representation for better image variation generation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. With web-scale images, this paper successfully find a good application on top of it to perform Image variation generation, which is inspiring and effective as illustrated in the experiments section.\n2. The final conclusion that DINOV2 produces image representation for better image variations is also interesting, explaining self-supervised representation learning is promising in achieving effective image generation/editing.\n3. The proposed new way of evaluate performance of image variation techniques seems interesting and inspiring."
            },
            "weaknesses": {
                "value": "1. The analysis on image encoder part for image variation (sec 2.2) seems not informative enough, although it's one main contribution of the paper, as it's common sense that the effectiveness of conditional representation is crucial for image generation.\n2. A new application of web-scale image for image variation is not well explained. It's not clear how the authors come up with this idea of using web-scale image for image variation, making it hard to evaluate the significant of the contrition.\n3. Data filtering should be clearly introduced as it's one of the main contribution. However, Sec. 5 is not clear (see Questions 3).\n4. The experimental results section seems weak in proving superiority of the proposed solution. (see Questions 4)"
            },
            "questions": {
                "value": "1. Image representation learning, e.g. self-supervised, image reconstruction based, contrastive learning and etc, are wildly studied. How the other encoders perform compared with DINO? e.g. MAE (ref 1)\n2. Web-scale image is indeed Episodic WebLI in this paper. Are there any other web-scale images? Why choose Episodic WebLI in particular?\n3. Please explain robustness of the method with respect to the cleanest of the web-scale images (line 306-315), e.g. what if the web-scale images are not well filtered out, how the method perform then?\n4. As explained in the paper, the commonly used metrics, e.g FID, fail to evaluate variation performance. However, it's still not clear how the proposed solution (sec 7) can be effective in measure diversity, e.g. Fig. 3 should be further analysed to explain diversity superiority.\n\nre1: Masked Autoencoders Are Scalable Vision Learners, CVPR 2022"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper if focus on the challenge of generating image variations, with large diversity while preserving the the conditions' semantics.\nThe authors suggest a new pre-training method exploiting the semantic relations of images within the same web page.   \nIt also discuss the challenge of measuring image diversity and suggest a few-short metrics approach"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The idea of using the semantic relations of image pairs with in the the same web page is a good contribution which can lead to even more  research direction then the focus of this paper\n- The observation of image diversity issue in current SOTA , mitigate it and suggest new metric to evaluate it"
            },
            "weaknesses": {
                "value": "The claim that \"Standard image-level metrics such as LPIPS and distribution-level metrics such as FID fail\nto capture diversity in image variations\" is not backup with number and/or examples"
            },
            "questions": {
                "value": "Please provide specific examples or quantitative evidence demonstrating how LPIPS and FID fail to capture diversity in image variations and should do a comparative analysis between the proposed one-shot metrics and LPIPS/FID, including specific examples where the new metrics better capture diversity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}