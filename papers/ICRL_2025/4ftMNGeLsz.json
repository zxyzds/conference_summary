{
    "id": "4ftMNGeLsz",
    "title": "FedGO : Federated Ensemble Distillation with GAN-based Optimality",
    "abstract": "For federated learning in practical settings, a significant challenge is the considerable diversity of data across clients. To tackle this data heterogeneity issue, it has been recognized that federated ensemble distillation is effective. Federated ensemble distillation requires an unlabeled dataset on the server, which could either be an extra dataset the server already possesses or a dataset generated by training a generator through a data-free approach. Then, it proceeds by generating pseudo-labels for the unlabeled data based on the predictions of client models and training the server model using this pseudo-labeled dataset. Consequently, the efficacy of ensemble distillation hinges on the quality  of these pseudo-labels, which, in turn, poses a challenge of appropriately assigning weights to client predictions for each data point, particularly in scenarios with data heterogeneity. In this work, we suggest a provably near-optimal weighting method for federated ensemble distillation, inspired by theoretical results in generative adversarial networks (GANs). Our weighting method utilizes client discriminators, trained at the clients based on a generator distributed from the server and their own datasets. \nOur comprehensive experiments on various image classification tasks illustrate that our method significantly improves the performance over baselines, under various scenarios with and without extra server dataset. Furthermore, we provide an extensive analysis of additional communication cost, privacy leakage, and computational burden caused by our weighting method.",
    "keywords": [
        "Federated learning",
        "ensemble distillation",
        "data heterogeneity",
        "generative adversarial network"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4ftMNGeLsz",
    "pdf_link": "https://openreview.net/pdf?id=4ftMNGeLsz",
    "comments": [
        {
            "comment": {
                "value": "First, we would like to express our gratitude for the time and effort you put into reviewing our paper. We agree with the feedback that the use of an unlabeled dataset on the server (no need to share with clients for FedGO) or pretrained generator may not always be feasible. In fact, our paper presents a solution for such situations: we developed a method for our FedGO algorithm to operate in a data-free setting without the need for an additional dataset or pretrained generator, and we included both the method and experimental results in the paper.\n\n\nAs outlined in Section 3.2, we propose a data-free approach: when the server does not have a dataset (S2), the generator is trained using FL techniques like FedGAN (G3), and a distillation dataset is created with that generator (D3). The experimental results for this approach are provided in Appendix F.3. Furthermore, we offer a comprehensive comparison and analysis in Appendix G, addressing communication, privacy, and computation aspects for scenarios involving an additional dataset, pretrained generator, or a data-free approach.\n\n\nTo summarize, **our paper already proposes a data-free approach that does not require an external dataset or pretrained generator, and it includes experimental results as well as a multi-faceted analysis covering privacy and computation aspects**. We hope that our comprehensive analysis of various scenarios will have a positive impact on your evaluation. Thank you."
            }
        },
        {
            "summary": {
                "value": "This paper proposed a novel federated ensemble distillation approach that utilizes generative adversarial networks (GANs) to address the challenges posed by data diversity across clients. Specifically, the proposed approach employs GANs to optimize the weighting of client predictions, thereby improving the quality of pseudo-labels generated during the ensemble distillation process. The paper provides theoretical insights that establish the effectiveness of the proposed method. Comprehensive experiments demonstrate that the proposed approach outperforms existing methods in robustness against data heterogeneity."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper provides a theoretical foundation for the proposed approach, which validates its effectiveness and enhances its credibility.\n2. The paper analyzes communication, privacy, and computational complexity within different scenarios, providing valuable insights for implementing the proposed approach."
            },
            "weaknesses": {
                "value": "1. This paper needs to demonstrate the effectiveness of the proposed approach on different model structures, such as VGG and MobileNet.\n2. The effectiveness of the proposed method relies on the quality of the discriminator and generator. The paper needs to conduct related ablation studies.\n3. This paper should conduct ablation studies to analyze the impact of hyperparameters (e.g. $E_s$ and $E_d$) on the effectiveness of the approach.\n4. The experimental settings of the baselines are not clearly stated, and it is important to clarify the fairness of the experimental comparison.\n5. The additional computational and communication overhead introduced by the GAN-based approach may not be suitable for FL scenarios, particularly those with strict resource constraints."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a new approach to address the issue of data heterogeneity in federated learning. By applying Generative Adversarial Network (GAN) techniques to federated ensemble distillation, the paper proposes a near-optimal weighting method that enhances the training process of the server model. Extensive experimental validation demonstrates significant improvements in model performance and convergence speed across various image classification tasks. Moreover, the study provides an in-depth analysis of the potential additional communication costs, privacy leaks, and computational burdens introduced by this method, showcasing its practicality and flexibility in protecting data privacy and enhancing system efficiency."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper demonstrates originality through its innovative integration of GAN-based techniques with federated ensemble distillation. The use of discriminators trained at the client side to optimize the weighting of client contributions during the distillation process is a novel approach that has not been extensively explored in previous federated learning research.\n\nThe method's originality is further enhanced by its theoretical grounding, which employs results from GAN literature to develop a provably near-optimal weighting method. \n\nThe experimental setup is well thought out"
            },
            "weaknesses": {
                "value": "The paper claims near-optimal performance based on theoretical justifications rooted in GAN literature. However, these claims might depend heavily on certain idealized assumptions about data distributions and discriminator performance. Real-world deviations from these assumptions could lead to suboptimal performance. The paper does not explain how to select discriminator architectures."
            },
            "questions": {
                "value": "1. In introduction on page 2, Our main contributions are summarized in the following: \"Federated Ensemble Distillation\" instead of \"Ferated Ensemble Distillation\".\n\n2. In theoretical analysis, near-optimal performance is heavily affected on discriminator performance. I do not understand how to select the discriminator architectures? Can you give me some detailed description?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes FedGO: Ferated Ensemble Distillation with GAN-based Optimality, for federated ensemble distillation. This algorithm incorporates a novel weighting method using the client discriminators that are trained at the clients based on the generator distributed from the server and their own datasets. The generator distributed from the server can be either off-the-shelf or trained with the unlabeled dataset on the server. The exchange of the generator and the client discriminators between the server and the clients occurs only once before the main FL algorithm starts, resulting in minimal additional overhead.\nExtensive experiments demonstrate significant improvements of FedEDG over existing research both in final performance and convergence speed on multiple image datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n2. The authors conducted extensive experiments to verify the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "As far as I am concerned, distillation-based FL is data dependent and requires access to an auxiliary dataset derived from publicly available proxy data sources for knowledge transfer, whereas a desirable auxiliary dataset is not always available since its construction requires careful deliberation and even prior knowledge about clients\u2019 private data to achieve satisfactory performance, which is inconsistent with the privacy-preserving nature of FL. In addition, I argue that FedEDG with Pretrained Generator proposed in this paper also has the above-mentioned issues. This is because pre-trained generator needs to be trained on public datasets. Therefore, I remain skeptical of this research direction, even if the paper contains theoretical evidence. Furthermore, if the author wants to convince me, please provide some feasible solutions to address the aforementioned issues.\nI'll raise my score if author can address the above problems."
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}