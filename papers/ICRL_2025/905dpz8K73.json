{
    "id": "905dpz8K73",
    "title": "Complementary Coding of Space with Coupled Place Cells and Grid Cells",
    "abstract": "Spatial coding is a fundamental function of the brain. Place cells in the hippocampus (HPC) and grid cells in the medial entorhinal cortex (MEC) are two primary types of neurons accounting for spatial representation in the brain. These two types of neurons employ different spatial coding strategies and process environmental and motion cues, respectively. \nIn this work, we develop a computational model to elucidate how place and grid cells can complement each other to integrate information optimally and overcome their respective shortcomings. Specifically, we build a model with reciprocally coupled continuous attractor neural networks (CANNs), in which a CANN with location coordinate models the place cell ensemble in HPC, and multiple CANNs with phase coordinate model grid cell modules with different spacings in MEC, and the coupling between place and grid cells conveys the correlation prior between sensory cues. We theoretically derive that the dynamics of our model effectively implements the gradient-based optimization of the posterior. Using simulations, we demonstrate that our model achieves Bayesian optimal integration of the environmental and motion cues, and avoids the non-local error problem in phase coding of grid cells. We hope that this study gives us insights into understanding how place and grid cells complement each other to improve spatial representation in the brain.",
    "keywords": [
        "Place cells",
        "Grid cells",
        "Complementary Coding of Space",
        "Coupled Attractor Networks"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=905dpz8K73",
    "pdf_link": "https://openreview.net/pdf?id=905dpz8K73",
    "comments": [
        {
            "summary": {
                "value": "This paper constructs a neural network model of recurrently coupled grid and place cells which performs maximum a posteriori (MAP) estimation of the animal's position. The model is somewhat biologically plausible and is surprisingly tractable to theoretical analysis, despite its relative complexity."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is somewhat slow to get to the main point and results in section 4 + 5. Once it gets there, the basic analysis and numerical experiments are pretty interesting and intuitive. In essence, the paper shows that the place cell attractor network can select the most likely position through what is essentially winner-take-all dynamics.\n\nThis idea is intuitive, but the authors do a reasonably good job of formalizing this intuition. The theoretical analysis is *very* detailed &mdash; in fact, probably too many details are provided in the main text. I would like to see section 5 expanded and section 4 stripped down to the most fundamental equations (e.g. relegate equations 12+13 to the appendix)."
            },
            "weaknesses": {
                "value": "* The analysis assumes that neural response noise is independent across all neurons, a common assumption which is well-known to be violated in real biological systems (e.g. Abbott & Dayan, 1999). This limitation to the analysis is not discussed by the authors in detail.\n\n* The model makes various parametric assumptions about the nature of spatial tuning and it is unclear how sensitive the analysis is to these assumptions. For example...\n    * Equation 1 posits a unimodal Gaussian tuning curve for place cells &mdash; not only are place cells not really Gaussian, but they are often multi-modal in large environments (e.g. [Fenton et al 2008](https://www.jneurosci.org/content/28/44/11250.short) and [Rich et al. 2014](https://doi.org/10.1126/science.1255635)).\n    * Equation 5 posits a Gaussian noise model which is also biologically unrealistic, relative to other common choices, e.g. Poisson.\n    * The different grid cell modules are not reciprocally connected, which seems like a dubious assumption.\n    * Overall, it is not clear to me that these parametric assumptions are critical to the main story that the authors make. If they are not critical, this should be demonstrated directly. Furthermore, if they are not critical to the main results, then many of these equations in sections 3 and 4 should be relegated to the Appendix/Supplement as they are highly distracting.\n\n* The authors claim that \"optimal decoding is achieved by maximum a posterior (MAP)\" (line 220). At best this is a mathematically incomplete statement. At worst, it's flatly incorrect. The optimal decoding rule will depend on the loss function you posit on your point estimator. The authors do not appear to formalize a loss function on the decoder, but the most common choice is to use the expected squared error. Under this choice, the *posterior mean and **not** the posterior maximum/mode* is the optimal point estimate. My complaint here is pedantic and fixable, but on the other hand this is very basic stuff (see \"Examples\" section on [the wikipedia page for Bayes optimal estimators](https://en.wikipedia.org/wiki/Bayes_estimator)) and it doesn't inspire my confidence when papers miss details like this that are central to their narrative.\n\n* The critical theoretical prediction of the paper seems to be equation 19, which shows that the dynamics of the network perform gradient ascent on the posterior. It would be great to include a numerical simulation showing the accuracy of this prediction (since it relies on a few simplifying assumptions).\n\n* More citations and references should be provided throughout the manuscript. For example, in section 2 the work of Fiete et al (2008) should be cited when explaining the importance of having co-prime factors, and in section 2.2 there should be a reference about Fisher information and Cramer Rao bound (which is alluded to but not explicitly mentioned). Also, MacKay's textbook is cited in two different formats in the references (once as MacKay David 2022 and another time as David JC MacKay 2003). The citation to MacKay's book at the beginning of section 2.1 (here it is cited inline as David 2022) is puzzling to me. Which chapter / section are you referring to?\n\n* The main point of the paper seems to be that one can construct a recurrent neural network to de-noise an estimate of an external variable (via MAP inference in this case). This conceptual point doesn't actually strike me as that novel &mdash; it is the basic idea behind Hopfield networks. Bayesian interpretations of Hopfield networks (e.g. in [this paper](https://doi.org/10.1109/ICNN.1993.298580)) and of continuous attractor networks (e.g. in [this paper](https://doi.org/10.1073/pnas.2210622120)) have been put forth in the literature before and don't seem to be cited / discussed by the authors.\n\n* I hate to say it, but I think this paper is likely a better fit for a physics journal or a neuroscience journal than ICLR. There is very minimal appeal to this flavor of work to the broader ML / AI community. Overall, I'm okay with including papers like this in ICLR, but the area chair may feel differently."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a computational model that investigates how place cells in the hippocampus (HPC) and grid cells in the medial entorhinal cortex (MEC) collaborate to achieve robust spatial representation. Recognizing that place cells and grid cells have different spatial coding strategies\u2014place cells localize specific positions based on environmental cues, while grid cells encode position through a periodic phase code driven by self-motion\u2014the authors develop a model with reciprocally coupled continuous attractor neural networks (CANNs) to represent these neural populations.\n\nThe model, which includes coupled CANNs for place and grid cell networks, demonstrates how reciprocal interactions allow optimal integration of environmental and motion cues, leveraging each system\u2019s strengths with respect to coding while mitigating their respective limitations. The authors theoretically derive that the model effectively performs gradient-based optimization of the posterior distribution of location, achieving Bayesian optimal cue integration. Simulations validate the model\u2019s ability to reduce non-local errors in grid cell phase coding and show that this network configuration leads to accurate spatial representation even with noisy inputs.\n\nThe paper\u2019s contributions include:\n\n - Formulating the interaction between HPC and MEC as a probabilistic inference model for cue integration.\n - Proposing a coupled CANN model to implement this integration and performing gradient-based optimization of the posterior (GOP).\n - Demonstrating through simulations that the model achieves Bayesian optimal integration of sensory cues and mitigates non-local errors in grid coding.\n - This study offers insights into how place and grid cells may interact to enhance spatial coding accuracy and flexibility in the brain, further exploring spatial representation mechanisms addressed in the literature."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Originality:**\nThe paper presents a novel framework for understanding the complementary roles of place and grid cells by modeling them as coupled continuous attractor neural networks (CANNs). The approach of using reciprocal interactions to enable Bayesian integration of environmental and self-motion cues creatively extends existing ideas on spatial representation. This combination of probabilistic inference with neural dynamics is an innovative contribution to models of hippocampal-entorhinal interaction.\n\n**Quality:**\nThe theoretical derivation of gradient-based optimization of the posterior (GOP) within the coupled CANN model is rigorous, showing clear mathematical foundations for the proposed integration mechanism. The simulation results are well-designed to validate the model's effectiveness, particularly in achieving Bayesian optimal integration and minimizing non-local errors in grid coding. The use of multiple noise levels to test robustness adds credibility to the results.\n\n**Clarity:**\nThe paper is generally well-organized, with each section building logically on the previous one. Key concepts, such as the difference between localized space coding (LSC) and phase space coding (PSC), are clearly explained, making the paper accessible even to readers less familiar with neural models of spatial representation. Equations and diagrams are effectively used to support the conceptual flow.\n\n**Significance:**\nThe model has important implications for understanding how the brain integrates sensory information to form stable spatial maps, a fundamental aspect of cognition and navigation. By addressing the limitations of phase coding in grid cells through a biologically plausible mechanism, the work advances the field's understanding of error correction in neural representations of space. This approach could also inform future work in both neuroscience and neural-inspired AI systems, making the findings broadly relevant."
            },
            "weaknesses": {
                "value": "1. The model assumes that grid cells receive direct position-based input (eq 13), rather than path-integrated input derived from velocity and head direction signals, as experimental studies suggest. The paper could benefit from incorporating self-motion signals more directly, allowing grid cells to compute position from velocity and direction cues. This also points to potential misalignment with the Gaussian error of eq 13 as path integration would lead to error accumulation and necessitate correction within the grid-place cell network. This error correction is often proposed corrected by border cells [Hardcastle et al.](https://www.sciencedirect.com/science/article/pii/S0896627315002639), and/or place cell inputs [Bonnevie et al.](https://pubmed.ncbi.nlm.nih.gov/23334581/)\n\n2. The model relies on several parameters\u2014like coupling strengths and noise levels\u2014that may significantly impact its dynamics and stability, but these sensitivities are not explored in depth. A systematic analysis of how changes in these parameters affect model performance would strengthen robustness claims and clarify under what conditions the model's optimal cue integration holds. Testing a broader range of parameter values, such as varying the coupling strength between place and grid cells, could also demonstrate how adaptable the model is to different spatial and temporal contexts.\n\n3. The paper validates its model mainly by comparing it to MAP-based decoding, which may not fully illustrate the model\u2019s advantages or limitations compared to other established hippocampal-entorhinal network models. Adding comparisons with additional models, such as attractor networks or the \u201cconstrained range\u201d model (Sreenivasan & Fiete, 2011), would provide a more complete evaluation and clarify if this model truly overcomes non-local errors or merely trades one set of limitations for another. Moreover, a comparison with broader literature is missing, such as catastrophic errors [Lenninger et al.](https://elifesciences.org/articles/84531) and integration of landmarks in MEC [Ocko et al.](https://www.pnas.org/doi/10.1073/pnas.1805959115).\n\n4. The claim that the model reduces non-local errors in grid coding is central, but the current simulations only partially support this claim. It would help to design specific tests that induce non-local errors\u2014such as controlled phase shifts or systematically increasing noise levels\u2014to see how effectively the model mitigates them. Additionally, statistical validation of the error reduction compared to baseline models would make this evidence more robust and provide a clearer indication of improvement.\n\n5. The model relies on reciprocal interactions between place and grid cells but does not fully discuss the anatomical evidence supporting this communication. Specifically, mapping the model\u2019s feedback dynamics to known hippocampal projections (e.g., CA1 to MEC layer V or subiculum to MEC layers II/III) could help clarify the model\u2019s relevance to actual neural circuitry. Additionally, exploring the functional implications of these specific pathways within the model could provide a more comprehensive picture of how place and grid cells coordinate spatial representation in the brain."
            },
            "questions": {
                "value": "The paper is good, but there is insufficient comparisons and alignment to existing literature, to change my opinion I would like to see the weaknesses addressed as well as \n\n## Comparison with models\nGiven that the paper uses Continuous Attractor Neural Networks (CANNs) to model place and grid cell interactions, specific comparisons with alternative models should focus on how well CANNs address spatial stability, error correction, and flexibility in comparison to other neural network models that represent spatial information, particularly in grid and place cells. \n\n### 1. Comparison with Classic Attractor Networks\n   - While CANNs represent continuous variables well, they can suffer from drift and boundary effects. Comparing this model to classic attractor networks (like ring or torus attractors) could reveal whether CANNs offer better stability over large spatial maps. Moreover, there are several models with integration of landmarks in CANNs that should be discussed [Ocko et al.](https://www.pnas.org/doi/10.1073/pnas.1805959115), [Campbell et al.](https://pmc.ncbi.nlm.nih.gov/articles/PMC6205817/)\n   - Classic attractors often use recurrent feedback to \u201csnap\u201d activity patterns back into place when noise occurs. A direct comparison would show if the CANN model is more robust or if it encounters similar drift issues.\n\n### 2. Comparison with the Constrained Range Model (Sreenivasan & Fiete, 2011)\n   - The constrained range model uses non-overlapping spatial scales to extend range without aliasing. Comparing this to the CANN model could clarify if CANNs handle large, unambiguous spatial ranges better.\n   - The constrained range model achieves wide spatial coverage with few grid scales. Evaluating whether the CANN model requires more parameters or fine-tuning to achieve similar coverage would reveal if it offers any unique advantages in precision or robustness.\n\n## Comparison with experimental literature\nIn the paper, you claim that place cells and grid cells recieve independent input, which are cue-based and self-motion-based, respectively. However, this does not coincide with recent literature on cue integration in grid cells; please justify your claims wrt [Campbell et al. 2018](https://pmc.ncbi.nlm.nih.gov/articles/PMC6205817/), [Campbell et al. 2021](https://pubmed.ncbi.nlm.nih.gov/34496249/). Of particular interest is the latter paper, which concludes, \"Our gain change experiments in low-contrast conditions revealed that MEC map shifts were larger when landmark inputs were less certain, although the contrast sensitivity appeared to be sharp. This finding is consistent with the general Bayesian principle that inputs should be weighted according to their certainty\". These recent results could be a very nice opportunity to align model predictions with experimental findings.\n\n**Primes and approximations**\nWhile primes provide an elegant theoretical tool for modeling large, unambiguous spatial ranges, there is no direct evidence that grid cells employ prime numbers for encoding spatial information. The observed ratios in experimental studies are not exact primes; they are approximately constant ratios, around 1.4\u20131.7 between adjacent modules [Stensola et al.](https://www.nature.com/articles/nature11649)\n\nMoreover, the approximation $\\prod_i^M \\lambda_i\\approx \\bar{\\lambda}^M$ is not justified when $\\lambda_i$\u00b4s are primes, so why use this approximation when $\\frac{\\ln\\left(\\prod_{i=1}^M \\lambda_i\\right)}{M N_0} = \\frac{\\overline{\\ln \\lambda}}{N_0}$ gives the same conclusion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This theoretical paper applies continuous attractor neural networks (CANNs) to model hippocampal place cells (using a single CANN) and entorhinal grid cells (using multiple CANNs). The coupled CANN framework captures correlations between environmental and motion cues, linking place cells in the hippocampus with grid cells in the medial entorhinal cortex (MEC)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Place cells and grid cells are very important topics in the neuroscience field.\n\n2. This is a strong computational/theoretical paper with 20 equations in the main text and an additional 79 equations in the supplementary text. I briefly went through these equations and did not find any errors.\n\n3. The presented figures are relatively clear.\n\n4. Raw code is uploaded."
            },
            "weaknesses": {
                "value": "1. I am unsure whether this paper fits ICLR. The primary focus of this study is in the area of \"applications to neuroscience & cognitive science.\" There is no machine learning or deep learning component, only computational neuroscience work. It would likely be a better fit for journals like Neural Computation, PLOS Computational Biology, or Frontiers in Computational Neuroscience, and could also be suited for NeurIPS. I am not aware of ICLR publishing a pure neuroscience paper without a direct ML/DL connection in the past five years, which reflects the primary audience of the conference.\n\n\n2. There is no validation with experimental data. While theoretical analysis is important and useful, it is not sufficient. Over the past decade, many open-source datasets on place cells and grid cells have become available, such as those from the Buzs\u00e1ki Lab (https://buzsakilab.com/wp/database/), the Moser Lab (which publishes data with most papers, for example, https://doi.org/10.25493/SKKX-4W3, https://figshare.com/articles/dataset/Toroidal_topology_of_population_activity_in_grid_cells/16764508), and CRCNS (https://crcns.org/data-sets/hc). A recent Nature Neuroscience paper from the Dombeck Lab (2024) also provides raw data on both place cells and grid cells in a virtual linear track (https://www.nature.com/articles/s41593-023-01557-4#data-availability). I think experimental data like this would fit well with Figure 1 in this paper."
            },
            "questions": {
                "value": "1. Could you please change some of the colors in Figures 3c and 3d? The caption mentions \"blue, yellow, and green,\" but I could not identify any yellow. I don\u2019t believe I am color-blind, so I suggest using the top four colors provided in Matplotlib: ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'].\n\n2. In Figure 1d, there is a character \"c\" under the \"$\\pi$\". Was this intentional?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}