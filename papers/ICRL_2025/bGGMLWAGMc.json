{
    "id": "bGGMLWAGMc",
    "title": "Improving Reasoning Ability of Large Language Models via Iterative Uncertainty-based Preference Optimization",
    "abstract": "Direct Preference Optimization (DPO) has recently emerged as an efficient and effective method for aligning large language models with human preferences.\nHowever, constructing high-quality preference datasets remains challenging, often necessitating expensive manual or powerful LM annotations. Additionally, standard DPO exhibits suboptimal performance in complex reasoning tasks, such as mathematical and code reasoning.\nIn this paper, we introduce an approach to collect preference pairs through iterative sampling and execution feedback, tailored to the current learning state (e.g. well-learned, mis-learned, and unlearned) of the policy model.\nTo alleviate the failures of DPO and improve its applicability in reasoning tasks, we propose IUPO, an iterative uncertainty-based preference optimization method that achieves fine-grained preference control by assessing model confidence.\nWe validate our approach across three reasoning tasks, incorporating five established reasoning datasets and one self-curated dataset. Our experimental results demonstrate an overall improvement of 3.6% over the standard DPO method. \nFurthermore, our approach exhibits promising generalizability involving weak-to-strong (8B to 70B) and cross-model (Llama to Mistral) generalizations.",
    "keywords": [
        "Preference Optimization",
        "Large Language Model",
        "Iterative Optimization",
        "Uncertainty"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "This paper introduces an iterative uncertainty-based preference optimization method, improving the reasoning ability of large language models across four reasoning tasks.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=bGGMLWAGMc",
    "pdf_link": "https://openreview.net/pdf?id=bGGMLWAGMc",
    "comments": [
        {
            "summary": {
                "value": "This paper introduce Iterative Uncertainty-based Preference Optimization (IUPO) to improve the reasoning abilities (math and coding specifically in this paper) of large language models. Previous DPO always fail in complex reasoning tasks which require long reasoning chains because of the scarcity of high-quality preference data and the inherent limitation including coarse-grained (response-level) preference signal and decrease in preferred probability. To tackle these challenges, the authors propose the IUPO which automatically generates preference data (for math and coding) by compare the generated results with ground truth and additionally apply uncertainty measure to improve the models' confidence. Results on 6 datasets and plenty of analysis seem to show the improvements of IUP."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed IUPO works well for code and math settings.\n\n2. The introduced modification of DPO based on uncertainty makes sense."
            },
            "weaknesses": {
                "value": "However, there are several limitations:\n1. It seems that the answer extractor and executable environment are specifically tailored for code and math setting. As a results, can this framework be generalized to other reasoning tasks such as spatio/commonsense reasoning or even planing task?\n\n2. Also, since the preference creation process are relied on the accuracy of the answer extractor, what is the extraction accuracy? Will the extraction quality affect IUPO a lot?\n\n3. Are the models learned with IUPO on math and code data be generalized to other reasoning settings? The author might want to present some out-of-domain evaluations if the claim is to improve the reasoning abilities."
            },
            "questions": {
                "value": "1. How would you select the number of iterations? Are there any creteria?\n\n2. What do you think of the computation overhead vs performance gains? It seems that 3 or more iterations might results in extensive computation overhead while the performance improvements seems to be marginal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes IUPO: an iterative method for preference optimization in LLMs. The method works by iteratively (i) collecting preference data through response sampling and execution feedback from a virtual environment and (ii) optimizing the LLM with a modification of DPO that integrates an \"uncertainty score\". The approach has been tested on SQL, code, and math tasks and shows an overall 2.1/3.6% improvement over standard preference optimization approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The experiments are thorough and well-executed\n- The approach has an overall 2.1/3.6% improvement over standard preference optimization approaches on SQL, code, and math tasks.\n- The paper is well-structured and well-written:\n\t- The algorithm proposed is simple and clear\n\t- The author proposed a nice overview of the current limitations of DPO\n- The code is released with the paper; it's well-structured and documented."
            },
            "weaknesses": {
                "value": "- (Major) The uncertainty definition used is not mathematically grounded and lacks a rigorous definition and connection with common uncertainty estimators; it seems more like an ad-hoc heuristic used to boost performance than a proper estimator.\n- (Major) The data-collection procedure for IUPO requires Execution Feedback from a virtual environment. This drastically limits the applicability of the method to tasks with an execution environment available (e.g., coding, SQL). The generalizability of the approach to most tasks that do not have this virtual environment available is unclear. \n- (Major) The proposed method does not introduce exceptional variations compared to DPO. The method just introduces (i) a different way of sampling data and (ii) a minor modification of the DPO loss with the \"uncertainty score\" computed\n- (Medium) The paper is well-written, but the main section of your method (Section 3.3) is not very clear (see also Questions section):\n\t- The definition of uncertainty $\\Delta_t$ is not clear (see Questions)\n\t- The section uses inconsistent mathematical notation that makes it hard to follow. For example the subscript in  $\\Delta_t$ is used to indicate the token in the sequence but in the following equations the functional form $\\Delta(\\cdot)$ is used without explaining how the uncertainty for the full sequence is computed.\n\t- Equation 3 is quite cryptic and several details are left to the reader (e.g., the definition of relative distance $k$ and window size $K$)\n\t- The \"formal analysis\" (Equations 6-7) is almost entirely derived from [Pal et al. (2024) Equations 4-6](https://arxiv.org/pdf/2402.13228#page=20.55), however I believe there are several missing pieces in the explanation that make it hard to follow in your paper\n\t- It is not clear how IUPO addresses every issue raised in 3.2\n- (Medium) I believe the experimental part is missing an ablation of DPO with the uncertainty-based preference optimization (without the Iterative data collection part). It's not clear the role of the uncertainty score if used in standard DPO training."
            },
            "questions": {
                "value": "- Why did you choose Equation 2 to model uncertainty instead of other approaches? The choice of subtracting the top two tokens seems quite arbitrary and not grounded. I got that Wang and Zhou inspire it, but that paper has not been peer-reviewed. Can you elaborate a little bit on this choice? \n- Why do you call $\\Delta_t$ \"uncertainty\"? If the second token is lower it means that the model is less uncertain, but $\\Delta_t$ is actually higher (the opposite of uncertainty).\n- \"Specifically, we mine tokens with uncertainty measure below a fixed threshold \u03c4 and adjust the confidence of tokens within their subsequent window K:\" + Equation 3. I did not fully get what you meant by \"mine.\" Can you elaborate a little bit?\n- \"where \u2206(\u00b7) is a set of uncertainty measures for all tokens in response.\" It is not clear to me the definition of this set. \"Since \u2206t is less than 1, the probability of the token after the difference with preferred in \u03c0\u03b8(yl|x) will decrease, and the corresponding gradient will be lower, thus alleviating the decrease in the preferred probability issue.\" Could you elaborate?\n- There are probably several typos in: Equations 4-8 $\\mathcal{L}_{UPO}$ ; L323 \"U-DPO\" ; Figure 6 \"UDPO\"; Figure 5 BIRD \"IU-DPO\"\n- Why is the weak to strong generalization experiment (Table 3) performed just on the Text-to-SQL task and without comparing it with DPO and DPOP?\n- It's not clear how Figure 5 is computed. From my understanding you compute $\\Delta_t$ for each token but how is this measure aggregated over the length and the dataset? This is not explained in the paper (I suppose mean). Again it is counter intuitive that $\\Delta_t$  is called uncertainty while larger $\\Delta_t$  means more confidence in the generation. \n- \"Since the parameters \u03b8 of models are numerous, we focus on the logits $\\theta_j$ , which is input to softmax\". What is $\\theta_j$ ?\n- Why did you set the threshold $\\tau$ to 0.3? What is the rational behind?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Iterative Uncertainty-based Preference Optimization (IUPO) as an enhancement over Direct Preference Optimization (DPO) for large language models in complex reasoning tasks. IUPO incorporates iterative response sampling and execution feedback, specifically optimizing policy with token-level uncertainty measures. The experiments demonstrate improvements over DPO, showing IUPO\u2019s benefits in reasoning tasks across models and datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. IUPO represents an advancement by using uncertainty measures for token-level adjustments, potentially enhancing fine-grained optimization.\n2. The results demonstrate IUPO's effectiveness in improving reasoning abilities, specifically surpassing DPO on various benchmarks.\n3. The paper includes a range of experimental analyses, including model confidence evaluations and the impact of iterations, which help illustrate IUPO's capabilities and limitations."
            },
            "weaknesses": {
                "value": "1. The enhancement over DPO, while valuable, is relatively modest, especially on hard tasks like math. It appears incremental given that DPO's limitations are well-documented, and similar iterative techniques exist in the literature [1].\n2. Some contributions, particularly the automatic generation of preference data, lack clarity in their novelty relative to existing data generation techniques.\n3. The data collection settings required the final answer is auto-verifiable by either comparing to the gold answer or code execution. In such case, a clear and robust reward is already available and it is not clear why DPO is still necessary here.\n\n[1] Iterative Reasoning Preference Optimization. Richard Yuanzhe Pang et al."
            },
            "questions": {
                "value": "1. Continue on the weakness 3: given that your settings required to have an online automatic verifier to provide reward signal, you are able to directly apply policy gradient here. Without the need of training a reward model, the policy gradient algorithm is not much harder than DPO but have much better performance. One thing I am kinda confused by all the DPO papers nowadays is why they always get rid of the discussion with policy gradient method, especially in this kind of settings without the need of a reward model?\n2. Can you explain Figure 4 step 2? What's the meaning of circle N and circle 1? If that means the number of iteration, why you are sampling from the N-th iteration and 1-st iteration for the well-learned type?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduce an strategy for collecting preference pairs data through iterative sampling and execution feedback and a variant of the DPO(Direct Preference Optimization) algorithm named IUPO(Iterative Uncertainty-based Preference Optimization).\nThe authors claim that the IUPO method achieves fine-grained preference control by assessing model confidence and alleviates the distribution shift problem in offline DPO.\nMoreover, the authors conduct experiments across three reasoning tasks to demonstrate the effectiveness and generalization of IUPO."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. IUPO method is interesting and the \"Formal analysis\" part clearly demonstrates how the IUPO improves the DPO through uncertainty.\n2. The proposed method achieves a good performance on reasoning tasks in text-to-SQL, code and mathematical, offering an overall improvement of 3.6% over the standard DPO method.\n3. The motivation is clear, the analysis is coherent and well-reasoned, and the logic is sound."
            },
            "weaknesses": {
                "value": "1. Comparing IUPO only with DPO and DPOP may seem insufficient. Could the author compare the proposed algorithm with more relevant methods, such as KTO?\n2. The article does not mention anything related to training cost. I would like to know how much the training cost for three iterations increases compared to a single iteration? Does the IUPO have the advantage in training cost compared to other methods?\n3. There are some minor issues in the appendix: 1) In the caption of Table 7 and Table 8, the meanings of \"tick\" and \"cross\" have been reversed  2) On line 907, the format of the equal sign \"=\" is inconsistent with other lines of equal signs."
            },
            "questions": {
                "value": "See \"Weakness\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to address three main limitations of the DPO algorithm identified by recent works: (1) preference signals only on outcomes are too coarse-grained, (2) decreasing the rewards for both preferred and nonpreferred outputs, and (3) offline. The paper proposes a new preference data collection method and a new variant of the DPO algorithm to address this. For the data collection, given an instruction, the paper uses two models (before and after SFT) to collect three pairs of outputs, and iteratively uses them in the preference learning stage. For the preference learning algorithm, the paper proposes to augment DPO with token-level uncertainties that aims to down-weight the gradient of the uncertain tokens.\n\nExperiments on text2sql, code generation, and math reasoning are conducted with Mistral-7B and Llama3-8B. Promising results are achieved compared to DPO and SFT baselines.\n\nI will seriously consider revising my current negative score if the authors can help me understand some technical details in the response (details below)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The proposed data collection method and the IUPO algorithm are interesting and novel to the best of my knowledge\n- Motivation is clear\n- Comprehensive and interesting analysis"
            },
            "weaknesses": {
                "value": "- Some key technical details are missing, which makes it difficult for me to understand the algorithm and evaluate it. Specifically, the $\\odot$ operator in Eqs 4 and 5 are never explained. My guess from the context is that it denotes the elementwise product between two vectors; however, $\\pi_{theta}$ should be a scalar (probability) and it is unclear to me how this works. Maybe the authors can help me understand this\n- Can the authors comment on how the proposed algorithm relates to https://arxiv.org/abs/2404.12358 and https://arxiv.org/abs/2406.06887, and whether or not the paper should compare to them?\n- It would be interesting to explore whether DPO can benefit from the 3-iteration training\n- Probably due to LLM revision, some of the wording choices read strange and inaccurate to me. For example, \"unfortunate instances\" (line 150), the \"precision and clarity\" of the preference signals (line 138), \"coincides with\" (line238). I strongly recommend a thorough proof read."
            },
            "questions": {
                "value": "Can one go beyond 3 iterations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a method called Iterative Uncertainty-based Preference Optimization (IUPO) to enhance the reasoning capabilities of large language models (LLMs). The authors address the limitations of Direct Preference Optimization (DPO), particularly its suboptimal performance in complex reasoning tasks such as mathematical and code reasoning, due to the scarcity of high-quality preference data and the limitations of its alignment method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. IUPO automates the generation of preference data, which is typically a labor-intensive process. It does this by leveraging existing model responses and execution feedback, eliminating the need for additional manual annotations or more powerful models.\n2. IUPO employs an iterative approach to continuously update preference data, ensuring that the data remains relevant and in-distribution for the policy model. This iterative optimization is shown to be more effective than simply increasing the volume of preference data.\n3. This paper conducts comprehensive experiments across three reasoning tasks using established and self-curated datasets, showing an overall improvement of 3.6% over the standard DPO method."
            },
            "weaknesses": {
                "value": "1. The application of reinforcement learning algorithms for tuning SFT models with LoRA (Low-Rank Adaptation) might be somewhat trivial, given that LoRA adds some parameters. Could you demonstrate the performance advantage of your reinforcement learning algorithm under full-parameter fine-tuning settings?\n2. In terms of algorithmic performance, there are currently many algorithms that surpass the DPO(such as IPO and KTO). Relying solely on experimental validation with DPO may be somewhat insufficient and narrow. It would be advantageous to include some theoretical guidance or comparisons with other reinforcement learning algorithms to enhance the analysis."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}