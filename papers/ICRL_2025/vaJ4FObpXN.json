{
    "id": "vaJ4FObpXN",
    "title": "Learning to Explore and Exploit with GNNs for Unsupervised Combinatorial Optimization",
    "abstract": "Combinatorial optimization (CO) problems are pervasive\nacross various domains, but their NP-hard nature often necessitates problem-specific\nheuristic algorithms. Recent advancements in deep learning have led to the development of learning-based heuristics, yet these approaches often struggle with limited search capabilities.\nWe introduce  Explore-and-Exploit GNN ($X^2$GNN, pronounced x-squared GNN), \na novel unsupervised neural framework that combines exploration and exploitation for combinatorial search optimization:\ni) Exploration - $X^2$GNN generates multiple \nsolutions simultaneously, promoting diversity in the search space; \n(ii) Exploitation - $X^2$GNN  employs neural stochastic iterative refinement, where sampled partial solutions guide the search toward promising regions and help escape local optima.\n$X^2$GNN  employs neural stochastic iterative refinement to exploit partial existing solutions, guiding the search toward promising regions and helping escape local optima. By balancing exploration and exploitation $X^2$GNN achieves superior performance and generalization on several graph CO problems including Max Cut, Max Independent Set, and Max Clique.   Notably, for large Max Clique problems, $X^2$GNN consistently generates solutions within 1.2\\% of optimality, while other state-of-the-art learning-based approaches struggle to reach within 22\\% of optimal. Moreover, $X^2$GNN consistently generates better solutions than Gurobi on large graphs for all three problems under reasonable time budgets. Furthermore, $X^2$GNN exhibits exceptional generalization capabilities. For the Maximum Independent Set problem, $X^2$GNN outperforms state-of-the-art methods even when trained on smaller or out-of-distribution graphs compared to the test set.",
    "keywords": [
        "combinatorial optimization",
        "unsupervised learning",
        "graph neural networks"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "X^2GNN iteratively refines (exploit) a solution pool (explore) using GNN for combinatorial optimization, generalizing across problem distribution and size. % , and outperforming ML and traditional OR baselines.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vaJ4FObpXN",
    "pdf_link": "https://openreview.net/pdf?id=vaJ4FObpXN",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes an iterative neural approach to search for solutions to graph combinatorial optimization (CO) problems. The approach is based on two phases: the generation of a diverse pool of solutions and their iterative improvement. The training is unsupervised and relies on a composite loss that combines the continuous relaxation of the CO problem objective, a penalization of the constraint violation and a diversity-encouraging term. The approach is evaluated on three graph CO problems and shows a very good performance compared to learning and non-learning based methods as well as a strong generalization performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Novel unsupervised framework to generate solutions to graph CO problems \n* The framework components, in particular the architecture and the loss are generic and should apply to a variety of CO problems defined on unweighted graphs. \n* Original way to deal with several solutions for a given instance by constructing a \"K-coupled graph\" that allows to capture the whole collection of solutions to input to the refinement step.\n* Strong and consistent performance on the three problems and nice generalization to larger instances\n* The paper cites and compares to a number of relevant baselines in the experiments"
            },
            "weaknesses": {
                "value": "* While the paper explains well how the hyperparameters (K, C, T, $\\phi$) control the exploration/exploitation trade-off, the paper does not provide clear guidelines on how to choose good values for these hyperparmeters, except a grid-search. \n    * In Sec 5.5, the paper claims L253 \"Different search strategies are needed for MC and MIS due to the different feasible regions. MC requires exploration to avoid local optima, and MIS requires exploitation to improve solutions.\" I don't understand this argument, can the authors elaborate on this? \n    * In general, for all CO problems and search methods, there is a risk of getting trapped in a local minima and a need for solution improvement. I can't see how one can decide beforehand what is more important for a given problem, especially since it may depend on the instances. \n\n* Comparing the run times between learning-based approaches which usually run on GPUs and OR solvers which run on CPUs is always delicate to interpret and gives a partial view of the efficiency of the methods. While there is no straightforward way to make the comparison more fair, it should at least be acknowledged.\n   * In addition, the paper does not provide information on the machines on which the experiments were done -- this is especially important to appreciate the claims on the run times.\n\n* The main paper contributions are to compute meaningful output probabilities on the nodes but then only a simple rule or a greedy method is applied to construct a feasible solution (See paragraph Converting Soft Solutions to Hard Solutions). \n   * Using a threshold of 0.5 seems arbitrary to chose whether or not a node is part of the solution. Did the author try other values? How one can choose this threshold for a new problem?\n   * Given the probabilities, more sophisticated search methods can be applied such as beam search, Monte Carlo tree search or a least stochastic sampling (similarly to what is done when the model outputs heatmaps for example in the cited DIFUSCO method). \n   * Evaluating the proposed approach in combination with a stronger search technique, like the above, would be interesting and strengthen the claims. \n   * The question being: is the proposed approach useful only when a simple rule is used to construct the solutions or is it also helpful when combined with more sophisticated search?"
            },
            "questions": {
                "value": "* L258: the paper states that the training is done in two stages. Are they done sequentially or alternatively? The arrows in Figure 1 towards the \"loss block\" made it confusing to me.\n\n* In the Ablation section, when evaluating the impact of K, what was the value of C? In particular, it's important to evaluate the effect of K=1 with a large C, to demonstrate the value of the K-coupled solutions. \n\n* Remarks:\n  * L245, L252 it may be misleading to state that the diversity is \"imposed\" through a loss, \"encouraged\" would be more clear.\n  * It would be helpful to give an explanation of the corresponding equations L249 and L254 \n  * Since at training, T=1, the authors could get rid of the t index in the description to lighten the notations"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an explore-and-exploit Graph Neural Network (GNN) framework for combinatorial optimization (CO) problems. The key idea involves generating multiple solutions simultaneously to facilitate exploration while employing neural stochastic iterative refinement for exploitation. This approach effectively balances exploration and exploitation, leading to high-quality performance. Experiments conducted on three CO problems\u2014namely, the maximum independent set, maximum clique, and maximum cut\u2014demonstrate that the proposed algorithm outperforms learning-based algorithms in the literature."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed framework utilizes Graph Neural Networks and unsupervised learning to effectively balance exploration and exploitation for combinatorial optimization problems.\n\n2. Empirical results demonstrate high-quality optimization performance and goode generalization capabilities compared to learning-based baselines."
            },
            "weaknesses": {
                "value": "1. The encoder uses multiple original graphs as input; however, the rationale for connecting identical vertices across these graphs with edges is unclear. Table 5 only presents comparison results for the MIS and MC problems, why are the results for MCut not included? The description of the Drop Value is unclear, it would be better to provide a more detailed comparison of the results. Additionally, the drop value for K=2 shows a significant difference only in the context of MIS.\n\n2. There is a lack of an ablation study on the design of the total loss function. The total loss function includes includes objective quality, constraint satisfaction, and solution diversity. It would be useful to analyze the results when the loss function includes only one of these components, such as solely objective quality, as well as the combination of objective quality and constraint satisfaction, and the combination of objective quality and solution diversity. This comparative analysis could provide insights into the impact of each loss component on the overall performance.\n\n3. The result comparisons for each CO problem contain too few types of benchmarks. MC and MIS are closely related problems, and the instances tested in the experiments should remain consistent. Additionally, it would be beneficial to include more results for RB graphs and ER graphs. For the Max Cut problem, providing more results for BA graphs would also be helpful. Furthermore, testing the proposed algorithm on DIMACS and COLOR02 instances would demonstrate its generalization capabilities.\n\n4. It would be beneficial to explicitly state the limitations of the proposed approach, for example, the scalability issues."
            },
            "questions": {
                "value": "1. During the iterative refinement process, do the local optimal solutions occur at intermediate steps, or do they only manifest in the final iteration?\n2. How are the values \u200b\u200bof C and T determined for each CO problem?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose a framework that combines exploration and exploitation for combinatorial optimization (CO). The proposed framework explores the search space by generating a pool of solutions and exploits the promising ones through refinement. The model is based on Graph Isomorphism and Graph Attention Network, it outputs soft solutions that are heuristicly converted to hard solutions. The framework is applied and tested on three graph CO problems: the Maximum Clique Problem, the Maximum Independent Set Problem, and the Maximum Cut Problem."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The idea of using K-coupled solutions for exploration and Iterative Stochastic Refinement for exploitation looks original and promising.\n- The model shows excellent results; the proposed method outperforms state-of-the-art learning-based approaches not only on the training distribution but also in terms of generalization to larger problem sizes."
            },
            "weaknesses": {
                "value": "- Main concern: reproducibility seems impossible. There are no details about the implementation, only a brief description of the architecture  ('2L layers' of GIN and GAT), with no further details. There is no mention at all of the hyperparameters and the training process.\n- The proposed framework is tailored to a small subclass of CO problems. It can be applied to simple graphs, defined by their adjacency matrices, and to problems where solutions can be represented as binary decisions for each node. This makes the framework inapplicable to other classes of CO problems, such as routing or scheduling, as well as to any graph problems with node or edge features.\n- Although the paper claims that the method promotes solution diversity by generating multiple solutions simultaneously, in practice, the pool contains only two solutions. The method struggles when more diverse solutions are provided."
            },
            "questions": {
                "value": "1. Is it possible to apply this approach to other combinatorial optimization (CO) problems, such as routing or scheduling? Or to graphs with node/edge features (e.g., the Maximum Weighted Independent Set/Clique)?\n\n2. What is the motivation for using Graph Isomorphism Networks (GIN) and Graph Attention Networks (GAT) and constructing the multilayer graph in the way described? There is no theoretical or empirical discussion justifying this choice. The ablation study shows that using more than two coupled layers degrades performance, which is really surprising This may suggest that GAT struggles to propagate information effectively across more than two solutions and/or that the proposed simple multilayer graph, which connects only copies of the same node in G, is not powerful enough to represent relations between solutions. Did you try using a more sophisticated multilayer graph and/or a different method to aggregate the data between solutions?\n\n3. Following this, the multilayer graph for 2-coupled solutions (as used in the experiments) is very simple - it has 2N nodes and N edges (one edge per pair of corresponding original nodes). GAT is designed to aggregate information from many neighboring nodes, so using it on such a simple graph (in effect, it computes attention between just two nodes) seems odd and possibly unnecessary. Wouldn't a simple MLP achieve the same result?\n\n4. In the discussion of experiments, much emphasis is placed on comparing results based on running time, but no details are provided on how the experiments were conducted. Were the solvers and models run on the same hardware? Were they tested under the same conditions (e.g., serial or parallel execution)? Neural networks can often solve multiple instances in parallel batches on GPUs, which might not be the case for solvers executed on CPUs (which are inherently much slower than GPUs by design). Claims about running times are only comparable if all methods are tested under similar conditions; otherwise, the comparison could be confusing. E.g. claim No. 4 \"We additionally allow solvers a 30-minute time limit, which is at least 24 times longer than our longest-running model.\" could be misleading. By checking results, Gurobi is in most cases much faster than the proposed method (e.g. in Table 1 Gurobi vs. longest-running model for RB250 is 0.31s vs. 1.41s). \n\n5. All CO problems have simple greedy heuristics, such as choosing the node with the smallest degree for the MIS problem. Did you attempt to exploit this for the initialization of node features (e.g., assigning lower probabilities to high-degree nodes since they are less likely to be part of the solution)? This approach might provide a better initialization than random and could lead to faster learning."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes GNN-based framework to solve several classic combinatorial optimization problems. The proposed approach behaves like a population-based heuristic method. Since extensive efforts have been devoted to the development of machine learning methods for addressing combinatorial optimization, I'm concerned about whether it can outperform the state-of-the-art algorithms."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed network generates $K$-coupled solutions and behaves like a population-based heuristic method. This is kind of novel."
            },
            "weaknesses": {
                "value": "I have a few concerns below.\n\n- Line 245, the loss function includes constraint satisfaction and solution diversity. How to choose $\\lambda_1$ and $\\lambda_2$? Usually, the penalty method demonstrate very weak generalization capabilities. Hence, I personally think combining several terms in the loss function is not a good idea.\n\n- For MCut and MIS comparison, a state-of-the-art algorithm [1] should be considered as baseline. This algorithm [1] is quite scalable and able to provide high-quality solutions to MCut and MIS.\n\n- The proposed algorithm is not very scalable. For example, in Table 2 and 3, the computational time increases quickly with the problem size. MIS, MC and MCut are simple combinatorial optimization problems. Why not consider some large-sized instances (for example, Gset instances for MCut,  https://web.stanford.edu/~yyye/yyye/Gset)? How does the proposed algorithm perform?\n\n[1] Schuetz, M.J., Brubaker, J.K. and Katzgraber, H.G., 2022. Combinatorial optimization with physics-inspired graph neural networks. Nature Machine Intelligence, 4(4), pp.367-377."
            },
            "questions": {
                "value": "See the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}