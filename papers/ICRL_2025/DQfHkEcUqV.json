{
    "id": "DQfHkEcUqV",
    "title": "Learning Extrapolative Sequence Transformations from Markov Chains",
    "abstract": "Generative sequence-level models are appealing in settings where the desired outputs must adhere to global constraints. In these settings,  autoregressive sampling can struggle to explore the solution space sufficiently to find the optimal solution, especially when optimal solutions involve extrapolating beyond the training data. However, searching the solution space through approximate inference methods such as Markov chain Monte Carlo (MCMC) is computationally expensive. To address this computational burden, we propose to train a separate inference network based on selected states from Markov chains. The proposed approach is validated on three problems: protein sequence design, text sentiment control, and text anonymization. We find that the learned inference network confers many of the same generalization benefits as the slow sampling process, but with the additional benefit of high sample efficiency. This is particularly true in cases where the model must extrapolate beyond the range of values seen in the training data, but our approach demonstrates success even on the anonymization task, which relies solely on interpolation. Finally, we analyze the effects of various strategies to select states from the search space.",
    "keywords": [
        "Large language model",
        "Markov chain Monte Carlo"
    ],
    "primary_area": "generative models",
    "TLDR": "We can distill extrapolative sequence transformations from Markov chains.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=DQfHkEcUqV",
    "pdf_link": "https://openreview.net/pdf?id=DQfHkEcUqV",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a method for learning sample-efficient extrapolative sequence transformations by first using Markov Chain Monte Carlo (MCMC) sampling to explore the solution space, then training a separate inference network on selected states from these chains. The authors demonstrate their approach on three tasks: protein sequence design, text sentiment control, and text anonymization. The key innovation is sub-sampling informative state transitions from MCMC chains to train a more efficient model that can achieve similar or better performance with significantly fewer inference steps."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Tackles an important problem (extrapolative generation) with a novel approach\n- Impressive empirical results, especially on protein engineering\n- Thorough ablation studies examining different components\n- Clear practical benefits in terms of sample efficiency"
            },
            "weaknesses": {
                "value": "1. The validation methodology for protein engineering is fundamentally flawed. Without a separate validation set, there's no reliable way to assess generalization ability. While the authors acknowledge this and attempt some mitigation strategies, they don't address the core issue of potential overfitting to the test set through architecture and hyperparameter choices.\n\n2. The theoretical justification for extrapolation capabilities is weak. Although they discuss how their approach captures dependencies between hidden states, they fail to provide a compelling explanation for why this enables better extrapolation. The paper lacks formal analysis of extrapolation capabilities or theoretical bounds on performance.\n\n3. The baseline comparisons are inadequate, particularly in protein engineering where they only compare against two baselines from a single paper. This omits numerous recent protein design methods and alternative approaches. While sentiment and anonymization tasks include more baselines, they still miss obvious comparisons with state-of-the-art methods.\n\n4. Several key ablation studies are missing. There's no exploration of different MCMC sampling strategies, limited analysis of energy function designs, and no investigation of how performance scales with sequence length. These missing analyses make it harder to understand which components are crucial for success.\n\n5. The energy function design choices feel arbitrary and aren't well justified. The weighting between different terms lacks thorough explanation, and there's no meaningful discussion of how to design effective energy functions for new tasks, limiting the method's applicability to other domains.\n\n6. Memory consumption issues are acknowledged but not adequately addressed. The authors note their models use significantly more memory than alternatives but offer no solutions. The analysis lacks discussion of how usage scales with sequence length or batch size, and fails to analyze memory-computation tradeoffs.\n\n7. Reproducibility and practical concerns are significant. Many implementation details are buried in appendices, hyperparameter sensitivity isn't thoroughly analyzed, and there's limited discussion of failure cases or scalability to longer sequences. These omissions make it difficult for others to build upon their work or apply it to real-world problems."
            },
            "questions": {
                "value": "- Why not include more baselines, especially recent work in protein engineering?\n- How sensitive is the method to the choice of energy function?\n- Have you explored using curriculum learning approaches for training?\n- How does performance scale with sequence length?\n- What are the key factors limiting memory efficiency?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the problem of intractable inference in sequence models. It is proposed to sample MCMC chains using an infilling LM as a proposal, to create subchains using various procedures (e.g., selecting the points where the target energy decreases), and to train an autoregressive model on these subchains. This model can then be used to sample modes of the target distribution more efficiently. This method is evaluated on a protein design task and two language generation (editing) tasks and shows somewhat promising results."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The procedure of amortising subchains of MCMC chains by a non-Markovian sequence model is interesting and could be applicable to structured sampling problems well beyond the protein and language tasks studied here. \n  - For example, chain-of-thought reasoning, among other intractable inference problems in language, has been interpreted as latent variable inference and addressed using MCMC ([Phan et al.,  NeurIPS'23](https://arxiv.org/abs/2312.02179), [Lew et al.](https://arxiv.org/abs/2306.03081)), amortisation ([Hu et al., ICLR'24](https://arxiv.org/abs/2310.04363)), hybrid methods ([Zhao et al., ICML'24](https://arxiv.org/abs/2404.17546)), and distillation into tractable models ([Zhang et al., 2024](https://arxiv.org/abs/2406.13892)).\n- The inclusion of both LM and biological sequence design tasks is a good way to show the generality (although toy experiments on something synthetic and *very* simple, where the target is well-understood, would also be helpful).\n- Results generally comparable with prior work in each task with (possibly -- see questions below) higher efficiency of inference."
            },
            "weaknesses": {
                "value": "Overall, both the writing and the experiments need substantial improvement. It is quite difficult to understand the algorithm and the experiment setup, nor are the results particularly strong.\n\n- The writing in the first two pages is imprecise and does not set up the problem well.\n  - The first sentence of the abstract does not make sense to me. (Doesn't \"desired outputs\" already presuppose we are talking about generative modelling -- so what is the meaning of saying generative models are \"appealing\"? \"Sequence-level\" as opposed to what?) \n    - In general, the abstract does not explain well the problem setting and does not prime the reader to expect what is to come. The reader is left wondering: Are we talking about generative modelling from data or sampling given a density? Are we training a new model or constraining an existing one?\n  - The introduction does not set up the problem to be solved in a clear way. \n    - The first 2.5 paragraphs or so of the introduction seem to be saying approximately \"generalisation is important\". \n    - However, the **main objects** involved in the problem setting are not introduced. Thus it is not clear what is being talked about when \"original\" and \"sampled\" sequences are mentioned, what the EBM has to do with infilling, etc.\n    - Second paragraph: Lu et al. and Holtzman et al. are inaccurately described. The latter studies strategies to clip the tail of the next-token distributions, while Lu et al. studies MCTS as proposed by reference [59] in that paper, which seems to have no connection to such clipping.\n    - In the last paragraph of the intro, $q_\\theta$ appears, but it is never defined.\n- Section 2: Please define all symbols when they are first used. As far as I could infer:\n  - $\\cal X$: space of sequences\n  - $\\cal Y$: set of \"properties\"\n  - $o:{\\cal X}\\to{\\cal Y}$: oracle assigning a property to every sequence\n  - ${\\cal X}^{\\rm train}$: training set\n  - ${\\cal Y}^{\\rm train}$: unclear; is it $\\{o(x):x\\in{\\cal X}^{\\rm train}\\}$? That is, we wish to generate sequences whose properties, as computed by an oracle, were not observed in the training set?\n- Section 3:\n  - Is the score giving the density or the log-density? Contradiction between \"probability proportional to sequence-level score $s(x)$\" (L144) and the equation in L149.\n  - The following does not make sense to me (LL156-158): \"masked language modeling objectives implicitly define EBM with the masked-infilling objective serving as a proposal distribution\".\n    - How can a [training] **objective** define a distribution? Do you mean that the trained model defines a distribution? Same in LL162-163: the self-supervised pre-training process seems to have nothing to do with the use of the trained infilling model (so how can we say that MH sampling is based on the self-supervised training?).\n    - An EBM is defined by an energy function, not by a proposal distribution used to perform MH on it. Do you mean that the infilling model approximates a collapsed Gibbs sampler for a certain EBM? Or do you mean that the infilling model is an effective proposal for MH sampling of EBMs that happened to be trained on similar data?\n    - Even in the former case, things are somewhat more subtle than this. The conditional distributions given by infilling may not be compatible with any joint distribution, see, for example, [Henningen and Kim, ACL'23](https://arxiv.org/abs/2305.15501) and [Wang and Cho, NeuralGen@NAACL'19](https://arxiv.org/abs/1902.04094).\n    - In the end, I am left confused about the setting: the infilling model defines a EBM (L157) and is also a proposal distribution that is used to propose transitions (L160), but the proposals are accepted/rejected using the MH criterion on a EBM (L161), which seems to require an explicit energy model to compute the acceptance probability.\n  - A little diagram would help to understand the different ways of forming trajectories for training $q_\\theta$. (It is also confusing that $q$ (proposal) and $q_\\theta$ (amortised sampler) are denoted by the same letter.)\n- Experiments:\n  - Missing details:\n    - In each experiment, please specify exactly, in an equation, what the target energy model is. I found some notes on this in Appendix D, but they left me with more questions. In general, I would suggest to move some experiment details to the appendix, but state as directly as possible what sampling problem is being solved.\n    - It is not stated which version of training data for $q_\\theta$ is used. It can be inferred from the ablation study in Section 5, but would be good to make explicit. \n  - I am not convinced by evaluations in 4.1:\n    - The fluency is not reported for ICE.\n    - Extrapolation metric: why should we expect that \"make this negative\" would lead necessarily to 1-star and not to 2-star reviews? It would be more informative to compare the full distributions of the base model and with editing, for each editing method.\n      - How to understand that the second and third columns in Table 2 sum to more than 1, if they are the proportions of reviews with (in the example of negative sentiment) 2 and 1 stars, respectively?\n      - In fact, generating **too many** 1-star reviews, together with the high fluency, could actually indicate mode collapse. There is no diversity metric to show that this is not happening.\n  - The comment on diversity also applies to Section 4.2. Why was ICE not considered in this problem?\n  - How do the methods compare in execution wall time (both for training and drawing a single sample)?"
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a method for enhancing sequence generation in models that need to extrapolate beyond training data that aims to extend beyond Padmakumar et al. (2023). It aims to be comparable with sampling-based methods such as MCMC in performance while being computationally efficient as existing works. To this end, it introduces an inference network trained on selected states from Markov chains. This approach is tested on protein sequence design, text sentiment control, and text anonymization tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Overall, despite the fact that the paper is not well-written and easily understood, I admit that the paper conveys the problem background, the motivation, as well as their method relatively clearly. From my understanding, the problem of conditional sequence generation with extrapolated target properties is undoubtedly important, and I encourage the authors to conduct in-depth study of the problem. In addition, the goal of trying to find a method with (1) good performance, such as sampling-based methods, and (2) efficiency, such as existing iteration-based methods, is well-motivated and acute."
            },
            "weaknesses": {
                "value": "In my opinion, there are several major concerns that prevent current paper from being accepted. Specifically,\n\n- In the first place, instead of trying to improve on the existing ICE strategy, the authors should analyze whether and why current autoregressive (AR) models are unable to extrapolate, given their belief that iterative transformations are an alternative. Current LLMs have undoubtedly demonstrated their capacities to generalize beyond training data, especially in data-abundant areas. As a result, various conditional generation methods have proven effective, even when the conditions are highly user-crafted and unlikely seen in the training set. To well motivate the paper, the authors should explicitly analyze, (better) both theoretically and empirically, that current conditional generation methods such as prompts-based techniques or classifier guidance techniques, are insufficient for extrapolation. \n- Since the authors, unlike ICE, are training an additional model to directly \"predict\" minor transformations that improve a target property, it becomes unclear to me why the original AR models equipped with additional data cannot do this. I do not see the incentive to train an additional model (as a surrogate) for this. Specifically, if you consider each transformed \"sequence\" as a \"token\" (that actually is a sequence) of an \"AR\" model and train on this \"token sequence\", I believe the model is, by nature, designed to generate sequences with better target property iteratively. In this case, what is the reason to have two models in the first place?\n- Following the second point, I think the current comparison is unfair because the proposed method requires a large amount of preference data pairs to train. However, this data needs to be generated via standard MCMC-based methods and is unlikely to be generated computationally efficiently. As a result, the fact that the proposed method outperforms existing methods can be a natural fact that it is just leveraging more information, which again makes me confused about whether some alternatives can leverage these data more effectively. A more fair comparison should be made. For instance, when comparing efficiency, data generation & training time should be presented; when comparing performance, you should also show that existing methods with the dataset you generate are still inferior to your method.\n- Finally but minorly, I think the presence of tables is far from clear. For instance, in Table 1, $q_\\theta$ has lower $-1$ and $-2.5$ scores, and I am not sure whether this can be referred to as \"outperform\". There might be a misunderstanding, but the authors should make it clear what we are expected to read from each table."
            },
            "questions": {
                "value": "No additional questions beyond the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenge of **efficient extrapolation in sequence modeling**. The problem setup is as follows: Given a scoring oracle (or a function approximating it) that evaluates sequences\u2014such as protein stability or sentiment of review comments\u2014and training data limited to a specific score range, $\\mathcal{I}$, the objective is to generate sequences that achieve scores outside this range. This extrapolation aims to transform sequences beyond the initial constraints imposed by the training dataset.\n\nTo me, the proposed solution can be framed as a reinforcement learning (RL) approach with the following key steps:\n\n- Sample sequences from an initial model (policy).\n- Randomly mask parts of each sequence.\n- Predict and fill masked sections using a masked language model (MLM).\n- Evaluate the transformed sequence via the scoring function.\n- Retain transformations that yield improved scores.\n\nAdditional heuristics are employed to enhance training efficiency, such as optimizing inference by selectively dropping intermediate reasoning steps when appropriate."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The experimental results on sentiment extrapolation and stable protein sequences indicate improvements on the proposed metrics, supporting the method\u2019s effectiveness in generating high-scoring sequences beyond the training data\u2019s score range while maintaining the chosen reference quality measures.\n\n- The work provides insights into the limitations of MCMC, and demonstrates how a structured RL approach can offer improvements in extrapolative tasks."
            },
            "weaknesses": {
                "value": "1. **Lack of context**:\n   Although some works on RL for controlled generation are discussed in the appendix, presenting the approach as an RL-based sequence generation technique in the main text would better contextualize the work. The core steps\u2014sampling, masking, scoring, and retaining high-reward samples\u2014closely resemble an RL framework and have been studied in the context of language modeling. For instance I can think of the following:\n\n   - The MLM and scoring function could be interpreted as critics, similar to those used in RL formulations for text generation [1,2]. I think both works have similar criteria of retaining only if the rollout is correct (0-1 rewards), which is similar to your formalization.\n   - The approach of appending scored sequences mirrors reward conditioning or goal-conditioned policy training in RL [3,4].\n\n   It would be easier to understand, compare, and extend this work if it is framed within an established framework. \n\n2. **Comparisons with RL Baselines**:\n   Given the alignment with RL methodologies, comparisons to baseline RL approaches would strengthen the empirical analysis. For example, on tasks like protein engineering, including a simple reward-conditioning baseline that takes ddG values as input or similar methods referenced in the appendix would provide a more complete evaluation. For instance, the protein sequence experiments are only compared with Padmakumar et al. (2023). Comparing with simple/basic RL methods would also work also as an ablation, justifying the more complex steps of the proposed method.\n\n3. **Clarity and Editorial Suggestions**:\n   Certain sections of the paper could benefit from additional clarity. For instance, the sentence on line 365 is incomplete, and some overly long sentences could be simplified for readability. Introducing mathematical notation earlier (e.g., at the end of page 2) would also improve consistency and clarify the problem setup throughout the paper."
            },
            "questions": {
                "value": "To improve the interpretability of the evaluation, it would be beneficial to include example (text) sequences alongside quantitative metrics, such as fluency or Sentence-BERT similarity scores. While these metrics offer quantitative insights, they are difficult to interpret without sample outputs to contextualize them. For instance, it is vert vague for me what the impact of a 0.13% improvement in fluency or a BERT similarity of 0.1 would be. It would be clearer if  these metrics are accompanied by representative examples, e.g. showcasing semantic similarity, etc. I wonder how the actual output texts compare to baselines.\n\n**References**\n\n1. Zelikman et al., *STaR: Bootstrapping Reasoning With Reasoning*\n2. Zelikman et al., *Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking*\n3. Lynch and Semanet, *Language Conditioned Imitation Learning over Unstructured Data*\n4. Shypula, Madaan, et al., *Learning Performance-Improving Code Edits*"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}