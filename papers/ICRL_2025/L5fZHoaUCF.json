{
    "id": "L5fZHoaUCF",
    "title": "Cognitive Overload Attack: Prompt Injection for Long Context",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in performing tasks across various domains without needing explicit retraining. This capability, known as In-Context Learning (ICL), while impressive, exposes LLMs to a variety of adversarial prompts and jailbreaks that manipulate safety-trained LLMs into generating undesired or harmful output. In this paper, we propose a novel interpretation of ICL in LLMs through the lens of cognitive neuroscience, by drawing parallels between learning in human cognition with ICL. We applied the principles of Cognitive Load Theory in LLMs and empirically validate that similar to human cognition, LLMs also suffer from \\emph{cognitive overload}\u2014a state where the demand on cognitive processing exceeds the available capacity of the model, leading to potential errors. Furthermore, we demonstrated how an attacker can exploit ICL to jailbreak LLMs through deliberately designed prompts that induce cognitive overload on LLMs, thereby compromising the safety mechanisms of LLMs. We empirically validate this threat model by crafting various cognitive overload prompts and show that advanced models such as GPT-4, Claude-3.5 Sonnet, Claude-3 OPUS, LLAMA-3-70B-Instruct, Gemini-1.0-Pro, and Gemini-1.5-Pro can be successfully jailbroken, with attack success rates of up to 99.99\\%. Our findings highlight critical vulnerabilities in LLMs and underscore the urgency of developing robust safeguards. We propose integrating insights from cognitive load theory into the design and evaluation of LLMs to better anticipate and mitigate the risks of adversarial attacks. By expanding our experiments to encompass a broader range of models and by highlighting vulnerabilities in LLMs' ICL, we aim to ensure the development of safer and more reliable AI systems.",
    "keywords": [
        "In Context Learning",
        "Cognitive Overload",
        "LLMs Safety",
        "Prompt Injection Attack"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We examines ICL in LLMs through the lens of cognitive neuroscience, revealing that LLMs, like humans, can experience cognitive overload that makes them vulnerable to adversarial prompts, and demonstrates how this can lead to effective jailbreaks.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=L5fZHoaUCF",
    "pdf_link": "https://openreview.net/pdf?id=L5fZHoaUCF",
    "comments": [
        {
            "summary": {
                "value": "- Paper introduces a suite of additional problems to task the LLM with and shows that this degrades performance on the main task\n- Paper uses multi-task setup to design jailbreaks \n- The paper makes very strong claims about similarities between human cognition and LLMs, which are not always supported by evidence.\n- The paper's setting is already studied in prior works, and the novelty of the contribution is unclear to me in this context (see details below)\n- the jailbreaking results are unclear. It is not obvious whether the LLM jailbreak is actually caused by \"cognitive overload\"."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- easy to follow\n- simple experiments\n- well-motivated setting"
            },
            "weaknesses": {
                "value": "- \u201cdemonstrating that CLT applies to LLMs\u201c strong claim for the limited empirical results\n- HC not introduced in intro\n- \u201cLLMs process input tokens by identifying semantic patterns and relationships, which are abstracted into embeddings and hidden states, similar to abstraction of concepts in HC\u201c unsupported claim\n- Drawing experiment is only based on visual identification of two types of animals (this needs more data to be statistically significant)\n- Prior work on performance degradation in multi-task setting already exists -- how is this work novel here?\n\"LLM Task Interference: An Initial Study on the Impact of Task-Switch in Conversational History\": examines how task-switching within a conversation affects LLM performance\n\"Exploring the Zero-Shot Capabilities of LLMs Handling Multiple Problems at once\": investigates LLM performance when presented with multiple problems at once.\n- Why does Table 1 use different Judge LLMs?\n- It appears that much of the jailbreaks already occur for CL1 level for Forbidden Question dataset (Table 1). Is it really the cognitive overload that breaks the models or just the paraphrasing and repeated trials (6 times)?"
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates a phenomenon called cognitive overload in LLMs. Similar to human cognitive process, LLM also suffer from the overload problem, inspired by this, the author presents a cognitive overload attack to jailbreak aligned LLMs.\n\nThe contribution of this paper is as follows:\n1. This paper demonstrates that, cognitive overload, which occurs when multiple complex tasks are combined in a single prompt, will significantly degrades LLMs' performance.\n2. Building on this principle, the authors attempt to jailbreak an aligned LLM by embedding malicious prompts within cognitively overloaded instructions. Their experiments demonstrate the effectiveness of this attack method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to follow.\n2. The experiments of both cognitive overload identification and cognitive overload attacks are complete."
            },
            "weaknesses": {
                "value": "1. This work appears to be an incremental extension of previous research [1], offering limited novel contributions.\n2. The findings are somewhat predictable: when LLMs are tasked with handling multiple simultaneous instructions, their ability to properly follow any single instruction naturally deteriorates. Consequently, it's unsurprising that LLMs become more susceptible to executing jailbreak prompts when distracted by multiple competing tasks.\n\n[1] Xu et al. Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking"
            },
            "questions": {
                "value": "As listed in weaknesses part, my major concern is about its contribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed.",
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper introduces a jailbreak attack method that may induce harmful contents such as discrimination. The paper itself doesn't contain any discrimination / bias / fairness concerns."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The author's conducted the first study that directly compares In Context Learning in LLMs with human cognitive learning. They empirically demonstrated that increased cognitive load leads to cognitive overload in LLMs, which degrades performance on secondary tasks similar to human cognition (HC). Then they introduced an attack method combining with cognitive overload, which exploits cognitive overload to bypass LLM safety mechanisms. The method showed that higher-capability models can create cognitive overload prompts to attack other LLMs, demonstrating the transferability and widespread impact of cognitive overload attacks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Important motivation. Analogy between the reasoning process of LLMs and learning in Human Cognition.\n* Surprisingly, the purposed attack method has excellent performance with high attack success rates.\n* Paper is clear with no major problem in writing."
            },
            "weaknesses": {
                "value": "I am not an expert in cognitive science and this is why I chose confidence in 2, please correct me if I\u2019m mistaken in these aspects.\n* Firstly, I personally believe that this paper should be submitted in other venues rather than being applied in jailbreaking LLMs. Without a certain background, it is difficult to understand why the different tasks and patterns of cognitive load measurement (T1-T7) in Section 3 are designed and defined in this way. (I have reviewed relevant references)\n* It cannot be determined whether different combinations will have different cognitive load effects on the order between T1-T7 or C1-C6.\n* What does the average score mean in Figure 1B (I have seen the Figure 4)? Is the self-report method reliable and can other quantitative indicators be used to measure it?\n* The cognitive experiment only conducted visual analysis of the code task, more other tasks or datasets should be included.\n* In Cognitive Overload Attack, the author did not compare it with some existing attack methods, such as GCG, PAIR, etc. (see some benchmark in jailbreaking task)\n* The derivative questions generated may cause some questions not follow the instruction during paraphrasing, such as outputting an anwser or a nonharmful question.\n* Too many hypothesis need to be made under cognitive load conditions."
            },
            "questions": {
                "value": "Please see the weaknesses. The limitations are listed in detail in the appendix, but there are a few that I believe need to be addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper hypothesizes that cognitive overloading safety-aligned LLMs with multiple distracting requests alongside the original harmful one could be a successful jailbreaking technique. This hypothesis was based on analogies between human learning and cognitive load limits and LLMs. They spend a significant amount of the paper on this hypothesis and experiments to demonstrate cognitive load in LLMs based on the deterioration of performance under load. They then show that under cognitive load attacks, SOTA production LLMs are vulnerable to standard jailbreaking techniques,"
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1. I think that this paper's engagement with cogsci work is substantial, useful, and nontrivial. I think that papers like this, independent of actual contributions, serve a sort of bridging value. \n\nS2. In table 1, I think that the overall ASRs are fairly impressive."
            },
            "weaknesses": {
                "value": "W1: I think https://arxiv.org/abs/2403.08424 should probably be cited and discussed. And any others like it as well. Based on my view of it, they have some something of the same type and to the same effect as was done here. Also in your response, I'd be interested in comments on contrasts and novelty. \n\nW2: In general, I wonder the extent to which what is studied here is closely related and reframed compared to other jailbreaking techniques. For example, when people JB models using leetspeak, low resource languages, personas, or asking models to simulate hypothetical tasks, is this not conceptually similar to what is done here? Isn't the CL increased in all of these cases? One thing that I think this paper may not be the most clear on is the extent to which we should expect their hypothesis to apply to a large number of jailbreaks (which is not thoroughly investigated here) or just some very specific ones they showcased here. I would be interested in more work to clarify the scope of the authors' hypothesis and to further the differences between their techniques and other common ones."
            },
            "questions": {
                "value": "Q1: What procedure was used to calculate the ASR? What was the baseline no-attack ASR for the rows of table 1? A common problem with calculating jailbreak ASRs is that incompetent but compliant responses are sometimes marked as successful attacks under some methods. I would like to see the autograding details, the baseline, and an discussion of the potential false positive rate."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethical concerns."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}