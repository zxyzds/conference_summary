{
    "id": "UunCPtPOlZ",
    "title": "McEval: Massively Multilingual Code Evaluation",
    "abstract": "Code large language models (LLMs) have shown remarkable advances in code understanding, completion, and generation tasks. Programming benchmarks, comprised of a selection of code challenges and corresponding test cases, serve as a standard to evaluate the capability of different LLMs in such tasks. However, most existing benchmarks primarily focus on Python and are still restricted to a limited number of languages, where other languages are translated from the Python samples (e.g. MultiPL-E) degrading the data diversity. To further facilitate the research of code LLMs, we propose a massively multilingual code benchmark covering 40 programming languages (McEval) with 16K test samples, which substantially pushes the limits of code LLMs in multilingual scenarios. The benchmark contains challenging code completion, understanding, and generation evaluation tasks with finely curated massively multilingual instruction corpora McEval-Instruct. In addition, we introduce an effective multilingual coder McEval trained on McEval-Instruct to support multilingual programming language generation. Extensive experimental results on McEval show that there is still a difficult journey between open-source models and closed-source LLMs (e.g. GPT-series models) in numerous languages.",
    "keywords": [
        "Benchmark",
        "Code Intelligence",
        "Multilingual",
        "Large Language Model",
        "Multilingual Multitask Code Evaluation"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "To facilitate the development of code LLMs, we introduce a complete framework that includes the multilingual code instruction corpora, multilingual coder (mCoder), and multilingual code evaluation benchmark.",
    "creation_date": "2024-09-20",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=UunCPtPOlZ",
    "pdf_link": "https://openreview.net/pdf?id=UunCPtPOlZ",
    "comments": [
        {
            "summary": {
                "value": "The authors propose a new code benchmark called McEval. The proposed benchmark has 40 programming languages with 16k samples. The benchmark spans code generation, code completion and code understanding tasks. The authors also introduce mCoder trained on the McEval-Instruct corpora."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The paper is well-written and is very easy to read and follow. The paper is also a significant leap towards better evaluation for code models. Current models are mostly evaluated on smaller benchmarks like HumanEvalPack or MBPP which have very few languages (<10) and this limits the ability to effectively evaluate code models.\n\nThe authors' work is quite significant since it introduces a new benchmark with 40 programming languages which is more than 4x of any previous benchmarks. The paper also presents an instruction corpora.\n\nI overall like the paper and the amount of experiments done by the authors."
            },
            "weaknesses": {
                "value": "The instruction corpora is generated using GPT-4 and thus is commercially unusable. I understand that collecting good human annotated instructions is a significant undertaking and might be infeasible."
            },
            "questions": {
                "value": "1. Is the dataset being released?\n2. Have you evaluated [Granite code models](https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330), it would be nice to have them in the report since there are very few SOTA code models available under Apache 2.0 license?\n3. Are the human annotators fairly compensated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The MCEVAL benchmark introduces a challenging, multilingual evaluation framework for code LLMs, covering 40 programming languages with 16K test samples. Alongside this, MCEVAL-INSTRUCT - a curated multilingual instruction corpora, and MCODER\u2014a multilingual coder trained on MCEVAL-INSTRUCT are introduced."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "(1) Extensive study and effort has been spent in generating and documenting this code benchmark.\n\n(2) While HumanEval and MBPP are the popular code programming benchmarks today, Code LM community needs additional multilingual programming benchmark. Hence, this paper addresses a problem statement that is of demand today."
            },
            "weaknesses": {
                "value": "GPT models, like, gpt-4-1106-preview, has been used to generate the problem description for code instruction corpora. That could have influenced GPT models to lead in the benchmarks with significant performance margins over other models."
            },
            "questions": {
                "value": "Is there any study done to validate the accuracy or correctness of  MCEVAL benchmarks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces MCEVAL, a massively multilingual code evaluation benchmark covering 40 programming languages with 16,000 test samples. It aims to advance the evaluation of code LLMs in multilingual scenarios. The benchmark includes tasks for code generation, completion, and explanation. Additionally, the authors present MCODER, a multilingual coder trained on the MCEVAL-INSTRUCT corpora, demonstrating its effectiveness in multilingual programming language generation. In its experiments, this paper highlights the gap between open-source and closed-source models and provides a comprehensive framework for evaluating and improving code LLMs across diverse languages."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This is truly a BIG project. The authors have made significant contributions to the field of multilingual code generation."
            },
            "weaknesses": {
                "value": "1. Although this work makes a significant contribution to benchmarking the multilingual code generation capabilities of LLMs, its main contribution is labor-intensive, with limited technical contributions or insights (MCEVAL-INSTRUCT looks similar to MagiCoder by generating instruction data from code snippets).\n2. Is it meaningful to benchmark code LLMs on 40 languages? Can the author elaborate on why it is important to measure the generation capabilities of code LLMs in 40 languages simultaneously?\n3. Based on the experimental results, MCEVAL and MultiPL-E have similar measurement capabilities, which diminishes the significance of MCEVAL compared to MultiPL-E.\n4. It is said that \"three volunteers are employed to evaluate the correctness of the benchmark (> 90% accuracy) and correct the errors\" but no details are provided. Can the author explain how to ensure that the entire benchmark has a 90% accuracy rate? How was the \"90%\" quantified (we need ground truth to calculate the 90% right)?"
            },
            "questions": {
                "value": "1. Table 2 shows that MCEVAL has 16,031 questions, but in reality, there are only 2,007 completely independent programming problems. I think that comparing the benchmarks based on independent programming problems would better illustrate the quality of each benchmark, as a benchmark can increase its number of questions by \"creating new tasks\", but it cannot do the same to generate new independent programming problems.\n2. The fonts in the leaderboard tables are too small. \n3. The programming tasks differ between languages, making it hard to directly compare a model's capabilities across different languages."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents MCEval,  a massively multilingual code benchmark covering 40 programming languages with 16K test samples. MCEval provides a new evaluation benchmark in multilingual scenarios. In addition, the authors also present a new instruction dataset MCEVAL-INSTRUCT for multilingual code LLMs. The experiments results suggest that the benchmark MCEval could measure the gap between open-source and closed-source models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper is easy to follow. The figures presented are clear and easy to understand.\n2. The benchmark MCEval is comprehensive, covering 40 programming languages with 16K test samples."
            },
            "weaknesses": {
                "value": "1. The contribution of MCoder is unclear.\nThe contribution of this paper is comprise of two parts, a multilingual evaluation benchmark MCEval and a multilingual code LLM MCoder (trained on an instruction corpora MCEVAL-INSTRUCT). However, the performance of MCoder (using gpt-4 to generate data) are still behind the previous sota code LLMs like magicoder who uses gpt3.5 to generate instruction data.  The authors claim that MCoder is used as a strong baseline for MCEVAL(line 073), but why others don't use a better model (Magicoder) but MCoder as their baseline?\n\n2. The method of constructing dataset is lack of novelty.\nThe biggest contribution of this paper seems the number of programming languages included in their dataset. However, the method of constructing such their dataset is using LLM to generate and refine from the code snippets, which has been used in many previous works such as bigcodebench(https://arxiv.org/pdf/2406.15877)."
            },
            "questions": {
                "value": "1. What impact will the Unbalance on Different Languages (in section 5) bring to the evaluation process of code LLMs\uff1f\n2. What kinds of languages will help code LLMs to generate better? What will do harm?  These problems maybe more insightful for multilingual code evaluation of coding LLMs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}