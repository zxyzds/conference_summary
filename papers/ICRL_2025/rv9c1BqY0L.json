{
    "id": "rv9c1BqY0L",
    "title": "SimUSER: When Language Models Pretend to Be Believable Users in Recommender Systems",
    "abstract": "Recommender systems play a central role in numerous real-life applications, yet evaluating their performance remains a significant challenge due to the gap between offline metrics and online behaviors. We introduce SimUSER, an agent framework that serves as believable and cost-effective human proxies for the evaluation of recommender systems. Leveraging the inductive bias of foundation models, SimUSER emulates synthetic users by first identifying self-consistent personas from historical data, enriching user profiles with unique backgrounds and personalities. Then, central to this evaluation are users equipped with persona, memory, perception, and brain modules, engaging in interactions with the recommender system. Specifically, the memory module consists of an episodic memory to log interactions and preferences, and a knowledge-graph memory that captures relationships between users and items. The perception module enables visual-driven reasoning, while the brain module translates retrieved information into actionable plans. We demonstrate through ablation studies that the components of our agent architecture contribute to the believability of user behavior. Across a set of recommendation domains, SimUSER exhibits closer alignment with genuine humans than prior state-of-the-art, both at micro and macro levels. Additionally, we conduct insightful experiments to explore the effects of thumbnails on click rates, the exposure effect, and the impact of reviews on user engagement. The source code is released at https://github.com/SimUSER-paper/SimUSER.",
    "keywords": [
        "Recommender systems",
        "Recommender systems evaluation",
        "agent-based language models"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "SimUSER is an agent framework that simulates human-like behavior to evaluate recommender systems, using self-consistent personas and memory modules for more realistic assessments.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=rv9c1BqY0L",
    "pdf_link": "https://openreview.net/pdf?id=rv9c1BqY0L",
    "comments": [
        {
            "comment": {
                "value": "### Reviewer Comment\nIn the main experiment (Table 1), as the ratio of positive to negative samples decreases, [...]\n\n### Response\nThank you for noting this. We agree that the observed trend might seem counterintuitive at first glance. However, we\u2019ve found that similar dynamics\u2014where precision increases while recall drops as the ratio of positive to negative samples decreases\u2014have been observed in prior work, including in Agent4Rec. We will still review our experimental setup to confirm these results, but this alignment with previous findings suggests that the trend may be inherent to the way the LLM agents handles sparse positive samples.\n\n---\n\n### Reviewer Comment\nThere are several points in the presentation of the paper that are somewhat confusing: (1) There is an incorrect citation on line 57. (2) The \"Related Work\" section could benefit from reorganization. The discussion on \"simulating users in recommendation\" should include content related to large models, rather than being separated from the section on \"LLMs in Recommender Systems.\" (3) In Equation (5), given that Px->y denotes a path from x to y, the subscript Px->x is unclear and may cause confusion. (4) In Table 3, the third column of evaluation metrics contains some incorrectly bolded and underlined values. (5) In Table 1, some data points are incorrectly bolded.\n\n### Response\nThank you for these detailed observations; they will help us improve the paper's clarity and precision. We will make sure to correct the citation error on line 57 and reorganize the \u201cRelated Work\u201d section to integrate discussions on large models directly into the section on user simulation in recommendation. This should make the flow of related work more cohesive. We will also clarify the notation in Equation (5) so that the meaning of Px\u2192x is straightforward and does not cause confusion for readers. Additionally, we will fix any incorrect formatting in Tables 1 and 3, where some values are mistakenly bolded or underlined. These adjustments should enhance the overall readability and rigor of the paper.\n\n---\n\n### Reviewer Comment\nOne potential weakness of this paper is the approach of summarizing user profiles, such as age and occupation, based on their interaction history. This method may risk introducing bias, labeling, and weakening the personalization.\n\n### Response\nThank you for this valuable feedback. We agree that summarizing user profiles, like age and occupation, based on interaction history could introduce potential biases and labeling effects. To address this, we conducted two experiments (F.10.1 and F.10.2) specifically aimed at measuring the reliability of the extracted features, helping us assess the accuracy and consistency of this profiling approach. We also discussed the performance with and without persona, demonstrating that even with noisy personas, agents achieved better performance than vanilla agents. \n\nWe will add further discussion on this issue in the paper to provide a more in-depth analysis of the potential biases and limitations."
            }
        },
        {
            "comment": {
                "value": "### Reviewer Comment\nThe abstract mentions \"leveraging the inductive bias of foundation models,\"[...]\n\n### Response\nThank you for highlighting this point. We agree that elaborating on \u201cinductive bias\u201d in the context of foundation models would add clarity to our paper. Here, inductive bias refers to the model\u2019s built-in assumptions about how a user with a particular persona might react to a recommendation, decide on their next action, or respond based on their profile. Additionally, it includes the LLM's understanding of knowledge domains\u2014such as movies, books, and other content types\u2014which the model leverages to simulate realistic user behaviors. This bias allows the model reason without the need for domain-specific finetuning, even in cases with limited user history, like cold-start scenarios or few-shot learning.\n\n---\n\n### Reviewer Comment\nTo my knowledge, the primary contributions of SimUser are the introduction of several improved modules [...]\n\n### Response\nThank you for this feedback. While we agree with the listed contributions, we should emphasize some additional contributions: 1) more actions in the simulation to enable the simulation of more complex behaviors (e.g., click on items to get details), 2) a self-ask strategy to retrieve from the episodic memory, 3) self-reflection to synthesize past interactions into insights, and 4) novel reasoning steps to include emotion and mood when deciding what items to watch.  We should also emphasize that the proposed memory module is one of our key contribution, by enabling dynamic growth based on new interactions and persona-based retrieval. \n\nWe agree that further experiments are needed to fully illustrate the contribution of each component. In the revised paper, we will expand the ablation study to examine the impact of these additional modules, as well as the effects of knowledge graph memory and visual cues on SimUSER's overall performance. \n\n---\n\n### Reviewer Comment\nThe paper lacks important details in certain areas. For example, while the authors emphasize the significance of visual information and present its integration as a key contribution of their method, they do not provide sufficient explanation on how the visual data is utilized or stored. A more detailed description of this process would improve the clarity and rigor of the paper.\n\n### Response\nThank you for pointing this out. We appreciate the opportunity to clarify our approach. We agree that a more detailed explanation of how visual information is utilized and stored would enhance the paper\u2019s rigor. In our revised explanation, we will clarify that visual information in SimUSER is stored as plain text alongside dataset samples and is integrated into the reasoning architecture via the \"Page format\" prompt. In this prompt, visual cues along with the page representation are concatenated together. Some of these details can be found in the Appendix (Experimental Settings)."
            }
        },
        {
            "comment": {
                "value": "### Reviewer Comment\nIt is noticed that the users have relatively long historical behaviors in the datasets. However, practical users usually do not have too many recorded behaviors (e.g., cold-start or few-shot users). The few-shot user scenarios should be noted and discussed/evaluated.\n\n### Author Response\nThank you for the valuable observation. We agree that the cold-start and few-shot user scenarios are important to consider, as they are common in practical applications. To address this, we will include additional experiments that evaluate the model\u2019s performance across varying interaction history lengths, including scenarios with limited user behavior data. This will allow us to assess the framework's adaptability to cold-start conditions and understand its robustness with few-shot users.\n\nIn addition, we conducted an additional analysis to examine how varying levels of user interaction history affect the simulation's performance. We have conducted a new experiment to measure performance across different maximum interaction history lengths (e.g., 5, 10, 20, and 50 interactions). \n\n| **History Length** | **MovieLens RMSE** | **MovieLens MAE** | **AmazonBook RMSE** | **AmazonBook MAE** | **Steam RMSE** | **Steam MAE** |\n|--------------------|--------------------|--------------------|----------------------|---------------------|----------------|---------------|\n| 5 Interactions     | 0.6532             | 0.5890            | 0.6673              | 0.5331             | 0.6964        | 0.6273        |\n| 10 Interactions    | 0.6065             | 0.5187            | 0.6359              | 0.5053             | 0.6697        | 0.5987        |\n| 20 Interactions    | 0.5597             | 0.4870            | 0.6091              | 0.4733             | 0.6385        | 0.5781        |\n| 50 Interactions    | **0.5375**         | **0.4702**        | **0.5942**          | **0.4587**         | **0.6181**    | **0.5704**    |\n\n*Table: Performance of SimUSER (sim \u00b7 persona) in rating prediction with varying interaction history lengths on MovieLens, AmazonBook, and Steam datasets. Best results for each dataset are in bold.*\n\n---\n\n### Reviewer Comment\nThe overall framework involves lots of components. Although the authors have conducted extensive evaluations in Appendix, it is still unclear which techniques are the dominating reason for such improvement. I suggest that the authors could give a brief analysis in the main content, focusing on the insight of which techniques are the most essential ones. For example, the \u201cpickness\u201d strategy in Section 3.2.1 is essential for rating tasks, and the user CF-like strategy in Section 3.2.3 is also beneficial and has already been verified in classical recommendation methods. If the main improvements largely derive from such \u201ctricky\u201d points that were ignored in previous baselines, or from additional information, the contribution of this work will be discounted.\n\n### Response\nThank you for your feedback. Here\u2019s a breakdown of the most important components that drive the human-likeness scores in our model, in order of impact:\n1. **Knowledge Graph Memory**\n2. **Self-Ask Strategy** \n3. **Visual Cues**\n4. **Reasoning Steps** \n5. **Self-Reflection** \n\nWe believe this ranking clarifies the main drivers of our model\u2019s human-like qualities and highlights the unique strengths of our approach. Note that results were obtained by running a linear regression to predict the human likeliness scores of 300 runs that used different settings. \n\n---\n\n### Reviewer Comment\nThe current simulation task is not challenging enough (9 randomly selected negative samples is not that hard in recommendation). It is suggested that the authors could evaluate on other recommendation datasets that contain real-world exposed but unclicked samples (i.e., explicit negative feedback, which is harder than random negative samples). Good simulation results on such settings are much more persuasive.\n\n### Response\nThank you for the suggestion; it\u2019s a great point. We agree that using datasets with real-world negative samples (like unclicked items) would make the simulation more challenging and realistic, offering a better test of our model. We will add evaluations on datasets where negative feedback is more explicit\u2014where unclicked items genuinely reflect what users aren\u2019t interested in."
            }
        },
        {
            "comment": {
                "value": "### Reviewer Comment\nThis work proposes an overall framework for user simulation with lots of techniques [...].\n\n### Response\nThank you for raising these important points. We appreciate your feedback and understand the need for clarity regarding data and training methods for each model. To address these concerns, we will include a table that specifies the types of data used in each model. As for \u201cAgent4Rec/RecAgent,\u201d these methods were initialized using the historical dataset following the original implementation provided on GitHub (i.e., without using images or visual cues). They did not utilize visual insights as the original papers did not leverage visual cues in the reasoning architecture.\n\n---\n\n### Reviewer Comment\nThe authors mainly compare with LLM-based simulators, while what are the results of other conventional simulators without LLMs? The authors could give a discussion on these possible models in Experiment. For example, it is not that challenging for conventional ID-based models to find the top-1 item among 10 randomly selected candidates.\n\n### Response\nThank you for the suggestion. As for the dataset choice, we selected widely used datasets to allow a more fair and easier comparison between our method and prior work. We also picked these datasets since they were used by our director competitors, which made the comparison between the methods more fair. \n\nWe also agree that it is valuable to consider comparisons with conventional simulators that don\u2019t use LLMs. While ID-based models can indeed be effective at ranking items from a small set of candidates, our approach aims to provide a more human-like interaction with the system. Unlike conventional simulators, our method doesn\u2019t just rank items\u2014it simulates real user behaviors, such as browsing through pages, choosing to leave the system, or engaging with recommendations over multiple interactions. This gives us a richer set of metrics that better reflect actual user behavior, beyond simply selecting a top item. We\u2019ll add a discussion on how these differences impact performance in the experiments section.\n\n\n\n---\n\n### Reviewer Comment\nFor the user persona, will there be more informative features that should be included in the persona? For example, the classical user favorite tag/category/word that often exist in conventional user profiles.\n\n### Author Response\nThank you for the suggestion; it\u2019s a great idea. Currently, we include a \u201ctaste\u201d feature in the user persona, which captures similar information about user preferences and represents their general interests. \nHere is an example of \"taste\" feature for one agent: \n\"\"\"\nHIGH RATINGS: You tend to give high ratings to movies that are action-packed, adventurous, comedic, musical, or family-friendly, reflecting a preference for entertaining and uplifting genres.\n\nLOW RATINGS: While you generally avoid giving low ratings, you have rated several horror movies below 2, suggesting a dislike for intense horror themes\n\"\"\"\n\nWe agree that adding existing tags from the dataset could enrich this persona even further, helping to create a more detailed and informative user profile."
            }
        },
        {
            "comment": {
                "value": "### Reviewer Comment\nSome concepts are wrongly used. For example, the knowledge graph in this paper is actually the widely used user-item interaction graph [2] in collaborative filtering, which is significantly different from KG [3]. Please carefully check this concept and refine the writing.\n\n### Response\nThank you for pointing this out. We will include a new paragraph in the related work to discuss prior knowledge graph memories and their differences from our model. We agree that it will help to clarify the contribution of our method. Contrary to prior work that built a user-interaction graph, we propose a memory module that relies on a graph representing the user's past interactions to retrieve relevant information, such as similar movies. As a memory module, the memory is updated with the newest interactions to refine the agent\u2019s taste over time. By refining its KG memory over time, SimUSER can enhance the available context when facing a novel item. We also proposed a retrieval scheme, which leverages the graph structure and user personas to retrieve relevant past interactions.  \n\nWe will improve the description of this method to ensure that concepts are correctly named based on prior work.\n\n---\n\n### Reviewer Comment\nThe link to the code is expired.\n\n### Response\nWe are sorry for the issue; the repository was set as private. It should now be accessible."
            }
        },
        {
            "comment": {
                "value": "### Reviewer Comment\nThe contributions of this paper are marginal and incremental. The framework of SimUSER is similar to Agent4Rec [1] and the novelty lies in the incorporation of visual information and memory design, which is incremental.\n\n### Response\nThank you for this useful feedback. While we agree that our work builds on prior research like Agent4Rec, we have introduced several innovations aimed at enhancing the realism of simulated users. Specifically, we propose a novel technique for extracting detailed profile features, such as personality, which provides a richer user representation. The proposed method is based on consistency check, which we think is novel in our field. Additionally, our memory module, structured as knowledge graph, enables nuanced retrieval of past interactions, based on the history of interaction, \"external factors\" and the user's persona.\n\nWe also include more actions in the simulation to enable the simulation of more complex behaviors (e.g., clicking on items to get details) and a self-ask strategy to retrieve from the episodic memory. The inclusion of visual features further enriches the contextualization of user preferences, and our redesigned reasoning architecture brings a distinct approach to decision-making, setting SimUSER apart in its capability to simulate complex, realistic user behaviors. Finally, we believe that some studies, such as measuring bias of simulated users over time or user behavior under hallucination, are important for the advancement of the field.\n\n---\n\n### Reviewer Comment\nThe role of visual information is unknown. [...]\n\n### Response\nThank you for pointing this out. Regarding the knowledge-graph memory, Table 7 demonstrates that incorporating this module significantly enhances the agent's ability to predict item ratings more accurately by leveraging context and historical interactions. We acknowledge the concerns about the perception module\u2019s impact, as shown in Table 10, where the results with and without perception appear similar on the AmazonBook dataset. On the other hand, in non-book datasets, using the perception improves the human-likeness score. For instance, on MovieLens, the score goes from 4.12 to 4.22. \n\nTo improve the reliability of the study, we have carried out an additional study with 3,000 users:\n\n|                     | **MovieLens**         | **AmazonBook**       | **Steam**            |\n|---------------------|-----------------------|-----------------------|-----------------------|\n| SimUSER (\ud83d\udd38)        | _4.11 \u00b1 0.10_         | _3.97 \u00b1 0.12_        | _3.90 \u00b1 0.14_        |\n| SimUSER (\ud83d\udd39)        | **4.25 \u00b1 0.12***      | **4.02 \u00b1 0.13***     | **3.99 \u00b1 0.16***     |\n\n*Table 1: Human-likeness score evaluated by GPT-4o for SimUSER without (\ud83d\udd38) and with (\ud83d\udd39) perception module. Asterisks (*) denote statistically significant improvements when the perception module is activated.*\n\nWe hope this additional study helps to understand the contribution of these two key components."
            }
        },
        {
            "comment": {
                "value": "### Reviewer Comment\nClarity of Figures 1 and 3: Figures 1 and 3 are not vectorized, resulting in low clarity and readability.\n\n### Response\nThank you for pointing this out. We will update Figures 1 and 3 to fix this issue.\n\n---\n\n### Reviewer Comment\nCode Availability: Code is not available during the reviewing phase. Although the authors provided a link, it currently points to nothing.\n\n### Response\nThank you for highlighting this. We apologize for the oversight. We will ensure that the code link is updated and active before the final review stage to facilitate reproducibility and transparency."
            }
        },
        {
            "comment": {
                "value": "### Reviewer Comment\nLack of Human-in-the-Loop Simulation: [...].\n\n### Response\nThank you for this valuable feedback. Our current method is designed with the goal of enabling a synthetic human-in-the-loop process, where simulated users interact with the system in a way that generates new data to inform and improve recommendations. This allows us to mimic the dynamic feedback loop that real users provide, so the system can adapt and evolve over time based on these synthetic interactions.\n\nWe would appreciate any suggestions you might have for additional experiments that could further test this aspect. While our current experiments aim to evaluate the effects of synthetic feedback loops, we welcome ideas on how we might expand our evaluation. Based on review of similar papers, our evaluation follows similar protocols with those papers. Thank you!\n\n---\n\n### Reviewer Comment\nInadequate Ablation Study: [...]\n\n### Response\nThank you for your feedback. We appreciate the emphasis on a more comprehensive ablation study to clarify the contributions of each component in our framework. In the current paper, we provide evaluations with and without the persona, with and without the knowledge graph memory, and with and without the perception module, testing the effect of these components on the overall performance to assess their individual impact.\n\nHere\u2019s a breakdown of the most important components that drive human-likeness scores in our model, in order of impact:\n1. **Knowledge Graph Memory**\n2. **Self-Ask Strategy**\n3. **Visual Cues** \n4. **Reasoning Steps** \n5. **Self-Reflection** \n\nWe will add the results as an additional ablation study. The ranking was obtaining by fitting a linear regression on 300 that used different settings.\n\nWe also recognize that additional studies focused on the \u201cBrain\u201d module would help further clarify its role and impact on agent performance. We\u2019d be happy to include these in a revised version to provide even more detailed insights into each component\u2019s contribution. \n\n---\n\n### Reviewer Comment\nDependence on Abundant User Interactions: [...]\n\n### Response\nThank you for this insightful point. We agree that the amount of historical interaction data can impact the effectiveness of persona matching and memory retrieval. To address this, we conducted an additional analysis to examine how varying levels of user interaction history affect the simulation's performance. We have conducted a new experiment to measure performance across different maximum interaction history lengths (e.g., 5, 10, 20, and 50 interactions). \n\n| **History Length** | **MovieLens RMSE** | **MovieLens MAE** | **AmazonBook RMSE** | **AmazonBook MAE** | **Steam RMSE** | **Steam MAE** |\n|--------------------|--------------------|--------------------|----------------------|---------------------|----------------|---------------|\n| 5 Interactions     | 0.6532             | 0.5890            | 0.6673              | 0.5331             | 0.6964        | 0.6273        |\n| 10 Interactions    | 0.6065             | 0.5187            | 0.6359              | 0.5053             | 0.6697        | 0.5987        |\n| 20 Interactions    | 0.5597             | 0.4870            | 0.6091              | 0.4733             | 0.6385        | 0.5781        |\n| 50 Interactions    | **0.5375**         | **0.4702**        | **0.5942**          | **0.4587**         | **0.6181**    | **0.5704**    |\n\n*Table: Performance of SimUSER (sim \u00b7 persona) in rating prediction with varying interaction history lengths on MovieLens, AmazonBook, and Steam datasets. Best results for each dataset are in bold.*"
            }
        },
        {
            "comment": {
                "value": "### Reviewer Comment\nThe main motivation is the discrepancy between offline evaluation metrics [...].\n\n### Response\nThank you for this valuable feedback. We recognize the importance of demonstrating how our agent framework can bridge the gap between offline metrics and real-world dynamics in recommender systems. While traditional offline evaluations often rely on static, historical data, our agent framework introduces dynamic user interactions, enabling a more in-depth assessment of a system\u2019s performance. Namely, SimUSER can simulate user actions, such as exploring pages, exiting the system, and clicking on items, which traditional metrics typically don\u2019t capture. Therefore, we believe that this can be employed along with traditional offline metrics, especially to measure business metrics that do not necessarily correlated with offline evaluation [1].\n[1] Measuring the Business Value of Recommender Systems, JANNACH et al., 2019 \n\nAs for the \u201cbelievability\u201d of our agent framework, we have conducted several experiments that evaluate the human-likeness score of various methods, including ours. These scores reflect how closely the agent\u2019s behavior aligns with human interaction patterns, supporting our claim that SimUSER can serve as a believable proxy for user evaluation. We believe that most of recent papers in the field employed similar evaluation techniques. Additionally, we report results on rating prediction error, demonstrating the framework\u2019s accuracy in modeling user preferences.\n\nRegarding \u201ccost-effectiveness,\u201d we provide an analysis of the simulation cost for running 1,000 agents, showing that our approach offers a scalable solution for large-scale evaluations. We believe that the reported cost is much lower than collecting human interactions. \n\nWe also agree that it is necessary to show whether \u201cthis agent framework effectively improves recommender system evaluation or provides a clear advantage over traditional offline metrics\u201d. Hence, we will include such an experiment in the revised version of the manuscript. This experiment evaluates the recommender system\u2019s performance before and after incorporating feedback from SimUSER. This experiment reports a comparison between the best recommender system according to offline evaluation methods and SimUSER. It reports various metrics like engagement rate and hit rate that humans achieved when interacting with this recommender system.\n\n---\n\n### Reviewer Comment\nThe experiments in this paper are also conducted in an offline setting, [...].\n\n### Response\nThank you for raising this point. While we do use an offline dataset to initialize the agents, our approach goes beyond a traditional offline evaluation. After initialization, we run the simulation where agents interact with the system, allowing us to collect additional metrics that a pure offline evaluation would not capture. For example, we are able to measure the number of pages each agent visits, when they decide to exit, and other behavioral insights. In addition, we do not seek to replace offline evaluation, but instead bridge the gap between static data analysis and interactive user behavior, allowing us to assess not only traditional metrics but also engagement and decision patterns that reflect real-world use cases. By incorporating elements like page navigation, exit decision, and user experience, we provide a richer perspective on model performance that complements conventional offline metrics. \n\nIn addition, Table 3 shows that using augmenting existing dataset with grounded interactions, enables our approach to reduce the rating error. These grounded interactions were not included in the offline datasets. We agree with the reviewer that clarifying this point is important to demonstrate the contribution of our paper.\n\n---\n\n### Reviewer Comment\nUnrealistic Evaluation Setting: The authors split the dataset in an 8/1/1 ratio, [...].\n\n### Response\nThank you for pointing this out, and we apologize for not making this clearer in the paper. We did take timestamps into account when splitting the dataset to ensure that temporal shifts were represented in the training, validation, and test sets. We will make sure to clarify this in the revised paper, as we understand that accounting for temporal dynamics is essential for a realistic evaluation. We appreciate the chance to address this and improve the clarity of our methodology."
            }
        },
        {
            "comment": {
                "value": "We would like to extend our sincere thanks to you and each reviewer for the time, effort, and invaluable feedback provided. The constructive comments have given us a clearer perspective on areas for improvement. Most of the identified weaknesses can be addressed by conducting additional experiments and ablation studies, as well as by enhancing the related work section to clarify our key contributions. We hope that these revisions will effectively respond to your insights and further emphasize the robustness and novelty of our approach."
            }
        },
        {
            "summary": {
                "value": "The paper proposes a better agent framework for simulating users in recommender systems. The agent is framed using multiple modules like brains, and memories, and extracts persona from the user's historical interactions. Experiments are conducted on three public dataset to demonstrate the effectiveness of the proposed framework."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. Timely study on areas including user simulation through generative agents, the gap between offline evaluations and real-world user experiences in recommender systems, and the use of simulators for recommender system evaluation.\n2. The paper is well-structured, making it easy to follow and understand.\n3. Experiments are conducted across three public datasets.\n4. The authors perform significance tests to demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. Limited Insights: Using LLMs or generative agents to simulate users in recommender systems is not new. This paper\u2019s primary contribution appears to be an improved agent that better leverages additional signals (e.g., historical interactions, images, KG). While the experimental results may indicate a more effective and believable agent, the paper offers limited new insights to the research community.\n2. Overclaiming and Misalignment Between Motivation and Conclusions:\n    1. The main motivation is the discrepancy between offline evaluation metrics and real-world dynamics in recommender systems. The authors claim that they introduce an agent framework that can act as a \"believable and cost-effective human proxy\" for recommender system evaluation. However, no experiments demonstrate that this agent framework effectively improves recommender system evaluation or provides a clear advantage over traditional offline metrics.\n    2. The experiments in this paper are also conducted in an offline setting, where datasets are split into training, validation, and test sets to evaluate models. This is ironic for a paper that critiques offline evaluations but only tests its own method in offline conditions.\n    3. Unrealistic Evaluation Setting: The authors split the dataset in an 8/1/1 ratio, seemingly without considering timestamps. Randomly splitting interactions ignores the distribution shift across different time periods, which is a major challenge in offline evaluations for recommender systems. As a result, the evaluation here does not adequately reflect temporal distribution shifts.\n    4. Lack of Human-in-the-Loop Simulation: A significant issue with offline evaluations in recommender systems is the absence of a human-in-the-loop process. Recommender systems typically evolve as users interact with them, generating new data for system improvements. While simulators could potentially model this process, the experiments in this paper fail to reflect such dynamics.\n3. Inadequate Ablation Study: The proposed agent framework integrates numerous heuristic components (e.g., Brain, Memory) and multiple signals (e.g., KG, images). However, the ablation study is insufficiently detailed, only comparing variants with and without persona and \"zero or sim\". This leaves the audience unclear on whether such a complex framework is necessary or if so many modules are essential for creating believable agents.\n4. Dependence on Abundant User Interactions: Constructing the agent requires users to have a substantial history of interactions for persona generation and memory functions. This dependency may affect simulation performance based on the amount of historical interaction data. It would be better for the authors to discuss and analyze how varying levels of user interactions impact the simulation\u2019s performance.\n5. Clarity of Figures 1 and 3: Figures 1 and 3 are not vectorized, resulting in low clarity and readability.\n6. Code Availability: Code is not available during the reviewing phase. Although the authors provided a link, it currently points to nothing."
            },
            "questions": {
                "value": "Please refer to \"Weaknesses\" for details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes SimUSER, a framework that simulates the real user behaviors in recommender systems. SimUSER incorporates two new and crucial components: visual information and knowledge graph memory. The whole framework of SimUSER is similar to Agent4Rec [1] and the contribution is marginal. Moreover, the effectiveness of visual information is not verified and the concept of knowledge graph is wrongly used.\n\n[1] Zhang, An, et al. \"On generative agents in recommendation.\"\u00a0*Proceedings of the 47th international ACM SIGIR conference on research and development in Information Retrieval*. 2024."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The experiments in this paper are abundant. Experiments in this paper include: user preference alignment, rating prediction, rating distribution, LLM evaluation, and recommendation strategy evaluation, etc. These experiments showcase the effectiveness of the proposed method.\n2. This paper is easy to understand."
            },
            "weaknesses": {
                "value": "1. The contributions of this paper are marginal and incremental. The framework of SimUSER is similar to Agent4Rec [1] and the novelty lies in the incorporation of visual information and memory design, which is incremental.\n2. The role of visual information is unknown. While the main contribution of this paper comes from the knowledge-graph memory and visual-driven reasoning, the ablation study does not explicitly verify the effectiveness. In Table 10, w/o perception module and w perception exhibit similar performance, and the implementation detail of w/o perception is unknown. More importantly, given the sample number of 1000 users, the improvements are impossible to be significant (i.e., p < 0.05). These experiment results are questionable.\n3. Some concepts are wrongly used. For example, the knowledge graph in this paper is actually the widely used user-item interaction graph [2] in collaborative filtering, which is significantly different from KG [3]. Please carefully check this concept and refine the writing.\n4. The link to the code is expired.\n\n[1] Zhang, An, et al. \"On generative agents in recommendation.\"\u00a0*Proceedings of the 47th international ACM SIGIR conference on research and development in Information Retrieval*. 2024.\n\n[2] He, Xiangnan, et al. \"Lightgcn: Simplifying and powering graph convolution network for recommendation.\"\u00a0*Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval*. 2020.\n\n[3] Wang, Xiang, et al. \"Kgat: Knowledge graph attention network for recommendation.\"\u00a0*Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining*. 2019."
            },
            "questions": {
                "value": "Refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work concentrates on a very interesting and essential task: user simulation in recommendation. Compared with recent LLM-based recommendation user simulators such as Agent4Rec/RecAgent, the proposed SimUSER brings in the self-consistent personas from historical data, and designs both an episodic memory and a knowledge-graph memory for the memory module. Furthermore, they also introduce the visual information to the framework as multimodal recommenders for more comprehensive understandings. In experiments, the authors conduct extensive evaluations on different simulation tasks as well as analyses on various components in the framework, where the proposed model achieves the best performance compared to other LLM-based agent simulators."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1)\tThis work focuses on a challenging task and the proposed framework is sound and clear.\n2)\tThe authors have conducted extensive experiments in Appendix to verify the effectiveness of different designs.\n3)\tThe detailed prompts and other information are given for reproducibility."
            },
            "weaknesses": {
                "value": "1)\tThis work proposes an overall framework for user simulation with lots of technics (which may require different types of related data). We can find that the proposed framework achieves consistently better results compared to Agent4Rec/RecAgent. However, it is unclear that whether the comparisons are fair enough. For example, how are the baselines trained? Whether the proposed framework uses additional information (e.g., at least the visual information)? The authors are suggested to give a table containing the detailed data used in each model. We know the data are essential for good performance in LLM-based methods.\n2)\tThe authors mainly compare with LLM-based simulators, while what are the results of other conventional simulators without LLMs? The authors could give a discussion on these possible models in Experiment. For example, it is not that challenging for conventional ID-based models to find top1 item among 10 randomly selected candidates.\n3)\tFor the user persona, will there be more informative features that should be included in the persona? For example, the classical user favorite tag/category/word that often exist in conventional user profiles.\n4)\tIt is noticed that the users have relatively long historical behaviors in the datasets. However, practical users usually do not have too many recorded behaviors (e.g., cold-start or few-shot users). The few-shot user scenarios should be noted and discussed/evaluated.\n5)\tThe overall framework involves lots of components. Although the authors have conducted extensive evaluations in Appendix, it is still unclear that which techniques are the dominating reason for such improvement. I suggest that the authors could give a brief analysis in the main content, focusing on the insight of which techniques are the most essential ones. For example, the \u201cpickness\u201d strategy in Section 3.2.1 is essential for rating tasks, and the user CF like strategy in Section 3.2.3 is also beneficial and have already been verified in classical recommendation methods. If the main improvements largely derive from such \u201ctricky\u201d points that ignored in previous baselines, or from additional information, the contribution of this work will be discounted.\n6)\tThe current simulation task is not challenging enough (9 randomly selected negative samples is not that hard in recommendation). It is suggested that the authors could evaluate on other recommendation datasets that contains real-world exposed but unclicked samples (i.e., explicit negative feedback, which is harder than random negative samples). Good simulation results on such settings are much more persuasive."
            },
            "questions": {
                "value": "Refer to Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper designs SimUser, a user simulation environment with LLM as its core to bridge the gap between offline evaluation metrics and online testing of recommender systems. SimUser conducts some experiments on item selection, rating prediction an LLM evaluation to validate the effectiveness of their framework."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The motivation of this work is good.\n2. The method is easy to understand."
            },
            "weaknesses": {
                "value": "1. The abstract mentions \"leveraging the inductive bias of foundation models,\" which is an intriguing idea. However, the paper does not provide enough detail on this in the main body. It would be helpful to explain what inductive bias refers to in the context of foundation models and how it can benefit recommendation systems. Offering more clarity on this point would enhance the overall strength of the paper.\n\n2. To my knowledge, the primary contributions of SimUser are the introduction of several improved modules built upon existing frameworks: (1) a more refined user profiling mechanism, (2) a richer memory mechanism, and (3) enabling visual input. These modules are extensively discussed in the main body of the paper. However, the ablation study in the experimental section is overly simplified, and there are no corresponding experiments to verify the effectiveness of these newly introduced modules. This raises doubts about the validity of the contributions and leaves it unclear where the superior performance compared to the baseline originates from. Additionally, the ablation study is only conducted on the rating alignment experiment, with no exploration or validation of these components' effects on user preference alignment.\n\n3. The paper lacks important details in certain areas. For example, while the authors emphasize the significance of visual information and present its integration as a key contribution of their method, they do not provide sufficient explanation on how the visual data is utilized or stored. A more detailed description of this process would improve the clarity and rigor of the paper.\n\n4. In the main experiment (Table 1), as the ratio of positive to negative samples decreases, Precision unexpectedly increases across all three datasets, while Recall drops significantly. This outcome seems counterintuitive, and I would recommend the authors carefully verify the reliability of the experiments.\n\n5. There are several points in the presentation of the paper that are somewhat confusing:\n(1) There is an incorrect citation on line 57.\n(2) The \"Related Work\" section could benefit from reorganization. The discussion on \"simulating users in recommendation\" should include content related to large models, rather than being separated from the section on \"LLMs in Recommender Systems.\"\n(3) In Equation (5), given that Px->y denotes a path from x to y, the subscript Px->x is unclear and may cause confusion.\n(4) In Table 3, the third column of evaluation metrics contains some incorrectly bolded and underlined values.\n(5) In Table 1, some data points are incorrectly bolded.\n\n6. One potential weakness of this paper is the approach of summarizing user profiles, such as age and occupation, based on their interaction history. This method may risk introducing bias, labeling, and weakening the personalization.\n\n7. In conclusion, I do not see a significant difference between SimUser and existing user simulation methods utilizing LLM on the recommender system. And the experiment section of this paper is incremental and far from solid."
            },
            "questions": {
                "value": "Please refer to Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "One potential weakness of this paper is the approach of summarizing user profiles, such as age and occupation, based on their interaction history. This method may risk introducing bias, labeling, and weakening the personalization."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}