{
    "id": "wGVOxplEbf",
    "title": "SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation",
    "abstract": "The development of diffusion models has led to significant progress in image and video generation tasks, with pre-trained models like the Stable Diffusion series playing a crucial role.\nHowever, a key challenge remains in downstream task applications: how to effectively and efficiently adapt pre-trained diffusion models to new tasks.\nInspired by model pruning which lightens large pre-trained models by removing unimportant parameters, we propose a novel model fine-tuning method to make full use of these ineffective parameters and enable the pre-trained model with new task-specified capabilities.\nIn this work, we first investigate the importance of parameters in pre-trained diffusion models and discover that parameters with the smallest absolute values do not contribute to the generation process due to training instabilities.\nBased on this observation, we propose a fine-tuning method termed SaRA that re-utilizes these temporarily ineffective parameters, equating to optimizing a sparse weight matrix to learn the task-specific knowledge.\nTo mitigate potential overfitting, we propose a nuclear-norm-based low-rank sparse training scheme for efficient fine-tuning.\nFurthermore, we design a new progressive parameter adjustment strategy to make full use of the finetuned parameters.\nFinally, we propose a novel unstructural backpropagation strategy, which significantly reduces memory costs during fine-tuning.\nOur method enhances the generative capabilities of pre-trained models in downstream applications and outperforms existing fine-tuning methods in maintaining model's generalization ability.",
    "keywords": [
        "Diffusion Model",
        "Fine-tuning"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wGVOxplEbf",
    "pdf_link": "https://openreview.net/pdf?id=wGVOxplEbf",
    "comments": [
        {
            "title": {
                "value": "Related Work: Sparse High Rank Adapters (Neurips 2024)"
            },
            "comment": {
                "value": "Dear Authors,\n\nThanks for the interesting work SARA. It is important for the research community to focus on sparse finetuning techniques that are as memory-efficient as LoRA so we really appreciate your contributions. We recently published a highly related work at Neurips 2024 \"Sparse High Rank Adapters\" (SHiRA) which also presents memory benefits using a PEFT implementation compared to LoRA and analyzes multi-adapter fusion properties for sparse finetuning (see https://openreview.net/forum?id=6hY60tkiEK, older preprint: https://arxiv.org/abs/2406.13175). Can you please discuss our _concurrent_ work in your related work section? \n\nAnother important related work is SpIEL. https://arxiv.org/abs/2401.16405.\n\nIt would be nice to see qualitative differences between these recent works (SARA, SHiRA, and SpIEL).\n\nAll the best!"
            }
        },
        {
            "summary": {
                "value": "The paper presents a novel approach for fine-tuning pre-trained diffusion models called SaRA for visual content generation. The method builds on a key insight: parameters with the smallest absolute values in diffusion models contribute minimally to generation due to training instabilities, allowing for their selective reuse. SaRA enhances these low-impact parameters by applying a sparse weight matrix that learns task-specific knowledge while retaining the model\u2019s generalization abilities. To avoid overfitting, the authors introduce a nuclear-norm-based low-rank training scheme. Additionally, SaRA includes a progressive parameter adjustment strategy and an unstructured backpropagation approach to efficiently manage memory use during fine-tuning.\n\n\nNote: the supplementary materials contain the author's username and IP address (Supplementary Material/code/.idea/deployment.xml)"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The method visualizations (Figures 1 and 4) provide a step-by-step comparison of the proposed approach with previous techniques that helps the reader to easily understand the nuances of SaRA.\n2. The authors show that the method could be applied to different diffusion models denoisers architectures U-Net (SD1.5 and 2.0) and Diffusion Transformer (SD3.0).\n3. The design of the method allows a plug-and-play experience for the users that is highly beneficial for practical adoption.\n4. The authors demonstrate the capabilities of the proposed approach on various widely-used by the open source community datasets."
            },
            "weaknesses": {
                "value": "1. The authors introduce a novel Visual-Linguistic Harmony Index (VLHI) metric; however, it's described only in the appendix.\n2. No comparison with the recent SOTA PEFT techniques (e.g., DoRA [Liu et al. 2024] that is available for different models on mentioned in the paper CIVITAI).\n3. No ablation on scaling the trained SaRA weights for the inference (as the lora_scale parameter controls the influence of LoRA weights) or mentioning it in the limitations.\n4. The authors say that for the FID computation they sampled 5K images from the source and generated data; however, BarbieCore dataset has only 315 images which is definitely not enough for the proper FID evaluation. The details about the sizes of the used datasets should be in the paper.\n5.  CLIP L/14 used by authors is trained to provide overall image captions and could miss the details. The visual language models-based evaluations used in T2I-CompBench++[Huang et al. 2023] could be more accurate.\n6. The authors skip the most popular Stable Diffusion XL 1.0 version, whereas they include 2.0.\n7. Typographical mistakes such as:\n*) Table 1: wrong column 2&3 names\n*) Figure 1: addictive-> additive"
            },
            "questions": {
                "value": "see the weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "To leverage the unimportant parameters concept in model pruning, this paper proposes a model fine-tuning method for diffusion model by reusing these ineffective parameters. The authors find that such ineffective parameters with small absolute values are random and dynamically change over finetuning. Based on this observation, they further design some efficient strategies for tuning these parameters."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper verifies that the ineffective parameter concept also applies for diffusion model, i.e., the parameters with small absolute values are not important for the generation. \n2. This paper adopts a series of strategies for the efficient tuning of these ineffective parameters.\n3. The effectiveness of this method is verified on the stable diffusion series. It demonstrates better performance over the baselines."
            },
            "weaknesses": {
                "value": "1. The main limitation is that this paper poorly extends the concept in static model to the finetuning area, which is a dynamic model. Specifically, the ineffective concept works in model pruning, which is a given fixed model. The ineffective parameters in such static model can be discarded or reused. However, in this paper, the model parameters dynamically change during finetuning, and unimportant parameters also change. Thus, a right way to extend such unimportant parameters is to study their dynamics over change, instead of simple reuse.  Simple reuse may have several issues, for example, smaller optimal parameter search space in the finetuning case.\n2. Another concern may lie in how to merge multi tasks\u2019 parameters, and their merging performance. The multi-task parameter merging is a good property of LORA. It is encouraged to explain and verify this."
            },
            "questions": {
                "value": "How to understand the better performance after setting some parameters to 0 in Section 3.1? This is actually interesting and may be useful for understanding the behavior of diffusion model."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a new method for fine-tuning diffusion models by training only low-value parameters making them effective on a new task. Additionally, a nuclear norm is used to prevent overfitting, and efficient selective backpropagation and progressive parameter adjustment reduce the memory and time requirements during training. The results show SaRA is on par or better than other fine-tuning methods in terms of FID, CLIP score, and qualitative assessment."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. The paper is very well written and easy to follow. It has a good flow of information. Every claim is carefully explained and proven by experiments or analysis.\n2. The novelty of the method is good. The idea of fine-tuning only ineffective parameters was explored before but combined with nuclear norm regularization, novel approach to backpropagation of a sparse matrices, and adaptive fine-tuning, creates a valuable addition to the field.\n3. The experiments are extensive, both comparisons and ablation study.\n4. Qualitative results suggest an improvement over other methods.\n5. Quantitative results show the model behaves better or similar to other methods. I can see it becoming one of the methods of choice depending on one's needs."
            },
            "weaknesses": {
                "value": "1. Minor - as mentioned in Strengths 5., the results are not showing overall superiority over other methods."
            },
            "questions": {
                "value": "1. The authors should consider splitting Figures 2 b) and 5 into more subplots. In my opinion, they are too cluttered and it takes too much time to read them.\n2. In Table 1, the use of \"optimal\" and \"sub-optimal\" is not correct, e.g. optimal FID is 0. \"Best\" and \"second best\" or something similar should be used.\n3. Could the authors provide more qualitative results for the same prompts (Appendix C) with different seeds to see the diversity of the samples?\n4. It would be good to include some of the visual results in the main text.\n5. Can authors elaborate on how CLIP score is related to overfitting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a PEFT method for diffusion models, which progressively trains selectively chosen parameters with small, inefficient values. To prevent memory waste, it avoids storing the gradient of all parameters and instead stores only the chosen parameters in separate nodes, which are then replaced after training."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Unlike methods like LoRA that require additional parameters, this approach selects parameters within the existing model for fine-tuning, minimizing additional memory usage.\n- Existing selective PEFT methods continuously update masks, which requires storing gradients for all parameters, making them inefficient. In contrast, this paper\u2019s method progressively trains only the fixed parameters at each stage, storing only certain gradients, making it more memory-efficient.\n- The paper conducts extensive comparison experiments with various versions of Stable Diffusion (SD) and different sizes of fine-tuning parameters."
            },
            "weaknesses": {
                "value": "- Although the paper differentiates itself from selective fine-tuning, I think this method still appears to be a form of selective fine-tuning. The memory efficiency improvement seems to stem from implementing a separate node for gradient storage and selectively fine-tuning only those nodes, rather than from an inherent algorithmic difference.\n- Selecting parameters based on a specific threshold seems not a new concept. For example, the related work PaFi is described that it also trains based on absolute values.\n- The reasoning behind inefficient parameters becoming efficient during training due to the randomness of the training process is unclear. Since the initial weights are set randomly, many values are likely to change through training. There is no strong basis to assume a correlation with the initial values. Even parameters with initially small values are expected to converge toward the average distribution, making Figure 3 somewhat self-evident.\n- Although selecting a mask based on a threshold is computationally efficient, this method is relatively naive compared to other SFT methods that dynamically choose parameters during fine-tuning, which may lead to lower performance. Although it was compared with LT-SFT, there is a lack of comparison with other SFT methods.\n- In the table, it is unclear if this method offers a significant performance improvement. While the FID scores are mostly favorable, the CLIP scores appear to be higher for LoRA. Additionally, the L_rank removal in the ablation study does not lead to a significant performance change."
            },
            "questions": {
                "value": "- Would similar results be achievable if other SFT methods stored the selected parameters in leaf nodes and updated them as implemented here?\n- Could you provide more explanation regarding the impact of training process randomness on initially inefficient parameters becoming efficient?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents SaRA, a method for fine-tuning pre-trained diffusion models that introduces progressive Sparse low-Rank Adaptation (SaRA) to enhance efficiency and reduce memory costs in adapting diffusion models to new tasks. The proposed method leverages parameters with low absolute values, presumed to have limited initial impact on the model\u2019s performance, making them suitable for fine-tuning. SaRA combines sparse parameter updates with a nuclear norm-based low-rank constraint to mitigate overfitting. It also introduces a progressive parameter adjustment and unstructured backpropagation strategy, aimed at further memory efficiency. Extensive experiments demonstrate SaRA's superiority over traditional fine-tuning methods on image generation and customization tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper clearly explains the intuition in parameter efficiency by proposing the use of low absolute value parameters for adaptive updates, effectively avoiding overfitting through a nuclear norm constraint.  The progressive parameter adjustment strategy positively contributes to the stability and convergence of model performance, while the unstructured backpropagation method effectively reduces memory costs, making SaRA a practical solution in resource-constrained environments. Additionally, the extensive experiments cover various tasks, such as image generation and customization, thoroughly validating the advantages of the SaRA method in balancing the preservation of model priors and the learning of task-specific knowledge."
            },
            "weaknesses": {
                "value": "My main concern centers on the assumption underlying this approach\u2014that parameters with the smallest absolute values are inherently ineffective. This seems more empirical than rigorously substantiated. While small absolute values can indeed correlate with lower impact in certain contexts, particularly in pruning, their effectiveness actually depends on the model architecture and specific task. Small value parameters may exert less direct influence on the output, but they are not intrinsically ineffective; their impact can vary depending on training dynamics, model structure, and optimization objectives.\n\nSome minor weaknesses include:\n* The generalizability of the 'adapting small-value parameters' strategy across different architectures is crucial to ensure broader applicability. This paper only investigates the phenomenon of the pre-trained Stable diffusion models. In this sense, I'm worrying about whether it can be applied across different network architectures or frameworks.\n* In the caption of Figure 3, weight distributions are claimed to be Gaussian without further clarification, which seems empirical rather than solid.\n* The choice of threshold for selecting low-absolute-value parameters could be better justified. If this choice is sensitive, it could limit SaRA's robustness and generalizability across different diffusion models and tasks. An ablation study on this threshold choice would strengthen the claims.\n* While the experiments show SaRA's success, the paper could benefit from an analysis of its limitations. For instance, discussing cases where SaRA may not perform as well, such as tasks requiring extensive re-training of high-impact parameters, would improve the comprehensiveness of the evaluation."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}