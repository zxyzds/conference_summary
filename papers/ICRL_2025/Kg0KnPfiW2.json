{
    "id": "Kg0KnPfiW2",
    "title": "Automating Large-scale In-silico Benchmarking for Genomic Foundation Models",
    "abstract": "The advancements in artificial intelligence in recent years, such as Large Language Models (LLMs), have fueled expectations for breakthroughs in genomic foundation models (GFMs). The code of nature, hidden in diverse genomes since the very beginning of life\u2019s evolution, holds immense potential for impacting humans and ecosystems through genome modeling. Recent breakthroughs in GFMs, such as Evo, have attracted significant investment and attention to genomic modeling, as they address long-standing challenges and transform in-silico genomic studies into automated, reliable, and efficient paradigms. In the context of this flourishing era of consecutive technological revolutions in genomics, GFM studies face two major challenges: the lack of GFM benchmarking tools and the absence of open-source software for diverse genomics. These challenges hinder the rapid evolution of GFMs and their wide application in tasks such as understanding and synthesizing genomes, problems that have persisted for decades. To address these challenges, we introduce GFMBench, a framework dedicated to GFM-oriented benchmarking. GFMBench standardizes benchmark suites and automates benchmarking for a wide range of open-source GFMs. It integrates millions of genomic sequences across hundreds of genomic tasks from four large-scale benchmarks, democratizing GFMs for a wide range of in-silico genomic applications. Additionally, GFMBench is released as open-source software, offering user-friendly interfaces and diverse tutorials, applicable for AutoBench and complex tasks like RNA design and structure prediction. To facilitate further advancements in genome modeling, we have launched a public leaderboard showcasing the benchmark performance derived from AutoBench. GFMBench represents a step toward standardizing GFM benchmarking and democratizing GFM applications.",
    "keywords": [
        "genomic benchmark",
        "DNA",
        "RNA",
        "foundation model"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "Propose a framework for automated genomic foundation model benchmarking",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Kg0KnPfiW2",
    "pdf_link": "https://openreview.net/pdf?id=Kg0KnPfiW2",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a meta-benchmark for transfer learning that integrates tasks from four existing benchmark sets. They revise metrics used in these benchmarks to account for class imbalance. The authors implemented their evaluation protocol in a Python package they claim is easily extendable and flag this as their primary contribution.\n\nOrthogonally, the paper presents benchmarking results for several candidate foundation models for genomics. In addition to DNA language models, they include models trained on RNA. The authors highlight the strong performance of the RNA language model OmniGenome."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors address the problem of evaluating genomic foundation models, which is an important problem.\n2. They appear to have developed a comprehensive, well-documented framework for benchmarking.\n3. They convincingly demonstrated that current language models trained on DNA sequences do not learn representations of RNA sequences as effectively as those trained purely on RNA sequences.\n4. They provide strong evidence that supervising models on annotations of RNA structure provides these models with important information."
            },
            "weaknesses": {
                "value": "1. The paper would be stronger if the evaluations were more comprehensive. The authors could have included newer benchmarks\u2014like BEND (ICLR 2024) or those included in the recent preprint by Tang and Koo (https://doi.org/10.1101/2024.02.29.582810) \u2014that address important limitations of the previous benchmarks.\n2. It would also benefit from including additional language model baselines, like PlantCaduceus or GPN.\n3. It would also be worthwhile to evaluate models other than language models, since these have been shown to outperform language models on transfer learning tasks.\n4. I am not convinced of their claim that \u201cOmniGenome generalizes well to DNA-based tasks, likely due to shared sequence motifs and structural similarities between RNA and DNA.\u201d I do not think this explanation is true for noncoding elements. It is more likely that their benchmarking needs to be more thorough. From their comparisons, it is also unclear whether OmniGenome generalizes well\u2014only that it generalizes comparably to language models trained on DNA."
            },
            "questions": {
                "value": "Does OmniGenome utilize structural information in its input?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors present an integrated platform, GFMBench, for benchmarking genomic foundation models. The authors synthesized and standardized four existing benchmarks and created a unified software framework to run GFMs and calculate evaluation metrics in a streamlined manner. In general, I consider this work lacks novelty and does not contribute sufficiently to the field, which is my primary reason for recommending rejection. While there are engineering efforts to aggregate and standardize existing benchmarks, the contribution does not extend much beyond that.\n\nOne could argue that such work helps facilitate the benchmarking process, and I appreciate the effort the authors put into building the software. However, I do not think this type of contribution is suitable for a venue like ICLR. I acknowledge that others might hold a different opinion, so I am setting my confidence level to 3."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. There are interesting ideas in this work that could be expanded and delved into further. For example, the authors attempt to create a unified benchmark for DNA and RNA tasks, which raises the potentially interesting topic of studying the cross-modality transferability of GFMs. This aspect could be expanded and investigated more deeply to provide valuable insights.\n2. Having a unified platform like this can enhance reproducibility in GFM development and evaluation.\n3. The platform includes user-friendly features such as an API, an online hub, and a leaderboard."
            },
            "weaknesses": {
                "value": "1. As mentioned in the summary, this work generally lacks novelty.\n2. One of the four main contributions claimed by the authors, Data scarcity and bias, is vaguely stated and not particularly valid. First, the discussion seems to conflate training data and evaluation data. This work addresses only the evaluation side, so the writing should focus on that. Second, since the data sources are four well-established benchmark datasets, there is no additional contribution to resolving the data scarcity issue, as no new dataset is introduced.\n3. The authors did not discuss or justify the choice of the four benchmark datasets. There is no comprehensive review of existing benchmarks, which I think is conventional for such work. For example, the benchmarks from NT (https://www.biorxiv.org/content/10.1101/2023.01.11.523679v4), BEND (https://openreview.net/forum?id=uKB4cFNQFg), and Tang et al. (https://www.biorxiv.org/content/10.1101/2024.02.29.582810v2.full.pdf) are well-established and recognized benchmarks in the field but none were mentioned in the manuscript.\n4. The manuscript contains many claims that lack supporting evidence and concrete examples. For instance: \na. The authors mention metric variability but do not show empirical evidence. How would using the original metrics from each benchmark affect the results? \nb. The authors claim that this work helps test models on underrepresented sequences and tasks, but there is no further explanation or evidence on why or how this work addresses this issue.\n5. The writing could be improved for conciseness and clarity. And there are some overly repetitive statements throughout the article."
            },
            "questions": {
                "value": "There are two repeated paragraphs in lines 154-167, as well as some other typos in the manuscript."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces GFMBench, a framework for genomic foundation model (GFM)\nevaluation. The proposed benchmark covers a diverse set of genomic tasks and datasets, and\nprovides methods for standardised metrics and evaluation, thus \nenabling fair comparison of different GFMs. The papers include an evaluation of\nmultiple GFMs on the benchmark of standardised metrics, which allows the \ncomparison of the models. GFMBench is released as open-source software and\nit includes an online leaderboard."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Generally, the paper is well-written in good English and easy to follow.\nThe motivation for the paper is clear and the problem is well defined.\nOverall, the authors make a significant contribution to the field of genomics and\nfoundational genomics models. Providing a unifying benchmark for evaluation\nis a significant contribution and will very much contribute positively to the\ndevelopment of the progress of GFMs. The authors will release a webpage with a\nleaderboard for model comparison, which is really positive. The authors\nwill also release the code for GFMBench."
            },
            "weaknesses": {
                "value": "There are some issues regarding the clarity of the contribution of \nthe paper and the reported results. The paper also presents some claims\nthat are not backed up or briefly supported. The issues are listed below:\n\nMajor issues:\n\n- Data scarcity is mentioned to be an issue for genomics datasets in the paper.\nWhile it is true that diversity is a problem in the access to genomic data it is \nimportant to note that there are indeed many large-scale datasets available.\nThe main concern regarding this issue is that the authors claim that\nGFMBench tackles this problem, however, as far as I'm concerned, the benchmark\ndoes not contribute any novel data, but it is rather a compilation of\nexisting datasets. The claim that GFMBench mitigates \"issues of data scarcity\"\ndoes not seem truly accurate.\n\n- On page 5 it is claimed that \"GFMBench\" tackles the problem of \n\"benchmark standardization\". While I understand the contribution of common\nmetrics and implementation, I don't see how this is achieved in the\n\"hyperparameter settings\". How is standardization achieved in this regard?\n\n- In Line 486, it is said that \"OmniGenome consistently performs well across various genomic benchmarks\",\nhowever, Table 4 results do not seem to match this claim (correct me if I'm wrong).\n\n- Through the paper, the authors praise the performance of \"OmniGenome\" GFM,\nas it shows good performance, especially on RGB and PGB tasks.\nIs OmniGenome also a contribution to the paper? If so, it would be great\nto have more details on it. If not, it would be great to provide more insights\ninto the other models and why they perform better or worse in some tasks. \nSimilarly, since the contribution of the paper is a benchmark, it would be \nmore significant to provide some insight into the difficulty of the tasks\nand why performance varies across them, instead of commenting only on OmniGenome.\nI acknowledge that the paper highlights repeated times that the major performance\nof OmniGenome is due to the fact that it is trained on \"secondary structure\" tasks,\nHowever, this fact is repeated multiple times in the paper while no comment\nis provided on the other models or tasks at hand and why \"secondary structure\"\nimproves performance on the tasks."
            },
            "questions": {
                "value": "A list of questions, minor issues and suggestions is included here:\n\n\n- The abstract mentions \"the absence of open-source software for diverse genomics\".\nThis doesn't sound true to me. Genomics is especially rich in open-source\ntools (e.g. see the collection of tools on the Galaxy platform \n\"The Galaxy platform for accessible, reproducible and collaborative biomedical analyses\").\n\n- On page 2 \"lack of comprehensive and diverse datasets necessary for robust training and testing of GFMs.\".\nIt would be great to support these claims with related work or references.\n\nThe first paragraph of page 4 is repeated twice.\n\n- In Table 7 caption: \"in Agro-NT\".\nPlease include a citation to a tool, work, dataset or reference when mentioning\nit for the first time. This happens also later in the text, e.g. \"ViennaRNA\", \n\"Archive2\", \"Stralign\", etc.\n\n- In Table 5 caption: \"These benchmark datasets are held out or not included in the pretraining database\".\nIt is unclear to me what this means. Would be thankful if the authors \ncould clarify what this means.\n\n- What experiment is Appendix C referring to? Please clarify what these\nhyperparameters are for.\n\n- On page 6, \"We provide a tutorial for AutoBench in ...\".\nTo make the paper more consistent, it would be better to include the\ntutorial URL in a footnote too.\n\n- I understand that AutoBench is a standardisation suite for benchmarking, but\nI believe that the paper could benefit from a more detailed explanation of it\nand its contribution. For instance, how is it an \"automated\" benchmarking solution?\n\n- Genome data augmentation is mentioned as a contribution, however,\nno technical details are provided on how this is achieved. This is indeed\na subject of interest, and it would be great to have more\ndetails.\n\n- While I think that this is not a trivial issue, the paper does not mention\nhow are long sequences treated with the models. Is this because this is not an issue\nfor the model's evaluation? Otherwise, how is this managed? Is this handled or\nshould be handled in the GFMBench framework?\n\n- Why does \"GFMBench\" appear as a model in Table 4?\n\n- There are some spelling errors, e.g. \"pertaining\" in Line 1080.\nPlease double-check for spelling errors.\n\n- \"blastn\" in Line 1066 is usually written in uppercase."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces GFMBench, a framework for standardizing and automating benchmarking of Genomic Foundation Models (GFMs), which integrates four large-scale benchmarks containing 42 million genomic sequences across 75 datasets. The framework addresses several challenges in genomic modeling by providing unified protocols, consistent metrics, and automated evaluation pipelines that work across both DNA and RNA tasks from multiple species. GFMBench is released as open-source software with user-friendly interfaces, built-in tools for genomic tasks, and includes a public leaderboard and online hub for sharing benchmark results and resources with the research community."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper integrates 4 large genomics benchmarks from other groups into a unified framework. They also create a way to run a pipeline with these benchmarks with GFMs."
            },
            "weaknesses": {
                "value": "The paper exhibits several significant limitations. First, the technical innovation appears limited in scope. While the work integrates multiple existing benchmarks into a unified framework, it does not create new benchmarking methodologies or metrics. The automation and standardization approaches described largely follow established machine learning practices, without introducing fundamental methodological advances in benchmarking or evaluation techniques.\n\nThe work's focus is predominantly engineering-oriented rather than research-driven. The main contributions center on developing practical tools and interfaces for the genomics community. While valuable from a practical perspective, the emphasis on standardization over novel scientific insights raises questions about its suitability for a top-tier machine learning conference. Furthermore, similar benchmark aggregation platforms already exist in other machine learning domains, making this approach less innovative."
            },
            "questions": {
                "value": "What specific technical innovations in benchmarking does GFMBench introduce?\n\nWhat novel metrics or evaluation approaches are introduced?\n\nWhat specific reproducibility issues does it solve that other frameworks don't?\n\nWhy were these 4 benchmarks and then GFMs selected among others?\n\nIn Table 4, why is GFMBench listed as a model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}