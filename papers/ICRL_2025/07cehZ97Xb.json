{
    "id": "07cehZ97Xb",
    "title": "How to Build a Pre-trained Multimodal model for Simultaneously Chatting and Decision-making?",
    "abstract": "Existing large pre-trained models typically map text input to text output in an end-to-end manner, such as ChatGPT, or map a segment of text input to a hierarchy of action decisions, such as OpenVLA. However, humans can simultaneously generate text and actions when receiving specific input signals. For example, a driver can make precise driving decisions while conversing with a friend in the passenger seat. Motivated by this observation, we consider the following question in this work: is it possible to construct a pre-trained model that can provide both language interaction and precise decision-making capabilities in dynamic open scenarios. We provide a definitive answer to this question by developing a new model architecture termed Visual Language Action model for Chatting and Decision Making (VLA4CD), and further demonstrating its performance in challenging automonous driving tasks. We build VLA4CD on the basis of transformer-based LLM architecture. Specifically, we leverage LoRA to fine-tune a pre-trained LLM with data of multiple modalities covering language, visual, and action. Unlike the existing LoRA operations used for LLM fine-tuning, we have designed new computational modules and training cost functions for VLA4CD. These designs enable VLA4CD to provide continuous-valued action decisions while outputting text responses. In contrast, existing LLMs can only output text responses, and current VLA models can only output action decisions. Moreover, these VLA models handle action data by discretizing and then tokenizing the discretized actions, a method unsuitable for complex decision-making tasks involving high-dimensional continuous-valued action vectors, such as autonomous driving. The extensive experimental results on the closed-loop autonomous driving platform CARLA validate that: (1) the model construction method we proposed is effective; (2) compared to the state-of-the-art VLA model, VLA4CD can provide more accurate real-time decision-making while retaining the text interaction capability inherent to LLMs.",
    "keywords": [
        "vision language action model; decision making; autonomous driving; multimodal"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "We propose an approach to build a pre-trained multimodal model for simultaneously chatting and decision-making.",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=07cehZ97Xb",
    "pdf_link": "https://openreview.net/pdf?id=07cehZ97Xb",
    "comments": [
        {
            "title": {
                "value": "Response Weaknesses and Questions"
            },
            "comment": {
                "value": "Thanks for your time and efforts in reviewing our paper! We highly appreciate your thoughtful and constructive suggestions. Your thoughtful and constructive suggestions have been invaluable to us, and we have carefully considered each comment. Our responses to your queries are outlined below:\n\n***Response to Weaknesses1:***\n\nThank you very much for your recognition of our proposed questions, but I would also like to take this opportunity to provide a more detailed introduction to our motivation. As you mentioned, \"As stated in the abstract, driving and chatting simultaneously, however, the proposed LLM-based model is autoregressive decoding. From the architecture overview, it can be seen that the model can only provide predictions after a very long answer output.\" Our proposed VLA4CD model aims to mimic the multitasking capabilities of humans in complex environments, aiming to generate multiple outputs in parallel, thereby improving decision-making efficiency and user experience. Unlike traditional multimodal single-task models like LLaVA-like MLLMs, VLA4CD model is proposed to address the limitations of existing multimodal models in handling complex tasks, particularly the conflict between action decision-making and language generation. Current multimodal large models, due to their shared generation process, are unable to generate both efficient text and precise action instructions simultaneously in complex environments, leading to reduced decision accuracy and dialogue capabilities. VLA4CD overcomes this by using multimodal inputs and outputs and establishing independent loss functions for each task, enabling parallel task processing. This approach avoids task conflicts and significantly improves decision efficiency, adaptability, and overall performance in multitask, complex scenarios.\n\n***Response to Weaknesses2:***\n\nUnlike traditional multimodal single-task models such as LLaVA-like MLLM, VLA4CD model is proposed to address the limitations of existing multimodal models in handling complex tasks, particularly the conflict between action decision-making and language generation. Current multimodal large models, due to their shared generation process, are unable to generate both efficient text and precise action instructions simultaneously in complex environments, leading to reduced decision accuracy and dialogue capabilities. VLA4CD overcomes this by using multimodal inputs and outputs and establishing independent loss functions for each task, enabling parallel task processing. This approach avoids task conflicts and significantly improves decision efficiency, adaptability, and overall performance in multitask, complex scenarios. \n\nYour point that \"the method of combining these two tasks (chatting and decision-making) does not actually predict simultaneously but sequentially\" is very insightful. Due to our unclear expression, it caused a misunderstanding. VLA4CD emphasizes not only multimodal output but also multimodal synergy, specifically sequential prediction. In autonomous driving tasks, the importance of decision-making is higher than question answering. Therefore, we can enhance decision-making through text and images, as shown in Table 4, where we maintain the input as images and text, and removing the language part from the loss function significantly reduces the system's decision-making ability, corresponding to line 483, \"we see that including language in the loss function significantly enhances the quality of decision-making.\" This means sequentially predicting A based on text sensor input and Q, and then predicting action based on images, text sensor input, and Q together. This sequential prediction highlights the priority and importance of the two tasks, which we believe is a very valuable point.\n\n***Response to Weaknesses3:***\n\nThank you for your detailed prompts. We will thoroughly check the errors in the paper and revise and explain the points you mentioned regarding the interpretation of our views.\n\n***Response to Questions1:***\n\nDuring training, the minimum batch size for images is 8, with a context length of 1 or 4 in each batch. During inference, real-time inference is performed for each frame image based on the environment.\n\n***In summary***\n\nOverall, you have raised very valuable and insightful comments. We will ensure to revise the paper and clearly address the points you mentioned. Finally, we sincerely hope that these explanations will alleviate your concerns, and we sincerely hope that you will reconsider your score."
            }
        },
        {
            "title": {
                "value": "Response Questions"
            },
            "comment": {
                "value": "***Response to Q1 and Q2:***\n\nYou have pointed out a very good issue. I apologize for the oversight in our writing. The entire lines 151-157 regarding the character description should be revised as follows:\n\n> \"We consider a multimodal setting similar to (Xiao et al., 2020), wherein, at each time step $t$, upon the agent performing an action  $a_t$, the environment returns an observation consisting of both visual and textual modalities, denoted by $(o_t, \\hat{w}_t)$. Our objective is to build a generative model:\n> \n> $\\pi$(a_t, \\hat{w*}_t | o_t-H, \\hat{w}_t-H, a_t-H, ....... , o_t, \\hat{w}_t)\n> \n> which can generate both high-quality action decisions and text responses, given a sequence of historic observations and actions. Here, $\\hat{w*}_t $ denotes a text-formed response to the text-formed input $\\hat{w}_t$. If $\\hat{w}_t$  is a question, then  $\\hat{w*}_t $ can be seen as its answer given by our model.  $H$ denotes the length of the context.\"\n\n***Response to Q3:***\n\nFor simplicity, we omitted the index (n_i). We set the token counts for text and image inputs uniformly to 424 and 64, respectively, corresponding to the parameter `cutoff_len` in Table 6 and `num_patches` in Table 7.\n\n***Response to Q4:***\n\nAs shown in the different time series descriptions in Appendix 11, the text descriptions of Other Sensors Input 32 to Other Sensors Input 44 mainly come from fixed templates, with little variability, almost solely dependent on numerical changes. In real scenarios, this variability would significantly increase. Using the cross-entropy loss function may cause the model to struggle distinguishing between different time-step Other Sensors Input descriptions, leading to inaccurate predictions. As a result, the final output action values are almost identical, insensitive to numerical changes in text inputs, and unable to achieve finer-grained decision-making and differentiation. Therefore, adding label smoothing loss can enhance the model's perception of differences in Other Sensors Input descriptions at different time steps, making it more sensitive to numbers, and enabling finer-grained decision-making and control.\n\n***Response to Q5:***\n\nAlthough the VLA4CD (no language) model architecture is still based on Figure 1, with inputs still being images and text, ideally, human questions and {s_t^{l+1}, .., s_t^{l+n}} should be removed during training. However, to quickly verify the impact of removing language loss, we adopted the direct removal approach.\n\n***Response to Q6:***\n\nWhen balancing the two losses for DriverGPT4 and OpenVLA, we used the same hyperparameter settings as VLA4CD, with the specific parameter settings detailed on lines 263-264.\n\n***Response to Q7:***\n\nWe adopted the same discretization strategy for `action_bin` as OpenVLA and RT2, which involves discretizing continuous actions (acceleration and steering) into a fixed number of bins using a uniform discretization method. The discretized action values are then mapped to the end tokens of the pre-trained tokenizer's vocabulary to generate the corresponding tokens for actions. Similarly, these tokens can be inversely mapped back to the original continuous action values. This strategy allows encoding continuous actions as discrete tokens while preserving the precision of the continuous action information.\n\n***Response to Q8:***\n\nThis is a very good question. In Table 9, we compared the designs with and without image reconstruction loss when the inputs are both images and text. We found that the design with image reconstruction loss further enhances the final decision control. Our action space extends to multiple dimensions, which can be fully achieved by changing the output dimension of the action linear projection layer. Here, we only use two dimensions because we mainly consider the two key variables in autonomous driving scenarios.\n\n***In summary***\n\nOverall, you have raised very valuable and insightful comments. We will ensure to revise the paper and clearly address the points you mentioned. Finally, we sincerely hope that these explanations will alleviate your concerns, and we sincerely hope that you will reconsider your score."
            }
        },
        {
            "title": {
                "value": "Response  Weaknesses"
            },
            "comment": {
                "value": "Thanks for your time and efforts in reviewing our paper! We highly appreciate your thoughtful and constructive suggestions. Your thoughtful and constructive suggestions have been invaluable to us, and we have carefully considered each comment. Our responses to your queries are outlined below:\n\n***Response to Weaknesses1: \"What is the motivation of our paper?\"***\n\nThe motivation behind the VLA4CD model stems from key challenges faced by current multimodal models in handling complex tasks, particularly the conflict between action decision-making and language generation. Existing MLLM model, although using multimodal input, share the same generation process for both text and action outputs, which leads to task conflicts. This issue is especially evident in complex scenarios, where the model struggles to simultaneously generate efficient text and precise action instructions, ultimately affecting decision accuracy and dialogue quality.\n\nVLA4CD addresses this by introducing independent objective functions for action decision-making and language generation tasks, ensuring that these tasks can be processed efficiently in parallel. This design not only avoids task conflicts but also improves the model's adaptability and decision consistency in complex environments. Additionally, VLA4CD leverages multimodal inputs and outputs to take full advantage of multitask coordination, simplifying the system architecture and reducing training costs. With these improvements, VLA4CD aims to enhance overall performance, resource utilization, and user experience in complex tasks, providing a stronger solution for multimodal, multitask processing.\n\nWe define \"chatting\" as open-ended, multi-turn conversations, and VLA4CD enhances its parallel generation capabilities in decision-making and dialogue tasks through LoRA fine-tuning. Although the model is not specifically trained for everyday chatting, its performance in open-ended dialogues can be further improved by expanding the QA dataset or leveraging the language generation capabilities inherent in large models, thereby achieving broader conversational functions.\n\nFor the need to extend everyday chatting capabilities, we can follow this design approach. When expanding the QA system, we can keep the sensor input relevant to the driving scenario and only adjust the QA dataset content. This approach does not interfere with the model's decision-making ability, as modifying the QA content does not involve changes to sensor data or environmental perception data. Therefore, by reasonably adjusting and expanding the QA pairs, the model can maintain efficient decision-making capabilities in driving tasks and exhibit stronger dialogue capabilities in extended chatting scenarios.\n\n***Response to Weaknesses2:***\n\nIn both generating dialogues and action generation capabilities, we generate them simultaneously, as shown in Figure 5. The independent evaluation of text and action generation capabilities is solely to demonstrate the comparison between VLA4CD and models with only single dialogue or decision-making abilities. The model does not actually execute actions based on text outputs; as shown in Figure 5(a), we unify the action outputs for decision control to maintain consistency. Regarding text explanations, we use GPT-4o for scoring, as shown in Figure 4, where VLA4CD outperforms other solutions in this scenario, but cannot guarantee 100% correctness. Since GPT-4o has strong language understanding capabilities and avoids human evaluator subjectivity, we chose the third-party GPT-4o for scoring, which is also a mainstream evaluation method currently.\n\n***Response to Weaknesses3:***\n\nWe use benchmarks based on the Gym-Carla environment, designed specifically for reinforcement learning agents in autonomous driving scenarios. Since the core of reinforcement learning is optimization through reward functions, we customized the evaluation matrix according to task requirements, as detailed in Appendix A.5, which defines multiple evaluation criteria. These criteria may differ from traditional benchmarks in the CARLA leaderboard, leading to differences in experimental results. We use these customized evaluation matrices because our dataset is collected based on reinforcement learning agents, whose behaviors and adaptability need to be evaluated through reinforcement learning-specific methods. To better assess the explicit feedback mechanism and continuous learning capabilities we propose, these customized matrices can more accurately reflect the actual effectiveness of the method."
            }
        },
        {
            "title": {
                "value": "Response Part (3/3)"
            },
            "comment": {
                "value": "***Response to Q5: Why do the authors think using separate loss for language and action generation (unlike DriveGPT4) improves the decision making performance?***\n\nWe believe that using separate loss functions for language and action generation (unlike DriveGPT4) can improve decision performance mainly because this design avoids task conflicts and enables effective parallel processing of multiple tasks. In DriveGPT4, text generation and action generation share the same generation process, leading to conflicts between the two tasks, especially in complex environments where the model cannot simultaneously generate efficient text and action instructions. As a result, DriveGPT4 cannot guarantee fine-grained action instructions at each moment, affecting its decision accuracy and dialogue capabilities.\n\nIn contrast, VLA4CD sets up independent objective functions for text generation and action generation, ensuring that each task can be processed efficiently in parallel, avoiding task conflicts. This design allows VLA4CD to generate text and action instructions simultaneously in complex tasks, thereby enhancing the model's decision-making ability and efficiency in multitask environments. Through independent loss functions, VLA4CD ensures the effectiveness and completeness of each output when handling multi-objective tasks, improving the model's adaptability and decision performance.\n\n***Response to Q6: Why do the authors only focus on the self-driving task?***\n\nThe question you raised is very important and indeed worth in-depth discussion. Our current research primarily focuses on autonomous driving tasks because we built the multimodal fusion, alignment, and system loss functions from scratch, fine-tuning a large model based on Llama-7B, and completed dataset collection, model establishment, and full evaluation and inference design. This process consumed a significant amount of time and cost, so we decided to first implement and evaluate the model in the autonomous driving scenario.\n\nHowever, this does not mean that we are limited to the autonomous driving field. In the future, we fully intend to extend this approach to other areas such as robotic arms and achieve multiple objective task outputs (e.g., parallel processing of three or more tasks). Autonomous driving is merely the starting point for verifying and testing this multitask processing framework. As the technology matures and the model is optimized, we plan to promote it to broader application scenarios.\n\n***In summary***\n\nOverall, you have raised very valuable and insightful comments. We will ensure to revise the paper and clearly address the points you mentioned. Finally, we sincerely hope that these explanations will alleviate your concerns, and we sincerely hope that you will reconsider your score."
            }
        },
        {
            "title": {
                "value": "Response Part (2/3)"
            },
            "comment": {
                "value": "***Response to Q2: Why are there no use case of the model actually chatting? How do the authors define chatting? The example of the introduction mentions the authors are inspired by human driver talking to a friend while driving, but the model doesn't actually engage in free form chat that goes beyond a single step.***\n\nThe design goal of the VLA4CD model is to mimic human multitasking abilities in complex environments, particularly in parallel generation of action decisions and dialogues. We define \"chatting\" as open-ended, multi-turn conversations, and VLA4CD enhances its parallel generation capabilities in decision-making and dialogue tasks through LoRA fine-tuning. Although the model is not specifically trained for everyday chatting, its performance in open-ended dialogues can be further improved by expanding the QA dataset or leveraging the language generation capabilities inherent in large models, thereby achieving broader conversational functions.\n\nFor the need to extend everyday chatting capabilities, we can follow this design approach. When expanding the QA system, we can keep the sensor input relevant to the driving scenario and only adjust the QA dataset content. This approach does not interfere with the model's decision-making ability, as modifying the QA content does not involve changes to sensor data or environmental perception data. Therefore, by reasonably adjusting and expanding the QA pairs, the model can maintain efficient decision-making capabilities in driving tasks and exhibit stronger dialogue capabilities in extended chatting scenarios.\n\n***Response to Q3: How does author plan to make the model robust to noise when exposed to unrestricted chat?***\n\nGreat question! The key points you mentioned are how the model maintains robust decision-making capabilities in the presence of increased noise and how to extend VLA4CD's everyday chatting abilities by altering the QA system content.\n\nAs seen in Table 5, when sensor input noise increases, the model's decision-making capabilities are indeed affected. This is because noise interferes with the model's perception input, thereby affecting its decisions based on these inputs. However, VLA4CD is designed to minimize this impact when sensor inputs are relevant to driving tasks. Therefore, as long as the sensor input information is noise-free, the model can maintain strong robustness, ensuring that decision-making capabilities are not disrupted.\n\nFor the need to extend everyday chatting capabilities, we can follow this design approach. When expanding the QA system, we can keep the sensor input relevant to the driving scenario and only adjust the QA dataset content. This approach does not interfere with the model's decision-making ability, as modifying the QA content does not involve changes to sensor data or environmental perception data. Therefore, by reasonably adjusting and expanding the QA pairs, the model can maintain efficient decision-making capabilities in driving tasks and exhibit stronger dialogue capabilities in extended chatting scenarios.\n\nIn short, as long as the sensor input is effective and stable, the VLA4CD model can maintain high robustness and efficiency in decision tasks while adding everyday chatting functions.\n\n***Response to Q4: Why should we add language generation capability to VLAs? The motivation for that seems non-existent in this paper and there's no novel use case of the generated language.***\n\nThe VLA4CD model mimics human multitasking abilities in complex environments, aiming to generate multiple outputs simultaneously to enhance decision efficiency and user experience. Unlike traditional single-task models, VLA4CD handles both action decision-making and language generation tasks, simplifying system architecture and reducing coordination issues. By integrating language generation capabilities into a unified model, VLA4CD better adapts to complex tasks, improving decision consistency and effectiveness. Inspired by human ability to simultaneously make decisions and converse in autonomous driving scenarios, VLA4CD was initially designed for specific tasks but innovates by extending multitask processing capabilities, enabling the model to handle a broader range of applications, enhancing adaptability and interactivity. For example, the VLA4CD model can manage multiple tasks in smart home management, such as adjusting temperature, controlling lighting, and checking security devices; it can also parallel process tasks in scenarios like controlling multiple robotic arm systems, showcasing its advantages in multitask parallel processing."
            }
        },
        {
            "title": {
                "value": "Response Part (1/3)"
            },
            "comment": {
                "value": "Thanks for your time and efforts in reviewing our paper! We highly appreciate your thoughtful and constructive suggestions. Your thoughtful and constructive suggestions have been invaluable to us, and we have carefully considered each comment. Our responses to your queries are outlined below:\n\n***Response to Weaknesses1: \"What is the motivation of our paper?\"***\n\nThe motivation behind the VLA4CD model stems from key challenges faced by current multimodal models in handling complex tasks, particularly the conflict between action decision-making and language generation. Existing MLLM model, although using multimodal input, share the same generation process for both text and action outputs, which leads to task conflicts. This issue is especially evident in complex scenarios, where the model struggles to simultaneously generate efficient text and precise action instructions, ultimately affecting decision accuracy and dialogue quality.\n\nVLA4CD addresses this by introducing independent objective functions for action decision-making and language generation tasks, ensuring that these tasks can be processed efficiently in parallel. This design not only avoids task conflicts but also improves the model's adaptability and decision consistency in complex environments. Additionally, VLA4CD leverages multimodal inputs and outputs to take full advantage of multitask coordination, simplifying the system architecture and reducing training costs. With these improvements, VLA4CD aims to enhance overall performance, resource utilization, and user experience in complex tasks, providing a stronger solution for multimodal, multitask processing.\n\n***Response to Weaknesses 2 and 3: Please refer to the answers below for Q1, Q2, and Q3.***\n\n\n***Response to Q1: How does the model compare to DriveGPT4 and why does DriveGPT4 do so bad? DriveGPT4 is doing exactly same thing as this model aims to do (text generation + action generation).***\n\nCompared to DriveGPT4, VLA4CD demonstrates significant advantages, particularly in multitask parallel processing and output effectiveness.\n\nFirstly, DriveGPT4 is designed with a multimodal input but single-modal output framework, generating both text and action instructions through a detokenizer. However, this approach has inherent limitations, especially in generating fine-grained decision instructions at each moment. This is because text generation and action generation tasks in DriveGPT4 are not independent but share the same generation process. This leads to task conflicts, particularly in complex scenarios, where the model cannot simultaneously generate efficient text and actions. As shown in Figure 2 and Tables 1, 2, and 3, due to this task conflict, DriveGPT4 cannot consistently generate efficient and complete text and action instructions, severely affecting its decision accuracy and dialogue capabilities.\n\nIn contrast, VLA4CD employs a multimodal input and multimodal output architecture, with separate objective functions specifically for text generation and action generation. This ensures that the model can generate efficient text and action instructions simultaneously when handling complex tasks. By designing independent objective functions, VLA4CD can output text and actions in parallel, avoiding task conflicts. This parallel processing not only simplifies system design but also significantly enhances the model's decision-making ability and efficiency in multitask complex environments.\n\nMoreover, VLA4CD's parallel processing mode effectively improves the model's adaptability in multi-objective tasks, ensuring the effectiveness and completeness of text generation and action decision-making. In contrast, DriveGPT4, due to its shared generation mechanism, often struggles with task conflicts, resulting in the inability to generate high-quality text and precise action instructions simultaneously. This parallel design makes VLA4CD far superior to DriveGPT4 in multi-objective tasks."
            }
        },
        {
            "summary": {
                "value": "The current manuscript proposes to build a large language model (LLM) capable of understanding multiple modalities like text, vision, and actions; and producing them as outputs. In particular, it develops a Visual Language Action model for Chatting and Decision Making (VLA4CD) that produces continuous actions without losing its ability to chat with a user simultaneously. Notably, the action space is not discretized and kept continuous, unlike prior works in this area. The paper also demonstrates experiments on CARLA dataset to claim that this approach is effective and can provide real-time decision making compared to prior art, while retaining its text interaction capability."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "(S1) In general, the intended direction of this work, i.e., a model that can take actions while retaining the ability to generate textual responses to a user is useful. Please see weaknesses for further discussion.\n\n(S2) The technical details as presented in the paper are easy to understand and follow."
            },
            "weaknesses": {
                "value": "(W1) The current manuscript suffers from a clear lack of motivation for why we need a model that can produce both actions and also \u201cchat\u201d (L21, for instance) with a user. There are two main problems here:\n* Throughout, the ability of \u201cchatting with people\u201d (L88) has not been characterized well. It is not open-ended dialog on any topic but rather an explanation of what actions to take or why it has taken a certain action in a given situation. This is misleading as currently phrased.\n* Much of the motivation is around \u201ca human driver can operate while chatting with a friend\u201d, which does not apply to why we need a unified model. For instance, why not have an actuation model and an open-ended dialog model in the autonomous vehicle to achieve the above desired motivation? This indicates the lack of a clear motivation from an application standpoint.\n\n\n(W2) Even if one were to scope the \u201cchatting with users\u201d ability down to producing explanations as responses to a fixed set of templated questions (see A.10), the manuscript does not follow through via corresponding experiments. Both actions and text-generation capability has been evaluated independently, once again begging the question as to why such a unified system is useful. There are no experiments to verify the following:\n* The model actually actuates based on the textual outputs? I.e., if the model responds with \u201cI will take the right lane in 20 mins\u201d, does it actually do that?\n* Are these textual explanations correct/sound given the state of the environment?\n* What is the correlation of the GPT-4o score evaluation with human evaluation? \n\n\n(W3) There are some concerns around the experimental validation of the proposed methodology:\n* The reported experiments on town03 and town04 from the CARLA environment do not seem to match with any of the existing benchmarks with prior works (C, D). \n* To further exacerbate this issue, none of the baseline results are from literature and have been reported based on reproductions in this work. \n* Missing baselines, see [A] for more information.\nThis raises serious questions about the efficacy and usefulness of the proposed methods from an empirical standpoint. Why were existing, standardized benchmarks not used for model comparisons? Request the authors to address these concerns without which the benefits of this approach will remain unclear.\n\n\nReferences\n* [A] DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving. https://arxiv.org/pdf/2312.09245.\n* [B] Think2Drive: Efficient Reinforcement Learning by Thinking with Latent World Model for Autonomous Driving (in CARLA-v2). https://arxiv.org/pdf/2402.16720\n* [C] CARLA Autonomous Driving Leaderboard. https://leaderboard.carla.org/\n* [D] TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving. https://arxiv.org/pdf/2205.15997"
            },
            "questions": {
                "value": "(Q1) L154: Do you also include model textual response \\hat{w}_i, i = {1,..,H} in w_i?\n\n(Q2) Eq 1: \\hat{w} is overloaded as both the model textual response and the embeddings of text inputs\n\n(Q3) Eq 1: Each of w_i might have a different number of tokens (different from n). Do you pad them to n or is the index (n_i) dropped for brevity? That is: (w_i^1, w_i^2\u2026.w_i^{n_i}) instead of just (w_i^1, w_i^2\u2026.w_i^{n})\n\n(Q4) L222-L225: The observed phenomenon is not clear here. Referring to the appendix also doesn\u2019t add more details, apart from the empirical observation. Can the authors describe this with an example? \n\n(Q5) VLA4CD (no-language): What is the architecture, inputs for this model? Ideally, the human question in the input and the {s_t^{l+1}, .., s_t^{l+n}} must be removed while training.\n\n(Q6) L413: How did you balance the two losses for DriverGPT4? Did you have a hyperparameter search for the loss weights similar to your approach?\n\n(Q7) L477: The reasoning here is heavily dependent on the discretization strategy used for each environment. How were the actions discretized for this environment? Was there a hyperparameter search performed to get the best strategy?\n\n(Q8) L140-142: How is this problem avoided in the current setup? It\u2019s not clear in the text here.\n* Action space dimension is small, i.e., 2 (acceleration and steering) How does this scale with more variables?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "- The authors propose VLA4CD, a model for self-driving that is capable of generating both language and action tokens. \n- VLA4CD is trained using three different types of loss functions: language generation, action generation, and image reconstruction. \n- The trained model demonstrates superior performance in both decision-making and question answering compared to models such as DriverGPT4 and OpenVLA in the gym-carla environment."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The main contribution of this work is that the authors add a language generation capability to VLA in self-driving scenarios.\n- The paper is well-structured, making it easy to read and understand the authors' approach.\n- The finding that separating language and action prediction loss can improve decision-making is a significant contribution that provides valuable insights into how VLAs can be effectively trained. It is encouraging to see that this is empirically demonstrated to be useful in self-driving scenarios. However, it is concerning that introducing some language noise into the training dataset can have a considerable impact on decision-making processes. Since real-world datasets will inevitably contain substantial noise, developing methods to ensure robustness against such noise is essential for the model's practical application."
            },
            "weaknesses": {
                "value": "- I don't quite understand why VLAs need to chat based on the author's motivation. Chatting is an inherently multi-turn conversation with a specific topic, but example of such capability of the model is completely lacking. I wonder what the authors' definition of chatting is. The model doesn't actually \"chat\" but simply outputs action description. It is far from the example in the introduction where authors want to build a model that can talk with a friend while driving.\n- Text generation has already been explored with DriveGPT4. In this paper, text generation is not used for any novel applications other than simply translating action tokens into language. I fail to understand why does the author claim text+action generation is something novel since there's already a model that does it. \n- Adding chat capabilities could potentially make the model less robust when exposed to noise in language interactions. Since the model learns to associate language with specific action tokens, any slight disruption to that association (e.g., due to noise) could significantly impair its action prediction performance. If the model can engage in unrestricted conversation, it is likely to encounter more noise, which could seriously affect its decision-making abilities, which is the most important goal of VLAs. It might be more effective for VLAs to focus solely on action prediction and incorporate chat functionality with separate models. With the current motivation, it seems there is no strong rationale or necessity to integrate chat capabilities into VLAs.\n- Following the point above, I feel like the paper could be better framed on how to manage loss when training VLAs, which is a much interesting topic.\n\nTypo: Figure 1 interatcion"
            },
            "questions": {
                "value": "- How does the model compare to DriveGPT4 and why does DriveGPT4 do so bad? DriveGPT4 is doing exactly same thing as this model aims to do (text generation + action generation).\n- Why are there no use case of the model actually chatting? How do the authors define chatting? The example of the introduction mentions the authors are inspired by human driver talking to a friend while driving, but the model doesn't actually engage in free form chat that goes beyond a single step.\n- How does author plan to make the model robust to noise when exposed to unrestricted chat?\n- Why should we add language generation capability to VLAs? The motivation for that seems non-existent in this paper and there's no novel use case of the generated language.\n- Why do the authors think using separate loss for language and action generation (unlike DriveGPT4) improves the decision making performance?\n- Why do the authors only focus on the self-driving task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper opposed to simulating MLLMs as human in a real-world situation that require both chatting and decision making. For example, human drive can drive safely while having conversations with passengers. This is an important application problem in autonomous driving system."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The problem that developing a chatting and simultanenous decision making, itself is underexplored and important.\n2. The proposed model gives both reasonable question answering output and reasonable action output."
            },
            "weaknesses": {
                "value": "1. I don't buy the idea that by simply concatenating the question answer data and action prediction data together and supervised finetune the LLaVA-like MLLM can solve the proposed problem. As described in the abstract, driving and chatting simultaneously, however the proposed LLM-based model is autoregressive decoding. From the architecture overview, we can see this proposed model can only provide a prediction after a very long answer output. No inference speedup or simultaneous decoding technology is being used or proposed to achieve this.\n2. The only contribution to me seems combining action prediction and question answer data, which is very trivial. No siginificant improvement is achieved compared with specialist model in each single task. And the approach to combine these two tasks chatting and decision making tasks are not actually achieving prediction both but sequentially.\n3. The paper neeeds to be revised in the writing. Such as notions in Figure 1 is totally missing. Training and architecture details are missing in Experiments section, etc."
            },
            "questions": {
                "value": "1. What is the average number of images per training and inference case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}