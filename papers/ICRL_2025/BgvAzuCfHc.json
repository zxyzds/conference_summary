{
    "id": "BgvAzuCfHc",
    "title": "Self-Supervised Feature Re-Representation via Lennard-Jones Potential Loss",
    "abstract": "The Lennard-Jones potential, initially developed to model molecular interactions, is characterized by a repulsive force at short distances to prevent over-clustering and an attractive force at longer distances to maintain balanced proximity, resembling the equilibrium-seeking behavior of particles in natural systems.  This offers a potential pathway for more orderly entropy reduction in higher-order features.\nThis paper introduces a self-supervised approach for feature re-representation, utilizing a Lennard-Jones potential loss to constrain the gradient directions between positive and negative features in computer vision tasks.  Unlike supervised learning directly driven by downstream tasks or contrastive learning with multi-label data pairs and multi-feature extractors, the proposed loss term integrates with existing task-specific losses by directly constraining gradient directions, thereby enhancing the feature learning process.\nExtensive theoretical analysis and experimental results demonstrate that, across various domains, datasets, network architectures, and tasks, models incorporating the Lennard-Jones potential loss significantly outperform baseline models without this auxiliary loss in both accuracy and robustness.  This approach highlights the potential of physics-inspired loss functions to improve deep learning optimization.",
    "keywords": [
        "Physics-Inspired Optimization",
        "Pluggable Self-Supervised Loss",
        "Lennard-Jones Potential"
    ],
    "primary_area": "optimization",
    "TLDR": "A Loss function via Lennard-Jones potential for generalized gradient optimization.",
    "creation_date": "2024-09-16",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=BgvAzuCfHc",
    "pdf_link": "https://openreview.net/pdf?id=BgvAzuCfHc",
    "comments": [
        {
            "summary": {
                "value": "Inspired by the Lennard-Jones potential, this work proposed the corresponding loss function to help computer vision tasks. By setting hyperparameters appropriately, the loss aims to balance intra-class and inter-class distributions. Experiments are conducted on multiple tasks for evaluation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Exploring theoretical results from physics for deep learning is an interesting direction.\n\n2. This work considers multiple computer vison tasks, e.g., classification, segmentation, etc.\n\n3. Both ViT and ResNet are adopted in evaluation to demonstrate the effectiveness of the proposed loss function."
            },
            "weaknesses": {
                "value": "1. The motivation is unconvincing. In L247, it shows that the system is with zero net force when $r=\\sigma$. However, the gradient does not equal to zero in that case and the value of the loss function can be smaller than 0 by minimizing it. Moreover, the analysis below Enq.7 is inconsistent with that below Enq.5. For example, when $r<\\sigma$, which means the pair of examples are similar, the analysis for Eqn.5 shows that the repulsive term dominates. On the contrary, that for Eqn.7 states that the attractive forces dominate. According to my understanding, the loss function in Eqn.6 just pushes all pairs to a pre-defined similarity $\\sigma$, which is more close to the statement after Eqn. 5 and it cannot help representation learning significantly.\n\n2. The data sets for experiments are quite limited. Note that only small data sets, e.g., CIFAR and TinyImageNet, are applied for classification. Lager benchmark data sets, e.g., ImageNet, should be included for comparison.\n\n3. The effectiveness of the proposed method is not well justified. The performance of baseline with the proposed loss function is even worse than the original baseline as illustrated in Fig. 5. \n\n4. The parameter $\\sigma$ is crucial for the success of the loss function and can be sensitive for different tasks. Including an ablation study on parameters can be better for demonstration the proposed method."
            },
            "questions": {
                "value": "Please kindly check the above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new loss function called LJ loss for self-supervised learning. LJ loss can automatically constrain similar sample features to attract each other and dissimilar samples to stay away from each other. Experimental results show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "If I have to say the highlight of this article, it is that it proposes a new loss function based on the lennard-jones potential, which automatically constrains similar samples to be close to each other and dissimilar samples to be separated from each other."
            },
            "weaknesses": {
                "value": "1. The motivation for this article can be summarized as the search for a means by which similar samples can be automatically constrained to attract each other and dissimilar samples to stay away from each other. Similar solutions have been proposed by researchers long ago, e.g., in the literature [1]. However, this article does not analyze it comparatively.\n\n[1] Song, Zeen, et al. \"On the Discriminability of Self-Supervised Representation Learning.\" arXiv preprint arXiv:2407.13541.\n\n2. The related work section is pretty weak. First, the authors ignore very recent hot network architectures such as transformer and KAN. Second, the authors have a one-sided understanding of self-supervised learning. There are many self-supervised methods that do not require positive and negative samples, such as BYOL, Barlow Twins, and MAE. Finally, this understanding of physically-guided deep learning is also inadequate. For example, the authors do not mention ReduNet, a learning framework based on the Principle of Maximizing Rate Reduction.\n \n3. Section THEORY is over-claimed. This chapter deals with the general form and characterization of the Lennard-Jones potential and contains no new concepts, definitions, and theorems. At the same time, the authors do not give the logical relationship between Section 3.2 and Section 3.3, and it seems that Section 3.2 is redundant.\n\n4. As shown in SimCLR, MoCo, BYOL, and Barlow Twins, we can see that self-supervised learning not only performs well on tasks similar to the training data, but also on transfer tasks. There are often significant differences between different tasks, and more mining of valid information in the training task may lead to overfitting of the training task, thus affecting the transferability of the self-supervised learning method. Does the LJ loss proposed in this paper reduce feature mobility? If not, please conduct the corresponding theoretical analysis."
            },
            "questions": {
                "value": "1. In Lines 288-293 is confusing. It is always known that for classification tasks, the larger the distance between classes, the more helpful it is for classification, and this idea is also the core idea of SVM. But the authors go on to emphasize that classification problems should focus only on intra-class distances.\n\n2. The authors repeatedly emphasize the problems in self-supervised learning, but the experimental design does not include a comparison with self-supervised learning methods, which is puzzling."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the Lennard-Jones (LJ) potential from thermodynamics to the field of self-supervised re-representation. In particular, a Lennard-Jones is proposed to regularize the downstream task training.   \nThe proposed loss is motivated from physics with repulsive force and attractive force interaction in the design.   \nTheory from physics gives some explanations to overfitting issue. \n\nExperiments are conducted on 2D image recognition, and 3D point cloud classification and segmentation tasks, with ViT as the backbone.   \nFrom the results, introducing LJ loss increases the performance of various tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "### A novel method from thermodynamics\n- This method is well-motivated from physics and thermodynamics. The replusive and attractive forces align with the self-supervised pair interactions. \n- Discussion with contrastive learning method is presented. \n- Theory of physics try to explain the overfitting issue from a new perspective. \n\n### LJLoss outperforms No LJLoss in most settings\n- Comparing the introduction of LJLoss and No LJLoss, the performance gains are consistent and large. \n- Both 2D and 3D tasks (classificaiton and segmentation) benefit from the LJLoss."
            },
            "weaknesses": {
                "value": "### What is the connection between LJLoss and the concept of Global and local features? \n- Sec. 3.2 desribes global and local features in detail. But it is unclear or there is no detailed explanation about the relationship with LJLoss\n\n### Experimental verification is only LJLoss and NoLJLoss\n- The experiments verify the effectiveness of LJLoss as an effective regularizer on CE loss of downstream finetuning tasks.   \nHowever, how about the Contrastive loss? The paper claims the benefits of LJ over contrastive. \n- As a regularization loss, regularization term weight $\\lambda$ is an important hyper-parameter. I did not find ablation on this term. \n- Same to other hyper-parameters such as $\\sigma$ and $\\epsilon$. \n\n### Other minor issues\n- What does SE Loss in Figure 1 mean?\n- Typos such as left and right quotations, e.g. line 081, line 125, and line 126."
            },
            "questions": {
                "value": "- What is the relationship betweem Global/Local features and the LJLoss? \n- Is there any numerical comparison between the contrastive loss and LJLoss? \n- Hyper-parameter effect of $\\lambda$, $\\sigma$, and $\\epsilon$. \n- What does SE Loss in Figure 1 mean?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a self-supervised feature re-representation technique using the Lennard-Jones potential inspired by molecular interactions as a loss function, aiming to balance intra-class compactness and inter-class separation in feature space. The method enhances feature clustering without predefined positive-negative pairs. Empirical results on various vision tasks show that the proposed Loss improves performance and robustness in models like ViT and ResNets across classification and segmentation tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed LJ potential seems to provide a novel approach to feature clustering. Empirical validation shows enhanced performance across several architectures and datasets."
            },
            "weaknesses": {
                "value": "1. Writing: Personably, I think that this submission is tedious and verbose. In the main body, many parts, such as Sec. 3.1 and 3.2, can be removed to save space for the content in the appendix, such as more experiments. Also, the logic in the appendix is unclear to me where often I do not understand why some descriptions should appear. I strongly suggest the authors to improve the writing.\n\n2. Terms: I do not understand why the proposed loss is \"self-supervised\". Also, what is \"RE-representation\"? This term appears 3 times only, including one in the title, one in the abstract, and the one in the conclusion. What exactly does it mean????\n\n3. Experiments: In current format (main body + appendix), I am not convinced by the results. Ablation study is missing. The improvements are marginal. There is no state-of-the-art comparison, or comparison with other additive losses, or simply regularizations. I do not see the value of adding such a loss."
            },
            "questions": {
                "value": "See my comments in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}