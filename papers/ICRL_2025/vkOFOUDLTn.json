{
    "id": "vkOFOUDLTn",
    "title": "Linear Multistep Solver Distillation for Fast Sampling of Diffusion Models",
    "abstract": "Sampling from diffusion models can be seen as solving the corresponding \n   probability flow ordinary differential equation (ODE). \n   The solving process requires a significant number of function \n   evaluations (NFE), making it time-consuming. \n   Recently, several solver search frameworks have attempted to find \n   better-performing model-specific solvers. However, predicting the impact of \n   intermediate solving strategies on final sample quality remains challenging, \n   rendering the search process inefficient.\n   In this paper, we propose a novel method for designing \n   solving strategies. We first introduce a unified prediction formula \n   for linear multistep solvers. Subsequently, we present a solver distillation \n   framework, which enables a student solver to mimic the sampling trajectory \n   generated by a teacher solver with more steps. We utilize the mean Euclidean \n   distance between the student and teacher sampling trajectories as a metric, \n   facilitating rapid adjustment and optimization of intermediate solving strategies.\n   The design space of our framework encompasses multiple aspects, \n   including prediction coefficients, time step schedules, and time scaling \n   factors. \n   Our framework has the ability to complete a solver search \n   for Stable-Diffusion in less than 10 total GPU hours.\n   Compared to previous reinforcement learning-based \n   search frameworks, \n   our approach achieves over a 10$\\times$ increase in search efficiency. \n   With just 5 NFE, we achieve FID scores of 3.23 on CIFAR10, 7.16 on ImageNet-64, \n   5.44 on LSUN-Bedroom, and 15.69 on MS-COCO, resulting in a 2$\\times$ sampling acceleration ratio \n   compared to handcrafted solvers.",
    "keywords": [
        "Diffusion Probabilistic Model",
        "Diffusion Sampler",
        "Solver Schedule"
    ],
    "primary_area": "generative models",
    "TLDR": "We provide a solver distillation  framework for diffusion models and search for solver schedules based on it.",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vkOFOUDLTn",
    "pdf_link": "https://openreview.net/pdf?id=vkOFOUDLTn",
    "comments": [
        {},
        {
            "title": {
                "value": "Response to Reviewer AsEn"
            },
            "comment": {
                "value": "Dear reviewer AsEn, \n\nThank you very much for your review. Here are our responses:\n\nQ1: Does a different designer network needs to be learned for each choice of NFE, multistep order?\n\nYes, the designer network is trained individually for each NFEs. A related question arises: could we attempt training a universal designer network? While this is clearly feasible, it also implies more complex inputs and a multiplied parameter count. Repeatedly invoking a more complex designer network during sampling would directly raise the sampling costs. Therefore, it seems more reasonable to opt for multiple independent designer networks. Methods like USF and AMED also incorporate additional networks, all of which use separate networks.\n\nQ2: Is the designer network is somehow constrained such that always $t_N=0$ ?\n \nYes, when the step_index is $N-1$, we constrain the network to directly output $t_N=0$ (which is actually a positive value close to 0). We are willing to share further details about the designer network. We also enforce that the output $t_n$ must be less than the input $t_{n-1}$, which is essential in some experiments. Moreover, for positive outputs such as time step $t_n$ and time scale $s_n$, we actually apply exponential transformations.\n\nQ3 Could the authors provide the size of the designer network?\n\nIn line 372, we highlighted that the designer network comprises a two-layer MLP with a total parameter count of only 9k. This is negligible compared to the parameter count of diffusion models (e.g., SDv1.5 with a parameter count of 680M). Perhaps we should mention this earlier in our article. Thank you for your suggestion.\n\nQ4: The designer network is dependent only on $h_{t_{n-1}}$ or previous times as well?\n\nDue to the necessity for the designer network to be very lightweight, its input needs to have a low dimensionality. Therefore, we chose to extract features from the bottleneck layer of the U-Net and then compress them into a vector of only 64 dimensions through average pooling. For the Text-to-Image model, the bottleneck features encompass image, text, and time information, ensuring that the input to the designer network is concise yet informative. We only utilize the most relevant $h_{t_{n-1}}$ and discard previous features to simplify the input and save on space costs.\n\nW2: The method is not compared to any other solver distillation methods such as DPM-Solver-v3, Bespoke Solver.\n\nIn Tables 1 and 2, our DLMS method demonstrates significant superiority in performance and training costs compared to DPM-Solver-v3. DPM-Solver-v3 performs impressively in scenarios like Latent Diffusion, which are insensitive to time steps. In fact, we could use DPM-Solver-v3 to initialize distillation training, but we prefer our method not to rely on a similar approach.\n\nThe Bespoke Solver bears a strong resemblance to our approach. However, it has not been tested in broader scenarios such as guided diffusion and latent diffusion. The experimental results on models trained by themselves lack comparability. In the publicly available model EDM  trained on CIFAR10, our method achieved an FID of 2.53 with 7 NFEs, outperforming their result of 2.75 with 20 NFEs. We are still more than willing to cite this highly relevant article.\n\nW3: Discussion and comparison to model distillation is too minimal. \n\nWe agree that a more detailed introduction to model distillation is necessary, and we intended to add relevant content."
            }
        },
        {
            "summary": {
                "value": "This paper proposes a Distilled Linear Multistep Solver (DLMS) to learn a faster sampler for diffusion models, requiring fewer function evaluations. The distillation approach is to train a solver that minimizes the Euclidean distance between its trajectory and a teacher solver\u2019s trajectory. DMLS can be trained faster than previous reinforcement learning-based approaches to solver distillation. Experimental results in image generation using unconditional, conditional, latent space, and pixel space diffusion show improved FID scores compared to existing methods, especially in low NFE settings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The strengths of the paper are its well-motivated method and a wide range of experiments.\n- The approach can be initialized using existing solvers.\n- Unlike model distillation, solver distillation like DLMS can be used for downstream tasks like image restoration.\n- The proposed method is simpler than using a reinforcement learning-based approach.\n- The method is evaluated in multiple contexts (unconditional, conditional, latent space, and pixel space diffusion) and on multiple datasets with convincing results.\n- The paper is overall well-written."
            },
            "weaknesses": {
                "value": "The weaknesses of the paper include its figure and table captions and reliance on FID as the sole quantitative metric.\n- In general, the figures and table are not self-contained. Figure 4, for example, could be made more interpretable by describing the significance of dashed lines.\n- FID scores are the only quantitative metric used to evaluate the method. Quantifying the quality of generated images is challenging, so adding multiple metrics like IS or CMMD [1] would increase confidence in the results. \n- In the introduction (line 047) it is argued that distillation is expensive, requiring multiple GPU days of training. The reported training times for DLMS are still more than ten hours, so these methods could still be compared, if not to better understand their respective strengths and weaknesses.\n- The time comparisons in Table 2 are hard to draw conclusions from considering the reported times are compared to those from previous papers that were run on different systems with different GPUs and software environments. It is stated that this is due to limited code availability. If they are possible to obtain, FLOP counts (or an estimate of them) would be more comparable.\n\n**References**\n\n[1] Jayasumana, S., Ramalingam, S., Veit, A., Glasner, D., Chakrabarti, A. and Kumar, S. \u201cRethinking FID: Towards a Better Evaluation Metric for Image Generation\u201d, CVPR 2024"
            },
            "questions": {
                "value": "- On line 398 it is reported that the distillation time of DMLS is approximately 1.5 * 8 = 12 hours for stable diffusion, while the abstract claims that the framework \u201chas the ability to complete a solver search for Stable-Diffusion in less than 10 total GPU hours\u201d. Is this difference due to a rounding error or do these claims refer to different times? Clarify this discrepancy and ensure consistency between the abstract and results.\n\n**Minor suggestions that do not individually affect the score**\n- Line 129: \u201ccan be carry out\u201d -> \u201ccan be carried out\u201d.\n- Line 189: Remove \u201cprecious\u201d.\n- Line 263: Introduce the strop gradient operation.\n- Line 284: Reformulate.\n- Line 292: \u201cPLMS(iPNDM)\u201d -> \u201cPLMS (iPNDM)\u201d.\n- Line 340: \u201cAMED-Plugin(Zhou et al., 2024)\u201d -> \u201cAMED-Plugin (Zhou et al., 2024)\u201d.\n- Line 363: Specify \u201cvarious aspects\u201d and \u201cas well as the ablation\u2026\u201d -> \u201cas well as ablation\u2026\u201d.\n- Line 394: \u201cMS-COCO(2014)\u201d -> \u201cMS-COCO (2014)\u201d.\n- Line 485: \u201cHandcrafted(best)\u201d -> \u201cHandcrafted (best)\u201d."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a method for learning a trajectory-specific solver for diffusion models. The method suggest to use a small network to predict the best time step size and coefficients of a linear multistep at each step of the solver. The method is tested on a number of image generation tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The idea to learn a trajectory-specific solver is novel and interesting.\n2. The method shows good results on number of benchmark datasets.\n3. The method is compared to a number of diffusion dedicated solvers."
            },
            "weaknesses": {
                "value": "1. The authors make use of what they call \"bottleneck feature\" without any explanation what are those features, only referencing the relevant paper. The paper should be self contained and the authors should make an effort to give even brief explanation about these features.\n2. The method is not compared to any other solver distillation methods such as [1], [2].\n3. Discussion and comparison to model distillation is too minimal.\n4. The size of the designer network is not provided."
            },
            "questions": {
                "value": "1. Does a different designer network needs to be learned for each choice of NFE, multistep order?\n2. Is the designer network is somehow constrained such that always $t_N=0$?\n3. Could the authors provide the size of the designer network?\n4. The designer network is dependent only on $h_{t_{n-1}}$ or previous times as well?\n\n\n[1] Zheng, Kaiwen, et al. \"Dpm-solver-v3: Improved diffusion ode solver with empirical model statistics.\" Advances in Neural Information Processing Systems 36 (2023): 55502-55542.\n\n[2] Shaul, Neta, et al. \"Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models.\" arXiv preprint arXiv:2403.01329 (2024)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes DLMS, a flexible solver framework that incorporates the combination of previous model outputs, timestep schedule and timestep scaling factor. The authors further introduce a light weight designer network to dynamically decide the solver strategies for each single trajectory. Experimental results demonstrate DLMS achieves notable improvements over existing solvers and offers faster optimization than search-based methods."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. DLMS offers a flexible framework for diffusion solver, unifying existing methods.\n2. DLMS uses dynamic solving strategies for different ODE trajectories, enhacing the potential for diffusion solver.\n3. Experimental results demonstrate significant performance improvements compared to existing solvers, and the designer network's training cost is more efficient than that of search-based solvers."
            },
            "weaknesses": {
                "value": "1. Algorithm 1 implies that each NFE configuration may require independent designer networks, which could limit flexibility in the NFE-performance trade-off. If this is not the case, how does the designer network ensure that $t_N$ becomes a reasonably small when the step count reaches N?\n2. The time scaling factor may introduce input distribution misalignment, so further discussion on the motivation and explanation of this would be beneficial.\n3. The designer network currently relies on U-Net intermediate feature. As transformers gain popularity in diffusion models, it is uncertain if this approach is adaptable to such architectures.\n4. It would be helpful to illustrate differences in solver design choices provided by the designer network across various ODE trajectories to support the claim that a unifed choice for all trajectories is suboptimal."
            },
            "questions": {
                "value": "1. What is the relationship between the coefficients predicted by the designer network and those derived from Taylor expansion in previous methods? Could you provide a comparison of these coefficients with those from previous methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper considers diffusion distillation for  a student linear multistep solver from a teach linear multistep solver with more timesteps.  In particular,  the authors propose to train a lightweight neural network to predict the coefficients, time step schedules, and time scaling factors of the student linear multistep solver. The cost function when training the lightweight neural network is taken as the mean squared distance of the difference of diffusion states produced by the student and the teacher linear multipstep solver, respectively. Experiments on FID shows the effectiveness of the new method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The main strength of the paper is that the authors propose to train a lightweight neural network that produce not only the coefficients of the student linear multistep solver but also the timesteps and the scaling factors.  This is based on the assumption that for different ODE trajectories, the optimal coefficients, timesteps and scaling factors are different."
            },
            "weaknesses": {
                "value": "(0) One weakness is that a small neural network is required to be trained for each particular pre-trained model. Note that not every university or research institute has 8 A00 or H100 GPUs for conducting the training process.  \n\n(1) The literature is not thorough. This work is closely related to a recent paper [1], which is not mentioned at all. The work of [1] considers computing the optimal coefficients of a student linear multistep solver per timestep by solving a quadratic optimization problem. The computational complexity of [1] is negligible as the quadratic optimization problem takes a closed-form solution. The authors should include the performance of [1] in their work. \n\n(2) One thing that is not clear to me is results for the two experiments of Latent-Diffusion on LSUN-Bedroom and Stable-Diffusion on MS-COCO prompts, where the number of interpolation timesteps M=1. I would think that the teacher ODE solver with two times of the number of times perform betters than the student ODE solver.  Is it the case? If not, explain why. \n\n(3) I would think that in general, the higher the M value, the better FID score of the student ODE solver. So why in different experimental setups, M were chosen differently? Would higher M value in some cases lead to poor performance? If so, explain why and include the results in the revision. \n\n(4) Typo:  \"can be carry out\"\n\n[1] Guoqiang Zhang, Kenta Niwa, W. Bastiaan Kleijn, \"On Accelerating Diffusion-Based Sampling Processes via Improved Integration Approximation, ICLR, 2024."
            },
            "questions": {
                "value": "please check my responses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n.a."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}