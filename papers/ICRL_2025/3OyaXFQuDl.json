{
    "id": "3OyaXFQuDl",
    "title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling",
    "abstract": "Training on high-quality synthetic data from strong language models (LMs) is a common strategy to improve the reasoning performance of LMs. In this work, we revisit whether this strategy is compute-optimal under a fixed inference budget (e.g., FLOPs). To do so, we investigate the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model. We evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from WC models may have higher coverage and diversity, but also exhibit higher false positive rates. We then finetune LMs on data from SE and WC models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM. Our findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks and multiple choices of WC and SE models. These results challenge the prevailing practice of relying on SE models for synthetic data generation, suggesting that WC may be the compute-optimal approach for training advanced LM reasoners.",
    "keywords": [
        "large and small language models",
        "reasoning",
        "math",
        "compute-optimal",
        "sampling",
        "supervised finetuning"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=3OyaXFQuDl",
    "pdf_link": "https://openreview.net/pdf?id=3OyaXFQuDl",
    "comments": [
        {
            "summary": {
                "value": "This paper challenges the common practice of using strong but expensive (SE) language models to generate synthetic training data, proposing instead that using weaker but cheaper (WC) models may be more compute-optimal. The authors introduce a \"compute-matched sampling\" framework that enables fair comparison between WC and SE models by accounting for their relative compute costs. At a fixed compute budget, this framework shows that one can generate P_SE/P_WC more samples from a WC model than an SE model. The authors evaluate this approach across multiple model pairs (Gemma2 9B/27B and Gemini Flash/Pro), tasks (primarily mathematical reasoning), and training paradigms (knowledge distillation, self-improvement, and a novel \"weak-to-strong improvement\"). They assess the generated data along three key dimensions: coverage (problems solved), diversity (unique solutions per problem), and false positive rate (correct answers with incorrect reasoning). The results consistently show that training with WC-generated data outperforms SE-generated data when properly compute-matched."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. _Originality_:\n   - Introduces a novel compute-matched sampling framework with clear mathematical foundations\n   - Proposes a new \"weak-to-strong improvement\" training paradigm that challenges conventional wisdom\n   - Provides a fresh perspective on the compute-quality trade-off in synthetic data generation\n\n2. _Experimental Rigour_:\n   - Comprehensive evaluation across multiple dimensions:\n     * Multiple model pairs (both open and closed models)\n     * Various compute budgets and training paradigms\n     * Different dataset sizes and difficulty levels\n   - Thorough ablation studies that isolate the impact of coverage and diversity\n   - Both human and automated evaluation of false positive rates\n   - Clear validation of results through transfer learning (Functional MATH)\n\n3. _Practical Impact_:\n   - Demonstrates significant cost savings potential (0.15x cost for comparable or better performance)\n   - Shows consistent improvements across model sizes (7B to 27B)\n   - Provides actionable insights for practitioners\n   - Results particularly relevant given the trend of improving smaller models\n\n4. _Technical Depth_:\n   - Rigorous mathematical formulation of compute-matching\n   - Analysis of traed-offs between coverage, diversity, and error rates\n   - Ablation studies support main claims\n   - Clear empirical validation of theoretical framework"
            },
            "weaknesses": {
                "value": "1. _Theoretical Foundation_:\n   - Lacks formal analysis of when WC sampling should outperform SE sampling\n   - No theoretical bounds on the optimal sampling ratio\n   - Missing analysis of the relationship between model size and optimal sampling strategy\n   - Limited exploration of failure modes and their characteristics\n\n2. _Methodology Limitations_:\n   - Heavy reliance on ground truth for filtering solutions\n   - Limited exploration of alternative filtering strategies\n   - FPR evaluation methodology could be more robust (50 human samples probably insufficient)\n   - Some key implementation details relegated to appendices\n\n3. _Generalisation Concerns_:\n   - Primary focus on mathematical reasoning tasks\n   - Limited exploration of other domains (coding results show context-dependency)\n   - Unclear scalability to larger model sizes\n   - Performance on more complex reasoning tasks not fully explored\n\n4. _Practical Considerations_:\n   - Deployment challenges in scenarios without ground truth not fully addressed\n   - Resource optimisation strategies could be explored more\n   - Limited discussion of integration with existing training pipelines\n   - Cost-benefit analysis could be more comprehensive across different scenarios"
            },
            "questions": {
                "value": "I will try and cluster my questions in sensible groups.\n\n1. _Theoretical Understanding_:\n   - Can you provide theoretical insights into when WC sampling should outperform SE sampling?\n   - How does the optimal sampling ratio change with model size and task complexity?\n   - What are the key factors that determine the success of weak-to-strong improvement?\n\n2. _Methodology_:\n   - How would the results change with more sophisticated filtering strategies?\n   - Could you provide more details about the specific prompting strategies used?\n   - How sensitive are the results to the choice of temperature and sampling parameters?\n\n3. _Generalisation_:\n   - What characteristics of a task make it more/less suitable for WC sampling?\n   - How would the results scale to even larger model sizes?\n   - What is the relationship between FPR and final model performance?\n\n4. _Practical Implementation_:\n   - How would you recommend implementing this in scenarios without ground truth?\n   - What modifications would be needed for different domains or tasks?\n   - Could you provide more detailed guidance on optimal sampling strategies for different scenarios?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper revisits the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model, and finds that at a fixed sampling compute budget, finetuning LMs with data from a WC model can consistently outperform data from a SE model in multiple settings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The research question is significant, focusing on performance comparison of data sampled from WC and SE models, respectively.\n2. The findings are impressive, challenging the traditional belief that data from a strong model is better for finetuning models.\n3. The evaluation settings are diverse, demonstrating the effectiveness and robustness of this method, despite only the Gemma series models."
            },
            "weaknesses": {
                "value": "1. This paper centers exclusively on the Gemma series, and it is essential to extend the analysis to the Llama series to demonstrate the robustness of the conclusions.\n2. While the paper aims to highlight the lower computational cost of the WC model for data synthesis (particularly important for large-scale data generation), all the experiments are conducted on relatively small datasets. This discrepancy undermines the overall contribution of the paper.\n3. Compared to the SE model, the WC model can be regarded as a more diverse yet lower-quality variant. Therefore, it is crucial to compare it with techniques designed to enhance output diversity. Specifically, if adjusting the sampling temperature of the SE model consistently results in performance degradation relative to the WC model, this suggests that the WC model provides a superior quality-diversity trade-off compared to merely increasing the sampling temperature."
            },
            "questions": {
                "value": "1. Although both low and high budgets are studied, could you please provide the results of an extremely high budget where the cost is not an important factor? This should be indicative of diverse data scales.\n2. Despite the train-test splits of MBPP, this paper only trains models on MBPP and tests them on HumanEval. The testing results on MBPP are expected to be provided for a more comprehensive understanding.\n3. Writing\uff1a\n- 1. All the ref links are invalid.\n  2. l101, l104: \"i.e.\" should be \"i.e.\".\n  3. l103: grammar error for \"we supervise finetune\".\n  4. l109: use \\citet{} for \"(Zelikman et al., 2024;Singh et al., 2023).\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents the novel observation that generating synthetic data using a weaker but cheaper (WC) model is more effective than using a stronger but more expensive (SE) model. The authors demonstrate that, under the same budget, data generated by WC models tend to have higher coverage and diversity, though with a corresponding increase in false positive rates. Additionally, they show that models fine-tuned on WC-generated data consistently outperform those trained on SE-generated data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-structured and clearly written, making the methodology and results easy to follow.\n\nThe experiments are well-executed and provide convincing evidence of the benefits of the proposed approach.\n\nIt addresses a critical issue in synthetic data generation, offering a valuable contribution to this area of research."
            },
            "weaknesses": {
                "value": "The conclusion may not hold when using models from different companies. Based on my experience, under the same budget, data generated by a larger model like Qwen2.5 7B could outperform that of a smaller one like Gemma2 2B.\n\nThe paper could benefit from experimenting with more complex reasoning tasks, such as tree search algorithms, and using a reward model to evaluate the quality of the generated data."
            },
            "questions": {
                "value": "It seems that the difference in data quality between the WC and SE models becomes larger at lower budgets. Is it possible that the WC and SE models generate data of similar quality when the budget is very high?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates whether it is better to (self-)distill from a Gemma-27B LLM or to distill three times more finetuning data from a three times smaller Gemma-9B model. It finds that the three times more data of the smaller model, despite including more errors, leads to a higher performance of the finetuned student model."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper investigates not only knowledge distillation, but also a self-improvement setup\n* In Figures 4 and 5, it is interesting that training on model-generated solution paths (one per question; at least in the \"27B low compute\" setup) gives a better performance than training on human-provided solution paths (also one per question)\n* Figure 7 carries an interesting finding: Despite training on the data generated by the small model which has more errors, the ultimate trained model does not have more errors in its reasoning. This implies that the additional data mitigates its lower quality, which might add evidence to the discourse beyond the setup studied in this paper.\n* The writing and flow of experiments is mostly clear"
            },
            "weaknesses": {
                "value": "* In 21 of the 22 Figures, the paper hinges on \"matching the compute\", i.e., being allowed to generate 3 times more data when using the 3 times smaller LM. This confounds two factors of variation, making it hard to interpret the findings. This is the main weakness of the paper. One idea to improve on this weakness would be to test out distilling 1, 3, and 9 samples from the large LLM and 3, 9, and 27 samples from the small LLM (instead of the current 1+10 vs 3+30), so that there are both overlapping settings with a matched number of samples and with a matched compute.\n* In the only figure where the small LLM is compared to the large LLM without this advantage (Figure 20 in the appendix), the large LLM produces better training data. It can be expected that if we use the large LLM to generate enough data until the student model converges, it will make a better distilled model. Thus, the only real application of the proposed method is when we do not have enough budget to produce enough data to converge. For the finetuning setup of the paper, that would amount to not being able to generate data for 8k-12.5k questions. This is a setup with limited applicability in practice. It would increase the contribution (score) of the paper to investigate problems where budget limits are hit more frequently in practice, like pretraining, see also my question below.\n* Relative and absolute increases are reported inconsistently. E.g., in Figure 3b the fact that the proposed small model finds 11 instead of 5 solution paths per question (when it is allowed to generate 3 times more paths in total) is reported as a 125% increase (line 268), whereas the fact that 24% of its solutions paths are wrong compared to 17% of the large model is reported as a 7% increase (line 310). This inconsistency becomes problematic when reporting the increase on percentage numbers (e.g., line 258), where it is unclear whether this is a relative or absolute increase. Keeping the reporting consistent would increase both the presentation and the scientific soundness scores.\n* The paper only evaluates Gemma (/Gemini) models. It would help judge the generalization of the claims (and increase the contribution score) to test it out on at least one other LLM, like a Llama model.\n* The datasets are very limited to two math datasets, limiting the contribution. As above, more datasets would help judge the range of applicability, especially whether it also works on non-math and non-reasoning datasets.\n* The paper does not compare to baselines, despite citing multiple closely related approaches\n* The method still requires annotated data, because the LLM-generated data needs to be filtered out if it does not match the GT. It would increase the applicability of the score (and thus the contribution score) if there would be an ablation without filtering, i.e., answering whether the unfiltered erroneous data from the smaller model can still train a better model.\n\nSmall notes that did not influence my score and don't need to be rebuttled, I just note them to make the camera-ready better:\n* The first paragraph of Section 3 could be shortened; it's message (in Equation 1) is just \"if a model has x times more parameters, it takes x times longer to generate\".\n* typo in line 54, \"filters\"\n* typo in line 103 \"we supervise finetune\"\n* typo in line 151, \"consists\"\n* typo in line 157, \"for training student LM\"\n* typo in line 241, \"that where\"\n* The references exclusively list arxiv versions of the papers, not their actual published versions\n* The reference .bib file should best use double brackets for \"{{OpenAI}}\", \"{{The Llama Team}}\", to prevent the ill formatting in line 483 (\"Team, 2024; Anthropic, 2024; AI, 2024\")"
            },
            "questions": {
                "value": "* Your distillation setup is limited to finetuning. One setup where it would be more realistic to not have enough budget is pretraining. Do you have any results on this? I of course do not expect to pretrain a network until convergence during the rebuttal, but it would already be helpful if you could show the first couple of iterations just to make sure the worse data (higher FPR) does not seem to converge to a much worse model.\n* I'd be interested in sample-matched figures. The figures where I'd be most interested in a sample-matched comparison are Figures 4c and 5c. This would allow finding out if a small model can successfully improve a larger model, which would challenge beliefs in the field.\n* Just to go sure: In the self-improvement setups, you keep training a model iteratively on its own generations from the current parameters? Or do you mean that you finetune a \"fresh\" 7B model using an already converged 7B model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}