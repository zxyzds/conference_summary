{
    "id": "Hj1D0Xq3Ef",
    "title": "Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning",
    "abstract": "Large Language Models (LLMs) are trained on extensive datasets that often contain sensitive, human-generated information, raising significant concerns about privacy breaches. While certified unlearning approaches offer strong privacy guarantees, they rely on restrictive model assumptions that are not applicable to LLMs. As a result, various unlearning heuristics have been proposed, with privacy risks typically assessed empirically. The standard evaluation pipelines usually randomly select data for removal from the training set, apply unlearning techniques, and use membership inference attacks (MIAs) to compare the unlearned models against models retrained without the removed data. In this paper, we identify a critical flaw in this widely adopted evaluation approach: the privacy risks faced by minority groups within the training data are often significantly underestimated. We substantiate this claim through carefully designed experiments, including unlearning canaries related to minority groups, inspired by privacy auditing literature. Using personally identifiable information (PII) as a representative minority identifier, we demonstrate that minority groups experience at least 20\\% more privacy leakage in most cases across combinations of six unlearning approaches, three variants of MIAs, three benchmark datasets, and two LLMs of different scales. Given that the right to be forgotten should be upheld for every individual, we advocate for a more rigorous evaluation of LLM unlearning methods. Our minority-aware evaluation framework represents an initial step toward ensuring more equitable and thorough assessments of LLM unlearning efficacy.",
    "keywords": [
        "Machine Unlearning",
        "Large Language Models",
        "Membership Inference Attack",
        "Privacy Risk",
        "Minority Groups"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Hj1D0Xq3Ef",
    "pdf_link": "https://openreview.net/pdf?id=Hj1D0Xq3Ef",
    "comments": [
        {
            "summary": {
                "value": "The common method of evaluating the privacy leakage of unlearning in large language models is based on a chosen dataset $D_{forget}$ to unlearn from that model, say, $M^{(1)}$. The paper argues that a current method of using a random $D_{forget}$ significantly underestimates the real privacy leakage of the unlearned model. To do this, the paper proposes the method of analyzing the privacy leak of a synthetic dataset $D_{canary}$ created by replacing personally identifying information from $D_{forget}$ with the most unlikely personally identifying information from the whole training set $D_{train} = D_{keep} \\cup D_{forget}$. The paper compares the privacy leakage of the $M^{(1)}$ and $M^{(2)}$ that were trained on $D_{forget} \\cup D_{keep}$ and $D_{canary} \\cup D_{keep}$ respectively and then un-trained on $D_{forget}$ and $D_{canary}$ respectively. Differences in audits in the unlearned models and a model retrained on $D_{keep}$ from scratch shows privacy leakage. Since a synthetic dataset might not represent real world scenarios, the paper does a similar analysis where $D_{forget}$ is taken from the most unlikely data in $D_{train}$."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The paper highlights some weakness in current privacy evaluations for algorithm-agnostic unlearning techniques in LLMs and provides methods for improving privacy audits for these LLMs.\n\n* The paper covers multiple unlearning algorithms and multiple membership inference attacks.\n  \n* The paper overall is clearly written, for example it has clear explanations of the background such as model unlearning.  Figure 1 is a clear overview of the pipeline proposed in the paper."
            },
            "weaknesses": {
                "value": "* The paper seeks to explain that their evaluations of MIAs based on how *Canaries* and *Minorities* gives higher *PrivLeak* scores compared to *Random*. However, there is no justification for why the *PrivLeak* score is used to begin with. This seems important particularly given that the subsequent analysis in the paper hinges on this choice.\n  \n* The paper analyses the privacy-utility trade-off of models after unlearning by comparing the *PrivLeak* score and LLM perplexity. The analysis could benefit from other utility measures (for example it doesn't capture semantic meaning, and LLMs could be very confident about an incorrect prediction)."
            },
            "questions": {
                "value": "* Why is it useful to study the *Canaries* set if synthetic data does not reflect real world scenarios?\n  \n* What is the justification for 10 unlearning units? How does the large underestimation in PL compare to *random* under different unlearning budgets? \n  * For an LLM would it be practical to allow for longer unlearning times especially considering the retraining cost is much larger in comparison?\n* The paper highlights limitations in the typical evaluation pipeline (*random*). Are there limitations in this new pipeline?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper conducts a benchmark evaluation of different machine unlearning approaches, and computes membership inference metrics on the unlearned models.  They find that prior work underestimates the privacy risk on minority groups by about 20% in the settings they studied.  Their findings are fairly robust across a variety of different unlearning methods, MIA metrics, base models, datasets, and other experimental settings.   Based on this finding, the authors advocate for a minority-aware evaluation, and discuss which methods perform best under this evaluation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Paper introduces a solid benchmark and a carefully crafted set of experiments.\n* This main findings are interesting, and could help shape future research in this space."
            },
            "weaknesses": {
                "value": "* The main finding that MIA attacks have different success rates on different subsets of the population is not surprising, and is in line with other work in the privacy + fairness space.  \n* A good number of experiments were done, but only two main findings came out from them: (1) that MIA metrics differ based on { Random, Canary /Minority } and (2) that  Langevin Unlearning provided the best balance between privacy and utility.  Are there more findings or observations you can make based on the data you collected?  You should target at least one key takeaway for each Figure/Table you have."
            },
            "questions": {
                "value": "* What are the limitations of your methodology?\n* Can you offer any insight into why Langevin Unlearning does better under a minority-aware evaluation?\n* Table 4 and 5 are pretty busy, and there is no reference to Table 4 in the text.  If you can help the reader by telling them what they should look at in the table and how to interpret the full results, that would be nice."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the evaluation pipeline of LLM unlearning (i.e., efficient LLM modification so that it becomes statistically indistinguishable from a model retrained from scratch, without the data subject to removal), and identifies a flaw in the existing evaluation approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper highlights an issue with LLM unlearning evaluation, being data dependent. The current evaluation chooses to forget the dataset uniformly at random. \n2. This paper shows that minority groups experience at least 20% more privacy leakage in most cases across combinations of six unlearning approaches.\n3. This paper calls for a more careful LLM unlearning efficacy evaluation."
            },
            "weaknesses": {
                "value": "Given the \"right to be forgotten\" and too expensive cost of re-training LLMs from scratch without the data subject to removal, machine unlearning techniques have been proposed.\n\n\nAs there is no formal unlearning guarantee for deep neural networks and LLMs, evaluation of LLM unlearning uses membership inference attacks as follows:\n- Randomly select data for removal from the training set to create a \"forget\" dataset,\n- Apply unlearning techniques to the LLM\n- Use membership inference attacks to compare the unlearned models against models retrained without the removed data.\n\nThis paper identified a flaw in this evaluation pipeline (mainly in the first step): \"the unlearning privacy risk of minority populations within the training set is severely underestimated since the minority data are less likely to be selected in the unlearning evaluation pipeline.\"\n \n\nThis paper proposes the need of using worst-case data for creating forget dataset in the evaluation pipeline. However, the suggested choice of worst-case scenario suffers from multiple issues:\n\n1. This paper assumes that the worst case data corresponds to minority data. \n2. This paper defines minority data based on only personally identifiable information.\n3. This paper creates canaries by replacing personally identifiable information in randomly chosen data within forget dataset with least frequent ones. \n\n\n\nThis paper proposes a minority-aware LLM unlearning evaluation protocol to address the limitation of existing evaluation pipelines. However, the proposal depends heavily on the above choices: worst-case data, minority definition and canary creations. In addition, the broader motivation is unclear- for example if the data subject to removal does not blog to least frequent data, it is not clear what the applicability of the proposal would be.\n\n\nThe main finding of this paper (privacy risks of minority groups in the training data are usually underestimated) seems related to several existing works studying disparate vulnerability against MIAs.\n\n\nMinors:\n- All text in lines 197-215 + a table + a figure aim to say that frequencies of items vary in a dataset, or I am missing something non-trivial?\n\n- \"In the provided example,  Dforget unlikely contains the least frequent email with the 484 area code. As a result, the corresponding unlearning privacy evaluation may underestimate the privacy risk of minorities if unlearning minority data is inherently more challenging.\"\" --> Not clear how the latter got concluded just because of being least frequent? \n\n- bold claims without supports: \n\t- 237-238: \"albeit a similar idea extends beyond PIIs\"\n\t- 139-140: \"albeit our methodology extends to other cases whenever the indistinguishability to the retrained model is an appropriate metric\"\n\n\n\n\n- 93-94: \"the right to be forgotten should be respected for all individuals\" --> how to define individual? each individual has one record or multiple records? This is not discussed later on in the paper. \n\n- 40-41: missing refs for GDPR and right to be forgotten \n\n- 262: \"it cannot quantify underestimate the privacy risk for minorities in the real-world setting\" --> grammar issue\n\n\n- 377: \"Enron(Klimt & Yang, 2004) and ECHR(Chalkidis et al., 2019).\" --> missing space"
            },
            "questions": {
                "value": "1. Why minority data should be considered as worst-case data for LLM unlearning evaluations?\n2. Why minority data should be defined based on personally identifiable information? How generalisable the results/findings are when using alternative minority definitions? \n3. Why is the suggested way of creating canaries relevant?\n4. Why do the chosen minorities suffer from at least 20% more privacy leakage in studied cases? Is it due to being least frequent or their context/semantics?\n5. How are these results compared to those works analyzing disparate vulnerability against MIAs such as Kulynych et. al., Disparate Vulnerability to Membership Inference A\u001dacks (https://arxiv.org/pdf/1906.00389)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}