{
    "id": "izETL3emSv",
    "title": "BBOPlace-Bench: Benchmarking Black-Box Optimization for Chip Placement",
    "abstract": "Chip placement is a crucial step in modern chip design, because it significantly impacts the subsequent process and the overall quality of the final chip. The application of black-box optimization (BBO) for chip placement has a history of several decades. Nevertheless, early attempts were hampered by immature problem modeling and inefficient algorithm design, resulting in suboptimal placement efficiency and quality compared to the more prevalent analytical methods. Recent advancements in problem modeling and BBO algorithm design have highlighted the effectiveness and efficiency of BBO, demonstrating its potential to achieve state-of-the-art results in chip placement. Despite these advancements, the field lacks a unified benchmark for thoroughly assessing various problem models and BBO algorithms. To address this gap, we propose BBOPlace-Bench, the first benchmark designed for evaluating and developing BBO algorithms specifically for chip placement tasks. BBOPlace-Bench first collects several popular tasks and standardizing their formats, thereby providing uniform and comprehensive information for optimization.  Additionally, BBOPlace-Bench includes a wide range of existing BBO algorithms, including simulated annealing, evolutionary algorithms, evolution strategy, and Bayesian optimization, and evaluates their performance across different problem modelings (i.e., permutation, discrete, and mixed search spaces) using various metrics. Furthermore, BBOPlace-Bench offers a flexible framework that allows users to easily implement and test their unique algorithms. BBOPlace-Bench not only provides efficient solutions for chip placement but also expands the practical application scenarios for various BBO algorithms. The code for BBOPlace-Bench is available in the supplementary file.",
    "keywords": [
        "Black-box optimization",
        "Bayesian optimization",
        "Evolutionary algorithm",
        "Chip placement",
        "EDA"
    ],
    "primary_area": "optimization",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=izETL3emSv",
    "pdf_link": "https://openreview.net/pdf?id=izETL3emSv",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes BBOPlace-Bench, a benchmark for evaluating and developing a black box optimization for chip placement for EDA. BBOPlace-Bench evaluate the chip placement from problem formulation, optimization algorithm and problem evaluation, which provides a convenient tool for EDA community."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThis paper decouples chip placement tasks into problem formulation, optimization algorithms, and problem evaluation.\n2.\tThis paper provides a comprehensive study of BBO for chip placement tasks."
            },
            "weaknesses": {
                "value": "1.\tThis paper compares traditional black-box optimization (BBO) algorithms but lacks coverage of advanced BBO methods that leverage deep learning. Although the authors include some reinforcement learning (RL)-based methods, the omission of ChiPFormer, an advanced BBO method, is notable. Including ChiPFormer would be valuable, as it introduces deep learning-based advancements that are increasingly relevant to this field. Additionally, incorporating other advanced BBO methods using deep learning would provide a more comprehensive benchmark. Specific suggestions could include recent developments in neural architecture search or multi-objective optimization approaches, which could further enrich the paper\u2019s contributions.\n2.\tThe paper presents evaluation results but lacks in-depth analysis of the performance differences across methods. For example, in several cases, RL-based methods perform worse than traditional BBO approaches, yet the reasons for this underperformance are not explored. To provide more insight, it would be helpful if the authors conducted a detailed analysis of these cases. They could investigate factors like differences in problem formulation, hyperparameter sensitivity, or scalability limitations that may impact RL-based methods differently from traditional BBO approaches.\n3.\tBy focusing primarily on traditional BBO algorithms, the paper limits its contribution and does not fully capture the latest advancements in the field. To better represent the current state of BBO, the authors could consider including more recent algorithms or techniques that reflect the field\u2019s evolution. Suggestions might include recent innovations in deep learning-based approaches like neural architecture search or multi-objective optimization techniques relevant to chip placement. Including these would enhance the benchmark\u2019s value by offering a more complete picture of current methodologies and their practical implications in chip design optimization."
            },
            "questions": {
                "value": "Please see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces BBOPlace-Bench, a benchmark for evaluating and developing BBO algorithms in chip placement. BBOPlace-Bench collects several tasks and standardizes their formats. Simulating annealing, evolutionary algorithms, evolution strategy, and Bayesian optimization are included in the benchmark."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This paper presents a benchmark for chip placement that may have potential for industrial EDA applications."
            },
            "weaknesses": {
                "value": "(1)\tChip placement is an industrial problem. It is not clear that whether experiment results on the benchmark are similar to those in the real industrial scenarios. It is mentioned in subsection 4.1 that \u201cWe empirically test methods in BBOPlace-Bench on ISPD 2005 and ICCAD 2015 benchmarks\u201d. On one hand, the reasons for choosing the ISPD 2005 benchmark and ICCAD 2015 benchmarks should be better described. On the other hand, the empirical test may not reflect the real performance in the industrial scenarios. A detailed comparison between the chosen benchmarks and real industrial scenarios is favored. Moreover, consulting with industry practitioners or comparing to recent industrial datasets on the limitations of these benchmarks and the ways to validate the benchmark's relevance to industry is suggested.  \n\n(2)\tThe experiments are simplified due to various reasons, which makes the results less trustable. For example, only SP-SA and SP-EA are tested and GP HPWL is set to 200. Authors should clarify the impacts of their experimental choices more explicitly. For example, if GP HPWL is set to 300 or 400, whether the experimental results will change and why. Another typical example is that why you define the largest 512 cells by area as macros for ICCAD 2015. If the number 512 is changed, whether the experimental results will change as well. If they are not easy to be demonstrated in theory, additional experiments on those parameters are suggested. \n\n(3)\tThere are too many EDA references, which have little relation to do with topics of ICLR. Besides, many of the EDA references were published in the last decade or even earlier. For example, the methods in BBOPlace-Bench are tested on the ISPD 2005 and ICCAD 2015 benchmarks. Too many EDA references make the paper difficult to understand in this conference. And it is a little bit confusing that whether the problems this paper addresses are important in the current era. Clarifying the connection between these EDA references and current machine learning research relevant to ICLR is suggested. Providing more context on how these older benchmarks and references relate to current challenges in applying machine learning to chip design is also favored.\n\n(4)\tThe paper has many typos or grammar issues. Even in the abstract, at least one grammar issue can be found. For example, in the abstract, \u201cstandardizes\u201d instead of \u201cstandardizing\u201d should be used in the sentence \u201cBBOPlace-Bench first collects several popular tasks and standardizing their formats, \u2026\u201d. Another typical example is in subsection 4.1, where the word \u201ccontinuous\u201d is an adjective and cannot be used as verb in the sentence \u201cwe continuous the search space of HPO in our experiments\u201d. The writing of this paper should be largely improved."
            },
            "questions": {
                "value": "1, The chip placement is an important process in industrial chip design. To what point, is the BBOPlace-Bench similar with the practical industrial scenarios?\n\n2, It seems that the benchmark of HPO formulation in ICCAD 2015 has at least 18 cases. But in this paper, only 8 of them are tested and compared. Can you show the complete experimental results on the benchmark of HPO formulation in ICCAD 2015?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper discussed the background and recent development on chip placement. A number of placement benchmarks from ICCAD community are evaluated with a number of popular algorithms. A few future directions in this area are also briefly mentioned as part of the conclusion."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "For those interested in chip placement, this paper could be a convenient place to see the performance of different algorithms directly compared using (presumably) popular benchmarks."
            },
            "weaknesses": {
                "value": "The comparison is billed as proposing a benchmark. This is confusing as the benchmarks are really existing benchmarks ISPD 2005 and ICCAD 2015. Beyond presenting the background and comparison results, the paper offers very little insight."
            },
            "questions": {
                "value": "Can you more explicitly explain how the proposed BBOPlace-Bench differs from or adds value beyond simply using the existing ISPD 2005 and ICCAD 2015 benchmarks? What specific contributions or modifications does BBOPlace-Bench make to create a new benchmark framework?\n\nTo those in the subfield of optimizing the placement of chips, what are some lessons from this paper that they didn't know. To those outside that specific subfield, but are interested in optimization, what is the number 1 lesson they take away from the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a benchmark for a black box optimization algorithm with an application scenario of ASIC placement. The execution process of the benchmark involves macro placement first, followed by standard cell placement using DREAMPlace as the backbone. The solution space for the black box optimization in the benchmark includes sequence pairs (specifying the relative order of macros), grid-guide (optimizing the coordinates of macros), and hyperparameter optimization of the DREAMPlace backbone."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is very well-written.\n- The writing and illustrations are clear and help in understanding the motivation and solution."
            },
            "weaknesses": {
                "value": "- There is no comparison with analytical-based macro placement algorithms. In recent years, academia has proposed analytical-based macro placement algorithms. Additionally, there are algorithms for mixed-size placement that perform macro and standard cell placement simultaneously. Compared to RL-based methods, these algorithms are faster. The authors should include a comparison with analytical macro placement algorithms regarding performance and HPWL. This includes, but is not limited to:\n  - Yuan Pu, Tinghuan Chen, Zhuolun He, Chen Bai, Haisheng Zheng, Yibo Lin, and Bei Yu. 2024. IncreMacro: Incremental Macro Placement Refinement. In Proceedings of the 2024 International Symposium on Physical Design (ISPD '24). Association for Computing Machinery, New York, NY, USA, 169\u2013176.\n  - Peiyu Liao, Dawei Guo, Zizheng Guo, Siting Liu, Yibo Lin, and Bei Yu. Dreamplace 4.0: Timing-driven placement with momentum-based net weighting and lagrangian-based refinement. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 42(10):3374\u20133387, 2023.\n  - Y. Chen, Z. Wen, Y. Liang and Y. Lin, \"Stronger Mixed-Size Placement Backbone Considering Second-Order Information,\" *2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)*, San Francisco, CA, USA, 2023, pp. 1-9."
            },
            "questions": {
                "value": "- How is macro placement performed? Can the result of macro placement be obtained once sequence pairs and grid-guide are given?\n- What is the size of the discrete solution space for SP and GG? Please provide an estimate using big O notation.\n- In Section 4.1, Lines 368-370, why is it necessary to designate the largest cell as the macro? Can it be left unspecified? What impact would that have if it isn't designated?\n- Can you provide references or explain how \"Macro Placement HPWL\" is calculated in the text?\n- Why do different algorithms have different starting points in Figure 2? In terms of experimental settings, can different problem formulations like (SP and GG) have the same or as close as possible starting points? Currently, there is a tenfold difference in the starting positions in the figure."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}