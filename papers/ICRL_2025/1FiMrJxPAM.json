{
    "id": "1FiMrJxPAM",
    "title": "A Super-Aligned Driving Generalist Is Your Cockpit",
    "abstract": "The intelligent driving cockpit, an important part of intelligent driving, needs to match different users' comfort, interaction, and safety needs. This paper aims to build a \\textbf{s}uper-\\textbf{a}ligned and \\textbf{ge}neralist \\textbf{dr}iving agent, \\textbf{sage deer}. Sage Deer achieves two highlights: (1) Super alignment: It achieves different reactions according to different people's preferences and biases. (2) Generalist: It can understand the user's physiological indicators, facial emotions, hand movements, body movements, driving scenarios, and behavioral decisions. (3) Multimodal: He can understand RGB, NIR, and depth video to build more robust perception, understanding, and reasoning. To achieve the above requirements, we design retrieval-enhanced multimodal frameworks. We collected multiple data sets and built a large-scale benchmark. This benchmark measures the sage deer's perceptual decision-making ability and the super alignment's accuracy.",
    "keywords": [
        "Driving Cockpit; Super alined; Driving Generalist"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1FiMrJxPAM",
    "pdf_link": "https://openreview.net/pdf?id=1FiMrJxPAM",
    "comments": [
        {
            "withdrawal_confirmation": {
                "value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."
            },
            "comment": {
                "value": "Thank you for your effective suggestions. We will further improve the quality of our papers with these suggestions."
            }
        },
        {
            "summary": {
                "value": "The paper introduces Sage Deer, a multi-modal, multi-view framework for intelligent driving cockpits, designed to provide personalized, context-aware assistance. It integrates RGB, NIR, and depth cameras, and captures diverse data on driver states such as physiology, emotion, and behaviour which enables comprehensive monitoring and real-time response. This data is processed through a language model, allowing for nuanced comprehension and interaction capabilities.\n\nThe system\u2019s architecture relies on three core components: retrieval-augmented generation (RAG), multi-modal fusion, and expert knowledge incorporation. RAG allows Sage Deer to retrieve relevant external information, tailoring responses to user preferences. Multi-modal fusion combines data from various camera views, enhancing the model's understanding of the environment and driver states. Expert knowledge fusion further refines Sage Deer\u2019s outputs by integrating specialized insights into physiological and emotional monitoring, optimizing its response relevance and accuracy.\n\nExperimental results demonstrate Sage Deer\u2019s effectiveness in multitasking and adapting to diverse user needs, providing a benchmark for intelligent cockpit design. By aligning AI capabilities with user-centered safety requirements, Sage Deer advances the potential of personalized driver assistance systems, positioning itself as a foundational technology for future ADAS applications."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Sage Deer integrates multi-modal and multi-view data sources, combining RGB, NIR, and depth cameras to achieve a highly adaptive and personalized intelligent cockpit system. The model\u2019s use of Retrieval-Augmented Generation (RAG) allows it to pull relevant context-specific information from external sources, enhancing the system\u2019s real-time responsiveness and ability to deliver highly accurate, personalized interactions aligned with individual driver preferences. This capacity for personalization goes beyond standard cockpit systems, as Sage Deer monitors physiological, emotional, and behavioural states to tailor responses to the driver's unique profile, significantly boosting both user engagement and safety.\n\nThe fusion of diverse sensor data enables Sage Deer to accurately perceive and interpret complex, dynamic conditions within and outside the vehicle, making it capable of maintaining performance under varying lighting and environmental scenarios. Its robust, real-time capabilities show substantial potential for practical applications in ADAS, offering intelligent, responsive support that adapts continuously to real-world challenges. Sage Deer\u2019s architecture sets a new standard for intelligent cockpit systems, bringing together advanced AI components to enhance driver experience and overall vehicle safety in ways that align with the evolving demands of autonomous and semi-autonomous vehicles."
            },
            "weaknesses": {
                "value": "The paper needs improvement in writing. There are mistakes and citation errors. See at the end of this message.\n\nThe main issue is the novelty. It seems to combine multiple models to improve intelligent driving. This contribution is not good enough for a conference like ICLR.\n\nIn (Section 4.3), the authors mentioned that the model relies on visual tokenization of physiological data, such as heart rate and blood oxygen levels, to infer emotional or behavioural states. However, this approach assumes direct correlations with emotions, potentially leading to inaccuracies. While the authors have cited studies in psychophysiology suggesting links between physiological signals and emotions, the real-world application requires greater nuance. Authors should discuss impact on signals due to factors like individual baselines, environmental conditions, and physical activity.\n\nThe model\u2019s use of a pre-trained ResNet18 for tokenizing RGB, NIR, and depth inputs may lack the capacity to capture the complex nuances needed for an intelligent cockpit system. To address this, the authors should conduct ablation studies comparing ResNet18 with advanced models like ResNet50, EfficientNet, ViT, and Swin Transformer to assess improvements in accuracy and robustness. Additionally, the current concatenation-based fusion strategy may underutilize the complementary data from multi-modal inputs. Testing different fusion techniques, such as attention-based and cross-attention methods, could identify more effective integration approaches. Further analysis of each modality\u2019s impact would clarify the significance of RGB, NIR, and depth data, while transformer-based models could improve temporal understanding for tasks like fatigue tracking.\n\nThe reliance on a language model for contextual understanding may oversimplify dynamic driving scenarios, missing essential non-verbal cues for real-time safety. Ablation studies could address this by comparing language-only input to multi-modal input (e.g., visual, physiological, behavioural data) to assess non-verbal contributions to accuracy in safety-critical tasks. Testing each modality individually would highlight their impact while comparing the language model with and without RAG would clarify RAG\u2019s role in context accuracy.\n\nWriting:\nLine 37: He -> it\nLine 50: s possess?\nLine 53: with s?\nLine 61: reference a?\nLine 66: repeat of Sima et al.\nLine 188: Beachmarking -> Benchmarking"
            },
            "questions": {
                "value": "1. The novelty of the contribution.\n2. In Section 4.3, the model uses visual tokenization of physiological signals to infer emotions. How does the model account for individual differences in physiological baselines or external factors (e.g., physical activity, environmental conditions) that might affect these signals?\n3. Why was ResNet18 chosen over more advanced models like ResNet50, EfficientNet, or transformer-based architectures? Did you conduct any initial tests with these models?\n4. Would you consider performing ablation studies comparing ResNet18 with more powerful feature extractors to evaluate improvements in capturing behavioral and environmental nuances?\n5. How does ResNet18 perform in capturing temporal dependencies in sequential data, particularly for tasks requiring context awareness over time, such as fatigue tracking?\n6. Given the current use of a concatenation-based fusion approach, have you explored other fusion techniques, such as attention-based fusion or cross-attention mechanisms, to maximize the complementary data from RGB, NIR, and depth inputs? Have you considered ablation studies to evaluate the impact of each modality independently?\n7. How does the model handle or prioritize input from non-verbal cues compared to language-based cues in dynamic driving contexts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not applicable"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to build a super-aligned and generalist driving agent, called sage deer for the intelligent driving cockpit. A new dataset is constructed for many tasks, e.g., physiological estimation, emotional estimation, gesture estimation, body motion estimation, driving behavior estimation, driving behavior detection, and driving decision-making. An MLLM is trained for unified tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is the first to construct a unified dataset for MLLMs in intelligent driving cockpit. A multi-task dataset is provided and an MLLM is trained on the dataset."
            },
            "weaknesses": {
                "value": "- The dataset construction part is extremely lacking in details, including data curation, GPT4 labeling, etc. The paper states in many places that the details are in supplementary materials, but the supplementary materials have not been submitted. Besides, the contribution of \"An intelligent driving cockpit super alignment evaluation protocol involving generalization ability for different needs was established\" cannot be well-established in the paper.\n- The qualitative results are very limited. Only in Fig. 4, some conversations are provided, and from this figure, we cannot know the full ability of the model.\n- The writing of the paper was very hasty. many sentences are not clear and typos are everywhere, e.g., \"serves as the interface for human interaction with s\" in L053, and \"Tokenizing Multi-Model\" in L257."
            },
            "questions": {
                "value": "See weaknesses. Please provide more details as much as possible."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This research presents \"Sage Deer,\" an innovative super-aligned and generalist driving agent designed to enhance intelligent cockpit systems. The proposed framework addresses the challenges of personalized driving experiences and comprehensive omni-modal information processing through several key innovations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Strength:\n\n-\tSeamlessly integrates data from various sensors (RGB, NIR, depth cameras) and multiple perspectives, enabling comprehensive environmental understanding.\n\n-\tEmploys a unique mechanism that combines an updatable knowledge base with a large language model, enabling contextually relevant responses without extensive fine-tuning."
            },
            "weaknesses": {
                "value": "Weakness:\n-\tThe authors should provide more comprehensive information about the model architecture, including specifics such as the choice of LLM with its size and so on.\n\n-\tIn Figure 3, a \"Pre-trained Video Encoder\" is depicted, whereas Section 4.2 mentions the use of an \"ImageNet pre-trained ResNet18.\" Are these referring to the same component? Additionally, how does this encoder handle other modalities? Lastly, how many tokens does the encoder output? Providing more detailed explanations would enhance understanding.\n\n-\tIn Section 4.1, the author introduces specific start and end symbols to denote different modalities. Are these symbols newly added special tokens to the LLM's vocabulary? If so, how are these tokens initialized? Since the LLM remains frozen and is not further trained, how does the pretrained model recognize these new tokens?\n\n-\tIn Section 5.2, the maximum sentence length is set to 64. How was this value determined? Since text sentences are processed by a tokenizer, why not base this parameter on the number of tokens instead? Were any experiments conducted to evaluate the impact of this choice on performance or the training and inference computational budget?\n\n-\tThe sequence of tables and figures should be adjusted for consistency. For instance, Table\u202f2 is only mentioned in Section\u202f5.5, while Tables\u202f3 and\u202f4 are referenced earlier in the document before Table\u202f2.\n\n-\tThe manuscript requires improved writing quality, as numerous typographical errors are present. For example, on line\u202f414, \"model\" should be corrected to \"figure,\" and on line\u202f261, a space is needed between the text and the equation.\n\n\nThe manuscript currently contains several typographical and writing errors, as well as some missing details, which is not ready for submission. I believe it would benefit from further revisions to address these issues and ensure it meets the standards required for submission to ICLR."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a multi-modal LLM designed for human and scene understanding in autonomous driving. It integrates multi-view image and multi-modal inputs, using a retrieval-augmented generation strategy to improve test-time adaptation. For evaluation, a multi-modal, multi-view dataset for driving perception is introduced. The proposed method outperforms standard multi-modal LLM models."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The motivation to combine 3D scene perception with both visual and physiological signals from humans is clear and compelling. However, an ablation study on each signal and modality would enhance understanding of their individual contributions.\n2. A dataset is established, incorporating multi-modal, multi-view sensor data along with QA pairs for evaluating the LLM\u2019s scene understanding and reasoning capabilities.\n3. The proposed method shows improved overall performance on the provided dataset."
            },
            "weaknesses": {
                "value": "Overall, I regret to say that this submission appears of low quality, with numerous errors suggesting it was submitted without thorough proofreading\u2014a potential disservice to the reviewers.\n\n1. There are numerous typographical and formatting errors, including typos, incorrect notations, capitalization issues, and incomplete sentences. Examples include:\n   - L050: \"technologies, s possess\"\n   - L053: '\"with s,\"'\n   - L208: \"supplement the captain\"\n   - L261, L265: \"the language spaceemface \u2208 C \u00d7 L.\", \"emrgb \u2208 C \u00d7 L, < RGB bos > emrgb < RGB cos >.\"\n   - L271, L277: \"emfront \u2208 C \u00d7 L,\"\n   - L288: \"framework ash shown in Fig. 3\"\n   - L307: \"The Relationship Between Physiological State and Emotion: Classical\"\n   - L315-L317: \"other tasks, including: The Relationship Between Physiological State and Behavior\u2026\" (repeated thrice)\n\n2. The proposed method lacks novelty, as it is essentially a multimodal LLM with RAG, without any specific design tailored for the target task. Additionally, key methodological details, such as training strategies, specific model architectures, and hyperparameters, are missing.\n\n3. Experimental analysis is limited. In-depth experimentation and analysis are needed to substantiate the claimed benefits of using a multimodal approach.\n\n4. The dataset setup is unclear. Since the captions are generated by open-source VLMs, please clarify the measures taken to ensure their quality.\n\n5. The related work citations do not consistently support the claims made. For instance, L308 references \"Classical studies in psychophysiology (e.g., James-Lange, Schachter-Singer)\u2026\u201d without sufficient context.\n\n6. The appendix section is empty. Please remove the placeholder text: \"You may include other additional sections here.\"\n\n7. Finally, as the dataset includes human subjects, please provide an ethics statement to address concerns regarding its use."
            },
            "questions": {
                "value": "Please see weaknesses section.\n\nAdditionally, real-world dataset construction rarely captures abnormal behaviors. How, then, does training on the proposed dataset support effective human behavior anomaly detection?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)",
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I recommend this paper be flagged for an ethics review due to concerns related to the proposed dataset, which includes human subjects. Key ethical considerations include:\n\n1) The dataset's inclusion of human data raises concerns about compliance with copyright laws, data protection standards, and consent protocols under regulations like GDPR.\n\n2) The use of human data requires careful consideration of ethical research practices, including whether informed consent was obtained, how the data will be stored, and the responsible handling and potential release of this data.\n\nTo ensure an ethically sound review, an ethics reviewer with expertise in privacy, legal compliance, and responsible research practices would be most suitable."
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces \"Sage Deer,\" an intelligent driving cockpit system aimed at meeting personalized user needs through a multi-modal framework and a retrieval-augmented generation mechanism."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The concept of Sage Deer as a super-aligned, generalist agent offers a fresh approach to intelligent cockpit systems, adapting in real-time to individual user preferences.\n2. The tailored application of a retrieval-augmented generation framework for the driving domain is a notable contribution, enabling efficient and adaptive responses to evolving user needs.\n3. The development of a large-scale benchmark using a variety of datasets (AIDE, DMD, and others) to assess the system's decision-making and perception capabilities adds rigor and depth to the system\u2019s evaluation."
            },
            "weaknesses": {
                "value": "1. How are the various inputs (e.g., visual, physiological) integrated to influence real-time driving decisions? \n2. Could the paper delve deeper into how user interactions are managed, especially in complex scenarios? Are there any limitations to the system\u2019s ability to interpret nuanced or less common user behaviors?\n3. There are a few errors: for example, the purpose of \"s\" in lines 50 and 53 is unclear, \u201cOut View Caption\u201d is duplicated in Figure 2, and \u201cAccurate labele\u201d contains a spelling error."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes to leverage multimodal data and LLMs to understand driver physiology, emotions, and behaviors in real-time. The authors use a RAG framework combined with expert knowledge integration to provide personalized feedback."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1\u3001The paper introduce an interesting problem, emphasizing the importance of individual preferences in enhancing the driving experience.\n2\u3001The RAG framework addresses the need for flexible, personalized responses without extensive model fine-tuning.\n3\u3001The proposed method was evaluated on multiple driving datasets for its generalist and super-aligned performance."
            },
            "weaknesses": {
                "value": "1\u3001The paper shows limited novelty, like RAG, are pre-existing approaches.\n2\u3001The \"Expert Knowledge Fusion\" section  isn\u2019t clearly explained. Adding pseudocode or a flowchart could make it easier to follow.\n3\u3001The paper lacks ablation studies to verify the effectiveness of individual modules, such as physiological indicators and expert knowledge fusion."
            },
            "questions": {
                "value": "1\u3001Can you explain why the paper chose ResNet18 as the pre-trained model instead of a more powerful option?\n2\u3001With multimodal data and personalized preferences present, how does RAG decide which information to prioritize for retrieval? Is there a specific prioritization or weighting system?\n3\u3001Could Sage Deer be compared more thoroughly with other recent intelligent driving agents, like DriveGPT or DriveLM, to provide a deeper understanding of its performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}