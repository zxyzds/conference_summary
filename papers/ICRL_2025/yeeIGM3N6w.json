{
    "id": "yeeIGM3N6w",
    "title": "Retraining-Free Merging of Sparse Mixture-of-Experts via Hierarchical Clustering",
    "abstract": "Sparse Mixture-of-Experts (SMoE) models represent a significant breakthrough in large language model development. These models enable performance improvements without a proportional increase in inference costs. By selectively activating a small set of parameters during task execution, SMoEs enhance model capacity. However, their deployment remains challenging due to the substantial memory footprint required to accommodate the growing number of experts. This constraint renders them less feasible in environments with limited hardware resources. To address this challenge, we propose Hierarchical Clustering for Sparsely activated Mixture of Experts (HC-SMoE), a task-agnostic expert merging framework that reduces SMoE model parameters without retraining. Unlike previous methods, HC-SMoE employs hierarchical clustering based on expert outputs. This approach ensures that the merging process remains unaffected by routing decisions. The output-based clustering strategy captures functional similarities between experts, offering an adaptable solution for models with numerous experts. We validate our approach through extensive experiments on eight zero-shot language tasks and demonstrate its effectiveness in large-scale SMoE models such as Qwen and Mixtral. Our comprehensive results demonstrate that HC-SMoE consistently achieves strong performance, which highlights its potential for real-world deployment.",
    "keywords": [
        "Sparse Mixture-of-Experts",
        "Merging",
        "Compression"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "Our method (HC-SMoE) offers an efficient method for merging experts of large Sparse Activated Mixture of Experts (SMoE) models without retraining under task-agnostic settings.",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=yeeIGM3N6w",
    "pdf_link": "https://openreview.net/pdf?id=yeeIGM3N6w",
    "comments": [
        {
            "summary": {
                "value": "The paper presents a retraining free experts merging approach which employs a hierarchical clustering strategy. The authors claim that using expert outputs as the similarity metric for clustering is more effective compared with using router logins or weights employed by prior works. The experimental results reveal the proposed approach gains more performance improvements compared with existing methods across various benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. An output based similarity metric of expert clustering is proposed, which is more effective than previous works.\n2.The experimental results compared with the previous methods are very good."
            },
            "weaknesses": {
                "value": "There is a lack of theoretical analysis on the performance of expert clustering using different similarity metrics."
            },
            "questions": {
                "value": "I am wondering the results or analysis for more extreme expert reduction scenarios, such as reducing to 25% or 10% of the original experts. This would give insight into how the method performs under more aggressive compression."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work introduces Sparse Mixture-of-Experts (SMoE) models, which improve large language model performance without significantly increasing inference costs by activating only a subset of parameters. However, their high memory requirements hinder deployment. To address this, the authors propose Hierarchical Clustering for Sparsely activated Mixture of Experts (HC-SMoE), a task-agnostic framework that reduces SMoE parameters without retraining."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- HC-SMoE offers a practical solution for reducing parameters without the need for retraining, simplifying the implementation process.\n\n- The task-agnostic nature of HC-SMoE allows for broader applicability across different language tasks, enhancing its versatility.\n\n- The comprehensive experiments conducted on eight zero-shot language tasks provide strong empirical evidence of HC-SMoE's effectiveness in large-scale models like Qwen and Mixtral."
            },
            "weaknesses": {
                "value": "- While this work demonstrates competitive accuracy, it lacks a comprehensive assessment of efficiency metrics, such as speedup and memory usage. Given that efficiency is a key contribution, this aspect of the experimental results is essential.\n\n- A theoretical analysis of the effectiveness of expert merging and HC-SMoE would enhance the understanding of the method's performance.\n\n- Although HC-SMoE is validated on eight zero-shot language tasks, its effectiveness may vary in more complex tasks or domains, potentially limiting its broader applicability."
            },
            "questions": {
                "value": "Please refer to the Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed a new expert merging framework, named Hierarchical Clustering for Sparsely activated Mixture of Experts (HC-SMoE), to reduce SMoE model parameters without retraining. The proposed method is simple but effective, and the experiments demonstrated the efficacy of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The proposed method is simple but effective, \n* The paper is very easy to follow.\n* The experiments are comprehensive and the results are very promising."
            },
            "weaknesses": {
                "value": "* The motivation of using the \"Hierarchical\" clustering is not clear to me. I cannot intuitively get the idea of why hierarchical clustering is better than simple K-means clustering, although the results confirmed that K-means clustering is less effective. Besides, the paper proposed to use a \"hard\" hierarchical clustering, and I am wondering if it is more effective to use \"soft\" hierarchical clustering or simply \"soft\" clustering without hierarchies. \n* The choice of the calibration dataset. I did not see any ablation study about the choice of the calibration dataset, and I think the performance of the proposed method should highly depend on the calibration dataset. If the calibration dataset is not comprehensive enough, e.g., not covering enough domain specific data, the clustering may not be very informative, which may lead to poor performance. For example, if we want the LLM to perform well on a law-related or medical-related tasks, can you also rely on the same calibration dataset used in the experiments?\n* Some minor issues:\n  * In Fig. 1, why you did not compare the methods on 14B model?\n  * Section 3.2.1 presents the method of similarity metric but contains a lot of discussions about related work.\n  * In line 299/300, is alpha_i fixed or not? If it is fixed, will it also suffer from the issue that you mentioned in line 199-203 about frequency-based method?\n  * In Table 4, the best performance of 'ARC-c' should be the Average linkage using the Weight setting, right?"
            },
            "questions": {
                "value": "Please refer to my question above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Hierarchical Clustering for Sparsely Activated Mixture of Experts (HC-SMoE), a task-agnostic framework for merging experts within an SMoE model. HC-SMoE aims to reduce the model's parameters without requiring retraining. Experiment results on a series of benchmarks show its effectiveness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1) Pruning experts in MoE models can indeed reduce the difficulty of deployment.\n2) The paper is easy to follow, and the ablation study is comprehensive.\n3) The experimental results on Qwen and Mixtral are convincing."
            },
            "weaknesses": {
                "value": "1) O-prune [1] requires enumerating all possible combinations of experts, resulting in significant time overhead. I would like to know how HC-SMoE compares to other approaches in terms of runtime and resource consumption.\n2) O-prune [1] also conducts experiments on domain-specific tasks (e.g., GSM8K, Math). I am interested in the performance of HC-SMoE on these datasets.\n\n[1] Lu, Xudong et al. \u201cNot All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models.\u201d Annual Meeting of the Association for Computational Linguistics (2024)."
            },
            "questions": {
                "value": "See Weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}