{
    "id": "ech9J3xl9X",
    "title": "Narrow Transformer: Mono-lingual Code SLM for Desktop",
    "abstract": "This paper presents NT-Java-1.1B, an open-source specialized code language model built on StarCoderBase-1.1B, designed for coding tasks in Java programming. NT-Java-1.1B achieves state-of-the-art performance, surpassing its base model and majority of other models of similar size on MultiPL-E Java code benchmark. While there have been studies on extending large, generic pre-trained models to improve proficiency in specific programming languages like Python, similar investigations on small code models for other programming languages are lacking. Large code models require specialized hardware like GPUs for inference, highlighting the need for research into building small code models that can be deployed on developer desktops. This paper addresses this research gap by focusing on the development of a small Java code model, NT-Java-1.1B, and its quantized versions, which performs comparably to open models around 1.1B on MultiPL-E Java code benchmarks, making them ideal for desktop deployment. This paper establishes the foundation for specialized models across languages and sizes for a family of NT Models.",
    "keywords": [
        "Narrow Transformer",
        "Code SLMs",
        "Desktop Deployment",
        "Lightweight Code Language Models",
        "Small Language Models",
        "Language Specific Models",
        "Monolingual Code Language Model"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ech9J3xl9X",
    "pdf_link": "https://openreview.net/pdf?id=ech9J3xl9X",
    "comments": [
        {
            "summary": {
                "value": "The authors introduce NT-Java, a narrowly fine-tuned code language model of StarCoder's bade model of 1B parameters especially suited for edge applications on smaller devices. The model is finetuned on the Java-portion of the Stack, and training is performed with Nvidia's Megatron LM framework. The model's performance is evaluated on a next token prediction objective, and a fill-in-the-middle objective. Quantized versions of the tuned model are made available for wider use."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper is written with a coherent story-line, which leads one through the entire paper, and does a very good job in describing its experiments as well as the exact setup used. The reviewer is highly confident that one would be able to reproduce the results from their description in the paper."
            },
            "weaknesses": {
                "value": "The paper's core weaknesses can be narrowed down to 2 key points in the reviewer's eyes: **novelty**, and **strength of results**\n\nNovelty:\n* A number of large language model releases have evaluated their models at the smaller scale, and projects like e.g. llamafile go to great lengths to test these smaller models on edge devices, what distinguishes the \"Narrow Transformer\" from these models aside from being fine-tuned on Java-only?\n* The reviewer is left unconvinced that a Java-only fine-tuned transformer is a noteworthy result. While target applications for small language models are alluded to in the _Related Work_ section, they are not reflected in the evaluation design.\n* There exist a great number of blogposts out in the web showcasing the fine-tuning of pretrained models on distilled corpora. The reviewer is left unconvinced that this paper in its present state goes beyond this state of the art.\n\nStrength of Results:\n* It remains unconvincing to the reviewer from the presented results, that a model fine-tune only on a single programming language (after it was pretrained on a multi-programming language corpus) is superior to a smaller model trained on only Java from the outset. When making such claim, I would expect it to be backed up by experiments by e.g. training a StarCoder-style transformer from scratch on the distilled Java corpus."
            },
            "questions": {
                "value": "While I understand the premise & aspiration of the authors, I am left to question what the actual research question is in this instance?\n\nFinetuning of pretrained models is almost commoditized by now, and by taking a fine-tuning framework like e.g. Axolotl one could produce similar results fairly quickly if with access to the commensurate compute resources. How do the authors go beyond this, and contribute to small language model development?\n\nWhat (open-ended) questions I would furthermore like to leave the authors with are:\n* Are there specific small model optimizations which could e.g. improve latency, or the ability to deploy the model on tiny devices or even embedded hardware\n* Do existing evaluation metrics reflect the specific demands of small language models adequately? If not, which aspects are missing and should see custom evaluation tasks?\n* What applications do the authors envision for small language models specifically, and what evaluation design would be derived from these downstream demands?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work introduced the NT-Java-1.1B model, detailing its development process and evaluation results. NT-Java-1.1B is developed based on the StarCoderBase-1.1B model and trained on a subset of StarCoderData. Evaluation results indicate that NT-Java-1.1B outperforms StarCoderBase-3B in pass@1 performance on the MULTIPL-E benchmark, while it scores lower than StarCoderBase-1.1B on HumanEval-FIM (Java)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This work provides a detailed introduction to the training process of NT-Java-1.1B and examines the impact of FIM (Fill-in-the-Middle) training, which can offer reference value for developing other SLMs."
            },
            "weaknesses": {
                "value": "+ This work\u2019s novelty is limited, as it builds on the StarCoderBase-1.1B model [1] using training data from StarCoderData [2] and applies established methods such as Next Token Prediction [3] and Fill-in-the-Middle [4]. While it provides an application of these methods, it does not introduce new improvements or substantial contributions.\n\n+ The experiments in this work are insufficient, as there are too few baseline models compared to NT-Java-1.1B; only StarCoderBase-1.1B and StarCoderBase-3B are included, lacking comparisons with other models.\n\n[1] StarCoder: may the source be with you!\n\n[2] https://huggingface.co/datasets/bigcode/starcoderdata\n\n[3] Improving Language Understanding by Generative Pre-Training\n\n[4] Efficient training of language models to fill in the middle\n\nMinor issues: In Line 154, a subsection number is missing."
            },
            "questions": {
                "value": "At the end of the Evaluation section, the authors mentioned that they conducted qualitative evaluations through user studies, but details and results of this evaluation are not provided. Could you share more comprehensive results for this part of the evaluation?\n\nThe authors have trained a model and indicated that its performance has improved. Could you elaborate further on the novelty in your work and clarify if there are any unique contributions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces NT-Java-1.1B, a specialized code language model designed for Java programming, built on StarCoderBase-1.1B. It aims to propose small, efficient code models that can be deployed on developer desktops. NT-Java-1.1B achieves state-of-the-art performance on the MultiPL-E **Java** code benchmark, surpassing its base model and other similar-sized models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. Investigating small and efficient code models is meaningful."
            },
            "weaknesses": {
                "value": "1. This paper merely fine-tunes an open-source base model (StarCoderBase-1.1B) using part of public data (The Stack v1) and existing training methods. The result is evaluated on only part of the MultiPL-E benchmark. This paper offers no new technical contributions, lacks thorough experimental analysis, and provides no insights.\n2. The term \"Narrow Transformers\" is neither defined nor has any related references. It seems like a concept created by the authors. I'm confused about how this concept differs from small LLMs or efficient LLMs. Could the authors explain this?\n3. The experimental results are poor. The fine-tuning of the base model StarCoderBase-1.1B for JAVA yields results that are basically similar to StarCoderBase-1.1B itself, while StarCoderBase-1.1B is capable of handling multilingual tasks instead of just JAVA.\n4. This paper only contains 6 pages."
            },
            "questions": {
                "value": "No questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose to train a small code model called \"NT-Java-1.1B\" specialized for Java. Their small model achieves better performance on Java than the StarCoderBase-1.1B counterpart which they use as the initial starting checkpoint. The small model size allows the model to efficiently run on laptop devices or other edge hardware thereby increasing productivity of developers writing Java code."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper is easy to read and well-written and easy to follow.\n2. The authors release their models under the OpenRAIL-M license which allows both research and commercial use."
            },
            "weaknesses": {
                "value": "1. There is no novelty in the paper.\n2. Evaluations are missing for the GGUF model."
            },
            "questions": {
                "value": "The paper is easy to read and well-written although there are some gaps:\n1. There is no reason to talk about the binary .bin and .idx files format in Megatron in section 4.1\n2. line 130 says that the model uses FlashAttention, however, it should be noted that FlashAttention is not a part of the model, its just a training time optimization.\n3. The paper mentions using `load_in_4bit` and `load_in_8bit` arguments from HuggingFace APIs are used for evaluation however, as far as I am aware, `load_in_8bit` uses [LLM.int8()](https://arxiv.org/abs/2208.07339) algorithm to quantize the model. It would be better to use a better algorithm like GPTQ or AWQ to report the accuracy. The `load_in_4bit` argument uses FP4/NF4 quantization from the bitsandbytes library. While these are easily usable from a user's perspective, it would be nice to have evaluation numbers with the GGUF model.\n4. Table 3 and 4 can be combined into a single table and there is no need for 2 different tables.\n5. There is no comparison with Llama-3.2 1B model. It would be better to add that comparison as a baseline.\n6. It might be better to train a MoE model with less activated parameters for efficiency I think. Something like 3B full parameters with ~500-800M activated parameters for better model accuracy while still being efficient. However, I understand that this might be unfeasible due to compute restrictions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}