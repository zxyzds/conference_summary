{
    "id": "MEF8SyXuXG",
    "title": "Learning in complex action spaces without policy gradients",
    "abstract": "Conventional wisdom suggests that policy gradient methods are better suited to complex action spaces than action-value methods. However, foundational studies have shown equivalences between these paradigms in small and finite action spaces (O'Donoghue et al., 2017; Schulman et al., 2017a). This raises the question of why their computational applicability and performance diverge as the complexity of the action space increases. We hypothesize that the apparent superiority of policy gradients in such settings stems not from intrinsic qualities of the paradigm, but from universal principles that can also be applied to action-value methods to serve similar functionality. We identify three such principles and provide a framework for incorporating them into action-value methods. To support our hypothesis, we instantiate this framework in what we term QMLE, for Q-learning with maximum likelihood estimation. Our results show that QMLE can be applied to complex action spaces with a controllable computational cost that is comparable to that of policy gradient methods, all without using policy gradients. Furthermore, QMLE demonstrates strong performance on the DeepMind Control Suite, even when compared to the state-of-the-art methods such as DMPO and D4PG.",
    "keywords": [
        "action-value learning",
        "policy gradient methods",
        "complex action spaces"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=MEF8SyXuXG",
    "pdf_link": "https://openreview.net/pdf?id=MEF8SyXuXG",
    "comments": [
        {
            "summary": {
                "value": "The paper challenges the conventional belief that policy gradient methods are superior to action-value methods for complex action spaces. The authors propose that this advantage comes from universal principles that can also be applied to action-value methods, such as using Monte Carlo approximations, amortized maximization, and scalable architectures. They introduce Q-learning with Maximum Likelihood Estimation (QMLE), a framework that adapts these principles to action-value learning. Empirical results show that QMLE performs comparably to policy gradient methods on complex action spaces, particularly on the DeepMind Control Suite. The findings suggest that the strengths of policy gradient methods are not intrinsic but can be replicated within action-value approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper presents a novel perspective by challenging the conventional wisdom that policy gradient methods are inherently superior for complex action spaces, opening up new avenues for research and exploration.\n- The introduction of the Q-learning with Maximum Likelihood Estimation (QMLE) framework effectively integrates principles traditionally associated with policy gradients into action-value learning, showcasing versatility and adaptability.\n- The authors provide robust empirical results demonstrating that QMLE can achieve performance comparable to policy gradient methods on complex tasks, specifically in the DeepMind Control Suite, thereby validating their theoretical claims.\n- The paper emphasizes the scalability of the proposed methods, which is crucial for real-world applications involving high-dimensional action spaces, making it a practical contribution to the field of reinforcement learning.\n- By revealing that the strengths of policy gradient methods can be replicated in action-value approaches, the paper encourages a reevaluation of existing strategies in reinforcement learning, potentially influencing future research directions and methodologies."
            },
            "weaknesses": {
                "value": "- While the paper introduces the QMLE framework, it lacks in-depth theoretical analysis or proof of convergence properties, which could strengthen the foundational understanding of the proposed method.\n- The empirical evaluations are primarily conducted in the DeepMind Control Suite, which may not fully represent the challenges and complexities found in more diverse real-world environments, limiting the generalizability of the findings.\n- The paper could benefit from a more thorough comparison with other state-of-the-art action-value methods beyond policy gradient approaches to provide a clearer context for the advantages and limitations of the QMLE framework.\n- There is little discussion on the sensitivity of the QMLE approach to hyperparameters, which can significantly impact performance in practice; this omission could hinder practitioners from effectively applying the method.\n- The paper does not offer concrete suggestions for future research directions or improvements to the QMLE framework, which could leave readers uncertain about the next steps for advancing this line of inquiry."
            },
            "questions": {
                "value": "- What are the key theoretical assumptions underlying the QMLE framework, and how do they compare to traditional action-value methods?\n- How does the QMLE framework perform in environments outside the DeepMind Control Suite? Are there specific tasks where its advantages or limitations become more pronounced?\n- What guidelines can be provided for selecting hyperparameters when using the QMLE framework, and how does their choice impact performance across different environments?\n- Can you elaborate on the implementation challenges encountered when applying the QMLE framework in practice? Were there any unexpected behaviors observed during training?\n- How does the QMLE framework stack up against other recent approaches in reinforcement learning, particularly those utilizing action-value methods? Are there specific scenarios where it outperforms others?\n- What are the authors\u2019 thoughts on potential extensions or modifications to the QMLE framework that could enhance its applicability or efficiency in more complex scenarios?\n- How robust is the QMLE approach to noise and variability in the environment? Have any experiments been conducted to assess its stability under such conditions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper challenges the assumption that policy gradient methods are inherently superior for environments with complex/continuous action spaces. The authors identify core principles (Monte Carlo approximations, maximum likelihood estimation, and action-in architectures) in policy gradients and incorporate them into action-value methods. Their method achieves comparable performance to policy gradient methods in continuous control tasks in DMC, demonstrating that action-value methods can handle complex action spaces without the need for policy gradients\u200b"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The identification of the three core principles is noteworthy and has the potential to influence future research on policy gradient methods, even considering that the paper's goal is to provide alternatives to traditional policy gradients. \n2. The experimental setup appears sound, with comparisons against several baselines across multiple seeds, and the results generally favor the proposed method. However, I would suggest including an ablation study (see Point 3 below)."
            },
            "weaknesses": {
                "value": "1. My major concern is with the overall goal of the paper, as its central premise is unclear to me. First, what is the issue with policy gradients that the authors are aiming to replace them? This should have been clarified to justify the need for an alternative approach. If the goal is to develop a method that is simpler (i.e., fewer components, reduced computation, fewer hyperparameters) than policy gradients, similar to the approach in [1], I would argue that the proposed method is in fact more complex than policy gradients. It introduces additional hyperparameters and design decisions. Furthermore, the method eventually requires a parametric component for predicting the argmax (Principle 2 in Section 4). In my view, this component functions as a type of policy, albeit in a somewhat convoluted and unintuitive way. Its inclusion seems to contradict the paper\u2019s aim of eliminating the need for a policy and policy gradients. In fact, I would argue that the proposed method cannot be considered purely as an action-value method, as claimed by the authors.\n\n2. The argument regarding the equivalence of policy gradients and MLE (Section 3.2), while likely correct, is presented in a very informal manner. A more rigorous analysis or formal proof of this equivalence would significantly strengthen the paper.\n\n3. The paper lacks an ablation study on the different components of the proposed method (Principles 1, 2, and 3). Given the number of design choices involved, an ablation study is essential to better understand the contribution of each principle.\n\n4. Many of the equations in the paper (Equations 3, 4, 8, 9, 10, 11, 13, 14) are written without an equal sign, making them appear as disconnected mathematical expressions. While their meaning can be inferred from the surrounding text, the lack of clarity in some cases creates an informal mathematical tone and makes it difficult to interpret the exact role of these equations.\n\n5. The background section spans three pages and covers material likely familiar to most readers. I recommend condensing this section and moving some of the content to the appendix. This would allow for a more detailed presentation of the algorithm (currently in Appendix A) in the main body of the paper, as well as addressing the concerns I raised above.\n\n\n[1] Seyde, Tim, et al. \"Solving Continuous Control via Q-learning.\" The Eleventh International Conference on Learning Representations. 2023."
            },
            "questions": {
                "value": "1. What are the limitations of policy gradients that lead to their replacement?\n2. Why does the number of seeds vary across different tasks? Additionally, what do the shaded regions in the figures represent?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes three universal principles from policy gradient methods and incorporates them into action-value framework.\n\nThe method is evaluated in DMC domains with continuous action space.\n\nHowever, the proposed method does not show a significant improvement in performance over DDPG or D4PG, especially in domains with high-dimensional action spaces."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1, The proposed principles underlying the scalability of policy gradient methods are intriguing observations, and a careful analysis of the gap between the two paradigms is insightful.\n\n2, The presentation of the ideas proposed is clear.\n\n3, Experiments were conducted across multiple domains, although without significant improvement."
            },
            "weaknesses": {
                "value": "1, There is no theoretical guarantees for computing the maximization in A_m instead of A(eq.17).    (In policy gradient methods, using an MC estimator in place of exact summation or integration has theoretical foundations.)  \n\n2, The proposed method trains all the predictors from historical argmax approximation  to construct a small action space A_m for computing an approximation of best action. The iterative dependency restricted the actions to a subset where most actions are similar, potentially leading to a suboptimal solution.\n\n3, As illustrated in Figure.2, QMLE can transcend DPG through a uniform sampling over action space([global]). But in high-dimensional action space, uniform sampling is usually inefficient. This paper lacks an ablation study of the uniform sampling and the ensemble of argmax predictors in the domains with high-dimensional action spaces."
            },
            "questions": {
                "value": "1, The proposed method uses log-likelihood gradient to train the argmax predictors.  Why not use the policy gradient directly to update the the ensemble of argmax predictors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}