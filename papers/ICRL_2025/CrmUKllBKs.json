{
    "id": "CrmUKllBKs",
    "title": "Pseudo Physics-Informed Neural Operators",
    "abstract": "Recent advancements in operator learning are transforming the landscape of computational physics and engineering, especially alongside the rapidly evolving field of physics-informed machine learning. The convergence of these areas offers\nexciting opportunities for innovative research and applications. However, merging\nthese two realms often demands deep expertise and explicit knowledge of physical systems, which may be challenging or even impractical in relatively complex applications. To address this limitation, we propose a novel framework: Pseudo\nPhysics-Informed Neural Operator (PPI-NO). In this framework, we construct a\nsurrogate physics system for the target system using partial differential equations\n(PDEs) derived from simple, rudimentary physics knowledge, such as basic differential operators. We then couple the surrogate system with the neural operator model, utilizing an alternating update and learning process to iteratively enhance\nthe model\u2019s predictive power. While the physics derived via PPI-NO may not mirror the ground-truth underlying physical laws \u2014 hence the term \u201cpseudo physics\u201d \u2014 this approach significantly enhances the accuracy of current operator learning\nmodels, particularly in data scarce scenarios. Through extensive evaluations across\nfive benchmark operator learning tasks and an application in fatigue modeling,\nPPI-NO consistently outperforms competing methods by a significant margin. The\nsuccess of PPI-NO may introduce a new paradigm in physics-informed machine\nlearning, one that requires minimal physics knowledge and opens the door to\nbroader applications in data-driven physics learning and simulations.",
    "keywords": [
        "Pseudo Physics",
        "Data-Driven Physics Discovery",
        "PDEs",
        "Neural Operator",
        "AI for science",
        "Scientific Machine Learning"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=CrmUKllBKs",
    "pdf_link": "https://openreview.net/pdf?id=CrmUKllBKs",
    "comments": [
        {
            "summary": {
                "value": "This work attempts to provide regularization to deep operator network training by adding a \"pseudo-physics\" component when there is no knowledge of the PDE to inform training. A comprehensive experimental study with ablation is provided."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This is a method that attempts to provide physics regularization to the training of deep operator networks, which are usually trained only from data.\n\nA comprehensive ablation study is provided."
            },
            "weaknesses": {
                "value": "This is a bootstrapping approach where one attempts to learn the physics and then use it to improve training of the operator network over a data-drive baseline. It is not clear how the pseudo-physics constraint helps achieve a better solution. This could happen simply by additional training of the operator network. There is no firm rationale for how this should work.\n\nNo comparison is made with the physics-informed neural operator using the correct physics. The authors' method should give a solution with accuracy between the data-driven and the physics-informed cases, but we do not know how much improvement is made unless we can see what the accuracy of the fully physics-informed operator network is.\n\nThere is some incorrect terminology (see Questions) and incorrect technical statements. For example, in Section 3.1, the authors state that the PDE solution can be obtained through integration of Green's function, but this is only true for linear PDEs.\n\nThe authors assess the additional number of parameters in their model, which is small, but nothing is said about the additional training and inference time incurred. The latter is important because there is a lot of iterative training and refinement in the proposed method."
            },
            "questions": {
                "value": "The approach to learn the physics resembles that in Section IV-B of Zhang et al. \"Deep Learning and Symbolic Regression for\nDiscovering Parametric Equations\". The authors should give that reference and compare their approach to theirs.\n\nWhy do the authors use the acronym \"DONet\" for \"DeepONet\"? The latter is the term widely used in the literature.\n\nOn page 2, the discretized versions of u and f aren't \"collocation points\". That refers to points where a PDE residual is minimized.\n\nStill page 2, the efficiency of FNO does not reside in performing the convolution in the frequency domain, per se, but in learning the parameters in the frequency domain.\n\nOn page 3, it's not clear what the authors mean by having more data by decomposing the 128x128 input in 16,384 points. This is still the same amount of data.\n\nOn page 4, the authors say that the convolution layer is used to compensate for errors in the discretization of the derivatives. How?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Paper introduces pseudo physics-informed Neural operators tailored for complex scenarios where physics is not fully known and data is sparse. It presents a new data-efficient approach to train neural operators via incorporating a pseudo physics-informed module which maps the solution u and its derivative to the source function f using a limited (u, f) pairs. The learned mapping is then used iteratively as part of training a data-driven neural operator. The paper tested the performance of the proposed model against two baselines, namely, FNO and DeepONet across a range of benchmarks and a real-world application. The method is shown to enhance the accuracy of neural operators particularly with limited data."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "Paper is systematically written and easy to read. The idea presented is quite interesting, novel and an important one given the issue of lack of data and partially known physics one often encounters in real world applications. \n\nThe proposed model seems to notably improve the baseline models performance in limited data regimes despite marginally increasing the training time. The results presented sufficiently support the claims made by the authors. \n\nThe effectiveness of the proposed model was assessed in a real-world context scenario in fatigue modeling where no comprehensive PDE exists to fully describe the system. With the use of the pseduo-PI approach and sparse data, the proposed model is able to achieve accurate performance.\n\nThe author have ensured to highlight the limitations of the proposed models (e.g., being opaque and non-interpretable, and not applicable to input functions) which is appreciated."
            },
            "weaknesses": {
                "value": "The framework cannot be used for learning the mapping from the initial condition to the solution and the examples provided are mainly limited to mapping the source function to the solution. \n\nIt will help if the training procedure is described step by step, with one of the examples used in the results. \n\nCiting and differentiating your work from this paper is recommended - https://www.sciencedirect.com/science/article/abs/pii/S0021999120307166"
            },
            "questions": {
                "value": "Is it possible to use other operators (apart from differentiation) in the initial physics learning? For example, introducing operators such as sines, cosines or other complex ones? How about utilizing some concepts from this paper - https://arxiv.org/abs/2207.06240?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose the Pseudo Physics-Informed Neural Operator (PPI-NO), which couples the existing concepts of physics discovery and neural operator learning. In particular, a surrogate partial differential equation (PDE) representation is learned from data using a neural network. Afterwards, the neural-network-PDE model is used as a regularizer to refine the training of the neural operator. The authors claim that the coupling helps the neural operator learn effectively in the low data limit."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is written clearly and has appropriate results to support the authors' claim. Further, the paper proposes the integration of the non-trivial concepts of physics discovery and neural operator learning, which is an important problem."
            },
            "weaknesses": {
                "value": "1.The idea of coupling physics discovery with NN has been explored earlier. For e.g., see PINN-SR [1].\n\n2. The basic idea of the manuscript is problematic. The discovered \"pseudo\" physics is not exact and hence is of much lower-fidelity (and is unlikely to generalize). The data available is of higher fidelity. Therefore a composite loss function where one term is of higher fidelity and the other is of lower fidelity will, in theory, stop the model from generalization. This fact has been previously pointed on in [2] and as a remedy transfer learning was proposed. \n\n3. Even by incorporating rudimentary physics information, a significant decrease in error is not observed in Table 1 (which is not totally unexpected given the point above). In the results of the DONet-Darcy flow, DONet-Diffusion, and all Poisson and advection equations, the reduction in error is minimal, which makes the contribution of the discovered physics marginal.\n\n4. Like any other basis function-based physics-discovery algorithms, this framework also requires careful selection of the derivatives, which limits the proposed framework's applicability. It is evident in Table 1. Even when the training data is increased, the relative error increases instead of decreasing in some cases. This may be due to faulty physics identification. I will also add that since the exact terms are not known, using a L2 loss in generally not preferred (as with L2 error, even those terms that are supposed to be absent will have non-zero weights). This contributes to the error in equation discovery and hence, the accuracy of the overall method.\n\n5. Important aspects like the effect of incorporating physics on the zero-shot prediction on super- and sub-resolutions, as well as generalization to out-of-distribution input, have not been studied. These are required to gauge the strength of the proposed framework correctly.\n\n[1] Chen, Zhao, Yang Liu, and Hao Sun. \"Physics-informed learning of governing equations from scarce data.\" Nature communications 12.1 (2021): 6136.\n[2] Chakraborty S. Transfer learning based multi-fidelity physics informed deep neural network. Journal of Computational Physics. 2021 Feb 1;426:109942."
            },
            "questions": {
                "value": "1. l083. You define f(x) as the source function. I believe neural operators go beyond simply source functions to solution mapping.\n2. l087. \\mathbb{F} and \\mathbb{U} are not defined.\n3. l147. How order of derivatives should be chosen?\n4. Eq. (5). Why generate N' samples in the second term? Instead, why can we not use the available N samples from the first term?\n5. In section 4, important literature in this area are missing. For example, SNO [1], CNO [2], LNO [3], and PIWNO [4] are not reviewed.\n6. l301. Why are the same derivatives not used across all the examples? How are they chosen?\n7. l302. For the SIF example, why are polynomials of the derivatives not used? \n8. l311. What do the iterations denote?\n9. l317. For the SIF example, 400-600 training samples are used. Obtaining such a training set using high-fidelity crack simulations is very costly. This completely defeats the purpose of the proposed framework.\n10. Table 1. Why does the error in DONet-Darcy, DONet-Poisson, and DONet-Advection examples increase with the increase in training data?\n11. In Table 2. The decrease in error in the case of PPI-NO is very marginal. This indicates that the incorporation of rudimentary physics is ineffective in complex problems like the SIF prediction. \n12. l413. Should the baseline comparison be moved to an ablation study in the given setup? Otherwise, the comparison for physics accuracy should be made with dedicated physics discovery algorithms like PINN-SR [5].\n\n\n[1] Fanaskov, Vladimir Sergeevich, and Ivan V. Oseledets. \"Spectral neural operators.\" Doklady Mathematics. Vol. 108. No. Suppl 2. Moscow: Pleiades Publishing, 2023.\n\n[2] Raonic, Bogdan, et al. \"Convolutional neural operators for robust and accurate learning of PDEs.\" Advances in Neural Information Processing Systems 36 (2024).\n\n[3] Cao, Qianying, Somdatta Goswami, and George Em Karniadakis. \"Laplace neural operator for solving differential equations.\" Nature Machine Intelligence 6.6 (2024): 631-640.\n\n[4] Navaneeth, N., Tapas Tripura, and Souvik Chakraborty. \"Physics informed WNO.\" Computer Methods in Applied Mechanics and Engineering 418 (2024): 116546.\n\n[5] Chen, Zhao, Yang Liu, and Hao Sun. \"Physics-informed learning of governing equations from scarce data.\" Nature communications 12.1 (2021): 6136."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In complex systems with minimal information of the underlying physics, it is difficult to model physics based losses. To overcome this issue, the authors propose a surrogate model that learns the inverse mapping between the solution at discrete points, its derivatives and the source term at the corresponding discrete points. This model effectively serves as the \u201cteacher model\u201d for a neural operator framework that learns the solution from the source term. The derivatives of the solution are computed based on numerical differences. It is pseudo physics informed because the operator is trained using the surrogate model rather than loss functions and residuals defined over the actual PDE."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality: The use of the inverse model as the ground truth in cases where the data is sparse and the governing PDE is unknown is quite promising because in a lot of applied settings, it is not always the case that a governing PDE is known. \n\nQuality: The intuition of the paper is quite clear. There are thorough experiments on standard benchmark datasets. The authors perform several ablations to substantiate their claims. The figures clearly indicate the message the authors are trying to convey. \n\nSignificance: This is a novel idea that builds upon the Physics-informed ML literature, combining inverse-PDE estimates into the learning pipeline as an alternative to physics based residuals and losses."
            },
            "weaknesses": {
                "value": "They use the neighborhood information captured within the convolution layers as a way to compensate for errors in numerical differences. A graph neural operator would be both discretization agnostic and would be better for capturing neighborhood information. \n\nThe training dataset seems quite low. It is not clear whether 5 examples indicate 5 instances of the same PDE with different co-efficients or whether it\u2019s 5 different sparse representations, with the same co-efficients. \n\nThe property of neural operator is that it\u2019s discretization agnostic. The authors don\u2019t mention what discretizations they tested. 128x128 grid is not indicative of the discretization, but rather the resolution. By this I mean that this setting could be a set of densely located 128x128 points in a very small area within a large mesh or a set of 128x128 sparse points spread over the entire mesh. \n\nWhile comparing against data driven FNO models is a good baseline, the authors propose this architecture as a substitute for Physics informed ML. Therefore, it would be appropriate to show how this scales against PINNs and PINOs. \n\nIn the FNO paper, the models were trained on training sets with 1000 instances. However, the authors here use a significantly smaller training dataset. Could it be possible that the failure scenarios shown in Figure 3. are because the FNO models require a larger training set to converge? Perhaps a more fair comparison would be to train both the FNO model and the PPI-FNO model on the larger dataset. \nIt seems unreasonable to think that a system is so sparse that the training dataset only has 5 instances. Moreover, it is not clear whether sparsity refers to the size of the training dataset or the number of points within the mesh (sparse discretization)."
            },
            "questions": {
                "value": "1.\tWhen the source terms and the boundary conditions are known, the PDEs can be estimated using Monte-Carlo Walk-on-Spheres (WOS). Neural Walk-on-spheres trains neural networks based on WOS estimates. How does the error rate of the Surrogate model compare against random-walks that accumulate the source term over the green function? \n\n2.\tWhat is the justification for using convolution neural networks as the surrogate model, to capture neighborhood information? A radius based graph neural network is discretization agnostic and works especially well in sparse settings. \n\n3.\tThe surrogate model is not discretization agnostic. The functions sampled would have to be the same discretization as it was trained on. Which would mean that the neural operator model can predict any sparse distribution of points, but the second loss term (i.e. the surrogate model) has to be a fixed discretization. This seems like a bottleneck. Were there reasons for not making the second model a neural operator. Perhaps using [1] would be a good way to ensure operator learning through the entire pipeline. \n\n[1] Wang, Tian, and Chuang Wang. \"Latent Neural Operator for Solving Forward and Inverse PDE Problems.\" arXiv preprint arXiv:2406.03923 (2024)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper adapts physics-informed neural operators to settings without knowledge of the underlying PDE by approximating while learning the neural operator. This is then shown to improve the performance of neural operators in the case of scarce data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The approach is interesting, novel, and well-reasoned.\n- The ablation studies for different components are appreciated.\n- Thorough evaluations show the effectiveness of the method.\n- Clear presentation, easy to follow"
            },
            "weaknesses": {
                "value": "- The presented approach can only learn the operator mapping the source function to the solution function. That this is not the standard operator learning problem setting (as for example discussed in the FNO paper) is only mentioned in the limitations section at the end of the appendix. It would be helpful to mention the focus during the problem formulation already and put the limitation section in the main paper.\n- The training set sizes seem random and sometimes do not cover a broad range. This is the most significant for the SIF dataset. It would have been interesting to see different experiments with dataset sizes covering a broader range like 10, 100, 1000 \n- Timing: It would be interesting to have an actual time comparison between the methods. Furthermore, since the idea is that this idea saves time as less data has to be generated, a comparison to the time it takes to compute more data would be interesting.\n\nMinor Notes:\n- You often write feedforward layer/network, when I think you mean fully-connected layer/network. A convolutional layer is, for example, also a feedforward layer.\n- p.4: You state that you use \"numerical difference\" to compute the derivatives. Do you mean finite differences?\n- Eq. 4: p(f) is not defined\n- Figure 2 seems to be too early\n- p. 10, l.521: \"the best choice \\[of $\\lambda$\\] is often in between\" In between what? This seems to be a very vague statement"
            },
            "questions": {
                "value": "- The experiments in Table 3 and and Table 4a seem very similar. Can you explain the main difference (apart from including FNO) is?\n- While I understand that it is not the idea to use PPI-FNO when the PDE is known, it would be interesting to see a comparison between PPI-NO und PI-NO, to learn about the loss by approximating the PDE instead of using the correct one. Can you run some experiments with PI-NO for a comparison?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a way to train a solution operator for partial differential equations applicable when only few data points are available. For this, they first apply a system identification technique to approximate the underlying partial differential equation and then use this equation for physics-informed training of their solution operator. They evaluate their method on 5 non-trivial partial differential equations.\n\nSystem identification is a well-studied field and physics-informed training of neural operators has been done before, as the authors correctly describe. The novelty lies in their combination to train a solution operator with only few data points. To my knowledge, this has indeed not been done before. The idea is simple and makes sense.\n\nI think the biggest weakness is that the straight-forward way to train a solution operator with few data is not discussed and therefore, also not part of their experiments: After the PDE is approximated, one could use this to generate new data and train the solution operator in a supervised setting with many data points. I would expect this to work better than the author\u2019s method since neural networks are easier to optimize with data sets than with a physics-informed loss. Their only baseline is supervised training with few data points. As I expect training on few data points to require much less time than physics-informed training, I don't consider this a fair comparison. A convincing benchmark would require reporting the actual time spent on training and then further spent the same amount of time on a decent baseline, for instance, the one outlined above. For example, half of the time for the generation of further solution-source terms, and the other half for training the solution operator. \n\nAnother issue is that the author's method is basically using two other techniques from two different subfields in sequence rather than coupling the underlying principles into an improved method. While this is not inherently negative, it raises the expectation for a more generalized argumentation. For instance, including more than one specific technique for each of the subfields, such as another technique for system identification. Or making a theoretical argument about how the individual errors of the PDE approximation in the first step and the solution operator approximation add up to the total error. \n\nSome minor points: \n- I think many details on the experiments (convolutional kernel size, activation functions, frameworks used,...) should be moved to the appendix\n- Equation 2 indicates that the first neural network (denoted by phi in the paper) acts on quantities at a specific spatial point while Figure 1 indicates phi acts on entire fields on the spatial domain. This should be clarified. \n- The related work section should mention some works on system identification."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "See above."
            },
            "weaknesses": {
                "value": "See above."
            },
            "questions": {
                "value": "- The abstract states that the method \u2018enhances the accuracy of current operator learning\nmodels, particularly in data scarce scenarios\u2019.  As I understand the paper, the presented method is  suited only for data-scarce scenarios.\n- Why are physics-informed losses introduced as a regularization technique? (in the introduction)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}