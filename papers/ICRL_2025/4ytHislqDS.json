{
    "id": "4ytHislqDS",
    "title": "IFORMER: INTEGRATING CONVNET AND TRANSFORMER FOR MOBILE APPLICATION",
    "abstract": "We present a new family of mobile hybrid vision networks, called iFormer, with a\nfocus on optimizing latency and accuracy on mobile applications. iFormer effectively\nintegrates the fast local representation capacity of convolution with the efficient\nglobal modeling ability of self-attention. The local interactions are derived\nfrom transforming a standard convolutional network, i.e., ConvNeXt, to design a\nmore lightweight mobile network. Our newly introduced mobile modulation attention\nremoves memory-intensive operations in MHA and employs an efficient\nmodulation mechanism to boost dynamic global representational capacity. We\nconduct comprehensive experiments demonstrating that iFormer outperforms existing\nlightweight networks across various tasks. Notably, iFormer achieves an\nimpressive Top-1 accuracy of 80.4% on ImageNet-1k with a latency of only 1.10\nms on an iPhone 13, surpassing the recently proposed MobileNetV4 under similar\nlatency constraints. Additionally, our method shows significant improvements in\ndownstream tasks, including COCO object detection, instance segmentation, and\nADE20k semantic segmentation, while still maintaining low latency on mobile\ndevices for high-resolution inputs in these scenarios. The source code and trained\nmodels will be available soon.",
    "keywords": [
        "Lightweight Networks",
        "Efficient Networks",
        "Vision Transformers",
        "Classification"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-16",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4ytHislqDS",
    "pdf_link": "https://openreview.net/pdf?id=4ytHislqDS",
    "comments": [
        {
            "summary": {
                "value": "This paper designs iFormer, a new family of efficient mobile vision networks combining ConvNet and Transformers. The iFormer evolves from ConvNeXt with a series of efficiency designs. \n\nSingle-Head Modulated Attention(SHMA) is proposed as substitutional Transformer blocks to replace part of the Conv blocks in later stages of the enhanced ConvNeXt. SHMA replaces multi-head attention with single-head attention to improve efficiency and introduces a modulation mechanism to boost performance. \n\nThe resulting iFormer series achieves the best performance compared with state-of-the-art mobile-level models on different downstream tasks with lower latency."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper is well-organized and easy to follow. Detailed design specifications and comprehensive experiments enhanced the integrity of the article and demonstrated its contributions.\n\nThe main contribution, SHMA, provides a new approach to designing efficient attention and Transformer blocks. The resulting iFormer series outperforms sota baseline mobile networks with stronger performance and lower latency."
            },
            "weaknesses": {
                "value": "W1: \n\nThe motivation and necessity of substituting half of the conv blocks at the third stage and all blocks at the last stage into Transformer blocks in ConvNeXt are still not very clear. From Figure 2, changing the conv blocks into SHA blocks gains a 0.4% improvement in performance but is also 0.12 ms (about 10%) slower. I'd like to know further explanation for this design and ablation studies on the choice of stages or different ratios of Conv versus Transformer blocks if possible. \n\n\nW2:\n\nAccording to the citation of SHViT in this paper, I suppose the SHA refers to the Single-head self-Attention in SHViT design. But in Figure 4, full channels of input (CxHxW) are projected to Q/K/V (CxL) which does not align with the design of SHA in SHViT but looks like the traditional definition of Single-head attention that performs a self-attention on all channels of input using a single head. \n\nConsidering there are limited words about the details of SHA in this paper, I would expect further specification of which SHA is used in iFormer and comply with the pipeline figure accordingly.  \n\nW3:\n\nIn this paper, the additional reshaping operations in MHA are considered as the reason for the slower inference speed compared with SHA. But multiple factors have an impact on the runtime speed difference and there's no evidence to support the extra runtime only or mainly comes from extra reshapings. \n\nFirst, depending on the code implementation, replacing MHA with Single-head self-Attention may remove the reshaping operation in self-attention, but also introduce additional split and concat operations. And generally, split and concat operations cost more memory and are slower than reshape. \n\nSecondly, SHA applies self-attention on fewer channels, which largely reduces the computational cost and speeds up runtime. \n\nTherefore I suggest the authors conduct an ablation study or provide empirical evidence to isolate the impact of reshaping operations versus other factors like split/concat operation and reduced self-attention channels on the inference speed. This would help clarify the main factors contributing to SHA's efficiency and provide a more comprehensive understanding of the proposed method."
            },
            "questions": {
                "value": "1. What is the motivation and justification for the necessity of the design that replaces half of the third stage and full last stage conv blocks with transformer blocks? Please refer to Weaknesses 1. \n\n2. I wonder what is the performance and latency of MHA as the missing step between 'kernel sz.' and 'SHA' in Figure 2."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new family of mobile hybrid vision networks. By integrating the rapid local representation capability of convolution with the efficient global modeling ability of self-attention, the proposed architecture, iFormer, achieves significant performance in classification and several downstream tasks, while maintaining low latency on mobile devices for high-resolution inputs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The study of model architecture could inspire further exploration in designing more efficient architectures.\n2. The paper is well-organized and easy to follow."
            },
            "weaknesses": {
                "value": "1. In Table 1, iFormer-S achieves the same latency as RepViT-M1.0 with slightly fewer parameters, yet in larger variants, iFormer achieves lower latency with substantially more parameters compared to RepViT. What is the reason for this difference?\n2. Some studies are not included in the comparison or the related wotk section, such as [1, 2].\n\n[1] Cmt: Convolutional neural networks meet vision transformers.\n\n[2] Learning efficient vision transformers via fine-grained manifold distillation."
            },
            "questions": {
                "value": "please refer to weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a mobile hybrid vision network, iFormer. The paper goes from ConvNeXt to a lightweight mobile network. iFormer removes memory-intensive operations in MHA and employs an efficient modulation mechanism. The author conduct standard benchmark experiments on ImageNet, COCO and ADE20K."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I think the logic of exploration in this article, starting with ConvNeXt, first \u201clightening\u201d the ConvNeXt to create a streamlined\nlightweight network, then exploring the attention module, is reasonable. \n\nI think the analysis about \u201ccosine similarity between multiples\u201d proves that using a single attention is good and worth supporting. \n\nI think the experiment reported in this paper is comprehensive (imagenet, coco, ade-20k). The paper also reports some knowledge distillation results, which is suitable in mobile network papers."
            },
            "weaknesses": {
                "value": "1:  \nSingle head self-attention has been conducted in \"Shvit: Single-head vision transformer with memory efficient macro design\" .\n\nAlternative to standard self-attention has been conducted in GhostNetV2.\n\nModulation in the token mixer module has been conducted in Conv2Former. \n\nThis paper references many related methods, and while that is one approach, I don't think it stands out. Although such research is a decent format, I believe it impacts the novelty of this paper. \n\n2: \nThe process of evolving from the ConvNeXt baseline to the lightweight iFormer may not apply to slightly larger models, and some steps show very minimal improvements, making them hard to justify."
            },
            "questions": {
                "value": "The largest model shown by iFormer, iFormer-L, is only about 15M, which isn\u2019t considered large, even for edge devices, especially since recent edge LLMs can reach 1B parameters. I wonder how well a larger iFormer (around 100M) would perform."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a new family of mobile hybrid vision networks, called iFormer,  by integrating the fast local representation capacity of convolution with the efficient global modeling ability of self-attention."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is easy to follow, with clear writing and presentation.\n2. Evaluation results are comprehensive."
            },
            "weaknesses": {
                "value": "1. How does this method compare with neural architecture search (NAS) methods?\n\n2. How does the designed model perform on other mobile devices, such as NVIDIA Jetson Nano or Raspberry Pi?"
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Summary\n\nThis paper proposes a mobile friendly vision network that improves the latency and accuracy by combining the strengths of both CNNs and ViTs. The novel aspect of this work is the single head modulation self-attention (SHMA). This SHMA learns spatial context through optimized self-attention. It takes ConvNext as the base model and improves it further with various techniques. The authors streamline the ConvNeXt architecture, making it suitable for real-time use on mobile devices, such as the iPhone 13, focusing on reducing latency rather than FLOPs or parameter count. The combined techniques lead to more than 80% top-1 accuracy with 1.1ms latency on iphone 13. Overall a great contribution to the research community."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. fast local representation capacity of convolution and the efficient global modeling proficiency of the proposed SHMA\n2. A series of novel techniques such as stack of overlapping convolution instead of aggressive non-overlapping patch in the early layers\n3. The model is structured in four stages. The early stages use fast convolution to capture local features efficiently, using a modified and lightweight version of ConvNeXt optimized for mobile latency.\n4. In the lower-resolution stages, self-attention is used to model long-range dependencies. To address the challenges of traditional multi-head self-attention (MHA), the authors propose SHMA, which uses a single-head attention mechanism to minimize memory costs while retaining high performance. SHMA reduces latency by optimizing reshaping operations and leveraging spatial context interactions. SHMA is combined with a parallel feature extraction branch to enhance feature representation. The outputs from both branches are fused to enable dynamic information exchange, mitigating any performance drop caused by simplifying MHA"
            },
            "weaknesses": {
                "value": "1. What is the runtime complexity of iFormer network?\n2. When running on iPhone (mobile device), what is the peak memory consumption? \n3. How long the iPhone charge will last if an iFormer based app is run on certain fps?"
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}