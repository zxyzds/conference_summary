{
    "id": "Pbz4i7B0B4",
    "title": "Towards Continuous Reuse of Graph Models via Holistic Memory Diversification",
    "abstract": "This paper addresses the challenge of incremental learning in growing graphs with increasingly complex tasks. The goal is to continually train a graph model to handle new tasks while retaining its inference ability on previous tasks. Existing methods usually neglect the importance of memory diversity, limiting in effectively selecting high-quality memory from previous tasks and remembering broad previous knowledge within the scarce memory on graphs. To address that, we introduce a novel holistic Diversified Memory Selection and Generation (DMSG) framework for incremental learning in graphs, which first introduces a buffer selection strategy that considers both intra-class and inter-class diversities, employing an efficient greedy algorithm for sampling representative training nodes from graphs into memory buffers after learning each new task. Then, to adequately rememorize the knowledge preserved in the memory buffer when learning new tasks, we propose a diversified memory generation replay method. This method first utilizes a variational layer to generate the distribution of buffer node embeddings and sample synthesized ones for replaying. Furthermore, an adversarial variational embedding learning method and a reconstruction-based decoder are proposed to maintain the integrity and consolidate the generalization of the synthesized node embeddings, respectively. Finally, we evaluate our model on node classification tasks involving increasing class numbers. Extensive experimental results on publicly accessible datasets demonstrate the superiority of DMSG over state-of-the-art methods.",
    "keywords": [
        "Incremental learning",
        "Model Reuse",
        "Transfer learning"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Pbz4i7B0B4",
    "pdf_link": "https://openreview.net/pdf?id=Pbz4i7B0B4",
    "comments": [
        {
            "summary": {
                "value": "This paper addresses the problem of incremental learning in growing graphs with new-coming node classes. It introduces a novel holistic Diversified Memory Selection and Generation (DMSG) framework containing two modules: (1) a buffer selection strategy that considers both intra-class and inter-class diversities, with a greedy algorithm; and (2) a diversified memory generation replay method with variational generation, with integrity loss and reconstructed loss under adversarial learning. Experiments over evolving graphs on node classification tasks could verify its effectiveness, along with ablation study over memory generation replay module."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1. [Clear Motivation & Descriptions] The paper provides a well-defined motivation for addressing continual learning on growing graphs, focusing on memory selection and memory reply effectiveness. The reason for developing the proposed method can be compelling, along with clear and detailed descriptions of the problem background and definition, making it accessible and understandable.\n\nS2: [Rationale Methodology] The rationale for the DMSG approach is well-grounded, combining heuristic buffer selection and generative memory reply to broaden the replay capability. The methodology introduces two strategies: first, a buffer selection algorithm that maximizes intra-class and inter-class diversity; second, a variational generation layer that synthesizes embeddings for effective replay, which sounds rational and novel to me.\n\nS3. [Theoretical and Empirical Experiments] The paper validates its approach with both theoretical and empirical support, for instance, theoretical analysis to demonstrate the importance of effective buffer selection. Experimentally, the DMSG framework is tested on multiple evolving graph datasets, a good improvement over the AF/% metric."
            },
            "weaknesses": {
                "value": "Here are still some questions mainly regarding the buffer selection part and the variational part:\n\nW1: For the buffer node selection part, are the buffer nodes only node-set or graph-structural data, if graph data, how to connect these nodes? and if node-set, how to directly feed it into GNN in Figure 2?  Further clarification on this would be appreciated.\n\nW2: How many nodes are typically selected in the buffer for each class? How experimental results would change along with the varying number of nodes in buffers?\n\nW3: In the ablation study, the effectiveness of the buffer node selection is not verified. For instance, how to illustrate the effectiveness of the proposed heuristic greedy method? \n\nW4: Why the variational layer in the memory replays could broaden knowledge from the buffer? Does that mean, it generates something new $\\hat{Z}$ from the original Z? Additionally, the minimization term in equation (7) is difficult to interpret\u2014could you provide a more detailed explanation or expanded expressions for this term?\n\nW5: With the introduction of complex mechanisms for memory selection and replay, how do the time and computational costs of the proposed method compare to other baseline approaches in the experiments?\n\nW6: Why have a direct assumption that \"Let the loss function L(\u03b8, x) be \u03b2-Lipschitz continuous in respect to the input x\" in Theorem 1. ? And how can we evaluate whether the selected nodes in buffers are with diversity?"
            },
            "questions": {
                "value": "See Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel approach called Diversified Memory Selection and Generation (DMSG) for incremental learning in growing graphs. The authors address the challenge of continually training graph models to handle new tasks while retaining knowledge from previous tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The developed model is supported by a theoretical foundation. The authors provide a theoretical analysis to support the importance of buffer diversity in incremental learning scenarios.\n\nThe studied problem is important. The authors try to tackle the issues of limited buffer size and potential overfitting through their diversified memory generation approach.\n\nclear visualization and presentation. The authors use well-structured figures to convey the incremental learning process, memory selection, and generative replay methods in a relatively straightforward approach."
            },
            "weaknesses": {
                "value": "Lack of novelty. This paper largely builds on existing concepts of memory replay and buffer selection strategies that are already well-established in incremental learning and continual learning literature. While the paper proposes a diversified memory selection and generative replay approach, the techniques, such as adversarial learning and variational embedding, have been previously applied in other contexts. The paper simply combines different losses to facilitate learning.\n\nLack of baselines. The following papers should be discussed and compared for experiments:\nTowards robust graph incremental learning on evolving graphs. ICML 2023\nReplay-and-Forget-Free Graph Class-Incremental Learning: A Task Profiling and Prompting Approach. NeurIPS 2024\nOn the Limitation and Experience Replay for GNNs in Continual Learning. CoLLAs 2024\nTopology-aware Embedding Memory for Continual Learning on Expanding Networks. KDD 2024\nPUMA: Efficient Continual Graph Learning for Node Classification With Graph Condensation. TKDE 2024\nGraph Continual Learning with Debiased Lossless Memory Replay. ECAI 2024\nAccordingly, it is encouraged that the authors discuss the difference between their proposed model and these baselines.\n\nLack of comprehensive datasets. What about the performance of the proposed model on Citeseer, Pubmed, and PPI? How about Collab, IMDB, Proteins, and NCI1?\n\nLack of hyperparameter sensitivity study. The method introduces several hyperparameters such as \u03bb1, \u03bb2, and \u03bb3 for balancing the loss. A discussion on their impact and guidelines for tuning these hyperparameters would be valuable. More in-depth analyses about balancing these losses to understand the contribution and reasons for developing them are encouraged."
            },
            "questions": {
                "value": "see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies incremental learning on growing graphs, aiming to continually train a model on new tasks without loosing the inference ability on previous tasks. The method proposed in this work, namely holistic diversified memory selection and generation (DMSG), is a memory based technique, with a focus on improving the data diversity in the memory, which is neglected by the existing works. The unique design of DMSG comes from two perspectives. First, it proposes a novel method to sample the data, second, instead of directly using the buffered data, DMSG train a generator to generate the data to replay.\n\nOverall, this method proposes a novel memory based technique, and the experimental results justify the effectiveness of the method. However, there are also some major problems regarding the theoretical analysis, as listed below in the weakness part."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is overall well written, and the main idea is clearly conveyed.\n\n2. Experiments are comprehensively conducted on multiple benchmark datasets against multiple baselines.\n\n3. According to the results, the proposed method consistently outperform the baselines, although the improvement on some datasets are negligible."
            },
            "weaknesses": {
                "value": "1. The theoretical analysis is not rigorous enough. Theorem 1 reveals that smaller distance between the memory data distribution and the full data distribution indicates that the loss computed based on memory can more closely mirrors the expected over the full previous data. This is a straightforward and reasonable conclusion. But what follows is not convincing enough. First, it is assumed that the distributions of the memory data and full data is Gaussian. I guess this is acceptable, although a little bit arbitrary. Next, it is stated that the buffered data is typically less diverse, therefore increasing its diversity could make it closer to the full data distribution. This is not convincing, since the 'diversity' here refers to the covariance. Why is the buffered data has a smaller variance than the real (full) distribution? This should depends on the sampling strategy instead of the size of the set. Moreover, although the diversity here refers to the covariance, in the following of the paper, the diversity is measured by the average distance among the data, which is inconsistent.\n\n2. The implemented diversity maximization strategy is inconsistent with the theoretical motivation, as mentioned in the first weakness point."
            },
            "questions": {
                "value": "1. Please explain more on the probability distance used in Section 3.1, e.g. the specific formulation.\n\n2. Is the replay data generator trained from scratch each time when a new task comes in? Or is the generator train each time when a new task comes in, but is initialized from the generator from the previous task?\n\n3. If the generator is trained every time when a new task comes in, will this induce significant extra computational burden?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a memory replay approach to address the challenges of graph incremental learning. Specifically, the proposed method includes a diversified buffer selection module and a generative memory replay module to prevent the model from forgetting previous tasks when learning new tasks. The experimental results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The studied problem is practical and interesting.\n\nThe paper is well-written and easy to understand.\n\nThe proposed method introduces novel approaches for buffer selection and memory utilization."
            },
            "weaknesses": {
                "value": "1. The title presented on the website is not the same as the one in the paper\n2. The citation style is not used appropriately. Most of the citations should be in parenthesis using \\citep{}.\n3. The authors are encouraged to include the time consumption of different memory selection strategies.\n4. The statements in lines 178-181 are not clear. Please revise it to make it clearer.\n5. The definition of L_{adv} is not clear in line 334. Moreover, there is no Eq.A.3.\n6. The analysis of parameter sensitivity is missing.\n7. The comparisons to more recent baselines are encouraged."
            },
            "questions": {
                "value": "Please see the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}