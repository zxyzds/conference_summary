{
    "id": "m8yby1JfbU",
    "title": "Is Your Video Language Model a Reliable Judge?",
    "abstract": "Evaluating Video Language Models (VLMs) is crucial for advancing their capabilities in understanding complex video content. Traditional evaluation methods often rely on single models, which can be biased due to their training data and architecture or susceptible to hallucinations, thereby undermining their reliability. A straightforward approach is to apply the principle of collective intelligence, aggregating judgments from multiple VLMs to enhance evaluation reliability. This study investigates the efficacy of collective thought approaches in VLM evaluation, particularly when the pool of judges includes both reliable and unreliable models. Contrary to expectations, our findings reveal that incorporating collective judgments from a mix of reliable and unreliable VLMs does not necessarily enhance the accuracy of the final evaluation outcomes. The inclusion of less reliable models introduces noise and biases that can overshadow the potential benefits of aggregation, leading to evaluations that may be less accurate than those from individual reliable models. These findings emphasize the limitations of collective intelligence approaches in VLM evaluation and highlight the need for more advanced methods that can accurately account for the reliability of individual models. Our research contributes to the development of more robust evaluation frameworks for VLMs, promoting accuracy and fairness in evaluation of VLMs.",
    "keywords": [
        "Video Language Models",
        "Model evaluation",
        "Collective thought",
        "Reliability"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=m8yby1JfbU",
    "pdf_link": "https://openreview.net/pdf?id=m8yby1JfbU",
    "comments": [
        {
            "summary": {
                "value": "This paper explores an important question in the evaluation of Video Language Models (VLMs): the impact of collective intelligence approaches on evaluation accuracy, especially when combining reliable and less reliable VLMs. This is a relevant topic given the growing reliance on VLMs in various applications where understanding complex video content is essential."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Novelty of Approach: The manuscript presents a unique perspective by applying collective intelligence principles to VLM evaluation, attempting to leverage multiple models to mitigate biases present in individual models. This approach is innovative and relevant, as it challenges existing evaluation methodologies that typically rely on single-model assessments.\nAnalysis of Collective Judgment: The authors' findings regarding the drawbacks of combining reliable and unreliable models are noteworthy. These results caution against blind aggregation in evaluation, highlighting the risk of introducing noise and bias, which can degrade the final evaluation\u2019s reliability."
            },
            "weaknesses": {
                "value": "While the paper addresses an important topic, the methodology could be more detailed. Readers may benefit from a clearer explanation of how model reliability was determined and how judgments were aggregated. More specifics on the metrics and statistical techniques used would also strengthen the study\u2019s transparency."
            },
            "questions": {
                "value": "Great work\uff01I have no further questions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work aims to explore the effectiveness in the evaluation of Video Language Model (VLM), especially focus on collective thinking methods.  Traditional evaluation methods usually rely on a single model, which is susceptible to biases in training data and structure and limiting reliability. This work proposes to take different advanced VLMs and LLMs as judges to evaluate video question-answer pairs. An \"evaluation-judgment\" framework was implemented, where judges gave scores based on accuracy and relevance on a 1 to 5 scale. The advanced model GPT4o was used to integrate the preliminary review results of VLMs to generate the final evaluation. A mixed judge strategy was used to select the most reliable VLMs to contribute to the final evaluation of each visual dimension based on the weighted Cohen's Kappa score."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This work is well-written and easy to follow. \n2. The experimental results list the performance comparison of different models in various visual dimensions, including social context, emotional context, object instance count, etc. The scores of models such as GPT4o, GPT3.5, Agent Debate, etc. are compared to show their performance in each visual dimension. The performance scores of some models such as Llama-vid, GPT4o-mini, Internvl2, etc. are shown in table form."
            },
            "weaknesses": {
                "value": "1. There are so many new benchmarks for evaluate the VLM recently, and what is best benchmark for evaluating VLM.\n2. This work proposed a new method for evaluating VLM, but this area need an evaluation benchmark(system) rather than just a new method.\n3. Overall, even though this work proposes a good method and explore new insight, I think the contribution of this work is not enough for evaluating VLMs."
            },
            "questions": {
                "value": "Please refer to Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates whether Video LLMs (VLMs) can reliably evaluate other VLMs' performance, and explores if using multiple VLMs collectively as judges improves evaluation reliability. \n\nFour different judging methods are studied in the paper:\n\n**Review by LLM Agents Debate (Reference-guided grading)**: providing text LLMs with the reference answer and the generated answer and scoring it   \n**Review by individual VLM:** A single Video LLM reviews the results from the test model  \n**Review with Collective Thoughts:** A strong meta-judge Video LLM is provided with scores generated by individual Video LLM judges.  \n**Mixture of Judges:** A score is generated by weighing the scores predicted by individual Video LLMs on the basis of their performance on the specific question type\n\nThe CVRR-ES dataset containing 2,400 question-answer pairs from 217 videos, distributed across 11 different visual dimensions (e.g., multiple actions, social context, emotional context) is used for the evaluation. Weighted Cohen's Kappa is used to measure agreement between different evaluation methods.\n\nThe key finding is that current Video LLMs (except GPT4o) are not reliable enough to be used as standalone evaluators. GPT-4o, when used as a sole judge, outperformed its performance when combined with less reliable judges."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "As many recent works have utilized LLMs and VLMs are judges to evaluate other models, a work exploring the effectiveness of these judges is an interesting topic and of significant potential utility to the community."
            },
            "weaknesses": {
                "value": "- The paper is poorly structured and written, the different methods studied are not clearly separated and are mixed up in different sections  \n- The conclusions are based on a single not commonly used dataset, and are unusually strong, its possible that methods like mixture of judges not performing well on this setting is a result of this dataset, and not an universal fact.  \n- The overall presentation of the data is poor, the paper is full of radar charts with 11 dimensions, however there\u2019s often no real difference in the dimensions   \n- The quality of the plots is often poor, lacking visual clarity, making it hard to understand the data. I suggest the authors generate the figures in some kind of vector graphics format like pdf instead of generating jpeg images."
            },
            "questions": {
                "value": "I believe this paper is not quite ready for submission and requires quite a bit more work.\n\n1. Having results on more than one dataset would probably be the single biggest potential improvement. \n2. Re-organizing and rewriting methods section of the paper would be a necessity"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper argues that current VLMs, except for GPT-4o, are generally unreliable as judges for video understanding tasks. The study reveals that collective thoughts, which combine judgments from both reliable and unreliable VLMs, do not necessarily improve evaluation accuracy. Even a proposed mixture of judges offers only minor improvements, underscoring the need for more sophisticated methods to account for individual model reliability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper provides detailed experimental results, including both quantitative tables and visual comparisons, which effectively illustrate the trends.\n- It is written clearly and easy to follow, making the results seem reproducible.\n- The analysis highlights the limitations of relying on collective thought when using unreliable VLMs, contributing to the broader discussion on VLM evaluation."
            },
            "weaknesses": {
                "value": "- The paper contains several typographical errors, particularly in how different VLM names are written (e.g., inconsistencies with capitalization and dashes).\n- The conclusion that VLMs, other than GPT-4o, are unreliable as judges isn't entirely convincing. It would be helpful to provide more insights or potential solutions to address this issue or offer some speculative methods to improve VLMs as judges.\n- The use of \"hallucination\" as a primary explanation for unreliable performance could be expanded with alternative explanations or further justification.\n- This paper concludes that VLMs are not yet mature enough to serve as reliable judges for video understanding tasks. However, it lacks a thorough explanation or well-supported evidence to substantiate this claim. The observations and insights presented could be expanded to provide a clearer understanding of why VLMs fall short and what specific factors contribute to their unreliability."
            },
            "questions": {
                "value": "- Have you considered how the advanced judge processes the original video input and the VLM reviews? Is the final review a consolidation of the VLM assessments or a mixture with the advanced judge\u2019s review as well?\n- Could the selection of reliable judges in real-time (inference-time) be achieved without using pre-computed weighted scores (rely on LLM agent debate results, which require reference responses)? It might be useful to explore methods that remove outliers more efficiently and enable VLMs to work independently as judges.\n- Could the tendency of VLMs to give high scores be a result of their misunderstanding of the content, rather than an inherent bias towards favorable evaluations?\n- Is it possible that the use of Weighted Cohen's Kappa as a metric for selecting reliable judges is not optimal, leading to minimal performance improvements?\n- The choice of using the Weighted Cohen's Kappa metric and the specific implementation of collective thought for VLMs as judges are not fully justified. A clearer rationale for the implementation would strengthen the validity of the approach."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}