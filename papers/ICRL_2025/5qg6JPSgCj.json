{
    "id": "5qg6JPSgCj",
    "title": "Score-based free-form architectures for high-dimensional Fokker-Planck equations",
    "abstract": "Deep learning methods, which incorporate the PDE residual as loss function, have recently emerged to solve the Fokker-Planck equations. In the implement, proper normalization condition is required to avoid a trivial solution. However, specific network architectures may limit representation capacity, and soft regularization term requires careful balancing of multi-objective loss function. In this paper, we propose a novel framework: Fokker-Planck neural network (FPNN) that adopt a score PDE loss to decouple the score learning and the density normalization into two stages. Our method is mesh-free and causality-free, allowing for free-form network architectures to strictly satisfy normalization constraints by computing the partition function only once. We demonstrate the effectiveness on various high-dimensional steady-state Fokker-Planck (SFP) equations, achieving superior accuracy and over a 20$\\times$ speedup compared to state-of-the-art methods. Without any labeled data, FPNN achieve average relative errors of 11.36\\%, 13.87\\% and 12.72\\% for 4D Ring, 6D Unimodal and 6D Multi-modal problems respectively, requiring only 256, 200, and 980 parameters. Experimental results highlights the potential as a universal fast solver for handling more than 20-dimensional SFP equations, with great gains in efficiency, accuracy, memory and computational resource usage.",
    "keywords": [
        "Fokker-Planck Equations",
        "Normalization Condition",
        "Score Model",
        "Physical Constraints."
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "We propose a score PDE loss to decouple the normalization condition, allowing for free-form architectures in solving  high-dimensional Fokker-Planck equations.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=5qg6JPSgCj",
    "pdf_link": "https://openreview.net/pdf?id=5qg6JPSgCj",
    "comments": [
        {
            "comment": {
                "value": "Thank you for your insightful and kind comments! We are glad to hear that you recognize and appreciate our ideas.\n\nWe fully agree with your second point. You have accurately and succinctly summarized the key contributions of our work. As for the first point, there are several studies on adaptive sampling methods for PINNs [1-6], which usually involve resampling or adding training points based on the PDE residuals. Thus, data from the diffusion process could also be used to compute the PINN loss $\\mathcal{J}_{\\text{plain}}$. But it does not fundamentally address the core challenges in solving high-dimensional Fokker-Planck equations and is less natural than score matching.\n\nWe also recommend that you consider the numerical benefits of FPNN over PINN in learning high-dimensional densities (*Advantage 2* in our introduction). For instance, in the *10D Gaussian Mixture* problem, the magnitude of the true solution is around $10^{-5}$. If PINN is trained using the objective function $\\mathcal{J}\\_{\\text{plain}} + \\lambda \\mathcal{J}\\_{\\text{norm}}$ with an MLP architecture, it is challenging for the model to produce outputs of that magnitude, especially without labeled data or prior knowledge for post-standardization.\n\nIn contrast, FPNN leverages a score-based model, allowing the network to freely choose an appropriate scale and learn the unnormalized density $\\widetilde{p_\\theta}$. By calculating normalizing constant $Z_\\theta$ through a post-processing step, the model finally yields $p_\\theta(x) = \\widetilde{p_\\theta}(x) / Z_\\theta$, thereby enabling outputs on the scale of $10^{-5}$. This significantly mitigates the numerical difficulties inherent in high-dimensional Fokker-Planck equations.\n\n**References**\n\n[1] @article{lu2021deepxde,\n  title={DeepXDE: A deep learning library for solving differential equations},\n  author={Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},\n  journal={SIAM review},\n  volume={63},\n  number={1},\n  pages={208--228},\n  year={2021},\n  publisher={SIAM}\n}\n\n[2] @article{nabian2021efficient,\n  title={Efficient training of physics-informed neural networks via importance sampling},\n  author={Nabian, Mohammad Amin and Gladstone, Rini Jasmine and Meidani, Hadi},\n  journal={Computer-Aided Civil and Infrastructure Engineering},\n  volume={36},\n  number={8},\n  pages={962--977},\n  year={2021},\n  publisher={Wiley Online Library}\n}\n\n[3] @article{wang20222,\n  title={Is $L^2$ Physics Informed Loss Always Suitable for Training Physics Informed Neural Network?},\n  author={Wang, Chuwei and Li, Shanda and He, Di and Wang, Liwei},\n  journal={Advances in Neural Information Processing Systems},\n  volume={35},\n  pages={8278--8290},\n  year={2022}\n}\n\n[4] @article{wu2023comprehensive,\n  title={A comprehensive study of non-adaptive and residual-based adaptive sampling for physics-informed neural networks},\n  author={Wu, Chenxi and Zhu, Min and Tan, Qinyang and Kartha, Yadhu and Lu, Lu},\n  journal={Computer Methods in Applied Mechanics and Engineering},\n  volume={403},\n  pages={115671},\n  year={2023},\n  publisher={Elsevier}\n}\n\n[5] @article{hou2023enhancing,\n  title={Enhancing PINNs for solving PDEs via adaptive collocation point movement and adaptive loss weighting},\n  author={Hou, Jie and Li, Ying and Ying, Shihui},\n  journal={Nonlinear Dynamics},\n  volume={111},\n  number={16},\n  pages={15233--15261},\n  year={2023},\n  publisher={Springer}\n}\n\n[6] @article{tang2023adversarial,\n    title={Adversarial Adaptive Sampling: Unify {PINN} and Optimal Transport for the Approximation of {PDE}s},\n    author={Kejun Tang and Jiayu Zhai and Xiaoliang Wan and Chao Yang},\n    journal ={The Twelfth International Conference on Learning Representations},\n    year={2024}\n}"
            }
        },
        {
            "comment": {
                "value": "Thank you for answering my questions. Now I have a better grasp of the overall idea of this paper. I was impressed by the idea of connecting score matching with solving FP equations, and now I'm convinced it is also more advantageous over PINN. I will change my score when permission is given to me and recommend for **accept**. To update my evaluation:\n\nSoundness: 3: good\n\nPresentation: 3: good\n\nContribution: 3: good\n\nThe main contribution of this paper is to solve high-dimensional Fokker-Planck equations by score matching loss, which has two advantage that PINNs do not possess:\n1. Leveraging the data sampling method of diffusion process to generate data, which is latter used in score matching loss. And I do not see how such data can be easily integrated into PINN loss, but it is natural to use the data for score-matching loss.\n2. Circumventing the normalization condition in loss function which causes difficulty in training. Instead, the paper proposed calculating normalizing constant by postprocessing."
            }
        },
        {
            "comment": {
                "value": "$\\textbf{Suggestion in Weakness}$\n\nWith the reformulation and clarification of $\\mathcal{J}\\_{\\text{plain}}$ and $\\mathcal{J}\\_{\\text{score}}$, we can find that the comparison between the TNN-based FPNN and TFFN in our experiments essentially aligns with the ablation study you suggested. We controlled for the network architecture, spatial domain, number of training points, optimizer, learning rate, and other hyperparameters to remain consistent, with the only difference being the replacement of the PINN loss (residual loss) with our score-based FP loss. Using the same test dataset $\\mathcal{D}_\\text{test}$ (which is uniformly sampled from $\\Omega$ and not used during training for either model), we observed that FPNN significantly outperformed TFFN in terms of training speed, computational costs and evaluation metrics (MAE and MAPE).\n\n$\\textbf{Questions}$\n\nIn our experiments, SFP problems are constructed with potential functions for which analytical solutions are available to evaluate model performance (see Eq.(23) in Appendix C). Additionally, we did not randomly sample the test points from the entire space, as it could result in very small true values for density (often happens in high-dimensional settings) and render MAPE ineffective. Instead, we use the gradient ascent method on the analytical solution $p(x)$ to ensure that all densities of test data exceeds a certain threshold $\\epsilon$. We then record these spatial points $x$ along with their corresponding densities $p$ as test dataset.\n\nIf you have any further questions or suggestions, please feel free to give comments or remarks. We are open to a more in-depth discussion on both static FP (SFP) and non-stationary FP (TFP) equations."
            }
        },
        {
            "comment": {
                "value": "Thank you very much for your comments and constructive suggestions. There are some key points that may require further clarification to enhance your understanding of the improvements in our work.\n\n$\\textbf{Weakness 1}$\n\nPINNs inherently do not support a postprocess of calculating the normalizing constant, as the objective function is defined as follows: (plain PDE loss)\n$$\\mathcal{J}\\_{\\text{plain}}(p\\_\\theta)=\\mathbb{E}[|\\nabla\\cdot(p\\_\\theta\\mu)-\\nabla\\cdot(\\nabla\\cdot(Dp_\\theta))|]$$\nIt is evident that the zero solution ($p_\\theta = 0$) also satisfies this equation. Therefore, directly minimizing the loss function above often leads to network collapse, where $p_\\theta$ converges to a trivial solution. Existing strategies generally fall into two categories involving normalization constraints:\n1. Adding a penalty term $\\mathcal{J}\\_{\\text{norm}}=\\left(\\frac{|\\Omega|}{|\\mathcal{D}|}\\sum\\_{x\\in\\mathcal{D}} p\\_\\theta(x) - 1\\right)^2$, where the dataset $\\mathcal{D}$ is sampled uniformly from $\\Omega$.  \n2. Using specific structures to represent the density function.\n\nHowever, the first method requires manual balancing of multi-objective loss function, while the second method may limit the model's representation capacity (Fig.1).\n\nIn contrast, our method deviates from all existing approaches by changing the loss function directly. We introduce a new loss function as: (score PDE loss)\n$$\\mathcal{J}\\_{\\text{score}}(s\\_\\theta)=\\mathbb{E}\\left[|\\nabla\\log \\widetilde{p\\_\\theta}(x) \\cdot \\widetilde{\\mu}(x) + \\nabla\\cdot\\widetilde{\\mu}(x)|\\right], \\quad \\widetilde{\\mu}(x) = \\mu(x) - \\nabla\\cdot D(x) - D(x)\\nabla\\log \\widetilde{p\\_\\theta}(x)$$\nIn this form, the trivial solution $p_\\theta = 0$ no longer satisfies the equation. The *score PDE loss* retains equivalence with the *plain PDE loss* while avoiding a zero solution and eliminating the need for handling normalization conditions in training process.\n\nTo further illustrate, consider a special case where $D(x) = I_d$. If minimizing the score loss $\\mathcal{J}_{\\text{score}}$ drives $\\widetilde{\\mu}(x)$ to zero, the training process becomes equivalent to optimizing the objective function of *flow matching*:\n$\\mathbb{E}\\left[\\|\\mu(x) - \\nabla\\log \\widetilde{p\\_\\theta}(x)\\|^2\\right]$. Thus, our method inherently integrates score matching while adhering to the original Fokker-Planck equation and corresponding physical laws.\n\nTherefore, it is natural to perform a postprocess of calculating the partition function $Z_\\theta$ under the score PDE loss. The advantages are also clear: it reduces computational costs, eliminates the interference caused by normalization constraints on optimization dynamics, and ensures a more efficient and coherent training process.\n\n$\\textbf{Weakness 2}$\n\nRegarding the non-stationary Fokker-Planck (time-dependent FP, TFP) equation, your understanding is indeed correct. The transformation remains applicable to TFP equations, with one more term $\\partial_t \\log p_\\theta(x, t)$. However, our target is to completely decouple the normalization condition from training process and instead use a neural network to model $\\widetilde{p_\\theta}$. It is important to note that $\\log p_\\theta(x, t) = \\log \\widetilde{p_\\theta}(x, t) - \\log Z_\\theta(t)$, which implies that the loss function would introduce a term $\\partial_t \\log \\widetilde{p_\\theta}(x, t) - \\partial_t \\log Z_\\theta(t)$, necessitating the evaluation of the partition function. And the loss function is not merely in terms of $\\nabla \\log p(x, t)$ (i.e., the score form).\n\nMoreover, there is an additional technical challenge: for non-localized density functions, identifying a suitable integration domain $\\Omega$ at any given time $t$ to compute the time-varying normalizing function $Z_\\theta(t)$ is challenging. It may be feasible to calculating the discrete sequence $\\\\{Z_1, Z_2, \\ldots, Z_N\\\\}$ at time steps $\\\\{t_1, t_2, \\ldots, t_N\\\\}$ and then interpolating to obtain a continuous function $Z_\\theta(t)$.\n\nIn the future work, we aim to further explore and refine the score-based algorithm for TFP equations."
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors focus on steady state Fokker-Planck (SFP) equations and propose novel score-based PDE loss. The proposed loss does only depend on the score function $s_\\theta$, avoiding the necessity to compute the normalization constant of the probability distribution. Furthermore, the authors propose to investigate the proposed loss on two types of architectures, namely tensor neural networks (TNNs) and MLPs. Experiments on several PDE examples are performed, showing the good performance of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is overall well written and easy to follow.\n2. Experimental results are interesting and support the authors' claims."
            },
            "weaknesses": {
                "value": "1. There is a potential clash with the notion of score in the ML/DL community, see questions.\n2. It is difficult to evaluate the novelty of the work. For example, authors only compare the proposed approach to TFFN but no other baselines are provided. I would strongly suggest that the authors add more baselines to allow for an easier comparison with existing approaches, e.g. [1,3].\n3. Similarly, it is not clear whether the set of chosen experiments are commonly used in the  PINN community. For example, is the dataset of the authors present in [2]?\n\n(see refs in questions)"
            },
            "questions": {
                "value": "**Main comments**\n1. I am afraid that the notion of \"score\" that is central to the paper is fairly different to the one usually referred to in deep learning, where the score is implicitly learned through denoising and the associated distribution is never explicitely computed. Here, the authors rather use a differentiable network and use this property within the loss. Can the authors comment on that?\n2. Why is the computation of $Z_\\theta$ important in the context of Fokker-Planck (FP)? If I understand well, the authors parametrize FP with a neural network, with a direct access to both $\\nabla \\log p$ and $p$. Is the summation approach of (8) and (10) tractable as dimension increase?\n3. I struggle to understand the point of the authors lines 453-473. Firstly, the notation $|D_{norm}|$ is slightly confusing. Secondly, what is the source of randomness in $D_{norm}$  mentionned by the authors line 465 ? If the dataset for $\\mathcal{D}_{norm}$ is simulated, could the authors generate more samples?\n4. Fig. 9 is slightly unclear. The plots on top and bottom show two different things (top: MAP and MAPE, bottom: MAPE and Z) for two different architectures. In particular, why should Z and the MAPE be related? Could the authors comment on that?\n\n**Minor comments**\n1. While the paper is well written, some typos are remaining, e.g. \"Score-based generate model\" (line 129)\n2. The color scheme from Fig. 4 (c) (bottom) is not clear, it seems most of the maps are identically 0. The authors may want to reduce the threshold.\n3. The message would be more striking in Fig. 5 and 6 if experiment was run multiple time with different random seeds. This would allow the authors to provide smoother curves with mean and error bars. \n\n\n**References**\n\n[1]\n@inproceedings{zhai2022deep,\n  title={A deep learning method for solving Fokker-Planck equations},\n  author={Zhai, Jiayu and Dobson, Matthew and Li, Yao},\n  booktitle={Mathematical and scientific machine learning},\n  pages={568--597},\n  year={2022},\n  organization={PMLR}\n}\n\n[2]\n@article{lu2021deepxde,\n  author  = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},\n  title   = {{DeepXDE}: A deep learning library for solving differential equations},\n  journal = {SIAM Review},\n  volume  = {63},\n  number  = {1},\n  pages   = {208-228},\n  year    = {2021},\n  doi     = {10.1137/19M1274067}\n}\n\n\n[3]\n@article{cho2024separable,\n  title={Separable physics-informed neural networks},\n  author={Cho, Junwoo and Nam, Seungtae and Yang, Hyunmo and Yun, Seok-Bae and Hong, Youngjoon and Park, Eunbyung},\n  journal={Advances in Neural Information Processing Systems},\n  volume={36},\n  year={2024}\n}"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the Fokker-Planck Neural Network (FPNN), a novel framework for solving high-dimensional steady-state Fokker-Planck (SFP) equations. Traditional deep learning approaches to these equations face challenges with representation capacity, loss function balancing, and maintaining normalization constraints. The proposed FPNN addresses these issues by decoupling score learning from density normalization, using a score PDE loss that enables strict adherence to normalization constraints while allowing flexible, mesh-free architectures. FPNN achieves significant computational efficiency, with over a 20x speedup compared to state-of-the-art methods, and requires only minimal parameters for high accuracy. The authors demonstrate its effectiveness on 4D, 6D, and even 20D problems, achieving low relative errors without labeled data. The FPNN framework contributes a fast, efficient, and accurate solution method for high-dimensional Fokker-Planck equations, with applications in computational physics and related fields."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper presents a novel Fokker-Planck Neural Network (FPNN) that innovatively decouples score learning and density normalization for high-dimensional steady-state Fokker-Planck (SFP) equations. Traditional deep learning methods for Fokker-Planck equations generally incorporate the PDE residual as part of the loss function but often encounter issues with representation capacity, balancing multi-objective loss functions, and satisfying normalization constraints. FPNN addresses these challenges by using a score PDE loss, which separates score learning from normalization, allowing for a flexible, mesh-free network architecture. This approach is original as it removes the dependency on specific architectures, enables strict normalization through a single computation of the partition function, and offers substantial computational gains over existing methods. By rethinking how neural networks approach Fokker-Planck equations, the authors introduce a framework that improves upon limitations of previous methods, potentially broadening the scope of high-dimensional PDE applications in machine learning."
            },
            "weaknesses": {
                "value": "1. Limited Discussion on Practical Constraints and Limitations\n2. Lack of Scalability and Computational Complexity Analysis\n3. Lack of Comparison with Other Recent Advances"
            },
            "questions": {
                "value": "1. Could be the proposed method widely used for other PDEs?\n2. Please compare with recent developed methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces the Fokker-Planck Neural Network (FPNN) framework, a novel approach to solving high-dimensional steady-state Fokker-Planck equations by leveraging a score-based PDE loss that decouples density normalization from score learning. This decoupling allows FPNN to avoid continuous computation of the partition function, enhancing computational efficiency and stability, particularly for complex high-dimensional problems. Experimental results demonstrate FPNN\u2019s superior performance over existing methods, achieving high accuracy and significant speedups, making it a promising candidate for scalable and efficient solutions to Fokker-Planck equations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "(1) This paper proposes a novel network for solving high-dimensional steady-state Fokker-Planck equations. By utilizing a score-based PDE loss that decouples score learning from density normalization, the network achieves an effective balance between representational capacity and the constraints required for accurate solutions.\n\n(2) The performance results are promising, though further validation is needed to strengthen the findings.\n\n(3) The presentation of this paper is good."
            },
            "weaknesses": {
                "value": "(1) In this paper, using SRK as part of the data generation process for steady-state Fokker-Planck equations is effective, but it might lead to an \"unfair advantage\" in comparisons if other baseline methods do not leverage a similar approach for handling randomness or approximating steady states. Thus, it will be meaningful to test the performance of the proposed method with the same training data as proposed in TFFN paper.\n\n(2) Score-based methods have shown strong performance for in-distribution problems, but they often suffer from significantly reduced effectiveness on out-of-distribution (OOD) tasks. I am curious whether the authors evaluated the OOD performance for this model. Additionally, as mentioned previously, I wonder if the data generation approach used in this study simplified the distribution, making it easier for the network to learn. If that is the case, the improvements might be attributed more to the engineering aspects of data preparation rather than advancements in the network architecture itself.\n\n(3) Additional comparisons would strengthen this paper. Although FPNN outperforms TFFN in the results presented, I could not find any published reference for TFFN, suggesting it may only be available on arXiv and has not yet undergone peer review. To more convincingly demonstrate the effectiveness of the proposed method, I recommend that the authors include additional, widely recognized baseline methods. This would provide a more comprehensive evaluation of FPNN\u2019s efficiency and robustness."
            },
            "questions": {
                "value": "The questions are addressed in the weaknesses section. Although this research is not fully aligned with my area of expertise, I will follow the rebuttal process and hope that my suggestions help improve the clarity and presentation of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper intends to solve high-dimensional Fokker-Planck equations which faces several challenges including curse of dimension, normalization constraint, etc. The solution proposed is to train neural network with a score-matching loss which bypasses normalization constraint by computing normalization constant as post-process. The method belongs to supervised learning, where training data is generated by stochastic Runge-Kutta method. The result seems effective and surpasses baseline model in accuracy."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Originality** The score-based loss is novel and seemly interesting. Given the close connection between Fokker-Planck (FP) equations and diffusion process and the noticeable succuss of diffusion model with score-matching loss, it is worthy trying to solve FP with score-based loss.\n\n**Clarity** I find the paper very clear to read and well organized."
            },
            "weaknesses": {
                "value": "At the first glance, it is a seemly natural and attractive idea to solve Fokker-Planck (FP) equations with the proposed score-matching loss, especially considering the success in training diffusion models and the close connection between stochastic process and FP equations. However, after more careful thoughts I find it hard to reason through the following questions:\n\n1. If the proposed method needs a postprocess of calculating normalizing constant, why not treating PINN with the same postprocess? One of the major motivations of the paper is to deal with normalization condition (NC), which the authors criticized PINN being hard to satisfy with soft constraints. However, if PINN is obtained without NC and is normalized with the same quadrature technique afterwards, this motivation is weakened.\n\n2. The score-based FP loss (equation (6)) is derived for static FP equations. What is the difficulty with non-stationary FP? It seems to me the residual loss can be transformed similarly and just one more term is needed in equation (15), which is $\\partial_t \\log p_{\\theta}(x)$. Even if we only consider SFP, it is clear now that score-based FP loss is essentially equivalent to residual loss of PINN. Therefore, I wonder what benefit score-based FP loss can introduce? \n\nBased on these questions, I suggest the authors do an ablation study of replacing score-based FP loss with PINN loss (residual loss) without normalization constraint. Otherwise, the improvement of FPNN over TFFN may be purely due to removing normalization constraint from loss function."
            },
            "questions": {
                "value": "See weaknesses above. Also, when using MAPE for metric, how do you calculate $p(x)$ for test dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}