{
    "id": "ozTREVBARB",
    "title": "SIKeD: Self-guided Iterative Knowledge Distillation for Mathematical Reasoning",
    "abstract": "Large Language Models (LLMs) can transfer their reasoning skills to smaller models by teaching them to generate the intermediate reasoning process required to solve multistep reasoning tasks.  While LLMs can accurately solve reasoning tasks through a variety of strategies, even without fine-tuning, smaller models are not expressive enough to fit the LLMs distribution on all strategies when distilled and tend to prioritize one strategy over the others.\nThis reliance on one strategy poses a challenge for smaller models when attempting to solve reasoning tasks that may be difficult with their preferred strategy.\nTo address this, we propose a distillation method *SIKeD*: **S**elf-guided **I**terative **K**nowledge **D**istillation, where the LLM teaches the smaller model to approach a task using different strategies and the smaller model uses its self-generated on-policy outputs to choose the most suitable strategy for the given task. The training continues in a *self-guided* iterative manner, where for each training iteration, a decision is made on how to combine the LLM data with the self-generated outputs. Unlike traditional distillation methods, *SIKeD* allows the smaller model to learn *which* strategy is suitable for a given task while continuously learning to solve a task using different strategies.\nOur experiments on various mathematical reasoning datasets show that *SIKeD* significantly outperforms traditional distillation techniques across smaller models of different sizes.",
    "keywords": [
        "Reasoning",
        "Knowledge Distillation",
        "LLM",
        "SLM"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "Our approach enables smaller models to learn and choose from multiple reasoning strategies by iteratively combining LLM data and self-generated outputs",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ozTREVBARB",
    "pdf_link": "https://openreview.net/pdf?id=ozTREVBARB",
    "comments": [
        {
            "title": {
                "value": "Addressing Reviewers questions"
            },
            "comment": {
                "value": "Below is a discussion on the questions raised by the reviewer.\n\n### Q1: Can the authors elaborate on the underlying reasons for smaller models' bias towards specific strategies? Does the preferred strategy vary?\n\nOur experiments (Fig 5) suggests that **the dominant strategy (measured in terms of the performance) is often preferred by the smaller model**. This might be  because of relatively more samples present in the training data. However, **when we balance the samples per strategy  (for ablation study), the model prefers CoT, possibly because of the pre-training bias**. \n\n### Q2: Bias introduced due to discarding samples\n>The paper states that mixing LLM-generated data with self-generated outputs helps align smaller models with their learned knowledge (L83). However, discarding samples with mismatched outputs creates a different form of bias? Please clarify this.\n\nAlthough we discard the mismatched outputs from the smaller model and utilize the correct generations from LLM, **the KL divergence between the training data and SLM still decreases as all the data is not just from the LLM and mixing smaller model data reduces the KL divergence**. This is also demonstrated in Fig. 2.  \n\n### Q3: Please explain the mechanism by which mixing the data promotes diverse strategy selection in smaller models.\n\n- Please refer to Weaknesses 1 above. We have addressed this concern in details. \n\n\n### Q4: How to judge if the generated r_i is correct? (L294)\n**Knowing if the rationale is correct or not is a strong limitation of all the reasoning tasks involving intermediate steps and is not a specific limitation for our work**. This has been discussed in the past works like https://arxiv.org/pdf/2212.10001 \n\n### Q5: Is it a typo in L211 & L215 that the same notation appears in different contexts?\nL211 is the rolled out version of L215. We did not find any typo in those lines. Please let us know if there is anything specific you would like to point out. \n\n### Q6: Discussing how selection works for biased strategy\n>Could the authors clarify the strategy sampling process for smaller models? Specifically regarding L413, \"if both CoT and PoT are sampled correctly, our biased strategy choice is PoT\" - is this strategy determined by the smaller model's output? Additional details on the strategy sampling mechanism would be appreciated and helpful\n\n*The underlying assumption for using a biased strategy selection mechanism lies in the fact that for some datasets, a specific strategy might be more beneficial than the others* (L408-410). Based on this, we create biased training datasets for all the three strategies. **For example, if we are creating a biased dataset for PoT, we firstly select only the problems which can be solved via PoT.** We ignore the CoT and L2M reasoning chains for these problems. For problems that cannot be solved via PoT, we select CoT or L2M chains at an instance level, but not both. This helps us to create a dataset biased on PoT.  \n\nPlease let us know if all the concerns are addressed. We are happy to discuss more. \nThank you."
            }
        },
        {
            "title": {
                "value": "Addressing weakness pointed by the reviewers"
            },
            "comment": {
                "value": "We thank the reviewer for a careful examination of our work, and providing positive feedback for our research problem. We discuss the weaknesses and questions highlighted by the reviewer below. \n\n## Weaknesses \n### How SIKeD promotes strategy diversity\n>Although the method demonstrates effectiveness, the motivation requires further clarification. While iteratively forming new training datasets based on model outputs is an existing approach, the paper's contribution lies in showing this method can enable diverse reasoning strategies in smaller models. However, the underlying mechanism for how this approach promotes strategy diversity needs clearer explanation.\n\nKnowledge distillation uses all data from the LLMs while self-distillation relies on smaller models data for training. SIKeD lies in the middle of the two extremes, where at each iteration, the smaller model generates multiple outputs (k=10) and a reward is applied to it (0 if incorrect, 1 if correct). **The correct reward incentivizes the model to improve its performance over iterations and also to choose the right strategy for a given task which may not be the greedy strategy making the strategy choice more diverse** (if needed). In a sense, optimizing for performance automatically leads to strategy diversity over iterations, since the smaller model cannot solve all the questions with the same strategy. **The qualitative analysis presented in Fig. 7 shows that the smaller model changes its strategy to get the correct solution if needed**. Essentially, in our setting, diversifying strategy is entangled with improved performance. \n\n### The experimental evaluation is currently limited to mathematical reasoning tasks. Exploring the effectiveness of the proposed method in other scenarios would provide valuable insights into its generalization capabilities.\n\nWe have discussed this in detail in the general comments section. In short, we wanted to demonstrate the self-iterative knowledge distillation approach where a smaller model can learn to pick the right strategy to solve a given task following LLM. We used Chain of Thought, subquestion decomposition and Program of Thought as our strategies to pick from. Essentially **we were looking for a task that possesses the qualities of intermediate reasoning, can be decomposed into smaller tasks and can be represented in the form of a program. We found mathematical reasoning as the perfect fit for our use case** and other reasoning tasks were hard to fit into the chosen strategies. This is why we limited ourselves to mathematical reasoning and also added that to the title to not oversell our idea. We are collecting a dataset in a different domain that can work across multiple strategies but that is for future work."
            }
        },
        {
            "title": {
                "value": "Addressing questions raised by the reviewers"
            },
            "comment": {
                "value": "## Questions \n\n### Do self-distillation really work in small model continual training ?\n> Fig. 4 in your paper and Fig. 7 in [3] indicated the performance decreased, when the n of iteration became large. From a theoretical perspective on synthetic data, the data variance decreases with multiple generations n [4]. To prevent variance reduction, i.e., to enhance data diversity, this paper incorporates data synthesized by LLMs throughout the iterative process. This operation is very important. From data perspective, authors should analyze the data distribution shifting across n.\n\nFrom a theoretical perspective on synthetic data, the data variance decreases with multiple generations as discussed in  [4]. However **[4] uses a model that self-distills itself using a fixed strategy**. On the contrary, **SIKeD uses multiple strategies which allows the model to switch strategies over iterations for a given query, potentially increasing the data variance**. Moreover, we also mix data from LLM in 2 variations: a) All b) Sparse which also changes over iterations. We analyzed the change in strategy distribution in Fig. 2 which shows that **diversity may or may not increase but mixing data reduces overall KL divergence between the training data distribution and the distribution generated by the smaller model leading to better overall performance**. \n\n### Is there any new theoretical insights ?\n>The authors observed shifts in generation strategies over iterations n. What causes this phenomenon? Additionally, as more data is generated, the overall dataset size increases. \n\n- We observed that smaller models can learn to pick the suitable  strategy for a given task over iterations but this requires careful mixing of data between the LLM and self generated data controlled by the parameter \u2018alpha\u2019. **Simply using all the strategy data combined from the LLM wont work** as shown in the Results table (Combined baseline in Table 1).\n- In terms of overall data size, SIKeD increases the data size for \u2018All\u2019 variation, but not for \u2018Sparse\u2019. The dataset used for training \u2018Sparse\u2019 models utilizes correctly generated data by the smaller model, and for questions where the smaller model is unable to generate correct answers, corresponding data from LLM is added. **Over iterations, as the smaller model improves, additional training data from LLM decreases, essentially keeping the training dataset approximately constant. However, we observe that \u2018Sparse\u2019 performs as well as the \u2018All\u2019 variation**, if not better.  \n\nWe again thank the reviewer for all the important points raised. Please let us know if all the concerns are addressed. We are happy to discuss more. \nThank you."
            }
        },
        {
            "title": {
                "value": "Addressing weakness pointed in our work"
            },
            "comment": {
                "value": "We thank the reviewer for providing detailed and easy to follow feedback on our paper. We specially thank the reviewer for citing relevant literature along with the comments. We address the weaknesses and questions highlighted by the reviewer below: \n\n## Weaknesses \n### W1: The mathematical notation is overly verbose. \nWhile some of the notation may look complex, **it is necessary to formulate the iterative self-guided framework in detail, and Algorithm 1 is required to do so**. Furthermore, we use the mathematical framework to define and show the importance of the data mixing rate \"alpha\", without which it would have been difficult to discuss the two settings: \"All\" and \"Sparse\". Finally, the original knowledge distillation paper by Hinton et al. uses KL divergence to discuss distillation effects, and it was important to draw a comparison between SIKeD and KL divergence. \n\n### W2: Limited generalization \n>The approach only enhances the GSM8K dataset, but reasoning tests should be conducted on more realistic datasets, such as MATH, Arc-Challenge and so on. And more reasoning tasks also need to be evaluated, such as commonsense reasoning and symbolic reasoning.\n\nWe wanted to demonstrate the self-iterative knowledge distillation approach where a smaller model can learn to pick the right strategy to solve a given task following LLM. We used Chain of Thought, subquestion decomposition and Program of Thought as our strategies to pick from. Essentially **we were looking for a task that possesses the qualities of intermediate reasoning, can be decomposed into smaller tasks and can be represented in the form of a program. We found mathematical reasoning as the perfect fit for our use case** and other reasoning tasks were hard to fit into the chosen strategies. This is why we limited ourselves to mathematical reasoning and also added that to the title to not oversell our idea. We are collecting a dataset in a different domain that can work across multiple strategies but that is for future work. \n\nAlso, **to compare our work with the previous works of knowledge distillation, we limited ourselves to four datasets that were commonly used in the past work** on Magister et al. (https://arxiv.org/abs/2212.08410), Shridhar et al. (https://arxiv.org/abs/2212.00193) and  Zhu et al (https://arxiv.org/abs/2401.11864) which also form our baselines. Finally, we did initial analysis on the MATH dataset and found PoT and L2M to be a weaker strategy compared to CoT which biased the model to always pick CoT.\n\n### W3: The absence of important references. The self-distillation in small models is already studied in [1,2,3].\nSelf-distillation has been studied in the past but our work demonstrates that direct distillation and self-distillation are on the two extremes. A mixture of the two (controlled by alpha in our work) performs better as shown in Fig. 6. In addition, **our work also focuses on teaching multiple strategies to smaller models so that they can learn to pick the right strategy for a given task, which is missing from all previous work on self-distillation**. We will add the highlighted reference in our paper. Thanks for that."
            }
        },
        {
            "title": {
                "value": "Addressing reviewers concern"
            },
            "comment": {
                "value": "We thank the reviewer for the positive feedback on our experimental analysis and framework. We discuss the one concern highlighted by the reviewer below: \n\n## Concern: I\u2019m curious to see that among the three methods, 1) training with the proposed method, 2) pure distilling and 3) pure self-generating, which method can make the model generate the most diverse trajectories and whether the diversity is aligned with model\u2019s performance on OOD tasks\n\n**We refer to Figure 6 which shows that the best performing model is neither a pure distillation (alpha=1) nor pure self-generation (alpha=0), but somewhere in the middle**. SIKeD improves the performance of smaller models by tuning them to select the most accurate reasoning strategy often, but in addition it also enables the smaller model to switch strategies over iterations. Figure 5 and the qualitative analysis (Figure 7) shows this.  \n\nFrom a diversity perspective, pure distillation incorporating all the reasoning strategies (combined in our baseline) has the highest diversity, as a larger model is able to generate more diverse reasoning chains. The diversity of purely self-generating distillation is the lowest, as a smaller model is always biased towards one strategy (Please check Figure 1 for reference). \n\nHence,  **Diversity(Pure Distillation) > Diversity(SIKeD) > Diversity(Pure Self-Generating)**\n\nHowever, **Performance(SIKeD) > Performance(Pure Distillation) and Performance(SIKeD) > Performance(Purely Self-generating)**. This is mostly because only using LLM data has a distributional gap with the smaller model while purely using smaller model data has limited correct samples to improve. A mix of both worlds yields the best results as presented in our work. \n\nWe are happy to discuss more. Please let us know if any more concerns. Thank you."
            }
        },
        {
            "comment": {
                "value": "Below is a discussion of all the issues the reviewers wanted to discuss. \n\n## Q1: Have the authors considered applying SIKeD to tasks outside of mathematical reasoning to test the generalizability of the strategy selection mechanism?\nPlease refer to the first point under weakness above. \n\n## Q2: Does SIKeD require the ground-truth answers of the training data? If not, how does it handle the tasks where the ground-truth answers are not available?\nSince the knowledge distillation works with the LLM generating the training data for smaller models, we need to verify that the data generated by the LLM is correct.. If the ground truth data is unavailable, we can either use the LLM to verify the generations of the smaller model or do a self-consistency over the generated samples (since we have K=10) and use the most consistent answer. However, we did not explore these in our work due to the availability of the ground truth data. \n\n## Q3: Is there any knowledge distillation baseline that could be used to compare the performance of SIKeD on mathematical reasoning tasks? Current experiments only compare SIKeD against CoT, L2M, PoT and Combined\n**Comparison with the baselines of CoT, PoT, L2M and combined are based on the past knowledge distillation works**. CoT based distillation (CoT in Table1) was proposed by Magister et al. (https://arxiv.org/abs/2212.08410), L2M based distillation (L2M in Table 1) was proposed by Shridhar et al. (https://arxiv.org/abs/2212.00193) and the combined one is inspired from Zhu et al (https://arxiv.org/abs/2401.11864). All these comparisons are presented in Table 1 and are considered as baselines against which we compare our proposed methodology SIKeD. We will clarify this in the paper that our baselines come from previous distillation works.\n\n## Q4: Can the authors explain why does the improvement on Qwen 1.5B model is less significant compared to the other base models? What are the potential reasons for this discrepancy?\nA recent work \u201cCareful Examination of LLM performance on GSM\u201d (https://arxiv.org/abs/2405.00332) has suggested that **some models are over optimized for GSM8K style tasks** and we suspect that could be one of the reasons for minor improvements. \n\n## Q5: The small models are tuned with LoRA, what if the parameters of the small models are fully tuned? Would the performance of SIKeD be further improved?\n**Experimenting with fully fine-tuning small models showed that the performance worsened, possibly due to over-fitting**. We again refer to \u201cCareful Examination of LLM performance on GSM\u201d\u201d *(https://arxiv.org/abs/2405.00332) which suggests that some models are over-optimized on GSM8K style tasks* \n\n## Q6: \"The iterative training is stopped when accuracy shows only marginal improvements or declines.\" What specific criteria is used to determine the optimal number of iterations?\nWe trained the Gemma 2B model further till 5th iteration and the overall performance went down. We noticed that **training for one or two iterations after the accuracy starts to plateau or goes down is a good stopping criteria**. For our experiments, training up to 3 iterations seems to be a good stopping point.  \n\nPlease let us know if there are any more concerns. We are happy to discuss more."
            },
            "title": {
                "value": "Addressing reviewers questions."
            }
        },
        {
            "title": {
                "value": "Addressing weakness pointed in our work"
            },
            "comment": {
                "value": "We thank the reviewer for providing positive feedback for our experimental process, and also for a very clear, detailed and easy to follow list of questionnaires. We address the weaknesses and questions raised by the reviewer below: \n\n## Weakness1: The proposed method is only evaluated on mathematical reasoning tasks. It\u2019s unclear how well SIKeD would generalize to other domains that require more nuanced strategy selection.\n\nWe have addressed this concern in details in the general comment section. In short, we wanted to demonstrate the self-iterative knowledge distillation approach where a smaller model can learn to pick the right strategy to solve a given task following LLM. **We used Chain of Thought, subquestion decomposition and Program of Thought as our strategies to pick from based on the past works**. Essentially we were looking for a task that possesses the qualities of intermediate reasoning, can be decomposed into smaller tasks and can be represented in the form of a program. **We found mathematical reasoning to be the perfect fit for our use case**. The other reasoning tasks were difficult to fit into the chosen strategies. Therefore, we limited ourselves to mathematical reasoning and also added that to the title to not oversell our idea. We are collecting a dataset in a different domain that can work across multiple strategies but that is for future work. \n\n## Weakness2: The paper lacks comparison with knowledge distillation methods.\n**We compared SIKeD with past knowledge distillation works like CoT based distillation** (CoT in Table 1) which was proposed by Magister et al. (https://arxiv.org/abs/2212.08410), **L2M based distillation** (L2M in Table 1) which was proposed by Shridhar et al. (https://arxiv.org/abs/2212.00193) and **combined** is inspired from Zhu et al (https://arxiv.org/abs/2401.11864). All these comparisons are presented in Table 1 and are considered as baselines against which we compare our proposed methodology SIKeD. We will clarify this in the paper that **our baselines come from previous distillation works**. \n\n##  The code is not available for reproduction.\nWe are currently in the process of cleaning our code and we\u2019ll add a Github link soon."
            }
        },
        {
            "title": {
                "value": "Addressing Reviewers concern II"
            },
            "comment": {
                "value": "## Concern3:  Computational efficiency of SIKeD and number of iterations required for convergence\n\nWe performed a full Self-guided iterative training using SIKeD and with the \u201cSparse\u201d version of our approach, we only take the problems from the LLM that the smaller model could not solve. For all queries with incorrect solutions generated by the smaller model, we take the query output pairs from the LLM. \n\nFor example, Gemma 7B has a baseline accuracy of ~70%. Assuming the same accuracy on the training dataset (which is true), only 30% of the data from LLM is required for the next iteration. Since all baseline models and each iteration of SIKeD are trained for 3 epochs, training SIKeD with 30% data for 3 epochs (1 iteration of SIKeD) corresponds approximately to 1 epoch of baseline models. Thus, 3 iterations of SIKeD correspond to 3 epochs of baseline models, which is equivalent to 1 additional training round of baseline models. **In other words, the training cost of SIKeD is 2X of the baseline model with no additional test time cost**. Note that we trained the baseline model for 3 more epochs and it led to a worse performance possibly due to overfitting. \n\n**Convergence** - We trained the Gemma 2B model further till 5th iteration and the overall performance went down. We noticed that **training for one or two iterations after the accuracy starts to plateau or goes down is a good stopping criteria. For our experiments, training up to 3 iterations seems to be a good stopping point.**\n\n**Resources Needed** - All the models have been fine-tuned using LoRA using a single RTX GPU with 24 GB RAM. \nIn terms of the compute time, here are the training time per model:\n\n**Per iteration**\n- Gemma 2B - 1 hour\n- Qwen 1.5B - 1 hour\n- Qwen 0.5B - 30 mins\n- Gemma 7B - 2 hours\n- SmolLm - 1 hour\n\n**Each training iteration was done under 1 hour (except for Gemma 7B that takes 2 hours per iteration)**. For inference with VLLM, all models except Gemma 7B were run on a single RTX GPU with 24 GB RAM. For inference with Gemma 7B, a single A100 GPU was needed. The inference was completed within 2 hours.  \n\n \n## Concern4:  Scaling of SIKeD with model size\n\nSince all the models used in the paper were initially trained on different datasets (which in most cases are not public), it is hard to judge the scalability of SIKeD with model sizes. Also models of different sizes are often trained on different overall tokens which makes the comparison hard for us to do. Nevertheless, **we found that SIKeD works well across models of all sizes with no significant impact due to scaling the model sizes.**\n\nPlease let us know if we addressed your concerns and we are happy to discuss more. Thank you."
            }
        },
        {
            "title": {
                "value": "Addressing Reviewers concern I"
            },
            "comment": {
                "value": "We thank the reviewer for recognizing and valuing our work, as well as for their positive feedback on our paper writing, presented methodology and experimentation choice. \n\nWe address the weaknesses highlighted by the reviewers, along with the questions raised, in detail below:\n\n## Concern1: SIKeD is contingent upon the quality of the initial LLM data\n\nThis is an important point raised and *knowledge distillation as a concept depends on the quality of the teacher model data*. However,  **SIKeD is not limited by the initial LLM data** as SIKeD is a self-guided iterative learning framework. This means that with a weaker LLM, the quality of distillation data will be relatively poor compared to a stronger LLM, leading to a weaker distilled smaller model as baseline. But **our approach improves the baseline performance irrespective of the teacher model used.**\n\nWe replace Llama3 70B as LLM with Llama3 8B and report our results for Gemma 2B and 7B models for GSM8K below:\n\n**Gemma-2B**\n\n| Method   | Accuracy with LLaMA3-8B as LLM| Accuracy with LLaMA3-70B as LLM |\n|----------|---------------------------------|----------------------------------|\n| CoT      | 40.79                           | 36.54                           |\n| PoT      | 41.70                           | 44.05                       |\n| L2M      | 37                              |  36.92                         |\n| Combined | 42.08                           | 44.05                           |\n| SIKeD    | **44.35**                       |    **47.23**                   |\n\n\n**Gemma-7B** \n\n| Method   | Accuracy with LLaMA3-8B as LLM | Accuracy with LLaMA3-70B as LLM |\n|----------|---------------------------------|----------------------------------|\n| CoT      | 70.36                           | 67.40                           |\n| PoT      | 67.55                           | 71.34                           |\n| L2M      | 68.99                           | 69.29                           |\n| Combined | 70.66                           | 70.74                           |\n| SIKeD    | **71.04**                       | **73.84**                       |\n\nOverall **SIKeD improved the performance over baselines even when a weaker LLM was used**. We observed similar results with the Qwen models. \n\n## Concern2: Study primarily focused on mathematical reasoning tasks\n\nWe have addressed this concern in details in the general comment section. In short, we used Chain of Thought, subquestion decomposition and Program of Thought as our strategies to pick from based on the past works. Essentially **we were looking for a task that possesses the qualities of intermediate reasoning, can be decomposed into smaller tasks and can be represented in the form of a program. We found mathematical reasoning to be the perfect fit for our use case**. The other reasoning tasks were difficult to fit into the chosen strategies. Therefore, we limited ourselves to mathematical reasoning and also added that to the title to not oversell our idea. We are collecting a dataset in a different domain that can work across multiple strategies but that is for future work."
            }
        },
        {},
        {
            "title": {
                "value": "Thanking reviewers and addressing common concerns"
            },
            "comment": {
                "value": "We thank all the reviewers for their feedback on our work.  We thank the reviewers for positive feedback on our experimental analysis (reviewers JLQW, BCvZ, YiHw), novelty and performance of our approach (reviewers JLQW, EkQs) as well as the overall structure of the paper (reviewers JLQW, EkQs, BCvZ, ytP5). \n\n## Below is a summary of our approach: \n\nIn our work, **we propose a Self-guided Iterative Knowledge Distillation (SIKeD) approach** for mathematical reasoning tasks. **While Large Language Models can solve a mathematical reasoning task through various strategies, smaller models are often biased on a single strategy** (Figure 1). Simply combining all strategies in a knowledge distillation framework does not work well as the smaller model tends to learn a single strategy well (\u2018Combined\u2019 baseline in Table 1). \n\n**To solve this problem, SIKeD uses an iterative approach where the self-generated data of SLM is mixed with LLM data in each iteration for training**. The proportion of mixing LLM and SLM data is controlled automatically by the mixing rate \u2018alpha\u2019. We compare SIKeD against standard distillation approaches using single as well as multiple reasoning strategies. Our results show that **SIKeD increases the model performance by up to 5 points, with consistent gains across 5 models and 4 mathematical reasoning datasets**. \n\n## We address some of the concerns common across the reviews below - \n\n### Concern1: The study primarily focuses on mathematical reasoning tasks \n>(Reviewer JLQW\u2019s Weakness 1 and Question 1, Reviewer EkQs\u2019s Weakness W1 and Question Q1, Reviewer YiHw\u2019s Weakness 2, Reviewer ytP5\u2019s weakness 2) \n\n- We wanted to demonstrate the self-iterative knowledge distillation approach where a smaller model can learn to pick the right strategy to solve a given task following LLM. While it is challenging to find universally applicable datasets for reasoning strategies like CoT, PoT, and L2M, **we have ensured that our baselines align with well-established prior work**. This provides a strong foundation for evaluating SIKeD and underscores its performance improvements over state-of-the-art approaches.\nEssentially **we were looking for a task that possesses the qualities of intermediate reasoning, can be decomposed into smaller tasks and can be represented in the form of a program. We found mathematical reasoning to be the perfect fit for our use case**. The other reasoning tasks were difficult to fit into the chosen strategies. Therefore, we limited ourselves to mathematical reasoning and also added that to the title to not oversell our idea. We are collecting a dataset in a different domain that can work across multiple strategies but that is for future work. \n\n- While our current experiments are scoped to mathematical reasoning, we view this as a starting point. Due to the lack of a dataset that can be solved fairly well with strategies such as CoT, L2M and PoT that we explored in the paper, we are limited to mathematical reasoning.  We are actively working on extending SIKeD to other reasoning domains and collecting a new dataset to validate its broader applicability in future work.\n\n### Concern2: Comparison with other knowledge distillation work\n\nTo compare our previous works of knowledge distillation, **we use four mathematical datasets that were commonly used in the past work** on Magister et al. (https://arxiv.org/abs/2212.08410), Shridhar et al. (https://arxiv.org/abs/2212.00193) and  Zhu et al (https://arxiv.org/abs/2401.11864). These past works also form our baselines (CoT is taken from Magister et al., L2M is taken from Shridhar et al. and Combined is inspired from Zhu et al. in Table 1). Finally, we did initial analysis on the MATH dataset and found PoT and L2M to be a weaker strategy compared to CoT which biased the model to always pick CoT and hence was not suitable for our work. \n\n*In summary, SIKeD offers a robust solution to enhance smaller model performance through iterative, self-guided knowledge distillation.* \n\nWe discuss individual comments in details below."
            }
        },
        {
            "summary": {
                "value": "The paper presents SIKeD, a knowledge distillation approach to enhance smaller models with reasoning skills from Large Language Models (LLMs). SIKeD employs an iterative process that enables smaller models to learn multiple strategies and select the most appropriate for a given task, addressing the issue of strategy bias found in conventional distillation methods. The paper reports that SIKeD outperforms traditional techniques on mathematical reasoning tasks across various smaller model sizes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper presents a unique approach to knowledge distillation by introducing the concept of self-guided iterative training. This method allows smaller models to dynamically adjust their strategy preferences, which is a creative solution to the challenge of strategy distribution mismatch in traditional distillation.\n\n2. The experiments are well-designed and conducted across various mathematical reasoning datasets, providing a thorough evaluation of SIKeD's effectiveness. The improvements in performance metrics are substantial and clearly demonstrated.\n\n3. The paper is well-organized, with a clear problem statement and a logical flow of ideas. The methodology is explained in detail, making it easy for readers to understand the proposed approach and its implications."
            },
            "weaknesses": {
                "value": "1. The paper's methodology, SIKeD, is contingent upon the quality of the initial LLM data. There is a need for further exploration on how fluctuations in LLM data quality might influence the distillation process and the performance of the resulting smaller models.\n\n2. The study primarily focuses on mathematical reasoning tasks, with less clarity on the transferability of SIKeD to other reasoning domains such as commonsense or symbolic reasoning. Additional investigation into the broader applicability of SIKeD could be valuable."
            },
            "questions": {
                "value": "1. Can the authors comment on the potential of SIKeD to be effective in domains outside of mathematical reasoning? Have there been any preliminary experiments or considerations in this direction?\n\n2. Could the authors elaborate on the computational efficiency of SIKeD, especially in terms of the number of iterations required for convergence and the resources needed for each iteration?\n\n3. The paper discusses various smaller models, but does not discuss how the size of the smaller model affects the distillation process and the final performance. Are there any insights on how SIKeD scales with different model sizes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes SIKeD, a novel knowledge distillation approach for transferring multistep reasoning skills from large language models (LLMs) to smaller models, particularly for mathematical reasoning tasks. Traditional distillation methods struggle with strategy selection, often resulting in smaller models biased towards a single strategy. SIKeD addresses this by allowing the smaller model to iteratively learn and apply various strategies, combining LLM-generated data with its self-generated outputs to refine strategy selection. The method demonstrates improvements over single-strategy and combined-strategy distillations, achieving superior results across in-distribution and out-of-distribution mathematical reasoning datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The idea is novel. SIKeD leverages the idea of constructivist learning theory and uses an iterative self-guided approach for multi-strategy distillation, compared to previous single-step distillation.\n- SIKeD shows generalization across in- and out-of-distribution mathematical reasoning datasets, demonstrating its effectiveness in diverse contexts.\n- SIKeD is evaluated using various small model types, showing consistent improvements over the baselines across different model types.\n- The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- W1: The proposed method is only evaluated on mathematical reasoning tasks. It\u2019s unclear how well SIKeD would generalize to other domains that require more nuanced strategy selection.\n- W2: The paper lacks comparison with knowledge distillation methods.\n- W3: The code is not available for reproduction."
            },
            "questions": {
                "value": "- Q1: Have the authors considered applying SIKeD to tasks outside of mathematical reasoning to test the generalizability of the strategy selection mechanism?\n- Q2: Does SIKeD require the ground-truth answers of the training data? If not, how does it handle the tasks where the ground-truth answers are not available?\n- Q3: Is there any knowledge distillation baseline that could be used to compare the performance of SIKeD on mathematical reasoning tasks? Current experiments only compare SIKeD against CoT, L2M, PoT and Combined.\n- Q4: Can the authors explain why does the improvement on Qwen 1.5B model is less significant compared to the other base models? What are the potential reasons for this discrepancy?\n- Q5: The small models are tuned with LoRA, what if the parameters of the small models are fully tuned? Would the performance of SIKeD be further improved?\n- Q6: \"The iterative training is stopped when accuracy shows only marginal improvements or declines.\" What specific criteria is used to determine the optimal number of iterations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "It is hard for a small LLM to learn multiple correct trajectories towards the same ground truth answer, which might affect its reasoning capability, especially the generalization to OOD reasoning tasks. The paper has tested distilling multi-trajectory training data  for post-training smaller LLM from large LLMs and observed unsatisfactory results. To alleviate this problem, the paper proposes a mix of self-generated multi-trajectory training data together with the distilled data for SFT. Surprisingly it achieves better reasoning performance compared to existing methods, especially on OOD tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. intuitive and effective method\n\n2. thorough experimental analysis"
            },
            "weaknesses": {
                "value": "The only concern I  have on the experiment part is that only testing the small model\u2019s preference on COT, POT and L2M is a bit constrained. I\u2019m curious to see that among the three methods,  1)  training with the proposed method, 2) pure distilling  and 3) pure self-generating, which method can make the model generate the most diverse trajectories and whether the diversity is aligned with model\u2019s performance on OOD tasks. Because in each strategy, for example in COT, a model can also generate multiple cot trajectories that lead to the correct answer. I\u2019m curious to see which of the methods can improve the general diversity of the model\u2019s output trajectories the most and whether this diversity is aligned with model\u2019s OOD performance."
            },
            "questions": {
                "value": "Please see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes **S**elf-guided\u00a0**I**terative\u00a0**K**nowledge\u00a0**D**istillation (SIKeD), which utilizes outputs of LLMs (teacher) and small models (student) to Iterative train the student model. The main process can be summarized as follows:\n\nsource data (GSM8K training set) \u2192 LLMs generate Cot, PoT and other format reasoning \u2192 training small models \u2192 small models augment data \u2192 filtering \u2192 training small models \u2192 small models augment data \u2192 filtering \u2192 \u2026 \u2026 n \u2026\n\nSource data: GSM8K training set (7,473 samples)\n\nModel: Qwen2 0.5B, 1.5B, SmolLM 1.7B, Gemma 2B / 7B,"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper carefully presents the relevant method analysis and explores how generation strategies evolve across multiple iterations in small models, which is a very intriguing phenomenon."
            },
            "weaknesses": {
                "value": "1. The mathematical notation is overly verbose. There is no proof or any other theoretical contributions. However, this paper employs overly complex notation. And there is also no revision on loss function. The overly complex notation will prevent quick understanding. The main process of proposed method can be summarized in one or two sentences. And all operations performed at the dataset level.\n\n2. Limited generalization. The approach only enhances the GSM8K dataset, but reasoning tests should be conducted on more realistic datasets, such as MATH, Arc-Challenge and so on. And more reasoning tasks also need to be evaluated, such as commonsense reasoning and symbolic reasoning.\n\n3. The absence of important references. The self-distillation in small models is already studied in [1,2,3]."
            },
            "questions": {
                "value": "**Do self-distillation really work in small model continual training ?** \n\nFig. 4 in your paper and Fig. 7 in [3] indicated  the performance decreased, when the n of iteration became large. From a theoretical perspective on synthetic data, the data variance decreases with multiple generations n [4]. To prevent variance reduction, i.e., to enhance data diversity, this paper incorporates data synthesized by LLMs throughout the iterative process. This operation is very important. From data perspective, authors should analyze the data distribution shifting across n. \n\n**Is there any new theoretical insights ?**\n\nThe authors observed shifts in generation strategies over iterations n. What causes this phenomenon? Additionally, as more data is generated, the overall dataset size increases.\n\n------\n\n[1] Ho N, Schmid L, Yun S Y. Large language models are reasoning teachers[J]. arXiv preprint arXiv:2212.10071, 2022.\n\n[2] Fu Y, Peng H, Ou L, et al. Specializing smaller language models towards multi-step reasoning[C]//International Conference on Machine Learning. PMLR, 2023: 10421-10430.\n\n[3] Zhu X, Qi B, Zhang K, et al. PaD: Program-aided Distillation Can Teach Small Models Reasoning Better than Chain-of-thought Fine-tuning[J]. arXiv preprint arXiv:2305.13888, 2023.\n\n[4] Mobahi H, Farajtabar M, Bartlett P. Self-distillation amplifies regularization in hilbert space[J]. Advances in Neural Information Processing Systems, 2020, 33: 3351-3361.\n\n[5] Dohmatob E, Feng Y, Yang P, et al. A tale of tails: Model collapse as a change of scaling laws[J]. arXiv preprint arXiv:2402.07043, 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenge of knowledge transfer from large language models (LLMs) to smaller, more efficient models. The authors identify that smaller models typically demonstrate constrained capabilities and tend to favor single reasoning strategies. To address this limitation, the authors introduce SIKED, an approach that leverages self-generated outputs to create iteratively mixed training datasets. The method promotes the development of diverse reasoning strategies during the knowledge transfer process. The effectiveness of SIKED is demonstrated through empirical evaluations on several mathematical reasoning datasets, showing improvements over baseline approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed problem is worth exploring.\nThe paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. Although the method demonstrates effectiveness, the motivation requires further clarification. While iteratively forming new training datasets based on model outputs is an existing approach, the paper's contribution lies in showing this method can enable diverse reasoning strategies in smaller models. However, the underlying mechanism for how this approach promotes strategy diversity needs clearer explanation.\n2. The experimental evaluation is currently limited to mathematical reasoning tasks. Exploring the effectiveness of the proposed method in other scenarios would provide valuable insights into its generalization capabilities."
            },
            "questions": {
                "value": "1. Can the authors elaborate on the underlying reasons for smaller models' bias towards specific strategies? Does the preferred strategy vary?\n2. The paper states that mixing LLM-generated data with self-generated outputs helps align smaller models with their learned knowledge (L83). However, discarding samples with mismatched outputs creates a different form of bias? Please clarify this.\n3. Please explain the mechanism by which mixing the data promotes diverse strategy selection in smaller models.\n4. How to judge if the generated r_i is correct? (L294)\n5. Is it a typo in L211 & L215 that the same notation appears in different contexts?\n6. Could the authors clarify the strategy sampling process for smaller models? Specifically regarding L413, \"if both CoT and PoT are sampled correctly, our biased strategy choice is PoT\" - is this strategy determined by the smaller model's output? Additional details on the strategy sampling mechanism would be appreciated and helpful"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}