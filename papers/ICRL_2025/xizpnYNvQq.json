{
    "id": "xizpnYNvQq",
    "title": "Revisiting In-context Learning Inference Circuit in Large Language Models",
    "abstract": "In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the inference phenomena in large language models. Therefore, this paper proposes a comprehensive circuit to model the inference dynamics and try to explain the observed phenomena of ICL. In detail, we divide ICL inference into 3 major operations: (1) Summarize: LMs encode every input text (demonstrations and queries) into linear representation in the hidden states with sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge the encoded representations of demonstrations with their corresponding label tokens to produce joint representations of labels and demonstrations. (3) Feature Retrieval and Copy: LMs search the joint representations similar to the query representation on a task subspace, and copy the searched representations into the query. Then, language model heads capture these copied label representations to a certain extent and decode them into predicted labels. The proposed inference circuit successfully captured many phenomena observed during the ICL process, making it a comprehensive and practical explanation of the ICL inference process. Moreover, ablation analysis by disabling the proposed steps seriously damages the ICL performance, suggesting the proposed inference circuit is a dominating mechanism. Additionally, we confirm and list some bypass mechanisms that solve ICL tasks in parallel with the proposed circuit.",
    "keywords": [
        "In-context Learning; Induction Circuit; Mechanistic Interpretability"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We decompose In-context Learning into 3 operations and measure their operating dynamics to catch many inference phenomenon of ICL in Large Langauge Models.",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=xizpnYNvQq",
    "pdf_link": "https://openreview.net/pdf?id=xizpnYNvQq",
    "comments": [
        {
            "comment": {
                "value": "Dear Reviewer yFjP,\n\nWe sincerely appreciate your creative and professional review comments, as well as your positive feedback on our work. Your insights have been invaluable to us, and we are grateful for the opportunity to address your questions and refine our paper based on your suggestions: \n\n----\n\n**Reply to weakness:**\n\n**Q: Inference dynamics when the label corresponding to the query is not present in the context.**\n\nA: Thank you for your thought-provoking question in the weakness! Please allow us to summarize your concern as \u201cWhether the circuit fails when the ground-truth label of the query does not appear in the context\u201d?\n\nAs background, we would like to clarify one point: in some studies, this situation is typically considered an exception, referred to as In-weight Learning [1,2,3]. However, we believe that our framework remains expressive in such cases: \n\nIn this situation, the induction head in step 3 struggles to retrieve similar information from the context, meaning it cannot add label-related information to the query. However, it is still operational, albeit without making a positive contribution. Therefore, although all inference steps are functioning at this point, only step 1 actively contributes. Furthermore, when step 3 is unable to function, we do not actually need to be concerned about the step 2. Therefore, this type of in-weight learning scenario (even when some demonstrations are provided) aligns with the zero-shot case, where only the step 1 is active, and the model relies solely on the information within its internal parameters to solve the problem.\n\nHowever, please kindly note that our inference circuit still retains on such scenarios, so we have not imposed specific conditions on the demonstrations given.\n\nThank you for your insightful question. We will include a brief section in our upcoming revision to clarify this point.\n\n----\n\n**Reply to questions:**\n\n**Q: The source of inspiration for our work.**\n\nA: Regarding your inquiry about the inspiration behind our work, in brief, some previous studies ([1, 2], et al.) have utilized highly embedded inputs (e.g., image features processed through ResNet) as the inputs (namely, $x$ in our paper) to study the ICL process and have obtained intriguing results. However, these results cannot be directly applied to large language models, as the inputs of LLMs are typically not highly embedded. This motivated us to think: if we consider the first few layers of a language model as analogous to these embedding processors (such as the ResNet), and the hidden state corresponding to the last token of the input text (the forerunner token in the paper) as analogous to these embedded features, we could bridge the gap between prior research and LLMs. This paper is the outcome of such bridging, and corresponding to the previous observation about some inference phenomenon (mentioned in 5.3 of our paper), we also conducted detailed measurements to uncover more characteristics of the inference process, which we found to be both novel and intriguing.\n\n----\n\n**Coming Revisions (during the author-response period):**\n\nAs you kindly mentioned in the weakness, we are going to clarify and show how the circuit acts when the ground-truth label is not in the context. We will add a brief explanation to clarify this point.\n\n----\n\nWe are grateful for your detailed comments and the time you have taken to help us refine our study. Thank you again!\n\n**Reference**\n\n[1]. Chan, et al., Data Distributional Properties Drive Emergent In-Context Learning in Transformers. NeurIPS 2022.\n\n[2]. Gautam Reddy. The Mechanistic Basis of Data Dependence and Abrupt Learning in an In-context Classification Task. ICLR 2024.\n\n[3]. Chan, Bryan, et al. \"Toward Understanding In-context vs. In-weight Learning.\" arXiv:2410.23042. 2024."
            }
        },
        {
            "comment": {
                "value": "Dear Reviewer gt4r,\n\nWe truly appreciate the time and attention you devoted to reviewing our paper. Your thoughtful and thorough feedback has provided us with important guidance for revising our paper, and also inspiring new research possibilities. Below, we provide our responses to the points you have kindly raised.\n\n----\n\n**Reply to weakness:**\n\n**Q1: The term \u201csummarize\u201d lacks clarity.**\n\nA1: Yes, the term \u201csummarize\u201d refers to encoding capabilities, where LMs encode the information of input text $x$ into its forerunner token $s$. Could you please specify how this term lacks clarity (which concepts are not conveyed / with which concepts might be confused?) to help us choose a better term in our revision?\n\n**Q2: The significance of \u201csummarization\u201d.**\n\nA2: Our random baseline is 0.125, and the results we have obtained are twice that value, which we believe suggests the presence of this encoding process. Furthermore, in 3.2, we have demonstrated that the inner encoding can be successfully leveraged by a linear classifier, yielding high accuracy on downstream tasks. This further supports the reliability of the evidence.\n\n**Q3: The scope of the copy processing.**\n\nA3: Thank you for raising such an interesting question. Our current intuition is that the copying mechanism is widespread among tokens and does not show specific positional selectivity. However, we acknowledge that this intuition requires experimental validation, and we plan to include both this conclusion and the corresponding experiments in our next revision during the author-response period.\n\n**Q4: The reason for accuracy dropping in late layers in Fig. 5 Right.**\n\nA4. Thank you for your thought-provoking question again. Based on our experimental results, we believe we can support the point you kindly raised, \u201cit is due to gradual degradation of copied information\u201d. This is evident from the following observations: (1). the accuracy drops from layer 37 as shown in Fig. 5 Right; (2). the copying process in the second step does not persist into the deeper layers, as shown in Fig. 5 Left. We will include a brief description of this process in our revision. \n\nAdditionally, we believe there is a profound principle underlying this observation: why are these copied features erased, even with the presence of residual connections? This leads to a new research question: how do language models forget or update information within hidden states to maintain their hidden states capacity? While these questions extend beyond the scope of this paper, we believe they can inspire a more in-depth series of studies. We sincerely appreciate your insightful comments.\n\n**Q5: The setting of ablation experiments and the reference value.**\n\nA5: First, we would like to clarify the setup of the ablation experiment: we attribute each step of the inference process to specific attention connections, which are shown in Table 1. For instance, for step 1, we attribute it to the attention connections from the input text tokens to the forerunner tokens. When we aim to remove this step from the inference, we eliminate all corresponding attention connections from layer 0 to layer $\\\\{25\\\\%, 50\\\\%, 75\\\\%, 100\\\\%\\\\}\\times\\mathrm{Total Layers}$.\n\nFor the baseline problems, please kindly refer to the small numbers presented beneath the results in Table 1\u2014these are not standard deviations, but the control values when the same amount of randomly selected attention connections are removed. The accuracy drops in the control values are clearly weaker than the experimental values where the important attention connections are removed, leading us to believe our results are reliable. \n\n----\n\n**Coming Revisions (during the author-response period):**\n\n[To Q3] We are going to measure the scope of the copy processing.\n\n[To Q4] We are going to describe the erasing processing briefly in 4.2.\n\n[To Q5] We are emphasizing the setting of the ablation experiment in maybe Appendix. \n\nMoreover, we acknowledge that many experienced researchers might interpret the small numbers in Table 1 as standard deviations like the usual practice of academic papers, so we are revising the presentation to emphasize this, which will be included in the upcoming revision.\n\n----\n\nOnce again, thank you for your time and valuable feedback. We look forward to submitting the revised manuscript and hope it meets your expectations."
            }
        },
        {
            "comment": {
                "value": "Dear Reviewer Z26B,\n\nWe sincerely appreciate your thorough review and valuable feedback on our manuscript. Your insightful comments not only enhance our research but also prompt further research ideas. Below, we provide our responses to the points you have kindly raised.\n\n----\n\n**Reply to questions:**\n\nWe sincerely appreciate the valuable questions you raised, and believe that the two questions you kindly pointed out pertain to the causality of our proposed inference framework. \n\n**Q1: Lack of baseline in ablation studies.**\n\nA1: First, we would like to address your **second** question regarding the importance of referencing the effect of removing unimportant components in the ablation study. Please kindly refer to the small numbers presented beneath the results in Table 1\u2014these are not standard deviations, but the control values when the same amount of randomly selected attention connections are removed. The accuracy drops in the control values are clearly weaker than the experimental values where the important attention connections are removed, leading us to believe our results are reliable. \n\n**Q2: How significant / associated is this inference circuit?**\n\nA2: As for your **first** question, to our knowledge, measuring how various modules in a Transformer are *actually* connected remains an open question. Although some idealized concepts are provided in the literature (please refer to the Virtual Weights section in [1]) to calculate the similarity of the reading space (defined by attention head\u2019s $W_Q$, $W_K$ and $W_V$) and the writing space ($W_O$), they are difficult to apply to large language models as they overlook numerous modules (e.g., LayerNorm), which can deform the hidden space. Therefore, our paper uses the ablation study in Table 1 to confirm that our inference framework is effective and dominant.\n\n> *When we say \u201cactually\u201d, we want to evaluate whether two modules are sharing a common subspace of hidden states to communicate, as opposed to simply being sequentially \u201cconnected\u201d by the layer order.\n\n----\n\n**Coming Revisions (during the author-response period):**\n\n[To Q1] We acknowledge that many experienced researchers might interpret the small numbers in Table 1 as standard deviations like the usual practice of academic papers, so we are revising the presentation to emphasize this, which will be included in the upcoming revision.\n\n----\n\nOnce again, we are deeply grateful for your professional review and constructive suggestions.\n\n**Reference**\n\n[1]. Elhage, Nelson, et al. \"A mathematical framework for transformer circuits.\" Transformer Circuits Thread 1.1 (2021): 12."
            }
        },
        {
            "comment": {
                "value": "Dear Reviewer keS9,\n\nThank you for your insightful review of our manuscript. We greatly appreciate the thoroughness of your comments and the helpful suggestions, which will certainly strengthen our paper. Below, we provide our responses to the points you have kindly raised.\n\n----\n\n**Reply to questions:**\n\n**Q1: Exceptions of inference behavior.**\n\nA1: Firstly, in the ablation experiment described in 5.1, when we completely suppressed the proposed inference framework, the accuracy dropped to a random level, suggesting that the proposed inference framework might be the dominant mechanism.\n\nEven with exceptions (which we refer to as bypasses), we believe that our paper's contribution remains: LLMs are trained on natural datasets by gradient descent, during which numerous inference behaviors may emerge. The circuits we have identified highlight a significant pattern among these noisy behaviors, which we believe is sufficiently impactful. We fully agree with your statement in the latter part: it is nearly impossible to enumerate all inference pipelines and their interactions, but our paper at least uncovers one of the most significant patterns.\n\n**Q2: Novelty.**\n\nA2: The main novelty of our paper lies in adapting and conducting detailed measurements of inference circuits, originally discovered in toy models, within real, large-scale language models. This has provided numerous insights and successfully explained a range of observed inference phenomena with a single circuit. In our view, please allow us to state that the unification of these intricate inference phenomena (as summarized in Section 5.3 of the paper) into a single inference circuit is both novel and exciting.\n\n**Q3: Robustness of Kernel Alignment.**\n\nA3: Your insight is absolutely correct: in fact, the feature vectors generated by BGE and those produced by LLM are highly likely to exist in different feature spaces, which is why we use Kernel Alignment to test the similarity between these feature vectors. The reason is that Kernel Alignment measures the similarity of the neighborhood relationships within the feature sets derived from the same set of objects by different models, rather than directly computing the similarity of feature vectors from different spaces. This makes sense: our long-standing practice starting from word embeddings tells us that adjacency relationships determine semantics. Therefore, even if the feature vectors come from different models, Kernel Alignment can still stably compute their semantic similarity, as also shown in previous work [1].\n\nAdditionally, in 3.2, we have strengthened the results of Kernel Alignment: we have confirmed that the encodings on the forerunner tokens can be used for linear classifiers and achieve high classification accuracy, which means the features collected enough information from the input text.\n\n----\n\n**Coming Revisions (during the author-response period):**\n\n[To Q2] We will emphasize the novelty of our work, perhaps in the discussion section.\n\n----\n\nWe truly appreciate your thoughtful review and will incorporate your suggestions to improve the paper. Thank you again for your support.\n\n**Reference**\n\n[1] Huh, Minyoung, et al. \"Position: The Platonic Representation Hypothesis.\" ICML 2024."
            }
        },
        {
            "summary": {
                "value": "This paper aims to explain the mechanisms behind in-context learning (ICL) using the inference circuit framework.\n\nAccording to the authors, the ICL process consists of three internal steps:\n1. Summarize: Large language models (LLMs) encode each demonstration within its corresponding forerunner token,  $s_i$ .\n2. Semantics Merge: The semantics of each demonstration and its label are combined into the representation of the label  $y_i$ .\n3. Feature Retrieval and Copy: LLMs rely on the accumulated labels  $y_{1:k}$  to respond accurately to the query  $s_q$ , yielding the most appropriate answer.\n\nEach step is empirically validated using methods such as kernel alignment and embedding comparisons. The authors also seek to align their findings with those of prior research, reinforcing the credibility of the arguments presented in this work."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Attempts to explain the inner workings of ICL, based on reasonable assumptions and investigative tools.\n- The findings align with previous work, encouraging readers to accept the claims presented in the paper.\n- Visualized results help readers quickly grasp the core concepts and findings of the paper."
            },
            "weaknesses": {
                "value": "- While the proposed framework is logical and reasonable, it remains challenging to argue definitively that the core mechanism of ICL follows the assumptions presented in the paper. As noted in Section 5.2, there are exceptions that do not align well with the proposed framework, raising concerns that the explanations may be superficial and fail to capture the essence of ICL. This is understandable, as fully explaining the inner workings of neural networks is inherently difficult, if not nearly impossible.\n- I am somewhat unclear about the core novelty of this paper. As I understand it, the primary contribution seems to be the attempt to apply the existing inference circuit framework to the ICL of specific LLMs, including LLaMA 3. In Section 2.1, I did not find explanations that clarify why the procedure conducted in this paper is particularly innovative, compelling, or novel. More comprehensive comparisons with prior work employing induction or inference circuits to illustrate the inner workings of ICL would be helpful to underscore the merits and uniqueness of this study.\n- In Section 3.1, sentence embeddings generated by an external encoder (BGE M3) are compared to hidden representations computed by an LLM. Since these two representations come from different models, without any modification or fine-tuning, there is a risk that their vector spaces are not aligned or compatible. This raises concerns about whether this experiment is sufficiently reasonable."
            },
            "questions": {
                "value": "Please refer to the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a three-step inference circuit to capture the in-context learning (ICL) process in large language models (LLMs):\n\n1. Summarize: Each input (both demonstration and query) is encoded into linear representations in the model's hidden states.\n\n2. Semantics Merge: The encoded representation of each demonstration is merged with its label, creating a joint representation for the label and demonstration.\n\n3. Feature Retrieval and Copy: The model retrieves and copies the label representation most similar to the query's representation, using this merged representation to predict the query's label.\n\nThis circuit explains various ICL phenomena, such as position bias, robustness to noisy labels, and demonstration saturation. Ablation studies show that removing steps in this process significantly reduces performance, supporting the dominance of this circuit in ICL. The paper also identifies some bypass mechanisms that operate in parallel, assisting the inference circuit through residual connections."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors proposed to use the mutual nearest-neighbor kernel alignment of the intermediate representations of LLMs and sentence embeddings produced by another pre-training model to assess the quality of these representations. This method is novel.\n\n2. Extensive analysis has been performed on all three steps of the proposed framework. Possible explanations have also been provided for many phenomena.\n\n3. The experiments are performed with real-world LLMs and datasets, which makes the insights more likely to be useful in practice.\n\n4. The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The majority of the analysis is based on associations without verifying their strength and whether those effects are causal. For example, Figure 2 right does not look significant enough for me. The peaks highlighted in Figure 5 also look pretty noisy to me.\n\n2. The causal evidence that the authors provided in the ablation study only shows the effect of deleting the hypothesized important components in ICL. What if unimportant components are deleted? Would they have a similar effect? Only if the unimportant components have a significantly weaker effect on ICL performance, can we draw a causal conclusion that the proposed three-step process dominates ICL."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the mechanisms within large language models (LLMs) that enable in-context learning (ICL) tasks, breaking down the process into three distinct stages: summarization, semantic merging, and feature retrieval/copying. The study employs a variety of experiments across multiple LLMs to validate its findings. Overall, this paper presents valuable insights that can contribute to the field of LLM research, particularly within the ICL community."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The findings in this paper are clearly explained. Experimental results and visualizations enhance readability and help the audience follow the study's progression easily.\n\n2. This work is well-connected to existing ICL research, with discussions that compare its findings to prior studies on ICL explainability and demonstration selection.\n\n3. The study uses multiple LLMs, strengthening the generalizability of the findings across different model architectures.\n\n4. The insights provided are thought-provoking and have potential practical implications for ICL applications."
            },
            "weaknesses": {
                "value": "1. [Section 3.1] The authors used mutual nearest-neighbor kernel alignment to evaluate LLMs' summarization abilities. However, the term \u201csummarize\u201d lacks clarity. Does it refer to encoding capabilities similar to those in BGE?\n\n2. [Section 3.1] Additionally, the kernel alignment metric may not be sufficiently robust, as alignment scores in Figure 2 range only from 0.25 to 0.35, which is not significant enough. Consequently, the finding on \u201csummarization\u201d may hold only to a limited extent.\n\n3. [Section 4.1 \u2013 Copying from Text Feature to Label Token] It is unclear whether the copying mechanism is applied solely to label tokens or if it extends to other tokens within the input. Using results from other tokens as a baseline could provide a more nuanced understanding of the copying process.\n\n4. [Figure 5, Right] After layer 40, the classification accuracy drops significantly. The authors did not investigate potential reasons for this decline. Could it be due to the gradual degradation of copied information?\n\n5. The experimental setup in Section 5.1 is insufficiently detailed. For instance, how many attention heads are disconnected at each layer? Additionally, the experiments lack certain baselines, such as randomly disconnecting some attention heads to observe the impact on model performance.\n\nDespite these questions and weaknesses, I believe this paper still offers meaningful insights."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N.A."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a three-stage ICL circuit hypothesis and provides thorough empirical examinations of the existence and significance of these stages. Within this circuit framework, many phenomena are explained, such as how Forerunner Tokens encode input text representations and the bias in input text encoding towards position. These findings present intriguing insights."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- **Originality:** To my knowledge, the three-stage circuit proposed by the authors is a novel contribution.\n- **Quality:** The hypothesis put forward is reasonable, and the experiments are thorough with a well-crafted methodology.\n- **Clarity:** The arguments and evidence presented in the paper are clear, and the experimental descriptions are appropriately detailed.\n- **Significance:** Currently, ICL is one of the most important applications in the LLM field, and understanding the mechanisms behind ICL will greatly aid in enhancing its performance."
            },
            "weaknesses": {
                "value": "The three-stage ICL framework appears to have implicit applicability conditions, which I believe should be clarified.\n\nFor example, in Fig. 1 on page 2, a few-shot scenario with $ k=2 $ is presented, which indeed fits the three-stage ICL circuit framework. However, in a zero-shot scenario ($ k=0 $), step 1 may still exist, but steps 2 and 3 would not be applicable. In a few-shot scenario with $ k=1 $, steps 1 and 2 might still apply, but step 3 cannot exist.\n\nTherefore, the framework proposed in this paper should be limited to discussions of scenarios where $ k \\geq 2 $. A related question arises: if the focus is restricted to this scenario, what potential issues might emerge?\n\nFurthermore, if we condition on $ k \\geq C $ (where $ C $ is a fixed value), could this value vary depending on the problem type? For instance, in tasks like SST-2 and SST-5, which have different label set sizes, might the value of $ C $ differ across these scenarios?"
            },
            "questions": {
                "value": "The three-stage framework proposed in the paper is quite interesting. The questions I have raised are mentioned in the weaknesses section. Here, I would like to know what inspired you to propose this framework. Each part of the framework consists of very specific ideas\u2014were they derived from repeated trial and error, or were they inspired by something else? Alternatively, are they improvements on a significant prior work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}