{
    "id": "MvEkN2ejZ1",
    "title": "MamBEV: Enabling State Space Models to Learn Birds-Eye-View Representations",
    "abstract": "3D visual perception tasks, such as 3D detection from multi-camera images, are essential components of autonomous driving and assistance systems. However, designing computationally efficient methods remains a significant challenge. In this paper, we propose a Mamba-based framework called MamBEV, which learns unified Bird's Eye View (BEV) representations using linear spatio-temporal SSM-based attention. This approach supports multiple 3D perception tasks with significantly improved computational and memory efficiency. Furthermore, we introduce SSM based cross-attention, analogous to standard cross attention, where BEV query representations can interact with relevant image features. Extensive experiments demonstrate MamBEV's promising performance across diverse visual perception metrics, highlighting its advantages in input scaling efficiency compared to existing benchmark models.",
    "keywords": [
        "BEV",
        "Bird\u2019s Eye View",
        "State space model",
        "causal attention",
        "3d computer vision"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=MvEkN2ejZ1",
    "pdf_link": "https://openreview.net/pdf?id=MvEkN2ejZ1",
    "comments": [
        {
            "summary": {
                "value": "The main content of the paper is focused on enabling state space models (SSMs) to learn birds-eye-view (BEV) representations, specifically in the context of 3D representation learning tasks. \n\nThe authors propose a novel method called MAMBEV, which incorporates SSMs and linear attention to address the challenges of capturing temporal and spatial relationships in multiview video and fusing distinct visual representations. \n\nThey conduct experiments using the nuScenes dataset and achieve results comparable to the state of the art."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. This paper presents a method for applying Mamba to BEV detection."
            },
            "weaknesses": {
                "value": "1. The paper has several writing issues:\n* Many sentences are lengthy, which hinders comprehension. For example, line 188, 191, 201.\n* Certain sections lack citations, such as on line 206 and 207.\n* Some areas seem to be missing punctuation, possibly in line 225.\n\n2. The experimental comparison is not entirely fair, as the pretrained model used in this study differs from other pretrained models, which may affect the results.\n\n3. The overall improvement in the experimental results is quite limited, and the value of applying SSM to BEV detection is not clear.\n\n4. There is a lack of comparison with more results, such as VideoBEV, which is also a long-sequence application but was not included in the comparison.\n\n5. The paper mentions reducing computational load and improving efficiency. Therefore, the proposed method should provide relevant metrics related to training and testing, such as training time and model latency during testing."
            },
            "questions": {
                "value": "Please refer to section Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents MamBEV, a novel 3D object detection framework designed to improve BEV-based perception systems, particularly for autonomous driving. By introducing state-spatial separation and multi-axis multiview projection, MamBEV addresses the challenges in handling multi-camera BEV representation. The proposed method achieves promising results in complex driving scenarios, showcasing improvements in detection accuracy and computational efficiency."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Lower computational cost.\n2. Good performance, comparable to VideoBEV."
            },
            "weaknesses": {
                "value": "1.The paper could have provided more ablation studies to dissect the contributions of individual components of MamBEV.\n2. Limited discussion on potential limitations of the framework, such as scalability or adaptability to other domains beyond autonomous driving.\n3. limit novelty. The BEVFormer architecture introduces Mamba."
            },
            "questions": {
                "value": "1. An important reason for using Mamba is to achieve lower computational cost; however, the paper only provides theoretical calculations without experimental validation, which is a crucial aspect.\n2. In the bottom experiments of Table 1, why do BEVFormerV1-Small and BEVFormerV1-Base use different frames?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new approach to building bird's-eye-view (BEV) maps from multiple cameras. With an aim of mobile deployment, the paper investigates approaches that depart from the existing transformer-based push (from images to 3D) or pull (from 3d to images) methods. In particular, recent work on state space models (SSM) is an inspiration to replace transformers and their quadratic attention computational complexity. Of particular interest here is to leverage ideas from the Mamba-2 and Hydra SSMs and adapt them to the BEV problem. The new SSM-based BEV model is well described and well tested, including several ablation studies.\n\nTwo potential caveats noted below are that only one dataset is used (as seems to be the case with other SOTA approaches, so likely not the authors' fault), and the improvements over SOTA are small and sometimes mixed depending on which metric one looks at."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- significant problem domain where much improvement remains possible\n\n- very well explained technical approach\n\n- great to see asymptotic complexity analysis of the various stages of the approach\n\n- convincing evaluation on the nuScenes dataset\n\n- experiments on increasing temporal information, SSM vs deformable attention, feature normalization, scaling, and feature traversal order are interesting and convincing."
            },
            "weaknesses": {
                "value": "- table 1, the proposed model does not outperform SOTA in all metrics. This table needs to be described better. If NDS is the main metric, then what about Focal-DETR? Should explain more clearly which table rows are compared in the main text lines 395-396\n\n- should include a figure with reconstruction samples, for qualitative evaluation by the reader.\n\n- lines 221-222: nonsense sentence"
            },
            "questions": {
                "value": "- only tested on nuScenes (like some of the other SOTA models, including DETR3D and BEVformerV2). Is there another suitable dataset that this could be tested on to better demonstrate generality?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces the Mamba mechanism into 3D visual perception task to learn bird's-eye view (BEV) representations in order to address computational and memory efficiency problems, and designs a Spatial Cross Attention module for the task. The effectiveness of the design is demonstrated through extensive experiments and ablation studies, which would bring benefits to application scenarios such as autonomous driving and driver assistance systems."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. MamBEV proposes a design based on SSM, which exceeds the performance of existing Transformer-based structures in several metrics.\n2. MamBEV provides sufficient experimental results to illustrate the effectiveness of the design."
            },
            "weaknesses": {
                "value": "1. Lack of visualization of results. I am quite sorry to say that I am not familiar with the nuScenes dataset, so could the authors provide some visual examples to show the advantages of MamBEV?\n2. Lack of more detailed description of the effectiveness of the Spatial Cross Attention module. Since the authors mention in the Introduction that \"we further demonstrate that our method can better capture longer dependencies in multiview video\", whether this word \"better\" means the module proposed in the paper comparing to other SSM Attention designs, or Mamba-2 comparing to Mamba-1, or Mamba comparing to Transformer, could it be supported and presented through other ways in addition to the accuracy results on the specific dataset?"
            },
            "questions": {
                "value": "The main issues have been raised in Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents MamBEV, a novel framework designed for 3D visual perception tasks like 3D detection from multi-camera images, which are crucial for autonomous driving and assistance systems. The framework leverages a Mamba-based approach to learn unified Bird\u2019s Eye View (BEV) representations using linear spatio-temporal SSM-based attention, which improves computational and memory efficiency across multiple 3D perception tasks. The key is SSM-based cross-attention, which functions similarly to standard cross-attention, enabling BEV query representations to interact effectively with relevant image features. Experiments are conducted to demonstrate the effectiveness of the method. \n\nPos:\n- Enabling Mamba to BEV representation seems to be interesting.\n\nCons:\n- The authors claim that proposed method supports multiple perceptual tasks but has no proof.\n- The article keeps emphasizing computational efficiency, but without proofs.\n- The core figures are crude and difficult to understand\n- Insufficient experimentation and comparison methods"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Enabling Mamba to BEV representation seems to be interesting."
            },
            "weaknesses": {
                "value": "Cons:\n- The authors claim that proposed method supports multiple perceptual tasks but has no proof.\n- The article keeps emphasizing computational efficiency, but without proofs.\n- The core figures are crude and difficult to understand\n- Insufficient experimentation and comparison methods"
            },
            "questions": {
                "value": "Please refer to the above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}