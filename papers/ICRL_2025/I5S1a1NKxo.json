{
    "id": "I5S1a1NKxo",
    "title": "Data-scarce distillation for large-scale vision language models",
    "abstract": "Vision-language models (VLMs) have emerged as extremely strong zero-shot and few-shot image classifiers, performing on par with task-specific models. However, they can be unnecessarily heavy-weight for task-specific downstream applications. While existing lines of work have successfully compressed VLMs and other foundation models to varying degrees, most focus on preserving the generality of these models, rather than leveraging their power for a particular task.\nIn this work, we focus on the setting in which we have a limited amount of data on a downstream image classification task and a limited inference budget.\nTo satisfy these constraints, we focus on distilling the strong few-shot performance of CLIP on image classification tasks into a more efficient model. \nWe introduce the SIDCLIP (Synthesize-Initialize-Distill CLIP) method and highlight its three components that are critical to obtaining strong performance: 1) augmenting the classifier with \\textit{synthetic data} generated by leveraging CLIP itself; 2) \\textit{initializing} the modeling process using a smaller CLIP model pretrained on the target architecture; and 3) incorporating \\textit{knowledge distillation} to maximally mimic the performance of the larger model.  \nOur set of proposed strategies produces a compact model that performs within 16\\% and 10\\% of CLIP's linear probe performance on 1 and 8 shot datasets respectively, while using a model with less than 2\\% of the parameters of CLIP's image encoder. \nWe hope our work can be useful as a practical guide for leveraging the power of foundation models in downstream data-scarce and budget constrained settings.",
    "keywords": [
        "distillation",
        "few-shot",
        "compression",
        "CLIP",
        "foundation models"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=I5S1a1NKxo",
    "pdf_link": "https://openreview.net/pdf?id=I5S1a1NKxo",
    "comments": [
        {
            "summary": {
                "value": "This paper proposed SIDCLIP: Synthesize-Initialize-Distill CLIP, for few-shot distillation in data-scarce setting. SIDCLIP first augments limited labeled data using text-to-image generation model. Then, it trains a small model initialized with small CLIP. Finally it transfers knowledge from a large CLIP to the smaller model. Experiments on Stanford Cars, Oxford Flowers, Food-101 demonstrates the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Unlike general-purpose distillation methods, the proposed approach focuses on specific downstream tasks, being highly effective in specialized applications.\n\n* SIDCLIP has a fraction of the parameters of the original CLIP, making it computationally and parameter efficient."
            },
            "weaknesses": {
                "value": "* The use of text-to-image generation to create synthetic data can introduce computational overhead, potentially making the framework slower or more resource-intensive.\n\n* The paper primarily experiments with EfficientNet B0 as the small model architecture. How does SIDCLIP perform with different or larger backbones? Testing with a broader range of architectures would help demonstrating the generalizability of SIDCLIP.\n\n* While SIDCLIP focuses on task-specific performance, its robustness in handling out-of-domain data (datasets significantly different from those in CLIP\u2019s pretraining) is questionable. Since most of CLIP\u2019s parameters remain frozen, there may be limitations in SIDCLIP\u2019s adaptability to new domains, which could restrict its effectiveness in real-world, diverse data settings.\n\n* The combination of synthetic data generation, initialization, and distillation steps may limit SIDCLIP\u2019s practicality in scenarios requiring minimal preprocessing. The entire pipeline involves multiple stages that could increase setup and training time. It would be beneficial to provide the computation requirements compared to existing methods."
            },
            "questions": {
                "value": "* Could the authors provide more quantitative details on SIDCLIP\u2019s training efficiency compared to other distillation methods?\n\n* How sensitive is SIDCLIP\u2019s performance to the quality of synthetic data generated? How would different generation models (simpler generative model instead of Kandinsky) affect the few-shot results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper focuses on the issues of limited data for a fine-grained downstream task and limited inference budget, and introduces a method called (Synthesize-Initialize-Distill CLIP) (SIDCLIP), involving three components: \n- Synthesize (S): a novel approach to generate synthetic data by leveraging CLIP;\n- Initialize (I): model initialization of a smaller CLIP model pretrained on the target architecture; \n- Distill (D): knowledge distillation to a larger model. \nExperimental results are reported on three task-specific image classification datasets: StanfordCars, OxfordFlowers, and Food101."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The author(s) show a lot of analysis for experiments to support their claim. While the proposal involves three components, there are specific ablation studies to show the advantage of the new model design and the comparison of eval metrics and visualization of synthetic image to exhibit the data quality."
            },
            "weaknesses": {
                "value": "The new proposal is extending several existing methods and putting things together to reach some better performance. While this is innovative, it will be more convincing to analyze the role of each component for the overall performance improvement.\n\nLine 339 has a typo \"intialize.\""
            },
            "questions": {
                "value": "Related to the weakness, for example, will the synthetic data help other models also?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a framework to distill a vision-language model (CLIP) in a data-scarcity setting. Starting from a CLIP-ViT-L/14 pretrained model and a few-shot downstream data of interest, the authors propose to train a small network (EfficientNet B0) in order to tailor it for the specific dataset at hand. The framework, called SIDCLIP, consists of three components: 1- Synthesizing data using a pretrained latent diffusion model that takes as input a CLIP embedding, namely a linear combination of the text embedding describing the class and embeddings of images from the same class coming from the few-shot dataset, 2- Initializing the small student network by CLIP-like pretraining on DataComp, and 3- knowledge distillation from the large teacher model (i.e. CLIP-ViT-L/14) using classical logit matching. Experiments on 3 datasets demonstrate the effectiveness of each of the 3 components, as well as advantages compared to other \"small\" general-purpose baselines in the few-shot setting."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper is written in a simple language and is easy to understand and follow.\n* The combination of synthetic data generated using the few-shot samples, initialization with CLIP-like pretraining, and knowledge distillation shows a good performance on the downstream dataset, and ablation shows the effectiveness of each component."
            },
            "weaknesses": {
                "value": "* Besides combining synthetic data, initialization and distillation for the data-scarcity specific setting, the proposed approach is limited in novelty. The main elements (distillation [1] and data generation [2]) are straightforwardly utilized. While their combination is interesting, the results are not very surprising due to the expected complementary knowledge coming from different sources of information: the generation capacity of latent diffusion informed with the specific dataset at hand, the pretrained ViT-L/14 knowledge and the DataComp data for pretraining as initialization. \n* While the classical logit-based knowledge distillation approach seems to be effective, more advanced techniques were introduced in the literature ([A],[B],[C]). Since the approach is straightforward, it would be interesting to explore some other distillation methods as a positioning among other design choices. For example, some distillation methods were shown to improve classical distillation by decoupling target class and non-target class terms [B]. Other methods  show that distillation is also possible for \"smaller\" and \"weaker\" teacher [A]. Investigating such choices might help answering the question whether ViT-L/14 is needed as a teacher, or if the same or close results can be achieved with a smaller version of CLIP as teacher.\n* The method is tested on only 3 datasets. Corroboration on more datasets helps assessing the value of the proposed framework and its practical use, especially that the target is a data-specialized model not a general purpose one. For example, the authors could add more experiments on common datasets used for few-shot learning with CLIP: ImageNet, DTD, EuroSAT, FGVCAircraft, SUN397, Caltech101... Please refer to [D] for more examples of such datasets. Some of these dataset (like DTD, EuroSAT, FGVCAircraft) differ from the used ones in being more specialized and fine-grained. How would each of the components help the total performance for such datasets?\n* Compared to the baselines (TinyCLIP,TinyViT-5M), SIDCLIP uses a latent diffusion model, which makes the comparison unfair, and even the data-scarcity setting no more holding. Moreover, using DataComp pretraining data for the initialization stage makes the comparison more difficult, as TinyCLIP and TinyViT do not use the same dataset.\n\n[1] Hinton et al., Distilling the Knowledge in a Neural Network. NeurIPS Workshops 2014                                                               \n[2] Razzhigaev et al., Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion. arxiv 2023                                        \n[A] Yuan et al., Revisiting knowledge distillation via label smoothing regularization. CVPR 2020                                                                         \n[B] Zhao et al., Decoupled Knowledge Distillation. CVPR 2022                                                                                                               \n[C] Beyer et al., Knowledge distillation: A good teacher is patient and consistent. CVPR 2022                                                                                          \n[D] Zhou et al., Learning to Prompt for Vision-Language Models. IJCV 2022"
            },
            "questions": {
                "value": "* For the ablation in Figure 1, are there any specific reasons for not showing other options like ENB0+S or ENB0+S+I? For example training ENB0+S (using synthetic data, no specific initialization, no distillation), might help better assessing the role of synthetic data.\n* It is not clear how the linear probe (LP) was trained. Properly training LP is detrimental for getting the best of it, and classification results might largely fluctuate depending on the fine-tuning strategy [E],[F]. Could the authors give more details on this please?\n\n[E] Yu et al., Task Residual for Tuning Vision-Language Models. CVPR 2023                                    \n[F] Huang et al., LP++: A Surprisingly Strong Linear Probe for Few-Shot CLIP. CVPR 2024"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method to enhance model performance in data-scarce scenarios by distilling a smaller model using generated data derived from the CLIP model\u2019s own image information. The paper also examines the impact of this generated data on model performance. While the approach shows some innovation in synthetic data generation, the overall methodology remains relatively simple and is validated only on three fine-grained datasets without experiments on larger-scale data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper introduces a novel approach for synthetic data generation.\n2. It improves edge deployment of smaller models by distilling knowledge from the CLIP model."
            },
            "weaknesses": {
                "value": "1. The experiments are limited in scale, with no evaluations on large datasets, such as ImageNet.\n2. The range of compared models is narrow, including only EfficientNet-B0 (EB0)."
            },
            "questions": {
                "value": "1. What would the performance be if the CLIP teacher model provided a logits-based distillation?\n2. Does the proposed method perform similarly across all backbones? Specifically, how does it perform with larger models, such as ResNet-18 mentioned in the paper?\n3. How is the quality of generated data ensured?\nI would be glad to raise my score if the above questions could be addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}