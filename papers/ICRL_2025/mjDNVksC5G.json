{
    "id": "mjDNVksC5G",
    "title": "Can Transformers Perform PCA ?",
    "abstract": "Transformers demonstrate significant advantage as the building block of Large Language Models. Recent efforts are devoted to understanding the learning capacities of transformers at a fundamental level. This work attempts to understand the intrinsic capacity of transformers in performing dimension reduction from complex data. Theoretically, our results rigorously show that transformers can perform Principle Component Analysis (PCA) similar to the Power Method, given a supervised pre-training phase. Moreover, we show the generalization error of transformers decays by $n^{-1/5}$ in $L_2$. Empirically, our extensive experiments on the simulated and real world high dimensional datasets justify that a pre-trained transformer can successfully perform PCA by simultaneously estimating the first $k$ eigenvectors and eigenvalues. These findings demonstrate that transformers can efficiently extract low dimensional patterns from high dimensional data, shedding light on the potential benefits of using pre-trained LLM to perform inference on high dimensional data.",
    "keywords": [
        "Principle Component Analysis",
        "Transformers",
        "Machine Learning Theory"
    ],
    "primary_area": "learning theory",
    "TLDR": "Transformers can provably perform PCA.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=mjDNVksC5G",
    "pdf_link": "https://openreview.net/pdf?id=mjDNVksC5G",
    "comments": [
        {
            "summary": {
                "value": "The paper theoretically and empirically investigates whether one can use the Transformer architecture and supervised, in-context learning (ICL) to perform PCA, specifically the power method for computing top principle components.\nThe results build upon [Bai et al. (2024)](https://arxiv.org/abs/2306.04637)'s recent work on using Transformers and ICL to implement various ML algorithms (least-squares, ridge, lasso, and SGD).\nThe theoretical results provide an approximation bound on the PCs as well as a generalization error bound of $n^{-1/5}$.\nEmpirically, the authors show that the Transformer can approximate the first few principle components and the corresponding eigenvalues on Gaussian synthetic data and on real data (MNIST and Fashion-MNIST)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The high-level problem statement is clear, timely, and potentially interesting. The theoretical and empirical results are novel.\n- The theory portion of the paper appears to be rigorous (I did not check the proofs in detail).\n- The experiments are generally reasonable and the results are consistent with the theory to some extent."
            },
            "weaknesses": {
                "value": "- The biggest issue for me is that the paper is not very clear about the significance of the results: _why_ is it important that Transformers can perform PCA? I don't imagine people using Transformers to compute PCA instead of the existing methods, so instead the results have to give meaningful insights on what it is about Transformers, or the ICL procedure, that allows them to perform PCA. I think there is a lot of missed potential in the discussion section to elaborate on this.\n- A related question is: what is it about the task of PCA that sets it apart from other ML tasks in Bai et al. (2024), like least-squares, ridge, and lasso? Does the proof reveal any interesting insight about how either Transformers or ICL suit the specific task? Or is it just that any iterative algorithm can (in principle) be implemented in an ICL setting? How much would the performance degrade if I replace the Transformer model (partly or entirely) but keep the ICL framework? I think the paper has to be restructured in a way that some of the theoretical results are presented a bit more briefly and these questions are discussed (in words) in a lot more detail.\n- The empirical results show that the principle components are not very accurate beyond the first few and/or in high dimensions. I think one thing that will make things clearer is if there were a baseline using just the power method on the same data, as this will clarify whether this is a limitation of the Transformer or the power method itself.\n- Some of the conditions in the theorem are not well-explained/motivated in words, e.g., that the eigenvalues are distinct and that the L2 norm of the input is bounded (I don't see $B_X$ appearing in the resulting error bound). In particular, I think it's important to distinguish which conditions are necessary for the Power Method to work in the first place, and which are necessary for the Transformer to perform PCA.\n- I'm not sure if I agree with the claim that \"transformers are able to produce small error on predicting eigenvalues\" on real data. The RMSE numbers are 10x larger than the synthetic case, and the eigenvectors are not similar for k >= 3. I think it would be more accurate to say that, on real data, the Transformer can approximate the first few eigenvectors and eigenvalues well, but not necessarily all of them. (This is not a weakness per se, but I think it's important to be clear about the limitations of the method.)"
            },
            "questions": {
                "value": "- p. 4, text: I believe you means \"symmetric\" not \"asymmetric\" here? There are different terminologies being used between \"principle eigenvectors\", \"left singular vectors\", and of course \"principle components\" in PCA. My recommendation is to consistent terminology throughout the paper.\n- p. 5, figure 1: what's the difference between blue and purple blocks?\n- p. 6, remark 3: typo in \"frist\"\n- p. 6, remark 5: what is the significance of the $n^{-1/5}$ rate specifically? How does one make sense of how good or bad this rate is?\n- p. 7, line 375: typo \"Figure ??\", \"differernt\"\n- p. 8, line 431: why exactly do these metrics (not loss functions, to be precise) match the intuition of eigenvalues/vectors?\n- table 1: what is shown in parentheses? \n- figures 2 & 4: why does the RMSE decrease as k increases for synthetic data, but increase for MNIST? Also, it may be a bit more intuitive to show the RMSE for individual components rather than the sum of the first k components."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work studies the problem of whether transformers can perform PCA. For this, they used a supervised setting where the outputs are principal components of the inputs. Inspired by the classical power iteration method, they construct the weights of a transformer model that approximately does PCA. For this, they assume that the eigenvalues are spaced out and bounded, and use classical results on random vectors to construct an auxiliary matrix they utilize to prove their bounds. Experiments on synthetic and real-life data weakly validate their studies. The target audience are people working on ML theory."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The paper studies approximation capabilities of transformers from a theoretical perspective. This adds to a recent array of works which study the theoretical capabilities of transformers [1, 2, 3] and is potentially interesting.\n\n- The experiments probe ablations of a few different parameters in both simulated and a couple of real-life datasets. While the observations are intuitive, it (weakly) validates some observations of the theory.\n\n#### References:\n- [1] Transformers Learn Shortcuts to Automata\n- [2] Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers.\n- [3] (Un)interpretability of Transformers: a case study with bounded Dyck grammars"
            },
            "weaknesses": {
                "value": "- While the question of transformers can perform PCA sounds interesting on the surface, I'm unable to gauge how interesting the bounds given here are. For one, we have universality results for neural networks that the authors cite, but do not seem to carefully compare against. Secondly, the bounds derived seem highly complicated and as the authors mention, it's not clear if they're tight. It's also not clear if they're useful or not, apart from being some generic generalization bounds. See also the question at the end.\n\n- To continue the above point, this is more of a work on approximation of a transformer model to power iteration speficially, rather than to PCA. Lower bounds approximately validating their bounds would be useful here.\n\n- The experiments seem a bit standalone and does not connect deeply to the theory, in particular the terms that arise in the loss. For example, the dimension D is hidden in the universal constant in remark 3, however it may be good to quantify the exact dependence and moreover, validate a version of it in the experiments.\n\n- The paper seems hastily written, e.g. in L142, definition 3, The sentence defining \\tilde{D} seems incomplete, L375 contains a missing citation, see also typos at the end."
            },
            "questions": {
                "value": "Some questions were raised above.\n\n- I maybe misunderstanding something but aren't principal eigenvectors linearly related to the given vectors? If so, a linear matrix, instead of transformers, should suffice for this purpose?\n\n#### Typos:\n\n- L111: \"along the it gives us\"\n- L132: \"convinience\"\n- L233: \"propogation\"\n- L308: \"isotorpic\"\n- L721: \"setps\"\n- ReLU, relu and Relu are used interchangeably."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "1. The paper shows how to do PCA using a forward pass through the transformers without going through power iteration\n2. The error bounds are derived for Gaussian data, and they look sound\n3. As of now, the method can produce only the top eigenvector with sufficient accuracy, but no more"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The maths in the paper is well-derived and sound. The method works well for the top eigenvector for datasets such as MNIST or F-MNIST. It has the potential to become useful."
            },
            "weaknesses": {
                "value": "1. The error bound in Proposition 1 is proportional to d, the Gaussian dimension. I doubt the algorithm will work in high dimension \n\n2. The experiments on synthetic data can include datasets for high dimensions (D). For D=50, people will simply use power iteration. The method's usefulness lies in whether it can predict the eigenvector/values for a high dimension, which the authors skipped for the synthetic data.\n\n3. For the experiments on MNIST or F-MNIST, the cosine distance drops to 0.5 or below for k>1. This is concerning. If the transformer is trained to predict the top eigenvalue (say w) and eigenvector (say v) from X with sufficient accuracy, it can take another forward pass on X - w v v^T to predict the second pair. Why is the error for the eigenvector for k>1 so high? I believe the method needs some revision to be applicable"
            },
            "questions": {
                "value": "Please see Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores the potential of transformer models to perform Principal Component Analysis (PCA) through a theoretical and empirical lens. Authors demonstrate that a pre-trained transformer can approximate the power method for PCA. The paper provides a rigorous proof of the transformer\u2019s ability to estimate top eigenvectors and presents empirical evaluations on both synthetic and real-world datasets to validate these findings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The finding that transformers can effectively implement the power iteration method is intriguing and expands the known possibilities of transformers.\n- The paper is theoretically rigorous, and the experiments on both synthetic and real-world datasets effectively support the proposed theoretical framework."
            },
            "weaknesses": {
                "value": "- The practical implications of the results are somewhat unclear. Does this result necessarily imply that transformers can perform PCA effectively on in-context examples?\n- The novelty of this result is questionable; since even a linear autoencoder can approximate PCA, it is unclear why the fact that transformers with significantly more parameters can learn is surprising and valuable.\n- The supervised pre-training phase seems unrealistic in practical applications, which makes the analysis and experimental results appear less impactful.\n- The paper is challenging to follow due to unclear writing, which affects readability and the accessibility of its main ideas."
            },
            "questions": {
                "value": "- Lines 142-143: \"Consider output dimension to be \\tilde{D}, the\u2026\" \u2013 this sentence appears incomplete\n- Several notations in the main body are not clearly defined. Implementing a more systematic notation or including a glossary would significantly improve readability.\n- The notation L is used both for the number of layers and the loss function, which creates confusion.\n- Could PCA not be formulated as an optimization problem and solved with gradient descent, using existing methods (e.g., [1])?\n\n[1] Von Oswald, Johannes, et al. \"Transformers learn in-context by gradient descent.\"\u00a0*International Conference on Machine Learning*. PMLR, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper asks if a transformer can be used to calculate the top k eigenvectors of the covariance of an input data matrix. Towards this end, it uses the transformer to implement a power-method approach to this problem. The paper then analyzes the accuracy of this implementation (with respect to the true eigenvectors).\n\nThe question of what sort of data analysis transformers can do is interesting. While the paper makes some headway on this problem, it has some key limitations, discussed below:  \n\n\n1. For large enough transformers, why is it surprising that any analysis method, and in particular PCA can be implemented? Specifically, since the transformer described here can use self-attention (with relu) to calculate the covariance and then has enough layers to do the power iterations, why is it surprising that it can calculate PCA?\n\n\n2. In Theorem 3.1., epsilon0 is used before being defined. You should write this more clearly/correctly by first quantifying over epsilon0 and then using it. But it still is strange that tau is upper bounded rather than lower bounded. Eg why not use tau=0. This theorem needs to be better written. \n\n\n3. In the theory part ,d denotes the number of rows of X and D denotes the dimension after augmentation, but then in the experiments only D is mentioned as the dimension of the data. Is this a mistake? It seems like also in the experiments you would need both d and D.  Generally it is not clear what role P (augmentation matrix) plays in the experiments.\n\n\n4. The standard method for calculating multiple eigenvectors is Lancoz, which I don\u2019t think is what you are using. Have you considered using this instead?\n\n\n5. There is a long line of work on implementing PCA with online rules such as Oja. It would be good to comment about this.\n\n\n6. The paper is not very well written, with quite a few grammatical errors and typos (\u201cOne critical and most fundamental questions\u201d, \u201cHence, practioners use various of methods\u201d, \u201csuch that forward propagate along the it gives\u201d).\n\n\n7. What does \u201chelps us screen out all the covariates\u201d mean?\n\n\n8. It seems like the use of ReLU is important for carrying out the covariance computation and that it would be hard to do with a softmax. Although there is some discussion of this, it seems like a significant restriction since softmax is much more broadly used.\n\n\n9. I\u2019m not sure how to understand the empirical results. They seem to mostly show that for smaller models it is harder to calculate PCA, which is perhaps not that surprising. Is"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "See above."
            },
            "weaknesses": {
                "value": "See above."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}