{
    "id": "OlytBskWjc",
    "title": "Reinforcement learning on structure-conditioned categorical diffusion for protein inverse folding",
    "abstract": "Protein inverse folding\u2014that is, predicting an amino acid sequence that will fold into the desired 3D structure\u2014is an important problem for structure-based protein design. Machine learning based methods for inverse folding typically use recovery of the original sequence as the optimization objective. However, inverse folding is a one-to-many problem where several sequences can fold to the same structure. Moreover, for many practical applications, it is often desirable to have multiple, diverse sequences that fold into the target structure since it allows for more candidate sequences for downstream optimizations. Here, we demonstrate that although recent inverse folding methods show increased sequence recovery, their \u201cfoldable diversity\u201d\u2014i.e. their ability to generate multiple non-similar sequences that fold into the structures consistent with the target\u2014does not increase. To address this, we present RL-DIF, a categorical diffusion model for inverse folding that is pre-trained on sequence recovery and tuned via reinforcement learning on structural consistency. We find that RL-DIF achieves comparable sequence recovery and structural consistency to benchmark models but shows greater foldable diversity: experiments show RL-DIF can achieve an foldable diversity of 29% on CATH 4.2, compared to 23% from models trained on the same dataset. The PyTorch model weights and sampling code are available on GitHub.",
    "keywords": [
        "Protein Inverse Folding",
        "Diffusion",
        "Reinforcement Learning"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "We develop a novel protein inverse folding model, RL-DIF, that achieves SOTA performance on CATH, TS50, TS500, and CASP15 on foldable diversity, a new metric that measures the diversity of sampled sequences that fold into the target structure.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=OlytBskWjc",
    "pdf_link": "https://openreview.net/pdf?id=OlytBskWjc",
    "comments": [
        {
            "summary": {
                "value": "The paper focuses on inverse folding, i.e., recovering a residue sequence that would fold into a given 3D backbone structure. The key motivation for the paper is specifically to improve diversity of generated sequences. The method consists of a discrete diffusion model that is then fine-tuned using RL (policy gradient) to improve foldability (sc-TM). Evaluation includes a number of baseline methods, including ProteinMPNN, PiFold, etc."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is clearly written and the evaluations are fairly extensive, involve a number of relevant baselines, and seem conducted with care."
            },
            "weaknesses": {
                "value": "The proposed method is largely a direct combination of available techniques. Discrete diffusion component seems to be a variant of GradeIF/D3PM with small changes, and the RL fine-tuning is based on the rather straightforward policy gradient method (e.g.) from Black et al. I don't really see generalizable methodological innovations. Of course, it's fine to mix and match methods with adjustments to produce a well-performing technique but then the paper rests on its empirical results. \n\nI think it would be better to replace eq (8) with a measure calculated only within foldable sequences so as to separate foldability from diversity (so they can be assessed separately and together). Diversity was taken as the key motivation for the paper but the two notions (diversity and foldability) are now mixed together in the proposed metric. E.g., the downward slope in Figure 2 is in part just due to the fact that higher folding threshold automatically decreases the value of the current metric. Figure 2 can also give a wrong impression that RL-diff increases diversity since the metric can be improved by increasing foldability instead. In fact, the reward used for fine-tuning the diffusion model is sc-TM so tailored to increase foldability (not control diversity)."
            },
            "questions": {
                "value": "The expectation after eq (3) should really be a sum\n\nq(S_t|S_{t-1},v) should be q(S_{t-1}|S_t,v) in eq (9) \n\nIn the model architecture, line 183, do you mean p(s_0|s_t) rather than p(s_{t+1}|s_t) as currently written? \n\nTable 1 would be easier to read if summarized in terms of curves. For example, calculate diversity within all generated samples from a method that pass a sc-TM threshold and then vary this threshold."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces RL-DIF, a novel reinforcement learning-driven model for protein inverse folding, addressing the need for generating diverse ensembles of amino acid sequences that fold into specific, target 3D protein structures via inverse folding. Authors introduce a framework they call RL-DIF that applies a categorical diffusion approach pre-trained on sequence recovery, followed by reinforcement learning (RL) fine-tuning to optimize structural consistency. This two-phase training strategy enables RL-DIF to improve \"foldable diversity\"\u2014the diversity of sequences that maintain the correct fold\u2014while retaining sequence recovery and structural accuracy. In tests on CATH 4.2 and other datasets, RL-DIF achieved up to 29% foldable diversity, outperforming prior models, which peaked at 23%.\nKey contributions of the paper include:\nRL-DIF Model: A diffusion-based model refined with denoising diffusion policy optimization to balance diversity and structural fidelity.\nBenchmarking Results: Demonstrating RL-DIF\u2019s superior foldable diversity on multiple datasets.\nMethodological Advancements: Introducing foldable diversity as a new metric to assess inverse folding model quality.\nThe RL-DIF model code and PyTorch weights are available in HF."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Paper tables an important problem in inverse folding with an interesting solution in the combination of diffusion model and RL. It's an exciting contribution that could open up a new set of metrics for inverse folding models. The diversity of inverse folding models is a known problem in practice, but few direct attacks have been taken. The authors develop an interesting approach that shows improvement on sequence diversity metrics relative to base line models."
            },
            "weaknesses": {
                "value": "The paper feels close to the acceptance threshold for me. However,I felt the authors could do a bit more. The increase in seq diversity generated by RL-DIF is modest in some cases. The authors could provide (A) more information on the trade off between structural similarity and sequence diversity along the training trajectory (b) More information on how hyper parameter can be used to relax structural similarity constant to increase diversity (c) exploration of regularization strategies like entropy based regularization to increase the diversity of sequences generated about the gains shown. (d) validation of structural similarity of generated sequences via alpha fold."
            },
            "questions": {
                "value": "Could the authors could provide (A) more information (plots) on the trade off between structural similarity and sequence diversity along the RL training trajectory (b) More information on how hyper parameter can be used to relax structural similarity constant to increase diversity (c) exploration of regularization strategies like entropy based regularization to increase the diversity of sequences generated about the gains shown. (d) validation of structural similarity of generated sequences via alpha fold."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents RL-DIF, a discrete diffusion model trained for protein inverse folding and further fine-tuned on structural consistency. RL-DIF achieves comparable sequence recovery and structural consistency to existing methods, while having a better foldable diversity, beneficial to downstream optimizations."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed method combines recent advances in discrete diffusion and reinforcement learning alignment methods to improve the sequence diversity of protein inverse folding, an essential task in protein biology.\n- The paper is easy to follow, and the experiments on various protein datasets show the improvement in diversity with the proposed method."
            },
            "weaknesses": {
                "value": "- The novelty of RL-DIF is limited. Several existing papers utilize discrete diffusion models for protein inverse folding, for example, [1] and [2]. The idea of using structural consistency to fine-tune the inverse folding model has been explored in ESM3[3], although with language model as the base model.\n- The motivation for fine-tuning the model with structural consistency to improve the sequence diversity is not very clear. This is also reflected in the experiments in Table 1, where DIF-Only achieves higher diversity in most datasets except CATH-short and CATH-single. \n- The Foldable Diversity is only calculated based on 4 generated sequences for each protein structure, which can have high variance. It is better to use a higher number of generated sequences, and report the standard deviation of the results across random seeds. Besides, it is also helpful to provide the ratio of the generated sequences that satisfy the sc-TM score constraint to give some idea of to what degree the Foldable Diversity is affected by the structural consistency.\n- The model is compared to only non-diffusion based inverse folding baseline methods. A comparison with diffusion-based methods [1,2] is missing.\n- Hamming distance is used to measure sequence diversity. However, it may not be able to capture the high-level diversity between sequences, which is more related to protein functions and more useful in practice. Thus, the distance calculated in a pretrained embedding space (eg. ESM embedding) could be better.\n\n[1] Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design. ICML 2024.\n\n[2] Unlocking Guidance for Discrete State-Space Diffusion and Flow Models. arXiv 2024.\n\n[3] Simulating 500 million years of evolution with a language model. bioRxiv 2024."
            },
            "questions": {
                "value": "- How are the temperature values for the pretrained models selected? In Table 1, T=0.1 is much better than T=0.2 and T=0.3 for all baselines. How do they perform with a smaller T, eg. T=0.05 or T=0.01?\n- The reviewer understands that it is computationally expensive to train RL-DIF on the same amount of data as ESM-IF. For a rough comparison, how is the performance of ESM-IF trained on the same dataset as RL-DIF-100K?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}