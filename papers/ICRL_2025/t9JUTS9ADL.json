{
    "id": "t9JUTS9ADL",
    "title": "Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale",
    "abstract": "Large language models (LLMs) show remarkable potential to act as computer agents, enhancing human productivity and software accessibility in multi-modal tasks that require planning and reasoning. However, measuring agent performance in realistic environments remains a challenge since: (i) most benchmarks are limited to specific modalities or domains (e.g. text-only, web navigation, Q&A, coding) and (ii) full benchmark evaluations are slow (on order of magnitude of days) given the multi-step sequential nature of tasks. To address these challenges, we introduce the Windows Agent Arena: a reproducible, general environment focusing exclusively on the Windows operating system (OS) where agents can operate freely within a real Windows OS and use the same wide range of applications, tools, and web browsers available to human users when solving tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse Windows tasks across representative domains that require agent abilities in planning, screen understanding, and tool usage. Our benchmark is scalable and can be seamlessly parallelized in Azure for a full benchmark evaluation in as little as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we also introduce a new multi-modal agent, Navi. Our agent achieves a success rate of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted human. Navi also demonstrates strong performance on another popular web-based benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis of Navi's performance, and provide insights into the opportunities for future research in agent development and data generation using Windows Agent Arena.",
    "keywords": [
        "multi-modal agent"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We built a scalable open-sourced framework to test and develop AI agents that can reason, plan and act on a PC using language models",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=t9JUTS9ADL",
    "pdf_link": "https://openreview.net/pdf?id=t9JUTS9ADL",
    "comments": [
        {
            "comment": {
                "value": "We thank the reviewer for the valuable feedback. We are encouraged that the reviewer finds our benchmark to complement existing works, and provides a useful parallelization infrastructure. We address each question/concern below:\n\n**Additional benchmark improvements**\n- We agree that benchmark additions like multilingual tasks, improved metrics (e.g. cost, steps, time to completion) can improve agent evaluation. We note that our current tasks and metrics provide ample coverage across the most utilized Windows functions in terms of representative user workflows. Beyond our current analyses of human evaluation (Appendix A.5) like number of steps, difficulty ratings, and agent/task job runtimes (Appendix A.7, Table 11), we are happy to include additional details. \n- We agree; the points raised by the reviewer are improvements that we plan to make over time on the open-sourced codebase nonetheless; this includes improving benchmark tasks in quantity and quality (with a focus on multi-application tasks), adding new applications/domains, new modalities to the benchmark (e.g., audio-capable agents), and adding Chinese, Japanese, and Spanish versions of tasks to our benchmark. \n  - Given that our goal is to first make the benchmark scalable/parallelizable to enable faster evaluation/reiteration, our planned improvements can be easily added atop our framework, along with other features, as we co-develop our benchmark with the research community on Github\u2014especially as model/agentic capabilities evolve.\n \n**Comparison with other agents**\n- Our main goals with Windows Agent Arena are to provide a new benchmark for Windows OS and offer an accessible, preliminary agent (Navi) which researchers can adapt for their own needs. Navi is based on set-of-mark prompting and VLM reasoning, which is a common agent architecture. \n- There is a rich body of literature with multiple agent types, but implementing all these variants was out of scope given our main focus is on the construction of a scalable, parallelizable benchmark for the Windows OS environment. However, we still make a best effort to ablatively study our agent's capabilities as we compare 32 different Navi variants (Table 4) using multiple screen parsing models and VLMs (from small to large models) to help users understand the pros/cons of each configuration. \n  - Our intention for Navi is to illustrate the use of our benchmark and serve as an accessible but relatively capable starting point/template for the community. Together with our benchmark, our hope is that our contribution will enable faster reiteration in terms of research and development of AI agents.\n\n**[Q1] What are our contributions / How is this different from OSWorld?**\n- Please see **[N2]** in \"General Response to Common Questions from Reviewers\" at the top. \n\n**[Q2] Failure analysis**\n- Section 4.2 details common failure causes for Navi: i) visual-language misalignment (selecting the wrong element ID despite a reasonable textual explanation), and ii) incorrect screen parsing (e.g. grouping multiple elements under the same ID). \n- Figure 6 provides visual examples of these failures. Appendix B (Figure 16) provides another example. We are happy to include more examples in the appendix in the revised manuscript. We hope these examples help researchers when developing their own agents.\n\n**[Q3] Comparison with OSWorld\u2019s agent**\n- OSWorld's agent uses as input the accessibility (a11y) tree along with the raw screenshot. Navi has a similar implementation, but instead uses the Windows UIA tree and other models (e.g., Omniparser) to create SoMs on the screenshot to aid element selection. \n- Moreover, several of our agent tests/choices already share substantial overlap with OSWorld\u2019s including GPT-4V (which delivered the best on-average performance in OSWorld\u2019s experiments and received the most analysis). \n- We also include new models released since OSWorld (e.g., OpenAI\u2019s o1 for planning/reasoning or Omniparser for screen parsing) as well as other types of models not covered by OSWorld or other benchmarks (e.g., small language models like phi3). \n- Overall, we study and analyze 32 variations of Navi (using different screen parsing backbones and VLMs for reasoning/planning).\n\n**[Q4] Human performance**\n- Appendix A.5 provides details on human evaluation/performance. \n- Participants operated without search tools or the internet; failures were mostly due to not knowing the lengthy steps needed to change specific settings in media players and office apps. In several cases, participants were not as familiar with the program/application (e.g., more familiar with another media player). In others, the participants\u2019 typical workflows were more casual which did not require advanced use cases in their day-to-day tasks compared to what the task required. Our tasks intentionally reflect a range of difficulty levels. \n\nPlease let us know if there are any other questions and we would be happy to address them!"
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for the valuable feedback. We are encouraged that the reviewer finds that our benchmark is complementary to existing works in the field, and adds a more efficient evaluation. Below, we address each concern in turn:\n\n**Navi\u2019s novelty and Navi\u2019s role in our paper**\n- To clarify, we do not claim in this work that Navi is an entirely new type of agent. Yes, as noted by the reviewer, our goal/aim for Navi (and its variants) was to have it serve as a reasonable, capable, but preliminary starting point and baseline for users, researchers, and the open-source community at large to develop their own agents using our new benchmark. \n- Together with our novel contribution in making this benchmark scalable and parallelizable, we aim to provide an accessible way for the community to train/test agents and reiterate efficiently. \n- Navi follows similar approaches to other web / OS agents which use screen parsing + VLM reasoning. In Table 4 and in the results section of our paper, we provide a thorough analysis of 32 different combinations of Navi agents with distinct backbone models, which can help guide researchers in this area to develop and customize their own solutions. \n\n- We have also open-sourced these backbones to allow researchers to create their own agents which might contain better solutions and move the benchmark forward. In addition, we benchmark Navi on a more established, \u201ctraditional\u201d agent benchmark (Mind2web) to verify its performance. \n- Currently, we do not plan on testing Navi on other benchmarks for this paper since our main goal for this paper is not to make Navi competitive. Rather, our paper\u2019s main contribution and novelty are developing a Windows OS benchmark that is scalable through its parallelization infrastructure (for fast evaluation) and realistic for training/testing agents in a Windows environment. \n- To demonstrate the promise of agent development and help showcase the benchmark, Navi serves as both a beginning baseline and openly available template for users to play with and develop in this respect.\n\nWe hope our responses have addressed your questions. Please let us know if there are any other questions or suggestions and we would be happy to address them."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for the valuable feedback. We are encouraged that the reviewer finds that our benchmark fills a gap in current agent evaluations. Below we attempt to address each of your concerns within the space limit.\n\n**Accessibility and proprietary OS**\n- We are aware of this limitation which is why we have made significant effort so that anyone can access/use Windows OS in our benchmark; despite not being an open-source OS, our code repository enables users on to access a free evaluation copy which can be used for benchmark deployment with easy continued renewal after. \n- To the best of our knowledge, our benchmark is one of the few that provides users open/free access to the Windows OS and computing environment for research/development. Please see **[N1]** in \"General Response to Common Questions from Reviewers\" Official Comment at the top.\n\n**Resource-intensiveness**\n- See **(2)** of **[N2]** under \"General Response to Common Questions from Reviewers\" Official Comment. \n- In short, our benchmark does not need any high-performance computing setups and large storage capacities. \n- After each run, the internal state of the system/VM is reset to ensure that progress/completion on one task does not interfere with others. This preserves the integrity of the OS environment so tasks can be continuously run and parallelized without interruption or issue. This also removes all created files etc. resulting from agent action, further mitigating the need for a large storage capacity. \n\n**Configuration/maintenance**\n- Our benchmark and the underlying infrastructure make it easy to install additional software either on the VM directly or via the Docker image. This way, users can design new tasks that require new programs and add them to the benchmark as necessary. \n- Together with the ease in which a user can design their own tasks through a task configuration file/JSON (e.g., see README and Appendix A.6), our benchmark is easily configurable, flexible, and extensible to new tasks, programs, etc., with minimal risk of breaking the benchmark's functionality or integrity. \n  - In defining their own custom tasks, users can leverage our existing library of configuration or evaluation functions or write their own by modeling theirs after ours. We are happy to include even more detailed instructions on task creation, hosting, and configuration beyond what we already have. \n- Unfortunately, many agentic benchmarks with computer/OS environments (e.g., OSWorld, AndroidWorld, etc.) share similar weaknesses when it comes to maintenance: as OS systems undergo refreshes, updates, etc., it can be difficult to ensure continued compatibility with the benchmark. \n  - To mitigate this, we have frozen automatic update and instead conduct manual review/updates (e.g., monthly) to re-adjust and mitigate things breaking. Since we are adding new features, modalities, and tasks, we will be continuously maintaining the benchmark in terms of both our needs and those of the wider community (e.g., Github).\n\n**Ease of benchmark infrastructure**\n- A major contribution of our benchmark is precisely making it easy for researchers to scale up agent experiments. As such, we have made significant effort to automatically build/install all necessary programs, scripts, images, etc. to simplify evals. We also include resources to help add/configure new tasks. \n- Cost-wise, a full parallel run costs in the range of 10 USD in total with CPU machines in Azure. Model calls cost about $100-200 depending on tokens utilized and model choice. We will include these details in our revised manuscript alongside data we already report (Table 10 in Appendix A.7).\n\n**Windows-specificity / Why only Windows?**\n- Please see **[N1]** in the comments we made for all reviewers.\n\n**What are our contributions / How is this different from OSWorld?**\n- Please see **[N2]**.\n\n**Tasks**\n- Please see **[N1]**.\n\n**Navi\u2019s novelty**\n- To clarify, we do not claim that Navi is an entirely new type of agent. \n- We aim for Navi to serve as a reasonable starting point and baseline for researchers developing their own agents. \n  - As pointed out, it follows similar approaches as other web / OS agents which use screen parsing + VLM reasoning. \n- What we provide in Table 4 and in the results section is a thorough analysis of 32 different combinations of Navi agents with distinct backbone models to help guide researchers in this area. \n  - Out of those models we also open-source a few of these backbones to allow researchers to create their own agents which might contain better solutions and move the benchmark forward. We also benchmark Navi on a more well-established \u201ctraditional\u201d agent benchmark (Mind2web) to verify its performance.\n\n**Reward structure**\n- We believe that the completion-based reward function is a reasonable assumption for the benchmark. However, we do plan to add number of steps, time, and cost as auxiliary metrics to the benchmark in our open-sourced codebase."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for the valuable feedback and are encouraged that the reviewer finds our benchmark to be well-designed and diverse. We address each concern below in turn:\n\n**Why only Windows?**\n- Please refer to note **[N1]** in the \"General Response to Common Questions from Reviewers\" Official Comment at the top.\n\n**What are our contributions / How is this different from OSWorld?**\n- Please refer to note **[N2]** in the \"General Response to Common Questions from Reviewers\".\n\n**Extension to other OS**\n- Generally speaking, there are two aspects/issues to consider if a researcher wants to extend our benchmark to another OS:\n  - Task configurations/evaluation scripts, and benchmark parallelization infrastructure. While OSWorld already, and primarily, covers tasks with Linux OS in mind, it is not yet equipped with fast and scalable parallel evaluation in the cloud. Our approach/environment not only provides benefits when it comes to scalability and faster/parallelizable evaluation but also a more secure environment as well. \n  - Furthermore, OSWorld, to the best of our knowledge, does not natively support a full version of Windows OS to the extent that we do. A developer would be able to borrow infrastructure from Windows Agent Arena, but that project is outside of the scope of our paper. Porting the benchmark to MacOS would require both very heavy engineering work for porting tasks and infrastructure due to the structural differences between OS architectures.\n\n**Uniqueness of tasks and capability assessment**\n- We refer to **[N1]** for more detail. In short, because Windows OS has distinct UI, system-specific functions and command line, system-specific apps, idiosyncrasies, etc. than Linux OS, we expect agentic behavior, performance, and perception to vary considerably---as a result, the same task can have vastly different completion trajectories depending on OS. \n  - As just one example, UIA trees that are used as input (in addition to screenshots and SoMs) to help multi-modal agents understand/access UI elements on the screen exist on Windows; however, different structures exist on Linux, like AP-STI or ATK among others, which naturally will impact agent performance in how well it traverses these structures to perceive different UI/visual elements, etc. These will ultimately impact how the agent plans, reasons, and acts across different OS. We will clarify these aspects in the revised manuscript.\n- Only 10% of OSWorld's tasks are for Windows (43 out of 412) whereas the entirety of our tasks (150+) focus on Windows. Moreover, we include a considerable portion of newly designed tasks in addition to tasks we converted or transformed to suit Windows function and usage. We will add more discussion re: these differences in our manuscript.\n- Rather than focusing on Navi's evaluation across different benchmarks, our main goal is to provide a new benchmark for Windows OS, and offer an accessible, well-performing but preliminary agent (Navi) which researchers can adapt for their own needs. Navi is meant to be a capable starting point and template for the research/open-source community to develop and customize their own agents; together with our benchmark, our hope is that our contribution will enable faster reiteration in the research and development of AI agents.\n\n\n\n**Benchmark cost**\n- Our open-sourced repository README file provides a cost estimate for running the benchmark. This estimate includes both the VM cost (about 8 US dollars for all machines running in parallel for the duration of the evaluation), plus the models' costs (which can vary from 0 US dollars if the user runs a local VLM model, all the way to about 15 US dollars for GPT4o-mini or about 100 US dollars for GPT4o). \n- Appendix A.7 in our paper also includes a table of VM specs/costs and times. For times, we note that typical serial evaluation of a benchmark on the scale of Windows Agent Arena (or larger) would take significantly longer (likely multiple times longer) than our parallel setup.\n- We note that these costs are similar to costs associated with evaluating state-of-the-art closed source models on other agentic benchmarks as well. We also note that as part of benchmark\u2019s infrastructure, we do not need expensive high-performance compute as our parallelization can be done on inexpensive CPU cores (see **[N2]** in the \"General Response to Common Questions from Reviewers\") so any additional costs from running our benchmark beyond closed-model APIs are near-negligible. \n\n**Mistake in line 263**\n- Thank you for highlighting this mistake. We meant to point the reader to appendix A.6 (TASK DEFINITION & CONFIGURATION). We will change this in our revised manuscript.\n\nWe hope our responses have addressed the questions that reviewers have. Please let us know if there are any other questions or suggestions."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for the valuable feedback. We are encouraged that the reviewer finds our work easy to deploy. We address each of your questions/concerns below:\n\n**What are our contributions / How is this different from OSWorld?**\n - Please refer to **[N2]** in the \"General Response to Common Questions from Reviewers\" Official Comment at the top.\n\n**Uniqueness of Windows tasks**\n - Even for web browsing tasks (which are not inherently OS-related), a significant amount of time in terms of engineering and testing was required to modifying the configuration and evaluation functions so that they would work natively on the Windows OS given differences across OS kernels/support. \n    - As just one example for web browsing tasks using Chrome and Edge, functions that access browser cookies, and settings in cached files are usually significantly different between Linux and Windows, and each task had to be modified and verified manually. We also added Edge browser support for several Web tasks, which did not exist in OS World. \n- Additionally approximately double the number of hours was spent creating new tasks for Windows-exclusive apps (Microsoft Paint, File Explorer, Settings, Notepad, Clock). These newly designed tasks are not only meant to work under Windows OS but also aligned with the actions and parts of workflows of common users of Windows---we will better clarify and describe the differences between adapted tasks, converted tasks, and newly designed tasks in Section 3 of our revised paper.\n\n**Number of tasks**\n- With 154 task templates, our benchmark falls in line with major benchmarks in the domain of execution-based evaluation such as MiniWoB++ (114 templates), AndroidWorld (116), Web Arena (241), and OSWorld (369). We are also actively monitoring the open-sourced repository and adding new tasks that we create/source ourselves as well as those made by the research community to expand the benchmark, including users who have already designed their own tasks within our benchmark.\n\n**Agent design back-end**\n- In Table 4 and Section 4, we describe the 32 different agent back-ends we tested along with different combinations of screen parsing and VLM reasoning models. These models range in size and other aspects from small local and cloud models (e.g. Phi3.5 and GPT4o-mini) to large cloud models (GPT4o and o1). We will better clarify and describe the design of our agent Navi in Section 4 of our revised manuscript along with the individual components and their specific considerations.\n\nPlease let us know if there are any other questions or suggestions and we'd be happy to answer!"
            }
        },
        {
            "title": {
                "value": "General Response to Common Questions from Reviewers"
            },
            "comment": {
                "value": "We thank the reviewers for their time and greatly appreciate their insightful comments. We are particularly encouraged that all reviewers recognize the impact that a Windows-focused agent benchmark can have. We provide detailed responses to each reviewer\u2019s questions below their individual reviews but also summarize our answers to the most common questions below:\n\n======\n- **[N1] Why only Windows?**\n  - OSWorld is an agent benchmark majoritarily focused on agent tasks for non-Windows OS (~90% are Ubuntu/Linux tasks); Linux has ~4% market share among desktop operational systems. We focus this work exclusively on the Windows OS because, with 73% market share, it is the most widespread OS among PC users.\n  - Despite its prevalence, Windows is still relatively under-represented in environments/benchmarks for agentic capabilities. Therefore, OS agents need to include Windows, and make use of its distinct UI, system-specific functions and command line, system-specific apps, and idiosyncrasies.\n  - Given its uniqueness, proprietary nature, and under-representation as an OS, it is less clear how agents fare with even common actions/tasks in Windows. Differences between OS software ecosystems can significantly impact an agent\u2019s observation/action spaces and performance, as noted by OSWorld itself (their agent performs ~2x better on Linux than Windows).\n    - In our experiments, we see that common actions performed by humans in Windows can be challenging for state-of-the-art models that we use for our agents, suggesting that these tasks, the OS/environment, and the workflow aspects they embody still present specific challenges for agents.\n\n======\n- **[N2] What are our contributions / How is this different from OSWorld?**\n  - In sum, the key differences and contributions are: **(1) OS focus (Linux vs Windows)** and **(2) parallelization of evaluation (time reduction from days to ~20-30 min, ~100x faster)**. \n    - **(1) OS focus**: We mention in our paper that ~2/3 of our tasks are adapted/inspired from OSWorld, and 1/3 are new. There is a non-trivial amount of engineering effort in porting tasks: for each individual task, we rewrite from scratch all configuration functions (which set the initial conditions for the task) and evaluation functions (which verify completeness) so they are compatible not only within a Windows OS but also within our custom parallelizable infrastructure. \n      - Section 3.2 understates this effort, which consumed ~800 person-hours for coding, testing, verification, etc. The remaining 1/3 new tasks consumed ~1600 person-hours. This excludes agent testing, etc.\n      - **Only ~10% of OSWorld's tasks are specifically for Windows (43/412), We have >3x as many Windows tasks.**\n      - Also, despite not being an open-source OS, we made significant efforts to have Windows accessible via our benchmark as a self-contained docker image. We provide users a way to use a free evaluation copy which can be used for benchmark deployment and then easily/continuously renewed thereafter. \n      - As a result, our benchmark is one of the few (if not the only) that provides users open/free access to the Windows OS and computing environment for agent research and development. Our benchmark also allows users to install their own programs/applications, add and configure new tasks for their own needs, and more.\n\n    - **(2) Quick/flexible evaluation via benchmark parallelization:** Agent evaluations based on task completion is inherently slow because of the sequential nature of tasks---especially as a benchmark grows in size and complexity. \n      - To our knowledge, we are the first to open-source infrastructure which allows researchers to fully parallelize evaluation on our benchmark in under 20-30 min for an arbitrary number of tasks, as opposed to tens of hours or even days or weeks of wait. This enables not only faster evaluation but also faster reiteration in research and development. \n      - With our benchmark, there is also *no* need for any high-performance computing setups or large storage capacities. Our setup relies on widely available and relatively low-spec inexpensive CPU cores and does not rely on any GPU resources (Table 10 in Appendix A.7 for details). Our setup also allows users to customize/configure the RAM allocated to the VM---allocated memory can be reduced down to as *small* as **2 GB** and still be able to run the environment. \n      - Users can install their own programs onto the image or VM and create their own tasks following details in our paper, README, etc.\n      - We also note that our incurred costs are primarily from closed-source model API calls---which are similar to costs incurred from testing closed-source models on other agentic benchmarks as well. \n\nWe hope our responses have addressed the common questions that reviewers have. Please let us know if there are any other questions. We thank the reviewers for their time!"
            }
        },
        {
            "summary": {
                "value": "The paper introduces WindowsAgentArena, a comprehensive benchmarking environment tailored for evaluating multi-modal agents in Windows OS settings. This platform builds upon the OSWorld framework, specifically adapted to provide 154 diverse, complex tasks representative of typical Windows applications and workflows, such as document editing, web browsing, coding, and system customization. The benchmark environment is designed for scalability, allowing for parallelized evaluation on Azure, significantly reducing the time required for agent testing. As a demonstration, the authors develop Navi, a multi-modal agent achieving a 19.5% success rate on WindowsAgentArena tasks, highlighting the challenges in reaching human-level performance (74.5%)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. A new benchmark focuses on multi-modal tasks in the Windows environment.\n\n2. Easy to deploy environment.\n\n3. Authors design an extra multi-modal agent to validate benchmark\u2019s effectiveness"
            },
            "weaknesses": {
                "value": "1. This benchmark does not show clear differences toward OSWorld\n\n2. Agent design can be clearer\n\n3. The number of tasks is limited"
            },
            "questions": {
                "value": "1. For all the designed tasks (Office, Web Browsing, etc), which is specific to the Windows environment? To distinguish WindowsAgentArena from OSWorld, authors need to discuss the unique tasks for Windows.\n\n2. As stated within the paper, two thirds of the tasks are re-implemented from OS World and one third tasks are newly designed. It would be great if authors can show the newly designed tasks to help the audience clearly understand the contribution.\n\n3. What is the backend model for the Agent? Have you tested on different backend models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces WindowsAgentArena, a benchmark designed to evaluate LLM agents in performing multimodal tasks within the Windows OS environment, including 154 tasks covering office software, web browsing, coding, etc. The benchmark assesses the ability of LLM agents to interact with a complex operating system through various modalities. The paper also provides a baseline agent Navi and test its performance on the benchmark. Codes and deployment guidance are open-source."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "\uff081\uff09The paper proposes a new benchmark specifically targeting LLM agents\u2019 performance in complex, real-world tasks on Windows OS. The benchmark is well-designed, with a diverse set of tasks that test multiple aspects of agent capabilities. \n\uff082\uff09The paper provides adequate evaluation on the proposed benchmark with a proposed baseline agent and validates its competitive ability."
            },
            "weaknesses": {
                "value": "\uff081\uff09Although the benchmark is claimed to be extensive, it is specifically focused on Windows OS. I\u2019m not sure whether this benchmark could be adapted or extended to other operating systems or environments, as this could broaden its applicability.\n\uff082\uff09Apart from the difference in operating environments, it is unclear how the tasks in this benchmark uniquely assess agent capabilities compared to OSWorld. It seems that the tasks have much overlapping with those in OSWorld. More discussion about key differences is needed to distinguish this work from existing works.\n\uff083\uff09I\u2019m concerned about the usage cost of the benchmark. An API cost estimation for running the tasks in the benchmark is needed."
            },
            "questions": {
                "value": "\uff081\uff09What\u2019s the uniqueness of the tasks in the benchmark compared to OSWorld?\n\uff082\uff09Line 263 mentioned that the task procedure is shown in Appendix A.3, but the content in A.3 is about reward evaluation.\n\uff083\uff09Is it possible to extend the benchmark to other operating systems\uff1fIf the OS difference matters to the agent evaluation, maybe this is necessary"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents WindowsAgentArena, a benchmarking suite designed to evaluate multi-modal agents' abilities within a Windows OS environment. Building on previous work like OSWorld, WindowsAgentArena focuses exclusively on the Windows operating system, making it possible for agents to perform diverse tasks representative of real-world computer usage, such as document editing, web browsing, and multimedia manipulation. The study introduces a novel agent, Navi, which demonstrates the framework\u2019s capabilities, achieving notable results in task performance compared to humans. The benchmark suite also emphasizes scalable and fast evaluation through parallelization on Azure, which reduces evaluation time significantly."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. WindowsAgentArena fills a gap in agent benchmarking by focusing on the Windows OS, which is widely used but less explored in agentic evaluations.\n2. The ability to parallelize the evaluation on Azure for faster task completion enhances the framework's practicality and adaptability for large-scale studies.\n3. By including tasks across multiple categories (e.g., document editing, web browsing, system management), WindowsAgentArena effectively simulates a real-world computer environment.\n4. Detailed performance metrics, examples of successful and failed cases, and analyses of task complexity offer clear insights into the capabilities and limitations of agents."
            },
            "weaknesses": {
                "value": "1. Windows OS is proprietary, which may limit access for researchers who rely on open-source environments or who cannot accommodate the licensing and resource costs associated with a Windows-based setup.\n2. Running Windows virtual machines (VMs) with multiple agents and parallelized tasks can be resource-intensive. This may necessitate high-performance computing setups and large storage capacities, particularly for cloud-based evaluations. Configuring and maintaining the Windows environment, including installing software, handling updates, and ensuring compatibility with the benchmark suite, can also be complex and time-consuming.\n3. Agents trained and evaluated solely in a Windows environment may develop strategies tailored to Windows-specific UI elements and workflows, potentially limiting their ability to generalize to other OS environments, computing platforms, or even to updated versions of Windows.\n4. WindowsAgentArena builds upon OSWorld\u2019s approach but is specific to the Windows environment. This focus on a different OS may not represent a fundamentally novel leap, as it largely adapts pre-existing methodologies rather than introducing groundbreaking new concepts.\n5. Many tasks in WindowsAgentArena replicate basic or common actions found in Windows OS, such as file management or web browsing. The tasks may lack complexity or novelty that could reveal new challenges for agentic behavior, making the approach feel less groundbreaking.\n6. The Navi framework leverages established multi-modal approaches, such as UIA tree parsing, DOM tree extraction, and OCR for screen parsing. While effective, these methods are widely used in similar environments, potentially limiting the originality of the agent interaction techniques.\n7. The reward structure is based primarily on task completion, similar to other agent benchmarks. WindowsAgentArena does not seem to innovate with intermediate or adaptive reward mechanisms that might encourage more nuanced agent learning behaviors.\n8. The framework relies on traditional Set-of-Marks (SoM) and bounding boxes for visual grounding, which are widely used in agent evaluation. Introducing innovative ways to handle visual-language misalignment might have added novelty to the approach."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduced a OS Agent benchmark on Windows OS, and proposed a new MLLM agent called Navi. The goal of this benchmark is to solve the challenges of  restricted modalities and slow process in agent evaluations. The benchmark defined 150+ create 150+ diverse Windows tasks across different domains. Navi is designed with chain-of-thought prompting and tested on this benchmark with different SoMs and MIND2WEB Dataset."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The writing of this paper is well. The problem definition is very detailed and easy for the reader to understand. The benchmark is a good complementary to the existing work and the claimed evaluation system is more efficient to the current benchmark system."
            },
            "weaknesses": {
                "value": "The experimental results lack the comparison of well-known agents, but just use different settings to test on multiple base models. The design of Navi agent is relatively simple and motivation is not clear. The additional experiments on MIND2WEB is not enough to support the agent performance."
            },
            "questions": {
                "value": "Is Navi just designed as a baseline to this benchmark or an innovative agent method? \nIs there a plan to test on more benchmarks for evaluate the performance of Navi in the additional results?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces WINDOWSAGENTARENA, a benchmark designed to evaluate multi-modal agents interacting with a real Windows OS environment. The primary goal is to address the limitations of existing agent evaluation frameworks (which are often constrained to specific domains or modalities) by providing a scalable and reproducible environment that allows agents to perform a wide range of tasks within the Windows ecosystem. The benchmark includes 150+ tasks across various domains such as document editing, browsing, coding, and system management. \n\nAdditionally, the authors propose a new multi-modal agent, Navi, and report its performance on WINDOWSAGENTARENA, achieving a 19.5% success rate. The paper also compares Navi\u2019s performance to human benchmarks and other web-based agent benchmarks like Mind2Web."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The WINDOWSAGENTARENA augments the OSWorld benchmark and extends the evaluation environment to Windows, which is a good complement to the existing benchmarks.\n- The authors accelerate the evaluation by enabling parallelized task execution in Azure. This is an important contribution for researchers working with time-consuming multi-step tasks, as it reduces evaluation times from days to minutes.\n- The benchmark includes a relatively diverse set of tasks (154 in total), covering different types of user workflows."
            },
            "weaknesses": {
                "value": "- It looks to me that this benchmark is merely an extension of OSWorld to the Windows environment, with similar tasks, evaluation procedures, and metrics, which limits the contribution of this work. In fact, the benchmark could be further enhanced in the following ways: \n    - **Multi-lingual tasks.** The WINDOWSAGENTARENA only covers the tasks and applications in English, but web tasks in different languages are also crucial for comprehensively evaluating the performance of the web agents.\n    - **Diverse evaluation metrics.** Currently, the authors only use the success rate as the evaluation metric in OSWorld. Other metrics like the number of steps and abilities like self-reflection could be added.\n    - **Cross-application tasks.** All 154 tasks are single-application tasks, yet real-world tasks often include multiple applications.\n- The paper compares Navi's performance primarily with human benchmarks. More agents like MM-Navigator or WebAgent are expected to be included.\n- The task curation process is not fully transparent. While the authors mention adapting tasks from OSWorld, it is unclear how these tasks were modified for Windows or how the difficulty levels were assigned."
            },
            "questions": {
                "value": "- [Q1] Please clarify the contribution of this work compared to OSWorld---as the task definition, evaluation procedures, and metrics all follow OSWorld.\n- [Q2] Could you provide more detailed qualitative analyses of why Navi fails in certain tasks? For instance, are the failures due to perception errors, action space limitations, or planning issues? This would offer valuable insights for both researchers using the benchmark and those developing future agents.\n- [Q3] Why didn\u2019t the paper include a direct comparison with agents from OSWorld (which focuses on Linux)? Even though these benchmarks focus on different OS environments, comparing Navi\u2019s performance on similar tasks (e.g., browsing, coding) could provide more context for the challenges posed by WINDOWSAGENTARENA.\n- [Q4] For human performance, an accuracy of 74.5% looks a bit low to me. Could you also provide more detailed qualitative analyses of the failure reasons for human participants? Would it be possible that the given task is infeasible given the current context?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}