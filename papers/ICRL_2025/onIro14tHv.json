{
    "id": "onIro14tHv",
    "title": "Equivariant Denoisers Cannot Copy Graphs: Align Your Graph Diffusion Models",
    "abstract": "Graph diffusion models, while dominant in graph generative modeling, remain relatively underexplored for graph-to-graph translation tasks like chemical reaction prediction. We show that standard permutation equivariant denoisers cause severe limitations such tasks, a problem that we pinpoint to their inability for breaking symmetries present in the noisy inputs. We then propose to \\emph{align} the input and target graphs in order to break the input symmetries, while retaining permutation equivariance in the non-matching portions of the graph. We choose retrosynthesis as an application domain, and  show how alignment takes the performance of a discrete diffusion model from a mere 5\\% to a SOTA-matching 54.7\\% top-1 accuracy.",
    "keywords": [
        "Graph Diffusion",
        "Discrete Diffusion",
        "Equivariance",
        "Symmetries"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "We propose aligned permutation equivariance in order to break the symmetries of equivariant denoisers in graph diffusion.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=onIro14tHv",
    "pdf_link": "https://openreview.net/pdf?id=onIro14tHv",
    "comments": [
        {
            "summary": {
                "value": "The central objective of this paper is to design a discrete denoising diffusion model for graph-to-graph translation, where the task is to train a model to predict a target graph given a source graph and, perhaps surprisingly, a matrix of node mappings between them. The authors use [1] as their design template, though the state transition matrix of the forward process relies on the absorbing state formulation from [2]. In this model, the backward process relies on the denoiser network realized by an equivariant transformer architecture, which, on its own, can take only the target graph as input. The authors' main challenge is augmenting the denoiser by also taking the source graph as input. The authors demonstrate theoretically and empirically that a simple augmentation of the denoiser's input by the source graph is not sufficient even for simply copying graphs, claiming that the equivariance of the denoiser is the main reason for this issue. Therefore, the authors propose to partially relax the equivariance by further extending the denoiser's input with the node mapping matrix, proposing the aligned permutation equivariance. The key idea is to design the underlying transformer architecture such that if the source and target graphs are permuted (by two separate permutation matrices) and the node mapping matrix is permuted accordingly, then the output of the denoiser is equivariant only under the permutations of the target graph. In other words, the output remains sensitive only to the permutation of the target graph. The authors compare the performance of their model with many baseline methods on the task of chemical retrosynthesis, achieving state-of-the-art top-1 accuracy and mean reciprocal rank on the USPTO-50k dataset. Finally, they also show the suitability of their model in the context of the guided and conditional generation of molecular graphs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper presents a comprehensive set of experiments that demonstrate the benefits and the practical utility of the proposed model in the context of guided and conditional generation of molecular graphs. The model achieves state-of-the-art results in chemical retrosynthesis.\n\nThe paper concerns an interesting problem, and its central message is important to the community.\n\nNice and clear illustrations accompany the ideas introduced in the paper."
            },
            "weaknesses": {
                "value": "The writing deserves improvement. Certain parts of the paper need to be communicated more clearly. The notation sometimes needs to be clarified. These concerns are detailed in the following comments.\n\nMajor:\n\nThe content from the start of Section 3 up to Section 3.2 qualifies more like a background section. The material is heavily inspired by [1]. Please note that [1] also uses conditioning, though it is not presented in the equations in such an explicit way. It takes more work to recognize the details behind the authors' contributions in the current form. Can the authors clearly delineate which parts are background and which are novel contributions?\n\nIn line 124 the authors define $X=P^{Y\\rightarrow X}Y$ as $(X^N,X^E)=(P^{Y\\rightarrow X}Y^N,P^{Y\\rightarrow X}Y^E(P^{Y\\rightarrow X})^T)$, and in line 151 define $D_\\theta(X,Y)=(D_\\theta(X,Y)^N,D_\\theta(X,Y)^E)$, saying that $D_\\theta(X,Y)^N$ and $D_\\theta(X,Y)^E$ output a probability vector for each node and edge, respectively. Now, in line 107, the authors say that $X^N$ and $X^E$ are one-hot encoded (again, it is not suitable to use $\\mathbb{R}$ in lines 104 and 105 to define this), then how is it possible that $D_\\theta(X,Y)=P^{Y\\rightarrow X}Y$? There is no mention that the output of $D_\\theta(X,Y)$ is implicitly one-hot encoded. This confusion repeats throughout the text, and clear definitions should stabilize it. The authors should\n\n- clarify when and where $X^N$ and $X^E$ are one-hot encoded or real-valued matrices/tensors,\n- explicitly define the output space of $D_\\theta(X,Y)$,\n- explain how the equality $D_\\theta(X,Y)=P^{Y\\rightarrow X}Y$ is possible given the different encodings,\n- use consistent notation throughout the paper for the spaces these variables belong to.\n\nMinor:\n\nThe introduction should better motivate why graph-to-graph translation is important in practical applications.\n\nLine 50: *``a solution aligned''* -> *``a solution: aligned''*\n\nFigure 1: *``w.r.t to''* -> *``w.r.t.''*\n\nIn Section 3.3, the authors use phrases such as *``should be possible''*, *``should have learned''*, *``should be the same''*. Why do not use a more certain language? Can the authors please clarify what is or is not true instead of something that should be?\n\nThe definition of graph-to-graph translation task deserves improvements. Specifically, if the authors define that the node feature matrix and edge feature tensor belong to the set of reals, why do they redefine them as one-hot vectors in line 107? If they are one-hot, then they are only a subset of reals.\n\nThere should be $Y$ on the left side of (2). Consequently, how does this density differ from (3)?\n\nLine 153: *``such that we have a probability vector for each node and edge.''* Probability is not the whole set of reals. It only ranges from 0 to 1.\n\nLine 217: The use of *`respectively'* does not fit the sentence's construction.\n\nLine 243: $\\mathbf{X}=\\mathbf{P}^{\\mathbf{Y}\\rightarrow \\mathbf{X}}Y$ -> $\\mathbf{X}=\\mathbf{P}^{\\mathbf{Y}\\rightarrow \\mathbf{X}}\\mathbf{Y}$\n\nLine 323: What is DPS?\n\nLine 374: What is MRR? It is first defined in line 402.\n\nThe explanation of retrosynthesis in lines 337 and 358 is nearly identical.\n\nLine 337: *``3 parts\"* -> *``three parts''*\n\nTable A3 should be in the main text (e.g., Figure 8 can be moved to the appendix).\n\nFigure 3 demonstrates the case for $X_T$, yet there is $D_\\theta(X_t,Y)$."
            },
            "questions": {
                "value": "Do the authors need the node mapping matrix for this task?\n\nCan the authors explain why the left part of (6) should hold (in light of the above concerns with the output of the denoising network $D_\\theta$ and the one-hot encoding of $X$)?\n\nWhy do the authors present Theorem 1 for fixed t=T? Is the result for $X_T$ not rather obvious (by definition)?\n\nIf the authors use $P^{Y\\rightarrow X}$ and $Y$ as input into the denoiser, then these two terms already form the full information about $X$. The authors seem to train the denoiser that always has direct access to $X\\_0$ through $P^{Y\\rightarrow X}Y$. The consequence of this is that $\\tilde{p}\\_\\theta$ in (3) contains $P^{Y\\rightarrow X}$ in the condition, i.e., $\\tilde{p}\\_\\theta(X\\_0|X\\_t,Y,P^{Y\\rightarrow X})=\\tilde{p}_\\theta(X\\_0|X\\_t,X\\_0)$. This design choice is unsound. Please, can the authors clarify?\n\nCan the authors please provide a more comprehensive explanation of the following comment (starting in line 227): *``This makes it impossible for the model ... some random permutation of $Y$.''*?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a novel method to perform graph-to-graph translation using graph diffusion models. The authors show why equivariant models are inherently limited in their ability to perform this task when nodes from the prior distribution need to be aligned with nodes from the output. To that purpose, they demonstrate that an ideal equivariant denoiser cannot correctly copy a graph. Then, they propose several ways to break equivariance for the nodes that should be aligned between the input and the output, and empirically validate their method on the retrosynthesis task."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- To the best of my knowledge, the topic of breaking symmetries to perform graph-to-graph translation has not yet been addressed in the graph diffusion literature.\n- The problem of using equivariant models is clearly identified and backed by theoritical evidences.\n- The paper proposed several simple ways to solve this problem."
            },
            "weaknesses": {
                "value": "- Why using the aborbing state distribution and not the marginal distributions introduced in DiGress (Vignac et al., 2022) ?\n\n- I would avoid statements like \"it is easy to see\" l304 or \"Clearly\" l242 and provide instead clear motivations and explanations\n\n- Some claims or design choices lack motivation or justification :\n- For example, the claims in section 3.2 \"a model that is capable of copying graphs from one side of the reaction\nto the other should also be extendable to modulations of this task\". Do you mean that it's a prerequisite or that is implies that the model has the ability to peform \"the modoluations\". Overall, I fell that this subsection conveys very little information.\n- The proposed aligning methods are insufficiently motivated."
            },
            "questions": {
                "value": "- What is the difference between $D$ and $\\tilde p_{\\theta}$ ?\n- How is $\\psi$ generated in section 3.5 ?\n- How is Y passed to the denoiser ? I feel it's not clearly outlined anywhere except in the section \"Aligning Y in the input\"\n- I don't get how you build the dataset for retrosynthesis. If it is standard benchmark, could you please provide more explanations or point out to a reference"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an aligned permutation equivariant denoiser for a discrete diffusion model to address the graph translation problem. To align the initial state and target, several techniques, including nodal positional encoding, skip connections, and input alignment, are introduced. The model was validated on retrosynthesis, guided generation, and inpainting tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Aligning initial and target to generate efficiently makes sense and the results are promising compared to unaligned version."
            },
            "weaknesses": {
                "value": "Since alignment itself is an empirical approach that has been used previously, the contribution may be limited to explaining why alignment is necessary specifically in a discrete diffusion model with an absorbing state as the prior. When compared to RetroBridge in the retrosynthesis task, it is difficult to find a significant performance gap between the proposed model and RetroBridge. In the cases of guided diffusion and inpainting, only a few figures are provided without numerical comparisons, making precise validation difficult."
            },
            "questions": {
                "value": "1. The paper assumes that the prior distribution is absorbing state. Does the proposed model still effective even on the uniform distribution prior?\n\n2. What happens if the node-mapping matrix has uncertainty and is not a binary matrix?\n\n3. How does the performance compare when using only direct skip connections without positional encoding?\n\n4. How does the performance differ from graph translation methods in Related Works, such as G2gt?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}