{
    "id": "AdiNf568ne",
    "title": "Erasing Conceptual Knowledge from Language Models",
    "abstract": "Concept erasure in language models has traditionally lacked a comprehensive evaluation framework, leading to incomplete assessments of effectiveness of erasure methods. We propose an evaluation paradigm centered on three critical criteria: innocence (complete knowledge removal), seamlessness (maintaining conditional fluent generation), and specificity (preserving unrelated task performance). Our evaluation metrics naturally motivate the development of Erasure of Language Memory (ELM), a new method designed to address all three dimensions. ELM employs targeted low-rank updates to alter output distributions for erased concepts while preserving overall model capabilities including fluency when prompted for an erased concept. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across our proposed metrics, including near-random scores on erased topic assessments, generation fluency, maintained accuracy on unrelated benchmarks, and robustness under adversarial attacks.",
    "keywords": [
        "Safety",
        "Knowledge",
        "Concept Erasing",
        "Model Editing",
        "Safety",
        "LLM"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We introduce a novel framework for evaluating concept erasure in language models and propose ELM, a method that achieves superior performance in removing targeted knowledge while maintaining model fluency and capabilities.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=AdiNf568ne",
    "pdf_link": "https://openreview.net/pdf?id=AdiNf568ne",
    "comments": [
        {
            "summary": {
                "value": "In this paper, the authors proposed to evaluate LLM concept erasure with innocence, seamlessness and specificity, and put forward Erasure of Language Memory (ELM) as a new erasing method which claims to exile in all these three aspects by integrating losses (i.e. erasing, conditionally fluency, and retention) specifically designed for each of them. ELM was tested on WMDP dataset and evaluated with multiple choice questions, perplexity and MTBench/MMLU to exhibit innocence, seamlessness and specificity respectively. Based on the results on Zephyr 7b, ELM, compared with RepNoise and RMU, achieves marginally higher performance in terms of innocence and specificity, while processing a larger advantage in seamlessness. A similar experiment was conducted about erasing the concept of Harry Potter from Llama2 7b, the result of which, instead, indicates noticeably better innocence, with marginally better seamlessness and specificity, than RMU and WHP. There is also an ablation study to verify the impact of the losses on the three desiderata, as well as a robustness experiment by luring the LLM to comply with the request about the erased concept using GCG to show that ELM responses are less gibberish than the baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The overview picture about the two individual losses are helpful for understanding the design of ELM.\n2. The idea of keeping the models' response seamless post removal of a concept is interesting."
            },
            "weaknesses": {
                "value": "1. Inconsistent and Incomplete Experiments & Non-significant Improvement: In the main experiment, comparisons with baseline methods are only made for Zephyr 7b and missing for the Mistral and Llama 3 models, making it hard to draw a convincing conclusion about the merits of ELM. Additionally, in the experiment about removing the concept of Harry Potter, RepNoise was replaced with another baseline WHP, and the target model is switched to Llama 2 7b. The experiment settings are not consistent with each other. The results in the two experiments are also telling different stories: in both cases ELM only shows clear advantage in one of the three desiderata, and its seamlessness in the former and innocence in the latter, but there lacks discussion about this difference.\n2. Questionable Claim about Seamlessness: While seamlessness is definitely a desirable feather for concept erasure, measuring it with PPL is obviously insufficient. Based on the example responses in the main body as well as in appendix, responses post ELM are only less gibberish but not actually meaningful to make it acceptable seamless.\n3. Questionable Novelty: Tackling concept erasure by combining losses designed for forgetting and retaining is nothing new. The baseline RMU, for instance, also has a forget and a retain loss, with the only difference being using cross-entropy or L2 distance. The fluency objective is relatively new, but it only seems incremental to the existing framework.\n4. Poor Presentation: The section about probing and activation analysis, supposedly, is going to be interesting. However, Figure 3 is using a elusive legend and captions, making it impossible to relate the the analysis with the data in the figure."
            },
            "questions": {
                "value": "Most question are covered in the weaknesses outlined above, e.g. Why using such a inconsistent setup across experiments? Why so little comparisons? How to interpret the different improvement in different experiments?, etc. The authors are encouraged to include more results and explanations to alter my opinion on these weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies how to effectively erase corresponding knowledge from LLMs. The authors first propose three metrics (Innocence, Seamlessness, Specificity) to comprehensively measure the effectiveness of a knowledge erasure method. The authors claim that current erasure methods typically fall short in one or several metrics above.  Then, the authors propose a  method called the Erasure of Language Memory (ELM) that is motivated from the classifier-free guidance diffusion work to achieve more effective erasure performance on all three metrics."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The model unlearning/knowledge erasure problem is important and practical, as there are many cases that we want the LLMs to forget some targeted concepts.\n\n2. The motivation in Eq. (3)-(5) is interesting and clear, which makes the method easy to follow.\n\n3. The proposed metrics are comprehensive and practical.\n\n4. The ablation studies in Section 5.3 are thorough."
            },
            "weaknesses": {
                "value": "I have some questions for the authors:\n\n1. Regarding the Erasing Objective in Line 233-247, I think the form of probability function is somehow similar to the form of emulated fine-tuning/decoding-time alignment [1,2], which also manipulate the predicted probability distribution by multiplying a re-scale factor (though this factor is different in their works). Maybe the authors could add more discussion or comparison on these works to  make the contribution more clearer.\n\n2. Regarding the Conditional Fluency Objective, I am concerned about its necessity. As the Erasing Objective  is already included, why do we need this Conditional Fluency Objective? I would assume based on the Erasing Objective , the model can learn to generate fluent response by attempting to respond to the alternative concept $c_{+}$ even though the prompt is $c_{-}$.\n\n3. In Table 1, why do you not perform experiments with RMU and RepNoice on other three LLMs? Also, the advantage of ELM compared with RMU seems to be limited.\n\n4. The authors should include a Ethics Statement section after the main text before references, as knowledge erasure is related to some ethical concerns.\n\n[1] Mitchell, Eric, et al. \"An emulator for fine-tuning large language models using small language models.\" ICLR 2024\n\n[2] Liu, Tianlin, et al. \"Decoding-time Realignment of Language Models.\"  ICML 2024"
            },
            "questions": {
                "value": "See the weakness part.\n\nTypos and presentation errors:\n\n(1) Line 51, \"fine tune\" -> \"fine-tune\"\n\n(2) Line 243 and 244 ''..'' -> ``....''\n\n(3) Line 288, \\citet -> \\citep\n\n(4) Line 294, Llama3-7B -> Llama3-8B\n\n(5) Table 3, the R-PPL of WHP should also be highlighted."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I suggest the authors to include a Ethics Statement section after the main text before references."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a framework for erasing conceptual knowledge from language models, aiming to ensure that the erased knowledge is irretrievable while maintaining the model's general functionality and fluency. The authors propose the \"Erasure of Language Memory\" (ELM) method, which is designed to meet three criteria: innocence, seamlessness, and specificity."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The presentation is well-written and easy to follow. The proposed three criteria\u2014innocence, seamlessness, and specificity\u2014are particularly insightful."
            },
            "weaknesses": {
                "value": "1. The paper lacks a robust justification for using low-rank adapters. It claims that full fine-tuning may lead to overfitting, but it fails to provide comparative experiments to substantiate this claim. Demonstrating overfitting through empirical results would strengthen the argument.\n\n2. The benchmarking is limited. While the paper focuses on multiple-choice tasks, it does not explore how the method performs on open-ended generation tasks, which could provide a more comprehensive evaluation of the model's capabilities.\n\n3. In Table 1, the implementation of baselines such as RMU for models like Mistral-7B and Llama3-8B is missing. Why?\n\n4. I appreciate the ablation study conducted in Section 5.3; however, the approach could be more rigorous. Rather than simply removing one component of the loss function as per Equation 12, it would be more insightful to vary the hyperparameters $\\lambda_1, \\lambda_2, \\lambda_3$ to understand their individual and combined effects on the results. The author should also develop a framework to control those hyperparameters to achieve good tradeoffs/performance.\n\n5. The accuracy of RMU outperforms ELM in the virology benchmarks in Figure 2. Why? Are there any specific explanations?\n\n6. The range of attacks tested in Section 5.5 is limited. To demonstrate the robustness of their defense methods universally, it would be better to include a wider variety of adversarial attack methods. This would help validate the general applicability of the defense strategies against a broader spectrum of potential threats."
            },
            "questions": {
                "value": "See the Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}