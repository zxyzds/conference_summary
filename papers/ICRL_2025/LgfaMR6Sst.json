{
    "id": "LgfaMR6Sst",
    "title": "Flexible Active Learning of PDE Trajectories",
    "abstract": "Accurately solving partial differential equations (PDEs) is critical for understanding complex scientific and engineering phenomena, yet traditional numerical solvers are computationally expensive. Surrogate models offer a more efficient alternative, but their development is hindered by the cost of generating sufficient ground-truth data from numerical simulations. In this paper, we present a novel framework for active learning (AL) in PDE surrogate modeling that reduces the data acquisition cost and improves model accuracy. Unlike the existing AL methods for PDEs that always acquire entire PDE trajectories, our approach strategically queries only a subset of the time steps from a numerical solver along a trajectory, while employing a surrogate model to approximate values for the remaining steps. This dramatically reduces the cost of data acquisition, which is proportional to the number of time steps simulated by the numerical solver, and thus allows the active learning algorithm to try out a more diverse set of trajectories given the same computational budget. To accommodate this novel framework, we develop an acquisition function that estimates the utility of a set of time steps by approximating its resulting variance reduction. We demonstrate the effectiveness of our method on several benchmark PDEs, including the Heat equation, Korteweg\u2013De Vries equation, Kuramoto\u2013Sivashinsky equation, and the incompressible Navier-Stokes equation. Extensive experiments validate that our approach outperforms existing methods, offering a cost-efficient solution to surrogate modeling for PDEs.",
    "keywords": [
        "Active learning",
        "Partial Differential Equation (PDE)"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "We propose an active learning framework that reduces the data acquisition cost for PDE surrogate modeling by selectively querying time steps along a trajectory from numerical solvers.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=LgfaMR6Sst",
    "pdf_link": "https://openreview.net/pdf?id=LgfaMR6Sst",
    "comments": [
        {
            "comment": {
                "value": "Thanks for these explanations!"
            }
        },
        {
            "comment": {
                "value": "Thank you for your prompt reply. We are glad that the explanations helped in clarifying the confusions. We will update you with a revised version of the paper as soon as possible. In the meantime, please feel free to ask any further questions or comments if you have any."
            }
        },
        {
            "comment": {
                "value": "Thank you increasing the score. We will update you with the revised manuscript as soon as possible.\n\nTo answer your question about why FlexAL 10 is not the default method, we would like to note that FlexAL 10's performance in all four equations as reported in Table 14 lies almost in the middle of baseline and FlexAL. It thus might not have been appropriate to call FlexAL 10 exactly \"comparable\" to FlexAL, but it is definitely superior to all the baseline methods.\n\nBelow are answers to the \"less important\" questions. We thank you again for taking the time to provide such detailed and valuable feedbacks.\n\n> Line 053, \"we argue that querying all the states (...) is not cost-efficient\": does this sentence perhaps require more nuances? While sparse subsets of trajectories decrease the complexity (on paper), the suggested procedure for finding them is sufficiently expensive that the proposed algorithm is more costly than full-batch versions (Section 3.3, Table 4). \n\nWe will clarify in the paper that what we mean by \"cost-efficient\" is that we reduce the cost of data acquisition required for achieving a certain accuracy. A better wording might have been \"sample-efficient\".\n\n> Line 095: Perhaps Brandstetter et al. (2022b) are not the best reference for spatiotemporal PDEs. Personally, I don't think this sentence needs any reference, but if one should be used, perhaps something like Evans' \"Partial Differential Equations\" book would be more appropriate.\n\nThank you for the suggestion, we agree that it probably doesn't need a reference. We will make this revision.\n\n> Line 100: This statement about existence would benefit from a reference.\n\nThank you for the suggestion. Our claim should actually be fixed as follows: \"If the PDE and boundary conditions are *well-posed*, then there exists a unique evolution operator ...\". We will add reference to the fact that all the PDEs used in our experiments are well-posed.\n\n> Line 112: What does \"primary focus\" mean here? It seems to be the only focus. Have I missed something?\n\nNo, you haven't missed anything. We will rephrase it as \"In this\npaper, we focus on the autoregressive trajectory prediction task\".\n\n> Line 128: Why does this sentence introduce a distribution of initial conditions, but the rest of the manuscript (for example, Algorithm 1) operates on a pool of initial conditions? I understand that the pool is sampled from the condition, but it might be more reader-friendly to use a pool of conditions throughout.\n\nThis sentence was intended to acknowledge that the test data are sampled from the same distribution as the pool used for training. We agree that this might be confusing, and will instead rephrase it as \"We assume that there exists a pool $P$ of initial conditions.\" We will make the comment about the test data in the experiments sections.\n\n> Line 262: How do the results depend on this choice of T and epsilon?\n\nWe haven't done extensive experiments with different values of $T$ and $\\epsilon$. Increasing $T$ improves performance until it plateaus. Increasing $\\epsilon$ values higher than a certain point deteriorates the performance slighly. On the other hand, decreasing $\\epsilon$ too much also deteriorates the performance, but can be recovered with a higher value of $T$, leading to higher computational cost. We will add these in the revision.\n\n> Line 298: The abbreviation \"QbC\" is used early in the paper (for instance, in line 117 or 204). Maybe it would be good to introduce it before line 298.\n\nThank you for pointing this out. We will introduce it earlier in line 117."
            }
        },
        {
            "comment": {
                "value": "Thank you for the comprehensive reply. I'm looking forward to seeing the promised changes in the manuscript.\n\nThank you also for clarifying the FlexAL 10 is not a cut in the number of training epochs. I must've misunderstood this part in my initial review.\n\n>  in the greedy selection algorithm. Table 14 in the Appendix shows that the performance of FlexAL 10 is comparable to that of FlexAL, and always outperforms the best baseline method\n\nTable 14 shows this in a single experiment, and all other studies involve FlexAL instead of FlexAL 10. If FlexAL 10 is faster than FlexAL (which it seems to be), and if its output is comparable to FlexAL (which Table 14 suggests), I am now wondering why FlexAL 10 is not the default method for all other experiments. But I suppose that the choice of $T$ depends on the problem.\n\nThanks again for all the clarifications. I have increased my score, assuming that the promised revisions will end up in the manuscript."
            }
        },
        {
            "comment": {
                "value": "I think the authors for addressing my questions. This explanation has substantially improved my understanding of the work, and my estimation of its value. I am prepared to revise my score upwards conditional upon the final explanation in the paper clarifying the explanation and thus my confusion (which I observe I share with other reviewers).  The explanation in the common response is a good start; I think clarifying it to the satisfaction of the reviewers might need a little more intuition-building, and clarification of language throughout to disambiguate the various kinds of \"Steps\" etc. I think the authros have demonstrated good progress already in the review process already"
            }
        },
        {
            "comment": {
                "value": "To be clear my score does not depend upon the authors renaming the method; I merely mention the name problem because I am reviewing many papers that name something non-specific \"Flexible\" or \"Generalized\" etc; whether the paper is easy to find on google should probably be the arbiter of the suitability of the name, not my personal taste."
            }
        },
        {
            "comment": {
                "value": "We would like to clarify our paper's main contribution.\n\nOur core contribution is a novel framework for acquiring PDE trajectory data that allows sampling specific time steps rather than requiring entire trajectories. The key aspects are:\n\n1. Regarding \"method is similar to prior works\":\n- This is the first work to propose partial trajectory sampling for PDEs\n- Unlike prior works that must acquire complete trajectories, we can selectively sample time steps\n- Our acquisition function is specifically designed to handle this novel sampling setting\n- We demonstrate clear improvements over baseline methods, e.g., achieving same accuracy with half the data acquisition cost\n- We highly suggest reading through **Common Response 1. Simple explanation** for a simple explanation of our method\n\n2. Regarding \"nothing about PDE-ness\":\n- Our method explicitly leverages the Markov property of PDEs where each state depends only on the previous state\n- The sampling pattern design accounts for PDE-specific temporal dependencies\n- We demonstrate effectiveness on challenging PDEs including Navier-Stokes \n- While the general framework could potentially extend beyond PDEs, solving PDEs efficiently is an important problem with real-world applications\n\n3. Regarding \"ad-hoc and heuristic\":\n- Uncertainty-based approaches have solid theoretical foundations [1] [2]\n- We provide empirical validation across multiple PDEs\n- The improvements are consistent across different error metrics and quantiles\n\nWe hope this helps clarify our contribution. We welcome any specific questions about these aspects of our work.\n\nReferences:\n\n[1] Yoav Freund, H Sebastian Seung, Eli Shamir, and Naftali Tishby. Selective sampling using the query by committee algorithm. *Machine Learning*, 28:133\u2013168, 1997.\\\n[2] Steve Hanneke. Theoretical foundations of active learning. Carnegie Mellon University, 2009."
            }
        },
        {
            "comment": {
                "value": "> to my mind it would be more satisfying to search for useful initial conditions from the traiing distribution, or to simply give up on rollouts when a given trajectory was no longer adding useful value, or to learn a predictor which could be conditioned on a rollout time, and we might imagine some scheme more elegant than this Bernoulli masking\n\nWe appreciate these ideas. Note that we do actually search for useful initial conditions from the training distribution, as described in Section 3.3. This is done by a base AL method $\\mathcal A$ (e.g. SBAL) on which we attach FlexAL. As to your second idea, we actually started our research with the same idea, but found a critical problem with these kinds of approaches: they rarely sample the later time steps of a trajectory. This renders the surrogate model\u2019s performance extremely low on the later parts of trajectories. (Note that the distribution of the PDE states varies across time points, so data at earlier time steps are not guaranteed to help the model in later time steps.) Table 12 and 13 in the Appendix provide partial proof of this claim, as sampling initial time steps only (Initial Ber(p)) gave surrogate models with much worse performance, sometimes even worse than full-trajectory sampling. We hence realized that sampling time steps almost uniformly is crucial to success. To do so, we need to be able to skip some intermediate time steps, which we think is only possible with the surrogate model. Despite our initial concern with the resulting shift in the input distribution, we empirically found that its effect is much smaller than the overall benefit of sampling from more trajectories under the same budget. On your last idea of a predictor conditioned on rollout time, this is an existing method in the literature of PDE modeling, but is not standard as it tends to underperform compared to the approach of modeling individual time steps. Overall, we aimed at the simplest method possible that can deliver true robust gains in performance.\n\n> FlexAL is not IMO a great name; this is not particularly flexible compare to other active learning/DOE methods.\n\nWe also see how it might not have been the best name, as it can\u2019t be applied to active learning outside of PDE data. We appreciate the feedback, and will consider changing the name to a more suitable form."
            }
        },
        {
            "comment": {
                "value": "We appreciate your thoughtful questions and feedbacks. We encourage you to read through **Common Response 1. Simple explanation** for some clarifications. Below we also answer the specific questions you had about the method. If these still do not answer your questions, or if you have any other questions, please feel free to ask us.\n\n> AFAICT each successive step in the simulator depends on the previous ones. Figure 6 seems to support this, as does text l283 \"in our setting, we cannot directly acquire a time point, because there is a cost in the simulation of the trajectory.\").\n\nThat is right. As described in Section 3.1, we roll out the trajectory with a mixture of the numerical solver and the surrogate, and each successive step depends on the previous step.\n\n> OK, that seems fine, but at a later stage of my training I would get a different output from $\\hat{G}$\n\nThat is right.\n\n> so my training data would change and then I would need to recalculate $\\hat{u}^2$, right?\n\nNo, we just keep using the training data obtained in the earlier round, that is, $(x,y) = \\left ( \\hat G u^0, (G \\circ \\hat G)u^0  \\right ) $ obtained with $\\hat G$ in the earlier round. We also think our paper could benefit from some clarification on this, and plan to do so in a revised version. Thank you for the question.\n\nWe emphasize that time steps simulated with $\\hat G$, e.g. $(x,y) = (u^0, \\hat G u^0)$, are never added to our training data, but only those simulated with $G$ (Section 3.1).\n\n> This should introduce an asymmetry between the acquisition functions early in the time series, and later, since the later ones are likely to be sample from some other distribution than the training distribution (e.g. they may have failed to conserve mass or momentum or whatever, when the surrogate roll-out was used).\n\nWe designed the acquisition function, described in Section 3.2, to address this problem. Imagine an alternative acquisition function that simply selects time points with large variance (this is not what ours is doing). The variance would be larger for later time points because their inputs will be more out-of-distribution. Perhaps this is what you were imagining when asking this question. This would indeed be catastrophic, as the active learner will be heavily biased towards sampling the later time points, only worsening the problem. Our acquisition function is different, however, and estimates the reduction in variance. The acquisition function thus penalizes a sampling pattern $S$ that is expected to yield an out-of-distribution trajectory. We hope this answers your question, but if not, we would be more than happy to elaborate."
            }
        },
        {},
        {
            "comment": {
                "value": "> Table 4: It would be great to include the context of how much runtime a numerical solver needs to simulate Navier-Stokes. Whichever outcome (in terms of \"who's faster\") is fine, but the context would help assess the efficiency of the active-learning methods. If the solver is slow, the context would underline the statement in line 034 that states how costly numerical solvers can be.\n\nWe agree this would strengthen our motivation. Although the numerical solver for Navier-Stokes equation only takes a few tens of seconds per round, the numerical solver for KdV equation without batch processing takes 1,232 seconds per round, which could be effective in underlining our statement about how costly numerical solvers can be. We will add this information to Section 5.8.\n\nWe would like to thank you again for the valuable feedbacks and questions. We will provide answers to the \"less important\" questions soon. In the meantime, please feel free to ask us if you have any further questions or comments."
            }
        },
        {
            "comment": {
                "value": "> Table 1: log-RMSEs of  imply RMSEs of , which seems to be large. Is it fair to assume that the solvers learn anything reasonable? For example, what happens if one plots the solutions and compares the surrogate's solution to the solver's? Suppose the reconstruction is good despite these large errors. Could the table be made clearer (in the sense of \"success\" meaning \"RMSE far below 1\") by choosing (for example) relative RMSE over absolute RMSE?\n\nThe RMSEs are on different scales for each PDE due to the inherent scales of the solutions. I have attached an instance of KS equation where the surrogate model has instance-specific RMSE of 0.958.\n\nhttps://anonymous.4open.science/api/repo/iclr2025rebuttal-18AB/file/ks.png?v=5f58a930\n\nAs you suggest, normalized (relative) RMSE is a better measure of the \"success\" of a model that is scale-independent. In fact, the average log NRMSE of Random on KS is -1.683, compared to an average log RMSE of \u22120.258. We have followed the convention of Musekamp et al. [2] in using RMSE instead of normalized RMSE, but we now think that normalized RMSE would have been more appropriate. We already have tables of normalized RMSE in Appendix B, but not the graphs of normalized RMSE against AL round. We will add these graphs. We will also provide instances of the surrogate model's predictions like the attached image.\n\nReferences:\n\n[2] Daniel Musekamp, Marimuthu Kalimuthu, David Holzm\u00fcller, Makoto Takamoto, and Mathias Niepert. Active learning for neural pde solvers. *arXiv preprint arXiv:2408.01536*, 2024."
            }
        },
        {
            "comment": {
                "value": "> Section 3.2: Why choose the acquisition function based on variance reduction? Are there other candidates, and if so, why are they less suitable?\n\nOne can imagine several alternative acquisition functions.\n\nThe most straightforward alternative is to simply use the sum of the variances at time points for which $b_i = \\mathsf{true}$. The variances are larger for the later time steps since they accumulate, and in our preliminary experiments, we found that this is catastrophic as undersampling the earlier time steps leads to the sampled trajectory being very out-of-distribution, and hence the trained surrogate model underperforming on the test distribution.\n\nIt quickly became clear to us that we need some kind of measure of \"how much total uncertainty will be reduced by sampling these time steps\", instead of \"how uncertain is our model on these time steps?\" One way to approximate this is to use mutual information (or information gain), as used by Li et al. [1]. In other words, we rollout $N$ trajectories with $N$ surrogate models, and compute the mutual information between time steps for which $b_i=\\mathsf{true}$ and all time steps. However, in preliminary experiments, we found that this method underperforms, which we hypothesize is because relying simply on the covariance matrix of the committee between time steps is not a good enough method for computing the posterior uncertainty.\n\nWe identified two \"pathways\" through which sampling a time step reduces uncertainty in the remaining time steps. First, there is the \"indirect\" pathway: sampling a time step will reduce the model's uncertainty on similar inputs, hence reducing uncertainty on the remaining time steps. This is what is approximated by mutual information. Then, there is the \"direct\" pathway: sampling a time step $i$ gives out the $i+1$ th state, which starts a chain reaction of reducing model uncertainty on all successive states. Note that these two pathways are not distinct from a strictly theoretical view, but are rather two ways of approximating uncertainty reduction.\n\nThe direct pathway motivated our acquisition function based on variance reduction. In variance reduction, we calculate the posterior uncertainty by rolling out the trajectories with $N$ surrogate models, but collapse into one surrogate model at time steps for which $b_i = \\mathsf{true}$. This effectively computes the reduced uncertainty due to the effect of the direct pathway. With experiments, we confirmed that this acquisition function performs robustly, and behaves just like we wanted: it is slightly biased towards sampling the earlier time steps, and it chooses an appropriate frequency of time steps to sample that leads to good performance.\n\nWe will make sure to include a simplified version of this in our revised manuscript. Thank you for the question, we also think this is an important missing part of the current manuscript.\n\nReferences:\n\n[1] Shibo Li, Zheng Wang, Robert M. Kirby, and Shandian Zhe. Deep multi-fidelity active learning of\nhigh-dimensional outputs. In *International Conference on Artificial Intelligence and Statistics*,\nAISTATS 2022, 28-30 March 2022, Virtual Event, 2022b."
            }
        },
        {
            "comment": {
                "value": "We appreciate your thoughtful questions and feedbacks.\n\n> The submission acknowledges this shortcoming and discusses a fix that cuts the number of training epochs for FlexAL.\n\nWe would like to clarify that FlexAL 10 in Section 5.8 is actually not a cut in the number of training epochs, but the number of optimization steps $T$ in the greedy selection algorithm. Table 14 in the Appendix shows that the performance of FlexAL 10 is comparable to that of FlexAL, and always outperforms the best baseline method.\n\n> However, the increased runtime is a weakness of the proposed method compared to existing techniques nonetheless. \n\n> Now, to convince me that this weakness isn't one, it would suffice to find an example where the additional computation for batch selection (compared to full-trajectory QbC, for instance) can be negligibly small.\n\nWe appreciate this feedback. In practical settings, the cost of data acquisition (running the numerical solver) is so high that the cost of batch selection is almost negligible. For example, if we do not use batch processing for the numerical solver, acquiring KdV trajectories for one round of AL takes 1,232 seconds, while the batch selection process of QbC, FlexAL, and FlexAL 10 respectively take 10.6, 40.1, and 4.5 seconds. (With batch processing or for other benchmark equations, our numerical solver is too cheap to run that they don't really help in proving our point.)\n\nTo directly address your concern about the increased runtime in batch selection process, we provide an example where FlexAL adds minimal runtime compared to the base method. If the pool size is ten times larger (10,000 -> 100,000) than used in our experiment, the wall clock time of QbC and SBAL increases tenfold, while that of FlexAL remains constant because it is proportional to the budget, not the pool size. This larger pool size of 100,000 is used in Musekamp et al. [1], and is more reflective of real world settings where unlabelled data are abundant. For KdV, the runtime of QbC would be around $10.6 \\times 10 = 106$ seconds, while for FlexAL and FlexAL 10 it would remain constant at 40.1 and 4.5 seconds. A 4.5/106 ~= 4% increase in computational cost due to FlexAL 10 is negligible. The same analysis holds for all equations, as summarized in Table 1.\n\nTable 1. Expected wall-clock time of batch selection, with pool size 100,000.\n\n| Equation | QbC | +FlexAL | +FlexAL 10 | Ratio (+FlexAL 10 / QbC) |\n|---|---|---|---|---|\n| Heat | 103 | 43.0 | 4.3 | 0.04 |\n| KdV | 106 | 40.1 | 4.5 | 0.04 |\n| KS | 181 | 78.4 | 8.6 | 0.05 |\n| NS | 455 | 92.2 | 10.5 | 0.02 |\n\n(We used the smaller pool size of 10,000 due to memory constraints. The fact that wall-clock time of QbC increases proportionally with pool size was validated empirically with measurements at pool size of 100, 1000, and 10,000. The same was done for FlexAL and FlexAL 10 to check that their wall-clock times remain constant.)\n\nWe also think these are important points that should be addressed in the paper, and plan to revise the manuscript accordingly."
            }
        },
        {
            "comment": {
                "value": "> P2 line95, What is the space of \\mathbb{X}? Please define the space.\n\n$\\mathbb{X}$ is a general notation for any spatial domain. In our case, it would be $\\mathbb{X}=I$ where $I=[0,1]$, and for 2D Navier-Stokes, $\\mathbb{X} = I^2$.\n\n> P2 line 99, I highly doubt uniqueness of the operator G_{t0}. Given solution at $t=0$ and $t=Deltat$, there may exist infinite PDEs that satisfy both initial and final condition. \n\nThe PDE defines the operator $G_{t_0}$, not the other way around. For example, if we have a wave equation, $G_{t_0}$ takes the wave shape at time $t_0$ and outputs its shape at $t_0 + \\Delta t$. P2 line 99 is saying that this operator $G_{t_0}$ is unique to each PDE.\n\n> P3 eq. 2, Wouldn't PINN loss help here, if the PDE is known?\n\nUsing the residual loss as in PINN can help, and there are indeed active learning methods for PDEs that use this loss as an acquisition function. However, a low residual loss at a point doesn\u2019t necessarily mean that the model has low error globally, since the error can accumulate at earlier time steps. Moreover, it can\u2019t be applied to a more general setting where the PDE is unknown. We kindly refer you to Section 4.1 for a survey of the existing AL methods in PDEs, including ones that use the residual loss.\n\n> Algorithm 1, line 7: please clarify notation.\n\nThe $\\oplus$ symbol stands for XOR operation. Algorithm 1, lines 6 and 7 describe the process of applying random bit-flips to $S$ to obtain $S'$.\n\nWe will make sure to add these clarifications in a revised manuscript. Thank you for asking these, and please feel free to ask any other questions that you have."
            }
        },
        {
            "comment": {
                "value": "We appreciate your thoughtful questions and feedbacks. We provide a simple explanation of our method in **Common Response 1. Simple explanation** in hopes of clarifying some confusions.\n\n> The proposed method does not seem to have a significant improvement compared to the other compared methods. For example in Fig3, RMSE of proposed strategy is slightly (around 10%) better than SBAL, random, or QbC, for KdV, KS, and NS. I wonder why the method performs so much better in case of heat equation. It definitely needs further investigation.\n\n**We cannot see any ambiguity in the significance of our results.** Musekamp et al. [1] provide experiments with baseline methods on PDEs, and report marginal improvement over Random on challenging equations such as KS and NS. This is consistent with our results: for KS and NS, there is hardly any improvement with existing baseline methods (QbC, SBAL, LCMD) over Random, and some methods even underperform compared to Random. FlexAL is the only method that consistently improves over Random, and it does so by a significant margin. In the table below, we report the improvement in average log RMSE from Random, and the ratio between the improvement of FlexAL and that of the best baseline method. FlexAL provides two to six times improvements over the best methods.\n\nTable 1. Average log RMSE, improvement over Random.\n\n| Equation | QbC | LCMD | SBAL | SBAL+FlexAL | Ratio (SBAL+FlexAL / best) |\n|----------|-----|------|------|------------|------------------------|\n| Heat | -0.236 | -0.053 | -0.213 | **-0.616** | 2.6 |\n| KdV | 0.075 | 0.065 | -0.161 | **-0.279** | 1.7 |\n| KS | -0.010 | 0.304 | -0.017 | **-0.091** | 5.4 |\n| NS | -0.007 | 0.032 | -0.002 | **-0.042** | 6.0 |\n\nWe encourage you to look at the results in Figure 4 of Musekamp et al. [1], Figure 3(b) of Li et al. [2] (the yellow and blue lines correspond to Random and Coreset), and Figure 5 of Bajracharya et al. [3]. Our method provides arguably the largest and the most robust performance gain reported in the PDE active learning literature. In Appendix B, we have reported other metrics such as normalized RMSE, MAE, and 99%, 95%, 50% quantiles of RMSE, which all point to the superiority of FlexAL over baselines. Our results also show strong statistical signficance as evidenced by the non-overlapping error bars.\n\nMost importantly, we should consider the *data efficiency*, that is, the amount of data required to attain a certain accuracy. Consider the NS experiment for example. The RMSE of SBAL+FlexAL at the fifth round is lower than the RMSE of the best baseline at the tenth round. FlexAL therefore halves the amount of data required to attain this accuracy. Carrying out this analysis on all equations, we see reductions in data cost ranging from 10% to 50%. A 10% reduction in data acquisition cost is far from a minor improvement. This can translate to hours, days, or even weeks of time saved by FlexAL in real world simulations.\n\nAll in all, the **robust** and **large** performance gains of FlexAL are unprecedented in the PDE active learning literature, and we believe this is a significant contribution to the field.\n\n> I think the presented idea lacks novelty.\n\nWe explore, for the first time, the idea of simulating PDE trajectories with a mixture of numerical solvers and surrogate models. We also propose an active learning method that adaptively chooses which time steps to query to the numerical solver or the surrogate model. The proposed acquisition function is novel and also generally applicable, which we believe is a fundamental contribution to the field of active learning. We suggest reading **Common Response 1. Simple explanation** for a simplified explanation of our method, which we hope gives a clearer picture of our method's novelty.\n\nReferences:\n\n[1] Daniel Musekamp, Marimuthu Kalimuthu, David Holzm\u00fcller, Makoto Takamoto, and Mathias Niepert. Active learning for neural pde solvers. *arXiv preprint arXiv:2408.01536*, 2024.\\\n[2] Shibo Li, Xin Yu, Wei Xing, Robert Kirby, Akil Narayan, and Shandian Zhe. Multi-resolution active learning of fourier neural operators. In *International Conference on Artificial Intelligence and Statistics*, pp. 2440\u20132448. PMLR, 2024.\\\n[3] Pradeep Bajracharya, Javier Quetzalc\u00f3atl Toledo-Mar\u00edn, Geoffrey Fox, Shantenu Jha, and Linwei Wang. Feasibility study on active learning of smart surrogates for scientific simulations. *arXiv preprint arXiv:2407.07674*, 2024."
            }
        },
        {
            "comment": {
                "value": "**Common Response 1. Simple explanation**\n\nWe provide a simplified explanation of our method.\n\nGiven a PDE, a time-interval $\\Delta t$, and a numerical solver $G$ that computes how a PDE state evolves over $\\Delta t$, we aim to train a surrogate model $\\hat G$ using data from $G$. For example, if we have a wave equation, $G$ takes the current wave shape and computes its shape after $\\Delta t$ seconds. Since running $G$ is expensive, we want to acquire training data efficiently using active learning.\n\nUnlike existing methods that acquire entire trajectories using $G$, we propose acquiring trajectories using a mixture of $G$ and our current surrogate $\\hat G$. The process:\n\n1. Select an initial condition $u^0$\n2. Choose a sampling pattern $S=(b_1, \\dots, b_L)$ (e.g., $S = (\\mathsf{true}, \\mathsf{false}, \\dots, \\mathsf{true})$) \n3. Simulate the $i$ th time step using $G$ when $b_i$ is $\\mathsf{true}$, $\\hat G$ otherwise\n4. Add only $G$'s input-output pairs to training data\n\nis repeated until the batch reaches a fixed budget $B$. With budget $B = 32$:\n- Traditional: 2 full trajectories of length 16 \n- FlexAL: ~8 trajectories with 4 key time steps each\n\n\nWe propose a novel acquisition function to choose optimal patterns $S$. We emphasize again that we never train on $\\hat G$'s outputs \u2014 it only bridges gaps between solver-generated states.\n\n\nTo summarize, our method allows for sampling of more trajectories under the same budget, compared to traditional methods which are special cases of ours where $S = (\\mathsf{true}, \\mathsf{true}, \\dots, \\mathsf{true})$. Our method benefits the surrogate model by improving the diversity of the training set."
            }
        },
        {
            "comment": {
                "value": "We appreciate your thoughtful questions and feedbacks. We provide a simple explanation of our method in **Common Response 1. Simple explanation** in hopes of clarifying some confusions.\n\n> For example, given a pattern S = {T, F,F,..., T}, which is basically sampling an initial condition and final condition, how will we sample the data here without sampling all intermediate states? If we cannot do that, then how does the method reduce the cost of acquiring training data? \n\nAs described in Section 3.1, the intermediate states are sampled with the surrogate model $\\hat G$.\n\n> How much computation cost, data acquisition cost is reduced in this framework? The experimental results report the accuracy of the trained model, however since the original motivation of the work is about reducing such costs, presenting these additional statistics makes more sense than only reporting model accuracy alone.\n\nAs described in Section 2.2, each round of AL incurs the same data acquisition cost of $B$. Therefore, the graphs of the RMSE with respect to the AL round, such as Figure 3, can also be seen as graphs of RMSE with respect to the cost. We are effectively reducing the cost required to attain the same accuracy.\n\nWe can see how this can be confusing for the readers. We intend to clarify this point by explaining it in the experimental results section. Thank you for the question.\n\n> Algorithm 1 should be described line by line, probably best to do this at the end of section 3.3.\n\nWe appreciate this feedback. Although the last paragraph of Section 3.3 is intended to be a description of Algorithm 1, we failed to explain some notations and steps in Algorithm 1. We therefore plan modify the explanations as below. Modifications are highlighted in boldface.\n\n\u201cSpecifically, a full-trajectory AL method **\\mathcal A**, which we call a \\textit{base} method, first selects an initial condition $u^0$.\u201d\n\n\u201cIn the greedy algorithm, we start by initializing $S$ with all entries set to **\\mathsf{true}**. At each step, we propose a neighboring pattern **S\u2019** by applying a bit-flip mutation, where each bit of $S$ is flipped with a probability of $\\epsilon$. The proposal is accepted only if the acquisition value **a^*(\\bm u^0,S')** is higher than the current value **a^*(\\bm u^0, S)**. **This process of proposal and acceptance/rejection is repeated for T times.**\"\n\n> Figure 1 needs some rethinking, currently it is difficult to see the author\u2019s motivation, or the entire framework from this figure alone. Perhaps show how the cost increases with added data acquisition side by side with the baseline and FLEXAL strategy\n\nWe appreciate this feedback. We would first like to ask if the motivation behind Figure 1 might have become clearer to you after reading our answers above. In addition, we have modified the figure such that the number of queries to the numerical solver (black solid lines) is equal for both sides, better reflecting our fixed budget framework.\n\nhttps://anonymous.4open.science/api/repo/iclr2025rebuttal-18AB/file/Figure1_v2.png?v=189a6665\n\nWe also plan to add a more detailed explanation of Figure 1 in Section 3.1 as follows:\n\n\"In other words, the numerical solver and surrogate model are used selectively to acquire each time step. **Fig. 1 summarizes our framework. Each dot represents a PDE state, and a path connecting two dots represents a time step of a simulation. The black solid lines are obtained with numerical solvers while the red dotted lines with surrogate models. Both sides have the same number of black solid lines, reflecting the fact that we sample with a fixed budget $B$ at each round. Our method, FlexAL, can sample many more trajectories under the same budget.**\"\n\nIf you think Figure 1 needs other modifications, we would be more than happy to hear your thoughts."
            }
        },
        {
            "title": {
                "value": "Further details"
            },
            "comment": {
                "value": "Dear Reviewer Y8Kg, \n\ncould you provide further details on your review, in order to provide more concrete feedback to the authors and also to put your recommendation in perspective? \n\nNamely: \n\n- could you add references to claim that the method lacks novelty?\n- what do you mean with PDE-ness? How that plays a role in your criticism as it seems that this is method to train surrogate models. \n- could you provide an explicit recommendation for the baseline you suggest? (for simulation/image/3s scenes).\n- could you provide an example of a challenging problem? \n\nThis type of actionable feedback would be great for the authors, and also to put your recommendation in perspective. \n\nThank you again for your time and expertise.\n\nBest, \n\nAC"
            }
        },
        {
            "summary": {
                "value": "The authors propose an active learning framework for training surrogate models that can aid in solving partial differential equations. Traditional solvers for partial differential equations are computationally expensive, which motivated the development of surrogate models to efficiently solve PDEs. However, for training these surrogate models, costly numerical simulation data are required. Current active learning based strategies for training such surrogate models require entire PDE trajectories from a given starting condition, which is costly. The authors propose a flexible sampling strategy, which does not require entire PDE trajectories, and only samples upto a given budget. Experiments show that the method performs better in terms of RMSE compared to baseline."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The motivation of the work is presented well. The related literature is reviewed well in the introduction section and related work section."
            },
            "weaknesses": {
                "value": "The methodology of the paper requires a lot more elaboration. Here are a few points that are not clearly answered in the current manuscript:\nHow does sparse sampling works for numeric solvers? For example, given a pattern S = {T, F,F,..., T}, which is basically sampling an initial condition and final condition, how will we sample the data here without sampling all intermediate states? If we cannot do that, then how does the method reduce the cost of acquiring training data?\nHow much computation cost, data acquisition cost is reduced in this framework? The experimental results report the accuracy of the trained model, however since the original motivation of the work is about reducing such costs, presenting these additional statistics makes more sense than only reporting model accuracy alone.\nAlgorithm 1 should be described line by line, probably best to do this at the end of section 3.3\nFigure 1 needs some rethinking, currently it is difficult to see the author\u2019s motivation, or the entire framework from this figure alone. Perhaps show how the cost increases with added data acquisition side by side with the baseline and FLEXAL strategy"
            },
            "questions": {
                "value": "See weakness section above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a new approach of data acquisition for active learning. They suggest considering a subset of the time steps from a standard numerical solver along a trajectory, and use a fitted surrogate model to approximate the remaining of data. Given the proposed strategy does not provide a significant improvement compared to alternative methods consistently, I believe the current version of manuscript should not be accepted."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper makes a nice comparison between their suggested method of active learning and the other available methods in the literature."
            },
            "weaknesses": {
                "value": "I think the presented idea lacks novelty, and the shown numerical results are not suggesting any significant improvement compared to other methods."
            },
            "questions": {
                "value": "**Major:**\n\n- The proposed method does not seem to have a significant improvement compared to the other compared methods. For example in Fig3, RMSE of proposed strategy is slightly (around 10%) better than SBAL, random, or QbC, for KdV, KS, and NS. I wonder why the method performs so much better in case of heat equation. It definitely needs further investigation.\n\n**Minor:**\n\n- P2 line95, What is the space of $\\mathbb{X}$? Please define the space.\n- P2 line 99, I highly doubt uniqueness of the operator G_{t0}. Given solution at $t=0$ and $t=Delta t$, there may exist infinite PDEs that satisfy both initial and final condition. \n- P3 eq. 2, Wouldn't PINN loss help here, if the PDE is known?\n- Algorithm 1, line 7: please clarify notation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "For a partial differential equation (PDE) $\\partial_t u = F(u, \\partial_x u)$, fixed time-interval $\\Delta t$, and a solution/evolution operator $G$, which satisfies $G u(t, \\cdot) \\approx u(t+\\Delta t, \\cdot)$, the context for the submission is to approximate $G$ with a surrogate through active learning.\nThe submission proposes an alternative to computing full trajectories of the PDE solution and appending those to the dataset. Namely, the suggestion is to choose a sampling pattern S ~ (True, False, True, True, ...), simulate the PDE using a mix of surrogate and numerical solver (depending on True/False at each step), append all indices corresponding to the numerical solver to the dataset (skip the surrogate steps), retrain, and repeat.\nThe sampling pattern is chosen by optimising an acquisition function based on variance reduction, respectively, a batch version of it.\n\n\nNow, the driving question for assessing this method is how much this \"sparse\" sampling pattern improves over full-trajectory (or initial-step) active learning (e.g., Musekamp et al., 2024). The manuscript investigates an answer to this question on typical PDE benchmark problems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed algorithm is a clear generalisation of existing PDE active-learning methods: Instead of acquiring full trajectories or initial conditions, any combination of time points can now be used.\nThe manuscript is generally easy to follow (I point out specific questions below, but these are really minor). The experiments are comprehensive, and the paper is in good shape overall."
            },
            "weaknesses": {
                "value": "The proposed algorithm generalises existing schemes through corresponding choices of sampling patterns. \nHowever, optimising the sampling pattern is, simply put, a lot of work (concretely, $O(2^L)$ if one doesn't use the greedy selection algorithm, where $L$ is the sequence's length; Section 3.3 discusses this thoroughly). And while this additional work seems to improve the quality of the reconstructions (Tables 1, 2, & 3; Figures 3 & 4), the proposed FlexAL takes significantly longer to run (Table 4). \nWhether or not this increased runtime is problematic likely depends on the PDE. \nHowever, the increased runtime is a weakness of the proposed method compared to existing techniques nonetheless. \n\nThe submission acknowledges this shortcoming and discusses a fix that cuts the number of training epochs for FlexAL. Still, this discussion leaves some questions to be answered. For example, the training epochs could also be reduced for existing methods with corresponding runtime gains, and it needs to be clarified how this affects the results (unless I've missed something; I checked Appendix 5.8 and couldn't find such a discussion). \n\nNow, to convince me that this weakness isn't one, it would suffice to find an example where the additional computation for batch selection (compared to full-trajectory QbC, for instance) can be negligibly small. Alternatively, it would be interesting to see what happens to the reconstruction results in Table 1 or 2 if all columns do not receive the same number of iterations but are limited to roughly the same wall time.\n\nHowever, I understand that these changes are likely outside the scope of a revision, and I am in favour of accepting this paper without them. That said, if I get convinced that the assessed weakness isn't one, my score would be slightly higher."
            },
            "questions": {
                "value": "I group my questions into more important and less important ones.\nI don't expect a reply to the less important ones, but I would appreciate some clarification on the more important ones.\nThe answers do not affect my rating. However, I believe the manuscript would improve if the paper included them.\n\nMore important:\n\n- Section 3.2: _Why_ choose the acquisition function based on variance reduction? Are there other candidates, and if so, why are they less suitable?\n- Table 1: log-RMSEs of $\\approx 0.1$ imply RMSEs of $\\approx 1.0$, which seems to be large. Is it fair to assume that the solvers learn anything reasonable? For example, what happens if one plots the solutions and compares the surrogate's solution to the solver's? Suppose the reconstruction is good despite these large errors. Could the table be made clearer (in the sense of \"success\" meaning \"RMSE far below 1\") by choosing (for example) relative RMSE over absolute RMSE? \nTable 4: It would be great to include the context of how much runtime a numerical solver needs to simulate Navier-Stokes. Whichever outcome (in terms of \"who's faster\") is fine, but the context would help assess the efficiency of the active-learning methods. If the solver is slow, the context would underline the statement in line 034 that states how costly numerical solvers can be.\n\n\nLess important:\n\n- Line 053, \"we argue that querying all the states (...) is not cost-efficient\": does this sentence perhaps require more nuances? While sparse subsets of trajectories decrease the complexity (on paper), the suggested procedure for finding them is sufficiently expensive that the proposed algorithm is more costly than full-batch versions (Section 3.3, Table 4). \n- Line 095: Perhaps Brandstetter et al. (2022b) are not the best reference for spatiotemporal PDEs. Personally, I don't think this sentence needs any reference, but if one should be used, perhaps something like Evans' \"Partial Differential Equations\" book would be more appropriate.\n- Line 100: This statement about existence would benefit from a reference.\n- Line 112: What does \"primary focus\" mean here? It seems to be the _only_ focus. Have I missed something?\n- Line 128: Why does this sentence introduce a distribution of initial conditions, but the rest of the manuscript (for example, Algorithm 1) operates on a pool of initial conditions? I understand that the pool is sampled from the condition, but it might be more reader-friendly to use a pool of conditions throughout.\n- Line 262: How do the results depend on this choice of $T$ and $\\epsilon$?\n- Line 298: The abbreviation \"QbC\" is used early in the paper (for instance, in line 117 or 204). Maybe it would be good to introduce it before line 298."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Design-of-experiments for training surrogate models of time series. If simulated \"experiments\" are expensive, then it might be cheaper to train the surrogates sparsely. More specifically, a committee of surrogates is trained to estimate the uncertainty, and then value of new samples, which are then actually sampled and used to train the model."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea seems natural; In a sense a lot of machine learning is simply working out which data points need to be observed to train the correct model. In operator learning in particular we suspect much of the effort in training neural operators is wasted. \n\nThis paper is nearly simple, which is IMO great."
            },
            "weaknesses": {
                "value": "Given that sparse adaptive sampling from simulators is such a natural idea, the authors' solution \"feels\" surprisingly inelgant contrived. Sampling is strictly by masking over a fixed-timestep simulation pattern; to my mind it would be more satisfying to search for useful initial conditions from the traiing distribution, or to simply give up on rollouts when a given trajectory was no longer adding useful value, or to learn a predictor which could be conditioned on a rollout time, and we might imagine some scheme more elegant than this Bernoulli masking. However, this is not a blocker. Maybe this paper outlines the best progress than can be made at the moment? Maybe the best solution is not that elegant. If so, no problem.\n\nA bigger problem for me is that I have a hard time understanding how the rollout accounts for serial dependencies in the training data. I have done my best to try to understand it, but I suspect that this might be a deficiency in the authors' explanation itself. I would be prepared to revise my recommendation if they could make this part clearer. See Questions below for a lengthier series of questions on that theme. Without clarification on that I cannot really assess the later part of the paper.\n\n`FlexAL` is not IMO a great name; this is not particularly flexible compare to other active learning/DOE methods."
            },
            "questions": {
                "value": "I'm having a hard time understanding how the causal dependencies in a simulator are accounted for in this method. Can you lay it out for me? (\"Explain it like I'm 5\")\nAFAICT each successive step in the simulator depends on the previous ones. Figure 6 seems to support this, as does text l283 \"in our setting, we cannot directly acquire a time point, because there is a cost in the simulation of the trajectory.\").\n\nReasoning this through and attempting to crosscheck with the text, I haven't been able to work it out.\nSo if I \"mask out\" timestep $t$ but I wish to train on timestep $t+1$,  don't I still need to simulate at timestep $t$? How do I save the cost of the $t$th step at full fidelity? I get that we can roll out a surrogate or a solver, and that we can choose which to use, but when we want the actual \"ground truth\" solver, how do we get that? Does our actual roll-out incorporate mixtures of models, e.g. in a given training run we might have some \"mixed\" rollout like $\\hat{u}^2=G \\circ \\hat{G}  u^0$? (here $\\circ$ is composition) OK, that seems fine, but at a later stage of my training I would get a different output from $\\hat{G}$, so my training data would change and then I would need to recalculate $\\hat{u}^2$, right? This should introduce an asymmetry between the acquisition functions early in the time series, and later, since the later ones are likely to be sample from some other distribution than the training distribution (e.g. they may have failed to conserve mass or momentum or whatever, when the surrogate roll-out was used)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a method of active learning for PDEs. When training a neural operator for PDE, we often generate data using solvers. Solvers can be expensive, and now the question is how we can develop an active learning method to efficiently sample from a solver, in particular for temporal PDEs. \n\nThis paper proposes to train an ensemble of models and use their disagreement as a sign to collect samples."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper tackles an important problem in practice."
            },
            "weaknesses": {
                "value": "There is a set of weaknesses in this work that the authors are encouraged to address. \n\n1- The novelty. The method is very similar to prior works, with similar equitation function, ensemble approach, and philosophy. \n\n2- One might be able to say that not much about PDE-ness is present in this approach. Similar approaches are also in the game engine, video simulation, and 3D scene settings. Nothing about PDEs is exploited here, nothing I can see it is about PDE-ness, so the approach is not dedicatedly designed for PDEs. So what is particular about this method which is related to PDE-ness of the problem?\n\n3- Baseline active learning for simulation, video, and 3D scenes would be relevant. \n\n4- The datasets are quite simple for this paper.\n\nIf the authors tried this approach on a challenging task, then the first 3 points, as well as the next point, could be ignored. \n\n5- The method is quite ad-hoc and heuristic. What if all the models wrongly agree on many inputs and time steps? How can one guarantee this method doesn't collapse or capture the right \"variance\" and \"uncertainty\"? Just relying that this method, hopefully, does not collapse, is not sufficient. Again, this point could be ignored if we actually could tackle a challenging problem."
            },
            "questions": {
                "value": "minor, in Eq3, we often after expected value not the empirical loss."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}