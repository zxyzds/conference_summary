{
    "id": "DlqRpj68xe",
    "title": "From Reward Shaping to Q-Shaping: Achieving Unbiased Learning with LLM-Guided Knowledge",
    "abstract": "Q-shaping is an extension of Q-value initialization and serves as an alternative to reward shaping for incorporating domain knowledge to accelerate agent training, thereby improving sample efficiency by directly shaping Q-values. This approach is both general and robust across diverse tasks, allowing for immediate impact assessment while guaranteeing optimality. We evaluated Q-shaping across 20 different environments using a large language model (LLM) as the heuristic provider. The results demonstrate that Q-shaping significantly enhances sample efficiency, achieving a \\textbf{16.87\\%} improvement over the best baseline in each environment and a \\textbf{253.80\\%} improvement compared to LLM-based reward shaping methods. These findings establish Q-shaping as a superior and unbiased alternative to conventional reward shaping in reinforcement learning.",
    "keywords": [
        "reward shaping",
        "reinforcement learning",
        "large language model"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "Q-shaping is an extension of Q-value initialization and serves as an alternative to reward shaping for incorporating domain knowledge to accelerate agent training.",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=DlqRpj68xe",
    "pdf_link": "https://openreview.net/pdf?id=DlqRpj68xe",
    "comments": [
        {
            "comment": {
                "value": "Thank you for your careful reading of our paper and for pointing out the notation issues. We greatly appreciate your attention to detail, as it has helped us improve the clarity and consistency of our work. We have made the necessary corrections to the notations and ensured that the manuscript is now more accurate and easier to follow.\n\n**Q1:** It is currently unclear how this method fundamentally differs from Q-value initialization.\n\n**R1** Compared to recent work [1] that utilizes Q-value initialization to enhance online learning, it requires an accurate estimation of Q-values, whereas our work enhances online learning through imprecise estimation. Additionally, policy shaping is introduced to align the policy's behavior with the LLM\u2019s output, which accelerates the training process.\n\n[1] Nakamoto, Mitsuhiko, et al. \"Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning.\" Advances in Neural Information Processing Systems 36 (2024).\n\n**Q2:** what does $A^\\pi$ mean?\n\n\n**R2** $A^\\pi$ refers to \u201cactivity matrix\u201d,encoding $\\pi$'s state-conditional state-action distribution.\n$$A^\\pi(s, \\langle \\hat{s}, a \\rangle) = \\pi(a | s)  \\; \\text{if } s = \\hat{s}$$\n$$A^\\pi(s, \\langle \\hat{s}, a \\rangle) =  0  \\\\ \\text{otherwise} $$\n\n**Q3:** In line 181 the q function update formula is probably wrong?\n\n**R3** Theorem 1 and its proof is incorrect, and we apologize for misleading the reviewers here. \n \nTheoretically, Theorem 1 provides two conclusions:\n1. The new update formula is a contraction operator. Therefore, applying the heuristic TD update will lead to convergence. \n2.During the update process, the heuristic term should stop to ensure that optimality remains unchanged. \n\n\nTo derive Theorem 1, we first need to make two assumptions:\n\n1. The heuristic $h(s, a)$ provided by the large model does not change with the iteration count $k$.\n2. The heuristic $h(s, a)$ terminates at some iteration before convergence.\n\nOur Q-function update formula is:\n\n$$\n\\hat{Q}^{k+1}(s,a) = (1-\\alpha) \\hat{Q}^{k}(s,a) + \\alpha \\left( r(s,a) + \\gamma \\sum_{s' \\in S} P(s'|s,a) \\max_a \\hat{Q}^k(s',a) \\right) + \\mathbf{h}(s,a)\n$$\n\nWe define a new operator $\\mathcal{T}_h$ based on this:\n\n$\\hat{Q}^{k+1}(s,a) = \\mathcal{T}_h \\hat{Q}^{k}(s,a)$\n\n$= r(s,a) + \\gamma \\sum_{s' \\in S} P(s'|s,a) \\max_a \\hat{Q}^k(s',a) + \\mathbf{h}(s,a)$\n\nWe then prove that the operator $\\mathcal{T}_h$ still satisfies the **contraction property** in the appendix. Therefore, there exists a unique optimal fixed point $\\hat{Q}^*$. This proof allows us to apply our update formula to this new operator and find a new optimal fixed point.\n\nThe $\\hat{Q}^*$ is shifted and biased. Therefore, to allow $\\hat{Q}$ to converge to the optimal value function of the MDP $\\mathcal{D}$, we need to stop the heuristic $h(s, a)$ and let the value function update for a few steps towards $Q^*$.\n\nWe now pose the following question: How many steps in advance should we stop the heuristic function $h$ so that $\\hat{q}_D$ converges to $q^*_D$? Theorem 2 provides an upper bound for any random bounded $q$-values converging to $q^*$ in MDP $\\mathcal{D}$.\n\n\n**Q4:** what does $D_g$ mean?\n\n**R4** $D_g$ is typo, and it should be $D_{LLM}$, thank you for pointing out."
            }
        },
        {
            "comment": {
                "value": "**Q4:** How is the state designed? Can this method be scaled to image settings or real-world settings?\n\n**R4** The state design for the large model follows the requirements of the environment description. In our experiments, we tested the proposed Q-shaping method on three environments: MetaWorld, MuJoCo, and PyFlyt. For each of these environments, the state representation follows the default settings provided by the respective simulators.\nHere is an example of a state design for the door-closing task:\n\n```\ndef good_Q(self, batch_size):\n    actions = []\n    states = []\n    q_targets = []\n    for _ in range(batch_size):\n        # Generate a state where the end-effector is approaching the handle\n        handle_pos = np.random.uniform(0.1, 0.2, size=3)  # Approximate handle position\n        # Start end-effector at a position slightly away from the handle\n        end_effector_pos = handle_pos + np.random.uniform(-0.1, 0.1, size=3)\n\n        # Construct the observation\n        obs = np.zeros(self.obs_dim)\n        obs[self.end_effector_pos_idx] = end_effector_pos  # End-effector position\n        obs[self.handle_pos_idx] = handle_pos  # Handle position\n        obs[self.obs_dim - 3:] = np.array([0.2, 0.8, 0.15])  # Goal position\n```\n\nIn this example, the state is designed to represent the door-opening condition in a robotic manipulation task, where the LLM provides actions that lead to success.\n\nTo implement Q-shaping for online visual reinforcement learning, we have two potential plans:\n\n+ Plan 1: We need a (image + text) to (image + text) large model that can take an example state as input and output good s,a pairs.\n\n+ Plan 2: We can first allow the agent to explore some s,a pairs. Then, we apply a VQA (Visual Question Answering) model to analyze each state, provide good and bad actions, and form good and bad s,a pairs. We can then assign relative Q-values according to the model\u2019s confidence.\n\n**Q5:** The high-performance phase is not included in the calculation of sample steps.\n\n\n**R5** In the validation phase, the steps spent on high-performance selection are also included in the evaluation. Typically, SAC and TD3 allocate 25,000 steps for random exploration, PPO is set to 5,000 steps, and Q-shaping uses 5,000 steps for random exploration and 10,000 steps for filtering out low-performance agents. In the specific experiments, we did not exclude these steps in order to implicitly boost Q-shaping\u2019s performance. Furthermore, even if the 15,000 steps were excluded, the avg improvement in Q-shaping\u2019s performance would not be significantly affected.\n\n\n**Q6:** How is the correctness of the Q-value evaluated?\n\n**R6** The evaluation of the correctness of the Q-values from the LLM refers to determining whether the large model assigns a negative or zero value to good s,a pairs, or assigns a positive value to bad s,a pairs.\n\n\n\n**Q7:**  How to improve the output of the LLM and how many times of evolution are needed to get a good agent?\n\n**R7** LLMs often require 1 to 3 evolutions. From Experiment 3, we observe that the large model is capable of understanding the environment and outputting code that meets the standards. However, the action-state guidelines provided by the model may result in different performances. The optimization of these output is measured by the total return of the shaped agent. \n\nReward shaping methods, such as T2R or Eureka, typically require half a training cycle or a full training cycle (1e7 steps) to validate the impact of reward heuristics. On the other hand, the Q-shaping algorithm can immediately validate the performance of the large model's heuristic function.\n\n\n\n**Improvements in the Next Version:**\n\n1.Complete the ablation study.\n\n2.Add experiments on the impact of the number of LLM prompts on the agent's learning efficiency.\n\n3.Include a complete tutorial on how to use prompts in the appendix.\n\n4.Re-conduct the experiment on Eureka and provide details about its re-implementation."
            }
        },
        {
            "comment": {
                "value": "Thank you for your thoughtful and constructive feedback. The comments provided have been invaluable in helping us improve the clarity and thoroughness of our work. In this response, we address each of the concerns raised, with a focus on providing additional details, clarifications, and improvements to our experiments.\n\n**Q1:** In the 20 tasks, 6-7 tasks performed worse than the best baseline. Although there could be various reasons (such as the underlying RL algorithm, poor LLM output, randomness due to using only one seed, etc.), it is difficult to validate the generalization ability of the method.\n\n**R1** In the sample efficiency experiment, we also added a comparison with TD3. By incorporating the LLM heuristic module, the agent\u2019s performance improved by an average of 55\\%. This clearly demonstrates that the Q-shaping module can significantly enhance the sample efficiency of the underlying RL algorithm. In the 6-7 tasks where performance was worse than the best baseline, the action space complexity exceeded the LLM\u2019s understanding capability, making it difficult for the LLM module to provide accurate heuristic s,a pairs, and therefore, it could not compare with the best baseline.\n\n\n**Q2:** The paper does not introduce the concept of seeds.\n\n**R2** Thank you for your reminder. The paper does not discuss the seed because this work completely removes the use of seeds. Introducing a seed would reduce the complexity of the environment. The function **env.reset()** resets the environment and provides a random initial state, while **env.reset(seed=0)** would fix the initial state. This would mean the agent starts learning from a fixed initial state each time it is reset, which actually **reduces the learning complexity.**\n\nIn the 20 tasks, some tasks have fluctuating learning curves, which are mainly related to the **design of the reward function**. For example, in the \"ball_in_cup\" task, if the agent manages to throw the ball into the cup on the first attempt, it will receive a total reward of 15,000. However, if it fails on the first attempt, the total score will be much lower. This leads to fluctuations in the learning curve. However, in most environments, the agent\u2019s performance is relatively stable. Furthermore, the improvement brought by the LLM is also related to the complexity of the environment. The easier the environment is to understand, the more significant the improvement for the agent.\n\n\n**Q3:** Eureka and Text2Reward are not fairly treated.\n\n**R3** Thank you for your reminder. Text2Reward was validated using MetaWorld, and the designed reward functions were provided in the code repository. Therefore, we could directly use the GitHub code for verification. The only modification we made to the T2R code was to verify optimality every 5000 steps. As a result, the training of T2R is fair.\n\nRegarding Eureka, it was validated in the Isaac environment, and it designed many prompts. To transfer from the Isaac prompt to the MetaWorld prompt, some adjustments to the prompt are necessary.\nI believe that Eureka performed poorly for three main reasons:\n\n1.The Eureka prompt focuses on reward scaling and reward normalization, which may bias the learning process during early iterations.\n\n2.Eureka uses PPO, and in our experiments, we observed that PPO significantly lagged behind algorithms designed for continuous action spaces, such as SAC and TD3.\n\n3.The reward heuristic provided by Eureka requires genetic algorithm iterations before any performance improvement is seen.\n\nAdditionally, lines 67-73 of the paper explain that Q-shaping has a very fast verification cycle, allowing us to directly validate the impact of the algorithm. In contrast, Eureka needs to wait until halfway through training to obtain the fitness value. Assuming max_iter = 10^7, the verification cycle for Eureka is 5x10^6, which is 5x10^6 times longer than ours."
            }
        },
        {
            "comment": {
                "value": "**Q5** Lines 32-36 require further citations\n\n**R5** To improve efficiency, popular methods include (1) imitation learning, (2) residual reinforcement\nlearning, (3) reward shaping, and (4) Q-value initialization. Yet, each has limitations: imitation\nlearning requires expert data[3-5], residual RL needs a well-designed controller[1-2], and Q-value initialization[8]\ndemands precise estimates. Therefore, reward shaping[6-7] is the most practical approach, as it avoids the\nneed for expert trajectories or predefined controllers.\n\n[1] Johannink, Tobias, et al. \"Residual reinforcement learning for robot control.\" 2019 international conference on robotics and automation (ICRA). IEEE, 2019.\n\n[2] Trumpp, Raphael, Denis Hoornaert, and Marco Caccamo. \"Residual policy learning for vehicle control of autonomous racing cars.\" 2023 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2023.\n\n[3] Garg, Divyansh, et al. \"Iq-learn: Inverse soft-q learning for imitation.\" Advances in Neural Information Processing Systems 34 (2021): 4028-4039.\n\n[4]Chang, Jonathan D., et al. \"Adversarial Imitation Learning via Boosting.\" arXiv preprint arXiv:2404.08513 (2024).\n\n[5] Kostrikov, Ilya, Ofir Nachum, and Jonathan Tompson. \"Imitation Learning via Off-Policy Distribution Matching.\" Proceedings of the 8th International Conference on Learning Representations (ICLR 2020)\n\n[6] Xie, Tianbao, et al. \"Text2Reward: Reward Shaping with Language Models for Reinforcement Learning.\" The Twelfth International Conference on Learning Representations. \n\n[7] Ma, Yecheng Jason, et al. \"Eureka: Human-Level Reward Design via Coding Large Language Models.\" The Twelfth International Conference on Learning Representations.\n\n[8] Nakamoto, Mitsuhiko, et al. \"Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning.\" Advances in Neural Information Processing Systems 36 (2024).\n\n\n\n\n\n**Q6**: The concept of NPBRS needs clarification.\n\n**R6**\nIn [1], Ng introduced the concept of Reward Shaping and defined PBRS (Policy-Reward Shaping), where additional rewards are provided based on the potential function, ensuring that optimality remains unchanged. NPBRS refers to reward shaping methods that do not follow the potential function rule, and the learned policy does not guarantee optimality.\n\n[1] Ng, Andrew Y., Daishi Harada, and Stuart Russell. \"Policy invariance under reward transformations: Theory and application to reward shaping.\" ICML. Vol. 99. 1999.\n\n**In the new version of the paper, we will make the following revisions:**\n\nProvide a more detailed description of LLM-based RL algorithms.\nAdd more citations in the introduction to ensure objectivity.\nIn the related work section, we will cite LLM value-based methods to increase richness.\nFigures 4 and 6 will be further updated to prevent ambiguity.\nAn appendix will be added with a complete prompt for the large-model reinforcement learning case study."
            }
        },
        {
            "comment": {
                "value": "Thank you for your thoughtful comments and suggestions! Theorem 1 was incorrectly expressed, so we rewrote it to make it easier to understand and clearly express its purpose. If you still have questions, please let us know. Your suggestions are very valuable to us.\n\n**Q1:** The article needs to clearly explain why Q-shaping differs from recent LLM-based methods:\n\n\n**R1:** Q-shaping accelerates learning by directly modifying the Q-values. The advantages of Q-shaping are mainly twofold: Its effect on the agent can be immediately reflected in the next episode, leading to a quick validation cycle. In contrast, reward shaping methods need to wait until the end of the training to validate the performance of the reward shaping.\nQ-shaping remains optimal, whereas recent LLM-based reward shaping methods are not only difficult to design reward heuristics for, but also do not guarantee optimality.\n\n**Q2:** Regarding Theorem 1\n\n**R2** Theorem 1 and its proof is incorrect, and we apologize for misleading the reviewers here.  \nTheoretically, Theorem 1 provides two conclusions:\n1. The new update formula is a contraction operator. Therefore, applying the TD update will lead to convergence. \n2.During the update process, the heuristic term should stop to ensure that optimality remains unchanged. \n\n\nTo derive Theorem 1, we first need to make two assumptions:\n\n1. The heuristic $h(s, a)$ provided by the large model does not change with the iteration count $k$.\n2. The heuristic $h(s, a)$ terminates at some iteration before convergence.\n\nOur Q-function update formula is:\n\n$$\n\\hat{Q}^{k+1}(s,a) = (1-\\alpha) \\hat{Q}^{k}(s,a) + \\alpha \\left( r(s,a) + \\gamma \\sum_{s' \\in S} P(s'|s,a) \\max_a \\hat{Q}^k(s',a) \\right) + \\mathbf{h}(s,a)\n$$\n\nWe define a new operator $\\mathcal{T}_h$ based on this:\n\n$\\hat{Q}^{k+1}(s,a) = \\mathcal{T}_h \\hat{Q}^{k}(s,a)$\n\n$= r(s,a) + \\gamma \\sum_{s' \\in S} P(s'|s,a) \\max_a \\hat{Q}^k(s',a) + \\mathbf{h}(s,a)$\n\nWe then prove that the operator $\\mathcal{T}_h$ still satisfies the **contraction property** in the appendix. Therefore, there exists a unique optimal fixed point $\\hat{Q}^*$. This proof allows us to apply our update formula to this new operator and find a new optimal fixed point.\n\nThe $\\hat{Q}^*$ is shifted and biased. Therefore, to allow $\\hat{Q}$ to converge to the optimal value function of the MDP $\\mathcal{D}$, we need to stop the heuristic $h(s, a)$ and let the value function update for a few steps towards $Q^*$.\n\nWe now pose the following question: How many steps in advance should we stop the heuristic function $h$ so that $\\hat{q}_D$ converges to $q^*_D$? Theorem 2 provides an upper bound for any random bounded $q$-values converging to $q^*$ in MDP $\\mathcal{D}$.\n\n\n\n\n**Q3** Regarding Theorem 2:\n\n**R3** Regarding sample complexity analysis, many previous works have already provided different upper bounds for sample complexity, such as VI-LCB [1], PEVI-Adv[1], and Q-LCB [2].  \nThese works require the design of heuristics to derive a lower upper bound.  \nThey all derive tighter upper bounds by providing more refined heuristics. However, these works have some drawbacks:\n\n(1)These works that provide tighter convergence algorithms have not conducted experiments to verify whether their algorithms are effective.\n\n(2)The reference policy used to obtain the upper bound must satisfy ``single-policy concentrability,'' which limits their applicability.\n\nAs discussed in Theorem 1,the heuristic needs to stop before convergence. The goal of Theorem 2 is to provide experimenters with a reference for when to stop the heuristic, rather than comparing sample complexity with previous works.\n\nReferences:  \n\n[1] Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y. (2021b). Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. arXiv preprint arXiv:2106.04895.\n\n[2] Shi, Laixi, et al. ``Pessimistic q-learning for offline reinforcement learning: Towards optimal sample complexity.'' International Conference on Machine Learning. PMLR, 2022.\n\n**Q4** Is Q-shaping related to regularization algorithms?\n\n**R4** Regularization terms refer to those terms related to the values or quantities of model parameters, such as L1 regularization or L2 regularization. Their main purpose is to constrain the overfitting problem of the model. In this work, the main purpose of the heuristic term is to shift the Q-function so that the agent can perform actions that are of interest to the large model."
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel framework called \"Q-shaping,\" which enhances Q-value initialization by integrating domain knowledge to accelerate training in reinforcement learning (RL). Unlike traditional reward shaping methods, Q-shaping modifies Q-values directly, thereby improving sample efficiency without sacrificing the agent's optimality upon convergence. The experimental results indicate significant performance improvements."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. **Innovative Approach:** Q-shaping presents a fresh perspective on incorporating domain knowledge into RL, overcoming the limitations associated with reward shaping.\n2. **Empirical Results:** The paper includes comprehensive experimental evaluations demonstrating Q-shaping's effectiveness, with a 16.87% improvement in sample efficiency over the best baseline and a remarkable 253.80% enhancement compared to LLM-based reward shaping methods.\n3. **LLM Utilization:** The paper effectively harnesses large language models to guide agent exploration, revealing new potentials for LLMs in RL applications."
            },
            "weaknesses": {
                "value": "The current version lacks sufficient proof of completeness in both theoretical and experimental aspects. If the authors can convincingly address these issues, I would be open to reevaluating my score."
            },
            "questions": {
                "value": "1. **Comparison to Existing Works:** It\u2019s important to clarify why the challenges of reward shaping cannot be addressed by recent LLM-based methods (e.g., Eureka, text2reward). How does your work differ from these studies? It appears your approach utilizes LLMs to design regularization for RL.\n   \n2. **Proof of Theorem 1:** The proof seems unconventional; while you provide an update formula for the \\(\\hat{Q}\\) iteration, you immediately reference the Bellman optimal operator to support your theorem. Early works have established the convergence of the Bellman operator, so how can you demonstrate that your update formula aligns with it? This appears to assume the conclusion as a basis for your argument.\n   \n3. **Clarification on Theorem 2:** Theorem 2 establishes a lower bound rather than an upper bound. What is the convergence sample complexity relative to other works? Is your bound more favorable than existing results, and do other studies not provide established bounds?\n   \n4. **Relation to Regularization Techniques:** A deeper explanation of how your work relates to reinforcement learning methods employing regularization techniques would be beneficial. The core of your approach seems to hinge on introducing LLMs for regularization in RL.\n   \n5. **Experimental Settings:** The experimental setup raises some questions. You utilize GPT-4o as the LLM and TD3 as the RL backbone in your LLM-TD3 method. Which LLM do Eureka and text2reward utilize (notably, Eureka uses GPT-4 and GPT-3.5, while text2reward uses GPT-4)? Is GPT-4o also used for these works, and do they employ TD3 as the RL backbone?\n\n**Minor Issues:**\n1. In lines 32-36, the literature review on current RL works aimed at enhancing training efficiency lacks citations, which detracts from its objectivity.\n2. The origin of the concept of NPBRS (non-potential based reward shaping) in line 53 is unclear and needs clarification.\n3. A few LLM-assisted RL studies have focused on Q-function or value function design (e.g., \u201cHow Can LLM Guide RL? A Value-Based Approach\u201d). An analysis of these works should be included in the related works section.\n4. Figures 4 and 6 do not specify the units for steps (presumably in millions).\n5. The prompt example in the Appendix is too brief. A more comprehensive example, including the output Q function and policy function, would greatly enhance reader understanding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents Q-shaping, a framework to accelerate training of reinforcement learning agents by using LLMs to produce domain-knowledge based heuristic functions for initializing the Q-function and policy. Specifically, LLMs produce code to categorize good and bad state-action pairs in the environment. Before the start of training, these pairs are used to update the Q-network and policy, thus leading to better network initializations. Results across 20 environments show that Q-shaping can significantly improve sample efficiency and outperform LLM-guided reward-shaping methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The method is intuitive and simple to understand. The domain knowledge of LLMs is used to find good initializations of the Q-function. This can be generally useful for multiple RL tasks if structured information can be effectively elicited from LLMs.\n2. The paper has an appropriate number of citations and properly details existing work in the related work section. \n3. Although multiple works have considered using the domain knowledge of LLMs for improving RL, this work introduces another novel way to harness that expertise."
            },
            "weaknesses": {
                "value": "1) **Writing**: The overall writing is lacking and can be significantly improved. The style of writing is currently informal and often lacking important experimental details. For example, the evaluation criteria are not properly explained, some experimental details are not clear. The overall flow of the paper is also not smooth.\n2) **Result Discussion**: The discussion of the results is very limited. The ablations conducted are only discussed superficially. For an empirical paper, only 1 page dedicated to discussion of results is too less. I personally feel that more discussion is needed in the experiments section, and some of the theory and notation introduced is not critical to the paper and can be deferred to the appendix.\n3) **Significance of Results**: In 6-7 out of the 20 tasks, the presented method is worse than the best-performing baseline. While there are multiple potential causes of this (base RL algorithm, bad LLM outputs, randomness if only 1 seed is uses, etc), it is difficult to validate the generalization capability of the method."
            },
            "questions": {
                "value": "1. What are the number of seeds used? The curves oscillate a lot and it is difficult to draw conclusions from many of the plots. \n\n2. I am not convinced by the implementation of the Eureka and text2reward baselines. In 3 out of the 4 plots, both these baselines stay completely flat and do not improve at all. This is strange as Eureka was shown to perform well on a variety of robotic tasks. The tasks selected in this paper do not seem very different, and I am curious why these baselines are so bad. Setting the evolution round to 1 might be partially responsible for this but makes it unfair for the baseline. \n\n3. What is the state for the environments considered? There is no information provided on this and I do not see how this method will generalize when the states are images. Similarly, when doing RL on real robots, then clean environment code as assumed by this work will not be available. It will be useful to get an idea about the assumptions that this work makes.\n\n4. It will be helpful add the individual impacts of Q-shaping and policy-shaping in the ablation study on different training phases. Currently, it is unclear what the contributions of these two techniques are to the final performance of the method. \n\n5. I do not understand the significance of the sample efficiency results. Sample efficiency improves by an average of 17% compared to baselines. However, the presented framework also has a high-performance selection phase which is not a part of the baselines. As multiple agents are rolled out for a significant number of timesteps, a fairer comparison would be to add these timesteps into the sample efficiency calculations. \n\n6. How are the heuristic functions output by LLMs evaluated? For example, one of the evaluation criteria is correctness of assigned Q-values. How is this actually measured?\n\n7. How many times is a LLM prompted per task? If it prompted multiple times, how are they filtered? \n\n8. I think it is also important to release the entire prompts that are used for the LLMs as there could be a lot of domain knowledge provided in the task descriptions themselves. As the environment task descriptions are currently not provided in the paper, it is difficult to understand the contribution of the LLM."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors proposed a method called Q-shaping to enhance the sample efficiency of reinforcement learning algorithms. The main idea is to prompt a LLM to generate samples of good and bad state-action pairs and heuristic Q value estimates. These samples are used to train the initial Q function before turning to the standard RL pipeline. Experiments were conducted on a variety of continuous control environments showing significant improvement in sample efficiency in some environments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea is original to my knowledge and the experiments are well executed."
            },
            "weaknesses": {
                "value": "* The presentation of the idea is somewhat long winded and the notations are somewhat inconsistent as I point out in the questions.\n* It is not clear how the method is fundamentally different from Q value initialization."
            },
            "questions": {
                "value": "* Line 155, what does the $A^{\\pi}$ symbol represent? Is it the policy improvement operator? I couldn't find any explanation in the text.\n* Line 181, are the authors missing a $(1 - \\alpha)$ coefficient and brackets in the Q function update rule? The equation seems inconsistent with the update equation on line 744 in appendix B.2.\n* I am not too sure how Theorem 1 actually shows the contraction property of the shaped Q iteration and how it differs from the contraction property of the regular Bellman operator. Line 757 in the proof section appears to say that the optimality of the shaped Q iteration is only guaranteed if the addition of heuristic values is stopped. \n* In eq 1, that is $D_{g}$? Is it $D_{LLM} = \\{G_{LLM}, B_{LLM} \\}$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}