{
    "id": "aYYZBPoSHb",
    "title": "Multi-Objective Alignment of LLMs with ORPO using Self-Judgement",
    "abstract": "The alignment of Large Language Models (LLMs) is achieved through fine-tuning with human preference data, where preference optimization has become a critical part of the process. Many methods have scaled LLM performance by incorporating self-judgement, highlighting the importance of unifying LLM-as-a-judge with the alignment process. One such method, called Self-rewarding LLMs, iteratively samples new data from the model to improve alignment using self-judgement. Since this additional data is generated by the LLM, we argue that similar improvements can be achieved without new data. We propose a method that reuses alignment data in the form of a self-judgement classification task and defines a multi-objective optimization problem. Our self-judgement task is derived from a simple transformation of the primary alignment data, asking the LLM to select the superior response. It introduces no new data beyond the existing alignment data. Thus, we claim the improvements are due to positive interference between the two tasks. We focus on a direct preference optimization method called Odds-Ratio Preference Optimization (ORPO). We conduct a thorough study of linear scalarization on two objectives and introduce two alternative approaches that vary the emphasis on alignment versus self-judgement objectives. Our results on Mistral 7B indicate a promising direction for fine-tuning LLMs on multiple objectives, particularly for improving performance on related tasks without additional natural language data.",
    "keywords": [
        "LLMs",
        "preference optimization",
        "self-judgement",
        "LLM-as-a-judge",
        "supervised fine-tuning",
        "multi-objective optimization",
        "multi-task learning",
        "supervised learning"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=aYYZBPoSHb",
    "pdf_link": "https://openreview.net/pdf?id=aYYZBPoSHb",
    "comments": [
        {
            "summary": {
                "value": "This work presents an approach of multi-objective alignment of LLMs for both answering and self-judging abilities. Specifically, the model is tuned with both aligning and self-judgment objectives, and various ways to combine the two objectives are investigated. The tuning itself does not require sampling new data, which makes the process data and computational efficiency. Experiments on Alpaca Eval show the effectivenss of the proposed method in some way."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The direction of self-rewarding is interesting, which makes the model being able to judge its own outputs and get progress.\n- The idea of reducing dependency on high-quality data is attractive."
            },
            "weaknesses": {
                "value": "- Most of the ideas in this work seem to be from previous works and this work seems to focus on a combination of various methods. This is fine if this work aims to be an empirical one, however, the experiments and analyses are lacking, and there seems to be little insights on which factors work and why.\n- There should be more baseline methods to prepare the proposed method, which is important to demonstrate certain choices of the proposed method."
            },
            "questions": {
                "value": "- I'm wonderin why specifically using ORPO? It seems that there are no direct comparisons to other methods such as DPO, more explanations and experimental results should be provided."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a method to fine-tune LLMs by reusing existing alignment data for self-judgment tasks, eliminating the need for self-sampling. This approach utilizes the ORPO method as the primary baseline for supervised fine-tuning. The training process is approached as a multi-objective optimization problem, with comparisons among scalarization, conjoint, and conditional optimization methods. Experiments are conducted on the Mistral 7B model, benchmarked against the Llama 3 70B model on the Alpaca Eval dataset. The results show mixed to slight improvements in alignment in contrast to the ORPO baseline fine-tuned solely on the alignment task."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The integration of ORPO with a multi-objective optimization framework to simultaneously address alignment and self-judgment tasks is innovative and promises efficiency improvements in LLM fine-tuning.\n- The introduction, related work and methodology sections are clearly written, providing a solid understanding of the theoretical background and operational framework. This helps in appreciating how the proposed method diverges from and potentially improves upon existing techniques."
            },
            "weaknesses": {
                "value": "- The experiment and results section reads a bit rushed and lacks examples that reflect the discussed issues such as verbosity and overfitting.  These could be included in an appendix for reference. \n- The findings are limited by the use of only one model and one evaluation dataset. Expanding the testing to include models of varying sizes could better determine the method's general applicability and robustness.\n- The identification of verbosity as a significant issue without an applicable solution reduces the potential utility of the proposed approach. More testing on prompt engineering or response format control could help mitigate the verbosity bias ( see questions for more details)."
            },
            "questions": {
                "value": "1. The paper tests the proposed method only on the Mistral 7B model and on one evaluation dataset. I understand that finetuning and evals can be costly, testing on other models within the 2B-13B size range could help determine if verbosity is inherent to the method itself or specific to Mistral 7B model?\n2. With verbosity identified as a major issue, have you tried prompt engineering that could effectively reduce the bias in model responses, such as asking the model to respond briefly or follow a specific response format?\n3.  Regarding the overfitting issue, would separating the alignment dataset into an alignment subset and another non-overlapping self-judgment subset mitigate this problem? \n4. Have there been tests where only self-judgment training was conducted (w1=0, w2=1). As a contrast to w1=1, w2=0? I understand that this would not fine-tune on alignment data, but only on model\u2019s reasoning why it chose A over B (or vice versa). \n5. It is not clear how a longer context length might be an issue. The example in Figure 1 shows minimal differences between the two cases, as the self-judgment dataset wraps a rather brief prompt around the rather lengthier alignment data.\n6. It is not clear why some training runs have 2 epochs while others have 3. \n7. By equation 8, which is missing, do you mean equation 7 for the gradient of Conjoined method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes multi-objective optimization of self-judged preference data and available alignment data. The self-judged data is collected on a dataset where the human annotated preference labels are already available. The authors propose that in addition to optimizing the model on annotated preference data we should also optimize on self-judged preference data. The authors use two multiobjective optimization algorithm on these two objectives. Additionally they use one algorithm called CONJOINED MULTI-OBJECTIVE LOSS where they optimize against the self-judgement objective. The experiment result does not show any significant gains compared to the baselines."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors clearly explain their approach and it's connection to previous approaches. The novel contributions have been clearly explained.\n2. For self-judgment data collection the authors have addressed the issue of various types of biases. To mitigate the biases they have created about 50 prompts to get the correct judgement as per the model."
            },
            "weaknesses": {
                "value": "1. On Figure 3 (right) the Pareto frontier is visualized for different scalar weights. The solutions be smoothly moving on the multiobjective-loss space as the weights are changed. However, the solutions are all over the place. This indicates that the optimization have not been performed well or the random effects (of batch order etc) are too high. This puts into question the validity of the results from the first set of results. I would suggest authors perform thorough optimization through longer training. Otherwise, the authors should perform multiple runs with the same weight and show the averaged results. \n2. The proposed approach relies on annotated alignment data. This data already has the gold annotations. Using a small LLM (Mistral-7B) to annotate this again (ie self-judged data) and using this annotation as a separate ground truth does not make sense. The model may just be wrong on it's self-judgement. Unless this self-judgement annotations are collected on a large scale in an unsupervised fashion (as done on the paper Self-Rewarding Language Models), we cannot rely on this self-judgement annotations.\n3. The authors have not clearly explained why the two objectives (performance on alignment data and performance on self-judged data)  may be contrary to each other. One way to show that would be to provide the loss for self-judged data for the baseline model. If better performance on the baseline means better performance on self-judged data than the two objectives are always aligned and there is no point in doing multi-objective optimzation.\n4. For the CONJOINED MULTI-OBJECTIVE LOSS, the self-judgement objective is optimized against. This is a direct contradiction to the multi-objective optimization that is being argued by the authors."
            },
            "questions": {
                "value": "1. Can you explain why log-sigmoid is used in equation 3?\n2. Is there any numerical stability issues during training? As odds ratio are calculated it to require the probabilities which can be very small and cause numerical stability issues. Does it happen?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a new alignment method for LLMs that uses multi-objective optimization to combine (1) self-judgment and (2) odds-ratio preference optimization (ORPO; Hong et al. 2024).  This paper explores a few general techniques for performing multi-objective optimization for these two objectives.  They present some promising preliminary experiments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Exploring techniques for combining the strengths of various learning paradigms through multi-objective optimization is an interesting avenue to explore.  Similarly, looking at methods to exploit LLMs as judges is also interesting."
            },
            "weaknesses": {
                "value": "I found this paper hard to follow.  The clarity of the presentation and overall pitch of the paper needs to be improved.\n\nThe method's empirical validation is weak. I do not see a meaningful comparison to other methods.\n\nI do not see a good case for why the paper needs to combine specifically ORPO with LLMs-as-judge.  What about other competing alignment objectives?"
            },
            "questions": {
                "value": "What happens if you replace ORPD with a more popular objective like DPO?  Are you open to adding this comparison to the paper?\n\nMulti-objective optimization techniques should support more than two objectives.  Why did you limit the experiments to just two objectives? \n\n---\nSuggestions:\n\n - Please learn the difference between textual (`\\citet`) and parenthetical (`\\citep`) citations.\n\n - I suggest moving the long related work section to a later section of the paper or an appendix.  This will help refocus the paper in my opinion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a multi-objective alignment method based on ORPO. More specifically, the first objective is the same as the log odds alignment objective proposed in the ORPO paper, the authors propose to add a second objective where the LLM should predict which response is preferred by humans. They test their approach on Alpaca Eval and ablate different design decisions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The ablation study of the proposed method appears to be thorough. The authors systematically compare various strategies for combining the two objectives, including conditional, epoch-by-epoch, and task-by-task approaches, and report detailed results for each."
            },
            "weaknesses": {
                "value": "This paper would benefit from substantial improvements across several dimensions:\n\n**Presentation:** The framing of the paper is somewhat misleading. The primary contribution appears to be the proposal of combining two objectives and exploring various ways to integrate them. However, certain claims, such as the statement on line 97, position this work as an alternative to iterative fine-tuning. The intent of sampling from the current policy in iterative fine-tuning is generally to produce more on-policy data, which can help bridge the performance gap between offline and online methods. I would accept this framing if the paper presented a novel approach for obtaining on-policy data without direct sampling from the policy itself. As it stands, this framing seems to overstate the contribution.\n\n**Motivation:** The motivation lacks clarity. The authors suggest that because LLMs are often used to judge their own outputs, sampling from the model may not be necessary. This argument is unclear; it's possible that verification tasks are fundamentally easier than generation tasks\u2014an LLM may perform well as a judge but poorly as a generator, thereby effectively evaluating its own inputs and improving performance. Additionally, the paper does not seem to empirically test this hypothesis. If the aim is to demonstrate that the proposed approach can replace iterative fine-tuning, then a direct comparison with iterative fine-tuning methods is essential, showing that similar or better performance can be achieved with comparable or less data.\n\n**Experiments:** The experimental design has some gaps. Key baselines, such as the performance of the supervised fine-tuning (SFT) model or the pretrained model, are missing. To evaluate the effectiveness of the proposed method, comparisons with established alignment methods like Direct Preference Optimization (DPO), Proximal Policy Optimization (PPO), and other conventional techniques are necessary."
            },
            "questions": {
                "value": "- What is the y-axis in Figure 3?\n- Line 283 refers to equation 8, but I don't see any equation 8\n- In equation 6, $\\theta'$ is said to be frozen weights of the self judgement task, it is the parameters of the same model, right? Also, in the same equation, shouldn't you use a different prompt for this task and the alignment task? (it seems that both tasks are using $x$)\n- Why is the second task called self judgment in the context of your work? It doesn't seem to be that you are generating the responses from the same LLM, so it is not really **self** judgement, is it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}