{
    "id": "cazOlqncU6",
    "title": "Trustworthy Dataset Proof: Certifying the Authentic Use of Dataset in Training Models for Enhanced Trust",
    "abstract": "In the realm of deep learning, the veracity and integrity of the training data are pivotal for constructing reliable and transparent models. This study introduces the concept of Trustworthy Dataset Proof (TDP), which tackles the significant challenge of verifying the authenticity of training data as declared by trainers. Existing dataset provenance methods, which primarily aim at ownership verification rather than trust enhancement, often face challenges with usability and integrity. For instance, excessive operational demands and the inability to effectively verify dataset authenticity hinder their practical application. To address these shortcomings, we propose a novel technique termed Data Probe, which diverges from traditional watermarking by utilizing subtle variations in model output distributions to confirm the presence of a specific and small subset of training data. This model-agnostic approach improves usability by minimizing the intervention during the training process and ensures dataset integrity via a mechanism that only permits probe detection when the entire claimed dataset is utilized in training. Our study conducts extensive evaluations to demonstrate the effectiveness of the proposed data-drobe-based TDP framework, marking a significant step toward achieving transparency and trustworthiness in the use of training data in deep learning.",
    "keywords": [
        "dataset integrity; trustworthy dataset proof; data probe; watermark"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cazOlqncU6",
    "pdf_link": "https://openreview.net/pdf?id=cazOlqncU6",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces the Trustworthy Dataset Proof (TDP) problem, which aims to verify the authenticity of training datasets used to train machine learning models. TDP provides a mechanism to certify that a claimed dataset was used fully, without additional tampering, during model training, thus enhancing trust and transparency. The paper proposes a novel \u201cData Probe\u201d method, which embeds subtle markers in the dataset and uses model output distribution to confirm dataset integrity with minimal intervention in the training process. The authors demonstrate TDP\u2019s performance on several vision datasets, showing that it can detect when training data is tampered with or incomplete."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "**Definition of a New Dataset Verification Problem**\n\nThe paper identifies and defines the novel problem of Trustworthy Dataset Proof (TDP), which seeks to verify the integrity of datasets used in model training. Unlike existing approaches focused on dataset ownership or provenance, TDP aims specifically to ensure that the declared dataset was used in its entirety.\n\n**Novel Approach to Address the Problem**\n\nThe authors propose a novel solution to the TDP problem by introducing the concept of \u201cData Probes.\u201d This approach marks subsets of the dataset in a way that allows verification through model outputs with minimal impact and modifications to the training process. This probe-based strategy leverages distributional output patterns to detect incomplete or tampered datasets, and only requires black-box access to the model.\n\n**Efficiency and Low Overhead**\n\nThe TDP framework is designed to be computationally efficient, requiring only minimal overhead compared to regular training. This is beneficial compared to approaches like Proof of Training Data, which incur significant overhead during training."
            },
            "weaknesses": {
                "value": "**Inconsistent Threat Model Assumptions**\n\nThe main limitation is that the threat model assumes a dishonest trainer (L161) who attempts to modify the dataset and avoid detection. At the same time, the threat model assumes that the same dishonest attacker voluntarily sticks to the exact verification protocol specified by the verifier (defender), which cannot be verified (L192).\n\nIn practical terms, this means it is easy for the attacker to bypass the proposed verification approach. Instead of computing the hash over the modified dataset D*, the attacker computes the hash over the unmodified dataset D, and thus always reaches the goal of succeeding the verification.\n\n**The Dataset has to be Public**\n\nThe dataset has to be entirely available to verify the trained model. Since the data contains a significant part of the value, the trainer may be reluctant to publish it. Licenses and intellectual property may also work against this.\n\n**Limited Evaluation**\n\nThe evaluation is limited to small image classification datasets: CIFAR-10, SVHN, CIFAR-100, and Tiny ImageNet 200. These are relatively similar, small-scale image datasets with few classes. The results would be much more convincing if the experiments showed generalization to at least medium-sized datasets such as ImageNet and other data modalities. Since the approach claims low overhead and requires only regular model training, there should be no reason not to run larger-scale experiments.\n\nSimilar things can be said about the models: they are all smaller-scale convolution models. It would be very interesting to see how larger-scale models behave and if the properties transfer to different architectures, e.g., transformers.\n\n**Inconsistencies wrt. Secret k**\n\nIt is unclear to me what the purpose of the secret key k is. According to L295, it is used to make the hash values unique between different \u201cusers.\u201d Who is the user here? I assume the attacker since they are generating the secret according to Algorithm 1. If the attacker is in control of k, how does it make anything more secure? This understanding is, however, inconsistent with L483 where the claim is that hiding the key from the user (presumably attacker?) can solve the effect of adaptive attacks.\n\n**No Code Available**\n\nNo code was available for review, and the authors did not specify whether it would be released upon publication."
            },
            "questions": {
                "value": "Why did the authors only consider small-scale vision datasets? It seems to me like there should not be significant barriers to train on larger-scale datasets such as ImageNet.\n\nDoes this approach also work on other data modalities?\n\nL294: What security risk does hash calculation pose? How exactly could a constant hash pose a security risk, how could it be exploited, and how do individual user keys, which are then revealed, solve these issues?\n\nL465: Why is the attacker knowing the key k a worst-case assumption? According to Figure 2 and Algorithm 1, the attacker generates k to compute the hash and, therefore, always has knowledge of it.\n\nL482: \u201cKeeping the user\u2019s key hidden from the users [\u2026] might be a solution\u201d. \n\n1. How can the user\u2019s key be hidden from the user itself?\n2. Who is the user here? The trainer or the verifier?\n3. And how does hiding the key solve the attack?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on a watermarking approach for certifying dataset use in model training. It targets the research gap of usability and integrity of proposed solutions to the Trustworthy Data Proof problem. The paper\u2019s goals include fidelity of the watermarking verification approach, low-invasiveness on the model training process, harmlessness (ie. minimizing the model\u2019s performance degradation), and efficiency of the verification process.\nThe authors propose Data Probe, an approach conceptually similar to a weakened backdoor, where the probe input samples behave like special members of the dataset. However, Data Probe requires only a slight difference in model prediction confidence between the probe versus non-probe inputs  in order for the dataset to be successfully verified, as opposed to backdoors which trigger a specific output. The success of their approach relies on coupling of the dataset integrity verification with the data probe selection strategy, utilizing the uniqueness of hash functions. In particular, a user-specific keyed-hash is performed on the complete.\nThe authors propose and analyse the performance of 4 different types of data probe, as well as 4 different possible saliency scoring methods. Experiments are carried out to answer 4 RQs -  whether various types of data probes can effectively verify the integrity of the dataset, how many probes need to be implanted during training, which of the different probe score calculation strategies are most effective for detection, and how robust the verification mechanism is against adaptive attacks. Experiments show promise for their proposed Data-Probe-based solution to the TDP problem."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "TDP is an interesting and underexplored problem, that is well motivated in the paper.\nOverall the paper is well written and reasonably easy to understand, and the threat model seems reasonable, with the defender only requiring black-box access to the model to be verified.\nOverall results look promising on a decent range of experiments, including adaptive attack."
            },
            "weaknesses": {
                "value": "* The main weakness is that overall, while results for adaptive attack and case studies look promising, the experiments are only using Cifar-10 \u2013 which has only 10 classes of relatively low-resolution images (32x32 pixels), and each analysis is only performed on one NN architecture. This is not comprehensive enough to establish that the performance of Data Probe generalizes to diverse datasets and model architectures.   \n* a missed related work: \u201cDeepTaster: Adversarial Perturbation-Based Fingerprinting to Identify Proprietary Dataset Use in Deep Neural Networks\u201d by Park et. al. (ACSAC \u201923). This work identifies dataset use in model training with no invasiveness or harm in the approach. It may also be possible to compare overall accuracy of Data Probe with their model.\n* It is not stated what scores are shown in gray in Table 4.   \n* In the adaptive attack experiment, the difference between the data D Vs D*  is not clear.   \n* In the case study \u2013 duplicating data to create extra data is not very compelling. It would be more interesting to see something like the SVHN dataset augmented with some MNIST data.   \n\nTypos/etc:   \n- in the abstract: \u201cdata-drobe-based\".  \n- Please check the definition of Conf(M,x) on page 7. What is the maximum taken over?\n- Table 4 caption: \u201csocres\u201d should be \u201cscores\u201d"
            },
            "questions": {
                "value": "To improve the paper, please address the following:   \n* Expand the adaptive attack and case study experiments to include all the datasets and NN architectures covered in the Table 2 results (SVHN, CIFAR-100, Tiny-Image-Net-200, ResNet, MobileNet, DenseNet, ShuffleNet).   \n* Add a clear explanation of what the gray scores represent in the Table 4 caption or in the accompanying text discussing Table 4. This would help improve the clarity of their results presentation.    \n* Add a discussion of \u201cDeepTaster\" by Park et. al. (ACSAC \u201923) to the related work and compare the fidelity of their approach (if appropriate).   \n* Provide a more detailed explanation of how D* differs from D in the adaptive attack scenario. Eg. include a specific example to clarify this important aspect of the experimental setup."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel framework called Trustworthy Dataset Proof (TDP), aimed at ensuring the authenticity and integrity of training datasets. Unlike traditional watermarking, Data Probe uses subtle variations in the output distributions of models trained on specific data, allowing for the detection of a small, representative subset of the training data. Also, this proposed approach is model-agnostic. Experimental results demonstrates the effectiveness of this approach, showing that the data probe-based TDP framework is both practical and reliable in verifying dataset integrity."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+) The topic of trustworthy proof in deep learning training is timely and interesting\n\n+) The paper is well organized and easy to follow\n\n+) The proposed method is technically reasonable and sound"
            },
            "weaknesses": {
                "value": "-) The motivation and threat model is not clear to me\n\n-) Some assumptions in the problem setting are not very reasonable\n\n-) There lacks a theoretical analysis (or proof) how well the proposed method will work"
            },
            "questions": {
                "value": "a) This paper addresses a gap in the field by introducing the novel problem of verifying the authentic usage of datasets in deep learning. The topic is interesting as the community pushes toward responsible AI practices amid growing concerns about data misuse and authenticity. By focusing on dataset integrity through Trustworthy Dataset Proof (TDP), the authors make a timely contribution to the field.\n\nb) My main concern is the motivation and threat model is not clear to me. The paper proposed a method to verify the training was performed on 'a credit dataset D' such that 'the model trainer can gain trust from the users'. This claim seems to be not convincing to me. Why a user can have extra trust if the model is trained a specific dataset? What is the goal of the model user? It would be better to show some real examples to help clarify it.\n\nc) Some assumptions used in the paper need justification. For example, the goal of the paper is to verify if the entire dataset was authentically used to train a model. However, in practice, a dataset is often combined with other datasets to jointly train a model - this will change the distribution and the proposed method may not work. Also, trainer may apply on-the-fly augmentations/filters which may affect the training data distribution. It would be better to discuss how these would impact the proposed method.\n\nd) While the evaluation results show that the proposed method can work well, there lacks a theoretical analysis (or proof) on the effectiveness of the proposed method. The evaluated datasets are small datasets, therefore there might be a gap when we use it in practice. A theoretical analysis will help understand the gap and better justify the proposed method.\n\ne) Meanwhile, the adaptive attack results seems not very convincing to me. In this setting, the attacker has fully knowledge of the verification process and the exact data probe to be used for verification, so why the attacker cannot simply overfit the desired model output of the probe data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This study presents Trustworthy Dataset Proof (TDP), a new approach to verifying the authenticity of training data in deep learning, addressing a gap left by existing dataset provenance methods that focus on ownership rather than trust enhancement. Traditional methods face challenges like high operational demands and ineffective dataset verification. To overcome these limitations, the authors propose Data Probe, a model-agnostic technique that leverages subtle variations in model output distributions to detect the presence of specific subsets of training data, differing from watermarking methods. This approach enhances usability by reducing training intervention and ensures dataset integrity, allowing detection only when the full claimed dataset is used. Extensive evaluations demonstrate the efficacy of the Data Probe-based TDP framework, advancing transparency and trust in training data utilization in deep learning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe proposed problem of trust dataset proof is interesting and important. This work provides an effective solution letting model trainers claim authentic usage of certain datasets.\n\n2.\tThe proposed methods don\u2019t rely on cryptography techniques, thus being lightweight and efficient.\n\n3.\tThe paper is well-organized and easy for readers to follow."
            },
            "weaknesses": {
                "value": "1.\tThe verification protocol is unclear in terms of presentation. For example, in probe selection, there is a need to generate or specify a key to select the data probe, who is responsible for generating this key, the paper sometimes adopts \u2018trainer\u2019 and sometimes adopts \u2018users\u2019, which is very confusing. Figure 3 cannot depict the protocol clearly as well.\n\n2.\tThe definition of harmlessness is too narrow, which is much wider than model performance compromising. Actually, the proposed four types of data probes all introduce harm to the model. For example, Prominent Probe leverages higher overfitting that increases the vulnerability of membership inference attacks. Absence Probe does hurt the performance of data probes and similar data. Needless to say, the Untargeted and Targeted Probe. In terms of general harmless dataset provenance, I believe this work [1] provides a better solution.\n\n3.\tAccording to the experiment results, different types of data probes perform distinctively in different metrics. However, this paper does not provide a solution that can unify different types of data probes to achieve a comprehensively better type. Without this, in practical scenarios, how does the user know which type to choose for specific cases?\n\n4.\tIn lines 362-363, the paper mentions that the sample-level scores are aggregated to form a new metric, but I cannot find the detailed aggregation approach. Then it is unclear how and where PSA in Table 2 originates from.\n\n5.\tAs for the adaptive attack, I disagree that replicating and embedding the probe corresponding to D after training would be adopted by the attacker. The more potential way is replicating and embedding the probe during the training. Besides, the experiment setup is also unclear, like how is the modified dataset D* made and how large the difference between D and D*?\n\n6.\tAccording to my understanding of AUC, I don\u2019t think a score exceeds 50 but smaller than 60 is sufficient for distinguishing two distributions.\n\n7.\tFigure 4 needs improvement, like there is no legend for the dashed curve and the y-axis of the third subfigure can use a log scale.\n\n[1] Guo, Junfeng, et al. \"Domain watermark: Effective and harmless dataset copyright protection is closed at hand.\" Advances in Neural Information Processing Systems 36 (2023)."
            },
            "questions": {
                "value": "See the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}