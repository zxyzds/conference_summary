{
    "id": "opv67PpqLS",
    "title": "DNALONGBENCH: A Benchmark Suite For Long-Range DNA Prediction Tasks",
    "abstract": "Modeling long-range DNA dependencies is crucial for understanding genome structure and function across a wide range of biological contexts in health and disease. However, effectively capturing the extensive long-range dependencies between DNA sequences, spanning millions of base pairs as seen in tasks such as three-dimensional (3D) chromatin folding, remains a significant challenge. Additionally, a comprehensive benchmark suite for evaluating tasks reliant on long-range dependencies is notably absent. To address this gap, we introduce DNALONGBENCH, a benchmark dataset spanning five important genomics tasks that consider long-range dependencies up to 1 million base pairs: enhancer-target gene interaction, expression quantitative trait loci, 3D genome organization, regulatory sequence activity, and transcription initiation signal. To comprehensively assess DNALONGBENCH, we evaluate the performance of five baseline methods: a task-specific expert model, a convolutional neural network (CNN)-based model, and three fine-tuned DNA foundation models -- HyenaDNA, Caduceus-Ph and Caduceus-PS. We envision DNALONGBENCH having the potential to become a standardized resource that facilitates comprehensive comparisons and rigorous evaluations of emerging DNA sequence-based deep learning models that consider long-range dependencies.",
    "keywords": [
        "long-range DNA benchmark",
        "long-range DNA modeling",
        "long-range DNA foundation models"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "Proposing a benchmark dataset for long-range DNA prediction tasks",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=opv67PpqLS",
    "pdf_link": "https://openreview.net/pdf?id=opv67PpqLS",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a set of five benchmarking tasks for genomic language models, with a focus on tasks that involve modeling long-range DNA dependencies, including enhance-target gene interaction and 3D genome organization. These tasks represent a diverse set of biologically important characteristics. The authors benchmark five baseline methods on each task - task-specific expert models, CNN-based models, and three recent DNA language models, and find that the DNA language models they benchmark do not outperform the task-specific models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The tasks included in the benchmark are biologically significant and diverse, especially the focus on tasks with different dimensionalities (1D or 2D) and granularities (binned or base pair level). In addition, the paper does a good job explaining the biological significance of each task. Both the increased focus on long-range tasks and base pair resolution tasks represent novel aspects of this benchmark compared to other previously published benchmarks.\n- The results are presented in a clear and concise manner\n- The performance of the three evaluated DNA language models \u2014 HyenaDNA, Caduceus-Ph, and Caduceus-PS \u2014 on these tasks reveal important limitations of current DNA language models at modeling long range dependencies, even when the models are able to incorporate long context lengths."
            },
            "weaknesses": {
                "value": "- In my opinion, the main weakness of this paper is novelty. Although the benchmarking tasks are important and biologically motivated, all five of the tasks and corresponding datasets have already been used to benchmark models in previous publications and in some previous benchmarks, such as BEND (Marin, et al. 2023) and LRB (Kao, et al. 2024). The \u201cRegulatory sequence activity\u201d task directly uses the training, validation, and test set sequences from Enformer. Therefore, the main contribution of this paper is consolidating these tasks into one resource and comparing the performance of DNA language models and more traditional supervised approaches. One potential way to increase the novelty and contribution of this work could be to incorporate more datasets that weren\u2019t used in previous publications to increase the amount of benchmarking data for each task. For example, for the enhancer-target gene task, the dataset is relatively small, and different experimental datasets often suffer from their own technical and experimental biases. Since a number of similar datasets exist and are publicly available, it would improve and increase the impactfulness of the benchmark to aggregate multiple similar datasets."
            },
            "questions": {
                "value": "- The term \u201cExpert model\u201d is misleading. For example, L369-370 refers to \u201cexpert models tailored to each task,\u201d but in some cases, such as using Enformer as an expert model for eQTL prediction, the expert model has not actually been tailored to this task. An alternative could be to refer to these models at \u201cState of the art models.\u201d In addition, in some cases the choice of Expert/SOTA model could be revisited. For example, for the enhancer-target gene task, Enformer (Avsec et al. Nature Methods (2021)) or Borzoi (Linder et al. 2023) should be used as a state of the art model instead of Activity by Contact.  In addition, for the eQTL prediction task, Borzoi was shown to outperform Enformer in Linder et al. 2023.\n- Use of the term DNA foundation model and which models are classified in this category is inconsistent throughout the paper. For example, on L146, Avsec et al. 2021a is cited as a DNA foundation model (although it is not actually pre-trained on only DNA seqeunces), but later in the paper it\u2019s used as an \u201cexpert model\u201d baseline in contrast to the DNA foundation models.\n\nMinor suggestions:\n- L51-53 - Karollus et al. Genome Biology (2023) could be cited with reference to this point\n- L154-155 is unclear and should be reworded: \u201cIt has shown promising performance in long-range species classification tasks despite the problem itself is not well defined in real applications.\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents DNALongBench, which consists of 5 datasets which attempt to test a predictive model\u2019s ability to make predictions on DNA sequences which are somewhat related to long-range interactions in the genome. Each of these datasets consists of a set of inputs and outputs, along with a well-defined performance metric. The input is a long DNA sequence and the output is a label which can be a scalar, or one or more functional-readout profiles which are parallel to the input sequence.\n\nThese 5 datasets and tasks are: 1) given a sequence with an enhancer and a promoter in it, predict whether or not the enhancer is linked to the promoter functionally; 2) given a long sequence, predict the (symmetric) 2D contact map; 3) given a DNA sequence, predict the functional profiles of several regulatory-genomic assays in both human and mouse; 4) given the reference and mutated sequences of putative eQTLs, predict whether or not the eQTL is causal/functional; 5) given a DNA sequence, predict base-pair-level profiles of transcription initiation.\n\nFor each task, the authors computed the performance of the dataset on HyenaDNA and Caduceus, which have been fine-tuned for each task separately. They also trained a simple CNN as a baseline, and compared to an \u201cexpert model\u201d, which is the model which was trained specifically for that task. The authors report the resulting performance metrics, and describe some general trends, such as the observation that the expert model generally performed the best, and that the long-range language models (e.g. HyenaDNA) typically underperformed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "### Good test of actual functional abilities of long-range DNA language models\n\nIn the recent literature, a lot of effort has been made to port over LLMs from NLP to DNA sequences (and other biological tasks). In these works, the evaluation of DNA LLMs has been very sparse, and it was unclear as to whether or not these models could handle real biological tasks of significance (instead of very simplistic predictive tasks like species prediction, which is not biologically meaningful or challenging today as a computational task).\n\nThis paper offers a much-needed evaluation of whether or not these DNA LLMs can actually handle biologically meaningful and challenging tasks which rely on long-range interactions (which was the initial promise of these models). Through various experiments, this paper shows that, in fact, these DNA LLMs are not particularly well-poised to predict meaningful biological tasks, even after fine-tuning.\n\n### Good selection of benchmarks with meaningful biological significance\n\nThe selection of the 5 benchmark tasks is well-thought-out, and they span many areas of interest, from regulatory genomics, to chromatin organization, to variant-effect prediction (for eQTLs). Together, these tasks cover a decent space in testing the ability of models to capture trans effects in the genome.\n\n### Clear writing and well-organized paper\n\nThe paper is well-written, with clear organization and motivation. Reading through, things were simple to understand, and structured in a very good way."
            },
            "weaknesses": {
                "value": "### Other than large DNA language models, not many other models are tested\n\nThe DNA LLMs that were tested (i.e. HyenaDNA and Caduceus) are good representations of this sort of model, but there aren\u2019t that many other models that were tested in this benchmark. Other than these DNA LLMs, this work trains a very simplistic CNN, and applies a single expert model (generally, the model which was published along with the data). Together, the expert model is an example of one of the best models possible, and the CNN is an example of one of the easiest deep-learning-based baselines. The analyses in this paper show how DNA LLMs perform relative to these two endpoints, but there are not any other models that are tested.\n\nIn addition to these DNA LLMs, many researchers will use other models and architectures which can also detect long-range interactions (such as those described in section 2.2). Especially due to the computational cost of DNA LLMs, many researchers will necessarily be relying on other architectures. It would be much more informative to show the performance of these other architectures (e.g. Enformer, Borzoi, even DanQ, etc., both fine-tuned and not fine-tuned on the specific task), to see how these models also perform. In its current form, this paper is largely just a benchmark for DNA LLMs, even though several other models exist, whose performance also falls somewhere in the spectrum between the basic CNN and the expert model. It would be far more meaningful of a benchmark if those models are shown, as well.\n\n### Limited results on the various advantages and disadvantages of certain models\n\nThere are only limited results in the paper exploring and discussing the relative advantages and disadvantages of certain models. For a benchmarking paper, it would be useful to have more discussion on the various areas where certain models do better than others, and an exploration of why (including different ablation studies). Some interesting discussion points (a very non-comprehensive set of suggestions) may be:\n\n- How useful is reverse-complement augmentation or reverse-complement-aware architectures for these tasks?\n- Since the expert model really does much better, how much of its improved performance is due to its focus on a specific task (whereas the DNA LLMs are not trained on a specific task)? How much does fine-tuning the DNA LLMs allow them to perform much better on these specific tasks?\n- How much performance is gained from the sheer size/expressivity of the DNA LLMs? Would a different architecture of similar size/expressivity be comparable to the DNA LLMs (e.g. testing an Enformer-like architecture of high capacity, for each of these tasks).\n- Section 5.2 suggests a difference between long-range and short-range prediction performance in these models. Is the failure of DNA LLMs to predict specific peaks due to the excessively long input sequences (i.e. so the model has a hard time focusing on specific parts of a sequence)?\n\n### Suggestions for improvements in writing\n\nThere are several places where the paper has some typos or other grammatical errors. Here is a non-comprehensive list:\n\n- An en-dash (not a hyphen, and not an em-dash) should be between words/objects which specify a pair: enhancer\u2013target gene, enhancer\u2013promoter, variant\u2013promoter\n- \u201cmegabase pairs\u201d should be \u201cmega basepairs\u201d or \u201cmega base-pairs\u201d (L220)\n- \u201ccomplied\u201d should be \u201ccompiled\u201d (L278)\n- \u201ctetails\u201d should be \u201cdetails\u201d (L363)\n\nAdditionally, there are areas which could use more clarity\n\n- In the tables comparing performance between the models, because the expert model is effectively always by far the best one, it would be useful to underline the second-best model, as well (or use another way to emphasize the second-best model in addition to the best)\n- In Table 2, the shape of the output in contact-map prediction is a bit confusing, as contact maps are typically thought of as 2D objects rather than a single 99681-vector\n- The order of the datasets/tasks is not the same in the Table 2, the main text, and the supplement in different sections; there doesn\u2019t seem to be a reason why the ordering of these 5 tasks can\u2019t be presented in the same ordering each time\n- In the data repositories at `https://dataverse.harvard.edu/`, it is not clear what each file represents and how the data is organized *vis a vis* the descriptions in the appendix; a README should be added to each repository"
            },
            "questions": {
                "value": "1. What is the stratified sampling approach used in some of these tasks (i.e. enhancer\u2013target-gene prediction, eQTL prediction); that is, what is the stratification based off of?\n2. In any of these datasets/tasks, is it possible that the same genomic coordinate appears in multiple dataset splits (e.g. train and test)? This could be possible if the examples arising from the same gene end up in different splits, for example\n3. When masking out other enhancers in the enhancer\u2013target-gene task, what were these enhancers masked with?\n4. What was the reasoning for removing genes with too few positive/negative pairs (i.e. enhancer\u2013target-gene prediction, eQTL prediction)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes DNALongBench, a new benchmark for genomic tasks that consider long-range dependencies of up to 1 million base pairs. The benchmark provides five tasks, enhancer-target gene interaction, expression quantitative trait loci, 3D genome organization, regulatory sequence activity, and transcription initiation signal, with different requirements. The authors evaluate five models per task, including a task-specific expert model, a CNN baseline and three DNA foundation models. The authors claim that DNALongBench is the most comprehensive benchmark tailored to DNA long-range tasks to date."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper studies a significant problem in computational biology. DNA regulation and Chromatin structure appears to be one of the challenges in the field and improvements could lead to novel insights into gene regulation and cellular regulatory networks.\n\n- The paper is well-written and well structured.\n\n- The effort of collecting data from different sources, curating the data, and providing it for download is a strong part of the work.\n\n- Each individual task, the biological importance, the data, and evaluation is well-described. The authors seem to be well informed about the literature and the underlying problem."
            },
            "weaknesses": {
                "value": "Major:\n\n1. It is unclear to me how many of the tasks really require long context to achieve strong results. \n\n2. A clear limitation of the work, which is also stated by the authors in the conclusion, is that they do not evaluate transformer based models on the different tasks.\n\n3. The codebase is not well documented, there is only a readme and one line referring to a notebook with code for the dataloaders.  \n\n\nPlease see more explanations below.\n\nRegarding 1:\n\nSome overview tables/plots in the appendix that clearly show the long-range dependencies of the tasks would make sense I think.\n\n\nRegarding 2:\n\nWhile it makes sense that models for genomic data require long contexts, the argument of *NOT* training attention-based models appears weak. The authors use a CNN baseline which obviously does not have a receptive field spanning millions of tokens / base pairs. However, even this \u201clightweight\u201d baseline seems to outperform the foundation models in some of the tasks (see e.g. Table 4 RSAP Human and Avg, Table 5 WB, SNSES, MS). I would rather say that models with limited context size would be very useful to argue in favor of the development of a long-range benchmark if they really perform worse than non-attention based FMs. Otherwise, the benchmark might still add value, but probably the line of argumentation should be different.\n\n\nRegarding 3:\n\nI think a benchmark should seamlessly integrate with developer code for a new method to be useful in practice. The authors state that they envision that DNALongBench could be a valuable resource for future evaluations, however, this requires clear documentation, a user friendly interface, and in the best case a leaderboard. A benchmark should integrate into my own codebase with only a few lines of code but it seems like I would have to integrate my code into DNALongBench instead. This clearly limits the usability of DNALongBench and I strongly recommend that the authors work on their code base.\n\nFor example, an API as follows would be very helpful to increase the usability\n\n```\nimport myModel\nimport Benchmark\n\nbenchmark = Benchmark(task=\u2019someTask\u2019)\nmodel = myModel()\n\ndef prediction_wrapper(task):\n    prediction = model.inference(task.x)\n    return prediction\n\nresults = benchmark(prediction_wrapper)\n\nprint(results)\n```\nThe provided dataloaders might be useful, however, I'm missing a clear description and interface for the evaluation.\nOverall, the lack of documentation is an important weakness here. While the API could still be improved until the CRC deadline, I hope the authors can already show some improvements during time of rebuttal.  \n\nFor a good example benchmark see e.g. [1] (featured paper at NeurIPS 2022 DBT)\n\nMinor: \n\nLine 363 typo: tetails -> details\n\nLine 303: Maybe I missed it but SNPs are not introduced.\n\nLine 479: Additional the\n\n[1] https://openreview.net/forum?id=_HLcjaVlqJ"
            },
            "questions": {
                "value": "See also weaknesses.\n\n- How much of the tasks really require long-range interaction predictions?\n- How do attention-based models perform on the tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}