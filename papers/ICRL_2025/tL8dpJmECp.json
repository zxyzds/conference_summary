{
    "id": "tL8dpJmECp",
    "title": "Improving Fairness and Mitigating MADness in Generative Models",
    "abstract": "Generative models unfairly penalize data belonging to minority classes, suffer from model autophagy disorder (MADness), and learn biased estimates of the underlying distribution parameters.  Our theoretical and empirical results show that training generative models with intentionally designed hypernetworks leads to models that 1) are more fair when generating datapoints belonging to minority classes 2) are more stable in a self-consumed (i.e., MAD) setting, and 3) learn parameters that are less statistically biased.  To further mitigate unfairness, MADness, and bias, we introduce a regularization term that penalizes discrepancies between a generative model\u2019s estimated weights when trained on real data versus its own synthetic data.  To facilitate training existing deep generative models within our framework, we offer a scalable implementation of hypernetworks that automatically generates a hypernetwork architecture for any given generative model.",
    "keywords": [
        "Hypernetworks",
        "Generative Models",
        "Fairness",
        "MADness",
        "Maximum Likelihood Estimation",
        "Bias"
    ],
    "primary_area": "generative models",
    "TLDR": "We show how hypernetwork training leads to more fair, less biased, and less MAD generative models.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=tL8dpJmECp",
    "pdf_link": "https://openreview.net/pdf?id=tL8dpJmECp",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes penalized autophogy estimation (PLE), which is a method that impproves fairness and reduce MADness of generative models.\nThe key idea is to make the model recursively stable via a regularization term on the MLE loss.\nA naive formulation of this loss is intractable and the author(s) solve this issue by a hypernetwork that generates the parameters of the generative model\nExperiments show that the proposed method can improve fairness and reduce MADness of generative models on various datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Novel theoretical contribution connecting statistical bias in MLE to fairness and MADness issues in generative models\n- Comprehensive empirical validation across multiple types of distributions and models (VAE, BigGAN)\n- Strong technical foundation with clear connections to existing statistical theory\n- Results show meaningful improvements in both fairness metrics and stability against MADness"
            },
            "weaknesses": {
                "value": "- The motivation and problem setup in the introduction is not well structured, making it difficult to grasp the core contribution initially\n- Limited ablation studies on the choice of hyperparameters (e.g., PLE penalty $\\lambda$=0.1)\n- Some experimental results show inconsistent trends across different distributions without sufficient explanation\n- The presentation could be more accessible to readers less familiar with statistical estimation theory"
            },
            "questions": {
                "value": "1. How sensitive is the method to the choice of $\\lambda=0.1$? Was this value chosen empirically or is there theoretical justification?\n2. In Figure 2, why does FID still increase with PLE, albeit more slowly? Is this related to the hyperparameter choice (e.g. number of data points in equation 6)?\n3. In Figure 4, some distributions show stable performance while others show increasing MADness---why?\n4. How does the hypernetwork architecture choice and hyperparameters like $\\lambda$ affect performance? I think we need a systematic study on this?\n\nIn general I really like the paper and I'd like to increase my rating if the presentation is improved and my question on hyperparameter is answered."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper sets out to address the important problem of representation bias in generative models and MADness (decreased models\u2019 performance when trained using their outputs). The authors propose a method that enforces the parameters of the generative model to remain consistent when trained on the original or synthetic data. To alleviate the challenges of intractable search over generative models\u2019 parameters trained on different synthetic data, the proposed method uses hypernetworks to sample generative model weights. Experiments show the ability of the proposed method to mitigate unfairness and MADness."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-written and addresses important problems in generative models. \n2. The proposed method is intuitive and easy to understand. \n3. The experiments are well conducted and demonstrate the effectiveness of the method."
            },
            "weaknesses": {
                "value": "The paper lacks discussion and comparisons with existing bias mitigation methods in generative models. The authors could consider the following methods and explain how their method differs conceptually from these existing approaches and provide empirical comparisons that can support the benefit of the proposed method in the considered setup:\n\n- Xu, Depeng, et al. \"Fairgan: Fairness-aware generative adversarial networks.\" 2018 IEEE international conference on big data (big data). IEEE, 2018.\n- Choi, Kristy, et al. \"Fair generative modeling via weak supervision.\" ICML 2020.\n- Sabbagh, Kamil, et al. \"RepFair-GAN: Mitigating Representation Bias in GANs Using Gradient Clipping.\" Tiny Papers @ ICLR (2023).\n- Rajabi, Amirarsalan, and Ozlem Ozmen Garibay. \"Tabfairgan: Fair tabular data generation with generative adversarial networks.\" Machine Learning and Knowledge Extraction 4.2 (2022): 488-501\n\nThe experiments are not consistent enough; specifically, the datasets used in the paper are considered for different types of experiments. For example, fairness is evaluated only on MNIST using VAE, and MADness mitigation is evaluated on CIFAR 10 using GANs. The authors should provide justifications for the experimental design or consider evaluating different datasets under the same set of experiments, i.e., show that the method also mitigates MADness/fairness on MNIST with GANs, and similarly for CIFAR 10. This would demonstrate the generalizability of the proposed method under the metric being evaluated (e.g., fairness or MADness).   \n\nAnother important concern is the authors did not provide sufficient justification or insights into how the method can mitigate bias in generated data, particularly when a generative model trained on the original data is already biased, which can be exacerbated in the follow-up generation. More specifically, it is unclear how constraining the MLE to find the model parameters that remain consistent with synthetic and original data (Eq. 4) mitigates unfairness. The authors could provide a more detailed explanation or intuition for how their method addresses bias, particularly in cases where the original training data is biased.\n\nThe authors did not provide ablation on the impact of the parameter $\\lambda$ in Equation 5. As the regularization term proposed is the core contribution of the paper, providing these experiments would provide more insights into how the regularization term influences several aspects of the work: the quality of the generated data, the number of generations after which \u201cMADness\u201d occurs, and the fairness of the synthetic data. These experiments could be provided for synthetic distributions used in Figure 4.  In addition, the authors can discuss the range of $\\lambda$ values they think would be most interesting to explore and why.  \n\nThe Appendix contains unnecessary materials (e.g., B, C, D, E). While this material is interesting and well-written, it can confuse the reader since it is not directly linked to the paper's main contribution. Instead, the authors should consider referring the readers to books/papers that contain this background information.  \n\nFor the experiments on the MNIST dataset, Line 334 reads: _The majority class was the digit 3, and the minority class was the digit 6 (this choice was arbitrary)_. Instead of arbitrarily choosing the minority class, the authors should consider the class with the highest false negative rate when classified as the minority. This means the minority class confuses the most with other classes and can be harder to learn when it is underrepresented, for example, see how reference [1] chooses the class to artificially under-represent. \n\nThe paper highly depends on hypernetworks, which increase the complexity of the method and its practical usage. \n\n[1]Bagdasaryan, Eugene, Omid Poursaeed, and Vitaly Shmatikov. \"Differential privacy has disparate impact on model accuracy.\" NeurIPS 2019"
            },
            "questions": {
                "value": "Please see the weaknesses above. In addition, the paper needs proofreading to fix minor typos. Here is a non-exhaustive list of them:\n- Line 060: Autophogy Estimation => Autophagy\n- Line 078: [...] some events events[...] => some events\n- Line 088: [...] and is is described => and is described\n- Line 322: [...] S ompares the representation => S compares the representation\n- Line 414: For hyeprnetwork training [...] => For hypernetwork"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses fairness and bias in generative models, which often penalize minority classes and suffer from model autophagy disorder (MADness). The authors propose training generative models with hypernetworks to make them more fair, stable, and less biased. They introduce a regularization term to reduce discrepancies between a model's performance on real data versus synthetic data, helping mitigate bias and improve representation fairness. The experimental results show that their framework is scalable, supporting integration with existing deep generative models such VAEs and GANs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The overall paper is well-structured and clearly written, with concise explanations that make concepts like hypernetworks and MADness accessible to a broader readers.\n\n+ This paper tackles an important and timely issue by addressing fairness and bias in generative models, particularly focusing on challenges such as minority class penalization and model autophagy disorder (MADness).\n\n+ The discussion related to large language models (LLMs) adds practical relevance to this research, as the content generated by LLMs is widely distributed across the internet."
            },
            "weaknesses": {
                "value": "- The experiments are relatively weak, especially given the small dataset and the older models used (VAE and BigGAN). It would strengthen the paper significantly if the method were tested on diffusion models.\n- There is extensive discussion of ChatGPT and other LLMs in Section 1.3. It would enhance the paper if the proposed method could be applied directly to LLMs."
            },
            "questions": {
                "value": "1. In Section 1.1, why does it say that GAN is trained with MLE?\n\n2. In Line 087-088, should $ R_I = C_{Maj} / C_{Min} $ actually be $ R_I = |C_{Maj}| / |C_{Min}| $?\n\n3. In Equation 2, why is $S(M)$ linear? Could the authors provide a detailed explanation for this?\n\n4. In Line 316, how should we interpret $ R_{Fair} < |C_{Maj}| / |C_{Min}| $? According to your conclusion about the linearity of $ S(M) $, shouldn\u2019t the optimal $ R_{Fair} $ that fits the data be equal to $ |C_{Maj}| / |C_{Min}| $?\n\n5. How are the hypernetworks $H_\\phi$ trained?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper contains two new innovations:\n- A metric of fairness when there is a minority and majority class in the training data.\n- A hyperparameter network and corresponding loss function intended to prevent autophagous collapse (MADness)\n\nThe authors demonstrate the efficacy of their method to mitigate MADness over multiple generate-train loops and improve fairness.  They do this in both a deep generative model and a statistical context."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed method is certainly interesting, and if it truly works as claimed it would be an impressive and efficient way to train new networks.  The paper is clear and well-motivated.  It poses an original solution to mitigate MADness in generative models.  The results regarding fairness seem experimentally compelling."
            },
            "weaknesses": {
                "value": "- Concerns about the validity of the method: Using a 3-layer FC neural network with the objective stated in the paper will likely result in pathologies.  Suppose that $\\theta*$ is the MLE.  It is likely that the FC network H is learning to set its weights close to 0 and its biases close to $\\theta*$.  In this way, changing the data is unlikely to alter the parameters very much, making any additional training/retraining using $H$ redundant.  One experiment that you could do to test this (which would alleviate my concerns and requires no additional training) is as follows.  Let G be the BigGAN from figure 2, and let H be the corresponding hyper-network.  If H is really doing it's job, then when you fit $\\theta = \\sum_{i=1}^n H(x_i)/n for x_i$ only in the category of \"airplanes\", the resulting generative network $G(x|\\theta)$ should produce only images of airplanes.  I suspect that instead one of two things will happen: either the network will barely change, or it will produce images that are not discernible as anything.\n\n- Quality concerns: although the authors report an FID score, there are no generated images from CIFAR in the paper or the appendix.  There also is no clear explanation for why there is a sudden spike in MADness at iteration 4.  \n\n- The evaluations are rather limited: The deep generative models are restricted to CIFAR-10.\n\n- There is no theoretical explanation for why this method improves what the authors call fairness.  It is non-obvious to me why the method should improve fairness."
            },
            "questions": {
                "value": "See the weaknesses section.\n\nAlso:\n- The definition of fairness seems overly simple-- it would be helpful to understand how statistics deals with this issue and what the existing metrics are.  For instance, how do you adapt the current definition when there are multiple classes?\n- How does the method compare to more standard statistical techniques (i.e. upweighting the loss from the minority classes)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}