{
    "id": "IwPXYk6BV9",
    "title": "Enhancing Learning with Label Differential Privacy by Vector Approximation",
    "abstract": "Label differential privacy (DP) is a framework that protects the privacy of labels in training datasets, while the feature vectors are public. Existing approaches protect the privacy of labels by flipping them randomly, and then train a model to make the output approximate the privatized label. However, as the number of classes K increases, stronger randomization is needed, thus the performances of these methods become significantly worse. In this paper, we propose a vector approximation approach, which is easy to implement and introduces little additional computational overhead. Instead of flipping each label into a single scalar, our method converts each label into a random vector with K components, whose expectations reflect class conditional probabilities. Intuitively, vector approximation retains more information than scalar labels. A brief theoretical analysis shows that the performance of our method only decays slightly with K. Finally, we conduct experiments on both synthesized and real datasets, which validate our theoretical analysis as well as the practical performance of our method.",
    "keywords": [
        "label differential privacy"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=IwPXYk6BV9",
    "pdf_link": "https://openreview.net/pdf?id=IwPXYk6BV9",
    "comments": [
        {
            "comment": {
                "value": "I think the rebuttal addresses an interesting point. Hence, after reading the response and other reviewer's comments, I decide to raise my score."
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "Thanks for your feedback. We are encouraged that you have a positive evaluation on the significance, as well as the simplicity and effectivenss of our work.\n\n**The theoretical results provide only an upper bound for the proposed method. I suggest including additional comparisons and discussions between this bound and existing results.**\n\nWe think that this is a very good question. Deriving the lower bound of excess risk is hard. However, it can be shown that if we just use scalar labels, the probability of retaining original label decrease polynomially with $K$. \n\nDenote $M_{ij}=P(Z=j|Y=i)$ as the transition probability. Then $\\sum_{j=1}^K M_{ij} = 1$. From the $\\epsilon$-label LDP constraint, $M_ii\\leq e^\\epsilon M_{ji}$ for all $j$. Taking sum over all $i$ and $j$, \n\n$$K\\sum_{i=1}^K M_{ii} \\leq \\sum_{i=1}^K \\sum_{j=1}^K e^\\epsilon M_{ji} = Ke^\\epsilon.$$\n\nThe average probability of retaining original label satisfies $(1/K)\\sum_{i=1}^K M_{ii}\\leq e^\\epsilon/K$, which decays inversely with $K$. \n\nTherefore, with the increase of $K$, the privatized scalar random variables become increasingly unlikely to retain the original label. \n\nWe thank you for the other two valuable suggestions. In our revised paper, we will add experiments with long-tailed distributions or datasets with high class imbalance. The expected result is that classes with minor probability will not obviously affect the performance of our method, but baseline methods will be negatively affected. We will also improve the conciseness.\n\nFor the question, Table 3 is about experimentson on CIFAR-100 dataset. It seems that this table does not contain footnotes, thus we are not sure where you are referring to. Would you please explain the question further?"
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "Thanks for your positive evaluation and valuable comments.\n\n# Reply to weaknesses\n\n**1. Given strong conceptual similarities between ALIBI and the method proposed here, as well as the fact that the former is the current state-of-the-art for label DP task, I would have expected the theoretical analysis to focus on comparing the two methods.**\n\nWe think that this is a really good question. To make a rigorous theoretical statement, we need to show the **lower bound** of the risk of ALIBI. ALIBI uses Bayesian inference, which is a bit complex and it is thus hard to obtain a general lower bound. However, we can show that the performance of ALIBI decays with $K$ by a simple example.\n\nSuppose that for all $x$, $\\eta_1(x)=1$, $\\eta_2(x)=\\ldots=\\eta_K(x)=0$. Then all samples have label 1. According to eq.(3) in the ALIBI paper, \n\n$$p(Y=c|o, \\lambda) = Softmax(f(o_c)/\\lambda),$$\n\nin which we assume uniform prior. Then it can be shown that the probability of making a correct prediction is upper bounded by\n\n$$p(Y=1|o, \\lambda)\\leq \\frac{1}{\\sum_{c=2}^K 1(f(o_c)\\geq f(o_1))}\\leq \\frac{1}{\\sum_{c=2}^K 1(o_c\\geq o_1)},$$\n\nin which $f(o_c)=-\\sum_k |o_k-[c=k]|$, with $[]$ being the Iverson bracket. These are from the original paper of ALIBI. The second inequality holds because if $o_c\\geq o_1$, then it can be shown that $f(o_c)\\geq f(o_1)$.\n\nRecall Algorithm 3 in ALIBI paper, $o_1=1+W_1$, $o_c=W_c$ for $c=2,\\ldots, K$, in which $W_j\\sim Laplace(\\lambda)$ for all $j=1,\\ldots, K$. Therefore, in expectation, $\\sum_{c=2}^K \\mathbf{1}(o_c\\geq o_1)$ grows linearly with $K$. As a result, $p(Y=1|o, \\lambda)$ decays nearly inversely with $K$. \n\nFrom the example above, it can be found that the performance of ALIBI decays significantly with $K$.\n\nHope that the above example provides some intuition. However, currently it is hard to provide a complete analysis since deriving the lower bound of risk is usually much harder than the upper bound.\n\n**2.citing computational efficiency ($O(K^2)$ vs $O(K)$) when comparing the new approach with ALIBI (Malek Esmaeili et al. (2021)) looks misplaced, when in any practical ML application the cost of training would dominate over the computational cost of flipping labels.**\n\nIn this paper, we discuss general methods for label randomization under local label DP. The base learner is not necessarily a deep learning model. It can also be statistical machine learning methods. Therefore we think that it is worthwhile to mention the complexity. For example, in our numerical experiments using $k$ nearest neighbors, the computation cost of ALIBI is about 3-4 times higher than our method.\n\n# Reply to questions\n\n**1. Can you elaborate your reasoning on the line 107**\n\nThanks for this comment. The softmax operation normalizes the output to 1, which makes the values of each component smaller if the previous sum was larger than 1, or larger if the previous sum is smaller than 1, thus this operation introduces bias. For simplicity, suppose $o_1=1$ and $o_2=\\ldots=o_K=0$, then $f(o_1)=0$ and $f(o_2)=-2$. According to eq.(3) in ALIBI paper, after softmax operation, the probability $p(y=1|o,\\lambda)<1$, while $p(y=c|o, \\lambda)>0$ for $c\\neq 1$. This is an example that the softmax operation introduces bias. \n\nHowever, we agree that readers may get confused when reading this sentence at the first time. Therefore in the revised paper, we will restate this sentence, and mention the example in the reply of weakness 1.\n\n**2. In section 4.3 Practical Implementation, how does changing the activation function of the last layer affects the model performance? Do you use this in the experiments chapter and if yes, how do you modify the training process?**\n\nThe changing of activation function is required by our analysis. We hope that $g_j(x)$ approximates $E[Z(j)|X=x]$ (see the discussion after eq.(15) in our paper). Therefore, here we need to use the sigmoid activation. If we use softmax activation, then the network learns $E[Z(j)|X=x]/\\sum_l E[Z(l)|X=x]$.\n\n**3. To be pedantic, line 87 should say \"Malek Esmaeili et al. (2021) proposes PATE-FM\". Original PATE was introduced by Papernot et al., 2016, and is not specific to label DP.**\n\nThanks for this comment. We will change the statement and cite the original paper of Papernot et al."
            }
        },
        {
            "summary": {
                "value": "This paper consider the problem of supervised multi-class learning problem under the setting of \"label differential privacy\" (LDP). This is a setting that is well-studied in recent literature, where the input features are considered public, and only the label is considered sensitive and needs to be protected by differential privacy.\n\nThis paper proposes a new randomizing mechanism that satisfies LDP, and outperforms prior methods studied in literature. The new method is as follows: Suppose there are $K$ possible labels. Given any label $y \\in [K]$, construct the $K$-dimensional one-hot encoding of $y$ and apply the (binary) Randomized Response mechanism on each bit of the one-hot encoding, flipping each bit with probability $\\frac{1}{1 + e^{\\epsilon/2}}$.\n\nA theoretical analysis is done for the k-nearest neighbor learning rule showing that the excess risk grows as $\\sim \\sqrt{\\ln K}$ with increasing $K$ (keeping other parameter fixed).\n\nExperimental results show that this approach outperforms prior methods studied in literature on a synthetic dataset (mixture of Gaussians data with centers around a circle) as well as real data (MNIST, CIFAR)."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper proposes a simple method and shows that it outperforms prior methods proposed in literature of LP-MST (Label Privacy Multi Stage Training [[Ghazi et al. '21](https://arxiv.org/abs/2102.06062)]) and ALIBI [[Malek et al. '21](https://arxiv.org/abs/2106.03408)].\n\nThe method is also simple to implement with minimal computational overhead relative to prior methods.\n\nThe paper is well written and easy to follow."
            },
            "weaknesses": {
                "value": "I feel there is rooom for making the paper stronger on the theoretical side.\n\nI feel the analysis of k-NN is a bit incomplete (Appendix B.1) and doesn't highlight the complete picture. What I would have liked to see is how the excess risk decreases as $N \\to \\infty$ (see more details about this under \"Questions\").\n\nNevertheless, I find it quite interesting that such a simple method shows a clear improvement over prior methods in the experiments, and so I don't think of the above as a significant weakness, since the theoretical analysis is not critical for the experimental results."
            },
            "questions": {
                "value": "In particular, [Ghazi et al. '21](https://arxiv.org/abs/2102.06062) have an analysis in Appendix F where they show that the excess risk of learning with stochastic gradient descent is $K / \\sqrt{N}$ for pure-DP and $\\sqrt{K / N}$ for approximate-DP. A subsequent paper by [Ghazi et al. '24](https://arxiv.org/abs/2406.19040) provides a more complex method that can theoretically achieve a excess empirical error rate of $\\sqrt{(\\log K) / N}$.\n\nBut I think the $\\sim \\sqrt{(\\log K) / k}$ scaling does not exactly say how the excess error scales with $N$. I would imagine that as $N \\to \\infty$, the optimal learning rule would also be scaling $k$ in some way, and not keep it fixed. I think this analysis might bring in dimensionality factors into the picture, which are necessary for understanding the precise scaling of excess risk with $N$.\n\n---\n\nOn the experimental side, I am wondering if the authors have also considered datasets with non-uniform distribution over labels. For example, one could consider MNIST or CIFAR datasets by additional duplicates of certain labels to make the distribution of labels to be biased. How does the proposed method perform against prior methods, especially methods that use prior information (such as RRWithPrior or LP-MST)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new approach to local label differential privacy, transforming each label into a K-dimensional vector, where each component is independently randomized with a RAPPOR-like mechanism. This vector-based approach preserves more information than scalar label transformations while maintaining \u03b5-local differential privacy. The authors provide theoretical analysis showing that their method's performance only degrades slightly with increasing number of classes K. They empirically validate their approach using both synthetic data and standard benchmark datasets (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100), demonstrating that their method significantly outperforms existing approaches, especially when the number of classes is large and the privacy requirements are strict (small \u03b5). Notably, proposed method outperforms a similar approach proposed by Malek Esmaeili et al. (2021), also relying on expanding a label to a vector of probabilities and adding noise to the vector."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper studies an important and well defined problem of label DP, which however does not get as much attention in the literature as traditional DP, while being relevant for many practical applications\n2. The paper proposes a novel method to provide local label-only DP guarantees, and thoroughly explores it. The proposed method provides better utility (at the same privacy budget) compared to the prior methods on standard benchmarks. The paper provides necessary proofs of privacy guarantees, as well as general intuition behind the method.\n3. The paper does a good job of putting this work in the context of the overall field, providing important details of prior works (notably, by Malek Esmaeili et al. (2021) as it shares the most similarities with the current paper)\n4. The evaluation protocol looks solid and follows standard practices and baselines, including what is typically used by prior works.\n5. The proposed practical implementation is quite neat, making the implementation of the method computationally efficient."
            },
            "weaknesses": {
                "value": "At it's core, the proposed method can be summarised as applying technique from RAPPOR (Erlingsson et al. (2014)) to the \"soft label\" approach to label DP introduced by ALIBI (Malek Esmaeili et al. (2021)). Both techniques are properly acknowledged by the authors, and combining two existing techniques in a novel way is not a weakness by itself. Especially given the strong empirical results and accompanying theoretical analysis. \n\nHowever, I believe this should affect the focus of the analysis. In particular, the main focus area for the theoretical part of the paper is answering the question \"how does the excess risk grow with the number of classes K\", implicitly comparing it with the randomized response and RRWithPrior (Ghazi et al. (2021)), which are known to degrade in utility with growing K. Given strong conceptual similarities between ALIBI and the method proposed here, as well as the fact that the former is the current state-of-the-art for label DP task, I would have expected the theoretical analysis to focus on comparing the two methods. Or, if the analysis of $\\Delta(x)$ is relevant for comparison with ALIBI, it needs to be shown.\n\nAdditionally, citing computational efficiency ($O(K^2$) vs $O(K$)) when comparing the new approach with ALIBI (Malek Esmaeili et al. (2021)) looks misplaced, when in any practical ML application the cost of training would dominate over the computational cost of flipping labels."
            },
            "questions": {
                "value": "1. Can you elaborate your reasoning on the line 107: \"However, ALIBI generates outputs using Bayesian inference and normalizes them to 1 using a softmax operation. Intuitively, such normalization causes bias\"?\n2. In section 4.3 Practical Implementation, how does changing the activation function of the last layer affects the model performance? Do you use this in the experiments chapter and if yes, how do you modify the training process?\n3. To be pedantic, line 87 should say \"Malek Esmaeili et al. (2021) proposes PATE-FM\". Original PATE was introduced by [Papernot et al., 2016](https://arxiv.org/abs/1610.05755), and is not specific to label DP."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new (?) approach for local label differential privacy (LDP) in machine learning, presenting both the theoretical framework and a numerical evaluation of the proposed method\u2019s effectiveness. This method is based on a component-wise randomised response mechanism on the one-hot encoding of the class."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The paper is well-organized and easy to follow, helping readers grasp the main ideas and methodology.\n- The authors provide a numerical evaluation of the proposed methods, which helps to demonstrate its performance in practice.\n- The authors provide a semi formal justification of the performance of the proposed method"
            },
            "weaknesses": {
                "value": "- The focus on local differential privacy is not clear from the title and abstract, which could be misleading for readers. Revising these sections to clearly indicate this focus would improve accessibility and transparency.\n- The methods mentioned in the \u201cOthers\u201d section of the related work could be discussed in more detail to give readers a clearer understanding of the techniques used in prior studies.\n- The practical applicability of the method is limited, as it requires training a separate discriminant for each class, which could be cumbersome or impractical for larger or more complex tasks.\n- The novelty of the proposed method is questionable, as noted by the authors themselves in lines 232-233, where they acknowledge its prior use in the literature.\n- The authors claim that their method achieves better results by encoding more information in a vector rather than a scalar, but it remains unclear if this added information genuinely benefits the privacy-utility tradeoff. It would be valuable to see a theoretical result similar to Theorem 2 applied to the randomized response mechanism over the simplex to support a comparative analysis."
            },
            "questions": {
                "value": "Could the authors provide justifications or address the weaknesses mentioned above, especially regarding the novelty of the method and the theoretical basis of the claimed improvements? If these concerns are adequately addressed, I would be open to revising my rating and overall assessment of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a vector approximation method aimed at improving label differential privacy (label DP) for large multi-class datasets. By encoding labels as vectors rather than scalars, the authors argue that this approach retains more class information, enabling better performance in high-class settings where traditional label DP methods degrade significantly. They present both theoretical analysis and empirical validation on synthesized and benchmark datasets, demonstrating superior accuracy, especially as the number of classes $K$ grows."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper addresses a meaningful problem: label LDP learning with a large number of label classes. This has important practical relevance, particularly in applications with long-tailed data distributions.\n- The paper proposes a simple yet effective technique to tackle this issue.\n- The theoretical results are adaptable and could be extended to various scenarios under specific assumptions (e.g., smooth regression functions).\n- The experimental results demonstrate significant improvements over previous methods."
            },
            "weaknesses": {
                "value": "- The theoretical results provide only an upper bound for the proposed method. I suggest including additional comparisons and discussions between this bound and existing results. Additionally, could the authors establish any lower bounds for previous methods that use scalar labels alone (and possibly find the phase transition between $\\log K$ and $K$ dependence)? Such bounds would clearly demonstrate the necessity of the proposed method. \n- The experiments were conducted on standard datasets. It would be valuable for the authors to conduct experiments on datasets with long-tailed distributions to highlight the method's practical relevance. Furthermore, it would be interesting to discuss how a long-tail distribution or class imbalance might impact the theoretical guarantees of the algorithm.\n- The paper is somewhat repetitive and could benefit from further editing for conciseness."
            },
            "questions": {
                "value": "- What is the purpose of the footnote in Table 3?\n\nAlso, see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}