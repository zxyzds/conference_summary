{
    "id": "fIMf9zQo9d",
    "title": "GeoDream: Disentangling 2D and Geometric Priors for High-Fidelity and Consistent 3D Generation",
    "abstract": "Text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models has shown great promise but still suffers from inconsistent 3D geometric structures (Janus problems) and severe artifacts. The aforementioned problems mainly stem from 2D diffusion models lacking 3D awareness during the lifting. In this work, we present GeoDream, a novel method that incorporates explicit generalized 3D priors with 2D diffusion priors to enhance the capability of obtaining unambiguous 3D consistent geometric structures without sacrificing diversity or fidelity. Specifically, we first utilize a multi-view diffusion model to generate posed images and then construct cost volume from the predicted image, which serves as native 3D geometric priors, ensuring spatial consistency in 3D space. Subsequently, we further propose to harness 3D geometric priors to unlock the great potential of 3D awareness in 2D diffusion priors via a disentangled design. Notably, disentangling 2D and 3D priors allows us to refine 3D geometric priors further. We justify that the refined 3D geometric priors aid in the 3D-aware capability of 2D diffusion priors, which in turn provides superior guidance for the refinement of 3D geometric priors. Our numerical and visual comparisons demonstrate that GeoDream generates more 3D consistent textured meshes with high-resolution realistic renderings (i.e., 1024 * 1024) and adheres more closely to semantic coherence.",
    "keywords": [
        "3D generation; Text to 3D; Image to 3D; AIGC"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=fIMf9zQo9d",
    "pdf_link": "https://openreview.net/pdf?id=fIMf9zQo9d",
    "comments": [
        {
            "summary": {
                "value": "GeoDream presents an innovative approach to 3D asset generation by integrating disentangled 3D priors with 2D diffusion models. This method leverages a multi-view diffusion model to generate posed images, subsequently constructing a cost volume as a 3D prior to ensure spatial consistency. By separating 2D and 3D priors, GeoDream allows iterative refinement of these priors, enhancing the model\u2019s 3D awareness without compromising the diversity or fidelity of generated 3D models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The disentangled approach to handling 2D and 3D priors effectively addresses the Janus problem, resulting in improved 3D consistency.\n2. The framework supports rendering up to 1024x1024, surpassing most SDS-based methods in resolution.\n3. GeoDream demonstrates robustness across a wide range of prompts and asymmetric structures."
            },
            "weaknesses": {
                "value": "1. Lack of comparative analysis with state-of-the-art methods such as LucidDreamer (CVPR 2024), RichDreamer (CVPR 2024), and ImageDream (Arxiv 2023).\n2. While primarily compared with SDS-based methods, GeoDream only includes Wonder3D as an exception. Given the limitations of SDS in terms of low-quality mesh and texture, comparisons with direct reconstruction methods such as InstantMesh (Arxiv 2024), One-2-3-45++ (CVPR 2024), and Era3D (NeurIPS 2024) would strengthen the evaluation.\n3. The provided video reveals low mesh quality in certain outputs.\n4. Additional pure mesh comparisons with other methods are needed to better assess mesh quality.\n\n\nReference:\n\n[1] Liang, Yixun, et al. Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching. CVPR. 2024.\n\n[2] Qiu, Lingteng, et al. Richdreamer: A generalizable normal-depth diffusion model for detail richness in text-to-3d. CVPR. 2024.\n\n[3] Wang, Peng, and Yichun Shi. Imagedream: Image-prompt multi-view diffusion for 3d generation. arXiv. 2023.\n\n[4] Xu, Jiale, et al. Instantmesh: Efficient 3d mesh generation from a single image with sparse-view large reconstruction models. arXiv. 2024.\n\n[5] Liu, Minghua, et al. \"One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. CVPR. 2024.\n\n[6] Li, Peng, et al. Era3D: High-Resolution Multiview Diffusion using Efficient Row-wise Attention. NeurIPS 2024."
            },
            "questions": {
                "value": "1. Table 2 appears to have a formatting issue\u2014could you clarify the template used?\n2. Why does Figure 3 present the generated mesh using rendered RGB rather than pure mesh? This choice limits the ability to assess mesh quality directly.\n\nPlease see more questions in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper disentangles 2D and 3D priors on text-to-3D generation and can generation up to 1024 resolution."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Higher resolution text-to-3D generation\n2. Disentangled 2D and 3D representation\n3. Results are good\n4. They combined Neus and DMTet as 3D representation"
            },
            "weaknesses": {
                "value": "1. There are still some artifacts on the extracted mesh."
            },
            "questions": {
                "value": "It would be better if the authors can show comparison with baseline methods in the videos."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper tackles the task of Text-to-3D generation, addressing the technical challenge of the Janus problem that previous methods have encountered. The key insight and motivation is to leverage 2D diffusion priors to enhance the generation quality. The approach involves using a multi-view diffusion model to generate multi-view images, representing the 3D object with a cost volume, and then optimizing the 3D object using differentiable rendering from the generated multi-view images. In the second stage, the geometry decoder is fixed, and the texture decoder is optimized based on the SDS loss, resulting in improved 3D object generation from textual descriptions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The ablation studies are sufficient to validate the effectiveness of the proposed components.\n2. The paper is well written and easy to read."
            },
            "weaknesses": {
                "value": "1. The paper only compares with works from 2023 and does not include comparisons with more recent papers from CVPR 2024 and ECCV 2024. I have observed that some papers have better visualization results than this paper, such as One-2-3-45++. It would be beneficial for the paper to include these comparisons to demonstrate its standing in the current research landscape.\n2. I do not understand the rationale behind the second phase, which is the optimization of the texture decoder using the SDS loss. In my view, if the multi-view diffusion model performs well, the SDS loss would be unnecessary, as evidenced by some recent papers. Is it because the multi-view diffusion model trained in this paper does not generate satisfactory results?\n3. The use of cost volume to define the 3D representation seems odd. While I understand that cost volume adds 3D regularization, it has several drawbacks, including: (a) The cost volume essentially fuses multi-view features. If the input images have occlusion relationships, the cost volume cannot fuse to obtain the correct feature volume. (b) The spatial complexity of cost volume is quite large, which limits the resolution.\n4. Overall, the paper's design shows some technical improvements compared to 2023's work, but it seems somewhat outdated compared to the work presented at CVPR 2024 and ECCV 2024. Although the ablation studies are complete, I still do not believe this paper is suitable for acceptance by ICLR.\n5. The paper should present more generation results from the multi-view diffusion model to demonstrate its effectiveness, as this model is the core technology of the paper. It is also necessary to clarify how this diffusion model improves upon MVDream. Personally, I do not believe that improvements in 3D representation or training methods can bring about fundamental enhancements. The essence of 3D generation is to learn a stronger 3D prior, which may be represented by a multi-view diffusion model or a direct 3D generative model."
            },
            "questions": {
                "value": "Authors should compare with SOTA baseline methods and demonstrate the advance of method design."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenge of text-to-3D reconstruction with GeoDream, a method that integrates explicit 3D priors with 2D diffusion priors to capture clear, 3D-consistent geometric structures. The approach uses a multi-view diffusion model to generate posed images, then constructs a cost volume from these images as native 3D geometric priors. The 3D priors are integrated with 2D diffusion priors through a disentangled design. Comparisons with existing methods show that GeoDream generates more 3D-consistent textured meshes with strong semantic coherence."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The method constructs a cost volume as native 3D priors by aggregating predicted multi-view 2D images in 3D space, enhancing multi-view consistency in the generated 3D models.\n- By refining geometric priors through a 2D diffusion model, this work shows an improvement in rendering quality and geometric accuracy.\n- Comparisons with baselines highlight the method's effectiveness in generating 3D-consistent textured outputs."
            },
            "weaknesses": {
                "value": "The proposed method design seems incremental compared to the existing work like One-2-3-45 and has several key limitations:\n\n- Generation Quality: The method's quality does not appear to surpass existing approaches. The visual results, even in the teaser of Fig. 1, reveal artifacts around object boundaries and inconsistencies in geometry, especially visible in mismatched normal maps and images. In the supplementary video, examples like the EAGLE, MUSHROOM, and GIRAFFE also exhibit incomplete geometry, likely due to limitations in the generated cost volume. A comparison with RichDreamer (CVPR 2024), which uses a normal-depth diffusion model as a geometry prior, would be valuable.\n\n- Training of 3D Priors: The generalizabilty and quality of the 3D prior rely on the model for multi-view generation and cost-volume construction, raising questions about its generalizability and robustness for diverse prompts. The paper does not describe in detail how the 3D prior model is trained. The tested prompts cover a narrow range, making it difficult to assess performance across broader scenarios.\n\n- Evaluation and Analysis: The evaluation could be stronger. For example, the user study had only 20 to 30 participants, which may be too small for reliable comparisons. Additionally, the ablation study lacks quantitative metrics to substantiate the design choices.\n\n- Optimization Time: The method requires a significant optimization time, taking around 8 hours per instance, which limits its practicality for broader applications."
            },
            "questions": {
                "value": "- Clarification of the novelty.\n- clarification of  the sources of artifacts and inaccuracies in the generated geometry? These issues are visible in Fig. 1 and more pronounced in the video examples.\n- How does GeoDream compare to RichDreamer (CVPR 2024) in terms of geometric detail and model accuracy?\n- Could the authors provide a more thorough user study and ablation study to validate their approach across a broader range of metrics and prompts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}