{
    "id": "YlWvQSBCgl",
    "title": "Image Generation with Channel-wise Quantization",
    "abstract": "We present a novel image generation model with channel-wise quantization. Our method quantizes image feature along channel into discrete codes. Then based on the learned codes, our approach adopts masked-prediction paradigm for image generation. Compared with widely used spatial tokenizers, our channel-wise tokenizer has an efficient modeling for image structure and strong representational capacity. Besides, the codebook usage of our tokenizer can reach 100\\% under different codebook size. Using the channel-wise tokenizer, our generation framework achieves competitive performances on various benchmarks of image generation. In particular, on ImageNet 256x256 benchmark, our method significantly improve baseline by improving Frechet inception distance (FID) to 1.87. Furthermore, we also validate the effectiveness of our proposed method on text-to-image generation.",
    "keywords": [
        "generative models",
        "image generation",
        "visual tokenization"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=YlWvQSBCgl",
    "pdf_link": "https://openreview.net/pdf?id=YlWvQSBCgl",
    "comments": [
        {
            "title": {
                "value": "rebuttal"
            },
            "comment": {
                "value": "Thank you for your careful review and helpful suggestions!\n\n**Q1: However, channel-wise quantization is widely used in various applications, such as classification [a], LLM compression [b], and super-resolution [c].**\n\n**A1:** I disagree with this statement. The motivation and methodolgy are different. The channel-wise quantization used in [a], [b] and [c] belong to **model quatization**, which is **a technique to accelerate model inference**. While, the channel-wise quantization for image generation aims to **learn visual representation**. \n\n**Q2: Why not combine SD series for comparison? For example, SD1.5, SDXL and SD2.1.**\n\n**A2:**  As shown in section 4.3, we train our model only with MS-COCO (83K) dataset following U-ViT. However, SD series are trained with large-scale datasets. It is unfair to compare with SD series directly, due to the large data gap. Besides, to our knowledge, the FID of SDv2.1 on COCO is 12.43, refer to [1]. It lags behind our model (5.85). \n\n**Q3&Q6: More experimental results about high-resolution image generation should be provided to demonstrate the effectiveness of the proposed method. The authors only conducted experiments on MS-COCO and ImageNet datasets with the image resolution 256*256.**\n\n**A3&A6:** As shown in Table 3, we provide the class-conditional image generation on ImageNet 512x512, not only the image resolution 256x256. The results on ImageNet 512x512 demonstrate the effectiveness of the proposed method. For higher image generation, like 1024x1024, it spends much time to implement, since we need to train channel-wise tokenzier for 1024x1024. Besides, there is an option to implement image generation on 1024x1024. We can use a super resolution model to upscale image from 512x512 to 1024x1024. We leave it for future work.\n\n**Q4: The authors did not evaluate the text-image alignment between the text prompts and the synthesized images.**\n\n**A4:** We are conducting the evaluation on text-image alignment. We will update results as soon as possible.  \n\n**Q5: It is not reasonable to develop a channel-wise tokenizer, which is limited to one specific image resolution. One general channel-wise tokenizer algorithm that could be suitable for all image resolutions is favored. Such limitation heavily limits the contribution of this work.**\n\n**A5:** I disagree with this statement. First, for our method, it is an alternative to scale codebook. For codebook scaling, we conduct an experiment with codebook size 2^17=131072 without entropy regularization. The model obtains 1.33 rFID and 100\\% usage on ImageNet. This result validates the advantage of our method on codebook scaling. Second, **although a model is trained for one specific image resolution, we can train different models for any image resolution.**  Our method is not limited to one specific image resolution, as shown in Table 2 and Table 3. \n\n**Q7: Marginal performance.**\n\n**A7:** We update our results in the paper. With widely used sampling scheme, our models achieve higher performance than VAR on FID and IS scores, as shown in Table 1 and Table 2. \n\n**Q8: The best results under the same setting in Table 2 should be bold.**\n\n**A8:** Thanks for your suggestion. We have updated it in our paper.\n\n**Q9: Figure 2 is a little blur and seems that some parts are screenshots of other images.**\n\n**A9:** Thanks for your suggestion. We will update it in the final version.\n\n**Q10: What is the purpose of the image generation? Only evaluating the image quality of the synthesized images based on evaluation metrics like FID, SSIM and PSNR is not enough. Can the synthesized images promote the downstream visual perception performance such as classification?**\n\n**A10:** I disagree with this statement. In image generation field, almost all works evaluate the image quality of the synthesized images based on evaluation metrics like FID, SSIM and PSNR. Thus, it is enough to validate the effectiveness of our method using these metrics.  Using the synthesized images to promote the downstream visual perception performance is an interesting future work.\n\n[1] Dong, Runpei, et al. \"Dreamllm: Synergistic multimodal comprehension and creation.\"\u00a0arXiv preprint arXiv:2309.11499\u00a0(2023)."
            }
        },
        {
            "title": {
                "value": "rebuttal"
            },
            "comment": {
                "value": "Thank you for your careful review and helpful suggestions!\n\n**Q1: The motivation of proposing channel-wise quantization.**\n\n**A1:** As shown in section 3.2 Discussion, spatial tokenizers pay more attention to local image areas and are easily to collapse into a limited number of visual units. If image tokens concentrate on larger image areas, image tokens can be more diverse, resulting in high usage for codebook. For channel-wise tokenizer, it forces image tokens to focus on large image areas, due to its large receptive filed. Thus, channel-wise tokenizer is an alternative to codebook scaling.\n\n**Q2: The paper lacks a detailed analysis of how the channel-wise tokenizer behaves differently from spatial tokenizers.**\n\n**A2:** \n\n**Image structures:** As shown in \u201cComparisons with other image tokenizers\u201d, our tokenizer demonstrates superior results on SSIM score than spatial tokenizer. The higher SSIM score shows that our channel-wise tokenizer captures image structure. \n\n**local details:** As well known, IS score is used to evaluate the quality of generated image. Higher IS score means the more details of the generated images. As shown in Table 2 and Table 3, our models achieve higher scores on IS than all previous works. These results show that our channel-wise tokenizer captures local details. \n\n**Q3: Entropy regularization**\n\n**A3:** To validate our channel-wise quantization on codebook usage, we conduct a experiment using codebook size 2^17=131072 without entropy regularization. The model obtains 1.33 rFID and 100\\% usage on ImageNet. This result validates that channel-wise quantization is the main factor behind high codebook usage.\n\n**Q4: What does a channel token typically represent? Is it possible to visualize the learned channel tokens?**\n\n**A4:** We have add visualization of codebook tokens in Appendix A1. As shown in Figure 4, our channel-\nwise tokenizers can capture image structure and local details simultaneously. Please refer to A.1 for more details."
            }
        },
        {
            "title": {
                "value": "rebuttal"
            },
            "comment": {
                "value": "Thank you for your careful review and helpful suggestions!\n\n**Q1: Channel-wise quantization lacks theoretical justification and Intuitive rationality. Without spatial-based quantization, the model training appears to lose its causal nature**\n\n**A1:**\n\n**Theoretical justification and Intuitive rationality:** As shown in section 3.2 Discussion, the spatial tokenizers pay more attention to local image areas and are easily to collapse into a limited number of visual units. If image tokens concentrate on larger image areas, image tokens can be more diverse, resulting in high usage for codebook. For channel-wise tokenizer, it forces image tokens to focus on large image areas, due to its large receptive filed. Thus, channel-wise tokenizer is an alternative to achieve high usage of codebook. \n\n**causal nature:** I disagree with this statement. As demonstrated in MAR [1], masked generative models used by  our method are generalized autoregressive (AR) models, having causal nature. As shown in experiments, our method shows high performance on image generation. It demonstrates our image tokens are compatible with these generalized AR models. From this point, our channel-wise tokenizer do not lose causal nature.  For strict AR modeling, we leave it for future work.\n\n**Q2: As the model scales up, the number of tokens would need to increase, potentially making learning more difficult.**\n\n**A2:** We evaluate the scalability of our method from two perspectives. First, based on 16384 codebook size, we train a large model (1.0B) to validate the scalability of our method. Our model achieves 1.91 FID and 344.9 IS, outperforming VAR (2.0B) on these two metrics. Second, we conduct a experiment using codebook size 2^17=131072 without entropy regularization. The tokenizer obtains 1.33 rFID and 100\\% usage on ImageNet. This result validates the scalability of our channel-wise tokenizer on codebook size.\n\n**Q3: The resulting tokenizer becomes incompatible when image resolution changes**\n\n**A3:** **Although a model is trained for one specific image resolution, we can train different models for any image resolution.**  Our method is not limited to one specific image resolution, as shown in Table 2 and Table 3. \n\n[1] Li, Tianhong, et al. \"Autoregressive Image Generation without Vector Quantization.\"\u00a0arXiv preprint arXiv:2406.11838\u00a0(2024)."
            }
        },
        {
            "title": {
                "value": "rebuttal"
            },
            "comment": {
                "value": "**Q9: I have ignored going into details regarding quantitative results mainly because the issue of fair evaluation is not cleared yet. As of now, sometimes the model is better, sometimes worse, and it is unclear why and when one would want to choose this method over the spatial tokenization.**\n\n**A9:** We clarify the issue of fair evaluation in A1&A2. With the updated results, our method based on channel-wise tokenizer outperforms other generators, like VAR. Besides, our tokenizer can reach 100\\% codebook usage, especially 2^17 codebook size. Based on high generation performance and 100\\% codebook usage, our tokenizers is an alternative to spatial tokenization."
            }
        },
        {
            "title": {
                "value": "rebuttal"
            },
            "comment": {
                "value": "Thank you for your careful review and helpful suggestions!\n\n**Q1&Q2: Fair evaluation wrt compression ratios, more analysis on various token dimensions. The token count in Table 5 is potentially misleading, as the models use different compression ratios.**\n\n**A1&A2:** In table 5, for VAGAN and MaskGiT, they use 256x16x16 tokens (256x256). The usage of tokens is the same as Ours. The comparison is fair. For LlamaGen, it use 8x16x16 tokens. We use this type tokens for LlamaGen, due to its higher performance. If LlamaGen uses 256x16x16 tokens, the rFID is 9.21, which is worse than ours (1.64). We have updated the Table 5 to clarify this issue. \nFor Table 2, Table 3 and Table 4, the compression ratios are 16, 32, 16, respectively. Thus, tokens used by our model are all 256 (16x16). The comparison is fair.\n\n**Q3: The ablation study for token dimensions is incomplete. Experiments within the range [8, 256] are needed to understand scaling behavior better.**\n\n**A3:** We add the result of token dim 32 into Table 6 (a). In this setting, LlamaGen achieves 3.22 rFID and 20.9\\% usage. This result is worse than one with dim 8. Combining with other results, LlamaGen can not perform better than one with dim 8 in range (8, 256]. \n\n**Q4: Tokenizing\u00a0global information\u00a0along the channel dimension implies a significant correlation among tokens, meaning each image is learned as a whole rather than in parts. Consequently,\u00a0C\u00a0tokens per image are kind of memorized, making it difficult to repurpose tokens for other images due to global encoding. This also explains the high overall usage of the codebook, as tokens are not easily reusable.**\n\n**A4:** First, for our method, due to large receptive fields, image tokens are forced to capture large image areas and more diverse than ones generated by spatial tokenizers. This is the methodology of our channel-wise tokenizer. **Second, I disagree with the difficult reusable for our image tokens.** As shown in Table 6 (b), our tokenizer with 1024 codebook size can achieve 2.25 rFID. This means we can use 1024 tokens to represent 50000 images (ImageNet eval set) at least. Since an image uses 256 tokens, 50000 images means 12.8M image tokens. **If our 1024 tokens are not reusable, how to represent 12.8M image tokens?**\n\n**Q5: Adding codebook size, embedding dimensions, and compression ratios to the tables would improve comparability.**\n\n**A5:** Thanks for your suggestion. We have added some necessary information into Table 5 to clarify the fair comparison. \n\n**Q6: In Table 5, why is rFID significantly better while PSNR is worse, even though SSIM is better? I would expect a consistent trade-off between perceptual- and pixel-wise metrics.**\n\n**A6:** The reason is two folds. First, the codebook size is not large enough to represent all the details of images. When we use larger codebook size(65536), PSNR is increased to 18.72. Second, the tokens representing an image is also not large enough. As shown in Table 5, with 512 tokens, PSNR is increased to 21.47. Meanwhile, rFID and SSIM are also improved largely. The improvement on all metrics shows that there is not a consistent trade-off between perceptual- and pixel-wise metrics. \n\n**Q7: Exploring channel-wise quantization in autoregressive (AR) or diffusion tasks could be insightful.\u00a0I assume it may not perform as well for AR tasks, as the AR function would need to predict the entire global image in one step rather than progressively building it up from parts.**\n\n**A7:** I disagree with the assumption. For our channel-wise tokenizer, it forces image tokens to capture large image areas but not the whole image. If image tokens generated by our tokenizer represent the whole image, they are not reusable in other images. As shown in A4, tokens in codebook are reusable, it challenges the assumption. \n\n**Q8: The claims regarding low similarity between channel tokens, efficient modeling of image structure, and strong representational capacity lack clear definition and verification.**\n\n**A8:**\n\n**low similarity between channel tokens** \n\nThis is validated by our 100\\% codebook usage under different codebook size, especially 2^17 codebook size. If there is high similarity between channel tokens, codebook usage can not reach 100\\%.\n\n**efficient modeling of image structure**\n\nThis is validated by high SSIM score, demonstrated in Table 5. SSIM is sensitive to image structure changes. High SSIM means efficient modeling of image structure. We have discussed it in \u201cComparisons with other image tokenizers\u201d.\n\n**strong representational capacity**\n\nFirst, for image reconstruction, our tokenizer outperforms baseline (LlamaGen) on rFID and codebook usage, as shown in Figure 1, Table 5 and Table 6 (a). Second, for image generation, generators with our tokenizer outperform other generators, like VAR. These both demonstrate strong representational capacity of our tokenizer."
            }
        },
        {
            "title": {
                "value": "Overall Rebuttal"
            },
            "comment": {
                "value": "We thanks all the reviewers for their hard work! We are very honored that our work has been recognized for:\u00a0**1) interesting approch(Reviewer Xmae, xTUG); 2) novel idea (Reviewer xTUG); 3) clearly written (Reviewer Xmae, FnTD, xTUG, 6xWH).**\n\n## Updated Performance Results\nWe find MaskGiT sampling used by our method is out-of-date and is not compatible with our method. Thus, we adopt the guidance schedule from MDTv2 [1]. This guidance schedule is widely used by MagViT-v2, TiTok[2] and MaskBit[3]. Thus, it is fair to use the updated performance results for comparison.  We have updated performance results in the paper. Here, we list some critical results to show the effectiveness of our method. **Our method outperforms VAR on FID and IS by a large margin, especially on ImageNet 512x512**\n\n**ImagNet 256x256**\n\n+: codebook size is 65536.\n\n|   Models    | FID | IS | #Para| #step|\n| ----------- | ----------- |----------- |----------- |----------- |\n|VAR-d30 | 1.97 | 334.7 | 2.0B | 10 |\n|MagViT-2 | **1.78** | 319.4 | 307M | 64 |\n|Ours-B | 2.77 | 305.3 | 305M | 10 |\n|Ours-L | 2.46 | 302.5 | 634M | 10 |\n|Ours-H |2.39 |338.2 | 1.0B | 10|\n|Ours-B* | 2.21 | 301.2 | 305M | 64 |\n|Ours-L* | 2.02 | 323.4 | 634M | 64|\n|Ours-H* |1.91 | **344.9**| 1.0B | 64 |\n|Ours-L+ | 1.87 | 320.4 | 634M | 64 |\n\n**ImagNet 512x512**\n|   Models    | FID | IS | #Para| #step|\n| ----------- | ----------- |----------- |----------- |----------- |\n|VAR-d36-s | 2.63 | 303.2 | >2.0B | 10 |\n|MagViT-2 | **1.91** | 324.3 | 307M | 64 |\n|Ours-B | 2.68 | 318.5 | 305M | 10 |\n|Ours-L  | 2.46 | 336.4 | 634M | 10 |\n|Ours-B* | 2.22 | 323.4 | 305M | 64 |\n|Ours-L* | 2.01 | **341.5**| 634M | 64|\n\n**MS-COCO 256x256**\n|   Models    | FID | #step|\n| ----------- | ----------- |----------- |\n|U-ViT-S/2 | 5.95 | - |\n|Ours-L | 6.40 | 10 |\n|Ours-L*  | **5.85** | 64 |\n\n[1] Gao, Shanghua, et al. \"Masked diffusion transformer is a strong image synthesizer.\"\u00a0Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[2] Yu, Qihang, et al. \"An Image is Worth 32 Tokens for Reconstruction and Generation.\"\u00a0arXiv preprint arXiv:2406.07550\u00a0(2024).\n\n[3] Weber, Mark, et al. \"Maskbit: Embedding-free image generation via bit tokens.\"\u00a0arXiv preprint arXiv:2409.16211\u00a0(2024).\n\n## Methodology\nAs shown in section 3.2 Discussion, the spatial tokenizers pay more attention to local image areas and are easily to collapse into a limited number of visual units. If image tokens concentrate on larger image areas, image tokens can be more diverse, resulting in high usage for codebook. For our method, due to large receptive fields, image tokens are forced to capture large image areas and more diverse than ones generated by spatial tokenizers. Meanwhile, for reconstruct images in high quality, our tokenizers must learn local details. We have add visualization of codebook tokens in Appendix A1. As shown in Figure 4, our channel-wise tokenizers can capture image structure and local details simultaneously. Please refer to A.1 for more details."
            }
        },
        {
            "summary": {
                "value": "This paper provides an interesting perspective on image compression through learning a vector quantized autoencoder. Typically, we tokenize images spatially. That is, every (i,j) for i over h and j over w within an hxwxc latent space, each position is mapped to a token in a learned codebook. In contrast, this paper proposes to tokenize along the channel dimension of the latent space within the AE. This results in tokens that capture the global image along a latent channel dimension. Results are conducted on class conditional and text conditional image synthesis with various ablations and analysis."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper tackles an interesting approach to tokenize along the channel dimension within a VQ AE for images.\n- The paper is well-structured and written.\n- The approach is easy to understand and to follow.\n- Comparison against many relevant models (albeit not exactly fairly if I understood correctly; see below for details)."
            },
            "weaknesses": {
                "value": "My main concerns are regarding fair evaluation wrt compression ratios, more analysis on various token dimensions and exploration of usage of channel wise tokenization in downstream diffusion and AR tasks.\n\n- Since each token in channel space captures complete global information, configurations such as [4,64,64] result in only four tokens of size 64x64 each, rather than 64x64 tokens of size 4. This impacts discussions on code embedding size and codebook use, which are misleading because they affect the compression ratio. I suggest comparing against a fixed compression budget with various compression ratios rather than focusing solely on code embedding sizes (which does not give the full picture).\n- Adding to the above, the token count in Table 5 is potentially misleading, as the models use different compression ratios. If I understand correctly, the comparison is between VQAN using 4x16x16 tokens (4x256) and the proposed model with 256x16x16 tokens (256x256).\n- The ablation study for token dimensions is incomplete. Experiments within the range [8, 256] are needed to understand scaling behavior better.\n- Tokenizing *global information* along the channel dimension implies a significant correlation among tokens, meaning each image is learned as a whole rather than in parts. Consequently, *C* tokens per image are kind of memorized, making it difficult to repurpose tokens for other images due to global encoding. This also explains the high overall usage of the codebook, as tokens are not easily reusable.\n- Adding codebook size, embedding dimensions, and compression ratios to the tables would improve comparability.\n- In Table 5, why is rFID significantly better while PSNR is worse, even though SSIM is better? I would expect a consistent trade-off between perceptual- and pixel-wise metrics.\n- Exploring channel-wise quantization in autoregressive (AR) or diffusion tasks could be insightful. I assume it may not perform as well for AR tasks, as the AR function would need to predict the entire global image in one step rather than progressively building it up from parts.\n- The claims regarding low similarity between channel tokens, efficient modeling of image structure, and strong representational capacity lack clear definition and verification.\n- I have ignored going into details regarding quantitative results mainly because the issue of fair evaluation is not cleared yet. As of now, sometimes the model is better, sometimes worse, and it is unclear why and when one would want to choose this method over the spatial tokenization."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an alternative to standard VQ-VAE, which typically quantizes each spatial position as a token. Instead, this work quantizes image features along the channel dimension into discrete codes.\n\nFor comparison, in a standard VQ-VAE with a final feature dimension of C*H*W, there would be H*W tokens, whereas this work produces C tokens.\n\nAdvantages:\n1. The paper is clearly written and easy to understand\n2. The metrics appear reasonable\n\nDisadvantages:\n1. Channel-wise quantization lacks theoretical justification and Intuitive rationality.\n   Without spatial-based quantization, the model training appears to lose its causal nature\n2. As the model scales up, the number of tokens would need to increase, potentially making learning more difficult\n3. The resulting tokenizer becomes incompatible when image resolution changes\n\nI believe the motivation behind this idea is fundamentally flawed, which leads to several issues:\n- Limited generalizability\n- Sequence length problems\n- Modeling methodology concerns\n\nTherefore, I lean towards a negative assessment of this work."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Advantages:\n1. The paper is clearly written and easy to understand\n2. The metrics appear reasonable"
            },
            "weaknesses": {
                "value": "Disadvantages:\n1. Channel-wise quantization lacks theoretical justification and Intuitive rationality.\n   Without spatial-based quantization, the model training appears to lose its causal nature\n2. As the model scales up, the number of tokens would need to increase, potentially making learning more difficult\n3. The resulting tokenizer becomes incompatible when image resolution changes\n\nI believe the motivation behind this idea is fundamentally flawed, which leads to several issues:\n- Limited generalizability\n- Sequence length problems\n- Modeling methodology concerns\n\nTherefore, I lean towards a negative assessment of this work."
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose a new image tokenization method called channel-wise quantization, which quantizes image feature along channel. Then, based on the learned tokenizer, the authors use masked-prediction paradigm similar to MaskGIT to generate images. Experiments show that the proposed method achieves competitive performances compared to other image tokenizers and generative models. Besides, the proposed method can reaches 100% codebook usage under different codebook size."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The writing is clear and easy to follow.\n+ The idea of channel-wise quantization is novel and interesting.\n+ The reconstruction ability (especially rFID and SSIM) is greatly improved compared to spatial tokenizers."
            },
            "weaknesses": {
                "value": "- The motivation of proposing channel-wise quantization is not very strong to me. While spatial tokenizers often suffer from low codebook usage and reduced code embedding dimension limits expressive ability, it's unclear how these issues lead to the design of channel-wise quantization.\n\n- The paper lacks a detailed analysis of how the channel-wise tokenizer behaves differently from spatial tokenizers. For example, the authors claim that channel-wise tokens capture both global structures and local details, but there are no direct experiments to support this. It would be interesting if the authors could visualize the learned channel-wise tokens and discuss each channel's representation, so that we can have a deeper understanding of the channel-wise tokenizer.\n\n- The authors attribute the 100% codebook usage to the nature of channel-wise quantization. However, I note that entropy regularization, which is known to be helpful for increasing codebook usage, is adopted in codebook learning. Additionally, the compared method LlamaGen did not use entropy regularization. Thus, I'm not sure if channel-wise quantization is the main factor behind high codebook usage."
            },
            "questions": {
                "value": "+ As mentioned in weaknesses part, will the channel-wise quantization still reach 100% codebook usage without entropy regularization?\n\n+ What does a channel token typically represent? Is it possible to visualize the learned channel tokens?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents a novel image generation model that utilizes channel-wise quantization to convert image features into discrete codes along the channel dimension, adopting a masked prediction paradigm for image generation. This approach offers efficient modeling of image structures and strong representational capacity, outperforming or matching state-of-the-art methods on the ImageNet benchmark. The authors also performed the text-to-image generation and demonstrated transferability to text-to-image generation on the COCO dataset. The contributions of this work include a simple yet effective visual tokenizer with 100% codebook usage and a generation framework based on channel-wise quantization for image generation tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper is easy to follow and includes comprehensive experiments to demonstrate the effectiveness of the proposed method. It proposes novel channel-wise quantization, which offers efficient modeling of image structures and strong representational capacity. It also proposed a simple yet effective tokenizer with 100% codebook usage. These features enable the whole image generation framework to achieve superior or comparable performance to state-of-the-art methods across various image generation tasks."
            },
            "weaknesses": {
                "value": "1) This paper's biggest contribution is proposing a channel-wise quantization for image generation. However, channel-wise quantization is widely used in various applications, such as classification [a], LLM compression [b], and super-resolution [c].\n\n[a] RDO-Q: Extremely Fine-Grained Channel-Wise Quantization via Rate-Distortion Optimization. ECCV 2022.\n\n[b] OutlierTune: Efficient Channel-Wise Quantization for Large Language Models. Arxiv 2024.\n\n[c] DAQ: Channel-Wise Distribution-Aware Quantization for Deep Image Super-Resolution Networks. WACV 2022.\n\n2) Why not combine SD series for comparison? For example, SD1.5, SDXL and SD2.1.\n\n3) More experimental results about high-resolution image generation should be provided to demonstrate the effectiveness of the proposed method.\n\n4) The authors did not evaluate the text-image alignment between the text prompts and the synthesized images. \n\n5) It is not reasonable to develop a channel-wise tokenizer, which is limited to one specific image resolution. One general channel-wise tokenizer algorithm that could be suitable for all image resolutions is favored. Such limitation heavily limits the contribution of this work.\n\n6) The authors only conducted experiments on MS-COCO and ImageNet datasets with the image resolution 256*256. \n\n7) Marginal performance. Compared with VAR, the proposed method only shows very marginal performance improvement (or even a slight performance drop) under the same experimental setting: similar network parameters and inference step from Table 2.\n\nMinor issues:\n\n1) The best results under the same setting in Table 2 should be bold. \n\n2) Figure 2 is a little blur and seems that some parts are screenshots of other images."
            },
            "questions": {
                "value": "What is the purpose of the image generation? \n\nOnly evaluating the image quality of the synthesized images based on evaluation metrics like FID, SSIM and PSNR is not enough. Can the synthesized images promote the downstream visual perception performance such as classification?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}