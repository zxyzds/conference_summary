{
    "id": "BPAZ6yW3K7",
    "title": "Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval",
    "abstract": "Mitigating hallucinations is a prerequisite for trusting answers generated by large language models (LLMs) that are prone to making convincing but inaccurate claims. Grounding the answers in data generated and verified by humans provides a natural avenue for improving the reliability of LLMs. However, it can be hard to capture relevant facts for user questions based on just the semantic similarity, especially as questions becomes more complex and the relevant facts become more indirect. What if LLMs could query for relevant facts based on the user question? While this can enable retrieving relevant but indirect facts, zero-shot performance of instruction-tuned LLMs leaves more to be desired and generating supervision on how to retrieve relevant facts can be expensive and retriever dependent. Our key insight is that LLMs can learn to retrieve relevant facts by $\\textit{trying}$ different queries, learning to upweight queries that result in relevant facts. This leads to our reinforcement learning based framework, $\\underline{Le}$arning to $\\underline{Re}$trieve by $\\underline{T}$rying (LeReT), where the LLM generates queries for multi-hop retrieval and uses preference-based reinforcement learning to improve the LLM queries. Our experimental results demonstrate that LeReT can improve the absolute retrieval accuracy by up to 29\\% and the downstream generator evaluations by 17\\%. The simplicity and flexibility of LeReT allows it to be applied to arbitrary retrievers, and makes it a promising technique for improving general LLM pipelines.",
    "keywords": [
        "LLMs",
        "Reinforcement Learning",
        "Information Retrieval"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=BPAZ6yW3K7",
    "pdf_link": "https://openreview.net/pdf?id=BPAZ6yW3K7",
    "comments": [
        {
            "summary": {
                "value": "In multi-hop question answering, a model needs to perform multiple retrieval steps before arriving at an answer.  Since the reward (answer) is not known until the last step, this problem lends itself well to reinforcement learning (RL).  The paper proposes to optimize one component (the question generator for retrieval) in the multi-hop QA pipeline using RL.  The method uses direct supervision (gold retrieved docs) to first train a reward function.  Then a diverse set of queries are sampled from the model using varied prompts.  The rewards for these generations are fed into an RL algorithm (IPO) to improve the query generator.  The method improves substantially on pure SFT methods on HotpotQA and HoVer."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Well written paper, clear presentation\n* Results are strong and convincingly support the claim that using RL to improve query generator works better than pure SFT."
            },
            "weaknesses": {
                "value": "* A bit difficult to judge the novelty of the contribution.  Has similar methods been used for single-hop QA or RAG in general?  If so, the novelty here might be marginal, especially since the paper relies on direct supervision for the reward model.\n* The two multi-hop datasets used are not natural and somewhat out-of-date.  It would be great to see the methods usefulness on more relevant tasks and benchmarks.  The long-form generation attempt in the appendix is interesting, and could perhaps be developed more and moved into the main text?"
            },
            "questions": {
                "value": "* Has similar methods been used for single-hop QA or RAG in general?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a framework called \"Learning to Retrieve by Trying\" (LeReT), which aims to improve the grounding of large language models (LLMs) through reinforcement learning (RL)-based retrieval. LeReT enables LLMs to generate queries, learn from trial-and-error, and enhance retrieval by using diverse few-shot prompts combined with preference-based reinforcement learning. LeReT's flexibility makes it adaptable across different retrieval systems, with potential applicability in broader retrieval-augmented generation (RAG) contexts."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Introduces a unique reinforcement learning framework to improve retrieval accuracy in LLMs, especially for complex multi-hop queries.\n2. Demonstrates the effectiveness of iterative training in enhancing the retrieval and grounding abilities of LLMs.\n3. Compatible with various retrieval systems, including ColBERTv2 and Azure AI Search, indicating its broad applicability."
            },
            "weaknesses": {
                "value": "1. Primarily relies on direct supervision for labeling relevant documents, which may limit its scalability in cases where explicit relevance labels are unavailable.\n2. Requires extensive computation due to multi-hop retrieval and diverse query sampling, making it resource-intensive.\n3. The need for sampling across multiple hops is computationally intensive and less parallelizable, reducing scalability."
            },
            "questions": {
                "value": "1. While iterative training improves retrieval accuracy, does it potentially overfit the model to specific multi-hop tasks, thereby impacting generalizability in other retrieval-augmented scenarios?\n2. What is involved in adapting LeReT to new domains or types of queries that it was not originally trained on?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents LeReT, a framework for improving LLM retrieval and answer grounding using reinforcement learning. It uses prompt-driven diverse query generation, model optimization with preference-based RL, and reward labeling for retrieved documents. Experiments on HotpotQA and HoVer datasets show significant improvements in retrieval and downstream generation compared to baselines. Analysis reveals the importance of diverse few-shot prompting and LeReT's applicability across different retrievers. Limitations include the use of direct supervision, and future work may focus on indirect supervision and tool updating."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- LeReT is applicable to general retrieval-augmented generation (RAG) systems and can adapt to different retrievers.\n- LeReT significantly improves retrieval accuracy. Compared to the unadopted Llama and Gemma instruction models, the recall rate increases by 9-22% on HotPotQA and 27-29% on HoVer.\n- It can be used iteratively: applying LeReT for two iterations shows that the model performance after the second iteration is better than that of the standard non-iterative LeReT."
            },
            "weaknesses": {
                "value": "- The novelty assertion of the proposed method lacks clarity. Regarding the related work spanning from line 139 to 148, it remains ambiguous as to how the proposed method differentiates itself from those other methods. I comprehend that the proposed approach employs diverse query generation and IPO for preference learning. However, these seem to be more of incremental enhancements within an existing framework rather than representing a distinct novelty.\n- Also, the experiments lack comparisons with the most relevant recent works. Only the basic few-shot prompt baseline is compared."
            },
            "questions": {
                "value": "- Line 76 \"If LLMs can observe the retrieved documents for different search queries, they can learn which queries lead to better outcomes.\" What supports this claim? And if this is true, then why do you need the direct or indirect supervision to teach LLM \"how to query\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses the issue of hallucination in language models by enhancing their capability to generate effective queries for retrieving relevant facts, thereby improving the accuracy of model responses. The proposed approach, LeReT, generates diverse search queries by incorporating few-shot examples. It constructs comparative queries guided by the supervision of retrieval quality and optimizes the model using the IPO algorithm. The study empirically validates the LeReT framework, exploring various methods of collecting retrieval rewards and assessing performance across different retrievers."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The motivation that enhancing RAG performance through learning to generate better search queries is clear and reasonable.\n2. The experiments present the improvement in both retrieval quality and finally performance, which validate the soundness of proposed algorithm."
            },
            "weaknesses": {
                "value": "1. The writing and presentation of results in this work could be enhanced. On line 254, I think it should be, \"In this work, we consider XXX.\"\n\n2. Some baselines (https://aclanthology.org/2023.emnlp-main.585.pdf, https://openreview.net/pdf?id=vDvFT7IX4O, and https://aclanthology.org/2023.acl-long.99.pdf) could be added to both the experiments and the related works sections. Although these works may not directly focus on multi-hop retrieval, they fit broadly within the same topic as this work, i.e., \"query expansion.\" These works focus on generating better search queries to enhance RAG performance.\n\n3. Relying on annotated golden documents, referred to as \"direct supervision\" in this work, limits the vision of the study. In more general scenarios, collecting \"indirect supervision\" is more feasible. Moreover, signals from \"indirect supervision\" are the ultimate indicators for downstream tasks."
            },
            "questions": {
                "value": "1. On lines 218\u2013219, are there any concrete examples illustrating the differences between BFRS-generated queries and high-temperature sampling queries?\n\n2. Regarding line 260, what are the examples used in few-shot prompting? Why is few-shot prompting not feasible during testing? Is it due to considerations of inference efficiency?\n\n3. As for section 4.3, are there any details about how rewards are computed?\n\n4. For Iterative-LeReT and the discussion in Section 4.3, this multi-hop search process seems compatible with online RL training, such as using PPO, by incorporating process rewards at each hop and a final reward for the conclusion. Have the authors explored online RL training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}