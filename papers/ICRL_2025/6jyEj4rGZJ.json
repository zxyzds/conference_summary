{
    "id": "6jyEj4rGZJ",
    "title": "GroundingBooth: Grounding Text-to-Image Customization",
    "abstract": "Recent studies in text-to-image customization show great success in generating personalized object variants given several images of a subject. While existing methods focus more on preserving the identity of the subject, they often fall short of controlling the spatial relationship between objects. In this work, we introduce GroundingBooth, a framework that achieves zero-shot instance-level spatial grounding on both foreground subjects and background objects in the text-to-image customization task. Our proposed text-image grounding module and masked cross-attention layer allow us to generate personalized images with both accurate layout alignment and identity preservation while maintaining text-image coherence. With such layout control, our model inherently enables the customization of multiple subjects at once. Our model is evaluated on both layout-guided image synthesis and reference-based customization tasks, showing strong results compared to existing methods. Our work is the first work to achieve a joint grounding on both subject-driven foreground generation and text-driven background generation. Our code will be publicly available.",
    "keywords": [
        "Subject-Driven Generation; Text-to-image Customization; Diffusion Models",
        "Vision-Language",
        "Grounding"
    ],
    "primary_area": "generative models",
    "TLDR": "About gounding text-to-image customization",
    "creation_date": "2024-09-21",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=6jyEj4rGZJ",
    "pdf_link": "https://openreview.net/pdf?id=6jyEj4rGZJ",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a framework which allows users to customize an image by 1) specifying the position (layout) of the object, and 2) providing a reference image of the object. It supports either single object customization or multi-object customization. They design a grounding module to ground the provided image with text entities. The produced grounding tokens are then later used as the condition in their diffusion model to generate the final image. They conduct experiments on Dreambench and MS-COCO and show that their methods could produce high quality image while preserving the detail of the user-specified (reference) images."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The visualization results show that the proposed method can effectively preserve the identity of reference image while generating plausible images.\n2. The proposed method is able to simultaneously handle multi-object synthesis even with complex layout."
            },
            "weaknesses": {
                "value": "1. My main concern is that the authors claim that they are able to ground the text entities during generation. While the CLIP-T score of the model indicates that the generated image is less coherent with the text comparing to other baseline methods.\n2. While the paper claimed that they can control the spatial relationship between objects. It is difficult to evaluate this argument given the layouts are pre-determined.\n3. How are the metrics computed? For example, when computing the CLIP-I score, do you only consider the image similarity between the reference object and the corresponding region in the generated image? If so, how do you extract the corresponding region? More details of how the metrics are computed (CLIP-I, DINO, CLIP-T) could improve the clarity of the paper."
            },
            "questions": {
                "value": "1. For multi-objects cases, is each box in the layout assigned to an associated object label?\n2. In your experiments, are all the layouts pre-determined or only the layout of the reference object is given?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes GroundingBooth, a model for grounded text-to-image customization. It aims to place subjects received in input (marked with bounding-boxes in images) in new backgrounds (described in the prompt), while maintaining the identity and spatial location of the subjects. The authors show GroundingBooth is capable of generating complex requests while preserving the subjects in the input images (e.g., \u201ca [stuffed animal] and a [vase] with [plant] and [vintage lantern] on a quaint balcony\u201d)\n\nGroundingBooth incorporates a new Masked Cross Attention module in each block of the U-Net (Stable Diffusion 1.4\u2019s). In addition to input from the existing Cross Attention layer, the masked layer receives as input DINO-2 features of the subject images received in the input. GroundingBooth is trained this way on a dataset curated from MVImgNet. \n\nFinally, the method is tested and compared to a few existing baselines, using automatic measurements such as CLIPScore and DINO, and a human study."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The paper is well written and presented nicely\n* The method improves over the baselines it does test (see first weakness)\n* Such model can be useful in many real-life applications"
            },
            "weaknesses": {
                "value": "* The paper does not cover \u201cBreak-A-Scene: Extracting Multiple Concepts from a Single Image\u201d by Avrahami et al (2023). In this work, they extract concepts from an image using textual inversion, and use it to embed them in new images. They too work with masks and can even accept them from the user as input. This is especially important since the sentence before last in the abstract states \u201cOur work is the first work to achieve a joint grounding of both subject-driven foreground generation and text-driven background generation\u201d, which makes this imprecise. More importantly, the difference between these projects should be clearly stated. What does this work do that Break-A-Scene does not? \n\n* The use of Fourier embedding should be explained. What makes it suitable to this task?"
            },
            "questions": {
                "value": "* Why does this method use SD-1.4 when there are so many newer / stronger models? Is there some limitation in using them?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces GroundingBooth, a novel framework designed to enhance text-to-image customization by enabling precise spatial control of both subjects and background elements based on textual prompts. While existing models in text-to-image generation maintain subject identity, they often lack control over spatial relationships. GroundingBooth addresses this gap by implementing zero-shot instance-level spatial grounding, enabling precise placement of both foreground subjects and text-defined background elements.\nGroundingBooth supports complex tasks such as multi-subject customization, where multiple subjects and background entities are positioned according to input bounding boxes. Experimental results demonstrate its effectiveness in layout alignment, identity preservation, and text-image alignment, outperforming current approaches in controlled image generation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Unlike many existing layout-guided image generation methods that handle only single subjects, GroundingBooth supports multi-subject customization. This versatility broadens its applicability, especially for generating images where complex layouts and multiple subjects are essential."
            },
            "weaknesses": {
                "value": "1. InstanceDiffusion does not exist in baseline comparisons. Despite its notable relevance with capabilities for free-form language conditions per instance and flexible instance localization methods (single points, scribbles, and bounding boxes), InstanceDiffusion is missing from both our quantitative and qualitative baselines. \n2. FID, in contrast to other works dealing with similar tasks, is not suggested in this paper.\n3. Qualitative results demonstrating the model's performance on multi-subject generation tasks are notably absent from this paper."
            },
            "questions": {
                "value": "1. Previous research in layout-guided diffusion has demonstrated limitations in maintaining visual coherence when objects exhibit diverse textures. While these approaches often resulted in disharmonious image generation, our proposed method provides users with the capability to directly select and manipulate subjects. A comparative analysis with InstanceDiffusion would be particularly valuable, especially in terms of texture consistency and user control capabilities.\n2. Due to the lack of publicly available code and data, an accurate evaluation is difficult to conduct.\n3. It remains unclear why the paper emphasizes its zero-shot capability as a key strength even though the methodology clearly includes training procedures within the paper.([L37-40] & [L247-250])"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents GroundingBooth, a method for grounded text-to-image customization. Given a list of subject entities represented by images and text entities represented by textual descriptions, along with bounding-box locations, GroundingBooth aims to generate an image containing all subjects in the specified locations according to their bounding boxes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The authors tackle the important task of grounded image generation with both text and image localization conditions.\n* The writing is clear, making it easy to understand the proposed method.\n* The authors combine grounded generation from both reference objects and textual inputs within a single architecture, which is highly relevant for many applications.\n* The authors evaluate their method against a variety of prior works and datasets."
            },
            "weaknesses": {
                "value": "* In all the qualitative examples, the generated objects remain in the same pose as in the input image, despite the claim in line 191: \u201cMoreover, our work adaptively harmonizes the poses of the reference objects and faithfully preserves their identity.\u201d Could you provide examples where the input subjects change their pose while maintaining their identity? I would like to see examples where the prompt requires a significant pose change from the input subject.\n\n* The proposed Masked Cross-Attention module was presented in previous works; see, for instance:\n[1] Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation, Dahary et al. ECCV 2024\n[2] InstanceDiffusion: Instance-level Control for Image Generation, Wang et al. CVPR 2024\n\n* Overall, the proposed modules seem to lack novelty. The gated self-attention mechanism is borrowed from GLIGEN, and the masked cross-attention module exists in prior work, such as in [1].\n\n* I find the distinction between \u201cbackground\u201d and \u201cforeground\u201d objects confusing, as it actually separates objects based on their source (image or text) rather than their position in the background or foreground of the image.\n\n* The quantitative results are not convincing, as GroundingBooth shows lower scores than prior work on several metrics (e.g., Tables 1 and 2)."
            },
            "questions": {
                "value": "* For personalization of a single subject (Fig. 4, Table 1), how is the bounding box determined? How do you compare with methods that do not require a bounding box as input?\n* How well can the method generate interactions between input subjects? For example, could it make the teddy bear wear the red backpack?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on improving the accurate generation of spatial relationships between objects and backgrounds when creating personalized object variants. Technically, the authors propose a joint text-image grounding module that encourages both foreground subjects and background objects to adhere to locations defined by input bounding boxes. They also introduce a masked cross-attention layer aimed at preventing the unintended blending of multiple visual concepts in the same location, producing clear, distinct objects. Experiments are conducted on the MVImgNet and LVIS datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper tackles the task of generating personalized objects based on specific locations, which is an interesting setup.\n2. This work proposes integrating reference objects and their location prompts through a grounding module and masked cross-attention.\n3. Experiments are conducted on two benchmarks, accompanied by illustrative visualizations."
            },
            "weaknesses": {
                "value": "1. The paper primarily focuses on enabling the location-controlled generation of personalized objects, a setting already explored in prior work [3], which the authors seem to overlook. Additionally, the authors introduce a rather complex module to integrate location information but seem to lose focus on core functionalities like layout-to-image generation or personalized object generation.\n2. Missing References: Some relevant references in layout-to-image generation, such as [1,2] and subject-driven image generation [4], are absent.\n3. There are some limitations in model design. For example, the authors note that in cases where bounding boxes belong to the same class, the model cannot distinguish between a bounding box for a reference object and one for a text entity, leading to misplacement of the reference object. However, the paper does not clarify whether or how the proposed masked cross-attention module addresses this issue.\n4. Further analysis is needed on topics such as the maximum number of reference objects supported in a single input and the model\u2019s performance on subject-driven image generation without layout information.\n\n\n[1] LayoutGPT: Compositional Visual Planning and Generation with Large Language Models     \n[2] Layoutllm-t2i: Eliciting layout guidance from llm for text-to-image generation    \n[3] Training-Free Layout Control With Cross-Attention Guidance    \n[4] Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing"
            },
            "questions": {
                "value": "1. Does this work support simpler text-to-image generation, layout-to-image, or personalization tasks?\n\n2. Regarding the illustration of the masked cross-attention layer in Figure 2, is the number of layers determined by the number of reference objects? For example, if there are three reference objects in the input, does that mean three masked cross-attention modules are required? If so, this model design seems unreasonable. Sequential masking could result in information loss in subsequent modules, especially when reference objects have significant overlap."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}