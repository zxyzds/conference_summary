{
    "id": "QDNUuB5DeO",
    "title": "Disentangled interleaving variational encoding",
    "abstract": "Conflicting objectives present a considerable challenge in interleaving multi-task learning, necessitating the need for meticulous design and balance to ensure effective learning of a representative latent data space across all tasks without mutual negative impact. Drawing inspiration from the concept of marginal and conditional probability distributions in probability theory, we design a principled and well-founded approach to disentangle the original input into marginal and conditional probability distributions in the latent space of a variational autoencoder. Our proposed model, Deep Disentangled Interleaving Variational Encoding (DeepDIVE) learns disentangled features from the original input to form clusters in the embedding space and unifies these features via the cross-attention mechanism in the fusion stage. We theoretically prove that combining the objectives for reconstruction and forecasting fully captures the lower bound and mathematically derive a loss function for disentanglement using Na\u00efve Bayes. Under the assumption that the prior is a mixture of log-concave distributions, we also establish that the Kullback-Leibler divergence between the prior and the posterior is upper bounded by the cross entropy loss, informing our adoption of radial basis functions (RBF) and cross entropy with interleaving training for DeepDIVE to provide a justified basis for convergence. Experiments on anonymous bidding data from the National Electricity Market of Singapore (NEMS) show that DeepDIVE disentangles the original input and yields more accurate forecasts, outperforming current state-of-the-art baselines. In the context of the power market, this study can enhance operational decisions and bidding strategies by offering insights into the embedded supply curve via the representation space.",
    "keywords": [
        "Representation Learning",
        "Disentanglement",
        "Probability Theory",
        "Mathematical Optimization",
        "Variational Autoencoder",
        "Time Series Forecasting"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "Deep Disentangled Interleaving Variational Encoding (DeepDIVE) provides mathematical basis for learning disentangled embedding space in interleaving multi-task learning.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=QDNUuB5DeO",
    "pdf_link": "https://openreview.net/pdf?id=QDNUuB5DeO",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a novel framework for enhancing the interpretability of representations learned from time-series prediction. The authors derive a variational evidence lower bound (ELBO) that extends the original VAE's ELBO to accommodate time-series forecasting scenarios, along with theoretical and implementation details for optimizing this learning objective. Specifically, the latent space is designed into separate marginal (denoted by \"b\") and conditional (denoted by \"a\") distributions that contribute to the calculation of the log-likelihood of both the look-back window and the forecasting window. Experiments are conducted on two time-series datasets, and both qualitative visualizations and quantitative metrics are evaluated for the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "(1) The detailed theoretical derivations of the proposed ELBO for time-series forecasting are solid and make sense, and it could be promising and inspiring for future research on learning disentangled representations in time-series prediction tasks. \n\n(2) I appreciate the authors' efforts to report the variance (standard deviation) of the model's performance, which allows readers to better evaluate the results and comparisons."
            },
            "weaknesses": {
                "value": "The main weakness of this paper lies in its writing and evaluations. \n\n(1) The writing lacks consistency between the text, figures, and equations. For example: (1a) While the inference and KL divergence for q(b\u2223x) is well-discussed, I had difficulties to find any description for inferring q(a\u2223x,b). In Fig. 1, it appears that a is directly computed from x, which contradicts the equation, and I couldn't find any other text to explain this. (1b) Although the \"cross-attention mechanism\" and \"fusion stage\" are mentioned in the abstract and introduction as part of this paper's contributions, they are never described or explained in the rest of this paper. It appears that the proposed framework is implemented just using an auto-encoder architecture, as suggested by Fig. 1 and line 412. Additionally, in line 518, the authors state, \"Unlike attention maps and convolutional neural network (CNN) feature maps, DeepDIVE presents data representations in ...,\" which further increases the confusion about whether the proposed framework includes cross-attention. (1c) There are no descriptions or explanations in the main text for Fig. 3, and the figure caption is also limited.\n\n(2) Although the combined effects of a and b in the latent space are evaluated in Fig. 2, separate evaluations and comparisons of a and b are missing, which I believe is critical for evaluating the proposed framework. Readers may be interested in understanding the differences between the representations learned in a and b, and how using separate a and b is beneficial compared to using just one latent variable. The authors are encouraged to conduct comprehensive experiments to demonstrate the representations learned in the marginal and conditional distributions in the latent space, separately, and how they impact the final prediction tasks. For example, the authors could consider using t-SNE plots for a and b individually and presenting traversal results for both of them to identify any expected differences."
            },
            "questions": {
                "value": "(1) Total correlation is also an option for promoting the independence of the latent variables. Besides using Naive Bayes, do the authors have any experience or insights in deriving and optimizing total correlation terms within the KL divergence? Additionally, how would this affect the model performance compared to using Naive Bayes? For instance, one approach for optimizing total correlation can be found in [1].\n\n[1] Chen, Ricky TQ, et al. \"Isolating sources of disentanglement in variational autoencoders.\" Advances in neural information processing systems 31 (2018)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose to extend the use of Variational Autoencoders to generative forecasting and develop a novel objective for training."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The loss function seems to be novel and theoretically grounded."
            },
            "weaknesses": {
                "value": "- Presentation: The paper is not well-written, it is difficult to follow and understand the idea.\n    - The abstract is not specific. Multiple concepts such as multi-task learning, disentanglement, Naive Bayes, and other technical details are presented without a clear, coherent relationship among them. I suggest focusing on a central contribution, such as developing a non-conflicting objective for multi-task learning, and clarifying how elements like disentanglement, Naive Bayes, cross-entropy, and RBF specifically contribute to this goal.\n    - Line 46 of the introduction states model explainability to be the key contribution. However, the abstract does not discuss about explainability. I suggest emphasizing a central contribution and ensuring that all concepts throughout the paper align with it.\n    -  Line 50 of the introduction mentions conflicting objectives, but a more detailed discussion of how multiple objectives in multi-task learning may conflict and providing examples of these conflicting objectives in the context of the paper would make this concept clearer.\n- Baselines: No discussion or reference of the comparison baselines in Table 2 are AUTOCTS(-KDF/KDP), DsaNet, and MtGnn provided. Could the authors provide a brief discussion on these baselines and why they were considered for comparison? Also, why were more recent baselines such as DeepGLO, TCN, and TLAE (as discussed in section 2.1) not included in the comparison?"
            },
            "questions": {
                "value": "- I couldn't find the paper referenced in section 2 (Wong et. al.). Is it published yet? Could the authors provide a link to the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Deep-DIVE, a framework to learn disentangled features from the original input to form clusters in the embedding space and unify the classified features via the cross-attention mechanism. Experimental results on time series forecasting showcase that the proposed framework could disentangle the features and provide better forecasts than existing methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is clearly written, easy to follow and understand.\n2. Experiments compared with other baselines showcase that the proposed Deep-DIVE framework achieves better performance than existing baselines.\n3. In terms of novelty, the DeepDIVE framework proposed in this work decomposes the latent space z into two distinct dimensions: marginal dimensions b and conditional dimensions a. The marginal dimensions b capture general trends and are independent of each other, while the conditional dimensions a are conditioned on b. This design enables b to capture shared patterns across conflicting tasks, while a learns task-specific features to avoid conflicts. Compared with existing methods, this approach better addresses the challenge of using a single variational encoding to model conflicting time series, resulting in improved performance and disentanglement."
            },
            "weaknesses": {
                "value": "1. In the introduction section, the author motivates the proposed Deep-DIVE framework by criticizing existing deep learning approaches for time series forecasting as being black-box in nature and hard to optimize. However, time series forecasting (TSF) is a well-established problem. The author should provide further explanation to better justify why the proposed framework is helpful in TSF.\n\n2. The assumption 2 that $q_{\\phi}(b_i,b_j|x) = q_{\\phi}(b_i|x)q_{\\phi}(b_j|x)$ for any i and j is too strong. Although I understand the author's remark that this simplifying assumption often works well in practice, some explanation or intuition about why it often works would be helpful."
            },
            "questions": {
                "value": "In the experiment results in section 5, Table 2, what does 'std' mean?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes the new VAE architecture that has a special latent variable structure for time series forecasting.\nThis paper assumes that the target variable of VAE is a combination of future and past variables, \nand that the future variables do not depend on the latent variables, but only on the past variables.\nThe latent variables also consist of two vectors $a$ and $b$, and the posterior distribution of $a$ depends on $b$, but that of $b$ is independent of $a$.\nFurthermore, it is assumed that $b$ consists of a mixture model of K classes. Finally, the likelihood function obtained under these assumptions is the proposed objective function.\nIn the experiment, the forecasting accuracy is compared using two time series datasets, and the visualization of the latent variable is discussed."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed method is well explained using mathematical equations. \nThe expansion of equations is easy to understand, and the assumptions necessary for the expansions are also clearly stated. \nHowever, as written in Weakness, the practical validity and motivation of the assumptions are unclear.\n- In the experiments, the forecasting accuracy is higher than that of VAE and beta VAE on one dataset. \nHowever, if you only focus on the forecasting accuracy, it is difficult to say that VAE and beta VAE are appropriate baselines.\nIf the paper is a study of application, it should be compared with baselines for time series forecasting.\nOn the other dataset, baseline methods seem to be forecasting methods, but the proposed method does not necessarily outperform these baselines."
            },
            "weaknesses": {
                "value": "- This paper does not succeed in positioning itself in the context of previous research. \nI strongly suggest that this paper should clarify whether the focus of the paper is a proposal of a method for a specific application or one of a fundamental method.\nThe abstract seems to state that the paper solves a problem in general multi-task learning, but the introduction seems to claim that it is a technique for solving the problem of time series (or ASC?) forecasting.\nIt is inconsistent and makes the motivation unclear. The paper needs to consistently state the research problem and clearly explain why the proposed method is effective for that problem.\nIf this paper solves a specific problem in time series forecasting, it should justify why that problem is important and common to broad domains.\n\n- Related to the above, it is difficult to understand the reasons and principles behind the design of the proposed method.\nAlthough the assumptions that lead to the theorem are shown, this paper does not mention the assumptions of the target tasks or the most suitable use cases of the proposed method.\nAs a result, it is difficult to discuss the generality and importance of the task being solved by the proposed method, and the impact of this paper is unclear.\nThe graphical models of a, b, x, and y might be useful for readers to understand the assumptions, \nand illustrations of how these variables relate to practical tasks will also help readers understand the effectiveness of the proposed method.\n\n- The paper evaluates the visualization of latent variables but does not compare it with existing methods.\nI think that disentanglement of latent variables in time-series data have already been discussed in previous research such as [a].\nIf the claim of this paper is the importance of disentangling latent variables, the proposed method should be compared with previous methods.\n\n[a] Fraccaro, Marco, et al. \"A disentangled recognition and nonlinear dynamics model for unsupervised learning.\" Advances in neural information processing systems 30 (2017).\n\n- The baseline is not consistent in the evaluation on the two datasets.\nWhy are the baselines different in Table 1 and Table 2? Is it appropriate to compare VAE and beta VAE using time series data sets?"
            },
            "questions": {
                "value": "If I have misunderstood the paper, please point out it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}