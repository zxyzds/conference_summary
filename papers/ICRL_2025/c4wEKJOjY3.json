{
    "id": "c4wEKJOjY3",
    "title": "Out-of-distribution Generalization for Total Variation based Invariant Risk Minimization",
    "abstract": "Invariant risk minimization is an important general machine learning framework that has recently been interpreted as a total variation model (IRM-TV). However, how to improve out-of-distribution (OOD) generalization in the IRM-TV setting remains unsolved. In this paper, we propose a novel OOD generalization approach for IRM-TV, named OOD-TV-IRM, based on its theoretical analysis. The key idea is to deploy an autonomous TV penalty that depends on the invariant feature extractor. We construct the autonomous TV penalty using a neural network with another set of parameters, which can be learned via an adversarial scheme against the parameters of the invariant feature extractor. Experimental results show that OOD-TV-IRM outperforms IRM-TV in most situations.",
    "keywords": [
        "Out-of-distribution generalization",
        "total variation",
        "invariant risk minimization"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We propose an out-of-distribution generalization methodology for the total variation based invariant risk minimization.",
    "creation_date": "2024-09-17",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=c4wEKJOjY3",
    "pdf_link": "https://openreview.net/pdf?id=c4wEKJOjY3",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes to learn the penalty term $\\lambda$ in IRM-TV using a neural network to improve the OOD generalization performance of IRM-TV. As $\\lambda$ is associated with the feature extractor $\\Phi$, which also requires learning, $\\lambda$ and $\\Phi$ are learned alternatively via minimax adversarial learning. Experiments on a simulated dataset and real-world datasets show that learning $\\lambda$ can improve IRM-TV's performance when distribution shifts."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Developing methods to learn $\\lambda$ is important to improve IRM, which is a commonly used algorithm in OOD generalization. \n2. The proposed minimax adversarial learning is novel and looks like a promising method to learn $\\lambda$ well. \n2. The writing is clear to follow."
            },
            "weaknesses": {
                "value": "My concern mainly falls in the experimental results:\n1. The experiment seems only to be performed once as no standard deviation of the performance is reported. \n2. Adversarial learning could be hard to converge. Analysis like Figure 1 on real-world datasets could be given to facilitate the understanding of how capable OOD-TV is when facing larger $\\Phi$ and more complex datasets."
            },
            "questions": {
                "value": "1. Is there any repetition of experiments presented in section 4.2 to 4.4?\n2. How do $g(\\Psi, \\Phi)$, $h(\\Psi, \\Phi, \\rho)$, $||\\Phi^{(k+1)}-\\Phi^{(k)}||_2$ and $||\\Psi^{(k+1)}-\\Psi^{(k)}||_2$ change in real-world datasets as training goes by? Are $g(\\Psi, \\Phi)$ and $h(\\Psi, \\Phi, \\rho)$ able to converge and how long does it take to converge?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Previously proposed Invariant Risk Minimization aims to extract invariant features across different environments to improve the generalization and robustness of the model.  A recent work reveals that the mathematical essence of IRM is a total variation (TV) model. TV measures the locally varying nature of a function, which is widely applied to different areas of mathematics and engineering, such as signal processing and image restoration. In this paper, the authors proposed to generalized versions of IRM with the TV framework and demonstrates the improvements of IRM."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The generalized framework of IRM provides a novel perspective for IRM and provide new possibilities for improvements."
            },
            "weaknesses": {
                "value": "There are several weakness for this paper.\n\n1.The performance improvements are quite marginal. As shown in Table 3, the TV framework IRM can only outperform baselines by a slight margin. Besides, as shown in Figure 1, the curves are quite close with each other. It is suggested to add experiments to demonstrate what the previous IRM cannot do while the novel proposed IRM is good at.\n\n2.It is not clear why this framework demonstrates improvements. As min-max optimizes on the worst-case scenarios, is it possible this could bring potential overfitting issue. Additionally, there are already lots of paper incorporating min-max optimization procedure into IRM. By googling, it can be found that these following papers all use minimax methods and invariant learning:\n[1]Heterogeneous risk minimization\n[2]Invariant risk minimization games\n\n3.No comprehensive experiments on OOD generalization are conducted. This paper only evaluate on the spurious correlation dominated datasets. It can also evaluate the methods on diversity shift dominated datasets with widely-used OOD generalization evaluation benchmark, such as ood-bench. It is known that the IRM cannot deal very well on diversity shift dominated datasets. This can potentially strengthen this paper to point  out clearly the advantage of the proposed method."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies invariant risk minimization by introducing an additional penalty named TV penalty. The proposed TV penalty aims to quantify the global variation of a function and further enhance the performance of IRM. To realize such a penalty, the authors propose to leverage additional learnable parameters and train them in an adversarial manner. As a result, the learning performance and be further improved thanks to the variance penalty and enforces learning of invariant features. Through extensive quantitative results, the effectiveness of the proposed OOD-TV-IRM is carefully justified."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- By incorporating an additional penalty with learnable parameters, the proposed methodology is novel and interesting. The introduction of TV penalty is also novel in the field of invariant learning.\n- The proposed method is theoretically justified, which ensure the training stability when adversarial process is incorporated.\n- The writing is clear and sound, it is not hard to understand this paper.\n- Experimental results shows satisfactory improvement compared to baseline methods."
            },
            "weaknesses": {
                "value": "- Motivation needs to be further justified. It is interesting to introduce such a novel penalty to invariant learning. However, it is unclear that why such a penalty is beneficial compared to other techniques, such as ZIN. If such a penalty applies stronger regularization to extract invariant features compared to other methods, the intuitive reason should be further explained. \n- Introducing additional parameters just for a penalty is not computationally friendly. If the application is for large-scale machine learning problems, such a method would be unfavorable. Moreover, it requires extra effort on deciding the hypothesis class of the parameters.\n- Limited number of baseline methods. There are only a few baselines are chosen for comparison. It would be better if more recent and state-of-the-art invariant learning methods are considered.\n- Computational efficiency of the proposed method is not discussed.\n- Missing some related references:  \nHuang et al., Harnessing Out-Of-Distribution Examples via Augmenting Content and Style, in ICLR 2023.  \nYang et al., Invariant learning via probability of sufficient and necessary causes, in NeurIPS 2023.    \nXin et al., On the connection between invariant learning and adversarial training for out-of-distribution generalization, in AAAI 2023."
            },
            "questions": {
                "value": "Please see the weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This manuscript is motivated by the practical challenge of Out-of-Distribution (OOD) generalization within the context of the IRM-TV framework. Specifically, prior work on IRM-TV aimed to interpret Invariant Risk Minimization (IRM) through the lens of total variation models. Despite its theoretical contributions, the previous research did not offer any concrete algorithms or practical solutions to tackle the OOD generalization problem (according to the authors' assertion)\u2014an area where IRM is often regarded as a classic approach.\n\nIn response to this gap, the current study introduces OOD-TV-IRM, an innovative methodology that builds upon the insights drawn from IRM-TV. The authors highlight a critical observation: that the penalty parameter, which plays a vital role in regularization, should be tailored to vary across different extractors. This consideration is particularly important for effectively addressing the nuances associated with OOD tasks.\n\nTo implement this idea, the manuscript employs a neural network to dynamically fit the penalty parameter, thereby ensuring adaptability in response to the model's requirements during training. The proposed model engages in an adversarial training process, balancing empirical risk with the total variation penalty. This strategic approach not only enhances the model's robustness but also facilitates its generalization capabilities.\n\nTo validate the effectiveness of their proposed method, the authors conducted experiments using several toy datasets. These datasets serve as controlled environments that allow for a clear examination of the model's performance and its ability to generalize to unseen distributions. The results presented in the manuscript demonstrate the potential efficacy of OOD-TV-IRM in navigating the complexities of OOD scenarios, thus contributing valuable insights to the field."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The manuscript is commendably well written, providing a clear and logical progression of ideas. The motivation behind this study is articulated effectively, emphasizing the goal of addressing the shortcomings present in prior research. This clarity helps to engage the reader right from the beginning, setting the stage for a meaningful exploration of the topic at hand. Additionally, I would like to highlight Section 2, which is particularly well-executed. This section offers valuable insights and serves as an excellent guide for understanding the core concepts of the Invariant Risk Minimization through Total Variation (IRM-TV) framework. The organization and clarity in this section significantly enhance the reader's ability to grasp the foundational ideas that underpin the subsequent analysis.\n\n2. The theoretical analysis presented in this manuscript is notably comprehensive and rigorous. The authors begin with a clear definition of Invariant Risk Minimization (IRM) and Total Variation (TV), systematically leading to the derivation of the IRM-TV formulation. This step-by-step approach allows readers to follow the logical flow of the authors' reasoning easily. Furthermore, the manuscript provides compelling evidence to address the limitations of IRM-TV in effectively managing OOD generalization tasks. By articulating these limitations clearly, the authors lay a solid foundation for the necessity of their proposed method. Subsequently, the manuscript delves into a detailed explanation of the proposed approach, presenting it in a theoretical context. This thorough exploration not only clarifies the mechanics of the new method but also reinforces its relevance in tackling the identified shortcomings of previous works."
            },
            "weaknesses": {
                "value": "1. The significance of this work seems to be limited.\n2. The contributions of this work are not enough.\n3. The necessary summary on the theoretical analysis is missing.\n4. The experimental results are not enough.\n\nConcretely,\n1. The overall significance of this work to the broader research community appears insufficient. While the manuscript aims to implement an insight from the IRM-TV framework, it does not convincingly demonstrate the explicit advantages of applying this framework specifically to OOD generalization tasks. Assuming that IRM-TV is a significant advancement\u2014although it is yet to gain citations despite being accepted to ICML-24\u2014the manuscript fails to articulate why its implementation in OOD generalization is strategically important. Clarity on this point would greatly enhance the work's relevance to the community.\n\n2. When evaluating the contributions of this manuscript, it seems that they are primarily derived from insights proposed in IRM-TV, along with the design of an adversarial training paradigm. While the thorough theoretical analysis is commendable, it does not present novel findings that advance the field. The approach of using a neural network to fit hyperparameters is a well-established concept, seen in various contexts such as Neural Architecture Search. Without distinctive contributions that push the boundaries of current knowledge, the manuscript struggles to establish a strong impact.\n\n3. The manuscript presents an extensive theoretical analysis, but it suffers from a lack of necessary summarization. The inclusion of many intermediate derivative processes in the main text can overwhelm readers and challenge their ability to follow the core arguments. While the authors do provide remarks at the end of Section 3.2, there is no clear conclusion to encapsulate the preceding derivations effectively. A concise summary that highlights the key findings would greatly enhance the readability and coherence of the theoretical analysis.\n\n4. The experimental results presented in the manuscript are insufficient to draw robust conclusions. This inadequacy arises from two dimensions: the choice of datasets and the range of methods for comparison. The authors primarily use simulated toy data, CelebA, and Landcover for evaluation, yet they neglect to include mainstream datasets commonly used in the OOD community, such as NICO and Colored MNIST. Given that IRM is known to perform well in scenarios involving correlation shifts, it is imperative to include datasets that exemplify these characteristics\u2014for instance, NICO and Colored MNIST should be at least considered. Furthermore, expanding the range of compared methods would enhance the significance of the experimental findings. Including additional baseline approaches would provide a more comprehensive understanding of the proposed method\u2019s performance relative to existing techniques."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}