{
    "id": "TmKeT3IFTZ",
    "title": "Fair Anomaly Detection For Imbalanced Groups",
    "abstract": "Anomaly detection (AD) has been widely studied for decades in many real-world applications, including fraud detection in finance, and intrusion detection for cybersecurity, etc. Due to the imbalanced nature between protected and unprotected groups and the imbalanced distributions of normal examples and anomalies, the learning objectives of most existing anomaly detection methods tend to solely concentrate on the dominating unprotected group. Thus, it has been recognized by many researchers about the significance of ensuring model fairness in anomaly detection. However, the existing fair anomaly detection methods tend to erroneously label most normal examples from the protected group as anomalies in the imbalanced scenario where the unprotected group is more abundant than the protected group. This phenomenon is caused by the improper design of learning objectives, which statistically focus on learning the frequent patterns (i.e., the unprotected group) while overlooking the under-represented patterns (i.e., the protected group). To address these issues, we propose FADIG, a fairness-aware anomaly detection method targeting the imbalanced scenario. It consists of a fairness-aware contrastive learning module and a rebalancing autoencoder module to ensure fairness and handle the imbalanced data issue, respectively. Moreover, we provide the theoretical analysis that shows our proposed contrastive learning regularization guarantees group fairness. Empirical studies demonstrate the effectiveness and efficiency of FADIG across multiple real-world datasets.",
    "keywords": [
        "fairness",
        "anomaly detection"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=TmKeT3IFTZ",
    "pdf_link": "https://openreview.net/pdf?id=TmKeT3IFTZ",
    "comments": [
        {
            "summary": {
                "value": "This paper addresses the challenge of ensuring fairness in anomaly detection (AD), particularly in scenarios with imbalanced data between protected and unprotected groups.  To tackle this, the authors propose FADIG, a fairness-aware anomaly detection method. FADIG integrates a fairness-aware contrastive learning module and a rebalancing autoencoder module to address fairness and data imbalance issues. Theoretical analysis demonstrates that the proposed method ensures group fairness, and empirical studies confirm its effectiveness and efficiency across multiple real-world datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The issue of fairness in anomaly detection is a highly important and meaningful problem.\n2. The paper provides theoretical guarantees for the proposed method.\n3. The method's effectiveness is empirically validated on real datasets."
            },
            "weaknesses": {
                "value": "1. The paper presents two challenges: C1, handling imbalanced data, and C2, mitigating representation disparity. There is a strong coupling between these two challenges, as imbalanced data leads to representation disparity (i.e., group imbalance results in higher errors for protected groups, causing misclassifications). From this perspective, addressing C1 effectively resolves C2, which makes me question the necessity of the fairness-aware contrastive learning module.\n2. The paper lacks novelty. The conclusion that group imbalance leads to accuracy parity issues is well-known, and addressing accuracy parity by solving group imbalance has been widely used. From the results presented in this paper, I do not see any specific uniqueness in the context of anomaly detection.\n3. The paper lacks a discussion of related work, specifically in two areas:\n* The differences between this work and existing work on anomaly detection and imbalanced data.\n* The differences between the fair contrastive learning methods used in this paper and other existing methods.\n4. The writing is poor. For example, \n* the statement \"In this work, we focus on the group fairness notion which usually pursues the equity of certain metrics among the groups. For instance, Accuracy Parity (Zafar et al., 2017) requires the same task accuracy between groups and Equal Opportunity (Hardt et al., 2016) requires the same true positive rate instead.\" suggests that the authors consider both Accuracy Parity and Equal Opportunity as fairness metrics. However, in the experiments, only Accuracy Parity is used, which is confusing.\n* Figure 2 seems confusing."
            },
            "questions": {
                "value": "see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper focuses on fairness in outlier detection in unsupervised learning specifically addressing imbalance data that naturally arises in presence of minority groups. The paper presents a method for addressing representation disparity due to imbalance by proposing a fairness-aware contrative learning criterion and a reconstruction based network module with weights to account for patterns from minority groups. The work emphasizes the use fairness-aware constrastive loss as the main driver for framework as it leads to small total variation distance between the groups. Empirical results show the effectiveness of the proposed method across multiple real-world datasets when compared to exciting fairness-aware methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Rebalancing autoencoder with learnable weight for reconstruction loss is a simple way to encourage learning patterns from minority groups. I like the analytical calculation of \\epsilon. \n2. An elegant extension of contrasive entropy for uniform representation to incorporate fairness criterion as contrastive entropy across majority and minority groups.\n3. Paper is easy to read and follow."
            },
            "weaknesses": {
                "value": "1. In most applications we have multi-valued multiple protected attributes. It seems that it is non-trivial to extend the loss function to cater to such datasets. How does the method scale with multi valued multiple protected attribute setup?\n2. The choice of \\alpha hyperparameter is arbitrary. Why 4 works and not 8? How does one choose the value of this hyperparameter in real world scenario?\n3. The paper completely ignores the discussion on hyperparameter settings for the competitors. To ensure fairness to competitors, the detailed hyperparameter settings should be included for each compared method to better illustrate the benefits of the proposed method."
            },
            "questions": {
                "value": "Are there simpler considerations for \\epsilon that can be informed from the imbalance factor of groups? How does \\epsilon work for multi-valued attributes and multiple protected attributes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a technique to introduce weighted/cost-based loss function that pays attention to underrepresented groups in data such that the dominant groups do not have undue influence on the learned model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed technique shows that if it is known what the underrepresented groups are, then we can possibly use that information to improve anomaly detection through fairly straight-forward ways\n\n  2. The ablation experiments help to understand the importance of individual components of the loss function"
            },
            "weaknesses": {
                "value": "Overall, the paper needs to be written more clearly and unambiguously. Main comments are:\n\n\n1. Section 2: The paper defines protected groups on the basis of a single feature -- this might be rather simplistic. Instead, in real data, there could be a combination of one or more features that defines the protected/unprotected classification. In reality, it might not be easy to identify the underrepresented groups automatically.\n\n\n2. Why restrict to a single protected group? There might be multiple 'protected' groups that have very distinct characteristics.\n\n\n3. Wouldn't it be better to have separate anomaly detectors for each (prot/unprot) group? Was that approach considered as a benchmark?\n\n\n4. Figure 2: It is hard to differentiate between red and pink dots in the figure.\n\n\n5. Line 194: \"... learning can alleviate this issue by pushing examples uniformly distributed in ...\" -- Hard to understand this sentence. Maybe there is some grammatical error?\n\n\n6. Lines 195-196: \"... to learn representations by distinguishing different views of one example from other examples as follows:\" -- Not clear what this means exactly.\n\n\n7. Line 208: \"... to maximize the cosine similarity between the representations of the two groups, ...\" -- This is very ambiguous. It needs to be very clearly stated what the goal is: for example, maximize the similarity among the instances in the protected group? And separately among instances in the unprotected group? It seems that minimizing L_{FAC} in Eqn 3 implies minimizing the intra-group similarity (second term on the right). This has not been clearly stated. If there is more than one protected group, how would that affect the loss function?\n\n\n8. Line 238, Eqn 5: It is not clear what exact \\epsilon values were used in the experiments even though some selection technique is mentioned in the Appendix.\n\n\n9. Line 354-355: \"... improved by minimizing the discrepancy between the two distributions and ...\" -- this discrepancy is probably a property of the real-world which is not in our control. Therefore, trying to minimize it should not be an option.\n\n\n10. Line 375: \"... method which incorporates the prescribed criteria into ...\" -- Which prescribed criteria? Need to be more clear and specific.\n\n\n11. Table 3: While the proposed technique (FADIG) is shown to perform well, it has an unfair advantage because it has the extra information about the under-represented groups that is not available to other algorithms."
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a new fairness-aware anomaly detection method called FADIG. Their method uses contrastive learning and a modified, reweighted, autoencoder in order to detect anomalies in datasets with multiple groups of differing representation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper clearly states its purpose. The mathematics of the paper are fairly easy to follow and clearly substantiate the intuitive reasoning of the method. The proposal of the authors should also be lauded for its relative simplicity which achieves (seeminly) good results."
            },
            "weaknesses": {
                "value": "The points below are given in no particular ordering of importance (within category):\n\n**Major:**\n- The authors state in the second paragraph of the introduction that deep anomaly detection methods demonstrate significantly better performance than shallow anomaly detection methods. While this can easily be argued for certain applications such as computer vision, it is certainly not true for tabular data, which is also analyzed within this paper and several of the other methods benchmarked in this study. See for example recent comparisons in ADBench by Han et al. (2022) and Bouman et al. (2024) in \"Unsupervised Anomaly Detection Algorithms on Real-world Data: How Many Do We Need?\".\n- The use of an autoencoder as an anomaly detector relies heavily on an assumption stated by the authors that \" Anomalies tend to exhibit large reconstruction errors because they do not conform to the patterns in the data as coded by the autoencoder.\" This is an assumption which has been heavily questioned in recent studies, such as:\n\t- Nicholas Merrill and Azim Eskandarian. Modified autoencoder training and scoring for robust unsupervised anomaly detection in deep learning. IEEE Access, 8:101824\u2013101833, 2020.\n\t- Laura Beggel, Michael Pfeiffer, and Bernd Bischl. Robust anomaly detection in images using adversarial autoencoders. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2019, W\u00a8urzburg, Germany, September 16\u201320, 2019, Proceedings, Part I, pp. 206\u2013222. Springer, 2020\n\t- Marcella Astrid, Muhammad Zaigham Zaheer, Jae-Yeong Lee, and Seung-Ik Lee. Learning not to reconstruct anomalies. arXiv preprint arXiv:2110.09742, 2021.\n\t- Marcella Astrid, Muhammad Zaigham Zaheer, Djamila Aouada, and Seung-Ik Lee. Exploiting autoencoder\u2019s weakness to generate pseudo anomalies. Neural Computing and Applications, pp. 1\u201317, 2024\n\t- Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1705\u20131714, 2019.\n\t- Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In International conference on learning representations, 2018\n- The authors assume in section 3.2 that the model is only capable of fitting 2 out of 4 subgroups. Yet this seems like a strong assumption with little theoretical foundation.\n- For the CelebA dataset, all detectors perform much worse than should be expected, with almost all detectors being close to random. This indicates that perhaps it is not a well-suited benchmark dataset, or other detectors should be shown which do perform well.\n\n\n\n**Minor:**\n- The connection to relevant literature is too sparse. Applications and other studies into fairness of anomaly detectors should be elaborated upon in more detail during the introduction.\n- The captions of figures are somewhat shorthand. I would advise the authors to rethink the captions so they allow for the figures to be understood more easily as-as, rather than needing the full context of the section or paper.\n- The introduction currently contains a preview of the results, which disturbs the flow of the paper. I'd advise to move this to the result section in full."
            },
            "questions": {
                "value": "- I'm missing a connection of the fairness discussion, and the presence of multiple groups, to the more classical literature on anomaly detection. Specifically anomaly detection in the presence of multiple groups has been studied extensively, and relates to the concept of \"local\" and \"global\" anomalies, see for example \"LOF: identifying density-based local outliers\" by Breunig et al., \"LoOP: local outlier probabilities\" by Kriegel et al., other work by Kriegel et al. throughout the 00's and 10' as well as a recent examination by Bouman et al. in \"Unsupervised Anomaly Detection Algorithms on Real-world Data: How Many Do We Need?\". Can the authors elaborate on this connection? \n- Figure 2 is currently hard to understand. What representation is being shown? Why is the data circularly constrained, which dataset is shown, what do the x and y-axis represent? Consider revising this.\n- What does the FADIG abbreviation mean? The beginning of section 3 implies an abbreviation due to the capitalization, but it does not match up with the letters and is therefore slightly confusing.\n- It currently still eludes me how the model is applied in practice in the unsupervised setting. Specifically, without knowledge of whether a sample belongs to the protected or unprotected group at train time, how is the loss calculated? It seems that we need access to the protected and unprotected loss, but within the unsupervised setting we would never be able to differentiate between the two. Could the authors elaborate further?\n- In the characteristics of the datasets, it seems that the number of anomalies for the 4 datasets is 1205, 479, 364, and 5150. Yet, the k-values chosen for the results presented are not at these values, but rather at values close. Where does this difference come from? I would like to remark that this is not in line with other measures such as the r-precision, which are calculated with k being the exact number of anomalies in the dataset.\n- Some detectors do not perform well on any dataset. Could the authors elaborate on how this could happen? I've noted here that for example FairOD uses fully different benchmarking datasets, why not include these for this study?\n- The chosen architecture for the used autoencoder is fairly shallow, and notable barely compresses the data. How were these architectures chosen, and how did the authors ensure that they did not overfit on the proposed benchmark? Similarly, could more details on other hyperparameters and how they were chosen be provided?\n- In the reproducibility section, the authors list the random seeds that were used. These do not seem structured in either a logical, or a random way, which leads me to suspect that some, albeit accidental, cherrypicking may have occured. Could the authors elaborate on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper develops a fair anomaly detection model based on reconstruction-based detection. A fairness-aware contrastive learning and a re-balancing autoencoder are developed to ensure fairness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This paper targets an important but still under-exploited problem: fair anomaly detection.\n\n- The proposed approach can achieve a better trade-off between utility and fairness."
            },
            "weaknesses": {
                "value": "- Maybe I am missing something. Section 4 looks incomplete to me. It demonstrates the fairness with Rademacher Complexity, but I am not sure how the proposed approach can minimize the upper bound given in Theorem 4.6. That said, it would be better to provide a more explicit explanation of how the proposed components (the fairness-aware contrastive learning or re-balancing autoencoder) relate to minimizing the terms in the upper bound given in Theorem 4.6.\n\n- Theorem 4.5 states that the *risk difference* between the two groups is upper bounded by Eq 10. Are there any connections between the risk difference in terms of Rademacher complexity and risk difference in terms of fairness metrics, such as the recall-based metrics used in experiments?\n\n    - Please clarify whether there's a theoretical relationship between the risk difference bound and the recall difference metric used in the experiments. \n   \n\n- Overall, Section 4 looks more like a general theoretical analysis. It would be better to explicitly discuss how to leverage the theorems in Sec 4 to analyze the fairness issue in anomaly detection tasks, especially the reconstruction-based anomaly detection models."
            },
            "questions": {
                "value": "Please check my comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}