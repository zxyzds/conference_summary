{
    "id": "cLj51OYBsh",
    "title": "Power of Augmented Replicas in Out-Of-Distribution Detection",
    "abstract": "Data augmentation is widely used in machine learning to enhance training datasets by introducing minor variations to the original data, traditionally aiming to prevent overfitting and improve model performance. This paper explores a novel application of data augmentation during the inference stage to enhance out-of-distribution (OOD) detection. The proposed method involves replicating the inference image multiple times, applying various transformation techniques to each replica, and then evaluating the detectors using these augmented images. The effectiveness of this approach is assessed across different detectors, models, and datasets, demonstrating its potential to improve OOD detection capabilities.",
    "keywords": [
        "Data Augmentation",
        "Out-of-Distribution Detection (OOD)",
        "trustworthy ML"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "We propose using data augmentation during the inference stage to improve out-of-distribution detection by transforming different copies of the same image and evaluating the consensus of the detectors.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cLj51OYBsh",
    "pdf_link": "https://openreview.net/pdf?id=cLj51OYBsh",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a novel approach to improve  OOD detection by employing multiple augmented replicas of a single inference image. specifically, by replicating and transforming inference images, then calculating a median score across these replicas to assess the OOD prediction."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors validate the approach across multiple detectors, models, and datasets, including different types of transformations, to show that the method is effective, the approach is both straightforward and computationally feasible."
            },
            "weaknesses": {
                "value": "1. This approach feels more like an engineering trick than a genuinely novel method, as similar techniques are commonly used in tasks like image segmentation and classification.\n2.  The increase in computational demand does not appear proportionate to the improvements in detection accuracy.\n3. The method has not been validated on larger datasets, such as ImageNet."
            },
            "questions": {
                "value": "See Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores the potential of utilizing data augmentations to enhance existing out-of-distribution (OOD) detection techniques. The study shows that using the median score across different augmentations improves OOD detection performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper demonstrate that on CIFAR-10 and PathMNIST datasets, utilizing data augmentations and the median score reliably improves on existing OOD detection metrics."
            },
            "weaknesses": {
                "value": "1. The paper has major presentation issues:\n- Figure 2 plots AUROC and FPR95 on the same plot. This makes the information very difficult to digest because these two metrics are anti-correlated. Additionally, the range of each values are on opposite ends of the spectrum (FPR95 closer to 0 while AUROC closer to 100), making the performance change caused by increase in number of replicas illegible. \n- It is unclear what information Figure 2 is suppose to convey in addition to Table 1 and 2. It also isn't very information-rich since Figure 2 x-axis has large range from 0 to over 30, yet only 3 data points is plotted.\n- Table 1 and 2 is a huge table of numbers without any bolding. It is unclear what the key takeaway is.\n\n2. The datasets and model used for evaluation is not comprehrensive:\n- OOD detection is either only evaluated on CIFAR-10, which is low resolution, or PathMNIST, which is a pathology dataset. A more common large-scale image recognition dataset used for evaluation is ImageNet. \n- Results are only reported on MobileNetV2 architecture, which is an atypical choice, making it difficult to compare with existing literature. More common architecture includes: ResNet-50, ResNet-101, Wide-ResNet-101, ViT-L/14.\n\n3. Lack of comment on existing literature that augments model predictions using data augmentations. [1] demonstrates that data augmentation can be very helpful for the task of novelty prediction, which is very similar to OOD detection.\n\n[1] Bahat, Yuval, and Gregory Shakhnarovich. \"Confidence from invariance to image transformations.\" arXiv preprint arXiv:1804.00657 (2018)."
            },
            "questions": {
                "value": "1. Any particular reason why the median is used rather than the typical estimate of mean? Is the resulting score distribution too skewed?\n2. Is there any patterns that merges regarding the augmentation used for the median score? i.e. what kind of augmentation typically lead to the median score? It would be interesting if it is a particular transformation or equally interesting if it is random and no pattern emerges.\n3. Curious if there are any thoughts on whether or not the increase compute (extra passes through the model) is worth the increased detection performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "I recommend rejecting this paper due to several concerns. First, it employs well-known and widely used techniques (e.g., test-time augmentation) on an existing task (OOD detection), without introducing significant novelty. Additionally, a previously published study presents directly opposing results, which the authors have not addressed or discussed\u2014an essential omission in establishing the validity of their findings. Furthermore, the experimental setup in this paper raises questions about reliability and reproducibility. For these reasons, I believe a rejection is warranted."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper presents the proposed method with good clarity, making it very easy to follow and understand."
            },
            "weaknesses": {
                "value": "The paper lacks an in-depth understanding of why test-time augmentation is effective in this context. Please refer to the \"Question\" section for further details.\n\nAdditionally, the novelty of the work is limited as it use test time data augmentation directly without a specific design on OOD detection.\n\nThe statistical significance analysis in Figure 2 appears questionable, especially on the iSUN dataset. It seems as though the model may not be learning effectively."
            },
            "questions": {
                "value": "My primary concern with this paper is the lack of clarity on why test-time augmentation works effectively for the OOD detection task. This should be the central focus, yet it remains insufficiently addressed. The findings presented here appear inconsistent with those in [1], where test datasets created with data augmentation techniques showed a significant performance drop as more and stronger augmentations were applied. This aligns with existing literature suggesting that data augmentation often increases the domain gap from the original data, thereby leading to performance degradation in OOD tasks [2,3].\n\nThe authors should carefully reassess their claims and clearly specify which augmentation techniques are beneficial for OOD detection in this context.\n\nReferences:\n\n[1] Unsupervised Evaluation of Out-of-distribution Detection: A Data-centric Perspective, arXiv 2023.\n\n[2] Affinity and Diversity: Quantifying Mechanisms of Data Augmentation, ICLR 2022.\n\n[3] Are Labels Always Necessary for Classifier Accuracy Evaluation? CVPR 2021."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper use existing datasets. Thus no ethics concerns."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper uses image augmentation strategy during inference to improve OOD detection. Image augmentation brings performance improvement."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This paper does not require training and can improve the performance of OOD Detection using a very simple method."
            },
            "weaknesses": {
                "value": "This paper is not promising enough for contributing to the related domain and community. Here I will list the weaknesses of the paper:\n\n* The core design motivation of this paper is insufficient. This article does not analyze the advantages and disadvantages of current OOD detection methods well, and does not clearly explain why the designed method is proposed.\n\n* Based on the weak motivation, the method in this paper is more like an engineering trick than a research contribution. The author believes that the method brings more \"knowledge\" about the data to the model, but there is no explanation, reasoning, or experimental results to show what this knowledge is.\n\n* Why did the performance increase? This paper does not explain this question. There is not even any qualitative and quantitative experimental support for the specific design of data augmentation details and related ablation experiments.\n\n* The writing and presentation of this paper is not very clear and there are some errors. I hope the author can check it carefully. (For example, the last element of each matrix in Figure 1 should be f_xn instead of f_4n)"
            },
            "questions": {
                "value": "I suggest that the author could consider the following questions:\n\n* What is the essence of OOD Detection? Is it the expansion of knowledge? What knowledge does a trained model (space) need?\n\n* If training-free is a good premise, what is the relationship between generalization across different distributions and data augmentation?\n\n* What does the data augmentation proposed in the article actually do for feature distribution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}