{
    "id": "6jr94SCjH6",
    "title": "Reflect-then-Plan: Offline Model-Based Planning through a Doubly Bayesian Lens",
    "abstract": "Offline reinforcement learning (RL) is essential when online exploration is costly or unsafe, but it often struggles with high epistemic uncertainty due to limited data. Existing methods learn fixed conservative policies, which limit adaptivity and generalization. To tackle these challenges, we propose __Reflect-then-Plan (RefPlan)__, a novel _doubly Bayesian_ approach for offline model-based (MB) planning that enhances offline-learned policies for improved adaptivity and generalization. RefPlan integrates uncertainty modeling and MB planning in a unified probabilistic framework, recasting planning as Bayesian posterior estimation. During deployment, it updates a belief distribution over environment dynamics based on real-time observations. By incorporating this uncertainty into MB planning via marginalization, RefPlan derives plans that account for unknowns beyond the agent's limited knowledge. Empirical results on standard benchmarks show that RefPlan significantly improves the performance of conservative offline RL policies. In particular, RefPlan maintains robust performance under high epistemic uncertainty and limited data, while demonstrating resilience to changing environment dynamics, improving the flexibility, generalizability, and robustness of offline-learned policies.",
    "keywords": [
        "Offline reinforcement learning",
        "Model-based planning",
        "Bayesian inference",
        "Bayesian reinforcement learning"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=6jr94SCjH6",
    "pdf_link": "https://openreview.net/pdf?id=6jr94SCjH6",
    "comments": [
        {
            "summary": {
                "value": "When data coverage for offline RL algorithms is incompete, this can lead to high epistemic uncertainty. The authors aim to improve performance in such settings at deployment time by incorporating a Bayesian-based approach. Specifically, their approach, called Ref-Plan, integrates model-based planning and uncertainty modeling. An empirical evaluation on standard offline RL benchmark domains considers the  performance of RefPlan in environments where dynamics change or where data availability is limited."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The problem motivating this work is important,  and to the best of my knowledge this algorithm seems like a novel contribution\n*  Many parts of the paper that are nicely written, including the motivations outline in section 1, and discussion in section 3. \n* The math discussed in the work, e.g., 4.1 and 4.2 did not seem to have errors. \n* The number / types of environments seems adequate to provide rankings between algorithms (in aggregation)"
            },
            "weaknesses": {
                "value": "*Background / Improving Clarity:* The paper could be improved with more clarity on a lot of the background. While many algorithms / ideas were mentioned then cited, having a fuller description of these works in the paper (main body or appendix) would be beneficial, especially when these are used in the main algorithm or often referenced. E.g., BAMDP, control-as-inference framework, quantifying epistemic uncertainty. \n\n*Experiments:*  \nRQ1. Further explanation connecting the environment settings chosen and resulting epistemic uncertainty would improve the flow. \nRQ3 & RQ4. Q3 seems to be comparing performance under epistemic uncertainty but when that uncertainty is produced through limited data as opposed to RQ1? Improved clarity between these RQs would be beneficial. RQ1 seems to be a superset of RQ3&4. \n\nSmall Confusions / Errors\n- the last comment in alg2 refers to line 5 in alg1, but there are no line numbers,  line 5 specifically is the beginning of a loop\n- what are the error bars used in the experiments? \n- bold vs underline meaning in Table 1? \n- H-step is mentioned without being defined."
            },
            "questions": {
                "value": "RQ3 and RQ4 seem to be a superset of RQ1. Could the authors clarify this? Instead, is it the case that these RQs each consider different causes of uncertainty?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work addresses the problem offline reinforcement learning and proposes a Bayesian-inspired model-based solution based on VariBad and control-as-inference.\n\nVaribad is a Bayesian solution for meta-learning: given data on a set of tasks, how to quickly identify the task during testing (and do well in it).\nThis is done through variational inference, where an encoder is trained to capture distribution over the task (in the form of a latent variable) given a trajectory (paired with a decoder that is trained to reproduce the trajectories).\nControl-as-inference models decision making as a probabilistic problem, \"probability of policy given optimality\", using the expected return as a likelihood measurement.\n\nThe performance is compared to typical offline model-free and model-based methods on D4RL benchmark, where they show performance comparable to \"LOOP\"."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1: The work is tackling a relevant (offline RL) problem that should be of interest to the a non-trivial section of the ICRL community.\nS2: The English is easy to understand and the math is (as far as can tell) sound.\n\nS3: I believe the proposed solution - the combination of control-as-inference and Bayesian inference over dynamics - is non-trivial and novel.\n    In particular, it attempts to leverage uncertainty in (offline model-free) policy and (offline model-based) dynamics in a computationally feasible way.\n    Though I am not familiar with the offline RL community/literature, the question on how to combine model-free and model-based is a long and important one in RL.\n    The way it is proposed here \"makes sense\": a policy pre-trained should be considered a prior, and fine-tuning this online when new data comes in a Bayesian fashion is (only in hindsight) an obvious one."
            },
            "weaknesses": {
                "value": "W1: One room of improvement, especially for someone with lack of background, is the accessibility of the background.\n    One, certain (seemingly?) important concepts were not clearly defined (e.g. \"epistemic POMDPs\", \"BA-MDPs\".)\n    Second, some concepts were clearly background (e.g. \"control-as-inference\"), but were not introduced.\n    As far as I know, they were explained as part of the method description, which  made it excessively hard to infer what was novel (and should be credited as well as scrutinized) and what was known in the literature.\n\nW2: A major concern, in my opinion, is the lack of experiments for online learning.\n    Conceptually, planning is useful if (1) it saves us computation time (plan for current states, not whole state space) or (2) we gain more information over time (improve learned model and thus our planning).\n    As far as I understand, the experiments here are the initial performance, which begs the question (Q2) whether this performance could have been trained/reached offline instead.\n\nW3: VariBad tackles meta-learning: it is assumed (and exploited) that the training data set is generated from different tasks.\n    In particular, it is optimized to capture the task characteristics from different tasks, capture this in latent variables, and infer them online.\n    As far as I understand, the experiments do not include this setting.\n    In particular, it is not clear whether the \"Bayesian\" argument holds here: Varibad's encoder might just collapse - as all trajectories come from the same environment - and there should be no (latent) information to capture.\n    As a result, while it is supposed to be \"double Bayesian\", the proposed method does not seem to have the Bayesian trait of doing optimal actions with respect to the uncertainty."
            },
            "questions": {
                "value": "Q1: Is 4.and 4.2 background (control as inference & varibad), or are there particular extensions / modifications hidden in there?\n\nQ2: Given concerns of W3 (offline RL vs meta-learning), , rather little information is learn until \"much data\" is gathered.\n    As a result, it feels as if any additional performance from online planning _could have been done offline_: refine policy by doing control-as-inference offline on Varibad's model.\n    Do you have any idea how well that could or would perform?\n\nQ3: Did you consider comparing with VariBad? How about and ablation study where you replace VariBad with other model-based approach (that does not do meta-learning)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "To effectively incorporate the uncertainty into planning, this paper propose Reflect-then-Plan (RefPlan), a doubly Bayesian approach for offline MB planning to enhances offline-learned policies for improved adaptivity and generalization. The performance is validated on three standard benchmarks (Hopper, HalfCheetah, and Walker2d). However, it is not sure that the agent has learned a near Bayes-optimal policy. It would be better to add theoretical support and/or to include a navigation task, along with visualizations of the agent's behavior."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The Reflect-then-Plan (RefPlan) framework combines Bayesian modeling of epistemic uncertainty with model-based planning in a unified probabilistic approach."
            },
            "weaknesses": {
                "value": "1. The paper uses VariBAD's VAE structure to learn environment dynamics but lacks strong evidence that the agent has learned a near Bayes-optimal policy. It is recommended to add theoretical support or to include a navigation task, along with visualizations of the agent's behavior.\n\n2. The paper lacks innovation; the approach of offline model-based planning as probabilistic inference is common (see [1]). Furthermore, the proposed algorithm is merely a minor modification of VariBAD, lacking novelty. In addition, related work in the field of offline meta-RL has shown adaptation and generalization across multiple tasks, which is more valuable than the single-task generalization problem addressed here (see [2][3]).\n\n3. The paper evaluates the algorithm on only three tasks, which is insufficiently persuasive, and the experimental results show only marginal improvements over LOOP.\n\n[1] Janner et al., 2022, Planning with Diffusion for Flexible Behavior Synthesis.\n\n[2] Yuan et al., 2022, Robust Task Representations for Offline Meta-Reinforcement Learning via Contrastive Learning.\n\n[3] Ni et al., 2023, MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL."
            },
            "questions": {
                "value": "See the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a principled approach based on Bayesian inference for offline MBRL. The proposed formulation uses Bayes-adaptive MDP approach, in which the uncertainty over model estimation is captured through belief representation in POMDP while planning under uncertainty for optimal actions is proposed to plan for actions that account for unknowns beyond the agent\u2019s limited knowledge. In overall, the doubly Bayesian views are applied to both model learning and policy optimization. The writing is easy to follow. The final results are encouraging."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Pros:\n\n- Clear and principled proposal for offline MBRL: The model distribution is learnt via variational inference, while the policy planning step takes into account this model uncertainty to plan for optimal actions to act under epistemic uncertainty.\n\n- The proposed formulation is sound.\n\n- Clear writing: It's easy to follow and understand both the technical part, e.g. maths behind, and the main proposal, e.g. Fig. 2 has a clear depiction of the proposal."
            },
            "weaknesses": {
                "value": "Cons:\n\n- It's questionable that RefPlan only fine-tunes a baseline policy. Either i) why the learnt model can be used to optimize a new policy, ii) it's a bit unfair in terms of extra computation needed in comparisons to the baselines, including both model-free and model-base. Especially the latter, the model learnt using model-based methods in the baseline will be discarded or unnecessarily unused in the fine-tuning stage of RefPlan. So it is expected to have a more light-weight fine-tuning approach for test-time planning.\n\n- Performance is mixed: RefPlan performs well on some tasks, which show clear improvements. However at some tasks e.g. Hopper the improvements are not obvious even in comparison to model-free methods. E.g Fig. 3 CQL can still perform well in Hopper though on OOD setting, without explicitly modeling epistemic uncertainty, and without doing extra training and planning at test-time. In addition, the gap to existing SOTA, LOOP which also did some extra computation, is not significant."
            },
            "questions": {
                "value": "See the above two main questions in Cons.\n\nOther major comments:\n\n- Fig. 1: Misleading as could be understood that a GM is input to the encoder. There are observed nodes used as data, however the connection like graph  is also provided?\n\n- Conceptually, how it's compared to the deterministic path and stochastic path in PlaNet and Dreamer model? The plan is also solved using sampling, which is however RefPlan can have a higher variance due to outer sampling w.r.t random variable \"model m\". \n\n- Some ablations are needed to understand the effect of the whole sampling step, e.g. trade-off between variance and performance and needed computation.\n\n- \"In the offline setting, we aim to enhance the prior policy \u03c0p via MB planning at test time by inferring the posterior over\": Policy and model are decoupled. Can it be revised to compute optimal policies directly from the model-based policy optimization and at least there is a comparison to this \"baseline\"?\n\n- Experiment in Section 5.1: The results are encouraging to say RefPlan is acting in the face of epistemic uncertainty, however it's hard to understand the effect of which component, e.g. visualization of uncertain region, or understand how policy is selected in such situation.\n\n\nMinor comments:\n\n- It would be better in Related work to discuss and include this work: \"Arthur Guez, David Silver, Peter Dayan: Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search. NIPS 2012\"\n\n-"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper incorporates Bayesian uncertainty estimation into offline model-based planning to improve the adaptivity and generalization ability of offline-trained policies. Empirical results are shown to demonstrate the claimed advantages of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The incorporation of uncertainty estimation into the offline model-based planning framework is well done, mathematically. As far as I know, this is the first work to do so under the variational inference framework.\n2. Accounting for changes during deployment in the real environment is important in practice. In this sense, this work is well motivated.\n3. The empirical performance is promising and well verifies the adaptivity of the proposed method."
            },
            "weaknesses": {
                "value": "1. APE-V (Ghosh et al., 2022) seems like a valid baseline for adaptive offline algorithms, however the paper does not compare with it, making the evaluation potentially incomplete and less convincing.\n2. It seems like the hyperparameters need to be carefully tuned for each task, which might limit the usability of the proposed method.\n\n\n**Reference**\n\n(Ghosh et al., 2022) \"Offline RL Policies Should be Trained to be Adaptive\", ICML 2022."
            },
            "questions": {
                "value": "1. Section 5.1: How exactly do you test the prior policy in states sampled from the R dataset?\n2. Model-based methods are usually computationally expensive in training and planning is costly when executing actions, compared to sampling from a policy network. It seems like RefPlan needs to use an additional VAE network, which may further increase the computation burden. So I wonder how is the computational efficiency of the proposed RefPlan method, in training and in executing, respectively?\n3. Figure 4: I wonder what the performance will be like when you use the full dataset for LOOP and RefPlan. Maybe continuing the lines in the plots to 1M would help readers see how much and how rapidly the performance degrades when reducing the dataset size."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}