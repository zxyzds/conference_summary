{
    "id": "DnfPX10Etk",
    "title": "JOOCI: A FRAMEWORK FOR LEARNING COMPREHENSIVE SPEECH REPRESENTATIONS",
    "abstract": "Information in speech can be divided into two categories: what is being said (content) and how it is expressed (other). Current state-of-the-art (SOTA) techniques model speech at fixed segments, usually 10-25 ms, using a single embedding. Given the orthogonal nature of other and content information, attempting to optimize both within a single embedding results in suboptimal solutions. This approach divides the model's capacity, limiting its ability to build complex hierarchical features effectively. In this work, we present an end-to-end speech representation learning framework designed to jointly optimize the other and \\enquote{content} information (JOOCI) in speech. By using separate learnable parameters, JOOCI addresses this optimization challenge by modeling other and content information independently. Our results show that JOOCI consistently outperforms other SOTA models of similar size (100 million parameters) and pre-training data used (960 hours) by a significant margin when evaluated on a range of speech downstream tasks in the SUPERB benchmark. Code and models are available at TBA.",
    "keywords": [
        "SSL",
        "Speech Representation Learning",
        "Joint Optimization"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "A framework capable of jointly learning comprehensive speech representations i.e., the content and other information.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-13",
    "forum_link": "https://openreview.net/forum?id=DnfPX10Etk",
    "pdf_link": "https://openreview.net/pdf?id=DnfPX10Etk",
    "comments": [
        {
            "title": {
                "value": "Clarifying JOOCI\u2019s Position, Effectiveness, and Comparisons on the SUPERB Benchmark."
            },
            "comment": {
                "value": "### We thank the reviewer for their comments and the opportunity to address their concerns.\n\n* We kindly ask the reviewer to specify what they found unconvincing. The baseline (WavLM) we compare to is state-of-the-art on the SUPERB benchmark leaderboard: https://superbbenchmark.github.io/#/leaderboard. We would appreciate it if the reviewer could point to specific models or methods for additional comparison. For semantic tasks, JOOCI\u2019s performance is very close to that of WavLM, and we mention that WavLM\u2019s use of large batch sizes likely contributes to these minor gains. Therefore these small differences on the sematic tasks could be just noise. As shown in Table 6, with using only the layers with higher content information, a property of JOOCI's content encoder, we can close the gap even further on one semantic task (IC) and beat on the other semantic task (SF). For IC task, the difference is of 0.02 only. \n\n* We also request that the reviewer share specific model architectures they believe would strengthen the discussion and comparisons, allowing us to draw more general conclusions in the paper. \n\n* We will address the reviewer\u2019s comment on Figure 1 in the revised version. Since WavLM (previous SOTA on SUPERB) is trained with data augmentation, we apply data augmentation in JOOCI for fair comparison given the smaller, cleaner pre-training LibriSpeech 960 hours data used.\n\n* We will add a section discussing model architectures and their relation to the SUPERB benchmark in the revised version, as suggested by the reviewer.\n\n* WavLM is one of the most recent models aimed at learning representations across orthogonal tasks such as SID and ASR. Compared to WavLM, our method demonstrates significant improvements on both the tasks. Data2Vec, another method, achieves better results on ASR but is pre-trained specifically to maximize content information. Additionally, on the PR task, JOOCI outperforms Data2Vec, showcasing the effectiveness of the content encoder initialization we used.\n\nWe hope this addresses the reviewer\u2019s concerns, and we appreciate any further feedback."
            }
        },
        {
            "title": {
                "value": "Rebuttal on JOOCI\u2019s Framework and Comparisons to Existing Models. JOOCI is not doing any disentanglement."
            },
            "comment": {
                "value": "### We thank the reviewer for their thoughtful comments and the opportunity to address their questions and observations.\n\n* Firstly, we would like to clarify that JOOCI does not aim to remove other information from the content or employ a disentanglement module. Instead, JOOCI optimizes both content and other information jointly, using separate paths/encoders to maximize the representation quality without attempting a strict separation observed in WavLM.  In fact, the concept of disentanglement is only mentioned in Section 4.3 of the paper (in the second-to-last section before the conclusion) as part of an ablation study on understanding learned representations.\n\n* We are seeking further clarification regarding the statement: \"JOCCI relies on a pretrained method RDINO for training the other encoder, whereas baseline methods such as HuBERT do not.\" Many state-of-the-art methods, such as Vall-E[1], also use pretrained models (Encodec) as teachers. We would appreciate the reviewer\u2019s insight on how this is considered a weakness in the context of our work.\n\n* Regarding the comparison with ContentVec, SPIN, and Data2Vec, we believe these methods serve different purposes from JOOCI, focusing primarily on content representation without optimizing for both content and other information. Our content encoder is initialized from MS-HuBERT, which, as demonstrated on the SUPERB benchmark, outperforms ContentVec, SPIN, and Data2Vec in the phoneme recognition (PR) task. JOOCI, however, is designed to perform competitively on both PR and speaker identification (SID) tasks, demonstrating its ability to effectively handle both types of information. A direct comparison would be WavLM. \n\n* ContentVec and SPIN are models that fine-tune other pre-trained models, whereas JOOCI focuses solely on the pre-training stage. Given this distinction, we are unsure how including these models would contribute to strengthening the claims of our paper. \n \n* We would appreciate the reviewer\u2019s perspective on how evaluating the impact of random versus MS-HuBERT initialization, as well as training the model with just the GRL loss and without the RDINO teacher, might help corroborate the effectiveness of JOOCI.\n\nWe hope this addresses the reviewer's concerns, and we appreciate any further feedback that may help clarify or strengthen our presentation.\n\n\n[1] Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers"
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "### We thank the reviewer for their follow-up and appreciate the opportunity to clarify further.\n\n* The motivation described in the abstract is focused on optimizing the joint representation of content and other information by leveraging two separate paths/embeddings within a self-supervised learning framework. This approach is not intended for explicit disentanglement but rather for learning orthogonal representations without imposing a separation of information. This is different to current methods which encode other information in the earlier layers of the encoder, resulting in the model dividing its total capacity, limiting their ability to build complex hierarchical features effectively for other information (characteristic for deep learning) . Our results show that JOOCI is able to achieve competitive performance on both PR and SID tasks. This is different from previous methods, such as data2vec which maximize PR performance (data2vec) while lagging behind SID. Or WavLM which finds a local minima for PR and SID (dividing its total capacity). JOOCI achieves better performance jointly on PR and SID compared to earlier methods. \n\n* If there are specific lines in the paper that may have implied a disentanglement motivation, we would be grateful if the reviewer could point them out. This feedback would help us address any potential ambiguities and refine our presentation accordingly for future."
            }
        },
        {
            "comment": {
                "value": "Thank authors for the response.\n\nUnfortunately, I'm confused by the author response, stating that my summary misunderstood the intention of the paper. \"JOOCI is not designed as \"speech representation learning for disentangling content information from non-content information,\" \" -- isn't that the motivation stated in the abstract of the paper?"
            }
        },
        {
            "title": {
                "value": "Addressing Misunderstandings Regarding JOOCI\u2019s Methodology and Novelty.  JOOCI is not doing any disentanglement."
            },
            "comment": {
                "value": "### We sincerely thank the reviewer for their comments and for the opportunity to clarify certain aspects of the JOOCI framework's design and novelty.\n\n* We will clarify the main claim and clearly mention specific tasks instead of the using the SUPERB benchmark term. \n\n* Regarding the reviewer's remarks on JOOCI's novelty and its alleged similarity to previous work, we would appreciate further clarification. The reviewer has highlighted two specific papers, which both employ a disentanglement module and use supervised data during training in some way. However, JOOCI is fundamentally different: it does NOT include any disentanglement module, nor is it trained with supervised data during pre-training. This distinction contrasts with the reviewer\u2019s assertion about JOOCI\u2019s similarity to the referenced works.\n\n* Additionally, we respectfully note that the reviewer's summary of JOOCI appears to contain a misunderstanding. Specifically, JOOCI is not designed as \"speech representation learning for disentangling content information from non-content information,\" as the summary suggests. In fact, the concept of disentanglement is only mentioned in Section 4.3 of the paper (in the second-to-last section before the conclusion) as part of an ablation study on understanding learned representations. This brief remark reflects an observed behavior in the model\u2019s learned representations rather than a deliberate design goal or imposed structure during pre-training.\n\n* We will add the details of the model as suggested. Lastly, as suggested we will improve the writing of the paper. We have updated the rebuttal, which does address this issue to a major extent. \n\nWe hope this clarification addresses the reviewer's concerns and would be grateful for any additional feedback they might provide on this matter."
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method for speech representation learning, particularly, for disentangle content information from non-content information. The paper reported strong experimental results on SUPERB benchmark."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The proposed method is sound. The experimental results on a subset of SUPERB benchmark are strong."
            },
            "weaknesses": {
                "value": "- The novelty is limited. The proposed method is very close to a number of existing works, e.g.:\n  - Chan et al., Content-Context Factorized Representations for Automated Speech Recognition, InterSpeech 2022.\n  - Zhao et al., CCSRD: Content-Centric Speech Representation Disentanglement Learning for End-to-End Speech Translation, EMNLP 2023.\n- The main claim is flawed. The paper claims SOTA on SUPERB. However, it only reports experimental results on a subset of the tasks from SUPERB (7 out of 10).\n- The writing needs improvements:\n  - Importantly, the name \"other encoder\" is a poor choice, which causes a lot of confusion for reading. Some simple choices such as \"non-content encoder\" would do a much better job.\n   - Secondly, many small claims are questionable throughout the paper. A few examples:\n     - Abstract: content and non-content information are orthogonal -- in the words from the paper, \u201chow it is expressed\u201d depends on \u201cwhat is being said\u201d\n     - Sec 2.2: \"Since JOOCI uses separate learnable parameters, the losses are summed directly without requiring\nadditional hyperparameter tuning.\" -- The previous paragraph said the opposite: \" The GRL scale the gradients during backpropagation by a factor of 1/10, preventing interference with the other loss.\"\n  - Lacks details of the model. While references to prior works is great, for completeness of the paper, you should describe the details of you model clearly, so that the readers understand your approach without having to jumping to many other papers."
            },
            "questions": {
                "value": "See Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a self-supervised speech representation model that combines two encoders, one intended to encode linguistic content and the other intended for \"other\" content like speaker and emotion information, trained with different losses.  The idea is that, by training a single encoder with a single loss, previous approaches have trouble encoding these two types of information equally well.  The various elements of the model are largely borrowed from previous work, but combined in a new way.  The model is compared in terms of performance on 8 common tasks (from the SUPERB benchmark) to other commonly used models (HuBERT, WavLM), finding improved performance on 4 of the tasks.  The paper also includes some ablation studies and analyses of several model components."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ Addresses an important need to account for both linguistic and non-linguistic content in speech representation learning.\n\n+ Obtains impressive results on several tasks, including speech recognition and speaker identification."
            },
            "weaknesses": {
                "value": "- Presentation of many details is unclear.  For example, the definition of \"content\" and \"other\" is never clearly stated.  Also, the model description is very brief, leaving many details to cited papers or the imagination (for example, is prosody ever/always/sometimes considered \"content\"?).  Either the writing should be much more precise or the paper should include equations specifying all of the model components.  See some other specific questions below.\n\n- The key claimed contribution is that the model encodes both linguistic and non-linguistic information and that these are disentangled into the two encoders' representations.  However, the results don't quite show this, since the results on tasks are mixed and the analyses don't really demonstrate disentanglement (again see questions below).  Overall, I don't see the community starting to use this model as a replacement for other currently popular models.\n\n- Some of the experiments do not, as far as I can tell, show the claimed findings (see details in \"Questions\" below).  \n\n- The writing is in general hard to follow at times, in part due to many grammatical errors."
            },
            "questions": {
                "value": "- The paper states that WavLM the \"previous SOTA method\".  By what measure is WavLM SOTA?  On what task(s)?\n\n- I don't follow the sentences \"As a result, the model cannot fully leverage all layers ... within a single embedding.\" nor the following sentence \"The strategy of dividing the layers...\"  Can you clarify what is meant there?\n\n- The description of the split and append layer is a bit hard to follow.\n\n- In Eq. 1, the index d is never used in the summand.  Also, should \"MPL\" be \"L_MPL\"?\n\n- In Eq. 3, what exactly are Student^PN and Teacher^RDINO?\n\n- In Table 1, where are the results for FBANK and other competitor methods obtained from?  Citations should be provided.  I also suggest including MS-HuBERT since JOOCI is based on it, and ideally also data2vec which has good results on many SUPERB tasks (but please let me know if you think these would not be relevant for some reason).\n\n- I don't quite follow the sentence \"We augment the data very lightly, so not to interfere with the content encoder a lot and divide its capacity.\"\n\n- The description of the main results in Section 3.2 seems a bit misleading.  The paper states that the \"results clearly indicate that JOOCI outperforms the current state-of-the-art (SOTA) models on the majority of tasks, except few ...\".  However, in Table 1 JOOCI appears to outperform other models on exactly half the tasks, and it is never explained in what sense those models are SOTA (though they are clearly commonly used models).\n\n- I do not understand the purpose of the comparison in Table 2, since JOOCI is not an alternative to adapters.  Also, \"Houlsby\" and \"CHAPTER\" need to be defined.  \n\n- How does Figure 2 show the effect of data augmentation?  Is there a pair of curves that differs only in the use of data augmentation?\n\n- For Figure 2, more information is needed about the y-axis.  How is CCA similarity defined?  How are the word labels encoded and how many words are there?  There has been prior work using CCA similarity for layer-wise analyses, e.g. Pasad et al., \"Comparative layer-wise analysis of self-supervised speech models,\" ICASSP 2023.  Figure 2 seems similar to some of this prior work, and so it would also be helpful to state how your CCA-based analysis is the same or different, and whether your HuBERT results are similar to Pasad et al.'s.\n\n- The ablation study in Sec. 4.1 is a bit confusing to me.  It claims to separately show the effect of DGRL and data augmentation, but as far as I can tell these two variables are changed simultaneously in the experiments.\n\n- In Table 3, why are the \"-\" results not included?  If those could be included, they could help to show to what extent JOOCI-C and JOOCI-O specialize for linguistic vs. non-linguistic information.\n\n- In Section 4.2, I have trouble following the first paragraph.  What kind of information is considered \"higher-level\" in the \"other\" branch, and what is the \"same trend\" that is referred to here?\n\n- Section 4.3 claims to \"prove that JOOCI is able to disentangle content and other information\", but I don't follow how the results show this.  (Also, the word \"prove\" is too strong here, as in most descriptions of empirical findings.)\n\n- In Table 5, what is the difference between the experiments in the last two lines (both labeled \"JOOCI (6-11)\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This submission introduces a framework for distinct representation learning of \"content\" and \"other\" properties in speech. The authors report improved performance on certain SUPERB tasks compared to other systems. Additionally, the submission includes comparisons with adapters, ablation studies on encoders, data augmentation, and learned representations."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The research community is highly interested in the topic of speech representation learning.\n* The proposed method's evaluation on certain SUPERB tasks yielded better results compared to the cited systems.\n* The discussions and comparisons presented are technically sound."
            },
            "weaknesses": {
                "value": "Major issues:\n\n* The model's effectiveness is unconvincing. The baselines cited are outdated and not state-of-the-art, and the model's performances on the semantic tasks are not better.\n* The paper's discussion of different model architectures is shallow, limiting its contribution and making it difficult to draw general conclusions. \n\nMinor:\n\n* Figure 1 could be simplified by removing the hyperparameters.\n* The discussion of \"Data augmentation\" in Line 52 seems out of place, as the initial focus was on model architecture for speech representation learning."
            },
            "questions": {
                "value": "* Can you offer insights into the relationship between SUPERB downstream task performance and model architecture designs?\n* How do your results compare to recent speech representation work that has also been evaluated on SUPERB?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes to disentangle the content \"what is being said\" and other \u201chow it is expressed\u201d information present in the speech data. The paper proposes the JOCCI framework, which uses two submodules, focused on maximizing the content and the other information. The content module is trained with a self-supervised objective whereas, the other module is optimized with a teacher-student objective. A regularization loss is added to minimize the information overlap in the two submodules."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The paper is well-written and addresses an important challenge of disentangling content and other information in the speech representation.\n\n2) The model performs well on the SUPERB benchmark and outperforms HuBERT and wavLM."
            },
            "weaknesses": {
                "value": "1) The baseline comparisons are limited. There have been other attempts to remove other information such as speaker information from the self-supervised representations such as contentvec[1] and SPIN[2]. Even the MS-HuBERT model used for initializing JOCCI is missing from Table 1.\n\n2) JOCCI relies on a pretrained method RDINO for training the other encoder whereas baseline methods such as HuBERT do not. \n\n[1] \u201cContentvec: An improved self-supervised speech representation by disentangling speakers\n\n[2] Self-supervised Fine-tuning for Improved Content Representations by Speaker-invariant Clustering"
            },
            "questions": {
                "value": "1) How does the JOCCI compare to the contentvec, SPIN, and Data2vec models?\n\n2) What is the impact of initialization on the model performance e.g. random vs Ms-HuBERT initialization \n\n3) Can the model be trained with just the GRL loss and without the RDINO teacher?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}