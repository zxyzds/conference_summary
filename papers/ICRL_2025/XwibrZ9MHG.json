{
    "id": "XwibrZ9MHG",
    "title": "PokeFlex: A Real-World Dataset of Deformable Objects for Robotics",
    "abstract": "Data-driven methods have shown great potential in solving challenging manipulation tasks, however, their application in the domain of deformable objects has been constrained, in part, by the lack of data. \nTo address this, we propose PokeFlex, a dataset featuring real-world paired and annotated multimodal data that includes 3D textured meshes, point clouds, RGB images, and depth maps. Such data can be leveraged for several downstream tasks such as online 3D mesh reconstruction, and it can potentially enable underexplored applications such as the real-world deployment of traditional control methods based on mesh simulations.\nTo deal with the challenges posed by real-world 3D mesh reconstruction, \nwe leverage a professional volumetric capture system that allows complete 360\u00b0 reconstruction. PokeFlex consists of 18 deformable objects with varying stiffness and shapes. Deformations are generated by dropping objects onto a flat surface or by poking the objects with a robot arm. Interaction forces and torques are also reported for the latter case. \nUsing different data modalities, we demonstrated a use case for the PokeFlex dataset in online 3D mesh reconstruction. We refer the reader to our  [website](https://anonymized-pokeflex-dataset.github.io/) or the  [password protected supplementary material](https://drive.google.com/drive/folders/1d8iNoJZ0dUVlzP6XxP7xwGPhdVtwQ7du) for demos and examples of our dataset (password in pdf).",
    "keywords": [
        "Deformable objects",
        "Robotics",
        "3D mesh reconstruction."
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "Novel dataset for deformable objects.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=XwibrZ9MHG",
    "pdf_link": "https://openreview.net/pdf?id=XwibrZ9MHG",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces PokeFlex, a new dataset focused on real-world deformable objects for robotics applications. The dataset includes:\n\n- Multimodal Data: 3D textured meshes, point clouds, RGB images, and depth maps.\n- Objects: 18 deformable objects with varying stiffness and shapes, including everyday items and 3D-printed objects.\n- Interaction Protocols: Deformations induced by poking with a robot arm (with force/torque data) and dropping onto a flat surface.\n- Purpose: For downstream tasks like online 3D mesh reconstruction and enable applications such as real-world deployment of control methods based on mesh simulations.\n- Data Acquisition: A volumetric capture system for 360 deg. reconstruction and integrates lower-cost RGB-D sensors for reproducibility.\n- Baseline Models: Shows the use of PokeFlex in online 3D mesh reconstruction using different data modalities and provides evaluation criteria."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Rich, Real-World Dataset: PokeFlex offers a comprehensive multimodal dataset with 3D meshes, point clouds, RGB images, depth maps, and force/torque measurements, captured with a high-quality volumetric system. This real-world data bridges the sim-to-real gap, enhancing applicability in deformable object manipulation.\n- Reproducibility and Accessibility: Open-source 3D-printed objects and affordable sensors (Azure Kinect, Intel RealSense) make the dataset accessible and reproducible, while baseline models and evaluation metrics support benchmarking and further robotics research."
            },
            "weaknesses": {
                "value": "- Limited Object Diversity: While the dataset includes 18 objects, the diversity may still be limited compared to the vast range of deformable objects encountered in real-world applications. Notably, the dataset focuses on volumetric objects and excludes thin deformable items like cloth or cables, which are important in areas like garment handling.\n- Controlled Interaction Protocols: The poking and dropping protocols are specific and may not capture the full spectrum of possible deformations in less controlled or more complex environments. Additional manipulation actions could enrich the dataset.\n- Potential Bias in Data Collection: The use of a transparent acrylic stick for poking and specific dropping heights might introduce biases that limit the generalizability of the dataset to other types of interactions and objects.\n- Related Work Coverage: The paper could benefit from a more thorough comparison with existing datasets and approaches in the literature, such as DOFS [1], CEPB [2], and other relevant works on deformable object datasets and manipulation. Discussing how PokeFlex complements or improves upon these datasets would strengthen the contribution.\n\nReferences:\n[1] Zhang, Zhen, et al. \"DOFS: A Real-world 3D Deformable Object Dataset with Full Spatial Information for Dynamics Model Learning.\" arXiv preprint arXiv:2410.21758 (2024).\n[2] Tripicchio, Paolo, Salvatore D\u2019Avella, and Carlo Alberto Avizzano. \"CEPB dataset: a photorealistic dataset to foster the research on bin picking in cluttered environments.\" Frontiers in Robotics and AI 11 (2024)."
            },
            "questions": {
                "value": "- Q1: How do you plan to address the limitations in reconstructing fine-grained details for smaller objects in future work?\n- Q2: Are there plans to extend the dataset to include thin deformable objects like cloth or ropes, which are significant in many robotic applications?\n- Q3: Do you intend to incorporate more complex or varied manipulation protocols beyond poking and dropping to simulate a wider range of real-world interactions?\n- Q4: Is there any intention to provide additional annotations (e.g., segmentation masks, keypoints) to facilitate other types of learning tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors collected a real-world multimodal dataset of 18 deformable objects. The dataset includes 3D textured meshes, point clouds, RGB images and depth maps.The authors use a volumetric capture system to build the 360degree reconstruction. It includes two types of deformation, poking and dropping. \n\nThe author further use the dataset to train neural network models for deformed mesh reconstruction based on template meshes and different modalites included in the dataset. The neural network model architecture is reasonable and the perforamnce is good on the different trajectories of the same objects."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The main advantage of this dataset is that it contains paired 3D meshes and contact forces. They further design a deformation mesh reconstruction method which takes the template mesh and force or image of point cloud sequece as input. By using the GT deformation meth for training, the trained model performs good on different trajectoris of the same objects."
            },
            "weaknesses": {
                "value": "1. According to Table2, the academic value and the difference between the submsion and PLUSH(Chenetal.,2022) should be further discussed, since it is not difficult to reconstruct the meshes from point clouds. Why do we need the mesh? What other application can it enable on top of deformation reconstruction?\n\n2. The generalizability of the network on unseen objects is not evaluated, which may be the most valuable part of the model. If the model can only work on trained objects, the usefulness of this method is very limited.\n\n3. The technical novelty of the network model is limited and not validated through ablation studies."
            },
            "questions": {
                "value": "1. What are the densesyntheticPoint Clouds(5kpoints) and SparsesyntheticPoint Clouds(100points)? Why are the error still large when using synthetic point clouds? \n\n2. What is NVP? IT should be explained briefly to make the paper self-contained."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents PokeFlex, a dataset that features real-world paired and annotated multimodal data, including 3D textured meshes, point clouds, RGB images, and depth maps. This dataset aims to enhance various downstream tasks and to demonstrate its effectiveness in online 3D mesh reconstruction.\nPokeFlex consists of 18 deformable objects with varying stiffness and shapes. Deformations are generated by dropping these objects onto a surface or by interacting with them using a robot arm, with corresponding interaction forces and torques recorded. Additionally, the authors introduce a novel deep learning model to tackle 3D mesh reconstruction using diverse input data types."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The PokeFlex dataset stands out as an original contribution to the field. It features a diverse set of objects, including 3D-printed and everyday objects. This variety enriches the dataset and makes it applicable to a wide range of research scenarios. The inclusion of different types of input data\u2014such as 3D textured meshes, point clouds, RGB images, and depth maps\u2014enhances its utility for various downstream tasks.\nThe paper clearly conveys the significant effort involved in acquiring this dataset, highlighting the meticulous process behind its compilation. Additionally, the introduction of an innovative deep learning model to address 3D mesh reconstruction challenges is particularly noteworthy, showcasing the authors' commitment to advancing the field. Furthermore, the paper is well-structured and easily readable, making it accessible to a broad audience."
            },
            "weaknesses": {
                "value": "The paper presents results for only a limited subset of the 18 objects in the PokeFlex dataset\u2014showing performance for just one object in some instances, five in others, and graphs for three. To better assess the dataset's overall effectiveness, it would be beneficial to provide results for all objects. This comprehensive analysis would establish a solid baseline for future comparisons.\nMoreover, the choice of a transparent acrylic stick used to poke was not adequately justified or discussed. The implications of this choice on RGB and depth data need more attention, particularly regarding how reflection and refraction may distort images and how time-of-flight depth readings may be inaccurate around the stick.\nTo further evaluate the dataset's effectiveness, it would be helpful to have a benchmark model trained on the dataset itself to determine its robustness and construction quality. While the paper mentions the \"dropping\" method, the experiments reported focus solely on \"poking.\" Including results from the dropping method would provide additional insights into the dataset's applications.\nAdditionally, the model that utilizes multiple data modalities only explored combinations of images and robot data. It would be interesting to see how other combinations of data types perform. \nMoreover, in Equation 3, the threshold epsilon is not defined, nor is there clarity on how values like 0.2 or 0.5 are determined. Addressing these points would enhance the clarity and rigor of the research.\nAdditionally, several citations in the references appear to be incorrect or improperly formatted, specifically the third, thirteenth, fourteenth, eighteenth, and twenty-ninth citations.\nAnother point concerns object licensing: one of the objects (Stylized pizza slice) in the dataset is reportedly subject to a non-distributable license, yet this restriction is not noted. This oversight may lead to potential misuse or distribution issues.\nFinally, neither the full dataset nor the trained deep learning model is currently available, which limits reproducibility. The sample dataset provided in the supplementary materials includes only one object, a dice, rather than a representative subset or the entire collection. Making the full dataset and model accessible would not only improve transparency but also enable a more comprehensive evaluation by the research community."
            },
            "questions": {
                "value": "1) In Equation 3, the value of epsilon is not specified. What value does epsilon take to define that the point p_i\u200b is close enough to the contact point t?\n2) In Equation 3, how was the value of 0.2 chosen, and what is the reasoning behind this choice?\n3) Why is the ROI loss weighted by 0.5 in the total loss calculation?\n4) In Section 4.2, it is mentioned that the train-validation split was generated by randomly selecting one sequence per object. Are the sequences consistent across all objects? For the multi-object training split (Section \"Learning from different data modalities\"), how was the split handled? Did you consider a sequence in the validation set for one object that fell into the training set for another?\n5) For the single-object training described in the section \"Learning from RGB images of different cameras\", could you provide results for additional objects? Results from only one object do not yield a statistically significant sample for generalizing findings.\n6) For Figure 11, could you include data for additional objects (potentially in the supplementary materials)? In this case, what is the training setup\u2014is the dataset composed of 20k samples, or just the 8.4k active paired poking frames? If the dataset used was the full 20k samples, how was it balanced to avoid an overrepresentation of frames without poking compared to those with poking?\n7) Still regarding Figure 11, the x-axis represents distance values\u2014how many samples are averaged at each value? Are there more samples at 0.1 than at 0.4? If so, how is this imbalance managed during training and validation?\n8) Were environmental conditions, such as lighting, explicitly considered during dataset acquisition? Was any analysis performed to evaluate how variations in lighting or surrounding conditions might influence the networks used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "It is important to specify whether the objects used in the dataset are under open licenses. For instance, as noted earlier, the pizza model appears to have a license that restricts redistribution. Clarifying the licensing status of each object in the dataset would help ensure compliance with legal guidelines and provide transparency to other researchers regarding any limitations on dataset sharing and reuse."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper mainly introduce a deformable objects realworld datasets featuring real-world paired and annotated multimodal data. The datasets can be mainly used for online 3D mesh reconstruction. The dataset has different data modalities and is primarily collected using a professional capture system, with the collection process also incorporating cameras such as Kinect and RealSense, and simultaneously synchronizing with the robotics system. The paper further shows the application of this datasets on mesh reconstruction and has comprehensive evaluation of learning-base reconstruction."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ RealWorld dataset: The most prominent feature of this paper is that it presents a real-world dataset featuring real-world meshes. Benchmarks for real-world meshes in robotics are relatively scarce. Additionally, the paper demonstrates good reproducibility, providing detailed 3D printing information in the appendix.\n+ Multimodal and Rich: Furthermore, this dataset is a multimodal dataset that, in addition to using high-cost acquisition systems, incorporates the commonly used RealSense camera as an eye-in-hand sensor and the Kinect as an eye-on-base sensor. The settings align well with robotic tasks and visual tasks within the field.\n+ Comprehensive evaluation on reconstruction task: Finally, the paper conducts detailed experiments on reconstruction tasks, including the use of RGB data and multimodal data."
            },
            "weaknesses": {
                "value": "+ The purpose of the dataset: One major issue with this paper is that it seems there are few other application tasks beyond online mesh reconstruction for which the dataset can be utilized.  Although online mesh reconstruction is an interesting computer vision task in itself, it appears that there are few practical applications for online mesh reconstruction. Therefore, this work seems to fall short of expectations in various fields. I will elaborate on this point from three different areas.\n     + robotics: In robotics, especially in the field of deformable object manipulation, meshes are rarely used; point clouds are more commonly employed[2][3][4]. The main challenges in this area are often related to occlusion and pose estimation issues. For example, Garment Tracking [1] addresses the problem of garment point cloud completion, but this work does not seem to tackle occlusion problems, thus reducing the significance of online mesh reconstruction in this context.\n     + CV: This task has limited difficulty in the computer vision field, primarily because the occlusion problem is less significant and the deformation of plush toys is minimal compared to garments. For instance, in dropping tasks, there appear to be only translational and rotational changes. Real-time 6D pose detection has already been extensively studied, so this part of mesh reconstruction can be easily addressed using mesh + pose estimation techniques.\n+ Number of object: The scale of 18 objects is small, and each object providing only two trajectories (poking and dropping) results in a limited data size. This makes it difficult to serve as a large-scale evaluation and can only be used for real-world fine-tuning. Papers in similar categories  such as garmentlab[2] have scanned nearly 100 objects, including at least 20 complex plush toys.\n+ Object Selection: The deformable objects chosen for this paper are somewhat too simple. There are many more complex plush toys, such as teddy bears, which could also be used for these tasks and would better reflect real-world scenarios. These more complex objects often involve occlusion issues, making the work more meaningful. Most of the objects selected in this paper are simple shapes like cubes or cylinders, which limits the scope and applicability of the task.\n+ Action Selection: The actions chosen, poking and dropping, are overly simplistic and lack practical value, especially in the robotics domain. The paper could benefit from including more complex actions, such as pick-and-place, which would add significant practical value and better reflect real-world robotic tasks.\n+ Simulation Evaluation: The authors mention in the paper that existing simulation methods require real-world fine-tuning. However, simulations actually perform very well in tasks such as dropping and poking, for example, using the FEM algorithm in IsaacSim. The authors need to include validation within simulators to verify whether the meshes scanned in the paper can be used for simulation and to conduct a thorough analysis of the differences between current simulation algorithms and real-world conditions. This would strengthen the setting of the paper.\n+ Action Protocol: The action protocol in this paper is vague. I recommend that the authors follow established protocols from datasets like YCB or GarmentLab[2] to refine and standardize their action protocols. This would enhance the clarity and reproducibility of the experiments.\n\n[1] GarmentTracking: https://github.com/xiaoxiaoxh/GarmentTracking\n\n[2] GarmentLab: https://garmentlab.github.io/\n\n[3] UniGarmentManip: https://warshallrho.github.io/unigarmentmanip/"
            },
            "questions": {
                "value": "+ Groundtruth mesh: If my understanding is correct, the ground truth mesh is obtained using an MVS (Multi-View Stereo) system. The paper's discussion of this point is somewhat unclear.\n+ Multimodal: In typical robotic tasks, the position and action of the end-effector are also crucial. Here, the authors use a transparent stick for poking, but the dataset should incorporate the point cloud or mesh of the stick into the trajectory. This can be simply achieved by combining the robot's position with the mesh of the stick. Without this integration, it is difficult to effectively utilize the robot action information provided in the dataset.\n+ Error of UR force and torque: The force and torque errors of the UR robot are significantly higher compared to other robots, such as the Franka Emika robot, as noted in the official product sheets.[1] The authors should discuss the impact of these errors on the experiments and results presented in the paper. \n\n[1] https://assets.ctfassets.net/oxcgtdo88e20/4zQbvCXiDH2RzaaiiuwWci/ca242c85669825e8c25ff3467f8774da/UR5e_Product_Factsheet_EN_WEB.pdf"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethnic review needed."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}