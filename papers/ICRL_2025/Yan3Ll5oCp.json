{
    "id": "Yan3Ll5oCp",
    "title": "Model Collapse Analysis and Improvement for Rectified Flow Models",
    "abstract": "Generative models aim to produce data indistinguishable from real distributions, but training on self-generated outputs can lead to model collapse, degrading performance. Focusing on Rectified Flow\u2014a simulation-free model prone to this issue due to its iterative use of self-generated data\u2014we provide a theoretical analysis of model collapse starting from Denoising Autoencoders. To prevent collapse, we propose methods that incorporate real data into the training process, even without direct noise-image pairs. Our approaches, Reverse Collapse-Avoiding (RCA) Reflow and Online Collapse-Avoiding Reflow (OCAR), effectively prevent collapse while maintaining sampling efficiency. Experiments on standard image datasets demonstrate improved generation quality and reduced sampling steps, confirming the effectiveness of our methods.",
    "keywords": [
        "Model Collapse",
        "Generative Models",
        "Iterative Training",
        "Diffusion",
        "Rectified Flow"
    ],
    "primary_area": "generative models",
    "TLDR": "We present a theoretical framework to analyze the behavior of model collapse in simulation-free generative models and propose methods to prevent it.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Yan3Ll5oCp",
    "pdf_link": "https://openreview.net/pdf?id=Yan3Ll5oCp",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes to avoid model collapse in rectified flow model for generative modeling. From theoretical analysis, this paper shows that the training of rectified flow model is affected by the training samples and proposes reverse collapse-avoiding reflow by mixing synthetic and real reverse pairs. This paper shows a toy example on Gaussian data distribution, CIFAR-10, and CelebA-HQ256 dataset. Overall, this paper shows some point interesting to avoid model collapse in rectified flow model, however, it seems this paper is not ready for a strong submission. Therefore, I slightly lean to reject this paper, but may change my final rating after reading other reviewers' comments and authors' rebuttal."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+ This paper provides theoretical analysis for rectified flow model, and points out the connection/differences to diffusion models. The rectified model is more efficient in sampling than diffusion models.\n\n+ According to the experiments with Gaussian distribution, this paper shows the effectiveness of the proposed solution."
            },
            "weaknesses": {
                "value": "+ Paper writing needs to improve. It seems this paper is completed in the rush and not ready for a strong submission. There are too many \"?\" in the Figure/Table/etc.\nLine 310 Appendix?\nLine 340 Figure ?\nLine 372 Algorithm?\nLine 417 Appendix ?\nLine 514 Table ?\n\n+ This paper claims the experiments are conducted for CIFAR-10 and CelebA-HQ256 datasets, however, the quantitative numbers and qualitative results are not shown in the main submission. Lacking of experiments is a fatal point and hard to make readers convinced.\n\n+ As mentioned in the abstract and introduction, the proposed method can be used to generate high resolution images. However, 256x256 images are generated as reported, which is not enough. Higher resolution such as 1024x1024 will be more interesting."
            },
            "questions": {
                "value": "What is the relationship between \\head{x}^{(i)} and x^{(i)}? Does the method sample x^{(i)} from the dataset randomly?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses the issue of k-rectified flow, specifically focusing on the problem of drift that eventually leads to mode collapse. According to the authors, the root cause of this issue is that reflow relies solely on synthetic data, causing the generated distribution to drift too far from the original real image distribution over time. This insight stems from a theoretical framework based on Denoising Autoencoders. To mitigate this, the authors propose incorporating real images into the reflow process. By applying inversion on these images, they obtain inverted noise-real image pairs that can be integrated into the reflow process (RCA). Additionally, to improve efficiency, they introduce an online version that generates and mixes inverted noise-real image pairs dynamically."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is technically rigorous, with a strong foundation in theoretical analysis that clearly leads to practical implementation. Using Denoising Autoencoders (DAEs) to illustrate mode collapse is particularly effective, as it provides a helpful visualization of the issue. The topic (mode collapse) is a fundamental limitation in k-rectified flow models, and addressing it could have significant impact on improving model stability. The proposed methods offers a well-rounded solution and quite efficient."
            },
            "weaknesses": {
                "value": "Presentation:\n- The presentation lacks thoroughness and one major part is put into the supplementary material.\n- A lot of missing/ undefined references\nExperiments:\n- Not in main paper\n- A lot of promised results are not presented, all the outcome is only on toy samples (gaussian dataset). All the experiment upon real dataset such as upon CIFAR-10, CelebA-HQ are mentioned however no where to be found. Also lacks of metrics to check about mode collapse such as recall.\n- Also no qualitative results upon the real datasets.\n- Lack of ablation studies upon several aspects: amount of real dataset samples, mix ratio $\\lambda$,..."
            },
            "questions": {
                "value": "The author should address problems mentioned in weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper analyses model collapse in rectified flow models, i.e. models that train on data-noise couplings induced by themselves. The main observation is that with the number of such reflow iterations the quality of the generated data decreases. The authors provide a theoretical investigation of the phenomena for the simplistic case of linear denoising autoencoders and based on that they propose several schemes of incorporating the original real data in the subsequent reflow iterations to prevent model collapse. The main idea is to map the original data to noise using the reverse ODE/SDE of the learned vector field. Experimentally, the authors justify the claims on gaussian data."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The motivation delivered in the introduction is clear. The investigated problem is interesting and the approach presented in the paper is, to the best of my knowledge, original. The authors also provide theoretical analysis of the problem."
            },
            "weaknesses": {
                "value": "The main weakness of the paper is the presentation. Frankly, the paper seems to be quite raw: there are a lot of typos, poor formatting of the equations (e.g. Equation 5), undefined variables (e.g. $\\Phi$ in Line 264, $U^*$ in Line 278 or $E_j$ in Line 282), broken references to the results (e.g. Line 310, 340, 372, 417, 514), missing results (experiments on CelebA were claimed, but never presented in the paper). Besides this, most of the evaluation is limited to only toy data (Gaussian-to-Gaussian mapping) and the results on real data (CIFAR10) are mixed (see Questions). Because of this, and despite the listed strengths, I cannot vote for acceptance of the paper."
            },
            "questions": {
                "value": "Here are some questions to the authors and further concerns that influenced my decision:\n- The connection between the theoretical analysis and the proposed method is unclear. How does integrating real data in the training help breaking the bound in Theorem 1? Appendix A.3 is the closest to discussing this, but given the poor presentation quality, it seems to be isolated from the rest of the paper.\n- The mixing scheme in Equation 11 is questionable. As far as I understand, the pairs are created by mixing independent synthetic and real data with a convex interpolation. In principle, this can destroy the target distribution. Could the authors provide more details regarding this? Another interpretation could be training on both pairs without mixing them, but maybe with balancing the ratio between synthetic and real data, if needed.\n- The FID on CIFAR from Figure 1 (4.67 at 10th Reflow-RCA) contradicts with the FID in Figure 5 that seems to be above 35. Could the authors clarify this?\n- How do the performances of the full model on real data differ with and without RCA? Based on the results presented in Figure 5, RCA is also prone to model collapse, although not as much as the vanilla Reflow. So there is a question, whether the model actually benefits from more than 1-3 Reflow iterations. If not and if the decrease in quality due to model collapse is not significant for the first couple of Reflow iterations, then the advantages of using RCA are unclear."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}