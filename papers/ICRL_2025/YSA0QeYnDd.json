{
    "id": "YSA0QeYnDd",
    "title": "Inference of Evolving Mental States from Irregular Action Events to Understand Human Behaviors",
    "abstract": "Inference of latent human mental processes, such as belief, intention, or desire, is crucial for developing AI with human-like intelligence, enabling more effective and timely collaboration. In this paper, we introduce a versatile encoder-decoder model designed to infer  evolving mental processes based on irregularly observed action events and predict future occurrences. The primary challenges arise from two factors: both actions and mental processes are irregular events, and the observed action data is often limited. To address the irregularity of these events, we leverage a temporal point process model within the encoder-decoder framework, effectively capturing the dynamics of both action and mental events. Additionally, we implement a backtracking mechanism in the decoder to enhance the accuracy of predicting future actions and evolving mental states. To tackle the issue of limited data, our model incorporates logic rules as priors, enabling accurate inferences from just a few observed samples. These logic rules can be refined and updated as needed, providing flexibility to the model. Overall, our approach enhances the understanding of human behavior by predicting when actions will occur and how mental processes evolve. Experiments on both synthetic and real-world datasets demonstrate the strong performance of our model in inferring mental states and predicting future actions, contributing to the development of more human-centric AI systems.",
    "keywords": [
        "temporal point process",
        "logic rule",
        "human-AI collaboration"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=YSA0QeYnDd",
    "pdf_link": "https://openreview.net/pdf?id=YSA0QeYnDd",
    "comments": [
        {
            "summary": {
                "value": "This paper proposed a model combining rule-based learning approaches with statistical approaches to infer evolving mental states and the resulting action seuquences of human beings. The proposed model is advantageous to predict some irregular action events, which is the severest weakness in statistical approaches that strongly depends on sufficient data collections. The proposed decoder is constituted of ruled-based modules which can provide interpretability to some extent. Furthermore, the proposed backtracking mechanism can detect the hiddenly evolving mental states and thus correct the action predictions. The experimental results demonstrate the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper combined the statistical model with the rule-base learning approaches, in addressing human behavior prediction. Although in general this kind of combination has been always a research trend in rule-based learning comminity, it is still an intriguing and original approach to be applied in this research problem. One of prominent reasons is that the incorporation of rules can indeed give some interpretation of human behaviors prediction, especially when data is lacking. In my view, this is no doubt a trend of machine learning research in the future.\n2. The description of the proposed approach is detailed, with sufficient descriptions on how each module is modelled. The experimental settings are clear and results are compred with multiple baseline algorithms. The illustration of figures helps understand the virtue of the proposed approach.\n3. Although this paper lacks some explanations about why some modules are modelled in specific ways, the general description of the proposed modules have been clearly explained. Also, it has evident motivations and sufficient experimental results. Standing on all these point, the quality of this paper is moderate.\n4. The research question studied in this paper is significant. Human behavior prediction is always a fundation in dowstream tasks such as human-Al interaction. Most of existing reserch focused on employing pure statistical models to answer this question. One reason is that statistical machine learning has stood out in contrast to logic-based systems since 1990s, and thus people who are knowledgable in logics are not as many as people who are expertised in statistical machine learning. From this perspective, the incorporation of (temporal) logic-based learning into statistical machine learning implemented in this paper is worth encouraging."
            },
            "weaknesses": {
                "value": "1. The paper writing is not good enough. Except that the motviation of this paper (research question) is easy to understand, the rest of contents are more like accumulation of descriptions, lacking clear logic to link the contituents (paragraphs) of this paper.\n2. Typo in line 158-159: Defome -> Deform\n3. In line 170-171, you mentioned that the instantaneous event probability is modelled via the hazard function. Could you give more reasons to explain why the hazard function is a the choice? For example, what is the advantage of this model than others?\n4. In line 179, you mentioned that once a mental event occurs, the discrete-time survival process resets. How is this statement justified by equation (1) and equation (2)? Some more details are needed.\n5. Why is the time embedding for the observed action time modelled as equation (4)?\n6. Why is the probabilistic model to sample mental event type modelled in equation (8)? Especially, is there any insights into adding log probability (\\\\(p_\\{|xi\\}|)) to collected samples (from \\\\(g_\\{|xi\\}\\\\))?\n7. One of the most serious weaknesses to logic-based learning is the construction of logic rule templates. I would like to see some more discussion on the remedy for dealing with situations that are less knowledgable or strange to humans.\n8. It is evident that the resolution of discretized time grid would strongly influence the performance of the proposed approach. Could you give some more discussion on some potential strategy to decide this hyperparameter?"
            },
            "questions": {
                "value": "Please address t he concerns in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces an encoder-decoder framework that infers latent human mental states (e.g., beliefs, intentions) from irregularly observed action events and predicts future actions. By integrating a temporal point process model with logic rules as priors and a backtracking mechanism, the model improves prediction accuracy and adapts effectively to real-time changes. Its performance is validated through experiments on synthetic and real-world datasets, outperforming existing baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The research problem is valuable in perception level task and can contribute to practical applications such as autonomous driving\n- Thorough experiments reveal promising results"
            },
            "weaknesses": {
                "value": "- Limited readability: the paper missed several key points for readers without much related background to understand the task. For example, is the task generative or predictive (i.e., does it generate the action and mental thoughts directly or does it select from an action and mental space); in a high level, how does the backtracking mechanism decide whether to generate/predict a mental state or an action? Instead of explaining a specific step in the mechanism, readers may want to understand the mechanism from the motivation.\n- If the model is simply selecting a mental state or an action out of a candidate pool, then the model is not able to intrinsically understand the environment and advanced human perception. Generating and mimicking human behaviors requires the model to process the information from the environment and act like human based on word knowledge and rules, which the proposed framework cannot comprehend as it has not seen it all (i.e., gone through massive pre-training). Therefore, I challenge the \"understand\" behavior referred in the title."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The article proposes an encoder-decoder model for inferring human mental states, based on irregular behavioral events to predict future actions. By leveraging temporal point processes and a backtracking mechanism, it captures behavioral dynamics, and introduces logical rules to enhance inference capabilities when data is limited. Experimental results demonstrate that this model outperforms baseline methods in intention inference and prediction accuracy."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The formulation in the article is relatively clear in terms of definitions and inference logic, particularly in the sections on conditional entropy and probabilistic modeling (such as the conditional entropy expression in formula (15)). These formulas effectively capture the temporal relationships between events. \nThe experiments in the article are relatively thorough, considering a comprehensive range of comparisons."
            },
            "weaknesses": {
                "value": "1. The model's performance is dependent on the quality of the logical rules defined and generated; if the rules are of low quality, it may affect inference results. \n2. The backtracking mechanism may increase computational complexity, and further discussion on its design could be interesting. Consider considering a priority filtering mechanism within the model to conduct backtracking checks only on the most relevant events, which could make the approach more intriguing. \n3. Currently, the model relies on predefined logical rule templates. While this offers strong initial inference capabilities, it may limit the model's adaptability in complex scenarios."
            },
            "questions": {
                "value": "1. If the prediction remains inaccurate after multiple rounds of backtracking, how does the mechanism adjust or avoid getting stuck in an infinite loop?  \n2. The paper mentions that the rule generator uses predefined templates and a column generation algorithm to generate logic rules, but the specific process and optimization details are not described. How have the most critical rules for improving model performance been identified?  \n3. Logic rules are embedded as prior knowledge to enhance inference capabilities, but do different datasets have different requirements for rules?  \n4. Does the performance of the backtracking mechanism vary depending on the embedded logic rules?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces an versatile encoder-decoder framework to infer and predict evolving human mental states from irregular action events. By implementing a temporal point process model, the authors handle the irregular nature of both action and mental data. A key enhancement is integrating a backtracking mechanism in the decoder, improving the accuracy of future action and mental state predictions. The model also incorporates logic rules as priors, providing robustness in scenarios with limited data availability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper proposes a novel model for asynchronous action sequence modeling and inferring latent mental events. It utilizes a flexible encoder-decoder architecture incorporating predefined temporal logic rule templates as prior knowledge and introducing a rule generator to refine them. The introduction of the backtracking mechanism improved the stability of the model\u2019s predictions. This method demonstrates promising results in both synthetic and real-world datasets. While the integration of rule-based logic for dynamic prediction adjustment presents an interesting angle, this concept may echo principles already employed in fields like imitation learning, where actions are predicted based on observed sequences and thought cloning that attempts to replicate cognitive processes."
            },
            "weaknesses": {
                "value": "W1. Encoder-decoder architectures and the inference of mental states have been explored in adjacent areas such as imitation learning and thought cloning. These methodologies similarly attempt to capture and reproduce complex patterns of behavior or cognitive states based on observed actions or external stimuli. How does the proposed model's use of temporal point processes and rule-based logic distinctly improve upon or differ from similar mechanisms used in imitation learning or thought cloning? Can you provide specific examples or comparative analyses that highlight these differences?\n\nW2. The paper does not discuss the computational demands and scalability of the proposed model, which is crucial for assessing its practical applicability in real-world scenarios.\n\nW3. The model's dependence on manually defined logic rules limits its flexibility and may introduce potential biases, reducing its ability to adapt to new data.\n\nW4. In the experimental section, the authors validated their method's effectiveness on both synthetic and real datasets. Although ER% and MAE provide basic quantitative metrics for model performance, considering more evaluation dimensions and a more detailed experimental design could offer deeper insights when dealing with complex psychological prediction models. Particularly, the method seems very similar to thought cloning, and more evaluation metrics should be considered for real datasets like Hand-Me-That to validate the method's practical effectiveness. Here are some references:\n\n[1] Thought Cloning: Learning to Think while Acting by Imitating Human Thinking\n\n[2] Enhaning Human-AI Collaboration Through Logic-guided reasoning \n\n[3] Infer Human\u2019s Intentions Before Following Natural Language Instructions\n\nW5. Although the model of mental states and action events is introduced, it lacks a detailed explanation of how sub-goals or mental states affect the generation of action events in long horizon tasks, or how action events feedback influences the prediction of mental states. This missing or unclear interaction could make the model's theory perplexing."
            },
            "questions": {
                "value": "Q1. The author mentioned, \"A novel backtracking action sampling mechanism iteratively refines predictions, significantly improving responsiveness to real-time fluctuations in human behavior.\" However, in the decoder (Section 4.2), the column generation algorithm involves frequent rule updates and optimizations, which may lead to high computational costs and execution times, especially when the rule set is large or the data volume is substantial. The backtracking mechanism (Section 4.4) sounds innovative, but repeatedly adjusting previously inferred mental states may also require significant computational resources, particularly in dynamic and changing environments. Therefore, could the authors include a comparison of computational costs in the experimental section to assess how well this mechanism performs in real-time applications?\n\nQ2. In Formula 13, the authors use the Monte Carlo method to estimate expectations, which may introduce bias, especially when the sample size is insufficient. Is there any theoretical or empirical analysis to address this issue?\n\nQ3. In Tables 4 and 7, the authors presented ground truth temporal logic rules and corresponding weights for Syn Data-1 and Syn Data-2. How are the rule weights determined? Are they predefined or calculated? Why are they 0.6 or 0.8, and how do these rule weights impact the results?\n\nQ4. The preliminaries mention the use of DT-RP to model mental states in discrete time, while the generation of action events is simulated in continuous time by TPP. Could the simultaneous use of discrete and continuous time in the model possibly lead to inconsistencies in time scales? How do the methods ensure effective transition and interfacing between these two-time scales?\n\nQ5. Although the model of mental states and action events is introduced, it lacks a detailed explanation of how sub-goals or mental states affect the generation of action events in long horizon tasks, or how action events feedback influences the prediction of mental states. This missing or unclear interaction could make the model's theory perplexing.\n\nQ6. Although self-attention can handle action sequences, its decision-making process is usually not transparent, with poor interpretability. Could techniques like visualization of attention weights be employed to explain how the encoder's mental state influences actions, especially when actions are frequent, or are there anomalies in the sequence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}