{
    "id": "fJ1hON2r2u",
    "title": "A Single Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces",
    "abstract": "Embedding spaces encapsulate rich information from deep learning models, with vector distances reflecting the semantic similarity between textual elements. However, their abstract nature and the computational complexity of analyzing them remain significant challenges. To address these, we introduce the concept of Semantic Field Subspace, a novel mapping that links embedding spaces with the underlying semantics. We propose \\textsf{SAFARI}, a novel algorithm for \\textsf{S}em\\textsf{A}ntic \\textsf{F}ield subsp\\textsf{A}ce dete\\textsf{R}m\\textsf{I}nation, which leverages hierarchical clustering to discover hierarchical semantic structures, using Semantic Shifts to capture semantic changes as clusters merge, allowing for the identification of meaningful subspaces. To improve scalability, we extend Weyl's Theorem, enabling an efficient approximation of Semantic Shifts that significantly reduces computational costs. Extensive evaluations on five real-world datasets demonstrate the effectiveness of \\textsf{SAFARI} in uncovering interpretable and hierarchical semantic structures. Additionally, our approximation method achieves a 15$\\sim$30$\\times$ speedup while maintaining minimal errors (less than 0.01), making it practical for large-scale applications. The source code is available at \\url{https://anonymous.4open.science/r/Safari-C803/}.",
    "keywords": [
        "Embedding Space Understanding",
        "Semantic Field",
        "Semantic Field Subspace",
        "Hierarchical Clustering"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We introduce a mapping between embedding spaces and the underlying semantics and a novel method for understanding the embedding spaces.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=fJ1hON2r2u",
    "pdf_link": "https://openreview.net/pdf?id=fJ1hON2r2u",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a method named SAFARI which constructs a cluster of semantic embeddings (I interpret as some meaningful word/phrase embeddings) from a set of such embeddings. The clustering algorithm is bottom-up hierarchical in nature, but employees a custom merge criteria based on their proposed semantic shift computation (Fig 4). The semantic shift between two clusters (hence two lists of embeddings) is calculated by first treating the list of embeddings as a matrix, and then finding their eigen-value and vectors, and finally taking the difference (eq 7). The authors noted that this process can be a bottleneck for the runtime because of the need of performing SVD, and proposed a approximation to this computation that seems to work well in the following experiments. Experimentally, the authors tested how strong their algorithm can separate embeddings from datasets coming from different domain, and compared with some baselines on classifying the embeddings of different textual classes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper proposes an interesting angle to semantic embedding spaces by quantifying it as the semantic shift when as new embedding is introduced.  An efficient workaround to their semantic shift metric that requires SVD is introduced, and might be useful in other scenarios where a similar calculation is needed."
            },
            "weaknesses": {
                "value": "1. The experiment section needs some more details, It is missing details on how the baselines are adapted in the work, the hyperparameters chosen, and how the dataset for the experiment is constructed.\n2. The intuition behind the semantic shift in eq (7) is lacking. It seems quite a big jump from an abstract definition of semantic fields to a concrete definition based on the differences of their eigen-values and eigen-vectors."
            },
            "questions": {
                "value": "1. The two weaknesses might be addressable via detailed experimental configurations and better intuition.\n2. There are many equations in the paper, the ones that are not referenced later probably does not need a label."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a new approach SAFARI, which is a novel algorithm for Semantic Field subspace determInation. It uses hierarchical clustering to uncover semantic structures present in a set of data representations. The paper claims to improve efficiency of the clustering process by efficiently quantifying the semantic drift during clustering. The paper presents several results on text classification datasets and analysis experiments to evaluate their method."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper tackles an important problem of automatically uncovering latent semantic concepts within a set of data representations. \n2. The paper uses several visual illustrations (Fig. 1-3) that help the reader understand better."
            },
            "weaknesses": {
                "value": "1. The soundness and novelty of the paper are not substantial. Specifically, the paper builds on a simple agglomerative clustering algorithm and modifies the cluster merging logic using semantic drift. \n2. The initial part of the paper is quite difficult to understand. This is mainly due to the inconsistent use of mathematical notations. Some of the claims in the Theorem proofs are incorrect. Please find my concerns below:\n\nDefinition 5 is confusing. What is matrix $\\mathbf{A}$? It wasn\u2019t defined before. What does it mean that $\\mathbf{A}$ can be approximated by $\\mathbb{S}$?\n\nWhat does the notation $F_{sem}(\\mathbb{S}) \u2248 \\Sigma, V^\\top$ mean? How can a matrix be approximated using two matrices?\n\nI\u2019m unable to follow the intuition behind Eq. 7. What does the difference signify? Earlier $F_{sem}(\\mathbb{S})$ was defined as a tuple of two matrices $\\Sigma, V^\\top$.\n\nLine 265: In the example, it is unclear to me how Fig. 4 is formed after just 3 iterations. In Algorithm 1, Line 5, the merging happens in a cluster-wise fashion, there are 8 merges in Fig. 4. \n\n\n$\\sigma_i([A_x|O]) = \\sigma_i(A_x)$: this isn\u2019t true as there are additional 0 eigenvalues for the matrix on the left.\n\nHow is Equation 8 proved after Theorem 2? The approximation is an upper bound on the original value. \n\n$r, r_1, r_2$ are not defined in Eq. 13.\n\n3. The presentation of the experimental section needs to be improved. The section should start by stating the overall goal of the empirical results and analysis experiments. Currently, the reader is confused about several setup issues. Please find my concerns below:\n\nThe experimental setup in Section 5.2 isn\u2019t clear to me. Are you using labels during training? If yes, then doesn\u2019t the reported results significantly underperform simple neural network classification baselines on top of BERT?\n\nIf the paper is posed as a new hierarchical clustering method, it should present results of dendogram accuracy on clustering datasets."
            },
            "questions": {
                "value": "Please respond to the questions in the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a method to understand embedding spaces through hierarachical clustering. The authors define semantic fields, semantic field subspaces, and semantic shifts. In the quest of quantifying semantic shifts, the paper introduces a method based on singular values. Then, one of the paper's key contributions is a method to approximate the semantic shift without the need for SVD, getting speed-ups of 15-30x."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- an interesting method to understand embeddings\n- the paper is very clear on the methodology part, building on a solid mathematical foundation"
            },
            "weaknesses": {
                "value": "- although the paper starts off with a generic motivation, it remains unclear how the proposed approach would tackle, e.g., mentioned problem of polysemiotic words\n- some assumptions seem to be made without justification. Unless I'm missing something, hierarchical clustering only ever assigns a point to a single cluster. This does not play well with the key motivation that words are polysemous and meaning determined by their use in context.\n- the experiment section is not very clear about what embeddings are used.\n- the evaluation seems weak (details below).\n- limited to text embeddings\n\nMy initial recommendation is to reject the paper. In short, although the method is interesting, the paper is not clear enough in connecting the proposed method to the motivation and the method seems to have major limitations that are either not addressed or at least unclear in the current form of the paper. Claims are vague (analyzing embedding spaces) and the experimental part is not very convincing as it does not fully capture the analysis/interpretability aspect laid out in the motivation.\n\nFor improving the paper (though going beyond the scope of the rebuttal), it would be interesting to see the method applied to contextualized word embeddings, e.g., of decoder-only language models, and maybe non-language embedding models, e.g., on images, graphs etc. This would support the generality that the authors suggest in the beginning of the paper."
            },
            "questions": {
                "value": "Q1: How do you disambiugate polysemous words in the clustering? It seems that the method only assigns a word to a single cluster?\n\nQ2: What embeddings are used for the experiments? Are they just tf-idf vectors? word2vec? contextualized embeddings from a language model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper is well-structured and introduces a novel approach to understanding the semantic structure of embedding spaces. It presents the SAFARI algorithm, which aims to uncover hierarchical semantic structures within high-dimensional embedding spaces using a method based on hierarchical clustering and Semantic Shift analysis. The concept and motivation are both relevant and timely, given the increasing need to interpret embeddings in NLP. The technical aspects are adequately explained, though a few areas could benefit from additional clarification to enhance accessibility for a broader audience. Also, the authors evaluate the proposed model from diverse aspects, including token classification accuracy, computational complexity and visualization. This diverse experimental validation is interesting and throughout.\n\nIn conclusion, this paper presents a relatively new contribution to embedding space interpretability. Its main strengths lie in the innovation of the SAFARI algorithm and the diverse experimental validation. With further clarifications and added discussions on the limitations, this paper could fairly impact interpretability research in embedding models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The introduction of Semantic Field Subspaces and the SAFARI algorithm is an interesting new approach. The approach is innovative and offers a structured way to interpret embedding spaces, potentially broadening the scope for further research in embedding interpretability.\n\nThe paper provides a comprehensive evaluation of SAFARI on multiple datasets, showcasing its performance in terms of accuracy, computational efficiency, and interpretability. This diverse evaluation adds credibility to the proposed method and its applicability across various real-world tasks.\n\nThe paper is well-written and well-organized. No English grammar errors are detected. The graphs and figures are presented with enough details. The proposed methods is well described."
            },
            "weaknesses": {
                "value": "Given all the merits mentioned in above, there is one relatively major defects: There is a lack of comparison, which is a common defects shared by many many papers. To this work, especially:\n\n1. There is a lack of baseline models: The authors compared the classification accuracy and training time of the proposed model against SVM, KNN, Random Forest and BERT on the AG-News dataset. The only notable baseline models is the BERT model. All others, SVM, KNN and random forest, cannot be regarded as formal baselines. You need to find more published models to serve as baselines. Now that you adapt BERT, why not further include RoBERTa, Electra and XLNet? More published baseline models are required to make the comparison more convincing.\n\n2. There is a lack of comparison scenario: The authors only conducted classification accuracy and training time comparison on the AG-News dataset. I think that is not enough. I am not familiar with this dataset, so I think the authors may provide a better background introduction to benchmark dataset used. Also, the authors may consider using more than one dataset for accuracy and training time comparison. In addition, beyond accuracy, how about F-1 score, precision, and recall comparison. Accuracy is sometimes not enough to fully evaluate the performance of a model."
            },
            "questions": {
                "value": "The authors need to find more published models to serve as baselines. Now that you adapt BERT, why not further include RoBERTa, Electra and XLNet?\n Why not conduct more comparisons on datasets other than AG-News? \n Why not use F-1 score, precision and recall other than classification accuracy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "**Title**: A Single Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces\n\n**Summary**: The authors present a novel algorithm titled SAFARI, which is claimed to discover hierarchical semantic structures in embedding spaces."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors introduce a novel algorithm inspired by classical hierarchical clustering. Throughout their work, they attempt to provide deep motivations for their design choices, although these justifications often appear somewhat forced or artificial.\n\nThe study aims to address the interpretability of embedding spaces generated by models such as BERT and Word2Vec. This is one of the most crucial yet poorly understood components of deep learning architectures in Natural Language Processing (NLP).\n\nThe proposed algorithm and accompanying theory could prove valuable in various scenarios where practitioners seek to interpret a model's embeddings. This potential for practical application is a noteworthy aspect of the work."
            },
            "weaknesses": {
                "value": "### Major Comments\n\n1. On line 160, the authors define the \"interpretable semantics\" of an embedding vector, $f_{int}(v)$. Their definition is based on the previously defined \"close neighborhood\" of said embedding vector, $\\mathcal{N}(v)$. In particular, they define $f_{int}(v)$ as any subset of $\\mathcal{N}(v)$. This implies that there are several \"interpretable semantics\" for each embedding vector. Furthermore, their notion of \"interpretable semantics\" is simply the intersection between the semantics of close neighborhood embeddings. It's unclear how this is \"interpretable.\"\n\n2. In lines 180-200:\n   - The authors rightfully claim that $f_{sem}$ is not computable and introduce the concept of \"semantic field subspace\" to address this issue.\n   - They motivate the choice to substitute a set of embedding vectors with a subspace of $\\mathbb{R}^d$, presumably to make the \"semantic field\" computable.\n   - However, their definition of \"semantic field subspace\" is still based on the non-computable $f_{sem}$, which seems to achieve very little.\n   - In definition 5, they approximate the \"subspace\" with a finite set of vectors, which raises the question of why the \"semantic field subspace\" was introduced in the first place.\n\n   This section feels like a convoluted and unconvincing justification for substituting $f_{sem}(v)$ with $v$. The argument would be more compelling without this section.\n\n3. In line 197 (definition 5), the authors state that the subspace $\\mathbb{S}$ can be approximated by a matrix $A$. This is unclear, as I am interpreting $\\mathbb{S}$ as a simple subset of $\\mathbb{R}^d$. Furthermore, the approximation $F_{sem}(\\mathbb{S})\\approx \\Sigma,V^T$ is not well-explained. The authors should detail this approximation more clearly.\n\n4. From what I can gather, each \"Semantic Field Subspace\" is simply a set of embeddings that the SAFARI algorithm found to be particularly semantically coherent. If this is the case, the name \"Semantic Field Subspace\" is somewhat confusing as it suggests a relation with fields and vector spaces that is not evident in the paper. I suggest the authors consider different nomenclature. I have similar concerns about the \"Semantic Field\" name.\n\n5. On line 247, the authors use the threshold $\\mu + 2\\tau$ to detect new clusters. They should better motivate this choice. It might be more natural to use a hyperparameter to control how much semantic shift should be allowed in each cluster.\n\n6. In section 5.2, it's unclear whether SAFARI accounts for the initial embedding training step (e.g., Word2Vec or BERT) in Figure 7. If not, the authors should mention this in the paper.\n\n7. The authors use clusters generated by SAFARI for text classification and compare it to alternatives such as SVM, BERT, and KNN. However, from what I can gather from their code, it appears these algorithms are trained on the top-n entities rather than directly on the textual data (which would be natural for BERT). The authors should clarify the training procedure for the baselines, at least in the appendix.\n\n8. The evaluation of SAFARI seems weak overall:\n   - Section 5.2 tests for text classification, but SAFARI's purpose is not text classification.\n   - Section 5.1 tests semantic field subspace alignment with BLINK entities at the dataset level, which seems less natural than using a single text classification dataset.\n   - Section 5.4 presents only an anecdotal example showcasing SAFARI's ability to uncover hierarchical structures.\n   - The absence of baselines makes it difficult to assess SAFARI's improvement over the state of the art.\n\n   I suggest the authors consider a more comprehensive evaluation.\n\n9. If SAFARI is indeed a clustering algorithm, shouldn't it be compared with other clustering algorithms rather than BERT or SVM?\n\n### Minor Comments\n\n1. On line 118, the authors mention the \"deep learning\" model $h:\\mathcal{X}\\rightarrow \\mathcal{E}$, but don't use it meaningfully in the rest of the paper. It would be clearer to simply describe $\\mathcal{E}$ as a set of embedding vectors, each representing a word in the vocabulary, possibly trained using a deep learning method.\n\n2. On line 128, I believe the authors meant the power set of $\\mathcal{M}$ (excluding the empty set) when writing $2^{|\\mathcal{M}|}\\setminus\\emptyset$. It would be clearer to write $2^{\\mathcal{M}}\\setminus\\emptyset$.\n\n3. In lines 125-135, the authors' argument about words not being fully interpretable in isolation seems flawed. Their example of \"Apple\" as both a fruit and a tech company doesn't support their point, as $f_{sem}(\\text{\"apple\"})$ could simply return $\\{\\text{\"fruit\"}, \\text{\"tech company\"}\\}$. The John Rupert Firth quote also seems irrelevant to their argument.\n\n4. On line 154, the authors' method for characterizing the \"close neighborhood\" of an embedding seems arbitrary. Using k-NN with $k=3$ to exclude strict synonyms may exclude relevant embeddings if a word has fewer than 3 synonyms. A distance-based test might be more appropriate.\n\n5. On line 167, the authors use the term \"field\" in a non-mathematical sense, which could be confusing. Consider changing the name to avoid this confusion.\n\n6. In Algorithm 1, line 5 (line 224), the authors write $d_{sem}(\\mathcal{C}_i,\\mathcal{C}_j)$, but $\\mathcal{C}_i$ and $\\mathcal{C}_j$ are sets of vectors while $d_{sem}$ is a distance function on vectors. The distance function should be specified more clearly.\n\n7. The authors use $\\tau$ to denote standard deviation. It would be more conventional to use $\\sigma$ instead.\n\n8. On line 389, the BERT reference points to the wrong paper."
            },
            "questions": {
                "value": "My primary concerns center on the paper's narrative and experimental validation. I am open to reconsidering my assessment of the paper in a future revision, particularly if the following points are addressed:\n\n1. Experimental Comparison:\n   - Provide experiments that demonstrate superior clustering capabilities compared to other clustering approaches.\n   - Note: Algorithmically generated datasets would be acceptable to keep the scale of experiments manageable.\n\n2. Narrative Refinement:\n   - Consider shifting the focus from \"an interpretability tool\" to a \"clustering algorithm for interpreting embeddings.\"\n   - Improve the nomenclature, or at least provide a clear rationale for the current terminology.\n\nI hope the authors find this review constructive. Despite my current reservations about the paper, I believe there is potential in this work and look forward to seeing how it evolves in the review process."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}