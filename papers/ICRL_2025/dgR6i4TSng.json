{
    "id": "dgR6i4TSng",
    "title": "Quantum-PEFT: Ultra parameter-efficient fine-tuning",
    "abstract": "This paper introduces Quantum-PEFT that leverages quantum computations for parameter-efficient fine-tuning (PEFT). Unlike other additive PEFT methods, such as low-rank adaptation (LoRA), Quantum-PEFT exploits an underlying full-rank yet surprisingly parameter efficient _quantum unitary parameterization_. With the use of Pauli parameterization, the number of trainable parameters grows only logarithmically with the ambient dimension, as opposed to linearly as in LoRA-based PEFT methods. Quantum-PEFT achieves vanishingly smaller number of trainable parameters than the lowest-rank LoRA as dimensions grow, enhancing parameter efficiency while maintaining a competitive performance. We apply Quantum-PEFT to several transfer learning benchmarks in language and vision, demonstrating significant advantages in parameter efficiency.",
    "keywords": [
        "parameter-efficient fine-tuning",
        "lora",
        "quantum machine learning",
        "orthogonality constraints"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=dgR6i4TSng",
    "pdf_link": "https://openreview.net/pdf?id=dgR6i4TSng",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces Quantum-PEFT, a quantum-inspired parameter-efficient fine-tuning (PEFT) framework for large language and vision models. Unlike traditional methods such as LoRA, which require low-rank adaptations of model weights, Quantum-PEFT uses quantum unitary parameterizations, resulting in a logarithmic scaling of parameters. Key contributions include the development of quantum-inspired modules using Pauli parametrization, significantly reducing the number of trainable parameters, while retaining competitive performance in various transfer learning benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Quantum-PEFT\u2019s application of quantum unitary parameterizations to PEFT is novel, differentiating it from conventional LoRA models.\n2. It drastically reduces trainable parameters without performance loss and it is very important for the field, especially for resource-constrained training."
            },
            "weaknesses": {
                "value": "1. I'm not familiar with quantum ML, so it's a bit hard for me to understand the core concept. I think it's also hard to digest for someone who is not familiar with quantum ML. But this is fine because I don't think the intended audience of this paper includes someone not familiar with quantum ML."
            },
            "questions": {
                "value": "See above. It would be great if authors could add some self-contained introduction to the necessary quantum concepts used in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 1
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Quantum-PEFT, a novel parameter-efficient fine-tuning method that uses quantum-inspired unitary parameterizations. The key innovation is using Pauli parameterization to achieve logarithmic scaling of trainable parameters with matrix dimensions, compared to linear scaling in traditional LoRA methods. The method is evaluated on language and vision tasks, demonstrating comparable performance to LoRA while using significantly fewer parameters."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper introduces a parameterization based on quantum circuits that require only (2L+1)log\u2082(N)-2L parameters, a significant improvement over LoRA's 2NK parameters.\n2. The reduction in parameters is experimentally verified.\n3. The authors proposed generalized RY and CZ quantum gates for arbitrary dimensions beyond power-of-2.\n4. The proposed method seems robust under quantization."
            },
            "weaknesses": {
                "value": "1. Analysis of how L layers affect entanglement capacity is limited. There is no evaluation of entanglement entropy between layers.\n\n2. The paper only focuses on benchmarking against LoRA/adapter variants on small models (such as GPT2). Benchmarks on larger (and newer) models such as LLaMA seem to be quite standard and more related to practical use cases. \n\n1. Some of the recently proposed PEFT methods are not adequately addressed/discussed (at least mentioned somewhere in the paper), e.g.\n\n[1] https://arxiv.org/abs/2403.17919 (NeurIPS)\n\n[2] https://arxiv.org/abs/2404.03592 (NeurIPS)\n\n[3] https://arxiv.org/abs/2405.12130 \n\n[4] https://arxiv.org/abs/2406.00132 (NeurIPS)\n\nBoth [3] and [4] seem to be able to model high-rank matrices, and [4] appears to be also based on quantum circuit."
            },
            "questions": {
                "value": "1. How does Eq. 4 work?\n2. What is the optimal number of alternating entanglement layers L for different model scales?\n3. For the quantum Shannon decomposition approach to handle non-power-of-two dimensions, how does the decomposition choice (N\u2081, N\u2082) affect model performance?\n4. Does the proposed method have a large computational overhead from the entanglement layers?\n5. How does the proposed method compare with other methods mentioned above? My understanding of the main difference between this method and [4] is the use of unitary and diagonal matrices. Is this correct?\n6. The proposed method focuses on reduction in parameters. However, [2] and [4] claim both reduction in parameter and performance improvement. Does the proposed method use even fewer parameters?\n6. How is the proposed method related to quantum machine learning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Quantum-PEFT, a novel parameter-efficient fine-tuning method inspired by quantum computing concepts. The core contribution lies in achieving logarithmic parameter scaling through Pauli parameterization while maintaining orthogonality via Stiefel manifold mapping."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Novel theoretical framework combining quantum-inspired parameterization with PEFT\n- Significant reduction in parameter count compared to existing methods\n- Comprehensive experiments across multiple tasks and architectures"
            },
            "weaknesses": {
                "value": "1. The paper lacks clear delineation of learnable parameters in the mathematical formulations. While extensive comparisons with LoRA are provided, the fundamental step of identifying and justifying which parameters are learnable is overlooked. This impedes understanding of the method's core mechanism.\n\n2. While the parameter efficiency is well-demonstrated, the computational complexity analysis is insufficient:\n- The introduction of Stiefel manifold mapping introduces additional computational operations beyond standard matrix arithmetic\n- Limited discussion of practical computational bottlenecks\n- Experimental results show no significant advantage in fine-tuning time efficiency\n- Absence of detailed analysis on computational overhead"
            },
            "questions": {
                "value": "* In Figure 5, which provides Intuitive illustrations of idea of Q-PEFT, is relegated to the appendix. Given its importance for understanding the method's foundations, especially for researchers from non-quantum backgrounds, innovations within this figure should be moved to the main text.\n\n* The mathematical notation requires better organization and clearer presentation. A dedicated notation section would significantly improve readability and accessibility.\n\n* What are the practical limitations and deployment considerations for real-world applications?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new formulation for low-rank based PEFT, based on quantum mechanical notation. \nThe authors demonstrate that their ansatz using Pauli Parametrization of low-rank adapters provides a superior paramters-accuracy ratio in numerious contemprary benchmarks"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I find the authors approach of unification of various low-rank adapter notations based upon quantum-mechanical notations interesting. \nThe paper has strong numerical evaluations."
            },
            "weaknesses": {
                "value": "While I enjoyed reading most of the paper, a few things are missing in the explanation of the method, causing some neccessary revisions. \n\n- The paper describe the compute ansatz in eq. (2), although the notations is quite heavy for readers that are not familiar with the field. The ansatz governs the compute instructions of the forward pass of the neural network. However, the backward-pass, especially computing the gradient update of the parameters is not covered. Especially with the dense notation, an algorithmic description is nessecary to explain how the gradient update is performed and a discussion of its computational feasibility should be added. \n\n- Updating a tensor-factorization as Tensor-Trains or Tucker Decomposition naively may induces unexpected perils for the low-rank optimization. In particular (in combination with the lacking update scheme), it needs to be discussed if a gradient descend on the Factors of the proposed parametrization indeed decreases the overall loss function of the method."
            },
            "questions": {
                "value": "- How does your factorized update scheme relates to robust Riemannian optimization schemes on Stiefel Manifolds, e.g. as described in \n[1] for Tucker Tensors and in [2,3] for Matrix Factorizations of the Type USV (as in Adalora)?\n\n- Is there an option to make the method rank-adaptive, as e.g. in [1,2,3] or AdaLora?\n\n[1] Emanuele Zangrando, Steffen Schotth\u00f6fer, Gianluca Ceruti, Jonas Kusch, and Francesco Tudisco.\nRank-adaptive spectral pruning of convolutional layers during training. In Advances in Neural\nInformation Processing Systems, 2024.\n\n[2] Steffen Schotth\u00f6fer, Emanuele Zangrando, Jonas Kusch, Gianluca Ceruti, and Francesco\nTudisco. Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations. In Advances in Neural Information Processing Systems,\n2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/\nfile/7e98b00eeafcdaeb0c5661fb9355be3a-Paper-Conference.pdf.\n\n[3] Steffen Schotth\u00f6fer and M. Paul Laiu. Federated dynamical low-rank training with global loss\nconvergence guarantees, 2024. URL https://arxiv.org/abs/2406.17887."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}