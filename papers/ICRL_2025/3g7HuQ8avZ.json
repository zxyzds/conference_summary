{
    "id": "3g7HuQ8avZ",
    "title": "OmniContrast: Vision-Language-Interleaved Contrast from Pixels All at once",
    "abstract": "In this work, we present OmniContrast, a unified contrastive learning model tailored for vision, language, and vision-language-interleaved understanding within multi-modal web documents. Unlike traditional image-caption data with clear vision-language correspondence, we explore a new contrastive fashion on maximizing the similarity between consecutive snippets sampled from image-text interleaved web documents. Moreover, to enable CLIP to handle long-form text and image-text interleaved content from web documents, OmniContrast unifies all modalities into pixel space, where text is rendered visually. This unification simplifies the processing and representation of diverse multi-modal inputs, enabling a single vision model to process any modality. To evaluate the omni-modality understanding of OmniContrast, we design three consecutive information retrieval benchmarks AnyCIR, SeqCIR, and CSR. Extensive experimental results demonstrate that OmniContrast achieves superior or competitive omni-modality understanding performance to existing standard CLIP models trained on image-text pairs. This highlights the potential of multi-modal web documents as a rich and valuable resource for advancing vision-language learning.",
    "keywords": [
        "vision-language contrastive learning"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=3g7HuQ8avZ",
    "pdf_link": "https://openreview.net/pdf?id=3g7HuQ8avZ",
    "comments": [
        {
            "summary": {
                "value": "This paper proposed OmniContrast, a unified contrastive learning model that processes multi-modal web documents by transforming all modalities, including text, into pixel space for a single vision model to handle. It achieves competitive or superior performance compared to standard CLIP models, demonstrating the value of multi-modal web data for advancing vision-language learning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. OmniContrast is among first to explore vision-language correspondence on image-text interleaved web documents in CLIP-style.\n2. Authors propose three consecutive information retrieval benchmarks, including AnyCIR, SeqCIR, and CSR to o facilitate the evaluation of omni-modality understanding.\n3. The effectiveness is validated by experimental results."
            },
            "weaknesses": {
                "value": "I am concerned about the motivation with the single modality in the pixel space. I believe it is limited in a few ways.\n\n1. It is ture that \"image-text interleaved content is natively present in visual formats such as screenshots\". Screenshot is a scenario, however, in more cases, such as the very rich html format image-text interleaved data (much richer than screenshots), images and texts are naturally presented in different modalities. \n\n2. Is it really practical unifying them into pixels? In many cases, we have seperated texts and images, where we have to re-organize them in the form of \"screenshots\" to use the model. It can be redundant. And organizing them in the form of \"screenshots\" itself can involve some issues, such as the limitation from the resolution, etc. I agree that CLIPPO (Tschannen et al., 2023) demonstrates that the vision encoder can learn meaningful textual representation directly from pixels, however, \"it is feasible to do so\" does not mean it is a good solution in different scenarios. I am looking for a strong motivation to do so.\n\n3. In Tab. 6, simple alternatives like CLIP-V+T, and UniIR-CLIP are very effective when compared to Omni. That is also why I am considering if unifying them into pixels is a good solution and well-motivated."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed.",
                    "Yes, Discrimination / bias / fairness concerns",
                    "Yes, Privacy, security and safety",
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)",
                    "Yes, Potentially harmful insights, methodologies and applications",
                    "Yes, Responsible research practice (e.g., human subjects, data release)",
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)",
                    "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)",
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper develops OmniContrast to unify vision-language modeling from image-text interleaved web data. To evaluate such a unified model, the authors develop the AnyCIR and SeqCIR benchmarks. These two benchmarks focus on evaluating the relevant snippet retrieval ability of the model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Clear presentation.\n\n- The evaluation of different methods on AnyCIR and SeqCIR seems sound.\n\n- The method is also straightforward, only a unified model saves the memory."
            },
            "weaknesses": {
                "value": "- The reviewer appreciates the development of benchmarks like AnyCIR and SeqCIR. One pitty is that the results of baselines are all reproduced by the authors. No third-party baselines are provided.\n\n- No results on common benchmarks are provided. In this case, the reviewer may think that OmniContrast is only developed for CIR, this specific task. It may discount the contribution of this work."
            },
            "questions": {
                "value": "- In Section 5.2, do the authors only use the vision encoder of CLIP/OpenCLIP for evaluation? Why not use the full CLIP/OpenCLIp model? \n\n- Could the authors provide results on common benchmarks like MS-COCO (text-to-image retrieval), Flickr30k (text-to-image retrieval), and GLUE benchmark? Like what CLIPPO [1] did. The reviewer thinks this can better figure out what can/cannot OmniContrast do. \n    - As said in the  Weaknesses, all results of baselines are reproduced by the authors. Comparisons on common benchmarks make the evaluation more strong.\n\n- Another question is, why we would choose OmniContrast when there are many next-token-prediction VLMs? For example, the Emu series[2]. Such VLMs may be the mainstream now. The reviewer thinks these VLMs can also do what OmniContrast can do. Relevant discussions/comparisons are required.\n\n\n[1] https://arxiv.org/pdf/2212.08045\n\n[2] https://github.com/baaivision/Emu"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents the OmniContrast model, which unifies vision and text into a pixel space for web document retrieval and understanding. Moreover, this paper presents three new information retrieval benchmarks (AnyCIR, SeqCIR, and CSR) to evaluate the ability of the model to retrieve continuous information in complex multi-modal documents."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The model performs excellently, achieving outstanding results in multiple baselines.\n- Good writing and detailed experiments.\n- A novel and useful approach for transforming interleaved data into pixel space."
            },
            "weaknesses": {
                "value": "- I'm not sure if I'm misunderstanding the model, but I think there is a lack of comparisons on some baselines, such as VQAv2 and GLEU like the comparisions in CLIPPO.\n- I think there is a lack of further discussion on the necessity and effectiveness of unifying text and images into pixel space, as well as a comparison of the differences between interleaved data and text-image pairs in this unified pixel space."
            },
            "questions": {
                "value": "I believe that the handling of interleaved data is a significant distinction between OmniContrast and CLIPPO. \n\nTherefore, I'm curious about the differences in the model's performance when using interleaved data compared to image-text pairs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "OmniContrast, a unified contrastive learning model for understanding vision, language, and vision-language interactions within multi-modal web documents. Unlike traditional models, OmniContrast:\n\n- Explores a new contrastive approach to maximize similarity between consecutive snippets from image-text interleaved web documents.\n- Unifies all modalities (text, images) into pixel space, rendering text visually, simplifying processing and representation.\n- Enables a single vision model to process any modality."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Excellent ablation study demonstrating the necessity of including each modality in the proposed pipeline (Table 1).\n2. Clearly outperforms baseline methods, allowing the model to work in different modality settings."
            },
            "weaknesses": {
                "value": "1. Despite the proposed method outperforming CLIPPO in terms of average scores, it seems that the baseline method is capable of handling all modalities in OmniContrast. Clarification on the contribution is needed.\n\n2. Data augmentation of the training data is a crucial part of the pipeline, but it is not well-documented, raising concerns about synthesizing low-quality training samples.\n\n3. Figure 2: The images and fonts are extremely small, making it difficult to understand. The caption fonts also appear too small.\n\n4. The concept of omni-modality seems odd from a reading perspective, as it appears the authors are solving vision-language problems.\n\n5. In the abstract, \"OmniContrast unifies all modalities into pixel space, where text is rendered visually\" was difficult to understand until reading the entire introduction and related work section. The term \"rendering\" suggests high-resolution 3D scenes, whereas simple text copying and pasting is not truly rendering."
            },
            "questions": {
                "value": "1. Does training a model in this omni-style make it easier or harder to converge?\n2. Related to Q1, do the authors believe that adding modalities helps the model learn each modality better, or does it make the training problem more complicated?\n3. What would happen to OmniContrast if there were abundant data in three modalities but limited data in the fourth modality?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}