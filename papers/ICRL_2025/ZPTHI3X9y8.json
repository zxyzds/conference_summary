{
    "id": "ZPTHI3X9y8",
    "title": "From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models",
    "abstract": "Hallucinations in Large Vision-Language Models (LVLMs) are a significant challenge, i.e., generating objects that are not presented in the visual input, which impairs their reliability. Recent studies often attribute hallucinations to a lack of understanding of visual input, yet ignore a more fundamental issue: the model's inability to effectively extract or decouple visual features. In this paper, we revisit the hallucinations in LVLMs from an architectural perspective, investigating whether the primary cause lies in the visual encoder (feature extraction) or the modal alignment module (feature decoupling). Motivated by our findings on the preliminary investigation, we propose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs. This plug-and-play method can be integrated into various LVLMs, utilizing adaptive virtual tokens to extract object features from bounding boxes, thereby addressing hallucinations caused by insufficient decoupling of visual features. PATCH achieves state-of-the-art performance on multiple multi-modal hallucination datasets. We hope this approach provides researchers with deeper insights into the underlying causes of hallucinations in LVLMs, fostering further advancements and innovation in this field.",
    "keywords": [
        "large vision language model",
        "hallucination",
        "virtual token"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ZPTHI3X9y8",
    "pdf_link": "https://openreview.net/pdf?id=ZPTHI3X9y8",
    "comments": [
        {
            "summary": {
                "value": "This paper addresses hallucinations in Large Vision-Language Models (LVLMs), proposing that they stem more from ineffective feature decoupling than a lack of visual input understanding. The authors introduce PATCH, a tuning method that employs adaptive virtual tokens to improve feature extraction from bounding boxes, effectively reducing hallucinations. PATCH claims that it achieves state-of-the-art results across multi-modal hallucination datasets, offering new insights into the architectural causes of hallucinations in LVLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. **Parameter Efficiency**: PATCH is designed to be parameter-efficient, reducing the need for extensive model adjustments while achieving high performance.\n2. **Performance on Hallucination Datasets**: The proposed PATCH method achieves state-of-the-art results across multiple multi-modal hallucination datasets, demonstrating its effectiveness and robustness.\n3. **Clarity and Simplicity**: The methodology is straightforward, and the paper is well-structured, making it easy to understand and follow."
            },
            "weaknesses": {
                "value": "1. **Lack of Novelty in Methodology**: The approach relies partially on using a pretrained object detection model for feature recognition and fine-tuning the vision-language model (VLM), which is a common strategy in previous works such as GLaMM, LISA, and Groundhog. This makes the method less innovative in this aspect.\n\n2. **Insufficient Ablation Study on Virtual Tokens**: The paper introduces virtual tokens as part of the PATCH tuning strategy but lacks comprehensive ablation studies to validate their effectiveness, limiting insight into how much these tokens specifically contribute to mitigating hallucinations.\n\n3. **Limited Plug-and-Play Functionality**: Although advertised as plug-and-play, the method requires fine-tuning for each personalized model, which contradicts its plug-and-play claim and may increase implementation complexity.\n\n4. **Lack of Baseline Comparisons**: The study does not adequately compare its approach against similar methods. Baseline plug-and-play strategies, such as visual prompt-based methods (e.g., Set-of-Mark and so on), as well as fine-tuning methods like GLaMM, Lisa, and Groundhog, are absent, making it difficult to assess the actual improvements PATCH offers."
            },
            "questions": {
                "value": "1. **Novelty**:  How does PATCH specifically improve over zero-shot visual prompts methods like Set-of-Mark and  fine-tuning methods like GLaMM, Lisa, and Groundhog?\n\n2. **Virtual Tokens**: Is there an ablation study showing the effectiveness of virtual tokens?\n\n3. **Plug-and-Play Claim**: Since fine-tuning is needed, how is PATCH truly plug-and-play?\n\n4. **Baseline Comparisons**: Would including comparisons with similar methods, like visual prompts mehtods and other fine-tuning models, provide further context for PATCH\u2019s effectiveness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a parameter-efficient tuning technique that introduces detection for hallucination mitigation in multimodal large language models. The method utilizes adaptive virtual tokens to extract object features from bounding boxes, thereby addressing hallucinations caused by insufficient decoupling of visual features."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Computational Efficiency: The proposed PATCH strategy achieves parameter-efficient tuning by freezing LVLM parameters and optimizing a small number of virtual tokens, making it lightweight and practical for deployment scenarios."
            },
            "weaknesses": {
                "value": "1.\tLack of Clarity in Preliminary Experiments: Section 2 is confusing, particularly regarding Table 1. Definitions of \u201ccorrect detection\u201d and \u201cwrong detection,\u201d along with their calculation methods, are not adequately explained.\n2.\tInsufficient Support in Analysis: The performance gap between detection and inference is not convincingly attributed to the struggles of the visual projection module. The reasoning lacks robust experimental evidence.\n3.\tContradiction in Motivation: While the motivation suggests that incorporating detection information may introduce redundancy, the method still relies on detection information plus soft visual tokens, creating a contradiction.\n4.\tLimited Novelty: The method primarily involves parameter-efficient tuning (soft prompt tuning), which does not introduce significant novelty beyond existing tuning techniques.\n5.\tUnfair Experimental Setup: The use of a portion of the test data for training gives an in-domain testing advantage, compromising the fairness of comparisons with baselines.\n6.\tIncomplete Baseline Comparisons: Not all baselines are consistently compared across different LVLMs, particularly in the main results table, diminishing the thoroughness of the evaluation.\n7.\tOmission of Experimental Details: Important experimental details, such as decoding strategies, are missing, limiting the reproducibility of the experiments.\n8.\tAbsence of Ablation Studies: An ablation study isolating the added visual tokens is needed to properly assess their contribution to the model's performance. (e.g., tuning only the projector on the same data)\n9.\tOverclaimed Results: Some claims, such as those in lines 316\u2013317 about enhanced semantic alignment via visual tokens, are overstated without sufficient empirical support.\n10.\tIncomplete Related Work: The related works section neglects recent advances in LVLMs, such as Qwen2VL and LLaVA-OneVision, as well as relevant hallucination mitigation methods like GAVIE, limiting the contextual relevance of the paper."
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates the root causes of hallucinations in LVMs, identifying the main issue as the insufficient separation of textual and visual features during multi-modal integration. To address this, the authors introduce a tuning-based method, designed to help LVMs leverage object detection to reduce hallucinations. The effectiveness of this method is demonstrated through validation on two multi-modal hallucination evaluation datasets across three different LVMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is very clear and easy to follow.\n\n2. The experiments support the main claims in the text, and the presented approach is simple and useful.\n\n3. Most of the design choices are ablated to verify their effectiveness."
            },
            "weaknesses": {
                "value": "1. The paper claims that the reason for hallucinations is the feature decoupling. Nevertheless, many of the hallucinations in Table 1 are also not detected by the detection model (308 vs. 191). This suggests that the encoder is an additional significant source of hallucinations. I would like to see the same table, after the replacement of $prompt_1$ with $prompt_2$.\n\n2. The paper claims that the additional computational cost of training the extra tokens is negligible. Nevertheless, the paper omits the fact that the model relies on an additional object detector that should also be trained. It is not clear to me if the additional computational cost and model parameters that were spent on the object detector, can be spent instead on a larger encoder/adapter/llm, and result in a much cleaner solution for hallucination reduction. \n\n3. Overall, the paper presents a simple soft-prompting approach for targeting hallucination reduction. Are the detections necessary, or can they be achieved solely from additional soft-prompting with more learned tokens?"
            },
            "questions": {
                "value": "1. For $prompt_2$ and along the rest of the paper, the additional information that is extracted from the object detector and given to the language model includes not only the existence of objects but also their location (x1,x2,y1,y2). Although according to Table 4, it is necessary, it is not clear why (the VQA is about object existence and not localization). Do you have any intuition for this property? Can you compare the accuracy of the bboxes to their hallucination removal rate?  \n\n2. Is the VLM even needed, given the soft prompt and the detection results? What is the baseline of soft-prompting a text-only LLM on the object detection results, without the use of images?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper revisits the reason for LVLM object hallucination by conducting a preliminary study and finding that visual feature decoupling instead of extraction is the main reason. Based on a potential solution, the authors propose PATCH, a novel plug-and-play prompt-tuning-based LVLM hallucination mitigation method. Experiments on the POPE and PhD datasets verify the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Starting from a preliminary study and then introducing the proposed method, the writing is good.\n- The proposed method is plug-and-play without requiring a large amount of computing.\n- The authors demonstrate the effectiveness of the proposed methods on three baseline models."
            },
            "weaknesses": {
                "value": "- About the Cascade Mask-RCNN head:\n  - Do you use a separate Cascade Mask R-CNN detector or just the detector head which is then connected with the vision encoder of the LVLM?\n  - If the former, you cannot get the conclusion in Sec. 2 since the capabilities of the detector and the vision encoder of LVLM might differ from each other.\n  - If the latter, how do you do that? Is the detection head exactly pre-trained with the same vision encoder of the LVLM, or do you need further training?\n\n- About PATCH:\n  - On the one hand, detection information might be redundant.\n    - In lines 226-227, the authors suggest that,\n      > In scenarios where no extra detection information is required, the LVLM can revert to processing the input using its standard capabilities without PATCH involvement.\n    - I wonder how to do that, since the detection module of PATCH is query-irrelevant, suggesting that the detector will detect all objects in the image regardless of the question.\n  - On the other hand, detection information is limited by the granularity of the detector.\n\n    - PATCH aims at directly providing the object information detected by the detector to LLMs, which, however, cannot provide information that cannot be detected by detectors. For example, attributes (colors and shapes) cannot be detected, which are more common phenomenon of LVLM hallucinations nowadays.\n    - Moreover, objects beyond the recognizable classes of the detector are also not recognizable.\n  - Therefore, beyond experiment numbers, an analysis of why PATCH eases LVLM hallucination is also feasible. For example, will PATCH help if the queried object is *without* or *beyond* the class set of the pre-trained detector?\n\n- About experiments:\n  - Baselines: HA-DPO and HACL are both methods requiring training. Do you train them with the same training set with PATCH?\n  - We care about hallucination, but we do not want to hurt the utility of LVLMs. Therefore, besides experiments on hallucination benchmarks like POPE and PhD, results on utility benchmarks (e.g., MME, MMBench, and SEED) are also important to demonstrate that hallucination mitigation is not at the cost of utility. \n\n- Overall, I think this is an interesting paper, which, however, needs further work on 1) method clarification, 2) effectiveness analysis (i.e., what it can and cannot do), and 3) utility experiments."
            },
            "questions": {
                "value": "- About title: \n  - The overall idea of this paper is to help LVLMs better utilize the object recognition information extracted by the vision encoder. \n  - So what does the \"From Pixels to Tokens\" suggest in the title?\n- About the potential solution:\n  - How do you get the object-related information, by using the ground truths or prediction results of Cascade Mask R-CNN?\n  - How do you deal with objects which are not existing in the image?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}