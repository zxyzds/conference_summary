{
    "id": "iYkhxre0In",
    "title": "PaCA: Partial Connection Adaptation for Efficient Fine-Tuning",
    "abstract": "Prior parameter-efficient fine-tuning (PEFT) algorithms reduce memory usage and computational costs of fine-tuning large neural network models by training only a few additional adapter parameters, rather than the entire model. However, the reduction in computational costs due to PEFT does not necessarily translate to a reduction in training time; although the computational costs of the adapter layers are much smaller than the pretrained layers, it is well known that those two types of layers are processed sequentially on GPUs, resulting in significant latency overhead. LoRA and its variants avoid this latency overhead by merging the low-rank adapter matrices with the pretrained weights during inference. However, those layers cannot be merged during training since the pretrained weights must remain frozen while the low-rank adapter matrices are updated continuously over the course of training. Furthermore, LoRA and its variants do not reduce activation memory, as the first low-rank adapter matrix still requires the input activations to the pretrained weights to compute weight gradients. To mitigate this issue, we propose **Pa**rtial **C**onnection **A**daptation (**PaCA**), which fine-tunes randomly selected partial connections within the pretrained weights instead of introducing adapter layers in the model. PaCA not only enhances training speed by eliminating the time overhead due to the sequential processing of the adapter and pretrained layers but also reduces activation memory since only partial activations, rather than full activations, need to be stored for gradient computation. Compared to LoRA, PaCA reduces training time by 22\\% and total memory usage by 16\\%, while maintaining comparable accuracy across various fine-tuning scenarios, such as fine-tuning on the MMLU dataset and instruction tuning on the Oasst1 dataset. PaCA can also be combined with quantization, enabling the fine-tuning of large models such as LLaMA3.1-70B. In addition, PaCA enables training on 23\\% longer sequence data and improves throughput by 16\\% on both NVIDIA A100 and INTEL Gaudi 2 GPUs compared to LoRA. The code is available at [https://anonymous.4open.science/r/paca-366F](https://anonymous.4open.science/r/paca-366F).",
    "keywords": [
        "Parameter-Efficient Fine-Tuning (PEFT)",
        "Large Language Models (LLM)",
        "Memory efficient training",
        "Accelerating training"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "PaCA fine-tunes randomly selected partial connections within the pretrained weights, significantly reducing training time and memory usage compared to prior PEFT schemes.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=iYkhxre0In",
    "pdf_link": "https://openreview.net/pdf?id=iYkhxre0In",
    "comments": [
        {
            "summary": {
                "value": "The paper presents PaCA (Partial Connection Adaptation), a new method for parameter-efficient fine-tuning (PEFT) aimed at large models. Instead of adding adapter layers as in LoRA, PaCA fine-tunes only a selected subset of connections within the pretrained weights, which are chosen at random. This approach aims to reduce memory usage and training time without sacrificing accuracy. The authors provide theoretical backing and empirical results that suggest PaCA offers efficiency gains over existing methods, maintaining competitive performance on selected datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Below is the list of the strong points identified in this work:\n\n- One key strength of PaCA is that its selective adaptation of pretrained weights eliminates the need for additional adapter layers, which in turn reduces both latency and memory requirements. This is a promising approach that may lead to faster and more efficient fine-tuning, especially valuable for large-scale language models.\n- Additionally, PaCA is compatible with quantisation techniques, expanding its potential to handle even larger models.\n- The authors provided a convergence analysis that theoretically supports PaCA\u2019s efficiency and stability, giving a mathematical foundation to their claims.\n- Empirical evaluations are promising."
            },
            "weaknesses": {
                "value": "Below is the list of weaknesses that I would like to see refuted or clarified by the authors:\n\n- The primary limitation of the study is that it evaluates PaCA on a narrow set of models and datasets, leaving its generalisability to more complex or diverse tasks uncertain. Testing PaCA across a broader range of tasks and model architectures would strengthen the evidence for its effectiveness.\n- Additionally, the method\u2019s random selection of connections for fine-tuning is not fully explored or justified, and it remains unclear how this choice impacts performance or whether an alternative selection strategy could yield better results. Why not evaluate some optimisation or search strategies instead?\n- Another area for improvement is in the analysis of quantisation impacts. The current study lacks an in-depth look at how quantisation affects precision and performance at larger scales, which reduces clarity on how well PaCA might scale."
            },
            "questions": {
                "value": "Authors are requested to clarify or make changes, as appropriate, based on what is discussed in the \u2018Weaknesses\u2019 section.\n\nSome minor comment:\n- Providing more intuitive descriptions alongside the theoretical sections would improve readability for a broader audience.\n- The expedient of unifying the equal columns of some tables in Appendix B rather than lightening the visual load tends to make it unattractive. It would be much better to fill each column even with the same value."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses parameter-efficient fine-tuning (PEFT) for large language models (LLMs), highlighting a key limitation in existing PEFT methods: reduced compute doesn't necessarily translate to faster training due to the sequential processing of adapter layers alongside pretrained weights. This sequential approach underutilizes hardware resources and incurs latency, as GPUs typically handle one kernel at a time. Tools like CUDA streams can help parallelize processing but come with management and synchronization overheads.\n\nLoRA and its variants merge low-rank adapter matrices with pretrained weights to avoid this latency during inference, but this solution isn't applicable during fine-tuning, as adapter matrices must be trained separately. Additionally, LoRA doesn't reduce activation memory usage compared to Full Fine-Tuning (Full-FT), since input activations still need to be stored for gradient calculations. Thus, despite computational optimizations, sequential processing and memory demands remain challenges in PEFT methods.\n\nGiven the motivation, the authors propose PaCA (Partial Connection Adaptation), a memory-efficient parameter-efficient fine-tuning (PEFT) method that fine-tunes randomly selected partial connections in pretrained weights, without using adapter layers. Unlike prior PEFT approaches, PaCA reduces training time by integrating the forward and backward operations for both pretrained and partial connections, thus avoiding sequential processing. By only requiring the corresponding activations for gradient calculations, PaCA significantly lowers activation memory usage.\n\nThe authors provide a theoretical proof of convergence for PaCA and demonstrate through experiments that it achieves substantial reductions in both training time and memory consumption compared to prior PEFT methods, while maintaining comparable accuracy across various fine-tuning scenarios and GPU types.\n\nPaCA not only reduces training time by 19% compared to LoRA but also lowers memory usage by storing fewer activations during training. The authors back up these claims with both a theoretical convergence proof and a thorough set of experiments that demonstrate PaCA\u2019s ability to achieve comparable accuracy while being faster and more memory-efficient. PaCA increased the maximum sequence length by 23%, 108%, and 23% compared to LoRA, DoRA, and MosLoRA by storing only partial activations instead of all input activations. They show the highest throughput among the methods discussed in Fig 3 on two different GPUs. They also show what quantization combined with their method PaCA looks like and compare their method to QLoRA. Overall, the authors found near-maximum performance with the lowest memory and finetuning time overhead.\n\nThe paper presents a compelling solution to making large model fine-tuning more practical and scalable. It is a well-executed work with promising implications for efficient model fine-tuning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "i) The concept of selecting and fine-tuning only partial connections in the pretrained weights without adapter layers is novel and effectively addresses some of the inefficiencies of existing PEFT methods, like the latency introduced by sequential processing.\n\nii) By eliminating the need for adapter layers and reducing the activations that need to be stored, PaCA successfully reduces both training time and memory footprint. The motivations and the contributions are clearly demonstrated in text and in Fig 2.The empirical results show significant improvements over LoRA and its variants. \n\niii ) The authors provide proof of convergence for PaCA, ensuring that the proposed method can effectively minimize the loss in general neural networks.\n\niv)  PaCA's reduction in memory and computational overhead is good for resource-constrained environments, such as edge devices. The authors also present results with the best throughput in two GPUs over rest of the referred works.\n\nv) The paper includes a comprehensive set of experiments across different scenarios, including fine-tuning for specific tasks, instruction tuning, and using quantized versions (QPaCA). Comparisons with SOTA methods such as LoRA, DoRA, and MosLoRA provide a well-rounded perspective on PaCA's performance gains. The sequence length supported by PaCA also exceeds that of other referred methods. \n\nOverall, it is a good paper with good analysis."
            },
            "weaknesses": {
                "value": "i) The random selection of partial connections is a key component of PaCA. Yet, there is limited discussion on how this selection impacts training quality and whether alternative strategies could improve performance. A deeper exploration of the effect of different selection criteria on convergence and accuracy would significantly strengthen the paper.\n\nii) On a similar note, an empirical or theoretical analysis of the importance of selecting specific columns could have been highly informative.\n\niii) The convergence analysis relies on the gradient's Lipschitz continuity, but this is a standard assumption that may not hold for many real-world large-scale neural networks. A more detailed discussion of how these theoretical guarantees translate into practice and the possible limitations would have been helpful.\n\niv) Equations 2, 5 and 8 are incorrectly written."
            },
            "questions": {
                "value": "1)  Are there results on how the improvements and the convergence time vary if a different set of random connections are selected during finetuning?\n\n2) Have the authors considered different strategies for selecting partial connections, such as importance-based or gradient-based selection, to determine if these approaches lead to improved convergence or accuracy compared to random selection?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose PaCA, a novel PEFT algorithm selecting only partial connections within pretrained weights for finetuning, that reduces memory usage and compuational cost compared to LoRA and its variants. \nThe paper contains both an empirical evaluation of the effectiveness of PaCA in finetuning LLMs and a theoretical prove of the loss convergence in general neural networks when using PaCA."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The empirical evaluation shows a significant reduction in resource consumption when using PaCA, while being able to achieve similar results to LoRA and several of its variants (DoRA, MosLoRa, ...).\n- The authors include an extension of their method that works with quantized pre-trained weights, which further increases the applicability of their proposed technique.\n- The proof of convergence is very useful, not only for PaCA, but also for algorithms using similar partial updating strategies."
            },
            "weaknesses": {
                "value": "- Especially for on-device training of fully quantized CNNs on embedded systems/microcontrollers, updating only partial connections is already a well-known technique [1], [2].\n- Both [1] and [2] use heuristics ([1] has an offline heuristic based on XAI, [2] an online heuristic based on the magnitude of structures in feature maps) to decide which subset of weights to update compared to the potentially inferior random selection approach proposed in this work.\n\n[1] Lin, Ji, et al. \"On-device training under 256kb memory.\" Advances in Neural Information Processing Systems (2022).\n\n[2] Deutel, Mark, et al. \"On-Device Training of Fully Quantized Deep Neural Networks on Cortex-M Microcontrollers.\" IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (2024)."
            },
            "questions": {
                "value": "- For what kind of operators and NN \"types\" does the proposed technique apply? The paper initially focuses on MLPs (\"linear layers\"), but this seems to be only exemplary, as the evaluation is then performed for LLMs. Does the proposed technique also apply to e.g. CNNs (\"convolutional layers\")? \n- Figure 1. right graph: are these real measured numbers or is the graph just symbolic? If they are real numbers, their order of magnitude should be shown on the y-axis, as in Figure 2, otherwise I do not understand what new information not already discussed in the text is gained by the reader by showing it.\n- Eq. 1-9 seem imprecise to me. For example, in Eq. 2 should it not be something like $\\nabla X_{i-1} = W^T * \\nabla X_i$ since the error signal of the previous layer $i-1$ is calculated based on the error signal of the current layer $i$ instead of the \"in-place\" update of $\\nabla X_{in}$ shown in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors proposed a novel method for PEFT, named Partial Connection Adaptation(PaCA). PaCA fine-tunes randomly selected partial connections in pre-trained weights instead of using adapter layers like LoRA. This innovation leads to a faster training speed and reduced memory usage while maintaining almost similar accuracy. The authors presented results on fine-tuning large models such as LLaMA3, demonstrating PaCA's ability to reduce training time by 22% and memory usage by 16%."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. PaCA could significantly reduce memory usage and training time compared to other PEFT methods like LoRA by avoiding the need for additional adapter layers.\n\n2. PaCA performs well with large models and long sequence data, increasing the maximum sequence length and improving throughput.\n\n3. The authors provide theoretical analysis to demonstrate that PaCA effectively converges for general neural networks."
            },
            "weaknesses": {
                "value": "1. The authors choose to randomly select partial connections in PaCA instead of using some strategic selection. How could the random selection be generalized when fine-tuned with PaCA? Could a more targeted selection improve the performance?\n\n2. While this paper only presents results on LLaMA models, it would be beneficial to see how PaCA performs on a wider range of architectures, such as non-transformer-based models or other tasks beyond language models.\n\n3. Does PaCA introduce any stability issues in training, particularly when fine-tuning very large models with longer sequence lengths?"
            },
            "questions": {
                "value": "see weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}