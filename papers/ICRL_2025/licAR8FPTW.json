{
    "id": "licAR8FPTW",
    "title": "Evaluating Oversight Robustness with Incentivized Reward Hacking",
    "abstract": "Scalable oversight aims to train systems to perform tasks that are hard for humans to specify, demonstrate and validate. \nAs ground truth is not available for such tasks, evaluating scalable oversight techniques is challenging: existing methods measure the success of an oversight method based on whether it allows an artificially weak overseer to successfully supervise an AI to perform a task.\nIn this work, we additionally measure the robustness of scalable oversight techniques by testing their vulnerability to reward hacking by an adversarial supervisee.\nIn experiments on a synthetic domain, we show that adding an explicit reward hacking incentive to the model being trained leads it to exploit flaws in a weak overseer, and that scalable oversight methods designed to mitigate these flaws can make the optimization more robust to reward hacking.\nWe hope these experiments lay a foundation for future work to validate scalable oversight methods' ability to mitigate reward hacking in realistic settings.",
    "keywords": [
        "scalable oversight",
        "robustness",
        "reward hacking"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=licAR8FPTW",
    "pdf_link": "https://openreview.net/pdf?id=licAR8FPTW",
    "comments": [
        {
            "summary": {
                "value": "The paper claims to introduce a new approach to evaluate the robustness of scalable oversight techniques. The proposed methodology is to add an incentive for reward hacking. They use a synthetic task where the models are given a good and bad list, and one model must communicate a clue to the overseer which the overseer uses to guess words in the good list. They consider three oversight techniques: Base, Consultancy and Critiques. They are evaluated in a sandwiching setting using flawed overseers: lazy, negligent and biased. They test two ways to perform the clue generation, using classical search and LLM training with RL. True and overseer scores are reported for selected training runs, and observations are made about the resulting plots indicating adversarial incentives lead to more reward hacking."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper operationalises and studies scalable oversight, which is an interesting research challenge.\n2. Caveats and limitations are listed along with empirical observations."
            },
            "weaknesses": {
                "value": "1. **The paper is extremely poorly written and hard to understand.** \n\nMost statements are verbose yet imprecise. It almost seems like a ramble of various disconnected points with no logical flow in many places. I will provide concrete examples. First the introduction. The points in the \u2018classification\u2019 are unclear, and it's also unclear what implication they have for the paper. Is this a novel classification contributed by the paper? More examples: Intro L50: \u2018a method such as Critiques\u2019 \u2014 it is unclear what is being referred to, which makes this whole point hard to understand. What do you mean by overseer mistakes can be harder to learn?  \n\nSecond, related work barely talks about related papers and the marginal contribution in context of them, but rather is an extended introduction or background section. I would recommend shifting the final 2 paragraphs to the introduction section as \u2018contributions\u2019, for example. However, the second bullet point is hard to understand too.\n\nThe oversight protocols are hard to understand as described, and don\u2019t provide any reference. Did this paper come up with these, or are they used in prior literature? Similarly with the overseers. How did you make the design choices for each of them?\n\n2. **Most claims seem like post-hoc analysis** of experiments running the three different oversight protocols across the different overseers. No clear hypotheses are stated beforehand, and thus it is difficult to judge whether the experiments designed make sense to study them, and whether the conclusions drawn are free from confounders.\n\n3. It is not clear what the contributions of this paper are as it does not properly distinguish prior work. Is the CodeNames task first studied in this paper? What about the overseer policies and the oversight methods? Which of the conclusions drawn from the experiments are novel?"
            },
            "questions": {
                "value": "Please answer the questions raised in the Weaknesses section and also re-organize the paper so it's easier to judge the contributions, hypotheses, motivating the experimental design based on the hypotheses, and finally the strength of evidence in support of the hypotheses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper focuses on the robustness and efficiency of scalable oversight techniques\u2014specifically robustness to reward hacking, robustness to overseer flaws, and data efficiency. The authors focus on a simplified version of the game of CodeNames, and study three oversight protocols, introducing flaws into the overseers for each. They also introduce an adversarial incentive term to increase the likelihood of reward hacking."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The choice of CodeNames task is an interesting one which, while very simple, nevertheless raises the possibility of complex strategies.\n\nThe paper has a good discussion of scalable oversight and sandwiching, and suggests a wide range of different protocols and sets of experiments to be explored."
            },
            "weaknesses": {
                "value": "The paper makes many choices that each seem somewhat unprincipled or poorly-justified, and add up to make it difficult to draw clear conclusions from the paper overall. These include:\n- The three aspects of scalable oversight techniques tested\n- The three types of oversight protocols\n- The four types of overseers used\n- The two types of experiments\n- The design of the adversarial incentive function\n\nGiven how many such choices were made, it was difficult to interpret the findings of the experiments, or determine which were actually robust\u2014especially since only the CodeNames environment was used. The conclusions drawn were often fairly speculative. In general, the paper seemed exploratory in nature, rather than trying to test any pre-formulated hypothesis."
            },
            "questions": {
                "value": "1. What is the intended relationship between the flawed overseers and the additional adversarial incentive? Are they simply two different mechanisms for encouraging reward hacking in this toy setting? Could they in principle be combined?\n\n2. Which of your conclusions seem most likely to be replicable and generalizable across different tasks and different experimental setups?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors consider a scalable oversight setting, where one model (overseer) is trying to tell whether another model (overseen) is performing well or not. They show that by including an explicit incentive in the overseen model to trick the overseer, they are able to generate behaviors that make the overseer worse at giving reliable feedback."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea of including explicit incentives to try to trick an overseer is a relevant and interesting one."
            },
            "weaknesses": {
                "value": "* the paper reads like a LessWrong post that has been reformatted into ICLR style\n* the paper engages with very little existing deep learning literature, and does so in a weird/atypical way\n* many parts of the paper are written in a vague way\n* there are too many itemize and enumerate environments\n* tables are somewhat unclear\n* experiments are on one seed"
            },
            "questions": {
                "value": "## Questions\n* I'm confused why you call it \"reward hacking\" as opposed to something like \"deceiving\" or \"evading oversight\". Reward hacking is generally considered very narrowly to have to do with optimizing against a reward model and getting bad outcomes.\n\n## Suggestions\n* make sure your plots are readable (they are too small right now; ideally you would also use a serif font to match the ICLR style but that's less of a big deal), and have the caption of plots and tables explain what is going on in them. it should be possible to look at a Figure/Table and understand what is going on from the caption alone.\n* try submitting this to a workshop instead of a main conference; it'll be much easier to get in\n* before doing so, have one or two people with experience writing deep learning papers go over your paper and give suggestions. There are many small things that stand out that make the paper hard to read for someone used to reading deep learning papers.\n* before doing that, try to make the formatting of the paper match that of other deep learning papers more: need to engage more with the literature (not LW/AF, like deep learning literature), not use bullet lists more than a few times, make really clear what your setting is, explain the *implication* of things and why it's relevant instead of just stating the result, make sure all your claims are justified, be as precise and specific as possible in your language.\n* thank you for working on AI safety"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper discusses evaluating the supervision process where a weaker supervisor oversees a more capable AI model and gets evaluated by an expert at the end. The goal is to make sure the weaker supervisor is capable of identifying potential risks within the more capable AI model without being \"reward hacked\". The paper showcases an evaluation protocol where multiple models of the potential weak supervisors are provided and evaluated."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Writing is easy to follow. Both methods and experiments are clearly stated."
            },
            "weaknesses": {
                "value": "1. Insufficient contribution: since safety issues in LLM have been discussed extensively already, more discussion on the relationship between the problem of \"reward hacking in scalable oversight\" and LLM safety should be provided. This would also form the basis of potential solutions to \"reward hacking in scalable oversight\" discussed in this paper. Currently, the paper only provides evaluations on a simple codename game but no systemetic solution is proposed. \n2. Insufficient evaluation: I fully understand that LLM works are computationally expensive but given the nature of the models, it's hard to convince the users without extensive experiments."
            },
            "questions": {
                "value": "I'm a bit confused by the evaluation protocol. If we have access to the overseer rewards, can we evaluate the reward hacking by comparing overseer rewards directly against the reference rewards? In other words, we can get to the same conclusion without training LLMs, solely comparing the reward models would suffice."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes empirical evaluation method to test the robustness of scalable oversight through reward hacking. The paper conducts comprehensive experiments on CodeNames. It includes experiments on various settings of the task, the overseer and the inceptive. The paper implements an adversary to test the robustness of the oversight technique. The result successfully demonstrates the effectiveness of the adversary in exploitation, revealing the possible non-robustness of oversight technique."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The formulation of the problem is well-defined and interesting. In particular, considering mapping the real-world oversight to program  implementation is a great and meaningful idea.\n2. The experiments conducted are comprehensive and sounded, which effectively shows the oversight technique may not be robust towards adversary.\n3. The paper includes several discussions on implication of results and its meaning towards real-life scenario."
            },
            "weaknesses": {
                "value": "1. The reason why under some protocol (e.g. Consultancy) some weak overseers encounters exploitation while others don't is not clearly discussed. \n2. (As the author has partially acknowledged), this CodeNames environment might be too simple to evaluate 'scalable' oversights."
            },
            "questions": {
                "value": "1. See weaknesses.\n2. Does the 'Biased' in section 3.2 turns to 'Underweigh' in following sections?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores the challenge of scalable oversight from an adversarial perspective. It introduces a new testbed, CodeNames, designed to evaluate the robustness of scalable oversight methods. CodeNames is a two-player, single-turn cooperative game in which the cluer receives separate lists of \u201cgood\u201d and \u201cbad\u201d words and gives a single-word clue. The guesser then receives a combined list of all the words and must identify the \u201cgood\u201d words based on the clue. Using this testbed, the paper examines the robustness of oversight methods against reward hacking under different oversight protocols and overseer imperfections."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper develop a practical and accessible testbed specifically designed for the evaluation of scalable oversight methods. It provides a controlled environment where various oversight strategies can be systematically tested and compared.\n- The paper provides real-world examples of each flaw type of overseers, which strengthens the credibility of the proposed testbed."
            },
            "weaknesses": {
                "value": "- This research topic seems still in its early stages, but the authors introduce it without sufficient detail. I suggest moving the first paragraph of Section 2 to Section 1 for better clarity and context.\n- The adversarial incentive described in Section 3.1 is unrealistic. While its purpose is to maximize the discrepancy between the overseers\u2019 reward and the ground-truth reward, such a scenario does not occur in real-world settings.\n- It appears that CodeNames is an existing game (https://codenamesgame.com/), and some researchers have already used it to evaluate scalable oversight methods (https://samuelknoche.itch.io/player-of-games). The authors should properly cite these related works to acknowledge their contributions and provide appropriate context.\n- The main point of Figures 6 and 7 is unclear. It would be helpful if the authors provided a more detailed explanation of these figures.\n- The handwritten (or handwritten-like) figures are difficult to read."
            },
            "questions": {
                "value": "- The input for the critique model is not specified. What is the input for the critique model, and how is the model optimized?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}