{
    "id": "UwcZEoNP19",
    "title": "Triples as the Key: Structuring Makes Decomposition and Verification Easier in LLM-based TableQA",
    "abstract": "As the mainstream approach, LLMs have been widely applied and researched in TableQA tasks. Currently, the core of LLM-based TableQA methods typically include three phases: question decomposition, sub-question TableQA reasoning, and answer verification. However, several challenges remain in this process: i) Sub-questions generated by these methods often exhibit significant gaps with the original question due to critical information overlooked during the LLM's direct decomposition; ii) Verification of answers is typically challenging because LLMs tend to generate optimal responses during self-correct. To address these challenges, we propose a Triple-Inspired Decomposition and vErification (TIDE) strategy, which leverages the structural properties of triples to assist in decomposition and verification in TableQA. The inherent structure of triples (head entity, relation, tail entity) requires the LLM to extract as many entities and relations from the question as possible. Unlike direct decomposition methods that may overlook key information, our transformed sub-questions using triples encompass more critical details. Additionally, this explicit structure facilitates verification. By comparing the triples derived from the answers with those from the question decomposition, we can achieve easier and more straightforward validation than when relying on the LLM's self-correct tendencies. By employing triples alongside established LLM modes, Direct Prompting and Agent modes, TIDE achieves state-of-the-art performance across multiple TableQA datasets, demonstrating the effectiveness of our method.",
    "keywords": [
        "TableQA",
        "Triples"
    ],
    "primary_area": "generative models",
    "TLDR": "We propose Triples-Inspired Decomposition and Verification strtegy to improve LLM in TableQA.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=UwcZEoNP19",
    "pdf_link": "https://openreview.net/pdf?id=UwcZEoNP19",
    "comments": [
        {
            "summary": {
                "value": "The authors use Triples in different phases of TableQA. The generation of triples during Question decomposition phase helps in extracting key subquestions and conversion of answers during answer verification phase helps remove LLM bias in verification and enables better answer verification."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Incorporating triples in multiple stages of TableQA is a novel idea and as shown by the evidence in the paper, helps improve the overall QA performance. \n2. The authors demonstrate the effectiveness of TIDE by performing ablation study which is critical in identifying the impact of this method.\n3. This method works alongside DP and adds an additional step of generating triples, so this keeps all the benefits of DP as well."
            },
            "weaknesses": {
                "value": "1. Inorder to demonstrate the impact of TIDE, it could have been useful to see TIDE with other standard LLMs as well. The ablation study shows the impact of TIDE but is this method tailored to work with GPT 3.5 in any specific way?"
            },
            "questions": {
                "value": "1. Given that this is an additional step along the pipeline of TableQA steps, what does this step do to the average response time and number of tokens used? \n2. Does this method of generating triples work for mathematical questions such as average age/salary etc? Does this mean that the subquestions would require triples for every entry in the table?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed a decomposition and verification strategy based on triple to address TableQA tasks. Specifically, it decomposes and transforms questions using triples to capture more key information, while utilizes explicit structure for better verification. By combining LLM-based reasoning and symbolic reasoning based on structured representations, TIDE improves the performance in the TableQA task"
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The motivation of the work is clear. It leverages triple extraction as a tool to enhance understanding and validation of TableQA questions.\n  \n2. The experiment is sufficient, which compares TIDE with many TableQA baselines. Further study can reveal the properties of the proposed method."
            },
            "weaknesses": {
                "value": "1. The contribution is limited; it only proposes a framework based on LLMs with triple extraction. It would be better if it could be demonstrated on some open-source models like Llama3.\n  \n2. The conclusion drawn from the experimental results that TIDE improves the effectiveness of TableQA problems is questionable. The results of DP and Agent baseline for both datasets are not SOTA. So, can it be considered that the gain in this result comes from additional reasoning integration of DP and Agen rather than the introduction of triplet?"
            },
            "questions": {
                "value": "The appendix lacks cases, including successful cases compared to other methods, error cases analysis, etc. I think showing some cases can better explain the working mechanism of the entire pipeline."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposed an inference time approach, TIDE, to address table question answering. Their method builds on a Decomposition-Reasoning-Verification approach for Table QA using LLM, and they proposed to use semantic triples as intermediate representations instead of natural text. First, during question decomposition, they extract triples from the question then generate subquestions based on those triples. Then, they apply CoT (for DP mode)/generating python code (for agent mode) to answer each subquestions. Finally, in the answer verification stage, they compare the triples from stage 1 with triples from stage 2 to filter correctly answered triples. \n\nTheir contribution is using triples as intermediate representations in order to facilitates decomposition and verification, and showed that it leads to improvements over SOTA."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Incorporating triples as intermediate steps is an interesting approach, and it intuitively makes sense as a natural intermediate representation to tackle the verification of subquestions. The authors effectively combine existing ideas from different problems and apply it in the settings of table QA, showing SOTA results on challenging table qa datasets."
            },
            "weaknesses": {
                "value": "My main concern is clarity and contribution.\nFor clarity\n1.  Details of how each module is implemented is confusing. The formulation really only has one equation (eq 1), and it is about the Table QA problem. Formal explanation (instead of examples only) for each submodule would help improve clarity. Furthermore, I think the prompt for extracting triples are not provided?\n\n2. It\u2019s unclear to me how the final answer is synthesized after the verification stage. Again, providing formulation for this stage would be more helpful. \n\n3. In Table 3 of your ablation studies, you have TIDE vs TIDE-decomposition vs TIDE-verification. It is confusing to me how the ablated versions are implemented. Again, I think having formulation would help improve clarity.\n\nIn terms of contribution, since this is combining different existing ideas from other tasks (see below for examples), I would appreciate more in-depth analysis of the proposed method.\n\n1.  Would other language models be equally effective with generating triples? When does this assumption breakdown? How does model size affect the result and error propagation through each stage ... etc. Have you tested this with open source large/small sized language models or other information extraction approaches for triple extraction? Is it possible to substitute some LLM API calls with other non-LLM methods without hindering performance?\n\n2. The strong performance could potentially come from data contamination. How do you attribute the success of TIDE vs GPT having seen those tables in pretraining? \n\n3. In verification stage, you mentioned the LLM is prompted to regenerate if verification failed. I would appreciate more analysis on this part such as showing how often verification fails or how many times max you limit the LLM to regenerate.\n\n\nPS by combining existing ideas I mean\n1. Question decomposition is not new. See [1] [2] for example\n2. using LLM for information extraction is not new. See [3] for example\n\nI think there are some merits in combining these insights and seeing that triples could apply to this problem, so I think more analysis could help improve this paper.\n\n[1] Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. Multi-hop Reading Comprehension through Question Decomposition and Rescoring. ACL 2019.\n\n[2] Xiaoyu Yang and Xiaodan Zhu.  Exploring Decomposition for Table-based Fact Verification. EMNLP Findings 2021\n\n[3] Zhiyuan Fan and Shizhu He. 2023. Efficient Data Learning for Open Information Extraction with Pre-trained Language Models.  EMNLP Findings 2023"
            },
            "questions": {
                "value": "This goes back to the clarity of the verification stage.The example authors provided seems to be a \u2018COUNT\u2019 type questions. How does the system handle other types of numeric aggregation? For questions that are not numeric in nature (e.g. compose-style, multi-hop questions), how does the final stage work after verification? Does this approach only work on aggregation type questions? What algorithms are you using to unify the triples for each subquestion?\n\nMy initial recommendation is to reject this paper. First, there are some clarity issues so it's difficult for me to determine to what extent the contributions made by this paper are significant. Second, there are some missing analyses and experiments that, if presented, could bring more value to the community."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}