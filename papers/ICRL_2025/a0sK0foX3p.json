{
    "id": "a0sK0foX3p",
    "title": "FusionBench: A Comprehensive Benchmark of Deep Model Fusion",
    "abstract": "Deep model fusion is an emerging technique that unifies the predictions or parameters of several deep neural networks into a single model in a cost-effective and data-efficient manner. This enables the unified model to take advantage of the original models' strengths, potentially exceeding their performance. Although a variety of deep model fusion techniques have been introduced, their evaluations tend to be inconsistent and often inadequate to validate their effectiveness and robustness against distribution shifts. To address this issue, we introduce FusionBench, which is the first comprehensive benchmark dedicated to deep model fusion. FusionBench covers a wide range of tasks, including open-vocabulary image classification, text classification, and text-to-text generation. Each category includes up to eight tasks with corresponding task-specific models, featuring both full fine-tuning and LoRA fine-tuning, as well as models of different sizes, to ensure fair and balanced comparisons of various multi-task model fusion techniques across different tasks, model scales, and fine-tuning strategies. We implement and evaluate a broad spectrum of deep model fusion techniques. These techniques range from model ensemble methods, which combine the predictions to improve the overall performance, to model merging, which integrates different models into a single one, and model mixing methods, which upscale or recombine the components of the original models. FusionBench now contains a range of CV and NLP tasks, 74 fine-tuned models, and 19 fusion techniques, and we are committed to consistently expanding the benchmark with more tasks, models, and fusion techniques. In addition, we offer a well-documented set of resources and guidelines to aid researchers in understanding and replicating the benchmark results. This includes detailed documentation, code examples, and tutorials, making FusionBench a user-friendly and accessible platform for both beginners and experienced researchers.",
    "keywords": [
        "model fusion",
        "model ensemble",
        "model merging",
        "model mixing",
        "multi-task learning",
        "knowledge transfer"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "The paper introduces FusionBench, the first comprehensive benchmark dedicated to evaluating various deep model fusion techniques across multiple tasks, ensuring consistent and robust comparisons.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=a0sK0foX3p",
    "pdf_link": "https://openreview.net/pdf?id=a0sK0foX3p",
    "comments": [
        {
            "comment": {
                "value": "Thanks for sharing that."
            }
        },
        {
            "comment": {
                "value": "We sincerely thank all the reviewers for their thorough evaluation and for offering insightful and constructive feedback. We are also pleased to see the positive recognition of our work's significance and practical implications, as well as the writing quality."
            }
        },
        {
            "comment": {
                "value": "**W1: This work appears to be an engineering effort to generalise existing models and methods within a unified pipeline. However, it lacks scientific novelty and significant technological advancement. Therefore, it may be better suited for applied venues, such as workshops, rather than a top-tier fundamental research conference.**\n\nThank you for your thoughtful review and for highlighting your concerns.\nWhile our work does involve engineering efforts to unify existing models and methods, we believe it goes beyond mere integration and introduces significant scientific advancements. \n\n- Our work enables the systematic investigation of model fusion techniques across a wide range of tasks and domains. \n- Our work enables reproducible research through standardized benchmarking. This facilitates comparative studies across different fusion approaches.\n- **Research Impact:** There are already several studies that are based on the codebase of FusionBench, including research on more advanced model fusion techniques and investigations into trustworthy model fusion methods.\n\n**W2: Besides the above issue, it still seems unclear how the model mixing happens in the background, though they state a list of existing methods they use in Table 1.** and \n**W3: Figure 1 is not very illustrative as Fig-1(b) and Fig-1(c) are almost identical. Further demonstration might be required in this regard.**\n\nWe appreciate the feedback. The complexity of model mixing techniques presents significant challenges for clear visual representation, primarily due to the architecture variability - model mixing methods fundamentally alter the model architecture in distinct ways. Specific Technical Challenges are as follows:\n    \n- Depth Upscaling modifies the number of layers in the network resulting in variable depth structures.\n- Model Expansion methods alter the dimensional properties of hidden states. This changes the internal representation spaces and introduces new parameter matrices of different sizes.\n- Mixture-of-Experts (MoE) Upscaling transforms standard dense layers into complex MoE structures. This introduces routing mechanisms and expert networks. Creates branching architectures that are inherently more complex.\n\nTherefore, creating a unified visual representation would require multiple different diagrams.\nThe readers can refer to the online documentation for more detailed explanations of each model mixing technique in this [anonymous link](https://anonymous.4open.science/w/fusion_bench-gh-page-A360/algorithms/depth_upscaling/).\n\n**W4: In terms of evaluation, it would be more beneficial to see how the proposed system works on large-scale data, such as the ImageNet-1k involved.**\n\nWe appreciate the suggestion.\nIn fact, it is convenient to conduct evaluations on custom image classification datasets using FusionBench.\nWe provide additional examples of how to configure and run the benchmark on custom datasets in the codebase.\n[Anonymous links to the example configurations can be found here](https://anonymous.4open.science/r/fusion_bench-79EC/config/dataset/image_classification/train/tiny-imagenet.yaml).\n\nIn this benchmark, we are currently focusing on evaluating the performance of multi-task model fusion in the in-domain multi-task setting.\n\n**W5:  I could not find any evidence of the claimed deliverable in the submitted work as stated in the abstract. \"...This includes detailed documentation, code examples, and tutorials, making FusionBench a user-friendly and accessible platform for both beginners and experienced researchers.\"**\n\nDue to the double-blind review process, we can not provide a direct link to the online documentation in the paper. \nHere is the anonymous link to the online documentation for the FusionBench project: https://anonymous.4open.science/w/fusion_bench-gh-page-A360/"
            }
        },
        {
            "comment": {
                "value": "**W1: While the authors seem to evaluate against 26 tasks as claimed in the paper, I found the actual number of tasks quite limited. For example, there are 8 different datasets used for image classification, but the authors deem them as 8 tasks. Fairly speaking, they can only be regarded as 8 benchmarks on the image classification task. In that sense, the wordings in such as Table 5 should be seen/unseen domains instead of tasks.**\n\nWe appreciate the feedback. We have made the necessary changes in Table 2 to the terminology used in the tables to avoid confusion.\n\n- In the context of image classification, we acknowledge that we have used \"task\" and \"domain\" interchangeably, which may cause some confusion.\n- For text generation tasks, we maintain a more precise distinction where \"task\" specifically refers to fundamentally different operations - such as question answering, text summarization, sentiment classification, etc.\n  \nThe term \"domain\" would indeed be more appropriate when discussing variations within a single type of task, such as open-vocabulary image classification and  different applications in mathematics or coding challenges.\n\n**W2: For the scene understanding part, the tasks are all limited to the dense per-pixel prediction type (depth, normal, and segmentation), and are only conducted against the NYUv2 dataset. I would recommend adding additional tasks that are common in this area, such as object detection.**\n\nThank you for the suggestions. We are consistently working on expanding the FusionBench to include more tasks and datasets.\n\n**Q1: In Table 2, the term `tasks` seems abused. Technically, there are 8 datasets instead of tasks for the image classification task. The same applies to the text classification and generation task, with 7 and 8 datasets, respectively. The scene understanding does contain 3 tasks (segmentation, depth, and normal). I recommend the authors edit the terminologies used and make the tables clearer.**\n\nWe appreciate the feedback. We have made the necessary changes in Table 2 to the terminology used in the tables to avoid confusion.\n\n**Q2: All the scene understanding tasks are dense (per-pixel) prediction tasks. It would be interesting to consider tasks such as object detection. Also, using the NYUv2 dataset only covers the in-domain multi-task setting and is limited. How about doing a cross-domain multi-task setting? For example, a segmentation model on ADE20K, plus a depth model on KITTI.**\n\nWe appreciate the suggestions.\nIn fact, many implemented algorithms are model-agnostic and task-agnostic, and can be applyed to object detection tasks with minimal modifications.\n\nOf course, we will also consider adding object detection tasks and cross-domain multi-task settings in the future."
            }
        },
        {
            "comment": {
                "value": "We appreciate the suggestions. In fact, we have implemented some fusion algorithms for large language models (LLM) in FusionBench, such as SLERP, DARE-Task Arithmetic, AdaMerging for Llama, Depth Upscaling, and ExPO, etc.\nAnonymous links to the code are listed as follows:\n\n- [SLERP](https://anonymous.4open.science/r/fusion_bench-79EC/fusion_bench/method/slerp/slerp.py)\n- [DARE-Task Arithmetic](https://anonymous.4open.science/r/fusion_bench-79EC/fusion_bench/method/dare/task_arithmetic.py)\n- [AdaMerging for Llama](https://anonymous.4open.science/r/fusion_bench-79EC/fusion_bench/method/adamerging/llama_adamerging.py)\n- [Depth Upscaling](https://anonymous.4open.science/r/fusion_bench-79EC/fusion_bench/method/depth_upscaling/depth_upscaling.py)\n- [ExPO](https://anonymous.4open.science/r/fusion_bench-79EC/fusion_bench/method/linear/expo.py)\n\nBut the implementation of these methods is not yet fully tested and documented. We will continue to improve the documentation and testing of these methods."
            }
        },
        {
            "comment": {
                "value": "**Q1: Did you explore any non-fusion baseline models to provide a more direct comparison of fusion performance benefits? If so, could those results be included to clarify the relative impact of fusion methods?**\n\nYes, we include the performance of task-specific models in the supplementary material and online documentation.\n\n**Q2:  Do you foresee expanding FusionBench to include multi-modal tasks, or would that require significant modifications to the existing benchmark structure?**\n\nYes, we plan to include multi-modal tasks in the future.\nIn fact, many implemented algorithms are model-agnostic and task-agnostic, and can be applied to multi-modal models with minimal modifications.\n\n**Q3: Are there recommendations for tuning fusion methods based on specific task types, especially for users who may be new to model fusion?**\n\nTrying simple averaging or ensemble methods is a good starting point for users who are new to model fusion.\nSimple averaging is a robust and architecture-agnostic method that can be applied to any model and domain, without the need for additional training.\nAfter that, users can try to develop more advanced fusion method.\n\n**Q4: How does FusionBench handle variability in pre-trained model quality? Are there mechanisms to assess or adjust for model quality across tasks?**\n\nThe pre-trained models indeed play a crucial role in the performance of model fusion, better pre-trained models will often lead to better performance of the fused model. In FusionBench, we provide a set of fine-tuned mdoels for each task to ensure the quality of models used in the fusion process are consistent across different fusion methods."
            }
        },
        {
            "comment": {
                "value": "**W1: The absence of direct comparisons with baseline models.**\n\nWe include the performance of task-specific CLIP vision models, GPT-2 models and Flan-T5 models in the supplementary material. It can also be found in our online documentation in the [anonymous links](https://anonymous.4open.science/w/fusion_bench-gh-page-A360/modelpool/clip_vit/#performance-of-the-fine-tuned-models).\n\nFor example, for CLIP-ViT-B/32 models, here are the results:\n\n| MODEL       | SUN397   | Cars     | RESISC45 | EuroSAT  | SVHN     | GTSRB    | MNIST    | DTD      |\n| ----------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| Pre-trained | 63.2     | 59.8     | 60.7     | 46.0     | 31.6     | 32.5     | 48.2     | 43.9     |\n| SUN397      | **75.0** | 47.0     | 54.3     | 46.5     | 28.3     | 26.4     | 44.3     | 41.6     |\n| Cars        | 56.6     | **78.3** | 50.9     | 38.4     | 30.2     | 30.6     | 49.7     | 41.8     |\n| RESISC45    | 52.0     | 47.2     | **95.2** | 56.9     | 23.9     | 24.3     | 39.7     | 35.9     |\n| EuroSAT     | 49.0     | 39.9     | 33.5     | **99.0** | 11.8     | 22.9     | 33.8     | 35.5     |\n| SVHN        | 40.5     | 36.3     | 18.9     | 9.8      | **97.3** | 27.3     | 81.8     | 23.2     |\n| GRSRB       | 36.9     | 33.0     | 20.6     | 21.3     | 41.2     | **98.9** | 30.9     | 23.9     |\n| MNIST       | 50.3     | 40.0     | 31.3     | 17.7     | 50.1     | 19.3     | **99.6** | 30.7     |\n| DTD         | 54.6     | 51.3     | 36.8     | 25.0     | 28.9     | 21.8     | 47.3     | **79.7** |\n\n**W2: A deeper explanation for the categorization of model ensembles, merging, and mixing would help clarify their task-specific benefits.**\n\nModel ensemble aggregates the predictions of multiple models to improve the overall performance, while all models in the ensemble are kept separate. Model merging combines the weights of multiple models into a single model, *keeping the model architecture the same*. Model mixing combines the weights of multiple models into a single model, *changing the model architecture*. \nFor instance, with depth upscaling, a model mixing technique, the number of layers (depth) in the model differ from that of the original models.\n\nThe pros and cons of each techniques:\n\n- Model ensemble: simple to implement, but expensive in terms of memory and computation resources for inference.\n- Model merging: no additional cost for inference, but the performance may not be as good as model ensemble.\n- Model mixing: can archive better performance than model merging, but it is more complex to implement. Typically requires additional training for performance recovery after fusion.\n\n**W3: More insights on scalability to additional tasks or modalities would enhance FusionBench\u2019s applicability to diverse research needs.**\n\nThat is a great suggestion. The benchmarking suite is designed to be easily extensible to new tasks and modalities, and we will include more tasks and modalities in the future. \n\n**W4: FusionBench\u2019s effectiveness depends on high-quality pre-trained models, which might limit performance in under-resourced domains.**\n\nWe agree that the performance of pre-trained models is crucial for the effectiveness of model fusion.\nIn fact, the idea of model merging is widely used in machine learning to improve the performance of models with limited resources. \nFor example, in each round of federated learning, the weights of the models from different clients are merged and distributed back to the clients. This can also be considered as a form of model merging.\nOther techniques for optimization such as exponential moving average (EMA), stochastic weight averaging (SWA), and latest weight averaging (LAWA) also use the idea of model merging to improve the performance and generalization of the models."
            }
        },
        {
            "summary": {
                "value": "This paper presents FusionBench, a comprehensive benchmark platform to evaluate deep model fusion techniques across tasks in image classification, text classification, and text generation. FusionBench organizes fusion techniques into model ensembles, model merging, and model mixing, and includes 26 tasks, 74 fine-tuned models, and 19 fusion algorithms. The benchmark aims to standardize evaluations, ensuring fair comparisons and supporting both novices and researchers with accessible resources and tutorials."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. FusionBench covers a broad range of tasks and fusion methods, providing flexibility for various fusion scenarios in NLP and vision tasks.\n2. Accessible resources, detailed documentation, and examples make FusionBench highly usable for beginners and researchers alike.\n3. The benchmark offers a fair, standardized evaluation of fusion methods, supporting clear comparisons across tasks.\n4. Well-organized structure, visual aids, and concise descriptions make it easy to understand and navigate the platform."
            },
            "weaknesses": {
                "value": "1. The absence of direct comparisons with baseline models.\n2. A deeper explanation for the categorization of model ensembles, merging, and mixing would help clarify their task-specific benefits.\n3. More insights on scalability to additional tasks or modalities would enhance FusionBench\u2019s applicability to diverse research needs.\n4. FusionBench\u2019s effectiveness depends on high-quality pre-trained models, which might limit performance in under-resourced domains."
            },
            "questions": {
                "value": "Q1. Did you explore any non-fusion baseline models to provide a more direct comparison of fusion performance benefits? If so, could those results be included to clarify the relative impact of fusion methods?\nQ2. Do you foresee expanding FusionBench to include multi-modal tasks, or would that require significant modifications to the existing benchmark structure?\nQ3. Are there recommendations for tuning fusion methods based on specific task types, especially for users who may be new to model fusion?\nQ4. How does FusionBench handle variability in pre-trained model quality? Are there mechanisms to assess or adjust for model quality across tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a comprehensive benchmark for deep model fusion that evaluates various models across a wide range of CV and NLP tasks. They also develop a modular codebase for user-friendly realization and evaluation of different model fusion approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n\n2. The proposed framework is modular and user-friendly.\n\n3. The authors comprehensively evaluated deep model fusion across different tasks and models."
            },
            "weaknesses": {
                "value": "1. While the authors seem to evaluate against 26 tasks as claimed in the paper, I found the actual number of tasks quite limited. For example, there are 8 different datasets used for image classification, but the authors deem them as 8 tasks. Fairly speaking, they can only be regarded as 8 benchmarks on the image classification task. In that sense, the wordings in such as Table 5 should be seen/unseen domains instead of tasks.\n\n2. For the scene understanding part, the tasks are all limited to the dense per-pixel prediction type (depth, normal, and segmentation), and are only conducted against the NYUv2 dataset. I would recommend adding additional tasks that are common in this area, such as object detection."
            },
            "questions": {
                "value": "I appreciate the authors' engineering efforts in putting together such a framework for evaluating deep model fusion. If the authors can address my concerns properly, I would be happy to edit the rating.\n\n1. In Table 2, the term `tasks` seems abused. Technically, there are 8 datasets instead of tasks for the image classification task. The same applies to the text classification and generation task, with 7 and 8 datasets, respectively. The scene understanding does contain 3 tasks (segmentation, depth, and normal). I recommend the authors edit the terminologies used and make the tables clearer. \n- for the image classification task, the setting looks more like domain transfer and generalization\n- for the scene understanding, it is standard multi-task learning\n\nThe authors may argue that among the 8 datasets, there are object recognition, satellite image classification, etc. \"tasks\" (what is said in their paper). However, these are all under the open-vocabulary image classification task, and getting down to that granularity does not make sense if we consider the other tasks in the paper (such as depth, segmentation, etc.)\n\n2. All the scene understanding tasks are dense (per-pixel) prediction tasks. It would be interesting to consider tasks such as object detection. Also, using the NYUv2 dataset only covers the in-domain multi-task setting and is limited. How about doing a cross-domain multi-task setting? For example, a segmentation model on ADE20K, plus a depth model on KITTI."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposed the first deep model fusion benchmark FusionBench together with codebase. The codebase is composed of three main modules: Algorithm, Mode Pool, and Task Pool. FusionBench has 16 built-in fusion algorithms implemented. Their comprehensive evaluation results are presented in the paper."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is very well motivated. As the first evaluation benchmark for deep model fusion, it would surely benefit the future researches in the domain.\n2. Extendability is a very important aspect for open-source benchmark. Glad to see it has been taken into consideration."
            },
            "weaknesses": {
                "value": "Recently, LLM and T2I/V models have gained tremendous attention in the research community. It might be worth adding evaluation for that. E.g., fusion methods for LLaMA based models, StableDiffusion models."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The submission failed to follow the rule of \"Anonymous Url: I certify that there is no URL (e.g., github page) that could be used to find authors\u2019 identity.\" In the submitted codebase as Supplementary Material, there are author's name in the LICENSE file, and non-anonymous GitHub and arXiv links in \"docs/README.md\"."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents FusionBench, a benchmark for evaluating deep model fusion techniques, which unify multiple neural networks into a single, more effective model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1- The proposed pipeline offers various tasks (image and text classification, text generation) and includes multiple models and fusion strategies (e.g., model ensembles, merging, and mixing). \n\n2- FusionBench standardises evaluations across tasks, model sizes, and fine-tuning methods, providing resources like documentation and tutorials to support researchers in replicating results.\n\n3- The paper is well-written, making it easy to read and understand."
            },
            "weaknesses": {
                "value": "1- This work appears to be an engineering effort to generalise existing models and methods within a unified pipeline. However, it lacks scientific novelty and significant technological advancement. Therefore, it may be better suited for applied venues, such as workshops, rather than a top-tier fundamental research conference.\n\n2- Besides the above issue, it still seems unclear how the model mixing happens in the background, though they state a list of existing methods they use in Table 1.\n\n3- Figure 1 is not very illustrative as Fig-1(b) and Fig-1(c) are almost identical. Further demonstration might be required in this regard.\n\n4- In terms of evaluation, it would be more beneficial to see how the proposed system works on large-scale data, such as the ImageNet-1k involved. \n\n5- I could not find any evidence of the claimed deliverable in the submitted work as stated in the abstract. \"...This includes detailed documentation, code examples, and tutorials, making FusionBench a user-friendly and accessible platform for both beginners and experienced researchers.\""
            },
            "questions": {
                "value": "Please refer to the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}