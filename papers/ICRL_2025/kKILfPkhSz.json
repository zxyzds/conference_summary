{
    "id": "kKILfPkhSz",
    "title": "ShortcutsBench: A Large-Scale Real-world Benchmark for API-based Agents",
    "abstract": "Recent advancements in integrating large language models (LLMs) with application programming interfaces (APIs) have gained significant interest in both academia and industry. Recent work demonstrates that these API-based agents exhibit relatively strong autonomy and planning capabilities. However, their ability to handle multi-dimensional difficulty levels, diverse task types, and real-world demands remains unknown. \nIn this paper, we introduce ShortcutsBench, a large-scale benchmark for the comprehensive evaluation of API-based agents in solving real-world complex tasks. ShortcutsBench includes a wealth of real APIs from Apple Inc., refined user queries, human-annotated \nhigh-quality action sequences, detailed parameter filling values, and parameters requesting necessary input from the system or user. We put in significant effort in collecting and processing the data. We revealed how existing benchmarks / datasets struggle to accommodate the advanced reasoning capabilities of existing more intelligent LLMs. Moreover, our extensive evaluation of agents built with 5 leading open-source (size >= 57B) and 5 closed-source LLMs (e.g. Gemini-1.5-Pro and GPT-4o-mini) reveals significant limitations of existing API-based agents in the whole process of handling complex queries related to API selection, parameter filling, and requesting necessary input from the system and the user. These findings highlight the great challenges that API-based agents face in effectively fulfilling real and complex user queries. All datasets, code, experimental logs, and results are available at \\url{https://anonymous.4open.science/r/ShortcutsBench}.",
    "keywords": [
        "Benchmark",
        "Agent",
        "LLM",
        "Shortcuts"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-15",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kKILfPkhSz",
    "pdf_link": "https://openreview.net/pdf?id=kKILfPkhSz",
    "comments": [
        {
            "comment": {
                "value": "Thank you once again for your feedback. I truly hope that my responses above have helped clarify any concerns you may have. We would be very grateful if you could carefully consider your final rating. Please don\u2019t hesitate to reach out with any additional questions or comments\u2014I would be more than happy to assist further."
            }
        },
        {
            "comment": {
                "value": "4. Several references are outdated, pointing to arxiv whereas the paper has been already published at a conference (ex: ToolLLM).\n    \nI apologize for not updating the correct citations in a timely manner. We will carefully review and ensure the accuracy of all citations in the next version."
            }
        },
        {
            "comment": {
                "value": "3. The used language appeals to emotions rather than states the scientific value (ex: \u201cWe made great efforts to evaluate\u2026\u201d).\n    \nI apologize for any confusion caused by this statement. We will revise it in the next version."
            }
        },
        {
            "comment": {
                "value": "2. The analysis of the performance of GPT 4o with the structured output capability was not performed.\u00a0https://openai.com/index/introducing-structured-outputs-in-the-api/\n    \nStructured output refers to a function that controls the output format to ensure it is correctly formatted, such as in JSON. This is not directly related to the main focus of our research (API selection, parameter filling, and awareness in requesting necessary input). Furthermore, as mentioned in the paper, we did not conduct evaluations with GPT-4o due to budget constraints."
            }
        },
        {
            "comment": {
                "value": "1. It is a technical report that lacks a scientific component. It should go to a software engineering conference, for example, International Conference on Software Engineering (ICSE) or IEEE/ACM International Conference on Automated Software Engineering (ASE). The paper can also go to Datasets and Benchmarks tutorials at AI conferences. Whereas the work is technically impressive and valuable for the software engineering community, there is no scientific value in the assessment of how good LLMs are able to use APIs. The problem at hand is purely technical and as a clear indicator for this is that over two thirds of references are links to web pages rather than scientific studies.\n    \nThere are many excellent benchmark studies presented at top conferences such as ICLR, NeurIPS, and ACL. I am unsure what specific concerns you have about my work, and I am unclear whether your comments are directed at AI conferences or top software engineering conferences. If you have any concerns, please provide specific feedback, as it will help me address them more effectively.\n    \nRegarding your comments, such as \"It is a technical report that lacks a scientific component\" and \"there is no scientific value in assessing how well LLMs can use APIs\", I must respectfully disagree. Our work establishes a pipeline for rapidly collecting real-world action sequences and provides findings that we believe will be helpful to others, backed by substantial analysis, thought, and effort.\n    \nThe top-tier conference papers dedicated entirely to datasets/benchmarks include, but are not limited to, the following:\n    \n- (ICRL\u201924) SmartPlay : A Benchmark for LLMs as Intelligent Agents\n- (ICLR\u201924) AgentBench: Evaluating LLMs as Agents\n- (ICLR\u201924) ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs\n- (ICLR\u201924) MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use\n- (NeurIPS\u201924) FinBen: An Holistic Financial Benchmark for Large Language Models\n- (NeurIPS\u201924) ZSC-Eval: An Evaluation Toolkit and Benchmark for Multi-agent Zero-shot Coordination\n    \nEvaluation has always been a critical component in guiding the development of AI, so I do not understand why you believe \"there is no scientific value in assessing how well LLMs can use APIs\". I would appreciate further clarification on what you define as \"science\" and \"engineering\", and I look forward to any insights your deep thinking might offer.\n    \nAs for your comment, \"The paper can also go to Datasets and Benchmarks tutorials at AI conferences\", I would like to clarify that when we submitted to ICLR, we chose the Primary Area of \"Datasets and Benchmarks\", and the keywords we selected included \"Benchmark\". ICLR has accepted many high-quality benchmark evaluation papers.\n    \nRegarding your concern that \"two thirds of references are links to web pages\", we must emphasize that the web pages we referenced were considered necessary to clearly explain our methodology. I apologize if this caused any inconvenience. We will strive to reduce the use of web citations and ensure that we present our work as clearly as possible with minimal references.\n    \nFinally, thank you for your kind words about our paper being \"technically impressive and valuable\"."
            }
        },
        {
            "comment": {
                "value": "Thank you again for your valuable feedback. If you have any further questions, we are always here to help!"
            }
        },
        {
            "comment": {
                "value": "3. *A notable feature of this benchmark seems to be scalability -- extracting real-world APIs and action sequences from Shortcut apps and sharing sites seems relatively easy. Providing quantitative details on dataset construction\u2014such as time taken for each step and the actual extent of manual effort involved\u2014would be nice.*\n    \nThank you for your valuable feedback. Scalability is indeed a key feature of our framework. In fact, it\u2019s entirely possible to set up a pipeline where shortcut programming is performed manually to generate action sequences, allowing us to efficiently construct a large number of action sequences.\n    \nAs for the specifics regarding the time required to build the dataset, we regret that we did not keep detailed records, but we can provide a general estimate:\n- **Collecting shortcut sites:** 3 days; this phase was entirely manual.\n- **Scraping links via Selenium automated testing tool:** 2 weeks; this step required custom code for each site.\n- **Shortcut deduplication, API validity check, and shortcut validity check:** 4 weeks; shortcut deduplication was automated through iCloud links and action content cleaning. API validity checks require manual verification. Shortcut validity checks combined with automated and semi-automated methods: automated filtering was conducted using Apple Scripts to run shortcuts for initial filtering, followed by manual loading of shortcuts into the Shortcuts app for verification.\n\nThere were also various additional manual and automated checks, which we will not detail here."
            }
        },
        {
            "comment": {
                "value": "2. *No human validation: Given the synthetic nature of the benchmark, it\u2019s uncertain whether all tasks are truly solvable or what the benchmark\u2019s upper bound is. Including human performance as a reference would add clarity.*\n    \nRegarding \"*whether all tasks are truly solvable*\", please refer to our response to reviewer \"**XY2i**\" on their first question. In summary, we implemented three steps to ensure the quality of our dataset, with both Step 2 and Step 3 involving manual verification. Since these shortcuts are available in the Shortcut Store specifically to address \"real user needs\" and have undergone human review, we believe there are no inherently \"unsolvable\" shortcuts.\n    \nAs for \"*what the benchmark\u2019s upper bound is*\", we have not conducted a manual evaluation of API selection and parameter filling for each shortcut. However, we would like to note that none of the existing works referenced in Table 2, and also, AppWorld, as you mentioned, have performed this level of manual validation.\n    \nWe are very open to performing a manual validation of API selection and parameter filling. However, it is challenging to complete this in the short term. API calls require skilled individuals with specific knowledge of Shortcut programming, and parameter filling entails complex annotation tasks. Moreover, ShortcutsBench includes a substantial number of shortcuts, further adding to the workload.\n    \nWe sincerely appreciate your valuable feedback, and if you have any further questions, we are always happy to assist."
            }
        },
        {
            "comment": {
                "value": "1. *Limited evaluation: The paper primarily assesses the model\u2019s ability to choose correct actions based on ground-truth sequences but doesn\u2019t evaluate its end-to-end task success rate (as done in, for example, AppWorld [1]). Experiments linking these aspects are missing.*\n    \nThank you for your comment. We will consider building a relevant dynamic runtime environment on the macOS platform in future work. We have already undertaken some preliminary tasks, such as validating shortcut functionality, enabling apps exclusive to iOS to run on macOS via decryption, and translating API calls generated by LLMs into shortcut commands. To fully evaluate whether a shortcut is \"successful\", Apple Scripts are required to run the shortcuts and capture signals from the associated apps to determine the final status. Otherwise, manual evaluating would be necessary, and we regret that we cannot \"manually verify\" all model-generated shortcuts in the short term.\n    \nNevertheless,  we believe that ShortcutsBench offers distinct features and advantages compared to existing datasets/benchmarks, we believe we have provided a substantial amount of evaluation results, covering areas such as API selection, parameter filling, and feedback awareness in agent-based API execution. We hope these contributions will inspire future research in the field.\n    \nWe will also add a citation for AppWorld and discuss relevant similarities and differences in our paper. \n    \nThank you again for your valuable comment."
            }
        },
        {},
        {
            "comment": {
                "value": "Thank you once again for your feedback. I truly hope that my responses above have helped clarify any concerns you may have. We would be very grateful if you could carefully consider your final rating. Please don\u2019t hesitate to reach out with any additional questions or comments\u2014I would be more than happy to assist further."
            }
        },
        {
            "comment": {
                "value": "4. *Why is all API-based agents the authors evaluate based on the ReACT framework, it would be good if they could provide additional evaluation/analysis on other frameworks, such as CodeAct.*\n    \nReACT is a generate-feedback alternating execution method, and our reasons for using the ReACT are as follows:\n    \n- It aligns with the settings of prior work, such as MetaTool, ToolLLM, ToolAlpaca, and ToolQA mentioned in Table 2, whereas APIBench, APIBank, and ToolBench use zero-shot and multi-shot settings.\n- A simple yet effective framework better reflects the model\u2019s capabilities, rather than the capabilities of the framework itself.\n    \nOur research focuses on evaluating API-based agents, whereas CodeAct\u2014mentioned in your feedback\u2014is a valuable approach for generating Python code, as referenced in the second paragraph of our related work. However, it is not directly relevant to our research.\n    \nWe will include a discussion of CodeAct in the related works section."
            }
        },
        {
            "comment": {
                "value": "3. *The paper does not make it very clear what is their most important finding in the abstract/intro/conclusion. It would be good if they could highlight their most important findings in the abstract/intro/conclusion. For example, they could discuss how open source models perform comparably to closed source models on simpler tasks but not harder tasks.*\n    \nIn the second-to-last paragraph of the \u201ccontributions\u201d in the Introduction, we have highlighted the findings we believe may be of the greatest interest to the reader. We believe each finding may impact different readers in various ways. You can find the detailed findings in Section 4."
            }
        },
        {
            "comment": {
                "value": "2. *The authors cite each work too many times in the paper, for example, a research paper is is cited five times in one paragraph in Section 2. Referencing previous works is good practice, but referencing too many times affects readability. It would be good if they could remove repetitive references.*\n    \nWe apologize for any inconvenience this may have caused. Each cited paper is referenced multiple times because each has a distinct focus and contributes uniquely to different aspects of our discussion. In cases where methods fall under various classification schemes, multiple references to the same work allow for more accurate contextual alignment with each specific point.\n    \nWe apologize for any inconvenience caused by the extensive citations or repeated references across sections. For the final published version, we will adjust by moving some citations to the appendix to streamline the reading experience."
            }
        },
        {
            "comment": {
                "value": "1. *Section 3 is not well-elaborated. readers will benefit from clearer description of this process. For example, for (2), the authors say 'after duplicating based on icloud link, ....', it is not very clear what is duplicated and why this step helps. It would be good if the authors could refine their descriptions on their methodology.*\n    \nWith the sentence \"After deduplicating based on 'iCloud link' (Apple, 2024b), we got the source files of all 8675 shortcuts\", we intend to convey the following:\n    \n- During the data construction process, we extracted shortcuts from multiple different Shortcut repositories. Since these repositories store shortcuts as iCloud links, the same shortcut might appear in multiple repositories. Additionally, the actual shortcut content (i.e., the action sequences) linked by these iCloud links could also be identical. Therefore, we applied deduplication to ensure that each shortcut in the final dataset was unique.\n    \nBeyond deduplication, we performed several additional operations, such as filtering out shortcuts dependent on non-existent APIs and importing all shortcuts into the Shortcuts app to verify their validity and functionality. For conciseness, only key steps are summarized in the paper. The complete process is available at\u00a0https://anonymous.4open.science/r/ShortcutsBench, and further supplementary details can be found in the Appendix.\n    \nIf you have any further questions, please don\u2019t hesitate to let us know. We would be glad to assist in any way we can."
            }
        },
        {
            "comment": {
                "value": "Thank you once again for your valuable feedback and insights. I truly hope that my responses above have helped clarify any concerns you may have. We would be very grateful if you could carefully consider your final rating (out of 10). Please don\u2019t hesitate to reach out with any additional questions or comments\u2014I would be more than happy to assist further."
            }
        },
        {
            "comment": {
                "value": "4. *The source of the reported numbers in Table 1 could be more clearly specified. There is uncertainty regarding whether models like Qwen-2.5-7B and LLaMA-3-8B consistently achieve more than 90% on ToolBench and over 80% on ToolLLM. Furthermore, given that MetaTool (https://github.com/HowieHwong/MetaTool) does not provide comprehensive details or code for model evaluation and metrics, more information is necessary to verify the accuracy of the table data.*\n\nIn the second paragraph of the Introduction, we mention, \u201cOur evaluation of these less intelligent LLMs on 3 representative existing benchmarks/datasets,\u201d meaning that the results in Table 1 are from our own evaluation. We evaluated the API selection performance on 3 representative datasets across various smaller-scale language models, which are generally considered to have lower reasoning capabilities. All data, code, and results used in these tests are fully open-sourced and available at https://anonymous.4open.science/r/ShortcutsBench/pre_experiments/README.md, where you can review them to verify the source of the results.\n    \nRegarding your comment that \u201c*MetaTool does not provide comprehensive details or code for model evaluation and metrics\u201d,* this does not impact our experiments. We used https://github.com/HowieHwong/MetaTool/blob/master/dataset/plugin_des.json as the tool description and selected https://github.com/HowieHwong/MetaTool/blob/master/dataset/data/all_clean_data.csv as the query data, evaluating only the accuracy of API selection.\n    \nFurthermore, to ensure a fair comparison, we used the same prompt template for testing as described in Section 4."
            }
        },
        {
            "comment": {
                "value": "3. *The evaluation primarily features selected proprietary models like GPT-4o and Gemini. For open-source LLMs, it mainly compares with general LLMs such as Qwen-2-70B and LLaMA-3-70B. Considering comparisons with more robust, specifically developed AI Agent models such as AgentLM (70B from\u00a0https://github.com/THUDM/AgentTuning) and xLAM (8x7b or 8x22b from\u00a0https://github.com/SalesforceAIResearch/xLAM) could provide more insights.*\n    \nIn our experimental design, we categorized the LLMs to be evaluated from both closed-source and open-source perspectives. Within each category, we selected models that are considered to have varying reasoning capabilities, which we believe is a reasonable classification.\n    \nThe number of models we evaluated is already more than existing work, as shown in Table 2:\n\n| Tool | Number of Large Language Models (LLMs) |\n| --- | --- |\n| MetaTool (ICLR\u201924) | 8 LLMs |\n| ToolLLM (ICLR\u201924) | 5 LLMs |\n| APIBench (NeurIPS\u201924) | 4 LLMs |\n| ToolAlpaca (arXiv\u201923) | 5 LLMs |\n| APIBank (EMNLP\u201923) | 6 LLMs |\n| ToolBench (arXiv\u201924) | 4 LLMs |\n| ToolQA (NeurIPS\u201923) | 2 LLMs |\n| ToolLens (CIKM\u201924) | - |\n    \nDespite this, we have fully accounted for the potential evaluation and analysis of additional models. Once relevant models are deployed, evaluating results can be obtained with a single command using Python's `openai` package. Detailed instructions on how to run our code for new model evaluations are provided in our code and data repository, accessible via this link: https://anonymous.4open.science/r/ShortcutsBench/experiments/README.md. You only need to make minor modifications to a bash script, such as https://anonymous.4open.science/r/ShortcutsBench/experiments/restart_all_experiments_llama_3_72b.sh, and add the model in https://anonymous.4open.science/r/ShortcutsBench/experiments/all_experiments.py. This will allow you to conduct the evaluation with one command. Our code is designed to handle network errors automatically, reattempting execution until all results are successfully obtained.\n    \nIf you have any other questions or need further clarification on the details, please feel free to let us know. We\u2019d be happy to assist you."
            }
        },
        {
            "comment": {
                "value": "2. *In section 3.2, the paper describes using GPT-4o to simulate user queries. However, it would be helpful to include the steps taken to verify the correctness and ensure the diversity of these user queries.*\n    \nTo ensure the accuracy of our constructed queries, we employ two main strategies:\n    \n1. As detailed in Section 3.2, we leverage a more advanced model, GPT-4o, for query construction. GPT-4o was selected following a preliminary experiment comparing GPT-3.5 (active at the time of evaluating), GPT-4o, and Gemini-1.5-Pro across 100 queries. Human evaluators scored GPT-4o-generated queries highest, outperforming the other two models. GPT-4o effectively captures required parameters and provides a clear query description, meeting our criteria in 94 out of 100 cases. In contrast, GPT-3.5 often missed specific parameters, and Gemini-1.5-Pro performed slightly below GPT-4o. \n        \n    Although we cannot guarantee all 7,627 queries precisely meet our requirements, we consider our approach reasonable. Similar work, such as ToolLLM, was published in ICLR '24 with Spotlight, using GPT for extensive query and action sequence generation without guaranteeing full accuracy.\n        \n2. As described in Section 3.2, our query data includes a range of information\u2014not only API data but also action sequences and functional descriptions provided by humans for each shortcut command. For the 100 queries generated, 94 met our manual verification criteria, requiring that the query adequately describes the shortcut\u2019s function and integrates both primitive and enum data type parameters necessary for API calls. These parameters were extracted from the full action sequences, and the template we used is available in Appendix A.2.\n    \nRegarding diversity, we did not actively introduce diversity into our queries; instead, the diversity of our dataset derives from the shortcuts themselves. Each shortcut corresponds to a unique constructed query. As mentioned in \u201c*Further Processing\u201d* in Section 3.3 (or Appendix A.3), we categorized our shortcuts into 8 categories, following the classification method used in the Apple App Store, which further reflects the diversity of our dataset.\n    \nIf you have any additional questions about the details, please don't hesitate to reach out. We would be glad to provide further clarification."
            }
        },
        {
            "comment": {
                "value": "1. *Although the paper emphasizes that the benchmark includes high-quality human-annotated action sequences from shortcut developers and queries derived from real user demands, it only mentions the shortcut developers are our annotators. Further details in this area would be beneficial.*\n    \n    In addition to the inherent quality assurance of shortcuts developed by shortcut developers in ShortcutsBench, we implemented **three** additional steps to ensure the quality of action sequences:\n    \n    1. As mentioned in the final paragraph of Section 3.1 (and detailed in Appendix A.1), we removed duplicate shortcuts by iCloud links. Specifically, we removed duplicate shortcuts based on their iCloud links and action sequences; any shortcuts with either of these identical elements were deduplicated.\n    2. Additionally, we manually filtered out shortcuts that relied on outdated or inactive APIs, as well as those requiring APIs from paid apps. This ensured that all included shortcuts use operational APIs. You can review the details of this process in our [open-source documentation](https://anonymous.4open.science/r/ShortcutsBench/deves_dataset/dataset_src_valid_apis/README.md). This was a manual process, involving two reviewers who tested on macOS Sonoma and iOS 17, downloading all relevant apps and verifying API functionality. APIs deemed obsolete were removed directly; you can view a list of these APIs [here](https://anonymous.4open.science/r/ShortcutsBench/deves_dataset/dataset_src_valid_apis/APIs_to_be_filtered_out.py). Apps used in this evaluating process can be downloaded following the guidance in our [README](https://anonymous.4open.science/r/ShortcutsBench/deves_dataset/dataset_src_valid_apis/README.md).\n    3. Finally, we signed all remaining shortcut files, as described in [ShortcutsBench documentation](https://anonymous.4open.science/r/ShortcutsBench/README.md), and loaded them into the macOS Sonoma Shortcuts app, where unsigned shortcuts cannot be imported. We tested these shortcuts by running them individually, as loading thousands of shortcuts at once would cause the app to crash. To facilitate evaluating on macOS Sonoma, we added the required apps to the Applications folder, allowing the Shortcuts app to capture and execute the corresponding shortcut actions. Note that some iOS-only apps require specific modifications (known as [Clutch](https://github.com/KJCracks/Clutch)) to run on macOS. Throughout this process, we manually excluded empty or low-quality shortcuts, resulting in the final dataset of 7,627 shortcuts as described in our paper.\n    \n    For details on how we ensured the quality of queries, we combine the response to this question with the Question 2 you mentioned.\n    \n    If you have any further questions regarding the details, please feel free to ask. We would be happy to assist you."
            }
        },
        {
            "comment": {
                "value": "We thank all reviewers for their comments and feedback. \n\nWe summarized the strengths highlighted by the reviewers and categorized the concerns as follows:\n\nThe reviewer has affirmed the benchmark, method, evaluation, findings, and artifacts of our paper:\n\n1. **benchmark**: notable contribution by creating a comprehensive benchmark (XY2i);  more holistic benchmark that contains real APIs, well-designed queries and actions (AydN);\n2. **method**: interesting approach (2zSS); this benchmark seems to be scalability (2zSS);\n3. **evaluation**: detailed analysis (AydN); comprehensive evaluation (2zSS);\n4. **findings**: insightful findings (XY2i); analysis results in 4.2 are insightful (vqjw);\n5. **open-source**: the artifacts are provided and well-organized (vqjw);\n\nThe reviewer expressed concerns about the dataset construction process and the adequacy of the experiments; and raised questions regarding the methodological details;\n\n1. **dataset construction process**: Further details about action sequences would be beneficial (XY2i); including steps taken to verify the correctness and ensure the diversity of user queries would be helpful (XY2i); including human performance as a reference would add clarity (2zSS).\n2. **the adequacy of the experiments**: Consider comparisons with more specifically models like AgentLM and xLAM could provide more insights (XY2i); why the authors evaluate based on the ReACT framework, and it would be good if they could provide additional evaluation/analysis on other frameworks, such as CodeAct (AydN); analyse the performance of GPT-4o with the structured outputs. (vqjw).\n3. **the methodological details**: more information is necessary to verify the accuracy of the Table 1 (XY2i); Providing quantitative details on dataset construction\u2014such as time taken for each step and the actual extent of manual effort involved\u2014would be nice (2zSS);\n\nThere are also some suggestions regarding certain phrasing and citations. For more details, we kindly refer you to the specific responses to each reviewer."
            }
        },
        {
            "summary": {
                "value": "This paper introduces a comprehensive, large-scale benchmark designed to evaluate API-based agents in real-world scenarios. It addresses the shortcomings of existing benchmarks and datasets, which often lack the richness and complexity needed to thoroughly assess different LLM-based agent models. To overcome these limitations, the authors have developed a high-quality benchmark encompassing 88 applications, 1,400 APIs, and an average of 21 actions per API, each covering various aspects of real-world deployments.\n\nIn the evaluation, the authors tested 10 LLMs on several key tasks: API selection, parameter filling, and the models' ability to recognize when to request additional input from either the system or the user. The experimental results show interesting insights, such as the challenges these models face with multi-step reasoning and understanding when external input is necessary."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper makes a notable contribution by creating a comprehensive benchmark for API-based agents, utilizing data extracted from Shortcuts. Compared to other API-based benchmarks, it offers several benefits, including a focus on the agents' ability to request necessary input from either the assistant or user and diverse difficulty of tasks. It covers a range of tasks, from simple ones to those involving complex APIs, queries, and action sequences. Additionally, the paper ensures quality by involving human verification, with shortcut developers serving as annotators.\n\nThe evaluations demonstrate insightful findings, especially regarding the challenges agent models face in reasoning and planning capabilities as indicated by API selection, as well as the difficulties weaker LLMs encounter in API parameter filling. These insights are valuable for the development of more advanced agent models."
            },
            "weaknesses": {
                "value": "1. Although the paper emphasizes that the benchmark includes high-quality human-annotated action sequences from shortcut developers and queries derived from real user demands, it only mentions the shortcut developers are our annotators. Further details in this area would be beneficial.\n\n2. In section 3.2, the paper describes using GPT-4o to simulate user queries. However, it would be helpful to include the steps taken to verify the correctness and ensure the diversity of these user queries.\n\n3. The evaluation primarily features selected proprietary models like GPT-4o and Gemini. For open-source LLMs, it mainly compares with general LLMs such as Qwen-2-70B and LLaMA-3-70B. Considering comparisons with more robust, specifically developed AI Agent models such as AgentLM (70B from https://github.com/THUDM/AgentTuning) and xLAM (8x7b or 8x22b from https://github.com/SalesforceAIResearch/xLAM) could provide more insights.\n\n4. The source of the reported numbers in Table 1 could be more clearly specified. There is uncertainty regarding whether models like Qwen-2.5-7B and LLaMA-3-8B consistently achieve more than 90% on ToolBench and over 80% on ToolLLM. Furthermore, given that MetaTool (https://github.com/HowieHwong/MetaTool) does not provide comprehensive details or code for model evaluation and metrics, more information is necessary to verify the accuracy of the table data."
            },
            "questions": {
                "value": "Please refer comments in above fields."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new benchmark ShortcutsBench, which evaluates agents' capabilities in solving tasks through API calling. They compare multiple API-based benchmarks, and showcase why ShortcutsBench contributes to the assessing agents' API calling abilities. The paper evaluates several API-based agents constructed based on the ReACT framework. The authors perform detailed analysis on the evaluation results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The authors introduce ShortcutsBench, which is a more holistic benchmark that contains real APIs, well-designed queries and actions. This could contribute to better evaluation of current agents's API calling capabilities in solving real-world tasks.\n- The authors provide example instances from ShortcutsBench in the appendix, which helps understanding the types of tasks in this benchmark.\n- The authors provide detailed analysis based on the evaluation results of several API-based agents."
            },
            "weaknesses": {
                "value": "- Section 3 is not well-elaborated. readers will benefit from clearer description of this process. For example, for (2), the authors say 'after duplicating based on icloud link, ....', it is not very clear what is duplicated and why this step helps. It would be good if the authors could refine their descriptions on their methodology.\n- The authors cite each work too many times in the paper, for example, a research paper is is cited five times in one paragraph in Section 2. Referencing previous works is good practice, but referencing too many times affects readability. It would be good if they could remove repetitive references.\n- The paper does not make it very clear what is their most important finding in the abstract/intro/conclusion. It would be good if they could highlight their most important findings in the abstract/intro/conclusion. For example, they could discuss how open source models perform comparably to closed source models on simpler tasks but not harder tasks."
            },
            "questions": {
                "value": "- Why is all API-based agents the authors evaluate based on the ReACT framework, it would be good if they could provide additional evaluation/analysis on other frameworks, such as CodeAct."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces ShortcutsBench, which mines real-world APIs/action sequences from iOS shortcut app. They then synthesize tasks with language model, and test a range of model's ability on it. Results show the task remains challenging to today's LMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Interesting approach: mining APIs/existing action sequence from Shortcut Apps makes a lot of sense, which is a resource previous works haven't tapped into.\n- Comprehensive evaluation: the authors evaluated a wide range of LMs across Open and close source models."
            },
            "weaknesses": {
                "value": "1. Limited evaluation: The paper primarily assesses the model\u2019s ability to choose correct actions based on ground-truth sequences but doesn\u2019t evaluate its end-to-end task success rate (as done in, for example, AppWorld [1]). Experiments linking these aspects are missing.\n2. No human validation: Given the synthetic nature of the benchmark, it\u2019s uncertain whether all tasks are truly solvable or what the benchmark\u2019s upper bound is. Including human performance as a reference would add clarity.\n\n[1] AppWorld: A Controllable World of Apps and People for Benchmarking Interactive Coding Agents"
            },
            "questions": {
                "value": "1. A notable feature of this benchmark seems to be scalability -- extracting real-world APIs and action sequences from Shortcut apps and sharing sites seems relatively easy. Providing quantitative details on dataset construction\u2014such as time taken for each step and the actual extent of manual effort involved\u2014would be nice."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces ShortcutsBench, a benchmark to assess LLMs\u2019 ability to call external APIs and create structured text (ex: JSON). The dataset was collected from the Shortcuts tool available on Apple devices. The authors perform the analysis of performance of a number of open and closed-source LLMs against ShortcutsBench."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "[1] Analysis results in 4.2 are insightful and confirm the ranking of closed- and open-source LLMs on traditional general knowledge and reasoning benchmarks like MMLU.\n\n[2] The artifacts are provided and well-organized."
            },
            "weaknesses": {
                "value": "[1] It is a technical report that lacks a scientific component. It should go to a software engineering conference, for example, International Conference on Software Engineering (ICSE) or IEEE/ACM International Conference on Automated Software Engineering (ASE). The paper can also go to Datasets and Benchmarks tutorials at AI conferences. Whereas the work is technically impressive and valuable for the software engineering community, there is no scientific value in the assessment of how good LLMs are able to use APIs. The problem at hand is purely technical and as a clear indicator for this is that over two thirds of references are links to web pages rather than scientific studies.\n\n[2] The analysis of the performance of GPT 4o with the structured output capability was not performed.\nhttps://openai.com/index/introducing-structured-outputs-in-the-api/\n\n[3] The used language appeals to emotions rather than states the scientific value (ex: \u201cWe made great efforts to evaluate\u2026\u201d).\n\n[4] Several references are outdated, pointing to arxiv whereas the paper has been already published at a conference (ex: ToolLLM)."
            },
            "questions": {
                "value": "I suggest to submit to a more relevant conference, rather than to ICLR, as mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}