{
    "id": "9aIlDR7hjq",
    "title": "Augmented Conditioning is Enough for Effective Training Image Generation",
    "abstract": "Image generation abilities of text-to-image diffusion models have significantly\nadvanced, yielding highly photo-realistic images from descriptive text and\nincreasing the viability of leveraging synthetic images to train computer vision\nmodels. To serve as effective training data, generated images must be highly\nrealistic while also sufficiently diverse within the support of the target data\ndistribution. Yet, state-of-the-art conditional image generation models have been\nprimarily optimized for creative applications, prioritizing image realism and\nprompt adherence over conditional diversity. In this paper, we investigate how\nto improve the diversity of generated images with the goal of increasing their\neffectiveness to train downstream image classification models, without fine-tuning\nthe image generation model. We find that conditioning the generation process\non an augmented real image and text prompt produces generations that serve as\neffective synthetic datasets for downstream training. Conditioning on real training\nimages contextualizes the generation process to produce images that are in-domain\nwith the real image distribution, while data augmentations introduce visual\ndiversity that improves the performance of the downstream classifier. We validate\naugmentation-conditioning on a total of five established long-tail and few-shot im-\nage classification benchmarks and show that leveraging augmentations to condition\nthe generation process results in consistent improvements over the state-of-the-art\non the long-tailed benchmark and remarkable gains in extreme few-shot regimes of\nthe remaining four benchmarks. These results constitute an important step towards\neffectively leveraging synthetic data for downstream training.",
    "keywords": [
        "Synthetic Training Datasets",
        "Image Generation",
        "Generative Models",
        "Diffusion"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=9aIlDR7hjq",
    "pdf_link": "https://openreview.net/pdf?id=9aIlDR7hjq",
    "comments": [
        {
            "summary": {
                "value": "This paper focuses on improving the effectiveness of synthetic images generated by text-to-image diffusion models for training image classification models. By introducing \u201caugmentation-conditioning\u201d, the authors leverage real images with data augmentations as conditioning inputs, creating synthetic images that are not only realistic but also visually diverse. This approach enhances downstream classifier performance, particularly in long-tail and few-shot classification settings, without the need for fine-tuning the diffusion model itself. The method was validated across five benchmarks, showing consistent improvements over previous techniques."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper introduces an approach of augmentation-conditioning, which leverages real images with data augmentations to create synthetic images that are both realistic and diverse. This method bridges the domain gap between synthetic and real data, enhancing downstream classification performance without requiring extensive fine-tuning of the diffusion model.\n2. The method\u2019s effectiveness is demonstrated across multiple challenging benchmarks, including long-tail and few-shot classification tasks."
            },
            "weaknesses": {
                "value": "1. The technical novelty of the proposed method seems limited, as it mainly combines existing data augmentations, like Mixup, before inputting images into an existing diffusion model. More discussion of method\u2019s novelty is necessary. Besides, to better demonstrate the effectiveness of the proposed method, it would be beneficial to consider more recent tuning-free approaches for diffusion models, such as [1]. Additional discussion and experiments comparing the superiority of the proposed method with current works would strengthen the paper.\n\n2. While the proposed augmentation-conditioning method focuses only on tuning-free augmentation approaches, there are low-cost tuning methods, such as [2] using LoRA, that also deliver strong performance. Why prioritize tuning-free methods when low-cost tuning options might achieve better results with only a minor increase in computational cost?\n\n3. The application scope of the proposed method appears limited (i.e., primarily for long-tail or few-shot classification), and the experimental validation seems insufficient.\n\na) In Table 1, the performance gain is more pronounced in the few-shot setting (fewer than 20 images, 55.3 -> 63.5) than in the many-shot setting (100 or more images, 72.4 -> 72.0). This suggests the method may not be as effective in many-shot contexts, even with only 100 images. A clearer explanation of this phenomenon would enhance the method\u2019s credibility.\n\nb) All experiments are conducted on the ImageNet-LT dataset, which alone cannot comprehensively verify the method\u2019s performance. Testing on a wider range of datasets, including general datasets like Tiny-ImageNet-200, CIFAR-100, and full ImageNet as in [1], as well as domain-specific long-tail datasets like CUB-LT and Flower-LT as in [2], would provide more robust evidence.\n\n[1] DIFFUSEMIX: Label-Preserving Data Augmentation with Diffusion Models. CVPR, 2024.\n\n[2] Enhance Image Classification via Inter-Class Image Mixup with Diffusion Model. CVPR, 2024."
            },
            "questions": {
                "value": "Please refer to Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper demonstrates that conditioning the generation process on an augmented real image and a text prompt produces effective synthetic datasets. These synthetic datasets benefit downstream tasks, particularly for long-tailed (LT) classification and few-shot classification."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The approach appears valid and effective for LT and few-shot classification, as demonstrated in the experiments. The authors also tested various augmentations.\n- Conditioning on both the augmented image and text prompt seems effective for improving performance on classification tasks.\n- Experiments on different values of classifier-free guidance (CFG) are interesting, especially regarding how the optimal scale varies by task."
            },
            "weaknesses": {
                "value": "- The technical novelty of this paper is unclear. The concept of combining both augmented images and text prompts seems useful for LT and few-shot classification but lacks novelty. If this approach is not technically original, the paper should at least show a broad variety of downstream tasks that benefit from it, which it did not.\n- The contribution is not clearly articulated. Although it\u2019s evident that the synthetic dataset is effective, it\u2019s unclear for which specific tasks it is most useful. The focus is confined to LT and few-shot classification. Could this approach also aid in other areas, such as image generation? Expanding the application scenarios would improve the paper\u2019s impact.\n- LT classification works, particularly those focused on algorithmic improvements, were not compared in the evaluation. While the paper\u2019s approach differs by aiming to improve classification via synthetic data, it\u2019s worth questioning if this is truly beneficial. For example, the paper mentions that fine-tuning for classification is time-consuming. However, both fine-tuning and generating a synthetic dataset have costs - generating 1.16M images seems likely to take more time than fine-tuning."
            },
            "questions": {
                "value": "- In conclusion, what do you suggest as the best approach among the trials (e.g., Mixup-Dropout, Embed-Mixup-Dropout, Embed-CutMix-Dropout)? In Table 3, the last method performs best, but in Figure 8, Embed-Mixup Dropout seems to lead in many cases. Does the optimal choice depend on the task and dataset? Are there any noticeable patterns?\n- Why did you focus specifically on LT and few-shot classification? Couldn\u2019t this approach and synthetic dataset also benefit other discriminative downstream tasks or image generation?\n- How long did it take to generate approximately 1 million synthetic images (as in Table 3)? Did you use T=1000 for generation, or T=50? Does the choice of timestep T affect the downstream performance (e.g., classification accuracy)?\n- How do you support the claim that your dataset has the highest diversity? In Table 1, the FID scores are provided, but FID does not solely calculate diversity. Also, FID of your approach and the baseline (random images) are nearly the same, with the baseline slightly lower. To substantiate the claim of high diversity, perhaps an additional metric, such as recall, would be helpful.\n- In the bottom-right graph of Figure 5, why are the performances of the three methods almost identical? Is there any underlying interpretation for this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The author explores a novel method for training image generation, where the proposed approach produces in-domain images with enhanced visual diversity by conditioning the generation process on augmented real training images. This strategy is validated across five established long-tail and few-shot image classification benchmarks, demonstrating consistent improvements over state-of-the-art results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The problems and the proposed method are clearly presented and easy to understand.\n- The proposed method is effective and straightforward in practice, with extensive ablation experiments conducted to support its design.\n- The synthesized training data demonstrates strong performance in few-shot classification."
            },
            "weaknesses": {
                "value": "In Table 3, the Fill-Up method demonstrates higher accuracy than the proposed method. Although the positive correlation between accuracy and training dataset scale is discussed in line 376, it remains unclear whether the proposed method can outperform Fill-Up. Given the computing constraints, I suggest the authors:\n\n- Illustrate the positive relationship between accuracy and dataset scale using data synthesized by the proposed method.\n- Provide results for Fill-Up with different, smaller synthetic dataset scales that are more feasible."
            },
            "questions": {
                "value": "See above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "While synthetic training images generated by diffusion models can be effective for training, diversity and fidelity are challenges. Fidelity can be addressed by image conditioning with few-shot images, but many previous works fine-tune the diffusion model, which can be expensive. The authors propose a frozen alternative to increase the diversity, which conditions the diffusion model not only on few-shot images (done previously) but augmentations (novelty).\n\nThey evaluate this across different standard data augmentation techniques, comparing downstream training accuracy and FID to find the most effective augmentation techniques. They also evaluate the effects of CFG scale, compare their method to previous SOTA in long-tailed distributions, and across 4 datasets to previous few-shot SOTA by number of shots."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "A) Writing is overall quite clear.\n\nB) Experiments are varied across areas (long-tail, few-shot).\n\nC) Method section shows many qualitative examples to supplement the quantitative results in the experimental section.\n\nD) Some of the results show improvement."
            },
            "weaknesses": {
                "value": "Points are ordered roughly according to my perceived scale, with more important points being listed first.\n\nA) The method itself is quite simplistic from a novelty perspective (simply adding augmentations to the conditioning). I would consider this a strength, if the results were consistent (B) and strong (B, D, E, F) with a clear storyline for effective use-cases (C). However, I do not see this as being the case (see following points for details, as indicated in the corresponding parentheses).\n\nB) The results are mixed. Examples: 1) In table 1, the random image baseline actually has the lowest FID. I do not see this discussed, with lines 323 and 339 pointing out that \"the best-performing augmentation-conditioning method has one of the lowest FID scores, supporting our claim...\", which is misleading. 2) In table 3, unintuitive bolding hides that your method underperforms in some categories (with LDM t&i, medium is worse and few is tied). 3) In Figure 6, by 16 shots, the novel method is already underperforming on 2 datasets. Once again, the writing does not properly address this.\n\nC) Given the mixed results, there should be an in-depth analysis / explanation to understand when this method is most useful, but this seems to be missing. I want to stress that if done well, this could potentially make up for weakness B.\n\nD) The comparisons with Fill-Up are not clear, as \"Ours\" does worse but uses less data--it would be better to compare against Fill-Up with the same amount of data as well, otherwise you have not shown that you are beating SOTA.\n\nE) There seems to be some baselines missing in the few-shot section that could strengthen the context. 1) as the ResNet50 is pre-trained, it would be helpful to know the starting accuracy. 2) Figure 6 is missing the random-image baseline included in Table 1.\n\nF) I do not find the CFG scale experiments as adding significant value, although they take up a page in total. They are consistent with previous work, which I don't find unsurprising. While they don't really 'hurt' anything in themselves, they overall weaken the experimental section by taking the space of what could have otherwise been more interesting / surprising / novel results, and in my opinion they water down the impact of the experimental section.\n\nG) In the introduction, it is claimed that methods that fine-tune the diffusion model (Azizi 2023, Trabucco 2023, Shin 2023) are too expensive. However, this is never supported with numbers--it would make the claim stronger to quantify method costs. Especially because these methods have vastly different costs (e.g. in line 134, you claim that Shin 2023 uses textual inversion--this should be less expensive than full fine-tuning, correct? And what about methods that use PEFT?)\n\nH) Table 3 is misleading. The way the sections are split, the entire lower section should be compared. However, Fill-Up is a separate class with more synthetic data, which you are not comparing against. This is not very clear by the visual, and is confusing why the highest numbers are not the ones bolded (as Fill-Up is excluded). Sometimes the bolding seems entirely wrong--e.g. in medium, \"ours\" is bolded, but LDM (txt and image) is clearly better, and also at comparable synthetic data counts."
            },
            "questions": {
                "value": "A) In the description of Figure 4, you claim in line 236 that \"Augmentation-conditioned generations show more visual diversity in the coloration, pose, and angle of the hamster.\" Could you please elaborate on what you mean? Because I do not see this as being the case. To me, 1) coloration looks consistent across all images, 2) pose only shows clear differences for CutMix and CutMix-Dropout. All others are primarily face photos staring at the camera and 3) with the same exceptions as 2, angle is primarily straight-on.\n\nB) In table 1, it is shown that the improvements are the most significant on the few classes--this is potentially interesting. Is there some analysis, interpretation, explanation, etc. that could be added on this subject?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}