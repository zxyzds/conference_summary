{
    "id": "dKfcntLRjZ",
    "title": "Wiki Entity Summarization Benchmark",
    "abstract": "Entity summarization aims to compute concise summaries for entities in knowledge graphs.\nHowever, current datasets and benchmarks are often limited to only a few hundred entities\nand overlook knowledge graph structure. This is particularly evident in the scarcity of\nground-truth summaries, with few labeled entities available for evaluation and training. We\npropose WIKES (Wiki Entity Summarization Benchmark), a large benchmark comprising\nof entities, their summaries, and their connections. Additionally, WIKES features a\ndataset generator to test entity summarization algorithms in different subgraphs of the\nknowledge graph. Importantly, our approach combines graph algorithms and NLP models,\nas well as different data sources such that WIKES does not require human annotation,\nrendering the approach cost-effective and generalizable to multiple domains. Finally,\nWIKES is scalable and capable of capturing the complexities of knowledge graphs in\nterms of topology and semantics. WIKES features existing datasets for comparison.\nEmpirical studies of entity summarization methods confirm the usefulness of our benchmark.\nData, code, and models are available at: https://anonymous.4open.science/r/Wikes-2DDA/README.md",
    "keywords": [
        "Entity summarization",
        "knowledge graph benchmark"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We propose a novel entity summarization benchmark with dataset generation, automatically generated ground-truth, and access to the graph structure.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=dKfcntLRjZ",
    "pdf_link": "https://openreview.net/pdf?id=dKfcntLRjZ",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces a benchmark dataset for Wiki entity summarization called WikES with the help of random walks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "--> The authors introduce a benchmark dataset for entity summarization over wikidata."
            },
            "weaknesses": {
                "value": "--> Why the automated mapping from Wikipedia to Wikidata allows for better summarization over entities as stated in \"Wikidata to automatically map entities from Wikipedia to Wikidata. This automation allows us to efficiently generate summaries for any number of entities\"\n--> The motivation behind choosing these 4 algorithms for comparison is missing.\n--> A thorough comparison with all the benchmark datasets given in the related work is missing. \n--> The main contribution on why this benchmark dataset is needed should be motivated more clearly.\n--> The overall results in table two are very low, what would be the reason behind that?"
            },
            "questions": {
                "value": "--> Why the automated mapping from Wikipedia to Wikidata allows for better summarization over entities as stated in \"Wikidata to automatically map entities from Wikipedia to Wikidata. This automation allows us to efficiently generate summaries for any number of entities\"\n--> Why did authors only choose these algorithms for comparison?\n--> Why these algorithms are not also tested on all the benchmark datasets which are given in the related work?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new entity summarization(ES) benchmark, called WIKES, sourced from Wikidata and Wikipedia. Compared existing ES benchmarks, WIKES is generated automatically without relying on human labeling. WIKES also contains complex topology and semantics of the knowledge graph by including 2-hop connected sub-graphs  and diverse topics."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The new benchmark WIKES is the first ES benchmark that does not require human annotation. And the generation method could be easily applied to generate other ES datasets with diverse topics and scales. \n2. WIKES is the largest ES benchmark compared to existing benchmarks, which make it possible to explore the effectiveness of the ES methods over large scale datasets. \n3. Some results on the smallest datasets of WIKES  are presented, giving baseline results for further researches."
            },
            "weaknesses": {
                "value": "1. Relying Wikipedia\u2019s abstract to generate the ES datasets is cost-efficient and novel. But this makes the entity summarization generated based on the abstract  text rather than the triples of the entities in the knowledge graph. This might cause the entity summarization in WIKES not the gold entity summarization of the entities. \n2. The DistillBERT is used to annotate the property that should be included in the summarization. The correctness of the final property is not evaluated, which is important to the quality of the WIKES in terms of entity summarization. \n3. The datasets evaluation is not comprehensive. For example, \n    (1). Figure 3 only shows the F1 evaluation on WIkiProFem, part of the WIKES benchmark. The F1 score on other subdatasets are not presented. \n    (2). Table 2 shows that results of entity summarization methods on the smallest WIKES datasets. But the midium and the large WIKES datasets are not tested. It is not clear what would be the performance of current summarization methods on these two datasets. \n4. The dataset quality are not analyzed, for example, the correctness and diversity that are important for ES benchmark. \n5. Minor points and typos: \n   (1). In line 119, there is an extra question mark after \u201c(version 3.9)\u201d. \n   (2). The citation format in the main text seems not correct."
            },
            "questions": {
                "value": "1. Have you evaluated the correctness of property identification results based on the DistillBERT in terms of entity summarization? \n2. Why the left side of the Equation (2) equal to the right side? \n3. How would the minRW, maxRW, and minRW affect the random walk results for graphs in different scales? Especially how they would affect the quality of the entity summarization datasets? \n4. What is the meaning of Real-first and Real in the Figure 3? They are not explained in the main text."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduced a novel large-scale benchmark, named as WIKES, for entity summarization (ES) in knowledge graphs. Existing benchmarks are limited in size and often rely on expensive and time-consuming manual annotations. In contrast, WIKES aims to overcome these limitations by using Wikipedia abstracts and knowledge graph structures from Wikidata to automatically generate high-quality entity summaries without the need for manual annotators.\nThis paper proposed a dataset generation algorithm by combining graph algorithms with natural language processing models. The automatic processing makes the dataset be scalable and suitable for multiple domains.\nFinally, this paper  offers a comprehensive evaluation of several entity summarization methods, including both unsupervised (e.g., PageRank, RELIN, LinkSum) and supervised models (e.g., GATES). It highlights that unsupervised methods, particularly graph-based approaches, often outperform supervised models in large-scale knowledge graphs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1: The WIKES benchmark is scalable, leveraging automatic summary generation from Wikipedia and Wikidata without relying on costly manual annotations, making it applicable to large datasets across various domains. The use of random walk-based subgraph extraction ensures that the structure of knowledge graphs is preserved, capturing both topological and semantic complexities of entities while maintaining computational efficiency.\n\nS2: This paper provides a thorough evaluation of multiple graph-based entity summarization methods (e.g., PageRank, RELIN, GATES), allowing for direct comparison of unsupervised and supervised approaches, highlighting the advantages of graph-based methods.\n\nS3: The overall presentation is good and the authors provided source code for review."
            },
            "weaknesses": {
                "value": "W1: The summarization methods are limited to graph-based summarization techniques. The authors may need to evaluate some text generation methods. A broader comparison with recent NLP-based summarization techniques could add more depth.\n\nW2: The paper focuses on scalability but only evaluating the small version of their dataset. The methods without efficiency concerns could be used to conduct evaluation on the large version to show the effectiveness of the proposed dataset.\n\nW3: The random walk-based graph expansion approach may not always capture the most semantically relevant information for all types of entities. While the two-hop neighborhood approach is computationally efficient, it may miss out on key relationships that are further away in the graph but still contextually important for the entity. The authors could have considered a dynamic approach, where the hop count is adjustable based on the entity or relationship type.\n\nW4: The authors could provide some qualitative examples of the generated summaries of model like LinkSum in Appendix ."
            },
            "questions": {
                "value": "Q1: In your paper, entity summarization is derived from Wikipedia abstracts and infoboxes, primarily using extraction-based methods. However, you did not explore any text generation or abstractive summarization models (such as GPT or T5) to create summaries from the knowledge graph or the Wikipedia text. Given the recent advancements in text generation, have you considered evaluating the performance of generative models for entity summarization, especially in cases where the structured data might be sparse or incomplete? What are the potential reasons for not including these methods in your benchmark?\n\nTypos:\n- Line 34, Entity summarization (ES) => Entity Summarization (ES)\n- Line 119 \u201c(version 3.9) ? and\u201d\n- Line 122 \u201cINFO\u201d may need to be bold."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a method to automatically create a dataset for evaluating entity summarization. Compared with existing datasets, with this new approach:\n- larger datasets can be programmatically generated,\n- mutli-hop (empirically 2-hop in the paper) relations are allowed,\n- a connected subgraph is guaranteed to be sampled."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1. A programmatically generatable benchmark for entity summarization. Can be larger and better than existing ones. It represents a useful contribution to the community. I like it.\n\nS2. Generally reasonable approach to benchmark generation. The generated gold-standard summaries are likely to have high quality, despite lacking verification.\n\nS3. Well-written paper. Very easy to read."
            },
            "weaknesses": {
                "value": "W1. Missing some important technical details and experiments related to the quality of the automatically generated gold-standard summaries.\n- Line 175-176, how did you identify 'mentions' of Wikidata items in a Wikipedia page? If only relying on hyperlinks, recall could be low. Essentially it is an entity linking problem. I would like to see more details about its implementation. I also want to see a technical comparison of your approach with https://doi.org/10.1016/j.ipm.2013.12.001 which also relies on Wikipedia abstract to create gold-standard entity summaries.\n- The above-mentioned entity linking, and your heuristic relation selection method, both can be inaccurate. Have you conducted experiments to evaluate the quality of your generated gold-standard summaries?\n- Line 138-140, according to this problem definition, do you consider or ignore literals?\n\nW2. While your sampled graphs can be arbitrarily large, it seems that your gold-standard summaries are limited in the following aspects.\n- I am not sure whether generating a larger graph is really helpful for the entity summarization task. According to Table 1(b)(c)(d), while graphs differ in size, the number of gold-standard summaries remains almost unchanged, about 500 entities, if I understood correctly. You criticized existing benchmarks for their small size in terms of the number of entities with gold-standard summaries (e.g., 175 in ESBM), but your datasets are not significantly larger.\n- All these ~500 entities are instances of person, while there are many other types of entities in DBpedia/Wikidata and in previous benchmarks like ESBM. Why did you limit your datasets to person entities? It introduces a bias.\n- Only entities with English labels are considered. This is acceptable, but not necessary IMO."
            },
            "questions": {
                "value": "Apart from my questions in Weaknesses, I have the following further questions/comments.\n\nQ1. For relation selection, you compared relation embedding with abstract embedding. The latter covers a very large piece of text. How about comparing with the embedding of the sentence where the relation value is mentioned?\n\nQ2. Line 321-323, I did not understand why the bias comes from the small size and/or the incomplete edges of the sampled graphs. An extended explanation would be appreciated.\n\nQ3. LinkSum performed best in your experiments. Is it possible that its good performance came from its use of Backlinks, which coincides with your approach to generating gold-standard summaries (based on entities mentioned in Wikipedia abstracts)? It may represent a bias of your ground truth."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}