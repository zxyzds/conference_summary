{
    "id": "gRXLa6LS3J",
    "title": "Zero-shot Outlier Detection via Synthetically Pretrained Transformers: Model Selection Bygone!",
    "abstract": "Outlier detection (OD) has a vast literature as it finds numerous applications in\nenvironmental monitoring, security, manufacturing, and finance to name a few.\nBeing an inherently unsupervised task, model selection is a key bottleneck for OD\n(both algorithm and hyperparameter selection) without label supervision. There is\na long list of techniques to choose from \u2013 both classical algorithms and deep neural\narchitectures \u2013 and while several studies report their hyperparameter sensitivity, the\nliterature remains quite slim on unsupervised model selection\u2014limiting the effective use of OD in practice. In this paper we present FoMo-0D, for zero/0-shot OD\nexploring a transformative new direction that bypasses the hurdle of model selection\naltogether (!), thus breaking new ground. The fundamental idea behind FoMo-0D is\nthe Prior-data Fitted Networks, recently introduced by M\u00fcller et al. (2022), which\ntrains a Transformer model on a large body of synthetically generated data from a\nprior data distribution. In essence, FoMo-0D is a pretrained Foundation Model\nfor zero/0-shot OD on tabular data, which can directly predict the (outlier/inlier)\nlabel of any test data at inference time, by merely a single forward pass\u2014making\nobsolete the need for choosing an algorithm/architecture and tuning its associated\nhyperparameters, besides requiring no training of model parameters when given a\nnew OD dataset. Extensive experiments on 57 public benchmark datasets against\n26 baseline methods show that FoMo-0D performs statistically no different from the\n2nd top baseline, while significantly outperforming the majority of the baselines,\nwith an average inference time of 7.7 ms per test sample.",
    "keywords": [
        "zero-shot outlier detection",
        "prior-data fitted networks"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=gRXLa6LS3J",
    "pdf_link": "https://openreview.net/pdf?id=gRXLa6LS3J",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces FoMo-0D, a novel zero-shot approach for outlier detection (OD) that bypasses the traditional model selection process, a major obstacle in unsupervised OD tasks. Model selection, including algorithm and hyperparameter tuning, has long been a challenge due to the unsupervised nature of OD. FoMo-0D leverages Prior-data Fitted Networks (PFNs), a Transformer-based model trained on a large synthetic dataset from a prior distribution, allowing it to make direct outlier predictions on new datasets without further tuning or training. As a foundation model for zero-shot OD on tabular data, FoMo-0D delivers outlier predictions with a single forward pass, eliminating the need for manual algorithm selection or hyperparameter adjustments. The authors of the paper conduct extensive experiments to demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The authors of the paper tackles an important yet very challenging problem of unsupervised anomaly detection. \n- The proposed method is technically sound. \n- The paper is well written and easy to follow. \n- The authors of the paper conduct ample experiments to demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "- Despite the optimization done to scale the proposed method up, it is still not immediately clear to me how scalable the proposed method is. Does the proposed method still work if the training data contains millions of samples? Moreover, what is the inference latency and memory overhead? Would the proposed method incur very heavy computational burden if I have to handle a large number of samples during inference? Such a scenarios is arguably common in real world anomaly detection settings. \n- How limiting is the use of GMM for synthetic data generation for pre-training. Does such a method scale to more complex datasets?"
            },
            "questions": {
                "value": "- I wonder if it would be possible to do some sort of finetuning when a training data comes in, so that we don't need to feed in the entire training data into the model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper present FoMo-0D, the first foundation model for zero-shot OD on tabular datasets. FoMo-0D is pretrained on many synthetically generated datasets drawn from a novel data prior that the author introduce to capture various inlier and outlier distributions. Feeding the FoMo-0D with a new training set and a text point, it perform zero-shot inference on a new dataset via a single forward pass, fully abolishing the need for both model training on a new dataset and hyperparameter selection."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper presentation is clear. The proposed method is well-motivated.\n2. The experimental results are highly sufficient and also demonstrate the effectiveness of the proposed foundation model."
            },
            "weaknesses": {
                "value": "1. While it is noted that \"finding a prior that supports a sufficiently large subset of possible [data generating] functions isn\u2019t trivial,\" the paper demonstrates that the initial attempts were adequate to achieve remarkable performance even with a simple data prior. Could you provide some in-depth insights into this result?\n\n2. The paper conducted experiments on several benchmark datasets for out-of-distribution (OOD) detection, including CIFAR-10, Fashion-MNIST, MNIST-C, MVTec-AD, and SVHN. I am curious about the performance of FoMo-0D on more complex datasets, such as the ImageNet-level benchmarks recently proposed in OpenOOD v1.5 [1].\n\n[1] OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection, NeurIPS 2023 Workshop on Distribution Shifts"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper leverages Prior-data Fitted Networks, which can directly predict the (outlier/inlier) label of any test data at inference time, by merely a single forward pass\u2014making obsolete the need for choosing an algorithm/architecture and tuning its associated hyperparameters, besides requiring no training of model parameters when given a new OD dataset."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is written well and is easy to understand.\n\n2. The studied problem is very important.\n\n3. The results seem to outperform state-of-the-art."
            },
            "weaknesses": {
                "value": "1. The baselines compared in this paper are not competitive enough. There are much more advanced OD algorithms that are unsupervised or semi-supervised and do not require retraining on the new test set. Please refer to [1] for more advanced algorithms. There are plenty of strong post-hoc algorithms that can be efficient and effective at the same time. \n\n2. The inlier piror and outlier synthesis idea is explored in [2]. Could the authors clarify the differences? For me, I did not see any significant technical differences from the prior selection and the synthesis approach, even though there are naunces in model architecture and the focused problem. \n\n3. Is it possible for the authors to show the actual detection metrics rather than the rank so that it makes the readers easy to see the algorithm performance.\n\n\n\n[1] Yang et al., Generalized Out-of-Distribution Detection: A Survey\n\n[2] Du et al., VOS: Learning What You Don't Know by Virtual Outlier Synthesis"
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes to pretrain a foundation model on synthetic data for outlier detection, which utilizes in-context learning for the downstream detection tasks without hyperparameter tuning or post-training. The authors leverage the \"router mechanism\" to facilitate architecture scaling up and random linear transformation to speed up the synthesis process."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper presents a paradigm shift for outlier detection, training a foundation model for various downstream tasks without model selection and hyperparameter tuning.\n2. The paper is well-written, which makes it easy to follow.\n3. The experiments are sufficient for elucidating the intrinsic mechanisms of the proposed FoMo-0D."
            },
            "weaknesses": {
                "value": "1. While FoMo-0D doesn't require training on specific real datasets, it has inferior inference time compared to several baselines in Table 3, due to the forward pass of the whole training dataset during inference. If the training dataset scales up, there will be a high computation load during inference, undermining FoMo-0D's efficiency in time-critical inference scenarios."
            },
            "questions": {
                "value": "1. As the foundation model already achieves comparative performance compared with baselines, can the performance be continuously improved if we conduct post-training with specific real data, a common practice in NLP or CV?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}