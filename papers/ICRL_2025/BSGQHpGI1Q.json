{
    "id": "BSGQHpGI1Q",
    "title": "Characteristic Function-Based Regularization for Probability Function Informed Neural Networks",
    "abstract": "Regularization is essential in neural network training to prevent overfitting and improve generalization. In this paper, we propose a novel regularization technique that leverages decomposable distribution and central limit theory assumptions by exploiting the properties of characteristic functions. We first define Probability Function Informed Neural Networks as a class of universal function approximators capable of embedding the knowledge of some probabilistic rules constructed over a given dataset into the learning process (a similar concept to Physics-informed neural networks (PINNs), if the reader is familiar with those). We then enforce a regularization framework over this network, aiming to impose structural constraints on the network\u2019s weights to promote greater generalizability in the given probabilistic setting. Rather than replacing traditional regularization methods such as L2 or dropout, our approach is intended to supplement this and other similar classes of neural network architectures by providing instead a contextual delta of generalization. We demonstrate that integrating this method into such architectures helps improve performance on benchmark supervised classification datasets, by preserving essential distributional properties to mitigate the risk of overfitting. This characteristic function-based regularization offers a new perspective for enhancing distribution-aware learning in machine learning models.",
    "keywords": [
        "Regularisation",
        "Supervised Learning",
        "Neural Network Architecture Paradigms"
    ],
    "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)",
    "TLDR": "Defined a class of neural networks based on frequency distribution theory and used the idea of infinitely divisible decomposable distribution to regularise such networks; in a simple easy to understand setting.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=BSGQHpGI1Q",
    "pdf_link": "https://openreview.net/pdf?id=BSGQHpGI1Q",
    "comments": [
        {
            "comment": {
                "value": "Dear Reviewer, thank you for your kind and thoughtful comments.\n\nRegarding the weaknesses :\n\n1) We agree strongly with your point. As mentioned in reply to ZFUN \" the idea presented in the remaining motivation was specifically with regards to the regularization target but didn't focus on why the characteristic function was used. The advantage of it comes specifically from the idea that it addresses directly distributional behavior and also calculating convolutions becomes a simple step of point wise algebra. Since this paper was cut down from a larger manuscript we realized the motivation part might have been chopped a bit too aggressively, maybe would you think addressing this would help in the revision? \"\n\n2) This is also a good question. The idea is more so understanding/assuming a global viewpoint would allow to smooth any microcomplexities that may arise from the usual learning process. A simple example to visualise this is if you imagine drawing some random samples from something we imagine to be a uniform distribution, then you have a non-leveled histogram but by trying to \"preserve\"  the uniform distribution properties, we can allow for this jagged histogram to be smoothed out.\n\n3) Only singular forms and you idea of having the combined forms was also something that was on our mind but we did not add those into the table since it was getting too big (the permutations make it a \"too many columns to comfortably read\" table) . However, we do have the combined results from our tests. Would you recommend we put it in our revision for additional information, and if so how do you think we can neatly present it? \n\n4) The idea was more so choice of \"punishment metric \" where instead of using something like the L-p regularisation, this can be a handy and more \"explainable choice\" of regularization. More concretely, the regularisation of L2 penalizes large weights by adding a term proportional to the squared magnitude of the model's parameters to the loss function but heuristically, our regularisation punishes the deviation of the generated distribution through assessing how non-similar it is to some global distribution assumption we postulate it is supposed to take (this global assumption is essentially what we use to construct the design of the NN in the first place).\n\nRegarding your Overall comment:\n\nThank you for your kind words and it was nice to see you enjoyed the manuscript. We also totally see your point of view where it would have been nice to see where the model does well. I think as per what we addressed in Reviewer ZFUN's comment also, maybe our view on wanting the reader not to take away that this model is the \"best\" model by cherry picking examples may have worked against us because we ended up not showing examples where it shines despite it being difficult to explain exactly why but would it help if we add a section regarding that and putting back the results where it significantly outperformed the others in certain datasets in the revised manuscript?\n\nThank you once again!"
            }
        },
        {
            "comment": {
                "value": "Dear Reviewer, thank you for your comments.\n\nAddressing the questions:\n\n1) What data distribution assumption is used in the experiment to implement the proposed regularization? . You mention in your first line of the review \"more specifically the central limit theorem\". I think this is self explanatory. \n\n2) Where is the assumption used?  This is the defining assumption of a \u2018perceptron\u2019 network. The whole idea was to keep the simplest network to be able to test validity. I think this question highlights a fundamental misunderstanding in terms of the most basic part of the paper already as this assumption is supposed to be trivial (doesn\u2019t necessarily imply \u2018easy\u2019 but rather elementary that it doesn't need a complex or elaborate argument if one has some experience in theoretical ML ) as it would be difficult to continue understanding the rest of the paper clearly if this is something you are not too familiar with as the basis of most ideas are built from this school of thought. So maybe why there is a stark difference in viewpoints .\n\n\nRegarding Weaknesses: \n1) \"Don't see any novel architecture here\"  != \"novel regularization technique\" (abstract line 2) \n\n\n2) The phrasing suggests several misconceptions, as the questions contain multiple semantic errors. Addressing from back to front :\n\n\"Assume output of the model is Bernoulli\" -> Wrong. The output of the model is some probability distribution. Specifically for the toy example presented it is the LC of Bernoullis.\n\n\" MNIST ... continuous data \" -> another misconception since the MNIST data set is originally 8 bit $(i \\in \\mathbb{Z}, \\ 0 \\leq i \\leq 255)$ and it's normalized to [0,1]. This normalization doesn't make it continuous. It is still discrete. \n\n\"This does not make sense for practical data\" -> again wrong. The generating function is assumed to be a LC of Bernoullis which are in theory able to approximate many distributions. Also if by practical you mean anything stored on a computer, then it is even more so as it is discrete (refer to section 2.5 propositions).\n\n\"assume the data follows a linear combination of Bernoulli distributions\" -> \"follow\" does not necessarily mean \"generated by\".\n\n\n\n\n\n3) Again there are multiple false claims made here.\n\n\"  For data in practice ... hard to compute its characteristic function ... do not know its true distribution  \" -> This is the key misconception that you may have. As described in the first part of the paper the PFINNs are constructed in such a way that we assume SOME structure regarding an assumption about it's true distribution.\nFurthermore, the idea of ML in general is to derive a function/MAPPING with the help of heuristics as a closed form solution is difficult/impossible to find. In our case , the PFINNs we use have the goal of finding a MAPPING from the data to a preset probability function(be it a PMF or approximate PDF or even a singularity) that approximately generates the data (the learning is to find the best-fit parameters). This can be seen from the toy example provided. Since the practitioner constructs the architecture based on THIS apriori knowledge, this means it is easy for a person with sufficient knowledge to construct the limiting characteristic function (in our case the Normal characteristic function we use as the target for regularizing against) for the appropriate setting.\n\n\" This makes the proposed regularization method invalid.\" -> This is therefore a flawed claim.\n\n\"How do you update the parameters through the regularization?\" -> This is stated in the paper.\n\n\n\n4) Agree on the briefness, \n\nProfessionalism -> We think this is a slippery argument because we would say instead of vaguely suggesting \u2018high quality papers\u2019 to actually provide some concrete examples. This way there is some basis for discussion.\n\nLooking forward to your reply. I think there is lots to learn from discussion and maybe you can elucidate your points further to see if there is something that hopefully both of us can take away."
            }
        },
        {
            "comment": {
                "value": "Dear Reviewer,  thank you for your review. \n\nAlso with regards to your comment on the section 2.5 , semantically, a proposition is a statement that can either be true or false, but not both. It is a declarative sentence that makes a claim about a mathematical object or concept, and its truth value (whether it is true or false) is determined based on logical reasoning, definitions, and previously established facts or axioms.\n\nExample of which \"1 + 1 = 2\" is a proposition, and it is true based on the usual set theoretic axioms. This proposition doesnt carry the same idea of \"proposing\" which is what you are implying we did.\n\nOtherwise, since you don't have any questions and there is a signifcant overlap with your comment with the Reviewer ZFUN, I would invite you look at our discussion there.\n\nMore specifically let me highlight the parts that are relavent :\n\n\"This was a point of contention which we had when trying to distill the numerical results to be presented. From our discussions we decided to fix the parameter instead because it feels very much like p-value hacking just to squeeze the best possible results for a given setting to show where one method shines since it is really data dependant and from what we found, it quite difficult to explain exactly why it works as it is a non convex problem to be explained ; We do have results where the method obliterates the usual L1,L2 when hidden layers are added and on some specific datasets but I feel would be disingenuous to present it in this light as a \u2018shiny new best\u2019 alternative because the truth is rather it is supposed to be something that can be \u2018possibly\u2019 used depending on context. That is why the choice of fixed lambda and common datasets were used to present a more honest view regarding it. The empirical idea regarding the presentation of the numerics was more for showing it is a stable algorithm that works and works relatively okay in the wild on standard datasets with untuned parameters.\n\nPerhaps however, if you feel that a revised version where there are significant differences post-tuning to be shown would be better then we can adjust the numeric section accordingly again with the relevant results.\n\nWith regards to this your insight on the mean is spot on, infact after reviewing the manuscript again now we realised that the unfortunate auto placement of the table makes it seem like it was the main focus that the \u2018mean \u2018is the best of the 4 but it was meant to just be a general observation , that entire paragraph was just a general qualitative discussion which wasn't meant to be brief outline of the table and not meant to be a main driving/selling point of the method. Maybe the choice of word \"attractive\" was also counterintuitive (since we were actually trying to downplay how important it was) with regards to what we were trying say. Because, the real intended take away we wanted for the reader to have was the final point where the loss landscape , which is primarily data dependant can drastically change everything so having an alternative STABLE tool set is key; more so not that this alternative tool is \u2018better\u2019 but rather a new option that could be useful in a practitioners tool kit when the other options are not working well. Would you say maybe we can jettison that specific paragraph of describing the table results in a revised version to maybe drive the point clearer ?\n\nThank you for highlighting the redundance of the MNIST and classification sections. We just placed them for completeness but if it hinders the reading flow we are happy to remove it. (Likewise with regards to redundancy of section 5 which just served as a motivation for why we need numerical methods to solve a continuous function)\n\nWe hope these changes would help to rectify portions of the presentation and soundness aspects.\n\nOtherwise, to get some help with the revision process, could we take some of your time to explain the rationale for you rating of the contribution and soundness, since we feel that this approach yields some novel findings which have been supported by the relevant proofs with the help of numerical tools to validate that it is functioning as expected.\n\nSpecifically, our contribution lies in embedding prior probabilistic knowledge into the regularization process. This is the key idea that we want to share through the paper. To better explain, as we mentioned earlier, the primary goal of the numerical experiments was not to achieve higher accuracy scores, but rather to demonstrate the validity of our proof of concept. The numerics serve to confirm that the approach works as intended, rather than being focused solely on performance metrics. Hence as you observed, sometimes the non-regularize setting works best and to find a measure \"generalisation\" would be a difficult task. The metric we thought was sufficient to show the generalisation instead in numerics was a good enough and stable performance across a diverse range of datasets as presented.\""
            }
        },
        {
            "comment": {
                "value": "Dear Reviewer, thank you for your kind comments.\n\nRegarding the question about motivation, thank you for highlighting this, the idea presented in the remaining motivation was specifically with regards to the regularization target but didn't focus on why the characteristic function was used. The advantage of it comes specifically from the idea that it addresses directly distributional behavior and also calculating convolutions becomes a simple step of point wise algebra. Since this paper was cut down from a larger manuscript we realized the motivation part might have been chopped a bit too aggressively, maybe would you think addressing this would help in the revision? \n\nRegarding the weaknesses mentioned of why not use multiple lambda parameters -> This was a point of contention which we had when trying to distill the numerical results to be presented. From our discussions we decided to fix the parameter instead because it feels very much like p-value hacking just to squeeze the best possible results for a given setting to show where one method shines since it is really data dependant and from what we found, it quite difficult to explain exactly why it works as it is a non convex problem to be explained ;  We do have results where the method obliterates the usual L1,L2 when hidden layers are added and on some specific datasets but I feel would be disingenuous to present it in this light as a \u2018shiny new best\u2019 alternative because the truth is rather it is supposed to be something that can be \u2018possibly\u2019 used depending on context. That is why the choice of fixed lambda and common datasets were used to present a more honest view regarding it. The empirical idea regarding the presentation of the numerics was more for showing it is a stable algorithm that works and works relatively okay in the wild on standard datasets with untuned parameters. \n\nPerhaps however, if you feel that a revised version where there are significant differences post-tuning to be shown would be better then we can adjust the numeric section accordingly again with the relevant results. \n\n\nWith regards to this your insight on the mean is spot on, infact after reviewing the manuscript again now we realised that the unfortunate auto placement of the table makes it seem like it was the main focus that the \u2018mean \u2018is the best of the 4 but it was meant to just be a general observation , that entire paragraph was just a general qualitative discussion which wasn't meant to be brief outline of the table and not meant to be a main driving/selling point of the method. Maybe the choice of word \"attractive\" was also counterintuitive (since we were actually trying to downplay how important it was) with regards to what we were trying say. Because, the real intended take away we wanted for the reader to have was the final point where the loss landscape , which is primarily data dependant can drastically change everything so having an alternative STABLE tool set is key; more so not that this alternative tool is \u2018better\u2019 but rather a new option that could be useful in a practitioners tool kit when the other options are not working well. Would you say maybe we can jettison that specific paragraph of describing the table results in a revised version to maybe drive the point clearer ?\n\nThank you for highlighting the redundance of the MNIST and classification sections. We just placed them for completeness but if it hinders the reading flow we are happy to remove it.  (Likewise with regards to redundancy of section 5 which just served as a motivation for why we need numerical methods to solve a continuous function)\n\nWe hope these changes would help to rectify portions of the presentation and soundness aspects.\n\n\nOtherwise, to get some help with the revision process, could we take some of your time to explain the rationale for you rating of the contribution and soundness, since we feel that this approach yields some novel findings which have been supported by the relevant proofs with the help of numerical tools to validate that it is functioning as expected. \n\nSpecifically, our contribution lies in embedding prior probabilistic knowledge into the regularization process. This is the key idea that we want to share through the paper. To better explain, as we mentioned earlier, the primary goal of the numerical experiments was not to achieve higher accuracy scores, but rather to demonstrate the validity of our proof of concept. The numerics serve to confirm that the approach works as intended, rather than being focused solely on performance metrics. Hence as you observed, sometimes the non-regularize setting works best and to find a measure \"generalisation\" would be a difficult task. The metric we thought was sufficient to show the generalisation instead in numerics was a good enough and stable performance across a diverse range of datasets as presented."
            }
        },
        {
            "summary": {
                "value": "This paper proposes a characteristic function-based regularization method for contextual regularization. They compute the characteristic function for a linear combination of Bernoulli random variables and discretize it for use as a regularization term. They empirically evaluate the proposed regularization approach compared to existing baselines across 5 different classification datasets. Their results demonstrate that integrating this method improves performance on the considered benchmark supervised classification tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper investigates a novel regularization technique based on characteristic functions of datapoints, getting motivation from physics-informed neural networks."
            },
            "weaknesses": {
                "value": "- Some contents of the paper is unnecessary, which greatly diminishes the quality of the paper (e.g., extensive description of MNIST, and classification problem (Sections 2.1-2.2), informal proof sketches of Proposition 2-5 which are not proposed by the paper but already well-known) \n- Gains achieved by the method are weak. The authors state \"It is generally observed that the mean for the regularization we proposed, throughout 4 out of 5 datasets, achieve the highest mean\". However, as shown in Table 1, for those 4 out of 5 datasets, the performance of their method often times just match or is only slightly better (~0.0001) compared to no regularization at all (None column)."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a novel form of regularization which regularizes the model output probabilities towards that of a characteristic function defined over a sum of Bernoulli random variables."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The idea is novel and provides a new route of regularization which has not been considered before. \n- The presentation and derivation is clear, and well done."
            },
            "weaknesses": {
                "value": "- The overall motivation of using characteristic function regularization is not clear. \n- The abstract states \u201cimproves performance \u2026 by preserving essential distributional properties\u2026\u201d -> How does the preservation of such properties aid in generalization?\n- The abstract states that the method is meant to be used in conjunction with existing regularization methods. Were the results presented results utilizing multiple forms of regularization (such as $L_2 + \\psi_2$) or were the only singular forms of regularization?\n- In the conclusion, the author state the follwoing: \u201cintegrating these techniques can offer a probability theory based perspective on model architecture construction which allows assembling relevant regularization mechanisms.\u201d \u2014> I do not see how this can be done after reading the work. can you give a concrete example of how the results presented in this work may give any insight into model architecture construction?\n\n## Overall\n\nWhile I found the work interesting and captivating to read, after finishing the manuscript I am left wondering what possible benefit the regularization provides over existing methods. The results are somewhat ambiguous and I find they do not demonstrate why or when a clear benefit can be achieved by applying the given regularization method. If the authors could provide some insight as to when and why the method would be successful, I think it would go a long way in demonstrating the real-world usefulness of characteristic function regularization. Even if this could be demonstrated in a synthetic toy setting, it could provide interesting insights."
            },
            "questions": {
                "value": "Questions are covered above in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to incorporate probability rules into the neural network architectures, more specifically the central limit theorem. They use a linear model on MNIST as an example and propose to regularize the distance between the characteristic function of the data distribution and the normal distribution."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Incorporating probability rules into the neural network architectures is a good idea. The classification task in machine learning is essentially learning a prediction rule $\\mathbb{P}(Y|X)$. Incorporating probability rules into the model may facilitate the learning of this prediction rule."
            },
            "weaknesses": {
                "value": "- The described model in Sec 2 looks like just neural networks used in common practice. I don't see any novel architecture here.\n- The authors assume the data follows a linear combination of Bernoulli distributions. This does not make sense for practical data. For example, the MNIST data, which is given as an example in Sec 2, is continuous data in $[0, 1]$ and is not Bernoulli. Or does the authors mean to assume the output of the model is Bernoulli?\n- In line 315, the authors claim \"for a general class of PFINNs, one only needs to adjust the modeling of the random variable presented in Definition 3 to reformulate the equation in Proposition 1 accordingly.\" However, for data in practice, it is hard to compute its characteristic function as we do not know its true distribution and the distribution is what the model is trying to learn in some sense. This makes the proposed regularization method invalid. Even if we can compute the characteristic function of data distribution, the regularization is not a function of weight parameters. How do you update the parameters through the regularization?\n- The writing is not professional. The paper spends a lot of space introducing the setup of the model and MNIST dataset. The dataset and network architecture are quite common and can be introduced briefly. For example, the dataset can be represented generally as $\\\\{x_i, y_i \\\\}_{i=1}^N$. Some sentences are not professional in scientific writing, such as, \"To explore the existence of PFINNs as (neuro)symbolic AI and as hybrid systems would require significantly more than 9 pages in a manuscript. (line 052)\" and \"Maybe other potential questions may merit consideration? (line 171)\". I would suggest the authors read some high-quality papers and learn their writing styles."
            },
            "questions": {
                "value": "- What data distribution assumption is used in the experiment to implement the proposed regularization?\n- Where is the assumption 1 used in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes regularization via the characteristic function of datapoints to train networks, similar to physics-informed neural networks. \n\nThey derive the characteristic function for a linear combination of Bernoulli random variables, and discretize this function to use as regularization.\n\nThe perform experiments on a flattened version of MNIST with a linear model with a softmax activation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Proposes a new approach to perform regularization via a discretized version of the characteristic function"
            },
            "weaknesses": {
                "value": "I believe that a large amount of the content in this paper is unnecessary background. For instance, much of section 2 focuses on introducing the dataset of MNIST and the setting of a classification problem \u2013 all of which are standard and used in the field. \n\nFor section 5, the informal proof sketches are describing results that are not all that relevant to the paper and some of which are already known facts. For instance, it is already known that the set of reals are complete. Most of these propositions are not new and follow the content in (https://www.lix.polytechnique.fr/~bournez/load/MPRI/Cours-2024-MPRI-partie-I-goodMPRI.pdf)\n\nIn the experiments, the authors fix $\\lambda$ to 0.01 for all methods, while I believe that this should be tuned for each method individually on held-out validation data. \n\nFurthermore, I have some reservations about the authors' empirical results. There seems to be almost no difference between regularizing with $\\psi_{\\inf}$ and standard training without any regularization. While the authors claim the best mean performance across 4 of the 5 tasks, this roughly equivalent performance with standard training without any regularization makes up 3 of those best-performing tasks. Thus, it\u2019s unclear if this regularization is beneficial in general and just is essentially not performing any regularization."
            },
            "questions": {
                "value": "How is the characteristic function related to the motivations in section 3 (specifically with regards to the infinite series of inquiries about the input image)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}