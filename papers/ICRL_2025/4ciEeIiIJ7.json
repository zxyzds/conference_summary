{
    "id": "4ciEeIiIJ7",
    "title": "Let\u2019s disagree to agree: Evaluating collective disagreement among AI vision systems",
    "abstract": "Recent advancements in artificial intelligence (AI) have led to the development of AI vision systems that closely resemble biological vision in terms of both behavior and neural recordings. While prior research in modeling biological vision has largely concentrated on comparing individual AI systems to a biological counterpart, our study instead investigates the collective behavior of model populations. We focus on inputs that generate the most divergent responses among a diverse population of AI vision systems, as measured by their aggregate disagreement. We would expect that the factors driving disagreement among AI systems are also causes of misalignment between AI systems and human perception. We challenge this expectation by demonstrating alignment between AI systems and humans at the population level, even for images that generate divergent responses among AI systems. This unexpected finding challenges our understanding of the relationship between the limitations of AI systems and human perception, suggesting that even the most challenging stimuli for AI systems are reflective of human perceptual difficulties.",
    "keywords": [
        "deep learning",
        "representational similarity"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "We demonstrate alignment between models and humans at the population level, even for images that generate divergent responses among AI vision systems.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4ciEeIiIJ7",
    "pdf_link": "https://openreview.net/pdf?id=4ciEeIiIJ7",
    "comments": [
        {
            "comment": {
                "value": "> But it is not justified to use this as a basis to say that machines and humans are making mistakes in the same kind of way - it is much more nuanced than that.\n\nCould you clarify what is meant by nuanced here?\n\n> While the assessment in Fig 6 aims to show the proportion of human-annotated top visual attributes, it is unclear on an instance level how and why humans and artificial models reach (dis)agreement. Take for example the cases where the model makes random kinds of predictions humans clearly would not. For example, Figure 3c is clearly not a roof tile, a scorpion, or a sandal - no human would guess any of those, although they could still be wrong of course.\n\nKeep in mind that the percentage of models making those labels is far smaller (6% of models) for low agreement images than high agreement images (100% of models). So the model driven factors that lead to the labels of low agreement amongst models at the individual image level do not have consistency over a population of models by definition of the agreement metric. Based on the agreement metric, the prediction amongst models is very diverse and each model is making a prediction for reasons that are unique to that model amongst the 1032 models."
            }
        },
        {
            "comment": {
                "value": "> For a study comparing human and artificial visual systems, the authors might want to consider the body of literature that draws from neuroscience to better understand how convolutional neural networks (CNNs) could model early visual processing pathways \n\nActually there's a growing body of work that shows that architecture does not play a significant role in modeling visual processing pathways.\n\n\"We find that model scale and architecture have essentially no effect on the alignment with human behavioral responses, whereas the training dataset and objective function both have a much larger impact.\" \\cite{muttenthaler2022human, conwell2024large}\n\nMore recent work has found that many different architectures make very similar predictions \\cite{conwell2024large} and discrimination along the dimension of architecture are not viable \\cite{han2023system}. This was the original motivation for finding the 'disagreeable' stimuli, the variability of predictions amongst different models would help distinguish unique characteristics of potentially related to architecture. But to our surprise, as described in the introduction, the prediction variability among the model group was similar to the variability of human group.\n\n@article{muttenthaler2022human,\n  title={Human alignment of neural network representations},\n  author={Muttenthaler, Lukas and Dippel, Jonas and Linhardt, Lorenz and Vandermeulen, Robert A and Kornblith, Simon},\n  journal={arXiv preprint arXiv:2211.01201},\n  year={2022}\n}\n\n@article{conwell2024large,\n  title={A large-scale examination of inductive biases shaping high-level visual representation in brains and machines},\n  author={Conwell, Colin and Prince, Jacob S and Kay, Kendrick N and Alvarez, George A and Konkle, Talia},\n  journal={Nature Communications},\n  volume={15},\n  number={1},\n  pages={9383},\n  year={2024},\n  publisher={Nature Publishing Group UK London}\n}\n\n@inproceedings{han2023system,\n  title={System identification of neural systems: If we got it right, would we know?},\n  author={Han, Yena and Poggio, Tomaso A and Cheung, Brian},\n  booktitle={International Conference on Machine Learning},\n  pages={12430--12444},\n  year={2023},\n  organization={PMLR}\n}"
            }
        },
        {
            "title": {
                "value": "Thanks for the comments - score not changed."
            },
            "comment": {
                "value": "- The quotes from Geirhos et al. are mainly about strategies. It is fair to say that the quote \"the consistency between CNNs and human observers, however, is little above what can be expected by chance alone\" and fig 1 in Geirhos et al. are about mistakes, not just strategies, and it does raise questions that we observe human-model consistency that seems driven by the image/label rather than random chance. However, that means your paper demonstrates the need for context in Geirhos et al. It does not mean that the consistency you show is nonobvious or a significant contribution - that's a separate question.\n\n- So, let's come to that question and your second response point above. You said something similar to reviewer vUbw - \"humans and machines are potentially challenged in the same way by the same images. We think this is not obvious at the population level because all the models were individually trained/fine-tuned on the same labels, so there's no ambiguity in their training, but ambiguity and disagreement nonetheless arises. And that ambiguity and disagreement appears to be aligned with populations of humans.\" And to me, \"Although the labels come from humans, the labels this model population sees are unanimous amongst all models. Therefore, we see that despite all models being trained to provide the same labeling over the training set, they disagree on held out images in a way that is similar to human populations.\"\n  - You're right to say that the models were trained on the same labels for each image and that takes away one source of ambiguity. \n  - However, my point is that when it comes to ambiguous images, you'll have groups of images in the training dataset that contain similar features (along with some different ones), but have different labels, and groups that have the same labels but different features (along with some similar ones). That is another source of ambiguity, so \"all the models were individually trained/fine-tuned on the same labels, so there's no ambiguity in their training\" seems false. \n  - Not only is this a source of ambiguity, it's a well-known one. And not only is it well-known, I think it's the one driving your results. \n\nI like the idea of investigating populations and your approach to experimentation. I also think the paper is well-written and visualized. However, I don't think you've found something nonobvious yet, and would encourage you to keep investigating. I agree with reviewer vUbw that more careful interpretation of similarity is necessary. I'll maintain my score."
            }
        },
        {
            "comment": {
                "value": "We value your thorough review and we're responding as timely as possible because the points you have made are important and we would like to discuss further.\n\n> **It should not be surprising that both humans and machines have difficulty in correctly identifying the target class in these cases. But it is not justified to use this as a basis to say that machines and humans are making mistakes in the same kind of way - it is much more nuanced than that.**\n\nWe agree with this point. We are not saying that humans are making mistakes in the same kind of way, but humans and machines are potentially challenged in the same way by the same images. We think this is not obvious at the population level because all the models were individually trained/fine-tuned on the same labels, so there's no ambiguity in their training, but ambiguity and disagreement nonetheless arises. And that ambiguity and disagreement appears to be aligned with populations of humans.\n\n> **My first concern is related to the assumption from which the paper starts (L19) about the \u201c factors driving disagreement among AI systems are also causing misalignment between AI systems and humans perception\u201d - why would that be the case?**\n\nWhile the actual notion of disagreement among a population of models has not been measured before our submission, it has been an explicitly stated assumption that the mistakes that AI models make are distinct from the mistakes that humans make. For instance, [Geirhos et al. (NeurIPS 2020)](https://arxiv.org/abs/2006.16736) make the points:\n\n\"The consistency between CNNs and human observers, however, is little above what can be expected by chance alone\u2014indicating that humans and CNNs are likely implementing very different strategies.\"\n\n\"We conclude that there is a substantial algorithmic difference between human observers and the investigated sixteen CNNs: humans and CNNs are very likely implementing different strategies.\"\n\n\u201cCohen\u2019s $\\kappa$ for CNN-human consistency is very low for both models (`.068` for ResNet-50; `066` for CORnetS) compared to `.331` for human-human consistency.\u201d\n\nFurthermore, [Geirhos et al. (NeurIPS 2018)](https://arxiv.org/abs/1808.08750) make the point:\n\n\u201cAdditionally, we find progressively diverging patterns of classification errors between humans and DNNs with weaker signals.\u201d"
            },
            "title": {
                "value": "Discussion of some key concerns"
            }
        },
        {
            "title": {
                "value": "Also thank you for the review"
            },
            "comment": {
                "value": "We're responding to the reviews as timely as possible because the points you have made are important and we would like to discuss further."
            }
        },
        {
            "title": {
                "value": "Discussion of some key concerns"
            },
            "comment": {
                "value": "> **Motivation isn't that convincing**\n\nWhile the actual notion of disagreement among a population of models has not been measured before our submission, it has been an explicitly stated assumption that the mistakes that AI models make are distinct from the mistakes that humans make. For instance, [Geirhos et al. (NeurIPS 2020)](https://arxiv.org/abs/2006.16736) make the points:\n\n\"The consistency between CNNs and human observers, however, is little above what can be expected by chance alone\u2014indicating that humans and CNNs are likely implementing very different strategies.\"\n\n\"We conclude that there is a substantial algorithmic difference between human observers and the investigated sixteen CNNs: humans and CNNs are very likely implementing different strategies.\"\n\n\u201cCohen\u2019s $\\kappa$ for CNN-human consistency is very low for both models (`.068` for ResNet-50; `066` for CORnetS) compared to `.331` for human-human consistency.\u201d\n\nFurthermore, [Geirhos et al. (NeurIPS 2018)](https://arxiv.org/abs/1808.08750) make the point:\n\n\u201cAdditionally, we find progressively diverging patterns of classification errors between humans and DNNs with weaker signals.\u201d\n\n> **the training data is also a product of human reaction to ambiguity**\n\nAlthough the labels come from humans, the labels this model population sees are unanimous amongst all models. Therefore, we see that despite all models being trained to provide the same labeling over the training set, they disagree on held out images in a way that is similar to human populations."
            }
        },
        {
            "summary": {
                "value": "The paper compares the collective behaviour of 1,032 AI vision systems with 42 humans in annotating images, investigating how various visual factors influence agreement levels. It highlights that images that are challenging for the AI systems often pose similar difficulties for humans. The paper suggests that there is an alignment in visual complexity across both groups. The study quantifies (dis)agreement among AI systems and compares the results with human annotations. Additional factors such as difficulty score, minimum viewing time, and specific visual properties are examined. This approach offers insights into common challenges shared by AI and human perception."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The comparison between model performance and human annotations is interesting and insightful."
            },
            "weaknesses": {
                "value": "- The paper is difficult to follow\n- The motivation and contributions of the paper is not clear\n- The paper lacks novelty, as it mainly consists of a comparison between the performance of machine learning models and human annotators. Reader may expect a novel methodology to be derived from these analyses.\n- The paper lacks a discussion about the limitations and potential directions for future work"
            },
            "questions": {
                "value": "- It is unclear why the authors concluded from Figure 1 alone that the stimuli causing the most agreement/disagreement among AI systems also cause the most agreement/disagreement among humans. Although the figure shows the agreement levels, it lacks specific information on the stimuli that contributed to such obtained outcomes\n- In Table 1, what is the motivation behind comparing the models agreement with the human viewing time and the difficulty score?\n- It is unclear why the authors concluded from Table 1 that ObjectNet is more challenging for both humans and the models?\n- I would recommend to provide a correlation measure for Figure 5.\n- Do you expect any bias in human annotations?\n- In Figure 6, How did you determine the visual factors for the models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper assesses the disagreement among a population of artificial vision systems (1032 models) and compares it with the disagreement among a population of humans (42 human participants). Unlike previous works, populations of agents and humans are compared on a collective level, instead of an individual level. The paper aims to prove that factors that cause disagreement among AI systems coincide with the factors that cause human disagreement, at a population level."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper has the following (potentially) strong points: \n\n1. The paper assesses the overlap between AI vision models and human disagreement on a collective/population level, rather than an individual level. This is an original approach as far as I know. The assumption is that by identifying patterns in how populations of AI models fail similarly to humans, training methods or architectures that handle difficult stimuli could be developed, and thus improve model robustness and interpretability. The proposed many-to-many comparison is something worth considering in the future, alongside already-established measures.\n\n2. This study models the largest population (afaik) of artificial vision models, spanning 1032 AI models with various architectures, pretraining regimes and data. Such a population should provide a comprehensive view of collective disagreement. However, how each of these models influences the collective disagreement is not discussed enough, but could have been a point to add more value to the paper.\n\n3. It aims to uncover and highlight common factors between humans and artificial models of vision that cause difficulty in object recognition."
            },
            "weaknesses": {
                "value": "This work presents the following weaknesses:\n\n1. My first concern is related to the assumption from which the paper starts (L19) about the \u201c factors driving disagreement among AI systems are also causing misalignment between AI systems and humans perception\u201d - why would that be the case?  It states that the current study challenges (L484) \u201cthe assumption present in prior work that disagreement among AI systems is unrelated to human visual processing\u201d. But this assumption (L484) is not adequately founded, or at least not supported through the references provided which do not claim that disagreement between artificial models is unrelated to human visual processing. To reinforce, the initial assumption is not adequately discussed or supported by the correct references making it difficult to understand the motivation of the paper in the first place. \n\n\n2. For a study comparing human and artificial visual systems, the authors might want to consider the body of literature that draws from neuroscience to better understand how convolutional neural networks (CNNs) could model early visual processing pathways [e.g. A Unified Theory of Early Visual Representations from Retina to Cortex (Lindsey et al., 2019); Spatial and Colour Opponency in Anatomically Constrained Deep Networks (Harris et al. , 2019)]. Such works aim to understand the similarities between human visual systems and artificial models at the lower level of neurons and how the functional and structural layouts of biological visual systems could better inform DNN architectures.\n\n3. While the idea of comparing many to many is interesting and could add value on top of accuracy and one-to-one error consistency measures, the experimental setup seems to be (visually) ill-posed. For instance, the challenging examples are complex scenes, e.g. Figure 12, in which the label corresponds to just one small part of the scene. It should not be surprising that both humans and machines have difficulty in correctly identifying the target class in these cases. But it is not justified to use this as a basis to say that machines and humans are making mistakes in the same kind of way - it is much more nuanced than that. \n\n4. While the assessment in Fig 6 aims to show the proportion of human-annotated top visual attributes, it is unclear on an instance level how and why humans and artificial models reach (dis)agreement. Take for example the cases where the model makes random kinds of predictions humans clearly would not. For example, Figure 3c is clearly not a roof tile, a scorpion, or a sandal - no human would guess any of those, although they could still be wrong of course."
            },
            "questions": {
                "value": "In light of the previous comments, I think the main actionable points are:\n- the motivation of the paper needs to be reconsidered and clarified\n- so does the conclusion and interpretation of results, in particular, I would recommend more carefully interpreting the similarities between humans and artificial models.\n\nFurther clarification is also needed on:\n- Figure 1 -  the interpretation of the histograms for model and human agreement (\u201chistograms along each axis reflect the proportion of images at each marginal agreement level\u201d).  The caption states there is a positive correlation but does not state how this conclusion is reached.  Later on, Table 1 provides some values but the exact method for reaching those values is missing. Visually the histograms do not seem positively correlated, but again clarifying in text would be better.\n\n- Details of the pretraining of each model, or at least grouped per family of models (maybe grouped by architecture type) used in this analysis would have been relevant. Also, further discussion and interpretation of results, again grouped per family of models could have added value to this paper. For example, how do different model architectures contribute to the level of disagreement? \n\n- Again, for clarity,  it would be good to state clearly how the values for correlation between model agreement and the human behavioural measures (Table 1) are computed. \n\n- Line 432 - What is this subset of selected models? Based on what criteria were these models selected? \n\n- Regarding low-agreement images, it would be interesting to assess the factors that cause disagreement at certain levels of accuracy. Are these factors maintained, and what factors remain/are discarded as the acceleration of agreement occurs (as per L440-442)?\n\nFinally, I think a section on the limitations of this study should be included. For example:\n- the limited number of human participants might not reflect the full spectrum of human visual perception\n- how does approximating perceptual abilities to population disagreement lead to overlooking specific, individual visual factors?\n- is Fleiss\u2019 Kappa the most suitable measure and are there any other agreement measures that could be explored instead?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper attempts to establish similarity between artificial and biological vision by showing that populations of AI models and populations of humans show intra-group disagreement on the same stimuli. It motivates itself by claiming that prior work shows disagreement among models being a function of limitations in their development, rather than expressions of an underlying mechanism in both AI and human vision. \n\nThe paper defines agreement as Fleiss' $\\kappa$ for an image, calculated over a population of vision systems. It surveys ~40 humans and ~1000 models, trying CNNs, ViTs, and hybrids and varying model size, dataset size, and training methods (pretraining and finetuning). It also uses human minimum viewing time and difficulty score as comparison metrics. \n\nResults show:\n- All metrics appear to correlate with model agreement in intuitive ways - not strong correlations, but significant and all in the intuitive direction\n- The clearest relationship is for low-difficulty high-model agreement images \nThe paper takes human-annotated visual attributes from the ImageNet-X dataset, in which humans annotated what aspects of an image make it difficult to classify. The paper showed that for both low-human agreement and low-model agreement images, the percent of images with each top difficulty factor shows similar relative influence - the percentage of images for each factor decreases in mostly the same order for both humans and models. The most influential factors are found to be background, pose, color, pattern, and \"smaller\". \n\nThe paper also shows that model agreement increases as accuracy increases. \n\nThe paper then positions itself against other error analysis-related works, works that use synthetic stimuli to assess differences, and metamers (this being an opposite of a metamer)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "### Quality \n- Good problem setup: well-defined, statistical choices make sense, and experiences overall make sense (I will list a couple exceptions in the weaknesses) \n- Good application of ImageNet-X to get systematic error analysis on naturalistic images \n- Comparing to a population of models seems promising\n\n### Clarity\n- Writing style is very clear. I rarely felt confused when reading the paper, and the structure made sense.\n- Figures are well-designed. They are the most useful aspect for building intuition about the results - they look good, and show the right concepts. \n- Explanation of Fleiss' $\\kappa$ helps build intuition for what \"agreement\" means, and also helps strengthen the experimental design choices"
            },
            "weaknesses": {
                "value": "### Quality\n#### Problem\n- Motivation isn't that convincing - the paper claims that the typical assumption around model errors is \"intrinsic to these systems and unrelated to aspects of human visual processing.\" But that isn't always the case - I think ambiguous images (which seem to be the crux of this paper) are not only known to be difficult for models just as they are difficult for humans, but are easily cited by most researchers as a cause of model error and likely disagreement\n  - The paper also claims evidence that \"disagreement among AI vision systems is driven by aspects of human visual perception, particularly image difficulty\" - it's worth nothing that classifications are a human concept, not an inherent property of the image, and training data reflects that. Maybe the paper isn't directly making this claim, but it seems that it's suggesting there are similar mechanisms between models (at least model populations) and humans that drive disagreement; I'd argue that these images are simply actually ambiguous, the classification is a product of human reaction to ambiguity, the training data is also a product of human reaction to ambiguity, and the model directly encodes that rather than showing an interesting emergent behavior. \n- Data on variations of models is limited to a list in the appendix - would be good to be given a structured representation of the variations in a table\n\n#### Results\n- Though the correlation coefficients are nontrivial and the figures line up with them, and I wouldn't expect strong correlations for such a high-dimensional problem, the figures do show a lot of spread. \n- This also make the results seem less surprising - from both this and figure 6, where we see the factors being \"background\",\"pose\", \"color\", \"pattern\", and \"smaller\", it seems that the difficult images are simply truly ambiguous. It's not a matter of ML fallibility, but I wouldn't expect it to be. It's also not an underlying surprising mechanism in human vision that makes humans fallible on them. The images are ambiguous and the humans who labeled them probably weren't completely sure what to label them. Even if we call it a shared mechanism/underlying principle of human vision, it's not surprising or unknown. \n- It makes sense that agreement increases as overall accuracy increases, but this is really not surprising. It could be that there are cases where models all classify the image as the same wrong class, but just given how training works, it's likely the original image is misclassified (or the original assumption is true). In either case, this doesn't offer an alternative to an explanation to the original assumption. \n\n### Clarity\n- Would help to have an explanation of why Fleiss' $\\kappa$ is a good measure of agreement, really just intuition on how it works. \n- Sections 3.1 and 3.2 don't need to be there - they explain concepts that are immediately clear from the figures. \n- More descriptive statistics on the figures would help understand how predictive the results are. \n\n### Originality and significance\n- I haven't seen this framing of this problem. However, the concept itself - that ambiguous images are difficult for both humans and models - doesn't seem novel. It also doesn't seem to warrant this much formalization."
            },
            "questions": {
                "value": "- I am curious how these experiments would fare for top-5 classification - possibly for humans, not just models \n- In figure 6, how should we factor in the difference in proportions between models and humans, even if the order of proportions is mostly the same? I realize you're not making this claim, but if we want to establish similar underlying mechanisms, we'd need to deal with the differences in proportion for each factor. What might this imply for future studies? \n- \"Images at low agreement levels are produce significantly lower Fleiss' $\\kappa$ than high agreement and all images, even for models at high performance levels\" - I thought that agreement is *defined* as Fleiss' $\\kappa$. Am I misinterpreting? Is the point that even when models are split and Fleiss' $\\kappa$ is recalculated, it is low for the images that had low Fleiss' $\\kappa$ across all models? That would be more meaningful, though continues to point to images that are simply ambiguous."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper brings a new point from population-level comparisons between AI and human vision systems, different from the previous individual Ai and human comparison. The authors conduct experiments using a large population of 1032 models and a previous user study with 42 human participants. They use Fleiss' kappa to quantify the level of agreement and find out a few interesting points on the correlation between AI model (dis)agreement and human (dis)agreement. They claim that the low agreement on hard images is due to intrinsic perceptual challenges shared by both AI and humans instead of model structure limitations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The strengths:\n\n- brings a novel view from population-level comparison of AI and human on vision systems. \n\n- conduct extensive experiments on a large population AI models\n\n- Interesting findings on AI models not perform well on difficult images due to perceptual challenges that human faces as well."
            },
            "weaknesses": {
                "value": "Weaknesses of this paper include:\n\n- Some findings are quite intuitive, for example, the correlation between AI (dis)agreement and human (dis)agreement. This probably is due to the labels are created by humans. \n\n- 42 participants from user study might be a bit bias. May conduct a few more user studies and combine with previous data.\n\n- The image style does not look very good, some images are taking too many spaces but contain relatively few contents.\n\n- at line 402, \"Images at low agreement levels are produce...\", should be \"... are producing...\""
            },
            "questions": {
                "value": "- In Fig 1, it is a bit surprising that there are very few images with high human agreement from the top histogram, which means humans rarely have full agreement on images. Could you explain possible reasons behind this?\n\n- If humans and AI cannot recognize the difficult images or the edge-case images, it means vision alone cannot solve the problem and we probably do not have a better solution using only vision. What other benefits could it bring to us if we study more on the difficult images? In other words, how does studying the edge-case images help?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates correlations between populations of humans and object-recognition systems on object-classification disagreements.  The results show that there is significant correlation between human and model population disagreements, as well as between human minimum viewing time and model disagreements.  The results support the hypothesis that this correlation is driven by aspects of human visual perception that makes certain aspects of images difficult to classify."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The experiments seem solid and the results are well-presented.  The authors tested over 1,000 different models, including CNNs, ViTs, and hybrid models.  The paper goes more deeply than just giving correlation statistics, and investigates what features low-agreement images have in common."
            },
            "weaknesses": {
                "value": "I'm not sure how useful these results are, either for understanding human or machine vision, or for improving machine vision systems.  A useful result would point in a new direction for experiments (to better understand underlying mechanisms) and/or architectural improvements.  But what are the next steps with these results?  The authors did not address this or make the case that these results are important for the field.  \n\nThe paper states: \"In this work, we challenge the assumption that disagreement among AI systems is intrinsic to these systems and unrelated to aspects of human visual processing\".  But what are the citations for this assumption? \n\nI didn't understand, in second paragraph, how this assumption \"aligns with standard approachs for comparing internal representations of AI and biological vision, such as representational similarity analysis\" or how it is \"explicit in behavioral extrapolation tests\" -- this needs better explanation."
            },
            "questions": {
                "value": "The paper states: \"AI systems might be more sensitive to background variations than humans and human population are more likely to disagree when pattern variations are present\".  Explain what \"pattern\" refers to here.\n\nWhen giving models' accuracy on ImageNet and ObjectNet datasets, are you using top-5 or top-1 accuracy?  What about for humans?\n\nFigure 7: What is \"Bin Mean Accuracy\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The article explores the disagreement behaviors of AI vision systems, diverging from traditional approaches that compare individual AI models to biological vision. Instead, this study investigates patterns of agreement and disagreement among a diverse population of AI models by measuring \"aggregate disagreement\" across model outputs. It aims to determine which inputs produce the most divergent responses among models and assesses whether these inputs also create discrepancies between AI systems and human perception.\nA significant finding is that even images causing high disagreement among AI models often align with human perceptual challenges. This alignment suggests that the limitations in AI models mirror similar perceptual difficulties in humans, offering valuable insights into AI-human vision comparisons at a population level. This work contributes to the field by reframing disagreement not as an intrinsic limitation of AI systems but as an opportunity to study the shared perceptual challenges between artificial and human vision systems."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.Innovative Research Topic:\nThe authors investigate an intriguing and novel research area by examining AI model and human visual disagreements at a population level. This approach is unique in that it moves beyond individual model comparisons to analyze the collective behavior of AI vision systems.\n2.New Method for Measuring Human-AI Discrepancy:\nBy introducing a method to measure disagreement at the population level, the study provides a new way to quantify the difference between AI models and human perception, adding a meaningful metric to the field.\n3.Focus on Naturalistic Stimuli:\nUnlike prior work that often uses synthetic stimuli, this study investigates the properties of naturalistic stimuli that elicit the most disagreement among AI models, making its findings more applicable to real-world scenarios.\n4.Insights into AI-Human Perceptual Alignment:\nThe article provides evidence suggesting that disagreements among AI systems are influenced by aspects of human visual perception, particularly in image difficulty, as measured by human behavioral data. This insight supports the idea that individual differences in AI vision systems may reflect differences in human visual processing rather than inherent AI limitations."
            },
            "weaknesses": {
                "value": "1.Limited Analysis of Outlier Cases:\nThe authors report correlations between model agreement and human behavioral measures, but they do not analyze specific cases where model agreement is high but human difficulty is low, or vice versa. Such an analysis could provide deeper insights into unique points of divergence.\n2.Lack of Architecture-Specific Insights:\nAlthough multiple model architectures are included in the study, the authors do not analyze how different architectures impact the results. This oversight limits the understanding of how architectural variations might contribute to AI-human agreement or disagreement on challenging stimuli.\n3.No Exploration of Methods to Reduce Disagreement:\nWhile the study highlights greater disagreement on images of higher human difficulty, it does not explore whether certain methods, such as targeted model adjustments or expanded training datasets, could reduce this disagreement and improve alignment with human perception.\n4.Insufficient Citations of Related Work on AI-Human Disagreement:\nPrior research has shown that there are links between AI-human disagreement and human visual processing at the individual model level, yet the authors do not reference these foundational works. Including these citations could strengthen their arguments by situating the study within the existing body of research."
            },
            "questions": {
                "value": "1.Did the authors consider analyzing cases where model agreement is high but human difficulty is low, or where model agreement is low but human difficulty is high? Such cases might offer valuable insights into the nuanced differences between AI model behavior and human perception.\n2.Although multiple architectures were included, why did the authors not explore the impact of different architectures on the experimental results?\n3.Can the higher disagreement on challenging human images be reduced through specific adjustments to models or training datasets?\n4.Previous research has shown links between AI-human disagreement and human visual processing at the individual model level. Why were these relevant studies not carefully discussed in the related work section?\n\nIf the authors can address these issues, I would be happy to raise my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}