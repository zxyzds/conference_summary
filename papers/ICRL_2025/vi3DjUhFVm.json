{
    "id": "vi3DjUhFVm",
    "title": "Alignment without Over-optimization: Training-Free Solution for Diffusion Models",
    "abstract": "Diffusion models excel in generative tasks, but aligning them with specific objec-\ntives while maintaining their versatility remains challenging. Existing fine-tuning\nmethods often suffer from reward over-optimization, while approximate guidance\napproaches fail to effectively optimize target rewards. Addressing these limita-\ntions, we propose a training-free sampling method based on Sequential Monte\nCarlo (SMC) to sample from the reward-aligned target distribution. Our approach,\ntailored for diffusion sampling and incorporating tempering techniques, achieves\ncomparable or superior target rewards to fine-tuning methods while preserving\ndiversity and cross-reward generalization. We demonstrate its effectiveness in\nsingle-reward optimization, multi-objective scenarios, and online black-box opti-\nmization. This work offers a robust solution for aligning diffusion models with\ndiverse downstream objectives without compromising their general capabilities.",
    "keywords": [
        "diffusion models",
        "alignment",
        "reward over-optimization",
        "sequential monte carlo samplers"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vi3DjUhFVm",
    "pdf_link": "https://openreview.net/pdf?id=vi3DjUhFVm",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes DAS, a training-free approach for aligning diffusion models with specific objectives. It uses Sequential Monte Carlo (SMC) with tempering for reward alignment. This method is demonstrated across generative tasks, including single-reward and multi-objective cases, with performance comparable to fine-tuning methods in terms of target reward optimization and diversity."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. DAS does not require additional training, which reduces computational cost.\n2.The use of SMC with tempering is justified through asymptotic properties.\n3. DAS balances reward optimization and diversity, and is demonstrated across single-reward, multi-objective, and online settings."
            },
            "weaknesses": {
                "value": "1. While DAS is compared with fine-tuning and guidance methods, comparisons to baselines like STEGANODE or controlled diffusion could have strengthened the evaluation.\n2. DAS assumes differentiable reward functions, which may limit applicability in scenarios involving non-differentiable objectives.\n3. Most experiments use Stable Diffusion v1.5, and additional models would have enhanced the generality of the findings.\n4. The paper can do more image tasks. Currently it emphasizes findings on aesthetic score, which might not generalize well to other tasks.\n5. Limitations: the setup of SMC with tempering, intermediate targets, and backward kernels can be technically demanding. And the effectiveness of DAS depends on the pre-trained model's quality, limiting performance on models with low initial diversity or reward alignment."
            },
            "questions": {
                "value": "1. The method relies on specific tempering schemes and parameters, and the practical guidelines for selecting these could be more detailed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a training-free diffusion sampling method based on Sequential Monte Carlo (SMC) to sample from the reward-aligned target distribution. By incorporating tempering techniques, it offers a robust solution for aligning diffusion models with arbitrary rewards\nwhile preserving general capabilities"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper is overall well-written and the motivation is clear. It aims to address the trade-off in diffusion models that align them with specific objectives while maintaining their versatility, which is a critical problem in generative modeling.\n\n2. DAS\u2019s effectiveness is comprehensively validated across diverse scenarios, including toy distribution simulation, single-reward, multi-objective, and online black-box optimization tasks."
            },
            "weaknesses": {
                "value": "1. More intuitive explanations of SMC are suggested to add between the motivation and method to make it more consistent and intuitive since the introduction of SMC in supplementary material is a bit abstruse to understand, making the superiority of adopting SMC to address the training problem unclear.\n\n2. How to choose hyperparameters such as $\\gamma, \\alpha$ and particles should be discussed across different scenarios."
            },
            "questions": {
                "value": "Please see the weaknesses part above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel, training-free sampling method aimed at generating samples from a target distribution, with a specific focus on applications in Reinforcement Learning from Human Feedback (RLHF). The authors compare their approach against existing fine-tuning baselines and guidance techniques."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The introduction provides a clear overview of the problem.\n\n- The proposed method appears promising and might be innovative (see Question 5.)"
            },
            "weaknesses": {
                "value": "- The choice of finetuning-based RLHF baselines may not be appropriate (see Question 1).\n\n- The paper is sometimes hard to follow due to the delayed definition of new notations. For instance, the symbol $\\gamma$ is used on line 208 but is not defined until line 250.\n\n- The evaluation metrics used in the paper (line 355 and onward) are not explained, making it difficult to assess their relevance and meaning."
            },
            "questions": {
                "value": "1. Baselines for Comparison: The authors correctly state that RLHF can be formulated as learning to sample from an unnormalized target distribution (Section 3.2). They show that current fine-tuning approaches in RLHF struggle to sample from multimodal target distributions, which highlights the limitations of these methods. However, this comparison may not be sufficient. There is significant research on using diffusion methods for sampling from multimodal distributions which would not fail at the examples presented in Figure 1 (e.g., [1], [2], [3] for continuous-time models, and [4] for discrete-time models). Including these approaches would provide a more convincing set of baselines. If this is not possible within the rebuttal phase's timeline, I believe it is necessary to at least mention this line of work in the paper. \n\n2. Clarification on Calculations: The calculation presented on line 153 and the following lines is unclear. Can you provide a detailed derivation?\n\n3. Explanation of Evaluation Metrics: The evaluation metrics mentioned in line 355 and onward lack a clear explanation. Currently, understanding them requires consulting multiple references. Could you include a brief explanation in the paper for clarity?\n\n4. Inference Time Comparison: How does the inference time of your method compare with fine-tuning techniques? It seems plausible that fine-tuning methods might produce samples more quickly. Is this the case?\n\n5. Novelty of the Method: Is the proposed method entirely new, or is it simply novel in the context of RLHF? How does it compare to other Sequential Monte Carlo methods?\n\nI will initially give a score of 3, but I am willing to update my score if my questions are properly addressed.\n\n# References\n[1] Zhang, Qinsheng, and Yongxin Chen. \"Path Integral Sampler: A Stochastic Control Approach For Sampling.\" International Conference on Learning Representations.\n\n[2] Berner, Julius, Lorenz Richter, and Karen Ullrich. \"An Optimal Control Perspective on Diffusion-Based Generative Modeling.\" Transactions on Machine Learning Research.\n\n[3] Vargas, Francisco, Will Sussman Grathwohl, and Arnaud Doucet. \"Denoising Diffusion Samplers.\" Eleventh International Conference on Learning Representations.\n\n[4] Sanokowski, Sebastian, Sepp Hochreiter, and Sebastian Lehner. \"A Diffusion Model Framework for Unsupervised Neural Combinatorial Optimization.\" Forty-First International Conference on Machine Learning.\n\n[5] Dongjun Kim, Yeongmin Kim, Se Jung Kwon, Wanmo Kang, Il-Chul Moon Proceedings of the 40th International Conference on Machine Learning, PMLR 202:16567-16598, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose a training-free sampling method based on Sequential Monte Carlo (SMC)  to align the diffusion models with specific objectives. Specially, Diffusion Alignment as Sampling (DAS) is designed to address the  limitations of the previous alignment approaches include fine-tuning and guidance methods. The author also provide theoretical analysis of DAS\u2019s asymptotic properties and empirically validate DAS\u2019s effectiveness across different tasks. Meanwhile, the authors conducte sufficient experiments to verify the validity of the DAS methodology."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is clearly written and there is a good discussion of the work involved. Based on the fact that the existing fine-tuning methods lead to the reward overoptimization problem while the guidance methods lead to the under-optimization problem, the authors propose the DAS method to alleviate these deficiencies. In addition, the authors provide a theoretical analysis of the method and give the relevant code, making the work very solid. Figure 1 illustrates the shortcomings of the existing methods as well as the advantages of the proposed method, and the experimental results are visualized by using an example of a mixed Gaussian distribution."
            },
            "weaknesses": {
                "value": "+ The models underlying the experiments in this paper have some weaknesses, and the Stable Diffusion (SD) v1.5 model is somewhat outdated now. The Consistency model [1] and Flow model (SD3) [2]  are widely used nowadays, so I suggest the authors to conduct some experiments on the newer model so as to further illustrate the validity of the proposed method.\n\n[1] Consistency models ICML-2024\n\n[2] Scaling Rectified Flow Transformers for High-Resolution Image Synthesis ICML-2024\n\n+ In addition to mixing Gaussian distributions, **Swiss rolls** are also commonly used to visualize whether a distribution has been learned or not, and due to their structural features, which can further reflect the model's ability to fit the distribution, the authors can give some visualizations that further illustrate the strengths of the proposed method."
            },
            "questions": {
                "value": "Please refer to Weaknesses. I will also refer to other reviewers' comments"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}