{
    "id": "ehlZFDxwJo",
    "title": "PERSONALIZED FEDERATED PARTIAL LABEL LEARNING",
    "abstract": "Partial Label Learning (PLL) is known as a valuable learning technique that trains Machine Learning (ML) models on partial label datasets, where the ground truth label is concealed within the candidate label set of each data instance. It learns label correlation based on a single centralized dataset to predict the latent true label. When data is non-independent and identically distributed (non-i.i.d.) among workers in Federated Learning (FL), the label correlation interference problem occurs. To address the issue, in this paper, we propose pFedPLL, a personalized federated partial label learning algorithm with two new designs. In Label Correlation Isolation (LCI), we first develop a twin-module architecture, where a feature-level correlation matrix layer for each worker is isolated locally to prevent it from being interfered with by others. In Label Correlation Personalization (LCP), we then propose a bi-directional calibration loss to identify a more accurate learning direction, where the positive calibration aligns the prediction result with the latent true label, and the negative calibration pushes away the prediction result that falls into the non-candidate label set. We provide a convergence analysis of pFedPLL with a rate of $O\\left(\\sqrt{\\frac{1}{T}}\\right)$ for smooth non-convex problems. Experiment results demonstrate that pFedPLL outperforms SOTA federated PLL algorithms and the federated version of centralized PLL algorithms across nine datasets.",
    "keywords": [
        "Federated learning",
        "Partial label learning",
        "Weakly supervised learning"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "This paper proposes the pFedPLL algorithm, which aims to address the Partial Label Learning (PLL) problem in a Federated Learning (FL) environment.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ehlZFDxwJo",
    "pdf_link": "https://openreview.net/pdf?id=ehlZFDxwJo",
    "comments": [
        {
            "summary": {
                "value": "This paper proposed a personalized FL to address the label correlation interference problem in partial label learning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper addresses a weakly supervised problem in federated learning where each client has access only to a candidate label set for each instance. This problem formulation aligns more closely with real-world FL applications, potentially reducing the burden of collecting \n a large amount of high quality data instances."
            },
            "weaknesses": {
                "value": "1. The problem of partial labeling in federated learning is indeed interesting. However, the motivation for introducing label correlation interference in heterogeneous FL settings is somewhat unclear. In particular, it's not immediately evident why instances on different clients might be labeled with varying candidate label sets. To strengthen the rationale behind this assumption, it would be helpful for the authors to provide concrete examples from real-world applications where such label correlation interference is likely to occur. \n\n2. The proposed solution seems not novel to me. As the parameter personalization has been widely explored in traditional FL research such as FedRep, FedPer, and PartialFed. More specifically, I do not see much differences between the proposed  method and FedPer or FedRep, as the label correlation matrix layer is a linear layer that can be directly merged into the classifier.\n\n3. Equations (6) and (7) appear somewhat unusual. It seems to me that \\alpha is directly proportional to \\wave{y}, so what is the difference between Eq. 7 and the traditional cross-entropy loss?\n\n4. What is the relationship between the convergence analysis and partially labeling? How does the partially labeling affect the convergence speed?\n\n5. The datasets used in the experiments are overly simple; the authors should consider evaluating their approach on larger-scale datasets, such as Tiny-ImageNet, to better demonstrate its effectiveness.\n\n6. In the experiments, the method for creating datasets with varying label correlations across clients is unclear. It would be helpful for the authors to clarify how they generated datasets with different label correlation structures.\n\n7. In Fig. 2 (b)\uff0c why does the training using only the negative  calibration unconverged but plus the other losses can make it converge. \n\n[1] Federated learning with personalization layers, 2019.\n[2] Exploiting shared representations for personalized federated learning, ICML 2021.\n[3] Partialfed: Cross-domain personalized federated learning via partial initialization, Neurips 2021."
            },
            "questions": {
                "value": "Please refer to the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes pFedPLL (personalized federated partial label learning) to address label correlation interference in federated learning (FL) with non-i.i.d. partial label datasets. The method introduces two core mechanisms:\n\n- Label Correlation Isolation (LCI): It isolates each worker\u2019s label correlation matrix to prevent interference during model aggregation.\n- Label Correlation Personalization (LCP): It incorporates a bi-directional calibration loss to guide learning toward the true label within the candidate label set and away from non-candidate labels."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper addresses a critical issue in federated partial label learning \u2014 the label correlation interference caused by data heterogeneity. The twin-module architecture with LCI is a unique approach, and LCP\u2019s bi-directional calibration is an innovative way to leverage partial label data.\n\n2. The method is well-structured with clear explanations of the proposed mechanisms, algorithms, and theoretical insights. Visual aids effectively illustrate the twin-module architecture and triplet loss setup."
            },
            "weaknesses": {
                "value": "1. The novelty of the proposed method is not clear. For example, sharing representation layers and constructing a correlation matrix and the final linear layer are standard methods. The novelty of the proposed positive calibration and negative calibration is also not clear.\n2. The optimality of using the proposed KL scores is not clear.\n3. Although pFedPLL performs well on relatively small datasets, the paper does not discuss potential scalability issues when dealing with significantly larger models or a larger number of clients. Expanding on computational and memory overhead in highly distributed settings would strengthen the practical applicability."
            },
            "questions": {
                "value": "Please address the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed a personalized federated learning algorithm for the label correlation interference problem \nin federated partial label learning with non-i.i.d. data among clients,\nand proved that the algorithm achieves a convergence rate of $O(1/\\sqrt{T})$ for smooth non-convex loss functions.\nThe proposed algorithm contains a new twin-module architecture which keeps the label correlation information local \nand only shares the representations with others,\nand a new bi-directional calibration loss which aligns the prediction result with the true label \nand pushes away the prediction result that falls into the non-candidate label set.\nFinally, numerical experiments were conducted to validate the effiency of the proposed algorithm."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper invents effective techniques for federated partial label learning and centralized partial label learning.\nThe experiments are also sufficient."
            },
            "weaknesses": {
                "value": "Although the experiments have verified the effectiveness of the proposed algorithm,\nthe theoretical contribution seems weak.\n\n(1) Theorem 1 can not demonstrate that federated learning improves the convergence rate.\nIn other words, the convergence rate is independent of the number of clients, $K$,\nand is same with that of centralized learning.\n\n(2) I also doubt the correctness of Theorem 1.\nThe authors claimed that the convergence rate is $O(1/\\sqrt{T})$.\nHowever, by Theorem 1, if we set $\\tau =\\sqrt{T}$, then the convergence rate is $O(1/T)$.\nWhat's more, it seems that the smaller the value of $\\eta$ is, the better the convergence will be.\nHowever, such a convergence rate i.e., $O(1/T)$, contradicts a konwn lower bound [1].\nFor smooth non-convex loss functions, the convergence rate of any stochastic first-order algorithms must be $\\Omega(1/\\sqrt{T})$[1].\n\n\nReferences\n\n[1] Arjevani et al. Lower bounds for non-convex stochastic optimization. Mathematical Programming, 2023."
            },
            "questions": {
                "value": "(1) Could the proposed algorithm be used to the case of i.i.d. data?\nIt seems that the proposed algorithm has assumed that the data is non-i.i.d.\nHowever, it is unknown to us that whether the data is non-i.i.d.\n\n(2) In Section 3.1, the definition of $M^k_i$ is somewhat unclear to me.\nTo be specific, $y^k_{i,j}\\in \\{0,1}$, while the elements in $Y^k_i$ should be a vector with lengeth $C$.\nSo, the notation $y^k_{i,j} \\in Y^k_i$ is unclear to me."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Partial label learning is an important learning technique on the datasets with inaccurate labels. Existing work focus on the centralized setting. This paper studies partial label learning in federated learning scenarios. This paper provides theoretical analysis as well as experiments."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The problem has some (limited) novelty."
            },
            "weaknesses": {
                "value": "1. The problem is not interesting. Can the authors provide some examples that needs PLL in FL?\n\n2. It is not clear what is the additional challenge of partial label learning in FL. What will happen if we directly combine partial label learning and FL methods?\n\n3. In general, the paper is hard to follow.\n\n4. For the theoretical result, it is necessary to make a comparison with the ordinary partial label learning, and FL with full  labels. Moreover, assumptions should be stated explicitly in the main paper instead of in the appendix.\n\n5. In section 5.1, at the bottom of page 7, the authors need to cite the original paper of Fed_CC, Fed_RC, Fed_CVAL, Fed_LW.\n\n6. A minor issue. It would be better to change the term \"Comparison methods\" to \"Baseline methods\" in section 5.1.\n\n7. Why does Fed_CVAL performs significantly worse than FedAvg?\n\n8. In general, the paper does not put forward any interesting point. I do not see any novelty that make this paper different from existing PLL paper or existing FL paper.\n\n9. What is $\\tau$ in eq.(13)?"
            },
            "questions": {
                "value": "See weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}