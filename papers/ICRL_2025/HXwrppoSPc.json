{
    "id": "HXwrppoSPc",
    "title": "COMiX: Compositional explanations using prototypes",
    "abstract": "Aligning machine representations with human understanding is key to improving interpretability of machine learning (ML) models. \nWhen classifying a new image, humans often explain their decisions by decomposing the image into concepts and pointing to corresponding regions in familiar images.\nCurrent ML explanation techniques typically either trace decision-making processes to reference prototypes, generate attribution maps highlighting feature importance, or incorporate intermediate bottlenecks designed to align with human-interpretable concepts.\nThe proposed method, named COMiX, classifies an image by decomposing it into regions based on learned concepts and tracing each region to corresponding ones in images from the training dataset, assuring that explanations fully represent the actual decision-making process. We dissect the test image into selected internal representations of a neural network to derive prototypical parts (primitives) and match them with the corresponding primitives derived from the training data. \nIn a series of qualitative and quantitative experiments, we theoretically prove and demonstrate that our method, in contrast to \\textit{post hoc} analysis, provides fidelity of explanations and shows that the efficiency is competitive with other inherently interpretable architectures. Notably, it shows substantial improvements in fidelity and sparsity metrics, including $48.82\\%$ improvement in the C-insertion score on the ImageNet dataset over the best state-of-the-art baseline.",
    "keywords": [
        "explanation-by-design",
        "class-defining-features"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We propose a method that improves interpretability in NNs by decomposing images into regions based on learned concepts and tracing these regions to corresponding parts in training images.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=HXwrppoSPc",
    "pdf_link": "https://openreview.net/pdf?id=HXwrppoSPc",
    "comments": [
        {
            "summary": {
                "value": "The authors introduced COMiX, an intrinsic explainable artificial intelligence (XAI) method designed to accurately identify prototypical regions within a test image and correlate them with corresponding regions in training images. This method is based on the feature encoder of the trained B-cos network and emphasizes the extraction of class-defining features that serve as prototypes, which are subsequently utilized for class predictions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- S1: The proposed method is well-motivated, and the relationship between the prototypes and the class-defining features is clear. The requirements for intrinsic interpretability that the authors address are essential components of intrinsic XAI approaches."
            },
            "weaknesses": {
                "value": "- W1: One of my primary concerns pertains to the insufficient details regarding the essential computations involved in the proposed method, which necessitate considerable computational resources. In particular, additional clarification is needed on the computation of mutual information maximization as described in Equation 8. Specifically, how are p(F_j) and p(l(F_j) = c) calculated? Does this process necessitate traversing every row of W_{1 \\to L}(d, \\theta)? This indeed involves significant computational effort, alongside other tasks requiring substantial computation, such as the generation of pseudolabels for each CDF.\n\n- W2: A secondary concern is the absence of baseline comparisons within the experimental results. While the authors sought to relate their method to prototypical approaches, they did not include comparisons with other state-of-the-art prototype-based methods, such as PIP-Net [1] and ProtoConcept [2], which demonstrate superior performance in accuracy evaluations. Additionally, Table 3 would benefit from including more baseline comparisons, as suggested by references [2, 4].\n\n- W3: In Table 2, the proposed method demonstrates inferior performance relative to the original B-cos in multiple cases, raising concerns about the suggested approach's efficacy. Additionally, it would be valuable to explore the performance of the end-to-end model by training the feature extractor of the B-cos.\n\n- W4: The experiment regarding sparsity requires further validation. Most studies on prototypical learning have addressed the impact of increased sparsity by utilizing global and local explanations or by adjusting the number of prototypes employed [1, 2]. Although Figure 6 presents an ablation study on the size of K, it would be beneficial to enhance this section by incorporating additional baseline comparisons.\n\n- W5: The experiment focusing on prototypical explanations was conducted solely in a qualitative way. For a quantitative approach, please refer to [2]. Additionally, refer to references [3] and [4] for insights into unsupervised concept discovery.\n\n- Reference\n- [1] Nauta, Meike, et al. \"Pip-net: Patch-based intuitive prototypes for interpretable image classification.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n- [2] Ma, Chiyu, et al. \"This looks like those: Illuminating prototypical concepts using multiple visualizations.\" Advances in Neural Information Processing Systems 36 (2024).\n- [3] Wang, Bowen, et al. \"Learning bottleneck concepts in image classification.\" Proceedings of the ieee/cvf conference on computer vision and pattern recognition. 2023.\n- [4] Hong, Jinyung,  Keun Hee Park, and Theodore P. Pavlic. \"Concept-centric transformers:  Enhancing model interpretability through object-centric concept learning within a shared global workspace.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2024."
            },
            "questions": {
                "value": "Most of my main concerns are listed in the Weakness section. Here, I listed additional questions.\n\n- Q1: In Section 4.4, the authors acknowledged the trade-offs of using l_2 distances. Have the authors considered employing alternative similarity measures, such as cosine similarity? What prompted the decision not to utilize cosine similarity, especially given that the B-cos method inherently leverages this approach?\n\n- Q2: It appears that the negative sign in the L2 distance equation (Eq. 10) may be a typo, as it relates to the process of extracting pseudolabels for each CDF based on the similarity. Is it a typo?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors proposed COMiX, a method for constructing an interpretable image classifier from a trained B-cos network. In particular, the method starts by training a base B-cos network, and finding M class-defining B-cos features for each class. For a given input image, COMiX first computes a transformation matrix corresponding to the composition of all the layers in the base B-cos network, and then selects a pseudo-label based on the closest training image in terms of the B-cos network's output. For each class-defining feature belonging to the pseudo-label class, the method computes a class prediction based on K nearest training images in terms of that feature's values, and the final prediction is made using majority voting. The authors compared their COMiX classifiers with competing methods (e.g., baseline convolutional networks, ProtoPNet models, B-cos networks) on a number of datasets, and found that their COMiX classifiers performed similarly (or better) to other methods in terms of accuracy and interpretability metrics."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Originality: The paper proposed an elaborate scheme to turn a B-cos network into a model with a capability to perform case-based reasoning (using k-nearest neighbors).\n- Quality: The proposed method did not significantly degrade the classification performance.\n- Clarity: The introduction is well-written, and the paper is well-motivated.\n- Significance: Interpretability is an important topic."
            },
            "weaknesses": {
                "value": "- Originality: The proposed form of interpretability (\"this part of the test image looks like that part of a training image\") has been explored in prior work (e.g., ProtoPNet). There is no novelty here.\n- Quality: The proposed method constructs a model that is not trainable end-to-end. Also, the proposed method is biased toward the pseudo-label predicted by the B-cos network, since the selection of class-defining features are based on the predicted pseudo-label.\n- Quality: The accuracy of COMiX is not particularly strong.\n- Clarity: The section describing the algorithm of COMiX is difficult to follow. The presentation is not clear. The notations are confusing. In particular, how is the class-defining features selected using equations (7) and (8), and how is the mutual information computed (equation (8))?\n- Significance: Given that there is little innovation in terms of interpretability and the accuracy results are mediocre, the proposed method cannot significantly advance the field of interpretable machine learning."
            },
            "questions": {
                "value": "How is the class-defining features selected using equations (7) and (8)?\nHow is the mutual information computed using equation (8)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Being able to interpret deep neural networks' decisions is becoming increasingly important with the integration of AI into high-risk areas such as autonomous driving and medical diagnosis. Existing methods attempt to explain existing model post-hoc, but may not be faithful to the inner workings of the network. The authors present COMiX, a prototype based method for image classification that links the prediction logic to image parts in the training data. COMiX utilizes B-cos networks for transparency (due to its linearity) from the input pixels to the class defining features used for classification. COMiX then aligns the features from the test image to the features in the training images to perform a KNN-based prediction in order to establish a \u201cthis part looks like this one\u201d example-based prediction. The authors measure their model\u2019s ability on four desiderata: fidelity, sparsity, necessity, and sufficiency. They apply their method to several vision backbones and one several popular image datasets. In addition, they qualitatively show the effectiveness of their method for explaining its predictions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Qualitative results are strong and convincing\n- Modeling / approach is simple\n- Robust use in different models / architectures\n- Robust ability across datasets\n- Creative use with B-cos networks and label aggregation\n- Sufficient set of quantitative evaluation metrics\n- Hyperparameter analysis / ablation studies present\n- Better sparsity than ViT baseline and best insertion scores w/ competitive deletion scores."
            },
            "weaknesses": {
                "value": "- The interpretability framework novelty isn't significantly more compared to ProtoPNet (Seems like another \u2018this looks like that\u2019 explanation just reframed)\n  - Much of the framework is similar to ProtoPNet with the exception of using pretrained features as concepts (as opposed to specialized vectors), a b-cos backbone, and KNN based prediction on feature similarity. \n\n- Presentation is unclear at times:\n  - Motivation/need for sufficiency is unclear\n  - Notation seems convoluted\n    - Confused about $s_{L}(x;\\theta)$ It seems that it should replace $W_{1->L}(x;\\theta)$ in equation 4.\n  - Not entirely clear on the attribution method. Is it which feature a pixel contributes most to?"
            },
            "questions": {
                "value": "- Could you clear up my understanding of the attribution used to make the visualization?\n- What is $s_L(x:\\theta)$ exactly? Could you provide a better/clearer definition?\n- I\u2019m not entirely sure of the definition of necessity, and I\u2019m unsure of the need for sufficiency. It seems redundant. Could the authors clarify necessity, and the need for sufficiency as a desiderata?\n- What is the speed of this method compared to others, given it has to look through the entire training data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a model that aims to be locally interpretable by design, as it combines B-cos Networks, which provide faithful saliency maps, with prototype methods. During inference, a pseudo-label is computed, whose M learned class features, or class prototypes, are then compared to the test image. The resulting classification seems to be the majority vote of the classes to which the most similar prototype belongs.\nAs B-cos Networks can provide faithful localizations on training and test images, this enables a visualization of the matching pixels.\nAs the features are not restricted to patches, the method is considered as object-level attention with prototypes."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper explores the important field of building interpretable models.\n\nThe core idea of the paper is presented clearly, with good figures 1 and 5. \n\nCombining b-cos with prototypes is a novel combination. \n\nThe method is evaluated on a sufficient number of datasets and across various architectures."
            },
            "weaknesses": {
                "value": "The paper is missing the discussion of several competitors, e.g.:ProtoPool, Pip-Net, Q-SENN (citations at the bottom);\nAll of these focus on sparsity and Q-SENN and ProtoPool additionally aim for object-level prototypes or features, not restricting the prototypes to patches.\nThey further achieve better results in e.g accuracy and sparsity, which leads to global interpretability.\nNotably, ProtoPool also uses exact training images as prototype for matching.\nThus, the novelty of the proposed method is very limited, as it just combines two existing methods, prototypes and b-cos, even without proper citation and comparison to in multiple ways superior SOTA. Notably, SOTA methods are backbone independent, so presumably compatible with b-cos. \n\nThe reported baselines are significantly too bad. Just training a vanilla resnet34 on CUB (on presumably the same data: 224x224, no crop) with hyperparameters optimized for a resnet50 on 448 gets 2.7 p.p. more accuracy. \nThis more than doubles the gap, and questions the validity of other results too.\n\nThe writing, especially of the method, is not clear:\n Eq. 4-11 lack explanations.\nThe algorithm 1 is unclear and lacks explanations. \nFigure 2 is not clear. \nWhat is trained when and how many settings (encoder training, CDF computation, inference?) exist and are shown is not clear.\nThe indices in eq.2-3 are unclear.\nWhat is M set to?\nTable 4 is not clear. What is the probed model for the other methods, and why is b-cos not a baseline?\n\nUsing a parametrized metric (PQ) with just one competitor is not convincing, when the parameters are chosen.\nAdditionally, the sparsity seems just slightly above a black-box model.\n\nThe definition of sufficiency in l308 seems different to the initially stated goal.\n\nThe standard deviations of results, excluding table 2,  and number of seeds per result are missing.\n\n\"Previous work has also shown that B-cos transformers inherently learn human-interpretable\nfeatures.\"  (l.217) needs a citation if available. I am not aware of any work showing that.\n\n\nProtoPool: Rymarczyk, Dawid, et al. \"Interpretable image classification with differentiable prototypes assignment.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\nPip-Net: Meike Nauta, J\u00f6rg Schl\u00f6tterer, Maurice van Keulen, Christin Seifert (2023). \u201cPIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification.\u201d IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\nQ-SENN: Norrenbrock, Thomas, Marco Rudolph, and Bodo Rosenhahn. \"Q-senn: Quantized self-explaining neural networks.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 19. 2024."
            },
            "questions": {
                "value": "Am I misunderstanding the method?\n\nWhy is an uninterpretable pseudo-label necessary?\n\nDid I use a wrong configuration for my baseline?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}