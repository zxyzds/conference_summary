{
    "id": "3xxxoh92Mo",
    "title": "Highlight: Learning Visual Prompts for Vision-Language Models",
    "abstract": "Large-scale Vision-Language Models, such as CLIP, demonstrate impressive capabilities and have multiple applications, from text-to-image generation to zero-shot classification. Recent work has suggested that visual prompts, such as a red circle, can steer the vision encoder to the circled region. While such vision prompts have now been used in various applications, they might be model-specific and depend on the model learning these behaviours from its training data. Discovering and evaluating various prompts might not be feasible given different models, tasks, and datasets. In this paper, we propose Highlight, a method to learn a visual prompt that highlights a region in an image or refines a manually engineered visual prompt. Using our framework, we can learn to highlight in a supervised way using a dataset of text-image region pairs or in an unsupervised way using synthetic captions or images only. Highlight outperforms other visual prompts, prompt learning approaches, and compute-intensive methods that use ensembles of multiple models and visual prompts.",
    "keywords": [
        "VLMs",
        "prompting",
        "visual prompting",
        "self-supervision"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=3xxxoh92Mo",
    "pdf_link": "https://openreview.net/pdf?id=3xxxoh92Mo",
    "comments": [
        {
            "summary": {
                "value": "This paper improves upon methods that add a manual highlight over image to improve clip performance. The authors optimize a learnable visual prompt and show that they can start from manual annotations to eventually propose the final method which relies on image crops from a pretrained object detector. The authors show that their formulation improves upon previous methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is easy to read and follow.\n2. The authors have a nice experimental framework that starts from manual annotations and builds up to a \"unsupervised approach\".\n3. The performance gains over previous methods are non trivial."
            },
            "weaknesses": {
                "value": "1. The authors have not validated their initial hypothesis that different models will have different biases. Table 6 compares several CLIP models but I am more interested to know if there is any difference in this behavior based on pretraining objective and dataset. How would this compare for SigLIP, SILC, EVA-CLIP, MetaCLIP etc and models trained on OpenAI dataset, Webli, datacomp, etc.\n2. How does the method compare with different localization methods. The authors have used MAttNet. Is there data leakage in the localization network? Is it possible to use a generic model here?"
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper works on a large Vision-Language Model (VLM), especially CLIP. It learns to generate markings on images so that a textual expression related to the image has high correspondence with the image region inside the marking. In short, the authors address the Referring Expression Comprehension (REC) problem using VLMs. While the problem of REC using VLMs like CLIP is not new, unlike previous studies, this paper does not assume the existence of predefined markers like red circle, square or arrow etc. Rather, the authors propose to learn the particular form of markers in both supervised and unsupervised manner. In supervised approach an infoNCE loss is minimized between annotated regions with expressions and the network generated regions (called highlights). In unsupervised approach, the authors use synthetic captions instead of manually annotated descriptions of the crops. Experiments show comparisons with related works that does not use supervision but ensemble of backbones in CLIP."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1.\tThe paper presents a learnable way to predict both shape and color of a visual marker such that regions bounded by such markers in images can correspond to textual descriptions.\n2.\tThe discussion on the relevant works is good.\n3.\tThe use of image crops and synthetic captions for unsupervised training is interesting."
            },
            "weaknesses": {
                "value": "1.\tThe novelty of the approach is limited. Let us take the example of the first work in this line which is RedCircle (Shtedritski et al., 2023). The authors there have done extensive analysis on drawing different types of visual prompting e.g., circle, square, arrow etc. with different colors on images and then processing it and the text query via a pretrained VLM. This paper concludes and shows that a red circle works best for REC with CLIP. One may argue that redcircle is dependent on manually tuning the markers for their shapes and colors. Manual tuning has already been replaced by learning to tune the visual markers in Xie et al. (2024a). Xie et al. also did not use supervision for training. The proposed work, on the other hand works good only when supervised training is used (compare row \u2018Highlight (unsupervised)\u2019 and \u2018RedCircle\u2019 rows as well as \u2018Highlight (Supervised)\u2019 and \u2018RedCircle\u2019 rows in table 2). The fact that \u2018Highlight\u2019 needs to use manual annotation from reference comprehension datasets to perform well, adds to novelty but takes away the major strength and flexibility of VLMs \u2013 i.e., zero-shot task transfer. The other incremental novelty of using synthetic texts does not perform well compared to the state-of-the arts.\n2.\tThe closest work to the proposed approach is Xie et al. (2024a) in flavor as this paper also learns to draw a visual marker in zero-shot manner. However, this has not been compared with in the zero-shot setting.\n3.\tThe proposed approach shows its usefulness only in one of the three tasks RedCircle (Shtedritski et al., 2023) evaluated themselves on. How does it perform on the other two tasks namely \u2013 \u2018Naming Keypoints\u2019 and \u2018Keypoint Localization\u2019 is missing.\n4.\tThere are a few strong statements that is not supported well. For example, Line 047 says - alt texts in pretraining image-text datasets summarise global image descriptions. This is in contrast to the common wisdom as alt-texts are usually very short and sometimes cryptic descriptions of a scene. Is there any reference or some examples that you can point to? Similarly, Line 60 says that whether a red circle is the prompt that best elicits these emergent behaviors in VLMs is unclear. However, in Shtedritski et al. (2023), there has been extensive studies on different types of markers e.g., circle, arrow, square, cross etc. or different colors of them. Table 2 in the main paper and table 10 and figure 10 in the supplementary clearly shows red circle works best.\n5.\tMinor typos: Line 63: \u2018unfeasible\u2019 -> \u2018infeasible\u2019; Line 261: \u2018bounding\u2019 -> \u2018bounding box\u2019."
            },
            "questions": {
                "value": "1.\tThis is in reference to the point 1 in the weaknesses. The apples-to-apples comparison (unsupervised regime) of the proposed approach with the existing ones can not cement the proposed approach\u2019s betterness. Any comment on this would be good to have.\n2.\tComparison with the closest paper Xie et al. (2024a) is missing and in the same level playing ground (without using manual supervision to train) Xie et al. (2024a) works better. Any comment on this would be good to have.\n3.\tThis is related to the third point in the weaknesses. Is it possible to comment on the missing experiments mentioned there?\n4.\tThe strong unsupported claims as detailed in point 4 in \u2018weaknesses\u2019 needs to support.\n5.\tIn Table 1, the results are impressive. However, I would like to see, for completeness, what would be the performance of the proposed method if it is measured in the same level playing field i.e., it is unsupervised and uses ensemble of the same backbones as the rest of the competing methods.\n6.\tI am not comfortable with categorizing methods like RedCircle with sup=x. Theres a fine line between an 'unsupervised' approach and a zero-shot approach. RedCircle can best be described as zero-shot and not 'sup=x' (which means it gets training but no supervision, which is not true for redcircle as it does not get any training).\n7.\tIn figure 2, When both are vision encoders, what encoders are you using? Are the two vision encoders the same in such a case?\n8.\tThe next question is related to eqn. (2). What does this symbol a cross inside circle mean? What is the relation between the two $\\mathcal{F}(v)$'s and $I(v)$? What is the difference between upper case I and lower case i in eqn. (2)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "1. [Summary]\nThis paper focuses on visual prompt learning for vision-language models. It introduces Highlight, a method to automatically learn a visual prompt that highlights regions for a VLM. The proposed method could work in both supervised and unsupervised manner. Experiments show that the proposed method achieves good performances."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "2. [Strengths]\n- a. It introduces Highlight, a method to automatically learn a visual prompt that highlights regions for a VLM.\n- b. The experiments are extensive, and the performance of the proposed method is promising.\n- c. The method is flexible, which can work in both supervised and unsupervised cases."
            },
            "weaknesses": {
                "value": "3. [Weaknesses] \n1. What is the different between this paper and the other popular visual prompt learning like [a, b]. [a, b] learn tokens as visual prompts. It would be better to discuss and clarify the similarity and difference between this paper and [a, b] and include the discussion in Section Related Work.\n[a] Visual Prompt Tuning.\n[b] Progressive visual prompt learning with contrastive feature re-formation.\n\n2. The core of this paper is to learn visual prompts for vision-language models. Have the authors tested the proposed method on other VLMs in addition to CLIP? Besides, the Section Related Work lacks a paragraph for introducing VLM. It would be better to add a new paragraph to introduce related work and recent advances in vision-language models, and could also cite the VLM survey [c] as the reference for the readers to know more about VLMs.\n[c] Vision-Language Models for Vision Tasks: A Survey\n3. Another question is that whether the learnt visual prompts are independent or dependent to input images? They are the same for all input images? Or works differently for different input images?"
            },
            "questions": {
                "value": "as shown in Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "1.The authors present Highlight, a method that learns visual prompts for Vision-Language Models (VLMs) to improve their ability to localize specific image regions. The key innovation is automatically learning both the shape and color of visual markers in a differentiable manner, rather than relying on manually designed prompts like red circles. \n\n2.The method can be trained either with supervision from text-image region pairs, or without supervision using synthetic captions or images alone. \n\n3.The authors evaluate their approach extensively on RefCOCO datasets using different CLIP models, demonstrating that Highlight outperforms existing visual prompting methods and even ensemble approaches while being more computationally efficient."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.The paper is clear and easy to follow.\n\n2.The research goal is meaningful. They propose to automatically learn visual prompts instead of designing them manually, which is a clever solution to a common problem in vision-language models.\n\n3. Their experimental results are impressive. The method works well across different versions of CLIP models and is more efficient than ensemble methods."
            },
            "weaknesses": {
                "value": "1.Robustness and Generalizability\n\n- The experiments are conducted exclusively on CLIP variants\n- I would like to see evaluations on other popular VLMs (e.g., Flamingo, GPT-4V)\n- Without broader validation, it's difficult to assess if this method is truly model-agnostic\n- I strongly suggest including at least 2-3 different families of VLMs in the evaluation\n\n2. Theoretical Understanding\n- The paper lacks insights into why learned prompts outperform manual ones\n- The visualizations don't effectively explain the mechanism's advantages\n- I recommend: \n(1) Providing case studies of success and failure cases\n(2) Including analysis of the learned prompt patterns\n\n3. Image Quality Impact\n- Looking at Figure 1, I'm concerned about the image blur introduced by the method\n- The authors should address: \n(1) How this blur affects the model's performance\n(2) If this is a limitation for certain types of images or tasks"
            },
            "questions": {
                "value": "1. I noticed the experiments were only conducted on CLIP models. Have you considered testing the method on other vision-language models? If not, what were the main challenges preventing such evaluations?\n\n2. From Figure 1, I can see that your method introduces some blur to the original images. Could you clarify:\n   * Does this blur affect the model's performance?\n   * Is this blur necessary for the method to work effectively?\n\n3. Your results show that learned prompts perform better than manual ones, but the mechanism behind this improvement is not clearly explained. Could you:\n   * Provide more analysis on why learned prompts work better?\n   * Share some failure cases to help understand the method's limitations?\n\n4. I'm interested in the robustness of your method:\n   * How stable is the performance across different runs?\n   * How sensitive is it to hyperparameter choices?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}