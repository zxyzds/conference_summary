{
    "id": "1ymGFnxfVB",
    "title": "LJ-Bench: Ontology-based Benchmark for Crime",
    "abstract": "Despite the remarkable capabilities of Large Language Models (LLMs), their potential to provide harmful information remains a significant concern due to the vast breadth of illegal queries they may encounter. In this work, we firstly introduce structured knowledge in the form of an ontology of crime-related concepts, grounded in legal frameworks. This ontology serves as the foundation for the creation of a comprehensive benchmark, called LJ-Bench, the first extensive dataset designed to rigorously evaluate the robustness of LLMs against a wide range of illegal activities. LJ-Bench includes 76 distinct types of crime, organized into a taxonomy. By systematically assessing the performance of diverse attacks on our benchmark, we gain valuable insights into the vulnerabilities of LLMs across various crime categories, indicating that LLMs exhibit heightened susceptibility to attacks targeting societal harm rather than those directly impacting individuals. Our benchmark aims to facilitate the development of more robust and trustworthy LLMs.",
    "keywords": [
        "Ontology",
        "Knowledge Graph",
        "Crime",
        "Language Models"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1ymGFnxfVB",
    "pdf_link": "https://openreview.net/pdf?id=1ymGFnxfVB",
    "comments": [
        {
            "summary": {
                "value": "The authors tackle the risk of Large Language Models (LLMs) providing harmful information by introducing LJ-Bench, a benchmark grounded in a legally structured ontology of 76 crime-related concepts. This dataset tests LLMs against a broad range of illegal queries, revealing that LLMs are particularly vulnerable to prompts associated with societal harm. By highlighting these vulnerabilities, LJ-Bench aims to support the development of more robust, trustworthy models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Benchmark development.\n- Systematic evaluation: Assessment of LLMs across 76 distinct types of crime.\n- Focus on societal harm: The article emphasizes an important aspect of model evaluation that can inform future research and development efforts aimed at enhancing model safety and trustworthiness."
            },
            "weaknesses": {
                "value": "- Fragmented structure, especially the Related Work section.\n- Some arbitrary choices, particularly regarding the selected prompts.\n- Limited justification on focusing on the Gemini model."
            },
            "questions": {
                "value": "This article presents an interesting contribution to the evaluation of Large Language Models (LLMs) in the context of harmful information, particularly through the introduction of LJ-Bench, a benchmark designed around a structured ontology of crime-related concepts. The systematic assessment of LLMs against a variety of illegal activities offers valuable insights into their vulnerabilities, particularly regarding societal harm. This focus is particularly relevant in today\u2019s landscape, where the safe deployment of LLMs is a pressing concern.\n\nHowever, the article also has several notable shortcomings that warrant attention. Firstly, the structure of the paper feels fragmented, with sections detailing specific aspects of the research without a coherent flow, which may hinder readers' comprehension of the overall argument. Additionally, some of the choices made throughout the study, such as the selection of prompts, appear arbitrary and lack adequate justification, raising questions about the robustness of the methodology. Furthermore, the decision to focus solely on the Gemini model is not sufficiently motivated; a broader evaluation involving multiple models could provide a more comprehensive understanding of LLM vulnerabilities in relation to illegal queries.\n\nLastly, the article does not adequately address how the proposed ontology will be maintained over time, which is crucial for its practical application and relevance. Overall, while the work has the potential to be a valuable resource for researchers aiming to enhance the safety of LLMs, these unresolved issues suggest that further refinement and discussion are needed to strengthen the overall contribution.\n\nQuestions:\n- Given the fragmented structure of the article, how do you envision improving the coherence of your arguments in future revisions to enhance reader comprehension?\n- What specific criteria did you use to select the prompts for evaluation, and how might you address the potential concerns regarding the perceived arbitrariness of these choices?\n- Could you elaborate on your rationale for focusing exclusively on the Gemini model for evaluation? Would you consider expanding this analysis to include other LLMs to provide a broader perspective on their vulnerabilities?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a legal crime jailbreaking benchmark based on California law. It also provides an ontology of crimes with 76 categories."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1. Jailbreaking benchmarks for law are very important.\n\nS2. The detailed ontology is good.\n\nS3. The results are detailed and explained well. The appendix includes lots of real cases and prompts and other details."
            },
            "weaknesses": {
                "value": "W1. The scope of this paper is very restricted. LJ-Bench is based on California law. How applicable is it to other countries?\n\nW2. What about harm \"against trees and plants\"? Is there no law in California against this?\n\nW3. Is the ontology vetted by law experts and professionals?\n\nW4. What is the point of augmented dataset of extended questions? Does it not fall in the same issues as in Fig 5, that is, of very similar text, and not really new content?\n\nW5. How effective the jailbreaking answers are should be evaluated by humans. Another LLM, that too of the same kind, may be biased in evaluation. Hence, a human evaluation is needed.\n\nW6. Is Table S3 not the full list? The caption says something different, though. Or does it need to be combined with Table S4 to get the full mapping of 76 categories and number of questions corresponding to each in the benchmark?\n\nW7. How applicable is this method to non-English prompts?\n\nW8. Typo: Contribution points 2 and 3 are repeated\n\nW9. Typo: Sec E.1 title"
            },
            "questions": {
                "value": "W1-W7"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper does contain ethical issues but it tries to address them.\nAnother review may be useful."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The widespread usage and ease of access of LLMs to information make it imperative that we\nstudy their robustness against potential harm they might cause to society. The authors\nintroduce a new benchmark called LJ-Bench, inspired by legal frameworks, and\nprovide the first detailed taxonomy on the types of questions whose responses would elicit harmful\ninformation. It contains crime-related concepts, supporting 76 classes of illegal\nactivities. The authors then conduct an experimental analysis of attacks on LJ-Bench, \nbased on the new types of crime as well as the hierarchical categories."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "--The use case and motivation behind the paper is reasonably strong, as evaluating the robustness of LLMs against a broad enough range of illegal activities is clearly important. \n--There is sufficient description of related work; in fact, I believe this may be the strongest part of the paper. \n--There is reasonable clarity in the way the paper is written, although I do believe it could use some more quality improvement and proofreading, as I state below."
            },
            "weaknesses": {
                "value": "--The experimental results are not up to the mark in this paper. First, they are not as extensive as they need to be, but more generally, they lack the type of scientific grounding (e.g., statistical significance results) that would be necessary in a paper purporting to be centered on responsible use of AI. \n--There are some presentation issues. First, the figures are not of sufficiently high quality. Second, the paper clearly lacks adequate proofreading e.g., on page 2, a bullet point is repeated, on page 8 the word 'original' is misspelt and so on."
            },
            "questions": {
                "value": "I am still not sure how the introduction of this benchmark helps us make more responsible use of LLMs. For people studying crime and legal issues, it seems that disabling the LLM from relying on this benchmark to answer questions (which I presume would be the obvious use case) would be overly broad. On the other hand, I'm not seeing sufficient evidence that, even if that were the goal, the benchmark could prevent it. For example, if I were to change the prompts and questions in slight ways, would the language model still not answer? I am not sure that there is a general and foolproof solution to the jailbreaking problem. More experiments and robustness studies would have helped express this more convincingly. Nevertheless, the authors should feel free to comment on this concern."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce LJBench, a benchmark of questions about crime-related concepts - designed to assess LLM safety in responding to such questions. The primary outputs of this paper are:\n - An OWL ontology, that re-uses some concepts from schema.org, for describing legal concepts from Californian Law and the Model Penal Code, describing 76 distinct types of crime\n - LJ-Bench: A dataset of 630 questions asking how to perform acts considered illegal under Californian Law or the Model Penal Code - with a fair distribution of questions across the 76 types of crime.\n - Structured OWL descriptions of each question from the LJ-Bench dataset, describing the type of crime each question relates to and whom the crime applies to.\n - Experiments to assess the outputs of Gemini 1.0 on these questions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The authors use their formal mappings to legal structures to ensure that the questions contained in their benchmark fairly represent all relevant types of crime described under Californian Law and the Model Penal Code.\n - The authors use their formal mappings to legal structures to ensure that the questions contained in their benchmark fairly represent all relevant types of crime described under Californian Law and the Model Penal Code. We see this as a food technique to ensure fair distribution of question types in a benchmark,\n - The authors present both the benchmark, and an experimental evaluation of how a model (gemini 1.0) performs against that benchmark."
            },
            "weaknesses": {
                "value": "**Comments on the ontology**\n\nWhilst the choice of formally representing legal concepts in an ontology is a sensible approach, we have some concerns around the methodology used to create the ontology. In particular:\n - There is extensive literature on legal ontologies which the authors do not reference, we encourage the authors to review the following papers:\n\t - \"A systematic mapping study on combining conceptual modelling with semantic web\"\n\t - \"Legal ontologies over time: A systematic mapping study\"\n    after reviewing these papers we suggest that the authors identify:\n\t - Whether there are existing ontologies capturing concepts from Californian law that should be re-used, and\n\t - Whether there are more suitable ontologies beyond schema.org that they should use as the foundation for the ontology for lj-bench\n - There is no rigorous methodology described for:\n    - How the authors identified the 76 distinct types for crime from Californian Law and the Model Penal Code, nor why they have chosen the 4 broader categories to class these into.\n    - How the four super categories of \"against a person, against property, against society, and against an animal\" were identified and selected.\n\nWe have also observed the artefacts that the authors have submitted, and have the following comments on the ontology design:\n - In the supplementary materials, only a fraction of the 630 questions from lj_bench are described in lj-ontology.rdf\n - There appear to be modelling errors in the disjoint class declarations. For instance \"rape\" is disjoint from \"sex offence\", when it likely should be classified as a subset.\n - nitpick: owl:ObjectPropertys defined in the schema are missing rdfs labels and comments (e.g. crime:steals)\n - nitpick: Classes defined in the schema are missing labels\n - nitpick: It is poor practice to have URIs with commas (,) question marks (?) or the (&) symbol\n - nitpick: Literals in comments inappropriately contain formatting, e.g. \"mis-\\nappropriates\" should be \"misappropriates\"\n - Information should not be implicitly encoded in the names of URIs; with crimes like \"crime:unlawful_interference_with_property\". Instead of having\n\n```\ncrime:unlawful_interference_with_property a crime:Unlawful_Interference_With_Property, owl:NamedIndividual .\n```\n\nhave\n```\ncrime:propertyInterference a crime:PropertyInterference, owl:NamedIndividual ;\n\trdfs:label \"Unlawful Interference With Property\"\n```\nI would also consider adding an rdfs:comment. \n\nPlease also review these suggestions https://chatgpt.com/share/6713d39d-1388-800c-a886-4e9ee3994efa, in particular on:\n - Naming conventions\n - Incomplete property definitions\n - Overlapping disjoint classes\n\n**Other Nitpicks**\n - We suggest the authors do note place \"few\" in brackets in the first figure\n - We request the authors include a turtle (ttl) serialisation of their ontology artefacts for human readability\n- Lots of quotes opened incorrectly, e.g. see list in attack section\n - Please reference schema.org better in the bibliography"
            },
            "questions": {
                "value": "- Is there a reason why this benchmark was not run on OpenAi and Anthropic Models?\n- Do you have a sense of how extensible this work is to other legal frameworks?\n - In \"For example, the nature of the answer would differ significantly when seeking classified information from the CIA (Central Intelligence Agency) compared to obtaining similar information from a local police station.\" how would you expect the answer to differ, could you have short examples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "As the benchmark is designed to assess model safety when asked to assess, or answer questions, about illicit acts - thus the dataset contains questions about how to perform illicit acts, including questions which can jailbreak current models."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}