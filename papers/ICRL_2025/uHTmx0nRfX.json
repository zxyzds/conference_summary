{
    "id": "uHTmx0nRfX",
    "title": "MoTE: Mixture of Task Experts for Embedding Models",
    "abstract": "Dense embeddings are a crucial component in various natural language processing applications, serving as the foundation for downstream tasks such as Retrieval Augmented Generation (RAG), search, classification, and clustering. To improve the performance of dense embeddings, recent approaches have focused on conditioning their representation with task information (e.g., by adding a task-specific text prefix), which allows to generate embeddings that take the task into account. This paper builds on this work and develops an approach that performs task conditioning by introducing a new architecture that has a dedicated set of parameters for each of the tasks. We refer to this model a Mixture of Task Experts (MoTE). For training, we introduce a task-aware training approach that allows us to optimize the training hyper-parameter for each task. Experiments on highly competitive tasks like MTEB with $56$ datasets across $7$ tasks show that, on average, MoTE achieves $1.62$ higher NDCG@10 on Retrieval datasets, $1.54$ higher MAP on Reranking datasets and $0.65$ higher overall performance across tasks when compared to contemporary approaches leveraging the same information.",
    "keywords": [
        "Embedding Models",
        "Representation Learning",
        "Mixture of Experts",
        "Retrieval Augmented Generation",
        "RAG",
        "Search",
        "Clustering",
        "Classification"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "This paper builds on instruction embeddings literature introducing a new architecture with a dedicated set of parameters for each of the tasks and a task-aware training.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=uHTmx0nRfX",
    "pdf_link": "https://openreview.net/pdf?id=uHTmx0nRfX",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces a novel approach for embedding models called Mixture of Task Experts (MoTE) that incorporates task-specific MLP blocks within the transformer architecture. During training, different tasks are directed to task-specific blocks, allow the model to generate different embeddings for the same query based on the task context.\nThis adds new parameters (the task-specific blocks) corresponding to the number of tasks during training. To reduce the parameter count, they apply WAVE (Weight Average of Vector Experts) where they average the weights of the task specific blocks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The embeddings of two different queries may need to be close or far apart depending on the task. Their approach to generating task-specific embeddings enables the optimization of distinct parameters for each task, promoting better task specialization."
            },
            "weaknesses": {
                "value": "1. Adding a new ML block after every other transformer layer will add significant memory requirement. It would be beneficial to explore optimization strategies to reduce this, such as adding ML blocks only in the topmost layers.\n2. The authors show that using the WAVE approach to reduce the gpu requirement retains 98.2% average performance. However, the more important metric is how much of the improvement is retained. Table 6 indicates that WAVE does not provide significant performance improvement.\n3. There should be an analysis of GPU memory increment as the number of tasks during training increases.\n4. From Table 3, task-aware training does not seem to improvement performance for most tasks.\n\nMinor:\n1. There are duplicate lines in Section 3.3.\n2. Page 5 line 216: texty-type should be text-type"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel approach for task-specific fine-tuning of embedding models in a mixture-of-experts framework, termed Mixture of Task Experts (MoTE). The MoTE model generates task-conditioned embeddings through dedicated parameter sets for each task, allowing for specialized representations tailored to specific downstream requirements. To address the increased memory demands associated with this approach, the authors introduce Weight Average of Vector Experts (WAVE), which mitigates the computational overhead of MoTE by efficiently managing memory without sacrificing performance. This dual approach overcomes the limitations of single-expert models applied across diverse tasks.\n\nTo evaluate its effectiveness, the proposed method was tested on seven in-scope and out-of-scope tasks across 56 datasets, demonstrating notable performance gains due to the MoTE architecture."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This work presents innovative methods for generating task-aware embeddings, building on and extending previous research.\n- It introduces a novel approach that leverages task-specific hyperparameters to create specialized embeddings while effectively managing the memory consumption challenges of task-conditioned models.\n- The method demonstrates practical applicability by addressing the limitations of using a single expert across multiple downstream tasks, showing promise for real-world implementations.\n- The paper is clearly written, with well-chosen examples and motivating illustrations that enhance comprehension.\n- It includes a robust experimental setup, testing across seven tasks (both in-scope and out-of-scope) with a total of 56 datasets.\n- A thorough ablation study and in-depth analysis strengthen the insights derived from the experimental results."
            },
            "weaknesses": {
                "value": "- This work represents an incremental advancement in mixture-of-expert models, focusing on generating task-aware embeddings for downstream tasks and addressing limitations of prior approaches.\n- The approach shows only marginal gains for out-of-scope tasks, such as in pair similarity. For task-aware applications like Classification, Clustering, and Retrieval, in general, they share semantic or latent spaces across them. However, the applicability of the proposed method does not generalize effectively to out-of-scope tasks and domain?\nFor example, in Table 2, the proposed approach yields only slight improvements on semantic textual similarity (STS) tasks, highlighting limited performance gains in out-of-scope settings."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tries to introduce the MOE architecture into the general embedding models and provide different hyperparameters for each task expert to improve performance. Meanwhile, the Weight Average of Vector Experts is obtained by merging the trained experts, which reduces the overall number of parameters and keeps the performance almost equal to that before merging."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The improvements presented in this paper are straightforward, with expected gains from the MOE architecture and Weight Averaging.\n2. The ablation experiments are sufficient, covering performance variations across different settings.\n3. The final model maintains the backbone parameter count, ensuring a fair comparison."
            },
            "weaknesses": {
                "value": "1. The work lacks innovation. It primarily enhances model performance by applying two established strategies (MOE and Weight Averaging) to generic representations without offering new insights.\n2. The paper centres on technical implementation but (1) does not commit to open-sourcing the code and (2) provides a very brief methodology, omitting details such as the implementation of the Task-Aware Training Strategy.\n3. The paper appears incomplete, falling short of the 10-page limit and lacking a thorough methodology section. Grammar and citation issues also affect readability.\n4. Weight Averaging is a model merging method, yet no background on model merging is provided, making the work hard to follow for readers unfamiliar with this area."
            },
            "questions": {
                "value": "1. The performance difference between \"TEM-Inst.\" and MoTE is minimal (less than 1% in Tables 1 and 2). However, as \"TEM-Inst.\" results don\u2019t appear to be from a public source and the hyperparameter search process isn\u2019t clearly detailed, could you clarify the hyperparameter tuning efforts made to ensure a fair comparison?\n\n2. The statement that \u201cexisting methods, ..., limits the degree of task-specialization of the final vector representation\u201d is unclear. In Tables 1 and 2, \"TEM-Inst.\" clearly outperforms STEM, suggesting that multi-task joint learning generally enhances performance across tasks. Thus, setting aside the results in MTEB:\n\n(1) What evidence shows that the trained representation space is task-specialized?\n\n(2) To what extent does the representation space differ between tasks?\n\n(3) Why can\u2019t instructions alone resolve this issue?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel architecture, MoTE, which allocates a dedicated set of parameters for each task to enhance dense text embeddings. MoTE offers a robust approach to improving dense text embeddings by employing a mixture of experts. This architecture, combined with a dynamic training strategy and WAVE, achieves better task specialization without intensively increasing latency or memory overhead. Experiments were conducted on the MTEB benchmark to compare MoTE with previous instruction-based embedding model training approaches and analyze the contributions of its techniques."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper explores the novel Mixture of Task Experts architecture for embedding models, which has the potential to effectively address the challenges posed by various retrieval tasks with differing intents.\n2. It conducts numerous analytical experiments and investigates the impact of training configurations, yielding insights that may aid further research on MoTE."
            },
            "weaknesses": {
                "value": "1. The experiments on MTEB do not compare MoTE against mainstream retrieval models, such as E5 and BGE, nor does it provide comparisons by using training data of mainstream retrieval models, which weakens the reliability of the experimental results.\n2. Many analytical experiments only present results without in-depth analysis, such as in sections 4.5 and 4.6.\n3. The writing is relatively poor. For example, the two paragraphs in section 3.3 appear to be repetitive."
            },
            "questions": {
                "value": "1. Is the WAVE method introduced in section 4.7 too simplistic, merely averaging the expert layers post-training but can maintain the performance?\n2. As many current MoE architectures are based on LLMs, could this approach be more suitable for retrieval models grounded on LLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}