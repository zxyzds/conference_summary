{
    "id": "yD2JMeKumt",
    "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
    "abstract": "Vision-language foundation models (e.g., CLIP) have shown remarkable performance across a wide range of tasks. However, deploying these models may be unreliable when significant distribution gaps exist between the training and test data. The training-free test-time dynamic adapter (TDA) is a promising approach to address this issue by storing representative test samples to guide the classification of subsequent ones. However, TDA only naively maintains a limited number of reference samples in the cache, leading to severe test-time catastrophic forgetting when the cache is updated by dropping samples. In this paper, we propose a simple yet effective method for DistributiOnal Test-time Adaptation (DOTA). Instead of naively memorizing representative test samples, DOTA continually estimates the distributions of test samples, allowing the model to continually adapt to the deployment environment. The test-time posterior probabilities are then computed using the estimated distributions based on Bayes' theorem for adaptation purposes. To further enhance the adaptability on the uncertain samples, we introduce a new human-machine collaboration paradigm which identifies uncertain samples, collects human-feedback, and incorporates it into the DOTA framework. Extensive experiments validate that DOTA enables CLIP to continually learn, resulting in a significant improvement compared to current state-of-the-art methods.",
    "keywords": [
        "Test-time",
        "uncertainty",
        "vision-language models"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "We proposes a new Distributional Test-time Adaptation (DOTA) method, which continuously estimates the distribution of test samples and incorporates human-machine collaboration to handle uncertain samples.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=yD2JMeKumt",
    "pdf_link": "https://openreview.net/pdf?id=yD2JMeKumt",
    "comments": [
        {
            "summary": {
                "value": "The authors address the problem of Test Time Adaptation of Vision Langugage models. They propose to continuously estimate the distribution of test samples, which they leverage through Bayes theorem, to make the final test predictions. They also collect human feedback to receive supervision for uncertain samples."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- They propose to estimate the class distributions in an online manner. \n- The adaptive fusion of zero shot text classifier based predictions and the distribution based feature similarities is simple and intuitive. \n- This being a backpropogation free approach, is very light-weight computationally, which is a great advantage for TTA.\n- The paper is well written and easy to follow, however several clarifications are required."
            },
            "weaknesses": {
                "value": "1. **Distributional Test Time Adaptation:** In a single image TTA setting, the whole section 3.2 is quite unclear. There is no **batch** of samples in this setting. So, how are the class distributions actually updated at each time step.\n\n2. **Samples for distribution estimation:** Are all test samples used for updating the class distributions? Wouldn't the use of low confident/uncertain samples lead to bad parameter estimates in eqn(4)?\n\n3. **TTA with human feedback:** While this is one of the major contribution of the paper, it appears to only result in modest improvements. 5% and 15% is a lot of data to ask labels for, from a human. However, the results improve only of the order of 1-2%. This makes the efficiency of this whole process questionable.\n\n4. **Performance evaluation of TTA with human feedback:** As the test samples arrive in an online manner and based on uncertainty, how is the final accuracy evaluated here? Are these samples inclusive when evaluating accuracy? If so, you should be using the ground truth as predictions for actively labeled samples. Then the accuracy should be up by about 5% or 15%. As this is not the case in the results reported, are the labeled samples excluded from evaluation? This needs to be clarified. For fair comparison, all results should be reported on the complete test set, even when using human feedback.\n\n5. **Need stronger baselines for TTA with human feedback:** To study the role of TTA with human feedback, stronger baselines need to be established, using different selection strategies, like random, confidence, entropy etc. and report the accuracy of complete test set. As all strategies have same amount of labeled samples included, the performance improvement due these strategies as well as the gains wrt no human feedback can be assessed.   \n\n6. **Amount of human feedback:** 5% and 15% is a lot of supervision and this may not be feasible during test time. It's more practical to ask labels for about 1-2% of test data. Experiments with stronger baselines, with lesser supervision, with correct evaluation method, is required to actually understand and evaluate the role of human feedback in TTA.\n\n7. **Choice of hyperparameters:**\nIn Implementation details, it is mentioned that validation sets are used to choose the hyperparameters. However, in TTA, one does not have access to any data from the test distribution apriori. Hence, validation data is not accessible in practice. Well, if one had access to validation data for test data, it provides a lot more information and could be used for more than just hyperparameter tuning."
            },
            "questions": {
                "value": "1. **Test Distribution Estimation:** In line 212 and from equations (4) and (6), it is described such that a batch of test samples arrive at each time step. However, prior baselines TPT, TDA perform single image TTA. Further, in Implementation details, in line 706, it is mentioned batch size is 1. Please clarify if single image TTA is done here as well, for fair comparison. If so, what does $n$ refer to in equations (5) and (6). How are $\\mu_k$ and $\\sigma_k$ estimated in single image TTA. Are you storing features, as done in TTA as well, along with the statistics?\n\n2. **Selection of uncertain samples:**\nConfidence is softmax applied over the similarity scores only right? Why is there such a large discrepancy using these two similar metrics (Table 7)? Also, is this similarity and confidence estimated from zero shot classifier or the classifier proposed? And why'd you choose what you choose?\n\n3. **Sensitivity to hyperparameters:** How sensitive is the method on the choice of the parameters $\\sigma, \\eta, \\rho$, as it's not practical to assume access to validation data before actually doing TTA? \n\n4. **Human in the loop TTA:** Please refer to the weaknesses and address the relevant concerns raised."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The author presents DOTA, a method that adapts to deployment conditions by continually estimating test sample distributions rather than memorizing them. Using Bayes\u2019 theorem, Dota computes posterior probabilities for real-time adaptation. A human-in-the-loop feature also gathers feedback on uncertain samples, enhancing adaptability. Experiments show Dota outperforms current methods with continuous test-time learning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper is well written and easy to follow.\n2. The idea is interesting,  the motivation of this paper is clear as well as the novelty of the method.\n3. The proposed method is extensively tested against prior work and outperforms on a variety of tasks/baselines."
            },
            "weaknesses": {
                "value": "1. This paper does not include a detailed case study focused on a particular domain or a challenging dataset.\n2. This paper does not include experiments assessing the model's sensitivity to hyperparameters. A detailed analysis of hyperparameter tuning could offer valuable insights into the robustness and generalizability of the proposed approach.\n3. The paper could benefit from additional visualizations illustrating Test-time adaption with human feedback"
            },
            "questions": {
                "value": "1. This paper is novel and interesting; would you consider making the code open source?\n2. In Table 1, for ResNet-50, DOTA\u2019s performance is only slightly better than TDA, with an average improvement of just 0.15%. I would like to understand the reason for this marginal gain."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The article addresses the problem of test-time adaptation of vision-language models. Differently from previous works (e.g., TDA, Karmanov et al. 2024) that use a cache to store samples, the proposed method, DOTA, stores an online estimate of the statistics (mean and variance) of each class of interest.  These statistics are then used during inference to refine standard CLIP predictions.  An active learning strategy exploiting this statistic is also proposed to improve the performance of the model further, asking a user to annotate the least confident examples. Experiments on a wide range of tasks show the efficacy of this approach."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. DOTA revisits principles in the literature on continual learning via nearest class mean classifiers (e.g., [a,b]) for improving the performance of VLMs at test time. Overall the approach is easy to implement and can be considered a valid baseline for future works, performing continuous TTA without the need for storing a cache, as in TDA.\n\n2. The article is well-structured and easy to follow, guiding the reader through all the design choices. \n\n3. DOTA is effective (as shown in the comparisons with TDA, e.g., Tab. 1 and Tab. 2) and computationally cheap (as shown in Tab. 3). \n\n4. Fig. 3 provides an analysis of how the performance of the two models (TDA and DOTA) vary w.r.t. the number of samples, showing the advantages of the latter.\n\n**References**:\n\n[a] Mensink, Thomas, et al. \"Distance-based image classification: Generalizing to new classes at near-zero cost.\" IEEE transactions on pattern analysis and machine intelligence 35.11 (2013): 2624-2637.\n\n[b] Bendale, Abhijit, and Terrance Boult. \"Towards open world recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2015."
            },
            "weaknesses": {
                "value": "1. DOTA continually updates its estimates of the statistics. Those might be affected by various factors linked to the experimental protocol, (e.g., order of the classes in the stream, batch size) as well as hyperparameters choice (i.e., the initial variance value, the shrinkage $\\epsilon$, $\\lambda$'s hyperparameters). Currently, the article does not provide too many insights on these factors, with the analysis mostly limited to the active learning percentile (i.e., Fig. 3). To assess the robustness of the model and provide a thorough study of its performance, it would be interesting to show results across multiple data ordering (i.e., currently it is not clear how many orders have been tested) and whether the performance changes w.r.t. the particular stream considered, even on the edge-cases where the data is non-i.i.d. [c]. Moreover, the hyperparameters may impact the speed of adaptation (e.g., variance initialization, $epsilon$) as well as how much the pretrained model is considered (e.g., $\\lambda$s): studying their impact is essential to fully evaluate the complexity of the approach and potential difficulties in applying it on real-world scenarios. \n\n2. While TPT and DiffTPT are strong models for test-time adaptation, they work on the episodic setting, i.e., where adaptation is held out on a single sample, and then the model is reset to its previous state. The possibility of storing/using test-time data (assuming coherence in the sequence) is a non-negligible advantage that DOTA has (and that it shares with TDA). This makes both the \"continual adaptation\" mark on TPT (Tables 1 and 2) potentially misleading, as well as TDA the only true baseline acting under the same priors of DOTA. To make the results stronger, it would be beneficial to add more baselines, such as DMN [d]. \n\n3. Following on the previous points, adapting to an evolving stream is a much more nuanced problem, where correlation between consecutive data may play an important role. Thus, various TTA settings with different types of stream and data dependencies (e.g., practical TTA [e], universal TTA [f]) could have been considered to further show the effectiveness of the approach. \n\n4. A key motivation behind DOTA relies on the test-time forgetting of TDA (lines 52-56). However, there are no experiments demonstrating this point (beyond the quantitative advantages of DOTA). An analysis clearly showing this phenomenon (and how DOTA is more robust to it) would strengthen the motivation behind the approach. \n\n5. The active learning strategy proposed to refine the performance for uncertain samples (Section 3.3) is a nice addition to make the approach more coherent but it lacks competitors. For instance, also TDA could employ a similar strategy (as the update of the cache is based on the confidence of the predictions). Moreover: (i) the accuracy with random feedback is also very close to those achieved with the proposed strategy (e.g., 0.6% gap on average in Tab. 6, 5% percentile); (ii) for the confidence-based scoring to work, the model is assumed to be calibrated, something not always true and that needs a proper discussion [g]; (iii) in Tab. 7 the accuracy of the random baseline is not reported: this is an important reference to put results into perspective.  \n\n6. Related work (Section 2) provides a limited discussion on the various types of TTA settings (e.g., [e,f]) as well as on previous methods employing online updates of statistics for continual learning/open world recognition [b] or prototype-based few-shot learning [I,j]. Expanding the discussion would help to better contextualize the work in the current literature.\n\n**Minors**: \n\n- Footnote 1 hints that the model could be applied beyond CLIP. However, there are no experiments confirming this claim. It would have been more thorough to show other models (e.g., SigLIP [h]) to support it.\n\n- Table 4 shows the results only for DOTA. It would be interesting to see the same analysis for the other baselines (e.g., TDA) to contextualize/provide a reference for the results.\n\n\n**References**:\n\n[a] Mensink, Thomas, et al. \"Distance-based image classification: Generalizing to new classes at near-zero cost.\" IEEE transactions on pattern analysis and machine intelligence 35.11 (2013): 2624-2637.\n\n[b] Bendale, Abhijit, and Terrance Boult. \"Towards open world recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.\n\n[c] Gong, Taesik, et al. \"Note: Robust continual test-time adaptation against temporal correlation.\" Advances in Neural Information Processing Systems 35 (2022): 27253-27266.\n\n[d] Zhang, Yabin, et al. \"Dual memory networks: A versatile adaptation approach for vision-language models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024.\n\n[e] Yuan, Longhui, Binhui Xie, and Shuang Li. \"Robust test-time adaptation in dynamic scenarios.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[f] Marsden, Robert A., Mario D\u00f6bler, and Bin Yang. \"Universal test-time adaptation through weight ensembling, diversity weighting, and prior correction.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2024.\n\n[g] Tu, Weijie, et al. \"An Empirical Study Into What Matters for Calibrating Vision-Language Models.\" International Conference on Machine Learning. 2024.\n\n[h] Zhai, Xiaohua, et al. \"Sigmoid loss for language image pre-training.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.\n\n[i] Snell, Jake, Kevin Swersky, and Richard Zemel. \"Prototypical networks for few-shot learning.\" Advances in neural information processing systems 30 (2017).\n\n[j] De Lange, Matthias, and Tinne Tuytelaars. \"Continual prototype evolution: Learning online from non-stationary data streams.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021."
            },
            "questions": {
                "value": "1. How does the performance change w.r.t. the data stream?\n2. What is the impact of the various hyper parameter?\n3. How does the test-time forgetting phenomenon happen?\n4. How does the model compare with other baselines, e.g., DMN?\n5. How does the work relate to existing ones on prototypical networks and the various TTA settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a Distributional Test-time Adaptation (DOTA) method, which adapt the pretrained Vision-language foundation models (e.g., CLIP) to the test target domain by estimating the distributions of different categories for test samples continually. The authors further introduce a human feedback collaboration method which identifies uncertain samples to further enhance the adaptability. Extensive experiments on diverse datasets validate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The writing and figures are good and easy to understand.\n\n2. The DistributiOnal Test-time Adaptation (DOTA) method for Vision-language foundation models without BP is simple yet effective during testing in new target domain, achieving a significant improvement compared to current state-of-the-art methods in most of the datasets.\n\n3. This paper first define the test-time adaptation problem with human feedback, allows the test-time adaptation for uncertain samples with human feedback.\n\n4. An adaptive final fusion probability is introduced to mitigate the potential negative impact when the number of test samples is insufficient."
            },
            "weaknesses": {
                "value": "1. The proposed method estimate the data distribution of samples in the current test environment during testing, but there lack some evidence or visualization. Why updating the feature distribution for different categories works better?\n\n2. The DOTA method seems somewhat similar to the T3A method [R1], which continually maintains a memory bank for prototypes during the testing stage. Could the authors clarify and analysis the difference and the advantages of the proposed method?\n\n[R1] Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization\n\n3. The proposed method seems works for not only VLMs but also other models in all classification tasks. Is it suitable for traditional TTA or Domain Generalization tasks?\n\n4. There could give more details and explanation about the $f_k(x)$ in Eq.(3).\n\n5. The uncertainty estimation method is simple. Is there any other uncertainty estimation method (like entropy) better?\n\n6. There missing an ablation study for the adaptive fusion probability."
            },
            "questions": {
                "value": "see the Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}