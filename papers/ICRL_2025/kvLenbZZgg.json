{
    "id": "kvLenbZZgg",
    "title": "Transformer Block Coupling and its Correlation with Generalization in LLMs",
    "abstract": "Large Language Models (LLMs) have made significant strides in natural language\nprocessing, and a precise understanding of the internal mechanisms driving their\nsuccess is essential. In this work, we trace the trajectories of individual tokens as they pass through transformer blocks, and linearize the system along these trajectories through their Jacobian matrices. By examining the relationships between these Jacobians, we uncover a **transformer block coupling** phenomenon in a variety of LLMs, characterized by the coupling of their top singular vectors across tokens and depth. Our findings reveal that coupling *positively correlates* with model performance, and that this relationship is stronger than with other hyperparameters, namely parameter budget, model depth, and embedding dimension. We further investigate the emergence of these properties through training, noting the development of coupling, as well as an increase in linearity and layer-wise exponential growth in the token trajectories. These collective insights provide a novel perspective on the interactions between token embeddings, and prompt further approaches to study training and generalization in LLMs.",
    "keywords": [
        "large language models",
        "transformers",
        "hidden representations"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We discover coupling in transformer block Jacobians and show a relationship with generalization.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kvLenbZZgg",
    "pdf_link": "https://openreview.net/pdf?id=kvLenbZZgg",
    "comments": [
        {
            "summary": {
                "value": "This paper presents an insightful exploration into the internal dynamics of Large Language Models (LLMs). The authors propose a novel framework to evaluate the coupling of transformer blocks through their Jacobian matrices, offering a structured perspective on their inter-token and cross-layer relationships. The study concludes that this coupling is positively correlated with model generalization performance and appears to be a more significant factor than other hyperparameters such as parameter budget and model depth."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The concept of using Jacobians to study transformer block coupling provides a fresh perspective on understanding the internal mechanics of LLMs.\n\n- The authors conduct an extensive empirical analysis across a variety of LLMs, lending credence to their hypothesis regarding the significance of transformer block coupling.\n\n- The paper is well-organized, with clear definitions and logical flow, making it easy to follow the complex concepts and their implications."
            },
            "weaknesses": {
                "value": "- The most concern lies in the strength of the generalization claims made by the authors. The paper presents empirical evidence suggesting a strong correlation between transformer block coupling and model performance on Open LLM LeaderBoard. However, it seems that some new models (e.g., LLaMA 3, Phi-2) which involve more pertaining steps (tokens) may have a more significant increase in performance than coupling. Revisiting the Figure 2, the coupling seems to emerge at certain training steps and then remain plain. So if the correlation only holds on a limited scope of pretraining steps?\n\n- The model performance is evaluated on Open LLM LeaderBoard. However, for some reasoning tasks, there may be some emergent capabilities [1] that only larger models have. In such settings, is the correlation between coupling and model performance still holding?\n\n[1] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
            },
            "questions": {
                "value": "See weaknesses section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "the paper analyzes and correlates jacobian singular value decompositions cross tokens and cross layer-depths. as a result, they define a 'coupling metric', and show that high coupling apparently correlates to high benchmark values.\nas such some intel on the inner processings of llm's is generated but the interpretation is widely left to the user."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- the paper offers a new perspective on by coupling training dynamics with model performance. \n- the method demonstrates somewhat robust as shown with the regression fit\n- potentially high impact on model architecture/ training and diagnosis (if better understood)"
            },
            "weaknesses": {
                "value": "the interpretation of this metric is vastly unclear. it is kinda clear that model training dynamics converge over training steps, in particular with common decaying lr-schedulers applied. so i wonder rather if one can interpret the score as a 'model training convergence rate' rather then performance-metric. falcon-7b (the entire model cluster > -.7 in fig1) demonstrates that high-coupling does not necessarily correlate with openllm-benchmarks afterall, tracing the activations/ gradients already shows such a convergence as well. afterall, on a converged model, activations only nuanced shift between layers. \n\nsimilarly i'm unsure how to interpret fig 6 etc.\ncomputational costs of the methods are also not discussed (?).\n\noverall i appreciate the solid work and do think there is merit in this methodology per se. however more focus should be made to practical implications of this rather abstract metric, and rigorously discussed."
            },
            "questions": {
                "value": "1) what is the value of fig 6/9?\n2) what are your computational costs?\n3) anything surprisingly found in the jacobians?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the inference dynamics in LLMs, by looking at several properties of the \"trajectories\" that tokens representations form as they evolve through the network. It is found that the linearized approximations of different blocks in a trained transformer tend to align with each other (in the sense of their top singular vectors being aligned), and that the magnitude of this alignment correlates with overall model performance. Several other properties of the trajectories are also discussed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper offers an intriguing observation, expanding on previous works that have reported similar phenomena in resnets. It makes a strong use of the large number of openly available LLMs to study hidden representations. The observation that the \"coupling\" performance correlates with model performance is of potential interest to the community and could encourage further research."
            },
            "weaknesses": {
                "value": "The paper seems to be somewhat missing a more coherent organization and message. In particular:\n- What is the motivation to consider the additional metrics of \"Linearity\" (Section 3.2) and \"Layer-wise exponential growth\" (Sec 3.3)? Can these be related, theoretically or empirically, to the \"coupling\" phenomena that is the main focus of the paper? (There's a small attempt to answer that in Section 6.2, I don't think it is very satisfying -- see below).\n- The coupling phenomena is presented as an \"interesting observation\", but the paper doesn't offer much more than that, and so the reader is left somewhat unsatisfied. I think it would significantly improve the manuscript if the authors would study this phenomena in a more \"functional\" way -- is this a uniform behavior for all types of tasks/prompts, or are there some variations that we can understand? Is coupling \"important\" to either learning or inference, or is it an epiphenomena?\n- Another possible way of making the findings more meaningful is a more mechanistic understanding of the phenomena (for example, does it happen in small models, trained on synthetic data? and so on; although this might be further away from the paper's overall approach).\n\nOther than that, there are some \"local\" issues:\n\nLine 35: The explanation for what a \"discrete non-linear dynamical system\" is, is pretty confusing. Almost every term seems to be at least inaccurate:\n- \"The term discrete reflects the network finite depth\" (line 38): the term discrete is typically referring to the fact that *time* is discrete in these models, and that the dynamics are described in terms of finite differences rather than rate of change (as would be in a differential equation). It would still be \"discrete\" even if the network depth would be infinite (i.e., in discrete-time RNNs). The converse is also not true, a continuous-time dynamical system can be evaluated/evolved for finite time.\n- \"coupled refers to the interdependent token trajectories enabled by self-attention\": this is inaccurate, the dynamics will be coupled even in a simple MLP (in fact even in a linear network -- consider the dynamics $\\mathbf{x}^{(t+1)}=\\mathbf{A}\\mathbf{x}^{(t)}$ : for a non-diagonal $\\mathbf{A}$, the dynamics is coupled because there are interactions between $x_i$ and $x_j$ through $\\mathbf{A}$.)\n- \"dynamical is due to the residual connections spanning various layers\": this is also an odd statement. the term \"dynamics\" is typically understood simply to refer to the fact that the state is changing with time. Consider the dynamical system from the previous bullet (as an example for dynamical system without the residual connections).\nIf the authors find it crucial to use non-standard interpretations of standard terms, they should at least note that, and hopefully provide a reasoning for their choice.\n\n\n\nFigure 2 and section 5.3: The fact that the rise in \"coupling\" is so fast seems like a strong indication that the correlation between coupling and performance (as shown in Fig 1) is due to some other variables/confounders, and that coupling in itself has no \"causal\" importance. The reason is that, presumably, performance decreases gradually and doesn't plateau after 10-20K training steps, like coupling. It could still be the case that coupling early-on facilitates further training (even if not a necessary component of inference in a trained model). In that sense, the authors statement  \"this finding suggests that the development of training methods to amplify coupling across transformer blocks may lead to favourable model performance\" (Line 431) seems to be a strong claim, as the authors haven't demonstrated any evidence for a causal relationship between coupling and performance.\n\n\nSection 6.2:\nI find the entire explanation rather confusing.\nThe linearization of $F^l$ in and by itself doesn't mean one can approximate the entire dynamical system as a linear dynamical system. The system is linearized *at* the particular input $x$, so we can't read out much about the dynamics of the same input -- one has to introduce a perturbation in $x$ and *then* use the linearized dynamics to study the evolution of this perturbation (and while it's not clear how to perturb input tokens, it is easy to perturb their embeddings in the first layer).\nI would also suggest to the authors to re-write their A.4 appendix in terms of a discrete dynamics (the same arguments can be made, but will be more relatable to the actual content/method of the paper)"
            },
            "questions": {
                "value": "Figure 3: The LSS is defined as $\\frac{L}{\\lVert \\tilde{x}^l_{i}-\\tilde{x}^0_{i} \\rVert}$ -- clearly a ratio of two positive numbers. How can it be that LSS values reported in Figure 3 are negative?. The authors themselves mention that \"Note that $\\text{LSS}\\geq1$\" (in Line 216) which is inconsistent with Fig. 3\n\nFigures 4 and 5: Readability will be greatly improved if some indications of the relevant \"axes\" is added to the figure itself (Layer/Time in Fig 4/5; embedding dimension on one of the panels; ). Since the different models basically exhibit \"identical\" results (at least qualitatively), I will advise the authors to keep just one column (one model) for each Fig in the main text, and use the freed space to better visual explanation (other models can be moved to Supplementary).\nEven more importantly: a scale/colorbar should be included. Currently it's not even clear if different sub-panels have the same scale or no. The choice to present absolute values is also a bit odd and not well-motivated."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes Coupling, a metric of a Transformer model.\nIt computes the similarity of two Jacobians, followed by a normalization; each is the sum of non-residual terms by Attention and FFN.\nThe main claim of this paper is that the Couping metric of a model, among a set of public pre-trained LLMs, is correlated with the average performance of a set of downstream tasks from the HuggingFace Open LLM Leaderboard (R^2 value of 0.7214).\nAdditionally, the paper explores the properties of the Transformer models in terms of the line-shape score (LSS), defined by Gai & Zhang (2021), and the exponential spacing (expodistance) of the hidden trajectories."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "To the best of my knowledge, this work is the first to explore the Coupling measures and indicate a correlation between them and the LLM performance on downstream tasks.\nIt also analyzes linearity in intermediate embeddings over the depth and exponential growth of the embedding norms."
            },
            "weaknesses": {
                "value": "The correlation between the Coupling metric and performance may not be a helpful or exciting finding. First of all, the paper does not provide any reasonable explanation or hypothesis about (1) why they investigated the Coupling, (2) why the Coupling could reflect performance, and (3) whether the correlation is from a causality (of a direction) or not.\nAdditionally, the paper does not perform intervention experiments, e.g., training a model with a regularization of increasing the Coupling and demonstrating the improvement. In total, it is difficult to conclude that this finding is helpful for the community so far. There are many experimental results, but they are not always well-organized and explained. I would like to know the real contributions of the results.\n\nThe correlation between the Coupling metric and performance might be very consistent. It may disappear in trends of recent high-performance models.\n\nThe contribution of the analysis of linearity and exponential growth is unclear. In my understanding, the analysis is conducted independently from the Coupling. Results are not specific to the models of interest and the Coupling. Many things might be obvious from Li & Papyan (2023). It would be great to add connections between them and Coupling (and the performance, if possible)."
            },
            "questions": {
                "value": "In summary, what can we concretely learn from the result? If we don't consider the quality, it's not very difficult to develop metrics that show a correlation but are ultimately not useful. Playing devil's advocate for a moment, for example, one of the most boring metrics could be the performance of a single downstream task like GSM8K. It may be correlated with the performance on the LLM Leaderboard. But, of course, this finding is not helpful. Compared with this or other candidates with such correlations, how can we claim that the Coupling metric is more interesting, helpful, or convincing?\n\nThe main claim may be based on Figure 1. But, the result looks very consolidated. Can we believe the presence of the correlation over various models or a limited number of models? For example, what happens if you only examine models that score above 50? Is there still a strong correlation, or does the relationship break down at higher performance levels? For example, if one observes phi-1 and phi-2, the Coupling rule may be broken because they may have similar settings, similar Couplings, and very different performances.\n\nI'm not sure why linearity and exponential growth are discussed and considered as a goodness metric. Playing devil's advocate for a moment again, if we make Transformer blocks, which just scale their inputs by large positive constants, it seems to satisfy the linearity and exponential growth but would fail to achieve the high performance as they the Transformer is just a scaler. Could you provide more discussion and reasons for choosing or designing the metrics for analysis?\n\nFigure 3 (c). Does this show that Linearity disappears at the end of training?\n\nFigure 6. MPT -> Mistral?\n\nCan we see Figure 6-style results of untrained models?\n\nFigure 8. Can we see the results of untrained models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}