{
    "id": "Wv9Gl1bFbc",
    "title": "Dynamic Self-Distillation via Previous Mini-batches for Fine-tuning Small Language Models",
    "abstract": "Knowledge Distillation (KD) has become a widely adopted approach for compressing large language models (LLMs) to reduce computational costs and memory footprint. However, the availability of complex teacher models is a prerequisite for running most KD pipelines. Thus, the traditional KD procedure can be unachievable or budget-unfriendly, particularly when relying on commercial LLMs like GPT4. In this regard, Self Distillation (SelfD) emerges as an advisable alternative, enabling student models to learn without teachers' guidance. Nonetheless, existing SelfD approaches for LMs often involve architectural modifications, assuming the models are open-source, which may not always be practical. In this work, we introduce a model-agnostic and task-agnostic method named dynamic SelfD from the previous mini-batch (DynSDPB), which realizes current iterations\u2019 distillation from the last ones\u2019 generated logits. Additionally, to address prediction inaccuracies during the early iterations, we dynamically adjust the distillation influence and temperature values to enhance the adaptability of fine-tuning. Furthermore, we propose Vocabulary Map Matching (VMM), aiming to address output inconsistency for auto-regressive LLMs. Last but not least, DynSDPB facilitates the seamless integration of existing self-correction and self-training techniques for small language models (SLMs). We apply DynSDPB to both encoder-only LMs (e.g., BERT model families) and decoder-only LMs (e.g., LLaMA model families), validating its effectiveness across natural language understanding (NLU) and natural language generation (NLG) benchmarks.",
    "keywords": [
        "Knowledge Distillation",
        "Self-Distillation",
        "Pre-trained Language Models",
        "Large Language Models",
        "Small Language Models",
        "NLP",
        "Fine-tuning"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "A model-agnostic and task-agnostic self-distillation method via the previous mini-batch's information.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Wv9Gl1bFbc",
    "pdf_link": "https://openreview.net/pdf?id=Wv9Gl1bFbc",
    "comments": [
        {
            "summary": {
                "value": "This paper propose a model-agnostic method soft-label based self-distillation method in NLP. Since in NLP, there seems not much recent work about self-distillation, this work reference DLB (in CV) which utilize previous mini-batch for self-distillation and further adjust two hyper-parameter dynamically (temparature and CE/KL scale). Experiments on many NLU and NLG tasks shows improved performance over standard finetune and knowledge distillation-related methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Self-distillation in NLP remains under-explored. Recent soft-label-based knowledge distillation do require a large teacher model, in which either online KD or storing the probability need additional cost. This paper show Self-distillation would be a promising way to imporve the performance while remain low cost."
            },
            "weaknesses": {
                "value": "1. Novelty is limited as an incremental method paper. Self-distillation using information from previous batch is an idea from DLB[1]. This paper only add a dynamic strategy about changing temparature and scale during distillation.\n2. This paper argue that DLB using fixed \u03c4 and \u03b1, which is not good. However, there lacks analysis on how dynamically change such hyper-parameters brings benefit (i.e. how this two parameters changed, and why this improve the performance intuitively)\n3. For decoder-only models, there is a lack comparison to recent soft-label-based distillation methods. \n4. For fine-tuning, it would be better to do full finetuning instead of Lora tuning. Lora tuning do not have comparable results to fine-tuning, and lora requires fewer compuation cost than your method I think, so your method would have an advantage over lora tuning which is expected. If the training resources does not support on large model, I woudl suggest to do full fine-tuning on small models such as 1B/3B.\n\n\n[1]Self-distillation from the last mini-batch for consistency regularization. Yiqing Shen, Liwu Xu, Yuzhe Yang, Yaqian Li, and Yandong Guo.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11943\u201311952, 2022."
            },
            "questions": {
                "value": "1. line 62: duplicated word \"inspiration\"\n2. It would be better to add an overall benchmark score for each method so that the overall improvement can be easily checked."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Knowledge distillation is a widely used method to reduce the computational cost of large language models (LLMs). However, the standard KD usually require an additional complex teacher model. Therefore, this paper focus on how to improve self-distillation in LLMs. Specifically, considering that the original self-distillation method is static, this paper introduces a dynamic SelfD method (DynSDPB) to address prediction inaccuracies during the early iterations. It dynamically adjusts the distillation influence and temperature value to enhance the adaptability of fine-tuning. Moreover, a vocabulary map matching is proposed to address the output inconsistency of the proposed method. Experimental results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Strengths**\n1. This paper introduces an adaptive method to dynamically adjust the influence based on the current states. \n2. The proposed method can be a plug-in technique to integrated into existing self-training method.\n3. Experimental results demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "**Weaknesses**\n1. The main idea of this paper is just an extension of the original SelfD, which makes it contributions as incremental.\n2. It seems authors only consider classification cases in NLU tasks. As authors claim their method as a task-agnostic method, so can you discuss how to adapt this method into regression tasks in NLU? \n3. Authors claim that at the early stage, model prediction could be incorrect and affect self-distillation training. So why not conduct self-distillation when the training / model prediction is stable? Can you compare your method with a baseline that applies self-distillation after some initial training and discuss the trade-off between these approaches?\n4. Why we need Output Mismatch Alignment? For decoder-only tasks, if the inputs are same, we only need to consider its logits and why we need to consider output mismatch problem during the fine-tuning? Could you provide some examples or explanations of when output mismatch occurs in practice during fine-tuning of decoder-only models.\n5. The results from some baselines are mismatched from the original paper (e.g., RoBERTa and DeBERTa). Please see my comments in questions. Could you explain these discrepancies or the experimental setups that cause these differences.\n\n**Minor issues**\n1. Line 62: ''taking inspiration inspiration from DLB'' $\\rightarrow$ ''taking inspiration from DLB''."
            },
            "questions": {
                "value": "1. How to apply the proposed method to regression tasks in NLU tasks (e.g., STS-B). \n2. Some baselines seem too weak. For example, the original baselines in RoBERTa [a] and DeBERTa [b]:\n\n|   | MNLI | QNLI  | QQP | RTE | SST | MRPC | CoLA |\n| - | ------- | ----      | ------  | ----- | ----- | -------| ------|\n| RoBERTa$_{base}$ | 87.6 | 92.8    | 91.9 | 78.7 | 94.8 | 90.2 | 63.6 |\n| DeBERTa$_{large}$ | 91.1 |95.3 | 92.3 | 88.3 | 96.8 | 91.9 | 70.5 |\n| DeBERTaV3$_{large}$ | 91.8| 96.0 | 93.0 | 92.7 | 96.9 | 92.2 | 95.3 |\n\nIt seems the baseline in this paper like (RTE, CoLA, SST, MRPC) are weaker than the original baseline. Please check these results. \n\n[a] RoBERTa: A Robustly Optimized BERT Pretraining Approach.\n\n[b] DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The work explores techniques of self-distillation in which logits from the latest mini-batch are exploited to supervise (regularize) the logits of the current batch.  Empirical studies are conducted with both encoder-only and decoder-only LMs on NLU and NLG tasks. \n\nHere are my concerns:\n1. Limited technical contributions.  The proposed method bears significant similarity with the existing work [Shen et al., Self-Distillation from the Last Mini-Batch for Consistency Regularization. In CVPR'22]. The only difference lies in dynamic adaption of the hyper-parameters. While the authors have emphasized the importance of dynamic hyper-parameter and empirical results seem verify the claim, I still think the innovations are limited given the existing work. Moreover, given the existing CVPR paper, it is not appropriate to claim that \"To the best of our knowledge, we are the first to propose a SelfD method called DynSDPB....\". Perhaps, you are the first to apply the technique in the field of NLP. \n2. Poor organization. Related Work is an important part of an academic paper. Normally, we cannot leave related work to appendix. I also suggest the authors conduct proof-reading, as I have found many typos like the repetition of inspiration in Line 62.\n3. More analysis are needed. It is better to show how the model gradually get improved in the process of self-distillation. It is unconvincing to just show the final performance and claim that the proposed method is effective."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Good performance on a wide range of tasks."
            },
            "weaknesses": {
                "value": "1. Limited technical innovations.\n2. Poor presentation."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}