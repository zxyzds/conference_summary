{
    "id": "YDuYWjbKDA",
    "title": "TreeDQN: Sample-Efficient Off-Policy Reinforcement Learning for Combinatorial Optimization",
    "abstract": "A convenient approach to optimally solving combinatorial optimization tasks is Branch-and-Bound method. The branching heuristic in this method can be learned to solve a large set of similar tasks. The promising results here are achieved by the recently appeared on-policy reinforcement learning (RL) method based on the tree Markov Decision Process (tMDP). To overcome its main disadvantages, namely, very large training time and unstable training, we propose TreeDQN, a sample-efficient off-policy RL method that is trained by optimizing the geometric mean of expected return. To theoretically support the training procedure for our method, we prove the contraction property of the Bellman operator for the tree MDP. As a result, our method requires up to 10 times less training data, performs faster than known on-policy methods on synthetic tasks. Moreover, TreeDQN significantly outperforms the state-of-the-art techniques on a challenging practical task from the ML4CO competition.",
    "keywords": [
        "reinforcement learning",
        "combinatorial optimization",
        "branch-and-bound",
        "ML4CO"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We present a data-efficient off-policy reinforcement learning method to learn a branching heuristic for the Branch-and-Bound algorithm.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=YDuYWjbKDA",
    "pdf_link": "https://openreview.net/pdf?id=YDuYWjbKDA",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces TreeDQN, an off-policy RL method that enhances the Branch-and-Bound approach for combinatorial optimization by learning efficient branching heuristics. With a proven Bellman contraction for stable training, TreeDQN requires up to 10 times less data and is faster than on-policy methods, achieving superior performance on both synthetic tasks and a challenging ML4CO competition task."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper presents TreeDQN, a novel sample-efficient off-policy RL algorithm designed specifically for combinatorial optimization, which addresses the limitations of high variance and slow training in existing on-policy methods.\n\n\n2. Theoretical and Empirical Validation: TreeDQN\u2019s theoretical foundation is strong, backed by the contraction property of the Bellman operator for tree MDPs. Empirical results show substantial improvements, including up to 10 times less training data and superior performance on both synthetic and practical tasks, notably the ML4CO competition task."
            },
            "weaknesses": {
                "value": "1. The experimental section primarily compares TreeDQN with basic methods and lacks extensive benchmarking against a wider range of state-of-the-art approaches in combinatorial optimization, which could better illustrate its relative strengths.\n\n\n2. The method is tailored to similar MILP tasks, potentially limiting its generalizability to significantly different combinatorial optimization problems, which the paper acknowledges but does not address experimentally."
            },
            "questions": {
                "value": "1. Could the authors clarify if TreeDQN\u2019s reliance on a contraction property holds under varying conditions or is sensitive to specific task characteristics, such as tree depth or branching factors?\n\n2. What impact does the choice of geometric mean over arithmetic mean have on convergence stability, and are there cases where this choice might be disadvantageous?\n\n3. Could the authors elaborate on how TreeDQN might perform on combinatorial tasks with differing structures, and are there any plans to extend the approach for more varied optimization tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies RL-guided Branch-and-Bound (B&B) method for Mixed Integer Linear Programs. Specifically, they modeled the procedure of B&B (select which integer variable to do splitting) as a tree MDP which is first proposed by [Scavuzzo et al., 2022]. They proposed a deep q-learning algorithm for the tree MDP, which they named as TreeDQN to guide the variable selection of B&B. Experiments on both synthetic and practical tasks are conducted to show the effectiveness of TreeDQN."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. They did experiments on both synthetic and practical mixed integer linear programming tasks to show the effectiveness of their proposed algorithm."
            },
            "weaknesses": {
                "value": "The theoretical basis seems not correct to me in this paper which is a big weakness.\n1. The definition of value function (2) is not correct. First, the value function (excluding optimal value function) should be policy-dependent. In (2), on the lefthand side, it is policy-independent while on the righthand side, action $a_t$ appears suddenly. Second, the value function shouldn't depend on the node selection strategy since we model the problem as a tree MDP instead of a temporal MDP based on the definition of tree MDP in [Scavuzzo et al., 2022]. Therefore, all the following analysis based on the value function is also under question.\n2. The definition of 'contraction in mean' sounds weird to me. What does it mean if an operator is stochastic? And 'contraction in mean' does not guarantee that under this operator, the value function can converge with high probability.\n3. The contraction property can be a justification for value iteration methods. However, using it as theoretical backing for q-learning methods is fragile."
            },
            "questions": {
                "value": "As I have mentioned in the weaknesses part, why does the node selection strategy matter in value function?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Authors propose an offline RL algorithm, TreeDQN, to scale the branch and bound algorithm for combinatorial optimization. The branching nodes are infered with a neural net forward pass. This approach is claimed to be faster than classical solvers such SCIP and CPLX because search trees are smaller with the proposed TreeDQN. To me, the contribution compared to previous work that also apply RL to branch and bound is not clear."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The use of RL for B&B algorithm is well-motivated (second paragraph of the intro). The experimental setup is detailed."
            },
            "weaknesses": {
                "value": "I believe the contriubtions are unclear. \n\nWhat is the difference between FMCTS and TreeDQN from a convergence point of view ? It seems to me that when you say on line 163 \" This method [FMCTS] is sample efficient since training data can be sampled from a buffer of past experiences. However, it may not converge to the optimal policy because its training data was obtained by older and less efficient versions of the Q-function \", this aso applies to TreeDQN despite proving contraction of operators. \n\nFurthermore, I think Tree MDPs can be rewritten as classical MDPs and thus one can just use the classical Bellman Operators to do Q-learning. Can't you just say your state space the set of all sub-MILPs coupled with the current tree depth, your actions are the set of branching node and the reward is -1 ? I am not sure why one needs to define tree MDPs and prove contractions of tree operators.\n\nI am very troubled by figure 1. In particular, I am not convinced that TreeDQN actually learns anything: it seems to me that solutions found at initialization of TreeDQN are already better than those of FMCTS. \n\nFurthermore, why can't you add tree sizes of solvers such as SCIP or CPLEX on your figure 1 ? This would be easier to compare your results with those of FMCTS (figure 2 of Etheve 2020).\n\nYou should add parenthesis to your citations in your introduction please (e.g. lines 50-54)."
            },
            "questions": {
                "value": "Is it really necessary to define and prove results about a new Bellman Operator?\n\nWhat is the difference between FMCTS and TreeDQN aside from the loss to optmize?\n\nWhy not include the tree sizes of SCIP or CPLX in your figure 1?\n\nAre you sure TreeDQN is actually learning? \n\nThank you in advance"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}