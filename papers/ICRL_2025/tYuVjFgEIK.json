{
    "id": "tYuVjFgEIK",
    "title": "Decoupling Variable and Temporal Dependencies: A Novel Approach for Multivariate Time Series Forecasting",
    "abstract": "In multivariate time series forecasting using the Transformer architecture, capturing temporal dependencies and modeling inter-variable relationships are crucial for improving performance. However, overemphasizing temporal dependencies can destabilize the model, increasing its sensitivity to noise, overfitting, and weakening its ability to capture inter-variable relationships. We propose a new approach called the Temporal-Variable Decoupling Network (TVDN) to address this challenge. This method decouples the modeling of variable dependencies from temporal dependencies and further separates temporal dependencies into historical and predictive sequence dependencies, allowing for a more effective capture of both. Specifically, the simultaneous learning of time-related and variable-related patterns can lead to harmful interference between the two. TVDN first extracts variable dependencies from historical data through a permutation-invariant model and then captures temporal dependencies using a permutation-equivariant model. By decoupling variable and temporal dependencies and historical and predictive sequence dependencies, this approach minimizes interference and allows for complementary extraction of both. Our method provides a concise and innovative approach to enhancing the utilization of temporal features. Experiments on multiple real-world datasets demonstrate that TVDN achieves state-of-the-art performance.",
    "keywords": [
        "Time Series Forecasting",
        "transformer",
        "multivariate time series forecasting"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=tYuVjFgEIK",
    "pdf_link": "https://openreview.net/pdf?id=tYuVjFgEIK",
    "comments": [
        {
            "summary": {
                "value": "In this paper, the authors investigate the problem of multivariate time series forecasting. The authors claim that overemphasizing temporal dependencies can destabilize the model making the model sensitive to noise. And the simultaneous learning of time-related and variable-related patterns can lead to harmful interference. So the authors propose the temporal-variable decoupling network to model the temporal and variable relationships respectively. The authors evaluate the proposed method on several datasets and achieve good performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "N.A."
            },
            "weaknesses": {
                "value": "1.\tThe relationship of inter-variables in the time sequence should have an instantaneous effect, but I am surprised that the author did not quote related work in this area [1,2].  \n2.\tWhy do learning time-related and variable-related patterns simultaneously harm inference? Please provide some rigorous evidence.  \n3.\tThe author said that Transformer puts too much emphasis on extracting time-dependent patterns and did not provide enough evidence that this statement was suspicious of the rationality of this assertion. From the perspective of the data generation process, future data generation has also received the effects of time delay and cross variables at the same time, which is the opposite of separating these two factors at the same time. This method highlights the intra-and inter- time series relationships, but many methods have studied this problem, such as [3].  \n4.\tWhy does the proposed method need to model cross-variable relationships and then model cross-temporal relationships? What is the motivation behind it, what if it is in turn?  \n5.\tMore recent comparison methods need to be considered, for example [4] [5].\n\n[1] . Identification of hidden sources by estimating instantaneous causality in high-dimensional biomedical time series.  \n[2] . Modeling nonstationary time series and inferring instantaneous dependency, feedback and causality: An application to human epileptic seizure event data.  \n[3] Yu, Guoqi, et al. \"Revitalizing multivariate time series forecasting: Learnable decomposition with inter-series dependencies and intra-series variations modeling.\" arXiv preprint arXiv:2402.12694 (2024).  \n[4] Jia, Yuxin, et al. \"WITRAN: Water-wave information transmission and recurrent acceleration network for long-range time series forecasting.\" Advances in Neural Information Processing Systems 36 (2024).  \n[5] Xu, Zhijian, Ailing Zeng, and Qiang Xu. \"FITS: Modeling time series with $10 k $ parameters.\" arXiv preprint arXiv:2307.03756 (2023)."
            },
            "questions": {
                "value": "Please refer to Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors argue that overemphasizing temporal dependencies can destabilize Transformer-based models, increasing their sensitivity to noise, overfitting, and weakening their ability to capture inter-variable relationships. Therefore, they propose a new Temporal-Variable Decoupling Networks (TVDN) which decouples the modeling of variable dependencies from temporal dependencies to address this challenge."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe proposed TVDN decouples the modeling of variable dependencies and temporal dependencies which useful in long-term time series forecasting.\n2.\tTVDN also separates the modeling of temporal dependencies into historical dependencies and predictive dependencies, with the latter having received relatively little attention in previous works."
            },
            "weaknesses": {
                "value": "1.\tIn line 61, the authors said that \u201clinear models and Cross-Variable Transformers do not extract accurate temporal dependencies because they essentially map historical series as unordered sets \u2026 \u201d, I disagree with the assertion that these models treat historical series as unordered sets. for example, the linear models predict future time steps by the equation $\\hat{x}_{i+1}=\\omega_{1}x_{i-L+1}+\u2026+\\omega_{L}x_{i}$, clearly treating historical series as ordered sets.\n2.\tIn equation (5) and (6), the authors define $T^{0}=Z_{h}+ Z_{CVE}$; however, in figure 2, it seems that the input of PSTD is consists only $Z_{CVE}$.\n3.\tAlso in figure 2, I cannot find the step (2) in the method section.\n4.\tIn line 246, $Z_{proj}$ is mentioned without a prior definition, I believe it should be $Z_{h}$.\n5.\tIn line 215, the notation $Z_{CVE} \\in {R}^{O \\times D}$ is used, yet I cannot find the definition of $D$.\n6.\tIn line 315,\u201cthe permutation-invariant models overlook dynamic temporal features \u201dseems inappropriate. \n7.\tIn line 332, I disagree with the claim that \u201cwe identified that the bottleneck of the traditional Transformer model lies in the ineffective utilization of historical sequence information\u201d, because the experimental results of PatchTST have shown that longer historical sequences will lead better performance. In addition, the reason why the transformer model performs poorly is that it over-captures channel dependencies, which can also be seen in the experimental results of PatchTST and DLinear. Furthermore, the poor performance of the Transformer model can be attributed to its tendency to over-capture channel dependencies, as evidenced by the experimental results of both PatchTST and DLinear.\n8.\tThe experiments are unfair, since PatchTST and DLinear perform better with longer historical windows. Although the authors have conducted experiments with $L=144$, I suggest the authors also set $L=336$ and $L=512$, and compared the results with PatchTST and DLinear.\n9.\tIn Figure 9, Transformer performs better than Transformer Decoder only which is insistent with  \u201deven with a significant reduction in historical information \u2026\u201din Line 912."
            },
            "questions": {
                "value": "please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I think there is no ethics concerns."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a dual-phase deep learning network architecture called the Temporal-Variable Decoupling Network(TVDN), which decouples the modeling of variable dependencies from temporal dependencies. First, during the variable dependence learning phase, the Cross-Variable Encoder (CVE), a permutation-invariant model, completely disregards the temporal dependence of the sequence and only extracts cross-features between variables, generating an initial prediction sequence. Once the CVE stabilizes, the second phase shifts to temporal dependency learning. The Cross-Temporal Encoder (CTE) divides time series dependence into two parts: Historical Sequence Temporal Dependency (HSTD), and Prediction Sequence Temporal Dependency (PSTD). The outputs of HSTD and PSTD are then fused to correct the initial prediction from the variable dependence learning phase. The approach reduces the risk that overemphasizing temporal dependencies can overfit the model, and enables broader parameter space exploration. The experiment in this paper shows that TVDN achieves state-of-the-art (SOTA) performance in electricity, traffic, weather, four ETT, and exchange fields by comparing it with some of the latest Time-Series Forecasting Transformer (TSFT) methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tTVDN decouples the modeling of variable dependencies from temporal dependencies to reduce the interference between the two, enhance the utilization of temporal features, and improve forecasting accuracy.\n2.\tTVDN adopts a dual-phase architecture, starting with CVE to learn inter-variable dependencies and then shifting to CTE focused on temporal dependency learning. The two-phase approach effectively addresses the interference of variable and temporal learning, which avoids leading to a degradation of models\u2019 performance.\n3.\tThe proposed method demonstrates SOTA performance in real-world forecasting benchmarks by capturing complex dependencies across variables and time. This is achieved with minimal computational overhead, making the model efficient and scalable."
            },
            "weaknesses": {
                "value": "1.\tThe experiment focuses on predicting accuracy and sensitivity to temporal dependencies but does not assess computational overhead and resource requirements. To provide a more comprehensive evaluation, it is recommended to include specific metrics for computational cost, such as training time, inference time, memory usage, and FLOPs. By reporting these metrics, the paper could offer a clearer picture of the model\u2019s practical feasibility, particularly for deployment in large-scale, real-time, or resource-constrained environments.\n2.\tThe experiment in Section 4.3 shows that the model\u2019s performance significantly declines when the temporal order is shuffled, indicating a strong dependency on temporal features. The high dependency may decrease the model\u2019s effectiveness on datasets that don\u2019t have clear temporal patterns, limiting its application domain."
            },
            "questions": {
                "value": "1.\tGiven the complexity of TVDN\u2019s dual-phase network structure, could the authors provide a quantitative comparison of computational costs versus accuracy gains relative to baseline models to allow for a more objective assessment of potential trade-offs between accuracy and efficiency?\n2.\tIn real-world applications, temporal sequences are often affected by various types of noise, which may impact model performance. Could the paper conduct experiments by introducing specific noise types, like Gaussian noise, to simulate random disturbances or missing values to simulate incomplete data, to observe TVDN's performance in these noisy environments, and to assess its robustness to noise?\n3.\tThis paper has shown that TVDN performs excellently on various datasets. Is there any limitation to the model? In cases of data scarcity or extremely complex intervariable relationships, can the model maintain its performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}