{
    "id": "zxqdVo9FjY",
    "title": "Generalization for Least Squares Regression with Simple Spiked Covariances",
    "abstract": "Random matrix theory has proven to be a valuable tool in analyzing the generalization of linear models. However, the generalization properties of even two-layer neural networks trained by gradient descent remain poorly understood. To understand the generalization performance of such networks, it is crucial to characterize the spectrum of the feature matrix at the hidden layer.\nRecent work has made progress in this direction by describing the spectrum after a single gradient step, revealing a spiked covariance structure. Yet, the generalization error for linear models with spiked covariances has not been previously determined.\nThis paper addresses this gap by examining two simple models exhibiting spiked covariances. We derive their generalization error in the asymptotic proportional regime. Our analysis demonstrates that the eigenvector and eigenvalue corresponding to the spike significantly influence the generalization error.",
    "keywords": [
        "Generalization",
        "Random Matrix Theory",
        "Spiked Covariance",
        "Two Layer Network",
        "Layer Wise Training"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zxqdVo9FjY",
    "pdf_link": "https://openreview.net/pdf?id=zxqdVo9FjY",
    "comments": [
        {
            "comment": {
                "value": "We thank the reviewers for their comments and help in improving the paper and hope that our responses with the new results have improved their opinions. If there are further questions that we can answer, we would be happy to continue the discussion."
            }
        },
        {
            "title": {
                "value": "Part 2"
            },
            "comment": {
                "value": "> In footnote 3 (Line 266), the authors say \"... \n\nIf we use these results, then similar to Eqautuons C.23 in Ba et al. 2022 and Equation (5) in Moniri et al. 2023, we would have that the value of Stieljtes transform is given to us as the unique solution to a set of consistency equations. Hence, we would replace the **explicit** values in Lemmas 7 and 8 with these **implicit** values. However, we did not do so since we wanted explicit closed-form expressions. However, please see the general response, showing how this can be achieved.\n\n> Why is there no regularization for the signal-plus-noise problem when there is regularization for the signal-only problem (Line 278-285)?\n\nThis limitation is primarily technical. The regularized case for the signal-plus-noise model introduces many additional terms whose mean and variance would need to be bounded, significantly complicating the analysis.\n\n> How do the authors arrive at \"Hence, we see that if the target vector y has a smaller dependence on the noise (bulk) component A, then we see that the spike affects the generalization error.\" in Line 380? Its connection to the previous part seems to be missing.\n\nHere we have that $ y_i = \\beta_*^Ta_i + \\beta_*^T z_i + \\varepsilon_i$. We see that for the bulk term: $a_i^T \\beta_* \\approx \\tau_A \\|\\beta_*\\|$. For the spike term: $z_i^T \\beta_* \\approx \\theta\\|\\beta_*\\|$\n\nUsing our scaling $\\theta = \\tau \\sqrt{n}$, the signal part is always larger. However, when the bulk also grows (i.e., $\\tau_A = \\Theta(d)$), the spike's effect becomes invisible. Specifically, we need:\n- $a_i^T \\beta_* = \\Theta(1)$\n- $z_i^T \\beta_* = \\Theta(\\sqrt{n})$\n(assuming $\\|\\beta_*\\|= \\Theta(1)$) for the spike to have a detectable effect.\n\n> Undefined symbols \n\nWe apologize. $f \\ll g$ means $f = O(g)$ and $f \\asymp g$ means $f \\ll g$ and $g \\ll f$\n\n> How do the authors come up with the equation for the peak point of double descent in Line 477? Is it an empirical observation or a theoretical result?\n\nThis is currently empirical. \n\nIt can be proved by computing the expression's derivative (and second derivative) and evaluating it at the derivative. The derivative expressions are quite involved; even symbolic programs such as SciPy struggled. **Hence, this leads us back to our contribution to simplified expression**. While our expressions are further simplified compared to Cui et al. 2024, we believe further simplification is only a positive.\n\n> Typos\n\nThe reviewer is correct in identifying typos. These shall be fixed."
            }
        },
        {
            "title": {
                "value": "Part 1"
            },
            "comment": {
                "value": "We thank the reviewer for their detailed feedback. Let us address the key points:\n\n> Limited contribution/novelty... Most of the results in this paper are trivial extensions of the results by Hastie et al. (2022) and Li & Sonthalia (2024)\n\nWe respectfully disagree. Our contributions extend beyond prior work in several important ways:\n\n1. Finite vs Asymptotic Analysis: Prior work focused on asymptotic results. We provide finite-sample corrections that reveal how spikes affect generalization. We show when these corrections matter (small bulk variance) and when they don't (large bulk variance)\n\n2. Technical Novelty: \n   - As the reviewers point out, we allow one eigenvalue to diverge compared to Hastie et al. 2022. This is a significant difference. \n\n   - Compared to Li and Sonthalia 2024. Only one of their two models allows for an eigenvalue to diverge. This model is closely related to the Signal only model in this paper. However, we have output noise $\\varepsilon$. This creates many new dependencies requiring novel analysis techniques\n       - Consider the proof sketch on page 10. Line 491 shows that for the signal-only problem, our solution is the solution from the Li and Sonthalia 2024 paper plus an extra term. The $\\hat{\\varepsilon}$ is not isotropic in the ridge regularied version. Hence, we do not immediately have free independence between $\\hat{Z}+\\hat{A}$ and $\\hat{\\varepsilon}$. Hence, we need to be very careful about the alignment between the two. \n       - Hence, we get terms that are cubic and quartic in eigenvalues (vs quadratic in prior work). These required the development of new concentration bounds for these higher-order terms. See Lemmas 17 through 22. \n\n      The Li and Sonthalia 2024 paper does not consider the signal plus noise case, which introduces further terms that need bounding. Finally, the Li and Sonthalia 2024 only consider the case when $\\tau_A = 1$ or more broadly $\\tau_A = \\Theta(1)$, we allow $\\tau_A = \\Theta(\\sqrt{d})$. **This is also significant**.\n\n> Although the paper is motivated by Moniri et al. (2023), there are significant discrepancies.\n\nWe agree. Please see the general response on how we can handle the dependency. \n\nAdditionally, we think of the difference in the targets as a strength of the paper, as we can show things as the following. \n\n1. Relationship between targets and spike size:\n   - For targets depending on the bulk (input data), large spikes are crucial\n   - The required spike size scales with input dimension and dataset size\n   \n2. Importance of spike-target alignment:\n   - The alignment between spike direction and targets significantly affects generalization\n   - This alignment term exhibits its own double descent behavior\n   - Small alignment improvements can yield large generalization gains\n\n3. Double descent characteristics:\n   - Peak location depends on bulk variance and regularization strength\n   - Suggests weight decay regularization primarily affects the bulk, not learned features (spike)\n\n> There exists a related work (Cui et al., 2024)... Dandi et al. (2024)\n\nThank you for bringing these to our attention. The second is quite new and, as the reviewer points out, was only posted well after the submission deadline. Hence, we believe that it is concurrent work and should **not** affect the review of our paper. We shall nonetheless discuss the two papers in the revision. \n\nThere are many differences to Cui et al. 2024\n\n1. We work in a more restricted setting but provide rigorous proofs. \n\n2. We simplify expressions. For example, $\\zeta$ in equation 17 in Cui et al. 2024  is exactly $\\xi - 1$ in our paper (see Lemma 13 in the appendix for a definition). The expression in Cui et al. 2024 is left in terms of $\\zeta$. **This is because the results are a product of dependent terms. Hence, simplification is not easy**. However we\n    a. Compute the expectations and variances of each of the terms \n    b. Compute the expectations of the products. \n    c. Greatly simplify expressions\n  These simplifications are a major strength of our work, as the expressions are interpretable without numerical computations. \n\n> In Line 178,  denotes the case with a single spike (as shown by Moniri et al., 2023)...\n\nWe only consider the single spike case. The analysis here is similar to Sonthalia and Nadakuditi 2023. Kaushik et al. 2024 extend Sonthalia and Nadakuditi 2023 to the higher rank version. We would need to do the same to extend to multiple spikes. As mentioned before, the difficulty in the analysis was\n    a. Compute the expectations and variances of each of the terms \n    b. Compute the expectations of the products. \n    c. Greatly simplify expressions\n\nThese are currently scalar expressions, so we can use commutativity. For multiple spikes, we have matrix expressions, so we no longer have commutativity. The analysis is possible, but it is just quite tedious.\n\nWe ignore the $o(\\sqrt{n})$ we shall highlight this as another discrepensacy"
            }
        },
        {
            "comment": {
                "value": "> The authors claim l.083 that Moniri et al. (2023) do not quantify the test error after one gradient step. To the best of my understanding, they do provide such a tight characterization (Theorem 4.5). Could the authors clarify their claim, and emphasize how their work is positioned with respect to Moniri et al. (2023)?\n\nWe should have been more precise in our claim. While Theorem 4.5 in Moniri et al. 2023 shows that the difference between the test error using $F_1$ (features after 1 step) and $F_0$ (initial features) converges to a constant, there are important differences in our approach:\n\n1. Moniri et al. 2023 have expressions requiring solutions to fixed point equations (see equations (5) in their paper). These only hold asymptotically with hard-to-quantify approximation rates. \n\n2. In contrast, we provide:\n\n   - **Closed form expressions** for the risk itself (not just differences as is the case in Moniri et al. 2023)\n\n   - **Better control on approximation error rates**, enabling analysis of finite matrices\n\n   - The above two allow better control in understanding the relationship between the bulk and spike. \n\n> I find the discussion l.332-355 somewhat confusing\n\nWe apologize for the unclear notation. We use the Vinogradov notation where $f \\ll g$ means $f = O(g)$. Therefore, setting $\\theta^2 = \\tau^2 n$ is consistent with our assumptions.\n\n> I believe more intuition about the different scaling considered would help solidify the intuition for the spn case\n\nYou raise an excellent point. Let's clarify the scaling relationships:\n\n1. For the bulk term: $a_i^T \\beta_* \\approx \\tau_A \\|\\beta_*\\|$\n\n2. For the spike term: $z_i^T \\beta_* \\approx \\theta\\|\\beta_*\\|$\n\nUsing our scaling $\\theta = \\tau \\sqrt{n}$, the signal part is always larger. However, when the bulk also grows (i.e., $\\tau_A = \\Theta(d)$), the spike's effect becomes invisible. Specifically, we need:\n\n- $a_i^T \\beta_* = \\Theta(1)$\n\n- $z_i^T \\beta_* = \\Theta(\\sqrt{n})$\n\n(assuming $\\|\\beta_*\\|= \\Theta(1)$) for the spike to have a detectable effect.\n\n> In 3.3, more discussion in the main text about why only the unregularized case is considered for the spn case, while generic  is considered for the signal-only model, would be helpful for intuition, whether it is for technical reasons or because it is not interesting.\n\nThis limitation is primarily technical. The regularized case for the signal-plus-noise model introduces many additional terms whose mean and variance would need to be bounded, significantly complicating the analysis.\n\n> typos\n\nThank you for identifying these issues. We will correct all typos in the revision."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for the feedback. \n\n> The paper is motivated by the spiked covariance from the one-step gradient feature learning in neural networks (Section 1). However, it did not show how the results can be applied to the feature learning scenario. I question the amount of contribution this paper provides.\n\nPrior work (Moniri et al. 2023 and Ba et al. 2022) established the existence of spikes for specific target types $y$ (single index models). Our work focuses on understanding the spike's effect through several novel contributions:\n\n1. We analyze various targets and alignments between spikes and targets, providing rigorous proofs of generalization bounds\n\n2. We demonstrate how asymptotic results may not capture behavior in finite networks\n\n3. We provide precise quantification of how spikes affect generalization in both finite and asymptotic regimes\n\n> The assumption in line 2221-222 and 253-255 is too strong. The analysis breaks down if there is dependency in the cross term. However, the paper did not show how big the difference the predicted result would be when there is dependence in the cross term. It is questionable if the result in this paper is applicable in realistic machine learning settings.\n\nWe have now removed this assumption. When considering the dependence structure from Moniri et al. 2023, the analysis actually simplifies. A key term in our analysis is the projection of the spike direction on the bulk eigenvectors. Due to the dependence, this term becomes more tractable. Please see the general response.\n\n> What novel results could the authors conclude in the feature learning setting in neural networks using the main theorems 3,4?\n\nOur analysis reveals several important insights for feature learning:\n\n1. Relationship between targets and spike size:\n\n   - For targets depending on the bulk (input data), large spikes are crucial\n\n   - The required spike size scales with input dimension and dataset size\n   \n2. Importance of spike-target alignment:\n\n   - The alignment between spike direction and targets significantly affects generalization\n\n   - This alignment term exhibits its own double descent behavior\n\n   - Small alignment improvements can yield large generalization gains\n\n3. Double descent characteristics:\n\n   - Peak location depends on bulk variance and regularization strength\n\n   - Suggests weight decay regularization primarily affects the bulk, not learned features (spike)\n\nWhile some of these phenomena have been observed before, we provide simplified, quantitative connections between them.\n\n> How could the authors show the assumption on the dependence does not affect the result? Is there any experimental validation?\n\nWe provide new theoretical results that explicitly handle the dependence structure. Please see the general response. \n\n> From Figure 4.1, we can see that the effect of the spike correction term is small when is large. Is the main theorem still useful to explain the phenomenon we see from feature learning?\n\nYes - prior work shows that the spike represents the learned feature. Our results allow for larger spikes than previously considered in works like Hastie et al. 2022. However, this shows that for the spike to effect the generalization, we need even bigger spikes. \n\n> This paper has problems with the wordings, even in main theorems. This makes the reading difficult. For instance:\n\nThank you for identifying these issues. We will fix all typos and improve clarity in the revised version."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for their comments. \n\n> They reference the work of Moniri et al., but this work is unrelated to neural networks or gradient descent; it addresses a purely linear regression problem for data with simple spiked covariances.\n\nWe respectfully disagree. Our work is directly motivated by and connected to neural networks through the following chain of reasoning:\n1. Ba et al. (2022) and Moniri et al. (2023) show that after one gradient step, the feature matrix $F_1$ can be written as $ F_0 + P$, where $P$ is a rank-$ell$ matrix. \n2. This creates a spiked covariance structure in $F_1^TF_1$.\n3. To understand the generalization error of such networks, we need to analyze least squares regression with $F_1$ as the feature matrix.\n4. Our work studies this exact setting, though in a simplified form, to make the analysis tractable. \n5. In the general rebuttal, we removed many of our simplifications. This further strengthens the connection\n\n> They do not account for the generalization ability of neural networks after a single gradient step, as they bypass the gradient step entirely by assuming the W1 matrix directly, which does not reflect the full process of neural network training.\n\nWe agree with the reviewer. Building on the results from Ba et al. 2022 and Moniri et al. 2023, our new results take us towards understanding the generalization error for two-layer networks. \n\nOur analysis provides valuable insights:\n- It shows how spikes affect generalization in finite vs asymptotic regimes\n- It demonstrates the importance of the alignment between the spike direction and the target function\n\nWe never meant our paper to claim that we understood the generalization error for two-layer networks, as reflected in our title's focus on least squares regression.\n\n> Could you provide a reference for the statement, 'It has been shown that to understand the generalization...' on line 39?\n\nIn addition to the RMt paper cited in the paper, see [1] for an empirical result on more realistic networks. \n\n[1] Martin and Mahoney, \"Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning\", JMLR 2021.\n\n> Is your generalization analysis very different from the work of Li & Sonthalia (2024)?\n\nCompared to Li and Sonthalia 2024. Only one of their two models allows for an eigenvalue to diverge. This model is closely related to the Signal only model in this paper. However, we have output noise $\\varepsilon$. This creates many new dependencies requiring novel analysis techniques. \n\n1. Consider the proof sketch on page 10. Line 491 shows that for the signal-only problem, our solution is the solution from the Li and Sonthalia 2024 paper plus an extra term. The $\\hat{\\varepsilon}$ is not isotropic in the ridge regularied version. Hence, we do not immediately have free independence between $\\hat{Z}+\\hat{A}$ and $\\hat{\\varepsilon}$. Hence, we need to be very careful about the alignment between the two. \n\n2. Hence, we get terms that are cubic and quartic in eigenvalues (vs quadratic in prior work). These required the development of new concentration bounds for these higher-order terms. See Lemmas 17 through 22. \n\nThe Li and Sonthalia 2024 paper does not consider the signal plus noise case, which introduces further terms that need bounding. Finally, the Li and Sonthalia 2024 only consider the case when $\\tau_A = 1$ or more broadly $\\tau_A = \\Theta(1)$, we allow $\\tau_A = \\Theta(\\sqrt{d})$. **This is also significant**."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for the feedback and comments. Key differences between our work and important prior research are that we (1) provide finite matrix correction terms and (2) offer simplified closed-form expressions.\n\n> Could the authors comment on the link between their results and (Ba et al. 2022) in the context of Gaussian Universality (see e.g. [1]) ? \n\nYes, the problem we and prior work are interested in understanding is the generalization error for the following. First, we solve a regression problem\n$$ \\beta\\_{LS} = argmin ||y - \\beta^T F||\\_F^2 + \\lambda ||\\beta||\\_2^2 $$\nThen, we are interested in the generalization performance of $\\beta\\_{LS}$. Let's call this risk $R(F)$ to highlight the dependence on $F$. The difference between setups lies in the $F$ term. There are three different $F$'s considered.\n\n1. $F_{CK} := \\sigma(WX)$. For Gaussian $X$ and $W$, after taking a step of GD. This is from Ba et al. 2022\n2. $F_{CE} := \\theta_1 WX + \\theta_2 A$ where $A$ has IID standard Gaussian entries independent of $W,X$ and $W$ is after taking gradient step of GD\n3. $F_{SP} := A + \\theta uv^T$, where $A$ has IID standard Gaussian entries. \n\nThe Ba et al. 2022 paper shows that in the small learning rate regime, $R(F_{CK}) = R(F_{CE})$ **asymptotically**.\n\nHowever, we do things differently:\n\n1. We allow spikes from the large learning rate limit. Hence, the result from Ba et al. 2022 does not apply. Equation 3.1 in Ba et al. shows that for small learning rate, the size of the spike is $\\Theta(1)$, where as for large learning rates, it is $\\Theta(\\sqrt{d})$ (note for the rank one spike the Frobenius norm is equal to the spectral norm). We are interested in the case when the spike is large. The idea behind the large step size is that we are in a regime in **which the Gaussian Equivalence Property is no longer true**. \n\n2. We provide more precise correction terms for finite matrices. While the prior work is purely asymptotical. \n\n3. In our rebuttal, we also generalize to models closer to that from Moniri et al.  \n\n> One additional weakness of this submission is the related works coverage. \n\nWe thank the reviewer for pointing us to these works. We shall add these references. \n\n[5] characterizes the risk for the setting from Moniri et al. 2023 using, as the reviewer and the paper says, using the non-rigorous replica symmetry method. The differences are three-fold:\n\n1. Our results in the paper are for a restricted setting; however, we provide proof. \n2. We simplify expressions. For example, $\\zeta$ in equation 17 in Cui et al. 2024 is exactly $\\xi - 1$ in our paper (see Lemma 13 in the appendix for a definition). The expression in Cui et al. 2024 is left in terms of $\\zeta$. **This is because the results are a product of dependent terms. Hence, simplification is not easy**. However we\n    a. Compute the expectations and variances of each of the terms \n    b. Compute the expectations of the products. \n    c. Greatly simplify expressions\n\nWe believe these are the main challenges we overcome in our proof.\n\n> Could the authors elaborate on the connection with Moniri et al. 2024?\n\nWhile Theorem 4.5 in Moniri et al. 2023 shows that the difference between the test error using $F_1$ (features after 1 step) and $F_0$ (initial features) converges to a constant, there are important differences in our approach:\n\n1. Moniri et al. 2023 has expressions requiring solutions to fixed point equations (see equations (5) in their paper). These only hold asymptotically with hard-to-quantify approximation rates. \n\n2. In contrast, we provide:\n\n   - **Closed form expressions** for the risk itself (not just differences as is the case in Moniri et al. 2023)\n\n   - **Better control on approximation error rates**, enabling analysis of finite matrices\n\n   - The above two allow better control on understanding the relationship between the bulk and spike. \n\n> What is the bottleneck for analyzing multiple spikes?\n\nThe analysis here is similar to Sonthalia and Nadakuditi 2023. Kaushik et al. 2024 extend Sonthalia and Nadakuditi 2023 to the higher rank version. We would need to do the same to extend to multiple spikes. As mentioned before, the difficulty in the analysis was bounding variances. These are currently scalar expressions hence we can use commutativity. For multiple spikes we have matrix expressions. Hence no longer have commutativity. The analysis is possible, but it is just quite tedious.\n\n> maximal scaling regime \n\nWe don't know this regime. Does the reviewer mean when the step size is too small and we do not see a spike or the regime from [1] where the spectrum becomes heavy-tailed? In the heavy-tailed situation analysis similar to [2] can be used.\n\n[1] Martin and Mahoney JMLR 2021 - Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning\n\n[2] Wang et al. 2024 AISTATS - Near-interpolators: Rapid norm growth and the trade-off between interpolation and generalization"
            }
        },
        {
            "title": {
                "value": "Introducing Dependency Between Bulk and Spike"
            },
            "comment": {
                "value": "A common criticism among reviewers was our abstraction of the dependency between bulk and spike components. Here we demonstrate how our proof framework extends to handle the dependent case from Moniri et al. 2023.\n\nRecall that Moniri et al.'s spike structure is:\n$$ \\sigma(W_0\\tilde{X}^T) + c (\\tilde{X}\\beta_{sp}) \\zeta^T $$\nwhere $\\tilde{X}$ is Gaussian data, $W_0$ is inner layer weights, and $\\zeta$ are outer layer weights.\n\nWe modeled this as:\n$$ A + \\theta vu^T $$\nwhere $A$ is Gaussian. Below we show how our analysis extends to Moniri et al.'s setting.\n\n## Introducing Dependence\n\nFirst, consider the intermediate structure:\n$$ X = A + \\theta (A\\beta_{sp})u^T $$\n\nTo analyze this, we only need to modify two parts of our proof:\n\n1. In Lemma 10, the norm of $v^T A^\\dag$  scaling changes:\n   - Original: $\\mathbb{E}_{\\lambda}\\left[\\frac{\\lambda}{(\\lambda + \\mu^2)^2}\\right]$\n   - New: $v = A\\beta\\_{sp}$, and norm becomes $\\|\\beta\\_{sp}\\|$\n\n   where expectations are over the Marchenko-Pastur distribution.\n\n2. The variable $t = (I-AA^\\dag) v$ is no longer zero.\n\nFor the Signal-Only case, this gives bias:\n$$ \\frac{\\theta_{tst}^2}{n_{tst}}\\left[(\\beta_*^T u)^2 + \\tau_{\\varepsilon}^2\\left(\\frac{(1+c)}{2T_1} + \\frac{\\mu^2 c - T_1}{2\\tau_{A_{trn}}^2 T_1} \\right)\\right] $$\nwhere $T_1$ is unchanged. We do not present the whole formula for brevity. To extract insights, we consider the same simplifications for the paper.\n\nUnder simplifications ($\\mu = 0$, $\\tau_{A_{trn}} = \\tau_{A_{tst}}$, $\\theta = \\tau \\sqrt{n}$), for $c > 1$ we get:\n$$ \\tau_{A}^2 (\\beta_*^Tu)^2\\left(1+\\frac{\\tau_A^2}{c}\\|\\beta_{sp}\\|^2\\right) + \\tau^2_{\\varepsilon}\\frac{c}{c-1}\\left(2 + \\frac{\\tau_A^2}{c}\\|\\beta_{sp}\\|^2\\right) $$\n\nNote: Due to the extra $A$ factor, this only holds for $\\tau_A = \\Theta(1)$ versus our original $\\tau_A = O(\\sqrt{d})$. \n\nNote: this is the Signal only version so $y = \\theta (A \\beta\\_{sp}) u^T\\beta\\_*$. \n\n## Full Moniri et al. Structure\n\nNow consider:\n$$ X = \\sigma(W_0A^T) + c (A\\beta_{sp}) \\zeta^T $$\n\nThe risk becomes a random variable dependent on $W_0$, $\\zeta$, and $\\beta_{sp}$. Using standard assumptions (isotropic with unit expected norm for the $\\zeta$, and $\\beta_{sp}$ and the rows of $W_0$), we analyze the expected risk. Importantly, since $W_0$ and $(\\beta_{sp}, \\zeta)$ are independent, the bulk remains independent of the spike. This is because functions of independent random variables are independent. Hence, our assumptions are reasonable. \n\nTo get the generalization error for this model, we need to replace Lemmas 7-9. As an example, the new Lemma 7 resembles equations from Moniri et al. (Eq. 5) and Ba et al. (Eq. C.23):\n\n**Lemma 7:** For $W_0$ ($m \\times d$), $X$ ($n \\times d$), $m < n$, with $d/n \\to \\phi$, $d/m \\to \\psi$, $m/n \\to c$:\n\n1. $\\mathbb{E}\\left[\\frac{1}{\\lambda+\\mu^2}\\right] = \\frac{c}{\\tau_A^2}m_c\\left(-c\\frac{\\mu^2}{\\tau_A^2}\\right)$\n2. $\\mathbb{E}\\left[\\frac{1}{(\\lambda+\\mu^2)^2}\\right] = \\frac{c^2}{\\tau_A^4}m_c'\\left(-c\\frac{\\mu^2}{\\tau_A^2}\\right)$\n3. $\\mathbb{E}\\left[\\frac{1}{(\\lambda+\\mu^2)^2}\\right] = \\frac{c^3}{2\\tau_A^6}m_c''\\left(-c\\frac{\\mu^2}{\\tau_A^2}\\right)$\n\nwhere $m_c(z)$ satisfies:\n$$\\frac{\\psi}{z} H(z) - \\frac{\\psi - 1}{\\psi} = m_c(z)$$\n$$H(z) = 1 + \\frac{H^\\phi(z)H^\\psi(z)(c_1 - c_2)}{\\psi z} + \\frac{H^\\phi(z) H^\\psi(z)c_2}{\\psi z - H^\\phi(z)H^\\psi(z)c_2}$$\nwith $H^\\kappa(z) = 1 - \\kappa + \\kappa H(z)$\n\n---------\nAdvantages of our approach\n---------\n\nThis approach avoids using the Gaussian Equivalence property, providing finer control over finite matrix approximation errors. While we must restrict $\\tau_A$ and $\\theta$ magnitudes and take expectations over $W_0$ and $\\zeta$, this allows us to:\n1. Better understand finite matrix effects\n2. Explore different target functions than Ba et al. and Moniri et al.\n\nWe are happy to represent the corresponding results for the Signal Plus Noise case and the missing details. We presented a shortened version for brevity. \n\n----------\n\nWe are currently still working on the revision and will post it shortly."
            }
        },
        {
            "summary": {
                "value": "The authors analyze the generalization properties of spiked covariate models. The theoretical analysis is motivated by recent works on two-layer networks trained with a single gradient step that showed how the feature matrix possesses different spikes associated with the learning rate scaling used in the optimization step. The proof scheme uses tools coming from random matrix theory that enables the asymptotic computation of the generalization error. The theoretical claims are accompanied by coherent numerical illustrations."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper is nicely written. The mathematical claims are correctly phrased and the numerical illustrations are coherent with the main text. The research problem is relevant in the theoretical machine learning community."
            },
            "weaknesses": {
                "value": "My main concern with the present submission is the lack of clear elements of novelty. The paper heavily relies on results coming from related works and it restricts their setting in many ways (as fairly reported by the authors at the end of the manuscript). More details are provided below."
            },
            "questions": {
                "value": "As hinted above my main concern on this manuscript is the close relationship with previous works, namely (Ba et al., 2022; Moniri et al., 2024). \n\nCould the authors comment on the link between their results and (Ba et al. 2022) in the context of Gaussian Universality (see e.g. [1]) ? From my understanding of their paper, i.e. a single spike in the feature matrix, they show that in the learning rate regime considered in this paper Gaussian Universality should hold. There is indeed an extensive regime of learning rates after the BBP transition that still falls under the umbrella of Gaussian models, resulting in effectively \"linear\" generalization properties. \n\nOne additional weakness of this submission is the related works coverage. The authors do a great job in covering the random matrix theory literature, while many manuscripts that analyze learned representations with gradient descent with different tools are not properly mentioned, see e.g. [2,3,4]. Although in these works the authors do not focus on the exact asymptotic calculation of the test error, many insights should translate to the present setting. On the other hand, [5] precisely characterize the generalization error using non-rigorous methods; what is the relationship with the present work?\n\nThe results in the present submission should relate directly to the ones in Section 4 of (Moniri et al. 2024), albeit the differences correctly reported by the authors in the two settings. Could the author elaborate on this? \n\nWhat is the bottleneck for the present thereotcial tools to analyze multiple spikes (corresponding to higher learning rate scaling in Moniri et al. 2024)? \n\nClosely related to the above, [5] worked along the lines of (Moniri et al. 2024) to provide the equivalent description in the regime where the spikes recombine with the bulk (maximal scaling regime). Do the authors see a possible extension of their analysis to this scaling? \n\n\n- [1] Hu & Lu 2022, Universality laws for high-dimensional learning with random features. \n- [2] Damian et al. 2022, Neural networks can learn representations with gradient descent. \n- [3] Dandi et al. 2023, How two-layer neural networks learn, one (giant) step at a time.\n- [4] Ba et al. 2023, Learning in the presence of low-dimensional structure: a spiked random matrix perspective.\n- [5] Cui et al. 2024, Asymptotics of feature learning in two-layer networks after one gradient-step."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Motivated by a recent work studying two-layer neural networks (Moniri et al., 2023), the paper studies linear regression under a data model with a spiked covariance (Couillet & Liao, 2022). The spiked covariance consists of a spike component (signal) and a bulk component (noise). Thus, the authors characterize the risk (a.k.a generalization error) with a specific focus on the effect of the spike. They find that the spike does not impact the risk in the underparameterized case. In contrast, the spike introduces an additional term (called \"correction term\") in the risk for the overparameterized case. However, they mention that the correction term is of order $O(1/d)$, which vanishes in the asymptotic case. Thus, the spike does not affect the risk in the asymptotic case but does in the finite case. Then, the authors focus on a case where the targets $y$ only depend on the signal (spike) component of inputs $\\mathbf{x}$ in order to highlight the effect of the spike on the risk. In this case, the correction term depends on the alignment between the spiked eigenvector $\\mathbf{u}$ corresponding to the spike and the target function $\\boldsymbol{\\beta}$. Furthermore, the paper illustrates how the generalization error for this setting exhibits the so-called double-descent phenomenon with a formula for the peak location (a.k.a interpolation threshold)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "* The motivation for this paper is good since the recent line of work studying two-layer neural networks after one gradient step (Ba et al., 2022; Moniri et al., 2023) has received significant attention.\n* The authors precisely characterize generalization errors (risk) for two linear regression problems with spiked covariance data, while the problems differ regarding the target function.\n   + They provide bias and variance decomposition of the risk.\n   + They illustrate the \"double-descent phenomenon\" and provide a formula for the peak location (a.k.a interpolation threshold) of the double-descent phenomenon, which is beneficial for understanding the phenomenon.\n   + The authors specifically focus on the impact of the spike (in the data model) on the risk for different cases. Thus, they show when and how the spike affects the generalization error."
            },
            "weaknesses": {
                "value": "* The presentation in this paper is not good\n  + Although the paper is motivated by Moniri et al. (2023), there are significant discrepancies between the setting of this paper and that of Moniri et al. (2023), as the authors mention in Section 5. While Moniri et al. (2023) considered two-layer neural networks after one gradient step under isotropic data assumption, this work considers linear regression under spiked covariance data assumption. There exists a relationship between these two, but they are not exactly the same. For example, there is a difference between the target $y$ generation of the two settings. Furthermore, $\\mathbf{A}$ (noise component) and $\\mathbf{Z}$ (spike component) are dependent in the case of Moniri et al. (2023), while the dependence is ignored here (see lines 251-255).\n  + Some notations are used without definition (e.g, $\\delta_{\\lambda_i}(\\lambda)$ in Line 126, or $\\Sigma(d_k)$ in Line 147).\n  + There are significant typos in equations. For example, $y$ should be a scaler in Line 76, but it is written as a vector, which makes the equation wrong. Another example is that $l_j$ in Theorem 2 (Line 204) is not defined, and I think the authors meant $l$ instead of $l_j$. A third example is that the function $R_{spn}(c;\\tau,\\theta)$ defined in Line 301 and its usage $R_{spn}(c,0,\\tau)$ in Theorem 3 (Line 317-321) are different in terms of parameters.\n\n* Limited contribution/novelty\n  + Most of the results in this paper are trivial extensions of the results by Hastie et al. (2022) and Li & Sonthalia (2024), which significantly limits the novelty and originality of the paper. Note that Hastie et al. (2022) studied linear regression under a generic covariance assumption with bounded eigenvalues. Here, some eigenvalues can diverge as dimensions go to infinity, but this case is also covered by Li & Sonthalia (2024).\n  + There exists a related work (Cui et al., 2024) that is not mentioned in this paper. Cui et al. (2024) characterized the generalization error (risk) for two-layer neural networks after one gradient step under isotropic data (same setting as that of Moniri et al. (2023)). Although there exist methodological differences between (Cui et al., 2024) and this paper, the motivations are the same, and their settings are similar.\n  + During the review period of this paper, a related work (Dandi et al., 2024) that can be considered as follow-up of (Cui et al., 2024) was appeared on arXiv. While Cui et al. (2024) used (non-rigorous) replica method from statistical physics for their analysis, Dandi et al. (2024) studied the same setting with random matrix theory, which is also the main tool in this paper. Therefore, this paper and (Dandi et al. 2024) studied similar settings with similar methodologies. Note that since (Dandi et al., 2024) appeared after the submission of this paper, I am only mentioning it for the sake of completeness.\n\nOverall, I think this paper should be rewritten with more focus on the impacts of the spike covariance on the generalization error of linear regression, and the new presentation should clearly differentiate the current work from the work by Hastie et al. (2022), Li & Sonthalia (2024), Cui et al. (2024), and Dandi et al. (2024).\n\nCui et al. (2024): Asymptotics of feature learning in two-layer networks after one gradient-step. (ICML 2024)\n\nDandi et al. (2024): A Random Matrix Theory Perspective on the Spectrum of Learned Features and Asymptotic Generalization Capabilities."
            },
            "questions": {
                "value": "1. In Line 178, $F_1$ denotes the case with a single spike (as shown by Moniri et al., 2023). However, Moniri et al., 2023 showed that $F_1$ can include multiple spikes, and the number of spikes depends on the step size of the gradient step. Where is the discussion about the effect of step size in this paper? Similarly, where is the discussion on the impact of $o(\\sqrt{n})$ term for $F_1$?\n\n2. What is $l_j$ in Theorem 2 (Line 204)? Do the authors mean $l$?\n\n3. In footnote 3 (Line 266), the authors say \"... the limiting e.s.d for $F_0$ is not necessarily Marchenko-Pastur distribution ... This difference is not too important, as instead of using the Stieltjes transform for the Marchenko-Pastur distribution in our paper, we could use the result from P\u00e9ch\u00e9 (2019); Piccolo & Schr\u00f6der (2021) instead.\" Why wouldn't the authors directly use the mentioned result directly?\n\n4. Why is there no regularization for the signal-plus-noise problem when there is regularization for the signal-only problem (Line 278-285)?\n\n5. Typo in Line 285: \"We consider on the instance-specific risk.\". Typo in Line 313: \" Then, any for data ...\". \n\n6. Undefined symbols in Theorem 3 (Line 312 - 324): $\\asymp$ and $<<$.\n\n7. How do the authors arrive at \"Hence, we see that if the target vector y has a smaller dependence on the noise (bulk) component A, then we see that the spike affects the generalization error.\" in Line 380? Its connection to the previous part seems to be missing.\n\n8. How do the authors come up with the equation for the peak point of double descent in Line 477? Is it an empirical observation or a theoretical result?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper considers the linear least squares regression for data with simple spiked covariance. They quantify the empirical risk of test data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. They construct two linear regression problems with spiked covariance.\n2. They well explain the previous work of Moniri et al. (2023).\n3. Precise quantification of the generalization errors are also provided for both model."
            },
            "weaknesses": {
                "value": "1. They reference the work of Moniri et al., but this work is unrelated to neural networks or gradient descent; it addresses a purely linear regression problem for data with simple spiked covariances.\n\n2. They do not account for the generalization ability of neural networks after a single gradient step, as they bypass the gradient step entirely by assuming the W1 matrix directly, which does not reflect the full process of neural network training."
            },
            "questions": {
                "value": "1. Could you provide a reference for the statement, 'It has been shown that to understand the generalization...' on line 39?\n2. Is your generalization analysis very different from the work  of Li & Sonthalia (2024)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper analyses the generalization error of linear regression with spiked covariance. Previous literature has been using asymptotic limit of the empirical spectral density to analyse the generalization error of linear regression. At the limit, the effect of the spike vanishes. However, it is not the case for finite sample size. This paper fills the gap by showing there is a correction term for finite sample size $n$."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper provides a detailed proof of their main theorems with clearly stated definitions. It extends over previous results like [1,2].\n\n\n[1]: Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in highdimensional\nridgeless least squares interpolation. Annals of statistics, 50(2):949, 2022.\n[2]: Xinyue Li and Rishi Sonthalia. Least squares regression can exhibit under-parameterized double\ndescent. Advances in Neural Information Processing Systems, 2024."
            },
            "weaknesses": {
                "value": "However, this paper has some obvious weaknesses:\n\n1. The paper is motivated by the spiked covariance from the one-step gradient feature learning in neural networks (Section 1). However, it did not show how the results can be applied to the feature learning scenario. I question the amount of contribution this paper provides.\n2. The assumption in line 2221-222 and 253-255 is too strong. The analysis breaks down if there is dependency in the cross term. However, the paper did not show how big the difference the predicted result would be when there is dependence in the cross term. It is questionable if the result in this paper is applicable in realistic machine learning settings.\n3. This paper has problems with the wordings, even in main theorems. This makes the reading difficult. For instance:\n\n> Theorem 3 (line 313): ...Then, any for data $X\\in\\mathbb{R}^{n\\times d}, y\\in\\mathbb{R}^n$ from the signal-plus-noise model that satisfy: $1\\ll \\tau\\_{A_{trn}}^2,\\tau_{A_{tst}}^2\\ll d, \\theta_{trn}^2/\\tau_{A_{trn}}^2<<n, \\theta_{tst}^2/\\tau_{A_{tst}}^2 << n_{tst}$. Then for $c<1$,...\n\nThe first sentence needs to be rephrased and the symbol $\\ll$ is not consistent. Also, there are some typos like:\n> line 350: Hence the spike has does not have an effect...\n\n> line 372, ... we see an affect that..."
            },
            "questions": {
                "value": "Regarding the weaknesses mentioned above, I would like to ask:\n\n1. What novel results could the authors conclude in the feature learning setting in neural networks using the main theorems 3,4?\n\n2. How could the authors show the assumption on the dependence does not affect the result? Is there any experimental validation?\n\n3. From Figure 4.1, we can see that the effect of the spike correction term is small when $n$ is large. Is the main theorem still useful to explain the phenomenon we see from feature learning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Motivated by the problem of training the readout of a two-layer network after on large gradient step on the first layer, the authors consider the problem of linear regression on a spiked data model. They provide a characterization of the test error, for two linear target functions, respectively depending on only the spike part or the complete input. They discuss how for the latter, the spike does not asymptotically influence the test error, but does non-asymptotically."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is well written, clearly motivating the study, and describing in simple terms the model considered. Sufficient intuition is provided at most steps of the discussion. The technical results are clearly exposed and sufficiently discussed. Although I did not go through the proof in detail, the technical results seem scientifically sound."
            },
            "weaknesses": {
                "value": "I have a number of concerns related to the technical discussions, and the relation to previous works, which I detail below, and in the question section. These concerns regard the discussion of the main theorems, and not the theorems themselves, although I have not carefully verified the proof. These concerns prevent me from giving a higher score to this submission. On the other hand, I would be very happy to increase my score, were those concerns to be addressed by the authors.\n\n- The authors claim l.083 that Moniri et al. (2023) do not quantify the test error after one gradient step. To the best of my understanding, they do provide such a tight characterization (Theorem 4.5). Could the authors clarify their claim, and emphasize how their work is positioned with respect to Moniri et al. (2023)?\n\n- I find the discussion l.332-355 somewhat confusing, as they discuss the specialization of Theorem 3 for $\\theta=\\tau\\sqrt{n}$. Doesn't this directly contradict the assumption $\\theta^2/\\tau^2\\ll n$ stated in the Theorem? Since this specialization leads to one of the main qualitative results of the paper (namely the spike only affects the test error in non-asymptotic cases), this point would gain to be clarified.  The same holds for l.452 with respect to Theorem 4.\n\n- I believe more intuition about the different scaling considered would help solidify the intuition for the spn case, regarding when the spike matters. In particular, the authors could for instance recall the scaling of the terms $z_i^\\top\\beta_*$, $a_i^\\top \\beta_*$, and emphasize their respective strengths in the different scalings of $\\theta, \\tau$ considered. I am curious if the signal part is much smaller than the second term when the spike has no effect, or if the argument is more subtle."
            },
            "questions": {
                "value": "- In the discussion above 4.1, the assumption $\\theta=\\tau^2 n$ seems again to contradict the assumption $\\theta^2/\\tau^2\\ll n$ in the statement of the Theorem. Furthermore, this seems to correspond to a strong spike regime, how can the authors recover the spikeless results of Hastie et al. (2022) in this case? This might be a misunderstanding on my side, but more discussion would be beneficial.\n\nMinor:\n\n- l.074, 347 : incomplete sentences\n- l.203 I think $\\ell_j$ is not defined\n- In 3.3, more discussion in the main text about why only the unregularized case is considered for the spn case, while generic $\\mu$ is considered for the signal-only model, would be helpful for intuition, whether it is for technical reasons or because it is not interesting."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}