{
    "id": "ONOe6cAE9I",
    "title": "A Generalist Intracortical Motor Decoder",
    "abstract": "Mapping the relationship between neural activity and motor behavior is a central aim of sensorimotor neuroscience and neurotechnology. \nMost progress to this end has relied on restricting complexity: studying specific simple behaviors, in limited subjects, with interpretable computational models. However, current trends in deep learning suggest that modeling a breadth of neural and behavioral data may be both possible and beneficial. We accordingly developed Neural Data Transformer 3 (NDT3) as a foundation model for motor decoding of neural data from intracortical microelectrodes. We pretrained NDT3 with 2000 hours of neural population spiking activity paired with diverse motor covariates from over 30 monkeys and humans from 10 labs. Pretrained NDT3 is broadly useful, benefiting decoding on 8 downstream decoding tasks and generalizing to a variety of neural distribution shifts. However, we find signs that scaling over diverse neural datasets may be challenging, as scaling from 200 to 2000 hours already requires increasing model size to 350M parameters to avoid model degradation, and several downstream datasets scarcely benefit from either data or model scale. We provide two demonstrations that this scaling is at least partially limited by variability in input and output spaces across neural datasets, which pretraining alone may not resolve.",
    "keywords": [
        "Neuroscience",
        "Foundation Model",
        "Motor Decoding",
        "BCI"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "We create a foundation model for decoding intracortical spiking activity and observe saturated scaling in data.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=ONOe6cAE9I",
    "pdf_link": "https://openreview.net/pdf?id=ONOe6cAE9I",
    "comments": [
        {
            "title": {
                "value": "Overall Initial Response to Reviewers"
            },
            "comment": {
                "value": "We appreciate the reviewers\u2019 careful feedback and look forward to a productive discussion period. Globally, reviewers appreciated NDT3\u2019s unprecedented scale and comprehensive experiments. Individually, reviewers commented on NDT3's good motivation and insightful analysis. Critiques focused on three main points:\n\n- _Clarity of writing_: There were concerns with the clarity of writing, with specific feedback concentrated in experiments (Sec 3). We have integrated existing feedback, updating text and Figures 3 and 4. Acknowledging that improving poor clarity should go beyond explicit feedback, we have updated the text throughout Sec 3, and are working on swapping Fig. 4C/D for the simpler supplementary analysis B.1. We hope that with the reviewers\u2019 feedback we can greatly reduce concerns on the clarity of writing by the end of discussion, and we would be glad to receive feedback on other confusions.\n\n- _Modest benefit of best model_: cs3p, xNEo, and USSQ have noted that our work\u2019s contribution is limited because the gain from pretraining over task-specific experts is small. We agree that the gain is limited. However, we would like to point out that the empirical results of our study does not emphasize NDT3\u2019s performance in any given task but rather its competence as a generalist. Further, we view our study as primarily a scientific evaluation of scaling in spiking activity, rather than a new model methodology. We expect our characterization, with its limitations, will be a critical \u201cscaling baseline\u201d to justify and guide future work on large scale pretraining. This framing is supported by the fact that our performance benchmarking is only the first third of our experiments, with the rest of our study providing concrete insights as to why scaling may saturate in our setting despite working at small scales.\n\n- _Insufficient baselines_: USSQ and kGOW noted that other works in the field are not compared to in this work. We agree our baselining is sparser than standard modeling work, given our focus on analysis. Unfortunately, the proposed baselines either have not released code (EIT) or have not released pretrained weights ([POYO, Azabou et al. 23](https://arxiv.org/pdf/2310.16046)), which makes comparison difficult. Further, such comparison may only be robust through external benchmarks given weak reproducibility in the subfield. This is evidenced by the high variability in method performance across the literature. For example, [MtM](https://arxiv.org/abs/2407.14668) reports that NDT2 underperforms NDT1, and [POYO] reports that [EIT](https://arxiv.org/pdf/2206.06131) underperforms NDT1, conflicting with the respective source works. We are only aware of one semi-relevant established benchmark, the [NLB](https://neurallatents.github.io/). Reviewer USSQ suggested comparison with POYO through the NLB, but this is also awkward because POYO\u2019s reported results are not visible on the NLB leaderboard, and the NLB is a benchmark on neural representation learning, not decoding. For now, we have evaluated NDT3 on the new, directly appropriate FALCON benchmark to assist future comparisons to NDT3. We are also now investigating the use of pretrained NDT2 as a baseline.\n\nChanges have been posted to the main PDF. Each reviewer has raised important clarification questions, which we address in individual responses. We have currently committed to two additional analyses (pretrained NDT2 as a baseline, and defense of normalization strategy), though we are willing to consider others pending further engagement with the reviewers."
            }
        },
        {
            "title": {
                "value": "Respone to kGOW"
            },
            "comment": {
                "value": "_Overall_: We thank the reviewer for their detailed feedback (and catching of several typos). The two main critiques were the insufficiency of baselines and the poor writing quality. In the detailed response we comment on why it is difficult to compare with the baselines you have requested, but we are beginning experiments comparing with the public pretrained NDT2 model, which we hope addresses the concern on baselines. As for the poor writing, we have amended the typos you have pointed out, and we are happy to receive further feedback on any low and high level area where the reviewer finds the writing confusing so that we might improve the text and overcome this issue of writing quality.\n\n_Weakness - Contribution / Baselining_: We would like to clarify the relationship of the listed models to NDT3. NDT and EIT are single-session models, trained on a single day\u2019s data, and are not foundation models. EIT further does not provide a codebase. POYO introduces a pretraining method and trains a model with up to 100-200 hours of data, but POYO does not release pretrained weights, which makes it difficult to compare against their method without greatly increasing the scope of our work (evaluating scaling on a fairly distinct architecture). Given the challenges to comparing against other works in our own paper, we have reported NDT3 on the newly released FALCON benchmark makes it easy to compare against NDT3 in future works. See also our global response on baselining.\n\n_Weakness - Writing Quality_: We apologize for the typos in the text and have corrected your examples. For L214-215, we are referring to benchmarks, the noun, not the action of evaluating different models. We are saying that there is no third party dataset with fixed preprocessing and private evaluation that many people in the field submit to for decoding, like ImageNet. The NLB benchmark is referenced in POYO, for example, but we believe its use is inappropriate (see response to USSQ for details). We are not claiming that NDT3\u2019s architecture is superior to NDT2 in general, only that it is easier to scale to decoding over large amounts of heterogeneous data for pretraining. However, the fact that from scratch NDT3 achieves better decoding than from scratch NDT2 across our extensive evaluation does show that it may be easier to train high performance decoders with NDT3 than NDT2.\n\n_Question - Why Change NDT2?_: We changed the architecture from NDT2 to the causal transformer in NDT3 for two reasons. First, it allows us to use one model to decode data of different dimensionalities without needing different readout layers. A similar approach is used in other generalist models like [TDM](https://arxiv.org/abs/2305.10912). Second, FlashAttention2, which allows several-times faster pretraining, does not support unconventional masking strategies. The open source implementation of NDT2 relies on a shuffling strategy that requires such an unconventional attention mask and is not easily compatible with FlashAttention 2\u2019s requirements.\n\n_Question - Why broken lines?_: Yes, this is attributed to an explosion in loss. This occurs in several of our pretrained models but is also not uncommon in pretraining overall (we believe), and generally is not known to degrade the quality of the outcome models at convergence.\n\n_Question - Dimensionality > 2_: NDT3 is pretrained on data from many different behavioral tasks. While some tasks are 2D, as you mention, other tasks, like when decoding recordings from different muscles in EMG, have many more target dimensions to decode. You can see these examples in Fig 2B. There are also high-dimensional kinematics, as in FALCON H1, which is 7D robotic upper limb control.\n\nWe hope these answers address some confusions and again, welcome further critiques on writing quality in the new section on experiments."
            }
        },
        {
            "comment": {
                "value": "_Weakness - Pretrained NDT2_:  1. Documentation: Thank you for pointing this out. As you mention, NDT3 is a successor of NDT2, and is also a standard Transformer, so there is not much room or perhaps need for technical innovation in the architecture. The primary difference is that NDT2 uses MAE (Masked-autoencoder) dropout of its neural inputs, while NDT3 is a next-step model. A list of more specific technical advances over NDT2 is provided in C.6, but we agree that a detailed comparison of model preparation would be useful, and will add this in the appendix shortly.\n\n2: Baselining: We did not collect pretrained NDT2 results as it was designed to show feasibility of scaling through unsupervised learning, not decoding, and from-scratch NDT2 under-performed from-scratch NDT3 on our evaluation datasets, providing no reason to believe that the pretrained NDT2 would perform better than pretrained NDT3. We agree though, that it provides important context, and will collect these results in the coming days. \n\n_Weaknesses - Novelty of Generalization_: We agree, certainly, the topic of pretraining large models is active work in many subfields of neuroscience. However, different neural data modalities serve widely different communities at this point in time, and spiking data is central enough so as to have a significant audience if a foundation model were successful. Further, Zhang et al\u2019s study of generalization appears most like Sec 3.1, showing broad generalization of the pretrained model when fine-tuning to different downstream tasks. However, Sec 3.2 and 3.3 differ from this general foundation model narrative in that we study generalization of fine-tuned representations, in that we evaluate generalization downstream."
            }
        },
        {
            "title": {
                "value": "Response to USSQ"
            },
            "comment": {
                "value": "_Overall_: We appreciate the reviewer\u2019s desire for rigor in the evaluation of NDT3. We agree that further baselining provides important context for the ultimate benefit of the NDT3 model, and will work to include baselines from the pretrained NDT2 checkpoint in this discussion phase. Considering that full-scale evaluations of new methods (even basic DNNs) are fairly expensive, we agree that external benchmarks are the natural place to compare additional methods, and will discuss these in detailed response. We also agree that the technical novelty and empirical benefit in this work is limited, but our contributions advance beyond model advances and also provide analyses showing where we can expect benefits currently and directions for future improvement (see global response to limited benefit).\n\n_Weakness - Limited benefit: As mentioned, we agree with the assessment of limited benefit, which motivated our framing of the model as a generalist that performs many tasks well rather than setting records on every task. Generalists are similarly limited in other fields ([GATO](https://arxiv.org/abs/2205.06175), [RT-X](https://arxiv.org/abs/2310.08864). We also want to emphasize that limited gains from scaling are not unprecedented. For example, Fig 4. In [Entezari 21](https://arxiv.org/pdf/2302.13602) shows that several downstream tasks don\u2019t benefit significantly from orders of magnitude more images in pretraining. Given this context, and given the extent of our other experiments, we hope to convey that limited performance is more an interesting challenge for the field rather than a failing of our methodology.\n\n_Weakness - Practical cost: We agree that larger models are in general harder to use, but 350M parameter models are small for modern machine learning (BERT/GPT2 were this size in 2018) and represent the starting point for most works in foundation modeling. Their practicality can be demonstrated by the fact that consumer GPUs can tune models an order of magnitude larger, with the right tooling. We do believe the necessity of scaling to larger model sizes is a concern, but we believe it is a concern for the field rather than a weakness of our method.\n\n_Weakness - Lack of baselines: We chose to sacrifice baselines used so as to try to instead maximize the number of datasets we evaluated on (we tuned over 2000 models). Nonetheless, we agree with the reviewer that this additional context is important, even if only partially evaluated. To this end, at least for basic baselines, RNNs are already evaluated on the FALCON benchmark, where they uniformly underperform multi-session NDT2 models (see appendix of the [FALCON preprint](https://www.biorxiv.org/content/10.1101/2024.09.15.613126v1) and current [leaderboard](https://eval.ai/web/challenges/challenge-page/2319/leaderboard/5747). We agree that the current scaling trends may not generalize to other foundation models, namely pretrained NDT2 or Azabou et al\u2019s POYO. However, each such assessment requires substantial resources, which we chose to instead allocate to the more detailed analyses in Fig 4 and 5.\n\n_Weakness - NLB: We believe this is an important nuance that has been confused both in the field. NLB is a representation learning benchmark, and decoding performance on the leaderboards is evaluated by remote training of a linear probe on submitted neural data firing rate predictions. In contrast, our evaluations directly predict the behavior, without explicit neural data modeling. Therefore it is somewhat inappropriate to evaluate NDT3 with NLB. In contrast the used FALCON benchmark directly evaluates behavior, though it is new and other models have not been tested on this benchmark yet. \n\nFurther, we believe that Azabou et al. used the datasets from the NLB but preprocessed them for their own analysis and conducted private evaluation, which defeats the purpose of the benchmark. For example, there is no submission for POYO on the [NLB leaderboards](https://eval.ai/web/challenges/challenge-page/1256/leaderboard/3188), even though the reported score is almost 5% higher than other methods. Since we cannot directly compare to Azabou\u2019s reported numbers in the official benchmark framing, and there is no pretrained weight available for POYO, we would have to train and tune POYO ourselves to make a comparison based on architectures. Beyond POYO, we are only aware of pretrained NDT2 and [MtM (Zhang et al 2024)](https://ibl-mtm.github.io/)) for motor decoding. POYO does not release pretrained weights, as far as we know. While MtM releases pretrained models, it is trained on mouse data outside the context of BCIs, and was released in July 24 (contemporary work).\n\n(Response continued in thread)"
            }
        },
        {
            "comment": {
                "value": "_Q - Joint vs Target_: This is an important concern, though we believe Fig 12 (11 in old PDF) already compares a very similar setup, since we use a single test session there. The only difference from your request is that we do \u201csequential\u201d tuning vs targeted fine-tuning on single-sessions. Sequential fine-tuning differs from the targeted fine-tuning that you ask for in that instead of completely discarding the data from other sessions, we first tune on those data before finally tuning on the final dataset, which achieves a higher performance as in ([Gururangan 20](https://arxiv.org/abs/2004.10964)). Fig 12A reconfirms that tuning for a specific target day alone (left-most point at \u201c0-hours\u201d on the x-axis) performs worse than if we jointly tune with data from other days (other points in panel 12A Cross-Session).\n\nFig 12 previously already showed that joint tuning is not systematically different from sequential tuning when the tuned data when models are pretrained and downstream data is heterogeneous (cross-subject and shuffled conditions). We\u2019ve now added the comparison between joint and sequential tuning for cross-session data, which is closest to our standard evaluation setting. Here, sequential models uniformly perform slightly worse than the jointly tuned models. Thus we conclude that joint fine-tuning is a reasonable method and not limiting our scaling trends. \n\n_Q - Task imbalance_: We chose model selection according to best validation loss on our pretraining data, where our validation split is randomly sampled from pretraining (not biased for any given evaluation). We believe this is a reasonable default that matches standard practice for large scale model pretraining. We agree with your concern that some tasks may fit and possibly overfit earlier than others. In general, dynamics in different downstream tasks are expected to vary, as shown in [Pythia](https://arxiv.org/pdf/2304.01373) and [Yang 24](https://arxiv.org/pdf/2404.01204). However, we are unclear how this imbalance might be resolved without new methods, and we would appreciate any references the reviewer has on this topic. Could the reviewer clarify if there is specific insight they hope to gain from evaluating imbalance in NDT3 specifically? We have not saved in-progress checkpoints and so would need to redo pretraining to perform this analysis."
            }
        },
        {
            "title": {
                "value": "Response to xNEo"
            },
            "comment": {
                "value": "_Overall_: \n\n_Weakness - Other factors_: These are interesting points. We discuss them in their corresponding questions (Q1 and Q3) below.\n_Weakness - Clarity_: We appreciate this feedback. We have decreased the number of models reported in the main panels in Figure 3 and believe the colors are now easy to distinguish. The colors throughout Figure 3 now use the same palette, so they still refer to pretraining volume. We are happy to receive other feedback on writing and figures.\n\n_Q - Upper bounds_: As you mention, there is no clear way to test the info-theoretic limit on possible decoding performance. Good deep networks are the current empirical strategy for estimating this limit. We want to distinguish two types of info-theoretic limits. First, there is the [Bayes error](https://arxiv.org/pdf/2110.02095) due to variability in a given neural dataset, timepoint by timepoint, which we believe you are referring to. This is a limit similar to failing to classify an image because it is too blurry. We can estimate this upper bound by scaling training data in our target distribution. For example, this \u201ctask-inherent difficulty\u201d cannot be the issue limiting our performance because scaling downstream data continues to improve performance on held-out timepoints in all models. We have calibrated the size of the fine-tuning datasets in our own evaluations so that we are not saturating them due to Bayes error. On the benchmark evaluation of FALCON M1, in contrast (Fig 11, rightmost panel), decoding performance in many models are essentially equivalent. \n\nSecond, there is a more subtle limit in how quickly a generalist can align to a test setting. Because the generalist must perform many tasks, we might not be able to be more data efficient in fine-tuning because of variability in the space of potential downstream tasks. This is why some tasks need lengthy prompts for language models to perform well, and is related to the task posed to toy Transformers performing in-context learning of linear regression ([Garg 22](https://arxiv.org/abs/2208.01066)). If the observations are noisy, 2 points may not be enough to fit the line. A final reference is a discussion of how Transformers can be shown to be computing this task recognition on this post by [Adam Shai](https://www.lesswrong.com/posts/gTZ2SxesbHckJ3CkF/transformers-represent-belief-state-geometry-in-their), which mentions computational mechanics as a formal framework for this problem. We have discussed this distinction qualitatively in the introduction in L68-78 and at the end of Sec 3.1.\n\nIf the reviewer is interested in a specific experiment to understand how different models overcome this second limit, we would recommend using the cleaner toy settings as studied in Garg 22. Neural data adds the additional complexity of unknown, temporally evolving noise.\n\n_Q - Normalization_: We chose to max-norm over z-scoring because our data is not always unimodal. Our human data often has bimodal behavior (a cursor click signal being off or on). Further our data sometimes contain extreme outliers from sensor errors, which we want to ensure doesn\u2019t disrupt model training. Normalizing by max value would effectively discard these points, while z-scoring is less robust to such outliers. We do not believe this choice impacts scaling, as normalizing data to a fixed small range is standard in machine learning, as long as the loss scale is reasonably adjusted. In practice the dynamic range of the normalized data is spanned by most points in pretraining and evaluation, so we also do not believe this choice of normalization impacts task balance. We are working on plots to demonstrate this point. Further, though this doesn\u2019t exactly address your concern, NDT2 uses z-score normalization and our method compares favorably in direct comparison.\n\n(Response continued in thread)"
            }
        },
        {
            "title": {
                "value": "Response to cs3p"
            },
            "comment": {
                "value": "_Overall_: We thank the reviewer for their responses. Given several comments on lack of clarity, we especially appreciate your suggestions for specific figures.\n\n_Weakness - Lack of clarity_: We agree the writing is currently dense and perhaps overly technical. We have updated both figures according to your suggestions. We were concerned about technical completeness in Figure 4 (indeed the specific issue on joint vs sequential tuning is asked about by reviewer xNEo), but we appreciate that this was distracting in the main text. We believe these higher level comments are valuable feedback and would welcome pointers to other places where the flow is distracted.\n\n_Weakness - Limited utility_: We agree that cross-subject use is a primary goal for NDT3. Our evaluations were designed entirely around cross-subject evaluation. Our primary evaluation shows that downstream benefit is variable but overall positive. We believe that this implies the potential for off-the-shelf improvements in at least some portion of the BCI community. On the other hand, we agree NDT3 may not be ready to be a general purpose foundation model for the field, but do believe our experiments highlight important challenges that might be informative for the Neuro-ML comunity.\n\n_Q - Non-neural tokens_: In inference, behavior input tokens are zeroed and phase and return tokens are removed entirely. This removal doesn\u2019t impact positional embeddings of the remaining tokens, as position embeddings are identifying only within each timestep (rotary embeddings are used across time), and to that end, specific ranges of position IDs are reserved for data of different modalities. For example, positions 10-20 are reserved for neural data, position 21 is reserved for return, and positions 21-36 are reserved for behavior. This factorized embedding strategy is described in Sec C.6. We have updated Fig 2 to distinguish the zero-ing vs dropping of tokens.\n\n_Q - Zero-padding_: We concatenate segments to increase the throughput of real data seen in the model. This strategy is standard in language model training and we mention in C.2 that Geiping and Goldstein 2022 has shown this preprocessing choice has little performance impact in language. A full comparison in the neural data domain would be helpful but would require training with the more expensive padded preprocessing.\n\n_Q - Statistics_: Thank you for pointing this out. We\u2019ve now measured significance using paired t-tests with FDR correction, finding that the largest model has statistical significance over all other models except the 350M 200 hr model. Differences between other pretrained models are largely nonsignificant. We\u2019ve highlighted some tests now in the main text and included a full significance matrix in the appendix (Fig 13). Since we are aggregating tasks of widely different performance (e.g. all data points in Fig 1B), we\u2019ve omitted the resulting wide CIs to avoid confusion. We believe this omission is standard in multitask evaluations. For example, on page 46 of the PaliGemma vision-language [model report](https://arxiv.org/pdf/2407.07726), extensive experiments yield average performance benefits across tasks on the order of 2-3%. Nonetheless, CIs derived from these data also overlap much like our own. We would welcome feedback from the reviewer on how to present this issue if they have insights.\n\n_Q -  Cross-entropy over Poisson_: In early experiments, we found this choice did not greatly impact our evaluations and chose the cross-entropy since it is a more broadly common and expressive model, even if Poisson losses are common for neural data models. We mention our existing justification of this choice in C.6: an ablation in Fig 8C shows that ablating the neural loss entirely has minimal impact on evaluation performance. Due to this result, we did not specifically ablate the choice to use the cross-entropy loss. However, we have run a new scaling analysis to support that the cross-entropy loss is nontrivial, labeled Fig 9 in the revision. We tested pretraining with only the unsupervised cross-entropy loss up to 200 hours of data, and fine-tune on downstream tasks, still in a supervised fashion. The fact that there is scaling of downstream performance implies the neural objective is useful. The MSE behavioral objective may obscure differences in the neural objective, though. We\u2019d recommend future work studying specifically neural representation learning, as opposed to decoding, explore this choice in more detail. Further, yes, there is a coefficient to balance MSE and cross-entropy loss to roughly equal orders of magnitude. We experimented with this briefly and found no consistent impact within an order of magnitude of changes on these coefficients.\n\n_Q - FEF/MT_: Yes, throughout our work, monkey evaluations were conducted on subjects held out from pretraining entirely, with the intention of evaluating cross-subject capability. This includes the S1/FEF/MT experiments."
            }
        },
        {
            "summary": {
                "value": "This paper introduces NDT3, a foundation model for motor decoding from spiking activity. NDT3 is trained on large scale and diverse motor tasks and subjects, demonstrating benefits of scaling law in some scenarios. The paper also reports cases where the model fails to generalize to downstream tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The NDT3 model is original, extending NDT2 with architectural changes to accommodate pretraining on large scale and diverse datasets.\n* The paper shows extensive experiments to evaluate the model and was open to discuss failure modes of the model.\n* The method is well motivated. Building foundation models for neural data has great implications for BCI and neuroscience applications in general."
            },
            "weaknesses": {
                "value": "* My main concern about the paper is its lack of clarity in writing. The paper seems to focus a little too much on technical details and verbose explanation, making it hard to follow the main points sometimes. For example, lines 371 to 376 briefly mention joint tuning and sequential tuning, which are minor technical details of fine-tuning choices and do not largely contribute to the main point being discussed in the paragraph which is about input order sensitivity in cross-subject transfer. It might be better to show one type of tuning and leave the other in the Appendix to maintain the flow of writing, which would also help make the plots in Figure 4 cleaner with key takeaways only. Another example is Figure 3C to 3E, where it might be better to only show the 1.5hr, 200hr and 2khr models to avoid cluttering in the plots while still convey the main points.\n* Cross-subject transfer which is the main use case of neuroscience pretrained models does not seem to be promising with the proposed model. The usefulness of such foundation model to the community therefore might be limited."
            },
            "questions": {
                "value": "* Figure 1: are non-neural tokens removed entirely or replaced by zeros tokens? If they are removed entirely from the sequence, would neural tokens assume new positional embeddings left by the non-neural tokens?\n* Line 156: why wouldn\u2019t segments be padded with zeros rather than being concatenated with another partial segment to form the two-second cut?\n* Figure 3D: what the errorbars would look like if plotted? Pretrained models seem to not differ much by their mean $R^2$. In the absence of errorbars, it\u2019s hard to know if it\u2019s worth scaling up the data and model size for additional performance if the differences are not statistically significant.\n* Line 181: why cross-entropy loss was used instead of Poisson likelihood loss that is traditionally used for spike counts? Is there a coefficient balancing MSE loss and cross-entropy loss?\n* Figure 5D: was the test monkey with S1/FEF/MT recordings seen during pretraining, i.e. the S1/FEF/MT data might have been recorded from a subject whose motor cortex recordings were present in the pretraining set? If not, this would be the case of cross-subject transfer?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a large-scale model that has been pretrained on a large corpus of spiking activity collected from 30 monkeys and humans across 10 labs during diverse motor tasks. It presents a multimodal transformer that tokenizes both neural activity and behavioral data into token streams, allowing the causal decoding of behavior tokens from neural tokens. The findings highlight the challenges of scaling across diverse neural datasets, showing that increasing both model size and data volume can prevent degradation during pre-training. Additionally, the paper suggests that scaling is constrained by variability in input-output relationships, which pre-training alone may not fully resolve."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper attempts to address an important question of whether pretraining large-scale models can yield field-wide improvements for neuroscience.\n2. While the model architecture itself may not be novel, there is a lack of effective large-scale multimodal models in this field, and this paper effectively fills that gap.\n3. The comprehensive evaluation of how pretraining affects the decoding of diverse motor tasks from datasets of humans and monkeys offers valuable insights for the neuroscience community."
            },
            "weaknesses": {
                "value": "1. The author observed scaling difficulties, suggesting they may come from variability in datasets. However, I think there are other factors worth considering. For example, model performance may saturate when there is not enough information about behavior in the neural data, and the gain provided by pre-training has less room for improvement. Another factor may be that different behavior modalities/tasks have different training dynamics s.t. the model selected for final evaluation may be good at some tasks but not the others. Thus, it may not be that the model cannot scale across heterogeneous datasets, but that the model has trouble balancing the optimization of different modalities/tasks. \n2. Although I don't assess a paper based on its writing and figures, I feel that clearer writing and more understandable figures could better deliver the paper's message. These aspects do not impact the overall quality of the work, but would certainly make it easier for readers to grasp the content. For example, in Fig 3C, it's hard to distinguish different pre-training volume by colors. In Fig 3E, what do the different colors mean?"
            },
            "questions": {
                "value": "I will probably increase my scores if the author can address the questions below:\n1. This paper argues that pretraining may be constrained by inherent variability within the neural datasets. Did the author explore other possible explanations for this issue? Given that the model is evaluated on decoding tasks, could it be that certain tasks are inherently easier, leading to performance saturation with little room for improvement? For example, if there is only a limited amount of decodable information about behavior in the neural data, then scaling can not allow us to go beyond that information upper bound. **This is just a hypothesis that is hard to test empirically, but could the author suggest a specific analysis or experiment to test this hypothesis about information upper bounds about some tasks in the neural data?**\n2. The author mentioned that \u201call behavior is normalized per dataset so that the maximum absolute value of each variable is one.\u201d I wonder whether this preprocessing step might influence model training. For example, the small values of all behaviors could lead to a low loss value, which might hinder backpropagation and parameter updates, preventing the model from effectively learning to optimize behavior prediction. Could this be a contributing factor to the scaling challenges? Additionally, why doesn\u2019t the author consider alternative preprocessing methods, such as using the mean and standard deviation to balance the behavior tasks? **Could the authors provide justification for their chosen normalization method, or to conduct an ablation study comparing different preprocessing approaches?**\n3. In the evaluation section, the author stated, \u201cTo manage compute and storage demands and to reflect that real-world datasets are rarely collected or analyzed in isolation, we fine-tune NDT3 jointly over data combined from multiple evaluation sessions.\u201d I wonder how much this fine-tuning process affects the scaling challenges observed. If the pre-trained model is fine-tuned on multiple, heterogeneous test sets, could this complicate the fine-tuning results? What if the author selected specific test sessions and behavior tasks for targeted fine-tuning? Would that alleviate the scaling difficulties? **Could the author do a simple experiment to quantify the impact of joint vs. targeted fine-tuning on scaling performance?**\n4. What specific criteria did the author use for model selection? A common challenge in multimodal multi-task training is that data from different modalities contain varying amounts of information, and some decoding tasks are inherently easier than others. As a result, when one behavior decoding task reaches optimal performance, other prediction tasks may suffer from overfitting. Could model selection contribute to these scaling difficulties? It\u2019s possible that the pre-trained model has already learned to decode each behavior very well during training, but the selected model for final evaluation may be good at certain tasks but bad at others due to imbalanced optimization. **Could the authors provide more details on their model selection process and consider conducting an analysis of task-specific performance across different model checkpoints to investigate potential imbalances?**"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes large-scale pretraining of a Transformer-based foundation model, called NDT3, for motor decoding. Building on prior work by Ye et al. (2023), the model scales up to incorporate diverse datasets involving both monkey and human motor tasks.  The authors evaluate their pretrained model on eight downstream datasets and find that while some downstream tasks benefit from extensive pretraining, others show minimal improvement. They report that NDT3 model needs to be scaled up to 350M parameters to avoid performance drops when pretrained with 2000 hours of data. The authors also hypothesize that the poor generalizability may stem from the input-output shift, highlighting challenges in developing foundation models for neural data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Extending the scale of foundation models pretraining for neural population spiking activity. The authors have used very large amount of data (2000 hours) for pretraining a generalizable model for motor decoding. This represents a valuable step forward in investigating the effectiveness of foundation modeling approaches for motor decoding, and more broadly, for neuroscience applications.\n\n- Building on prior research, the authors perform analyses to offer insights into scenarios where foundation models may be more effective. Understanding when and why these approaches are beneficial (or may fail) can illuminate domain-specific challenges for motor decoding."
            },
            "weaknesses": {
                "value": "- The main contribution of this work lies in extensive pretraining of large foundation models (45M or 350M parameters) using substantial amount of neural-behavioral data for better generalizability.  However, the performance gains from this heavy pretraining seems minimal and task-dependent. In several cases, the authors report that pretraining NDT3 with large datasets yields only minor improvements on downstream tasks. While the in-depth analysis of why generalizability has been limited is insightful (as outlined in the strengths), the results overall suggest minimal progress towards the goal. For example, in Fig. 3D, increasing the pretraining data volume from 200 hours (even from 1.5 hours) to 2000 hours only slightly improves the average downstream decoding. \n\n- The authors find that to benefit from pretraining with up to 2000 hours of data, the model\u2019s capacity needs to be significantly increased to 350M parameters. The goal of this extensive pretraining is to facilitate easy downstream use. However, finetuning such a large model entirely can itself be of significant cost. Therefore, it remains questionable whether or not scaling the model with data provides a practical solution. \n\n- Lack of sufficient baselines: NDT3 has been primarily compared to itself under different data regimes or training from scratch. The authors additionally compare it to two baselines trained from scratch: 1) the predecessor model NDT2, 2) a very simple linear method (WF). These baselines seem insufficient for careful evaluation of NDT3. More expressive baselines such as MLP or RNNs should be added to strengthen the results. Additionally, without comparisons to other foundation models, it\u2019s unclear whether the generalization challenges observed are unique to NDT3 or representative of foundation models for neural decoding more broadly. For example, comparisons with the foundation model in Azabou et al., 2023 will be needed, at least on the Neural Latent Benchmark. See next. \n\n- The authors note that: \u201cwe avoid the Neural Latents Benchmark (Pei et al., 2021) as it does not directly measure decoding performance\u201d.\nI did not follow the above reasoning. Decoding is also assessed on this benchmark (NLB) it seems. Could the authors clarify how NLB differs from the evaluated datasets? It\u2019s understandable that comparing to other foundation models may be challenging due to the lack of publicly available pretrained models. Nonetheless, aren\u2019t there any other foundation models with available code and pretrained weights for comparison? For example, Azabou et al., 2023 evaluated their method (in terms of behavior decoding) on NLB datasets with motor tasks similar to those analyzed here. If benchmark comparisons on the authors\u2019 data is not possible, NDT3 should be compared to such approaches including Azabou et al., 2023 on common downstream datasets such as NLB. \n\n- Novelty compared with NDT2: Novelty compared with NDT2 seems limited and the model differences compared to NDT2 (if any) are not discussed in detail. It is not clear if NDT3 is the same model (potentially with more parameters) just pretrained with more data. In fact, the smaller NDT3 with 45M parameters seems to have comparable capacity to the NDT2 models used (20M and 72M parameters).  Please provide a detailed comparison of the model architectures, training procedures, and key innovations of NDT3 compared to NDT2. This would help clarify the extent of the novelty in NDT3. Also, related to the previous point, why is pretrained NDT2 not provided as a baseline in the main figures? Please provide this in the main figures. As NDT2 is also a foundation model and its pretrained weights are available, providing its finetuned downstream performance is a more informative baseline than the one trained from scratch e.g., in Fig. 3D and Fig. 10. Does pretrained NDT3 outperform pretrained NDT2? Please provide these in main figures and discuss.\n\n- Novelty in terms of generalizability results: The utility of large-scale pretraining of foundation models and their transferability to downstream tasks for neural data (although not spiking activity) has been shown before. For example, Zhang et al., 2024 have proposed a foundation model for SEEG data with even larger pretraining data than NDT3. Similar to NDT3, Zhang et al., 2024 also evaluated the effect of model size on generalizability and showed improved performance with higher-capacity models. Therefore, the idea and conclusions about generalizability of foundation modeling with more brain data are not novel in the field.\n\nReferences:\n\nJoel Ye, Jennifer L Collinger, Leila Wehbe, and Robert Gaunt. Neural data transformer 2: Multi-context pretraining for neural spiking activity. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=CBBtMnlTGq.\n\nMehdi Azabou, Vinam Arora, Venkataramana Ganesh, Ximeng Mao, Santosh B Nachimuthu, Michael Jacob Mendelson, Blake Aaron Richards, Matthew G Perich, Guillaume Lajoie, and Eva L Dyer. A unified, scalable framework for neural population decoding. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=sw2Y0sirtM.\n\nDaoze Zhang, Zhizhang Yuan, Yang Yang, Junru Chen, Jingjing Wang, and Yafeng Li. Brant: Foundation model for intracranial neural signal. Advances in Neural Information Processing Systems, 36, 2024.\n\n\nMinor/questions:\n- What datasets, pretraining, and analysis do the results in Fig. 1B correspond to? I could not find that information in the manuscript. Also, in this figure, it seems the pretrained model is falling short of scratch model when abundant task data is available. How should that be interpreted?\n\nErrors:\n- The main pdf file does not include the appendix. The complete document is provided as a supplementary pdf file.\n- References are not listed in alphabetic order.\n- Line 76: typo \u201cmotion\u201d\n- Line 86 parenthesis missing in \u201c(Section 3.3)\u201d\n- Line 107: should be Fig. 2B not A\n- Citations, Figs, etc don\u2019t have links.\n- Line 1272. Typo: \u201cthe\u201d\n- Line 1112: typo: \u201cshow\u201d"
            },
            "questions": {
                "value": "My questions are the ones stated in weaknesses and Minor/questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors developed Neural Data Transformer 3 (NDT3), a neuroscience foundation model for decoding movements. They pretrained NDT3 using 2,000 hours of neural data, paired with various motor target labels, from over 30 monkeys and humans across 10 labs. Following this, they demonstrated the limitations of the pretrained foundation model on certain downstream datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This is the **largest dataset** used so far. A highly relevant previous study (Azabou et al., NeurIPS 2023) utilized 100 hours of recordings from 7 monkeys, whereas this paper includes 2,000 hours of recordings from 30 monkeys and humans. With this unprecedentedly large dataset, the authors found that scaling from 200 to 2,000 hours of data necessitates increasing the model size from 45M to 350M."
            },
            "weaknesses": {
                "value": "**Contribution of NDT3:**\\\nThe primary contribution I gather from this paper is the limitation of scaling when dataset size increases. To my knowledge, there are at least six foundation models in the neuroscience field (listed by year):\n\nNDT, Ye and Pandarinath, NBDT 2021\\\n**EIT**, Liu, ..., Dyer, NeurIPS 2022 (monkey motor tasks)\\\nNDT2, Ye, Collinger, Wehbe, Gaunt, NeurIPS 2023\\\n**POYO**, Azabou, ..., Dyer, NeurIPS 2023 (monkey motor tasks)\\\nWang, ..., Tolias, bioRxiv 2023/2024 (mouse visual cortices)\\\nZhang, ..., Dyer, Paninski, Hurwitz, arXiv 2024 (mouse brain regions)\n\nNDT3 has only been benchmarked against NDT2 and the Wiener Filter (WF) but not against two highly relevant baselines: **EIT and POYO**.\n\n**Writing Quality:**\\\nThe writing in this paper is poor, making it challenging to read due to some typos in the text.\n\nExamples when referring to Figures:\\\nL107: There is no left or right panel in Fig. 2A; it should be Fig. 2B.\\\nL466: It should be Fig. 5B instead of Fig. 5C.\\\nL479: There is no Fig. 5D; it should be Fig. 5C.\n\nA difficult-to-understand sentence:\\\nL214-215: \"Other Transformer variants have been proposed for pretraining on spiking data (Azabou et al., 2024; Zhang et al., 2024), but the field yet lacks consensus benchmarks to distinguish the most promising proposal to scale.\"\nDoes \u201cbenchmark\u201d here imply that the authors plan to benchmark their Causal Transformer against other Transformer architectures? In this study, they only benchmarked NDT2, which used the MAE structure. However, is the performance difference due to the different structures or different data sizes?\n\n**Minor Issues:**\\\nL200: Extra word: Prior work (Azabou et al., 2024; Ye et al., 2023; Zhang et al., 2024) **ran** focused evaluations by tuning...\\\nL277-278: Missing parentheses: ...that increasing model size and dataset size in tandem is important for performance **(**Dosovitskiy et al., 2021; Kolesnikov et al. 2020; Aghajanyan et al. 2023**)**\\\nL392: Remove the extra parentheses: ...which fail completely **(**(Rizzoglio et al., 2022)**)**.\\\nL482: Missing a word: While the former can be attributed **to** the close interaction of sensorimotor areas..."
            },
            "questions": {
                "value": "Why was the MAE Transformer structure in NDT2 replaced with the Causal Transformer in NDT3?\n\nWhy are the red lines broken in Fig. 3A and 3B? Does this indicate that R\u00b2 suddenly dropped between 100 and 200 epochs?\n\nRegarding the dimensionality of movement covariates (Fig. 2A), why could it be as high as 10? My understanding is that if a hand is moving on the XY plane, the dimensionality would be 2. Can a hand move in 10 dimensions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}