{
    "id": "XCugWIuHR8",
    "title": "Convex Distillation: Efficient Compression of Deep Networks via Convex Optimization",
    "abstract": "Deploying large and complex deep neural networks on resource-constrained edge devices poses significant challenges due to their computational demands and the complexities of non-convex optimization. Traditional compression methods such as distillation and pruning often retain non-convexity that complicates fine-tuning in real-time on such devices. Moreover, these methods often necessitate extensive end-to-end network fine-tuning after compression to preserve model performance, which is not only time-consuming but also requires fully annotated datasets, thus potentially negating the benefits of efficient network compression. In this paper, we introduce a novel distillation technique that efficiently compresses the model via convex optimization -- eliminating intermediate non-convex activation functions and using only intermediate activations from the original model. Our approach enables distillation in a label-free data setting and achieves performance comparable to the original model without requiring any post-compression fine-tuning.  We demonstrate the effectiveness of our method for image classification models on multiple standard datasets, and further show that in the data limited regime, our method can outperform standard non-convex distillation approaches. Our method promises significant advantages for deploying high-efficiency, low-footprint models on edge devices, making it a practical choice for real-world applications. We show that convex neural networks, when provided with rich feature representations from a large pre-trained non-convex model, can achieve performance comparable to their non-convex counterparts, opening up avenues for future research at the intersection of convex optimization and deep learning.",
    "keywords": [
        "Convex Neural Networks",
        "Convex/Non-Convex Optimization",
        "Knowledge Distillation",
        "Model Compression",
        "Label-Free Training",
        "Classification"
    ],
    "primary_area": "optimization",
    "TLDR": "We introduce a convex neural network distillation method that compresses large pre-trained deep networks without requiring labeled data or fine-tuning, combining convex and non-convex model architectures for efficient deployment on edge devices.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=XCugWIuHR8",
    "pdf_link": "https://openreview.net/pdf?id=XCugWIuHR8",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces Convex Distillation, a model compression method that replaces the non-convex layers of deep neural networks with convex approximations. By leveraging convex optimization, the method achieves efficient compression without the need for labeled data or post-compression fine-tuning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Paper is written well and easy to follow.\n\n- The idea of bridging convex optimization, distillation and compression is interesting."
            },
            "weaknesses": {
                "value": "- Practical contributions of convex optimization in model compression are limited. The convexity conversation is only valid and tested up to 3-layer DNNs. It significantly restricts the objective landscape. For simple tasks, it might be fine, while for more complex tasks, it often leads sub-optimal performance. \n\n- Experimental results are not satisfactory to justify the efficacy of the proposed methods. Only small datasets are included. Meanwhile, the ResNet18 baseline seems not well tuned (with low acc less than 90%)."
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This article proposes a novel distillation technique that efficiently compresses deep neural network models through convex optimization. This method eliminates intermediate non-convex activation functions and uses only the intermediate activations of the original model, enabling distillation without the need for labeled data, and achieving comparable performance to the original model without fine-tuning. Experimental results show that this approach not only maintains model performance when compressing image classification models on multiple standard datasets but also performs better and optimizes faster compared to traditional non-convex distillation methods. This work opens up new avenues for future research at the intersection of convex optimization and deep learning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "-  The paper introduces a novel approach to knowledge distillation that leverages convex optimization for efficient compression of deep neural networks. \n\n- The authors provide extensive empirical evidence to support their claims, demonstrating the effectiveness of their method across multiple standard datasets and in various scenarios.\n\n- The author provided Google Colab code with very detailed experimental instructions."
            },
            "weaknesses": {
                "value": "- Using existing convex neural network packages, there is a lack of originality and workload.\n\n- There is an issue with the network configuration. For datasets with small image sizes like CIFAR-10, the configuration used for ResNet on ImageNet should not be applied. It should not downsample by 4x from the start, which results in feature maps that are too small.\n\n- The experiments were only conducted on small datasets and very small networks. Can they be scaled up to larger datasets such as ImageNet?"
            },
            "questions": {
                "value": "See Weakness, is there a more reasonable network configuration, more comprehensive experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a convex distillation method by combining the representational power of large non-convex DNNs with the favorable optimization landscape of convex NNs. The proposed method can distill the models in a label-free manner without requiring post-compression fine-tuning on the training data. Experiments on several image classification datasets show that convex student models can achieve high compression rates without sacrificing accuracy and outperform non-convex compression methods in low-sample and high-compression regimes."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.A simple yet effective to distill classification models via convex networks\n\n2.An effective distillation acceleration tool and polishing are used to improve convex solver."
            },
            "weaknesses": {
                "value": "1.Activation matching is not novel for knowledge distillation.\n\n2.Experimental comparison is not sufficient to support the effectiveness of the proposed method. It lacks SOTA KD methods for fair comparison.\n\n3. It is not clear that how to distill all blocks. If the proposed convex distillation performs block-wise distillation, it requires a complex and time-consuming knowledge distillation for handling the entire networks."
            },
            "questions": {
                "value": "See the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a new distillation method that efficiently compresses the model by convex optimization-eliminating intermediate non-convex activation function."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This manuscript is very clear about the background knowledge and the motivation for undertaking this work is clear."
            },
            "weaknesses": {
                "value": "1.\tIn the section on related work, there is a lack of information on the most recent work, and the related work is introduced too little.\n2.\tThe notation in Eq. 1 and Eq. 3 is used incorrectly ($D_i \\in {D}' \\in \\mathcal{D}_x$).\n3.\tSome of the textual content in the figures is too small.\n4.\tThe innovative content of the article is not sufficient.\n5.\tIn the experimental part, there is a lack of validation results on large datasets such as ImageNet. Also, using only ResNet18 and MobileNet V3 for experiments is not convincing enough.\n6.\tThe results in Fig. 4 do not intuitively show the superiority of the proposed approach.\n7.\tThere is a lack of experiments to compare with other methods, only ablation experiments are performed."
            },
            "questions": {
                "value": "1. Why \u201cwhile DNNs have the capacity to memorize the training dataset, they often end up learning basic solutions that generalize well to test datasets\u201d can \u201cmotivate using smaller, compressed models\u201d?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}