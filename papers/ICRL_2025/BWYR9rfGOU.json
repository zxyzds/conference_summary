{
    "id": "BWYR9rfGOU",
    "title": "SATE: A Two-Stage Approach for Performance Prediction in Subpopulation Shift Scenarios",
    "abstract": "Subpopulation shift refers to the difference in the distribution of subgroups between training and test datasets. When an underrepresented group becomes predominant during testing, it can lead to significant performance degradation, making performance prediction prior to deployment particularly important. Existing performance prediction methods often fail to address this type of shift effectively due to their usage of unreliable model confidence and mis-specified distributional distances. In this paper, we propose a novel performance prediction method specifically designed to tackle subpopulation shifts, called Subpopulation-Aware Two-stage Estimator (SATE). Our approach first estimates the subgroup proportions in the test set by linearly expressing the test embedding with training subgroup embeddings. Then, it predicts the accuracy for each subgroup using the accuracy on augmented training set, aggregating them into an overall performance estimate. We provide theoretical proof of our method's unbiasedness and consistency, and demonstrate that it outperforms numerous baselines across various datasets, including vision, medical, and language tasks, offering a reliable tool for performance prediction in scenarios involving subpopulation shifts.",
    "keywords": [
        "Performance Prediction",
        "Subpopulation Shift",
        "Unsupervised Accuracy Estimation"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=BWYR9rfGOU",
    "pdf_link": "https://openreview.net/pdf?id=BWYR9rfGOU",
    "comments": [
        {
            "summary": {
                "value": "The authors tackle the problem of estimating model performance under subpopulation shift. They propose SATE, which estimates test-set group proportions by representing the mean test-set embedding as a convex combination of mean training subgroup embeddings. The test-set accuracy is then a convex combination of the per-group model accuracies. The authors evaluate their method on typical subpopulation shift datasets, finding that they outperform the baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The method is intuitive and easy to understand.\n- The authors evaluate their method on the common subpopulation shift benchmarks."
            },
            "weaknesses": {
                "value": "1. My main concern is regarding the significance of the method. To me, the problem of estimating model performance under subpopulation shift is largely trivial, as it is just a matter of estimating group proportions on the test set. If group labels are provided in the training domain as the authors assume, it is even simpler, and also a much more restrictive problem setup, which limits the applicability of the method. Given that the method is only theoretically bounded when subpopulation shift is the only shift that occurs (Assumption 1), and does not take e.g. the variation of sample difficulty within each subpopulation into account, I am not convinced that this method is useful.\n\n2. It is not surprising that the proposed method outperforms other performance prediction methods (Figure 4), as these baselines are not specific to subpopulation shift, and do not even utilize the training set attributes. There are several other intuitive baselines that the authors could consider, e.g. learning per-group clusters on the training set, learning a debiased group predictor on the training set, or directly learning a model to predict the errors of the original model.\n\n3. The authors should also show the predicted group proportions versus the actual proportions in the appendices.\n\n4. To improve the significance of the work, the authors should consider evaluating their method on domain generalization benchmarks such as DomainBed [1] or WILDS [2].\n\n[1] https://arxiv.org/abs/2007.01434\n\n[2] https://arxiv.org/abs/2012.07421"
            },
            "questions": {
                "value": "1. When computing the test-set group proportion $w$ in Algorithm 1 Step 10, how is it enforced that $w$ should sum to 1?\n\n2. In the result showing augmentations on the y=x line (Figure 2), has the model been trained with the same data augmentations? It seems like this would be an important factor."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces SATE (Subpopulation-Aware Two-stage Estimator), a novel method for predicting model performance under subpopulation shift scenarios, where the distribution of subgroups differs between training and test datasets. SATE's two-stage approach first estimates subgroup proportions in the test set by expressing test embeddings as a linear combination of training subgroup embeddings, then predicts accuracy for each subgroup using augmented training data to produce an overall performance estimate. Experiments show improvement when compared SATE with baselines such as ATC-MC and DoC."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Novel contribution: First performance prediction method specifically designed for subpopulation shift scenarios and first to address unsupervised performance prediction in NLP tasks.\n\n2. Theoretical foundations: Authors provide proofs of unbiasedness and consistency under certain conditions.\n\n3. Empirical evaluation:  Experiments across multiple domains (vision, medical, NLP) and demonstrates superior performance compared to baselines."
            },
            "weaknesses": {
                "value": "1. Knowledge of group annotations: the method requires attribute annotations for the training data, which may not always be available or could be costly to obtain.\n\n2. Scalability: The method may struggle with scalability when dealing with a large number of subgroups.\n\n3. Linear decomposition: the method relies on linear decomposition assumption for test set embeddings, which might not always hold.\n\n4. Discussions of limitations: there is no clear discussion of failure modes or performance under noisy/incomplete attribute annotations."
            },
            "questions": {
                "value": "1. How sensitive is the method to violations of the linear decomposition assumption for test set embeddings?\n\n2. What are the specific conditions required for the theoretical guarantees to hold?\n\n3. What is the memory requirement for storing subgroup embeddings?\n\n4. How robust is the linear equation-solving step when subgroup embeddings are nearly collinear? What happens when some subgroups have very few training samples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes SATE, a method for predicting test performance under subpopulation shift scenarios. The approach assumes access to test data but not to test set labels. SATE follows a two-stage process: in the first step, it calculates subgroup ratios by linearly expressing the average embedding of test data using the average embeddings of each subgroup in a subgroup-labeled training set. In the second step, it estimates subgroup performance using a subgroup-labeled augmented set (or validation set). The final predicted test accuracy is obtained by calculating a weighted sum of subgroup performance from step 2, using the subgroup ratios from step 1. The effectiveness of SATE is demonstrated on both image and language tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is clearly written and presents experiments across diverse benchmarks."
            },
            "weaknesses": {
                "value": "[W1] The rationale for predicting average accuracy based on the test distribution rather than evaluating using worst group accuracy is not clear. Is there a realistic scenario that motivates this? From a group robustness perspective, an ideal model should perform well across all subgroups. For this reason, group robustness studies typically evaluate models using worst-group accuracy or the average performance across subgroups (unbiased accuracy). However, this paper appears to prioritize sample average accuracy, aligned with the test environment distribution, rather than worst-group or unbiased accuracy. The reasoning behind this choice is not well-justified.\n\n[W2] Along with W1, using the labeled set $S'_i$ to measure subgroup performance seems more like conducting a test evaluation than performance prediction. Does assuming access $S'_i$- a labeled set considered unseen from the model\u2019s perspective- appear to be an overly strong assumption?\n\n[W3] For the experiments in Table 1, is the training dataset also composed of corrupted data?\n\n[W4] This method seems to handle only seen subgroups. How does it address unseen subgroups? If the goal is performance prediction, it should ideally be able to handle unseen subgroups as well.\n\n[W5] Obtaining subgroup labels is often costly, and thus many studies have long focused on learning methods that do not require subgroup labels. Requiring a labeled training set for performance prediction appears to set up an unrealistic scenario. This is especially relevant given that even the DFR method used in this paper does not require training set labels during learning. \n\n[W6] How would the approach perform if evaluated using a retrieval-based method? A straightforward solution, for example, could be KNN with $S'_i$.\n\n[W7] Some terms appear in formulas without clear definitions (e.g., $P_{T-emb}$, $P_{g-emb}$, $H_S$)"
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses how to predict the performance of an unlabeled test set in the presence of subpopulation shifts between the training and test sets. The authors propose a two-stage method. First, they estimate the proportions of different subpopulations in the test set by leveraging the average feature representation of all test samples and comparing it with the prototype features of each subpopulation in the training set. Next, they evaluate the performance of each subpopulation individually using a data-augmented version of the training set. Finally, the predicted overall test set performance is obtained by computing the weighted average of the subpopulation performances. The authors validate this approach with experiments on image and NLP datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The study of performance prediction methods robust to distribution shifts is practical and meaningful. \n2. The method proposed by the paper is straightforward and reasonable.\n3. The authors provide the source code, which is highly commendable."
            },
            "weaknesses": {
                "value": "1. The writing of the paper should be improved, as the flow of logic is unclear in several parts. For example, the logic between the first four paragraphs of the introduction is confusing, and the same lack of clarity is present in the four paragraphs of section 4.1.\n2. If I understand correctly, the terms subpopulation, subgroup, group, and subset in the paper are used interchangeably to convey the same meaning. This inconsistent terminology further increases confusion for the readers.\n3. The theoretical part of the paper is trivial, lacking valuable insights in both the proof process and the results presented. I suggest that this part should not occupy such a significant portion of the manuscript and could potentially be removed from the main text altogether.\n4. I have some concerns about the effectiveness of using a data-augmented training set. Modern image classification models typically employ a wide range of data augmentation techniques to enhance model performance. Therefore, the model should also perform well on augmented training images, especially given the simple geometric transformations like Crop, Flip, and RandomRotation used in the paper. I briefly reviewed the source code provided by the authors, and if I understand correctly, these augmentation techniques do not seem to be incorporated into the training process. This implies an assumption that appears to be rather unrealistic.\n5. The baseline methods mentioned in Section 2, such as Distribution Discrepancy-based and Model Agreement-based approaches, do not appear to be compared in the experiments.\n6. The authors emphasize spurious correlation in the motivation section, which raises a question for me: is the method aimed at addressing all types of subpopulation shifts, or is it specifically targeting spurious correlations? Based on my understanding, the former is correct. Therefore, what is the purpose of highlighting spurious correlation in this context?\n\nBased on my current assessment, this paper is not sufficient for publication at ICLR. I will adjust my score accordingly based on the authors\u2019 clarifications and modifications during the rebuttal phase."
            },
            "questions": {
                "value": "My questions that need clarification are included in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}