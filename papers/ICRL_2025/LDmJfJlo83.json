{
    "id": "LDmJfJlo83",
    "title": "Uncertainty Quantification with Generative-Semantic Entropy Estimation for Large Language Models",
    "abstract": "In recent years,  powerful foundation models, including Large Language Models (LLMs) and Large Multi-Modal Models (LMMs) have ushered in a new epoch of multi-faceted, intelligent conversational agents. Despite their significant early successes and widespread use, foundation models nevertheless currently suffer from several critical challenges, including their lack of transparency and predilection for \"hallucinations.\"  To this end, we introduce Generative-Semantic Entropy Estimation (GSEE), a model-agnostic algorithm that efficiently estimates the generative uncertainty associated with foundation models, while requiring no additional auxiliary model inference steps. In principle, for any foundation model input data, e.g., a text prompt, image, text + image, etc., GSEE numerically estimates the uncertainty encapsulated in the internal, semantic manifold of the LLM generated responses to the input data. In this way, high uncertainty is indicative of hallucinations and low generative confidence. Through experiments, we demonstrate the superior performance of GSEE for uncertainty quantification (UQ) amongst state-of-the-art methods across a variety of models, datasets, and problem settings, including: unbounded language prompting, constrained language prompting, high/low generative stochasticity, acute semantic diversity prompting, and as a barometer for hallucination/predictive accuracy.",
    "keywords": [
        "Large Language Models",
        "Uncertainty Quantification",
        "Explainable AI",
        "Trustworthy AI"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We introduce Generative-Semantic Entropy Estimation (GSEE), a lightweight, model-agnostic algorithm to estimate uncertainty quantification for foundational and related models by estimating the entropy of the subspace spanned by model responses.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=LDmJfJlo83",
    "pdf_link": "https://openreview.net/pdf?id=LDmJfJlo83",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a method to evaluate the uncertainty of a language model, namely GSEE.\nSpecifically, the method generates a few outputs from an input prompt, then the method calculates the length-normalized spectral entropy of those outputs' semantic as the uncertainty.\nThe paper further evaluates the proposed methods under varied settings and compares the proposed method with baselines to show the superior performance of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "(1) The paper gives a straightforward method to calculate the uncertainty of a language model and gives good illustration and intuition in Fig. 1.\n\n(2) The paper empirically shows the proposed method works well under varied settings."
            },
            "weaknesses": {
                "value": "(1) It's a bit weird for me since the uncertainty is calculated based on a set of outputs. I'm not familiar with the literature but I have seen some papers working on uncertainty estimation which estimates the uncertainty of a particular output/generation. I feel it makes less sense to estimate the uncertainty of a set of generations since we usually just care about the uncertainty of a particular generation. (I acknowledge that it makes sense to estimate the uncertainty of a set of generations if we care about the uncertainty in a set of generations.)"
            },
            "questions": {
                "value": "As weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Generative-Semantic Entropy Estimation (GSEE), a model-agnostic algorithm for uncertainty quantification (UQ) in large language models (LLMs) and multi-modal models. GSEE estimates predictive uncertainty by analyzing the spectral entropy of covariance matrices derived from the latent embeddings of multiple generated responses. This approach aims to detect hallucinations and low-confidence predictions by measuring semantic diversity."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The method introduces a new approach to UQ by leveraging spectral entropy of covariance matrices derived from latent embeddings of generated responses. \n2. GSEE\u2019s model-agnostic design makes it applicable across various foundation models without the need for additional natural language inference (NLI) steps, enhancing its usability and adaptability to different contexts and models. \n3. The paper provides a well-rounded evaluation of GSEE across multiple datasets"
            },
            "weaknesses": {
                "value": "1. The method relies on latent embeddings extracted from penultimate layers, yet the impact of this choice is not examined. A detailed ablation study on embedding layer selection could clarify whether GSEE\u2019s performance is sensitive to embedding depth, and if alternative embeddings could improve its accuracy. \n2. Despite claiming efficiency, GSEE\u2019s use of multiple generations and covariance calculations could be computationally intensive for larger datasets or more frequent real-time applications. The paper lacks a clear analysis of the computational costs and memory overhead of GSEE, which could hinder its scalability.\n3. The presentation can be unclear and should be improved."
            },
            "questions": {
                "value": "Can the authors provide more details on the computational costs associated with GSEE, especially in comparison to simpler UQ methods? Insights into the time and memory complexity of GSEE could help clarify its practicality for large-scale deployment."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Generative-Semantic Entropy Estimation (GSEE) to efficiently estimate uncertainty in generative models without requiring auxiliary model inference steps. GSEE numerically captures the uncertainty based on the spectral entropy of the covariance matrix of the generated outputs, linking high uncertainty to potential hallucinations and low confidence. The authors demonstrate that GSEE outperforms other uncertainty quantification (UQ) methods across various prompting conditions and models"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "(1) The effectiveness of applying GSEE to measure the semantic self-consistency in model responses has been demonstrated by extensive experiments on LLMs and LMMs.  \n\n(2) It is novel to design the experiments \u201cconstrained language prompting\u201d and \u201chigh stochasticity conditioning\u201d to showcase the good performance of GSEE.\n\n(3) The paper is well-written and accessible"
            },
            "weaknesses": {
                "value": "(1) How would this uncertainty estimation metrics GSEE enhance the trustworthiness of generated outputs? For instance, could it be effective in detecting hallucinations and how does it perform? Additionally, what is the performance of using the GSEE to predict the accuracy of the output? Could you please show some examples or use experimental results to support the claim that the GSEE could be beneficial in hallucination detection and improving the prediction accuracy?\n\n(2) What are the advantages of GSEE compared with other metrics like semantic entropy [1], which also measure the uncertainty from the semantic perspective.\n\n(3) Is the GSEE metric sensitive to the number of generated responses M and temperature? It seems that as more responses are generated, the diversity among them would increase, potentially affecting the stability of the GSEE.\n\n(4) The novelty of the definition of the metric is limited, as it relies on the existing metric to quantify the semantic diversity.\n\n[1] Kuhn, L., Gal, Y., & Farquhar, S. (2023). Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664."
            },
            "questions": {
                "value": "Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces GSEE, a method to estimate the semantic uncertainty of (vision) language models.\nThe method uses information in the internal state of the model to measure semantic uncertainty.\nIt is evaluated on four benchmark tasks, two pure language, two vision language tasks, using four different model architectures."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea of using the entropy instead of the determinant of the covariance matrix is a reasonable extension of the prior work of Chen et al.\nThe authors investigate both pure language as well as vision language models."
            },
            "weaknesses": {
                "value": "### Major:\n\n* The framing of GSEE being model agnostic is misleading, as it is complety reliant on the quality of the extracted answer embeddings. \nI am actually not aware of any method for UQ in LLMs/LMMs that would not be model agnostic as by this definition.\n* I don't see how step (1) for GSEE in the introduction (line 51) should ever by possible. \nIt states that \"Given an input datum ... we render multiple text outputs with the generative model through a single forward pass;\".\nI am not aware of any method to generate even one output sequence (consisting of more than one token) with a single forward pass, let alone multiple.\nHow it is described in line 126, it seems the usual setting of generating M sequences is used, with the expense of M forward passes times the lengths of individual sequences.\n* How is GSEE \"mathematically-principled\" as claimed in line 61? There is no derivation of any technical analysis, solely an empirical evaluation of an ad-hoc measure.\nThis is not necessarily bad, if it works well in practice, however the claim of being principled is definately not backed up by what is presented in the paper.\n* Since when is GPT3 an open-source LLM (line 196/197), available through HF transformers?\n* I am completely lost in understanding section 4.3. \nFirst, I don't understand the basic rationale behind introducing those \"bad prompt\" settings. \nSecond, why is there a need to \"bound\" prompts? \nAren't they given a priori, thus are completely under control of the experimentor?\nThird, why are those experiments evaluated using summary statistics over the proposed uncertainty measures and do not use the PCC as for the main experiments?\n* Critical experimental details are missing. E.g. what output sequence (of the M sequences) is actually used to evaluate correctness? The most likely one? \n* How does one know, that the latent representation captures semantics? \nIs there any investigation regarding this, e.g. comparing GSEE scores for handcrafted lexically diverse but semantically similar vs. semantically diverse but lexically similar answers?\nOtherwise, it is a very strong claim that the latent space captures the semantics of a sentence well and that the eigenvectors correspond to different semantics rather than lexical structure, which I don't buy without evidence.\n* In the ablation section, line 365, there is a statement that \"comparable gains were however less pronounced for baseline UQ methods\". \nI didn't find the corresponding results anywhere, table 6 ony provides results for GSEE.\n* The choice of baselines is very limited. There is a lot of work on semantic entropy (Kuhn, Aichberger, Farquhar, Duan, Bakman) that has very good performance but is not compared to.\nLooking at the paper of Chen et al., which is very heavily cited reveals that they used those same baselines without adapting to improvements in those last two years.\nAlso, length normalization for predictive entropy is not without debate, adding the non normalized variant costs basically nothing yet would give more credibility to the experiments.\n* Many things are not defined, e.g. is $\\Sigma_N$ the same as $\\Sigma$ or is there any difference? \nIs $\\Sigma_N$ normalized to make it a \"probability distribution\" for calculating the entropy? \nHow is the entropy actually calculated, e.g. only upper triangle or full matrix flattened?\n* I don't get the reason for dividing the entropy term by the mean length. \nThe semantic embeddings $z$ are already averaged, thus do not contain any length information? \nThis term rather induces length information in the uncertainty measure.\n\n### Minor:\n\n* Overall, the manuscript is very hard to follow. \nConsider streamlining the choice of wording, I often had to stop thinking about whether or not two things actually mean the same or not (e.g. prompting and conditioning in section 4.3).\n* The naming \"Generative-Semantic entropy estimation\" is not optimal, given the well established \"Semantic Entropy\" method (Kuhn, Aichberger, Farquhar).\n\n### Remarks\n\n* Please improve the visuals of figure 2\n* Please take a look at the style guide for tables, there is no need for vertical lines\n* line 55: eigenvlues -> eigenvalues"
            },
            "questions": {
                "value": "See major weaknesses. Additionally:\n\n* Most prior work (e.g. Kuhn, Aichberger, Farquhar, Bakman, Duan) except Chen et al., this work is heavily based upon, uses AUROC/AUPR of being correct as evaluation metric for their uncertainty measure.\nWhy is using the PCC better than those established measures?\n\n---\n## References:\n\nChen, C., Liu, K., Chen, Z., Gu, Y., Wu, Y., Tao, M., ... & Ye, J. (2024). INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection. ICLR24\n\nKuhn, L., Gal, Y., & Farquhar, S. (2023). Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. ICLR23\n\nAichberger, L., Schweighofer, K., Ielanskyi, M., & Hochreiter, S. (2024). Semantically Diverse Language Generation for Uncertainty Estimation in Language Models. arXiv\n\nFarquhar, S., Kossen, J., Kuhn, L., & Gal, Y. (2024). Detecting hallucinations in large language models using semantic entropy. Nature\n\nBakman, Y. F., Yaldiz, D. N., Buyukates, B., Tao, C., Dimitriadis, D., & Avestimehr, S. (2024). MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs. arXiv\n\nDuan, J., Cheng, H., Wang, S., Wang, C., Zavalny, A., Xu, R., ... & Xu, K. (2023). Shifting attention to relevance: Towards the uncertainty estimation of large language models. ACL"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}