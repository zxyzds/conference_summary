{
    "id": "J9FgrqOOni",
    "title": "Discretization-invariance? On the Discretization Mismatch Errors in Neural Operators",
    "abstract": "In recent years, neural operators have emerged as a prominent approach for learning mappings between function spaces, such as the solution operators of parametric PDEs. A notable example is the Fourier Neural Operator (FNO), which models the integral kernel as a convolution operator and uses the Convolution Theorem to learn the kernel directly in the frequency domain. The parameters are decoupled from the resolution of the data, allowing the FNO to take inputs of different resolutions.\nHowever, training at a lower resolution and inferring at a finer resolution does not guarantee consistent performance, nor can fine details, present only in fine-scale data, be learned solely from coarse data. In this work, we address this misconception by defining and examining the discretization mismatch error: the discrepancy between the outputs of the neural operator when using different discretizations of the input data. We demonstrate that neural operators may suffer from discretization mismatch errors that hinder their effectiveness when inferred on data with resolutions different from that of the training data or when trained on data with varying resolutions. As neural operators underpin many critical cross-resolution scientific tasks, such as climate modeling and fluid dynamics, understanding discretization mismatch errors is essential. Based on our findings, we propose a Cross-Resolution Operator-learning Pipeline that is free of aliasing and discretization mismatch errors, enabling efficient cross-resolution, multi-spatial-scale learning, resulting in superior performance.",
    "keywords": [
        "Neural Operators",
        "Operator Learning",
        "Discretization Mismatch Errors",
        "Discretization Invariance"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "A study of resolution invariance and discretization mismatch errors in neural operators",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=J9FgrqOOni",
    "pdf_link": "https://openreview.net/pdf?id=J9FgrqOOni",
    "comments": [
        {
            "title": {
                "value": "Author Rebuttal to Reviewer WGpV: Part II"
            },
            "comment": {
                "value": "> W3 (Q3): statement of Proposition 4.4; omitted M in Lemma B.2.\n\n  1. What we have shown in Lemma B.2 is that, if for training at a fixed finer resolution $M$, **the discretization mismatch error increases as the inference resolution $N$ decreases** (i.e., as the testing resolution gets lower, the upper bound of the DMEs gets larger). This works the other way as well.\n\n\n  2. Basically, as long as the inference resolution differs from that of training, it will suffer from DMEs, whether it is super-resolution or sub-resolution (lower resolution), as demonstrated in the experimental results in Sec. 5.1.\n\n  3. We simplified the proof a little for the general audience, and along the way, this caused some confusion. **We have updated the descriptions in Proposition 4.4 and included the full proof without simplification to avoid any confusion.** The proof is similar to that of the previous one, especially in the treament of the terms $\\omega^L$ and $\\prod_{\\ell=1}^L C_{\\ell}$, although additional efforts are included on the Fourier analysis.\n\n  4. We have also added a sentence (line 288) to help readers better understand this. Additionally, we have included interpretations of the upper bound after the proof (lines 820 and 874).\n\n> Minor notes\n\nThank you, we have taken actions to address all the minor notes.\n\n> Q1: References for the belief in the ability to perform cross-resolution tasks\n\n1. The zero-shot super-resolution capabilities of FNO are highlighted in the original FNO paper [1], and many recent works have conducted zero-shot super-resolution tests for this reason, such as [2,3,4] (there are a lot more, we just provide a few), although the performance often falls below expectations.\n\n> Q2: What is L in the equation in Definition 4.1?\n\nThis should be $x \\in \\Omega_J$. Thank you for catching this typo.\n\n> Q3: Lemma B.2\n\nPlease refer to our answers on W3\n\n> Q4: How much were the hyperparameters tuned for the baselines, particularly in the Navier-Stokes setting?\n\n1. For the NS example in Sec. 5.1, we use the original hyperparameters from [1]. The original reported error is $1.28\\%$, we report much better error ($0.58\\%$ for FNO itself) due to the updates made by the authors to FNO after the release of the paper and the teaching forcing training stategy.\n\n2. For the NS examples in Sec. 5.2.1:\n  - For FNO, we use the original set up from [1], except that we increase the number of modes to $24$ (originally $12$). We have also tried larger number of modes and channel width, we do not observe noticable improvements besides largely increasing the number of parameters; therefore, we use this setting.\n  - For CNO, we use their original set up from [5], we also used their model selection strategy to select the best performing model hyperparameters.\n  - For U-Net and ResNet, we directly use popular implementations without any modification.\n  - For DeepONet, there are different choices for the branch net. We have tried using MLPs, U-Nets, and even FNOs as the branch, however, none of these attempts are very succefully in reducing the errors. Similar performance of DeepONet on NS equations has also been noted in [5] (Table 1).\n\nReferences:\n\n[1] Neural operators for accelerating scientific simulations and design, Kamyar Azizzadenesheli et al., Nature Review Physics\n\n[2] Group Equivariant Fourier Neural Operators for Partial Differential Equations, Jacob Helwig et al., ICML 2023\n\n[3] Improved Operator Learning by Orthogonal Attention, Zipeng Xiao et al., ICLR 2024\n\n[4] Neural Operators with Localized Integral and Differential Kernels, Miguel Liu-Schiaffini el al., ICML 2024\n\n[5] Convolutional Neural Operators for robust and accurate learning of PDEs,\nBogdan Raoni\u0107, et al., NeurIPS 2023"
            }
        },
        {
            "title": {
                "value": "Author Rebuttal to Reviewer WGpV: Part I"
            },
            "comment": {
                "value": "Thanks a lot for your valuable comments! we have revised the paper and provided point-to-point responses as below.\n\n> W1.1: Novelty of the insights about discretization invariance\n\n1. **There may be some misunderstanding. Reference [1] does not mention or identify the root cause of why FNO may struggle with zero-shot super-resolution tasks. We focus on this aspect, find the root cause, and provide a solution.** In fact, [1] contains extensive discussion on the ability of FNO to perform zero-shot super-resolution tasks, such as:\n\n\"They can extrapolate and predict solutions at new locations unseen during training, i.e., perform zero-shot super-resolution.\" (direct quote from the abstract)\n\n\"Fig. 1 demonstrates that empirically, FNOs and other Neural Operators can do zero-shot super-resolution and extrapolate to unseen higher frequencies\" (direct quote from Sec 2.1 in [1])\n\n\"zero-shot super-evaluation, where a trained Neural Operator can be evaluated on a new, finer discretization, than what is seen during training, and produce a solution on that new resolution, thus potentially resolving\nhigher frequency details\" (direct quote from Sec 2.2 in [1])\n\n2. **Our contribution extends beyond theoretical analysis**; we offer valuable insights into this issue, **providing systematic studies and a practical solution**. We believe this is a critical problem to address for advancing the development of infinite-dimensional learning paradigms.\n\n\n> W1.2: Prior works have also demonstrated empirically that FNO performs well in zero-shot super-resolution tasks on smooth problems.\n\n1. **The Navier-Stokes example in Sec. 5.1 represents a relatively smooth problem, where FNO does not exhibit superior performance in cross-resolution tasks.**\n\n2. If your concern is on whether or not FNO really struggles with zero-shot super resolution tasks, **FNO-based models struggling with super resolution tasks can be found in several recent works, spanning both autoregressive rollout taks and one-to-one mapping tasks.** To name a few:\n\nIn [2], FNO has an relative $\\ell_2$ error of **$8.41\\%$** (from their Table 1 row 1) on the Navier Stokes equation with $\\nu=1 \\mathrm{e}-4$ and an super resolution test error of **$32.45\\%$** (from their Table 2 row 2).\n\nIn [3], an FNO trained on a resolution of $43\\times 43 (s=43)$ reported the following relative $\\ell_2$ errors (from their Table 3) on Darcy flow equations:\n\n| Resolution (s)      | Reported Error |\n|--------------------|----------------|\n| 61 x 61 (s=61)     | 0.1164         |\n| 85 x 85 (s=85)     | 0.1797         |\n| 141 x 141 (s=141)  | 0.2679         |\n| 211 x 211 (s=211)  | 0.3160         |\n| 421 x 421 (s=421)  | 0.3631         |\n\nIn [4], an FNO reported the following relative mean square errors (from their Table 5) on Darcy flow equations:\n\n| Resolution       | Reported Error           |\n|------------------|-------------------|\n| $ \\frac{1}{2} \\times $ | $ 1.475 \\cdot 10^{-1} $ |\n| $ 1 \\times $           | $ 5.867 \\cdot 10^{-2} $ |\n| $ 2 \\times $           | $ 8.646 \\cdot 10^{-2} $ |\n| $ 4 \\times $           | $ 7.731 \\cdot 10^{-2} $ |\n\nIn [4], an SFNO reported the following relative mean square errors (from their Table 5) on Spherical Shallow Water Equations:\n\n| Resolution       | Reported Error                      |\n|------------------|----------------------------|\n| $ \\frac{1}{2} \\times $ | $ 1.342 \\cdot 10^{-3} $ |\n| $ 1 \\times $           | $ 9.220 \\cdot 10^{-4} $ |\n| $ 2 \\times $           | $ 3.830 \\cdot 10^{-3} $ |\n| $ 4 \\times $           | $ 4.419 \\cdot 10^{-3} $ |\n\n\n> W2: Spectrum comparison between FNO and CROP\n\n1. There might be some misunderstanding here. **The limitations of FNO extend beyond the inability to learn unseen high-frequency information; DMEs also introduce errors in the low-frequency components.** Therefore, CROP's primary purpose is not to surpass FNO in generalization to unseen high-frequency information.\n\n2. For reference, we have included a spectral comparison between FNO and CROP in Appendix C.1 (Fig. 4). **The results show that CROP is at least on par with FNO in terms of learning higher frequencies beyond the band-limit under both same-resolution inference and high-resolution inference.**\n\n3. **The limitations of FNO extend beyond learning unseen high-frequency information; DMEs introduce large errors in the low-frequency components.** Similarly, we provide the spectral difference plot in Fig. 5 in Appendix C.1, which clearly reveals the superiority of CROP compared to FNO."
            }
        },
        {
            "title": {
                "value": "Author Rebuttal to Reviewer z3xH: Part III"
            },
            "comment": {
                "value": "> Q5: Choice of intermediate neural operator in CROP\n\n1. We found that for problems with multiscale spatial features, such as Navier-Stokes with high Reynolds numbers, an intermediate neural operator with the ability to capture localized structure is generally preferred (we used a U-Net). However, for smooth problems of a global nature, such as Navier-Stokes with a Reynolds number of 1,000 (as shown in Sec. 5.1), a global operator like FNO is more suitable.\n\n2. In terms of computational efficiency, it largely depends on the choice of intermediate neural operator and the selected bandlimit. However, as demonstrated in the paper, **CROP achieves strong performance while remaining comparable in efficiency to operators like FNO**.\n\n> Q6: Skip connections for FNO.\n\n1. **In experiments, we do have skip connections.** We use the original implementation provided by [2] (the updated version), and their implementation do have skip connections.\n\n2. Both training and testing errors at the same resolution as the training data remained fairly low. However, the cross-resolution error increases as the number of layers grows.\n\nReferences\n\n[1] Convolutional Neural Operators for robust and accurate learning of PDEs,\nBogdan Raoni\u0107, et al., NeurIPS, 2023\n\n[2] Fourier Neural Operator for Parametric Partial Differential Equations, Zongyi Li et al., ICLR 2021\n\n[3] PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers, Lippe el al., NeurIPS 2023"
            }
        },
        {
            "title": {
                "value": "Author Rebuttal to Reviewer z3xH: Part II"
            },
            "comment": {
                "value": "> W5: It would be also great to have pseudocode or the full architecture.\n\n1. **We have provided a description of the CROP method in Sec. 4.3.**\n\n2. The framework operates in three steps. First, a CROP lifting layer lifts the input function to higher channel dimensions within bandlimited function spaces to facilitate enriched representation learning. Second, an intermediate neural operator\u2014chosen freely\u2014maps the bandlimited high-dimensional function to another bandlimited high-dimensional function. Third, a CROP projection layer projects the latent function down to the output dimension while attempting to recover some lost high frequencies. If you find it necessary, please let us know, and we will provide more details in the appendix in the paper.\n\n> Q1: So, is CROP still a universal approximation for operators? It seems like its not but I may be mistaken.\n\n1. **The short answer is YES**, theorectically.\n\n2. For mappings between bandlimited function spaces, the result is straightforward: the lifting and projection operators serve as mappings within bandlimited function spaces, which can just be identity if needed, and the intermediate neural operator can be chosen to be an operator that possesses universality.\n\n3. For mappings between general Sobolev spaces, the universality of our approach can be established in a manner similar to that of FNO. For any desired accuracy, a suitable bandlimit can be always be found such that the Fourier projection error remains sufficiently small, allowing us to apply the universality result within bandlimited function spaces. However, as suggested in [1], general infinite-dimensional function spaces such as $H^s$ may be too broad to effectively learn, making bandlimited function spaces of greater interest.\n\n> Q2: use higher number of modes to capture those details\n\n1. **We actually have already used a larger number of modes for the results presented in the paper**. We use 24 modes as opposed to the original of 12 (we describe the use of 24 modes in implementation details under Sec. D.5); the resolution is 64, so full modes is 33 (in terms of RFFT). For your reference, we also have conducted additional experiments (over 3 runs) with full modes, and it results in an error of $7.27\\%$ for Reynolds number $5,000$, and $8.34\\%$ for Reynolds number $10,000$ (Similar to that of using 24 modes).\n\n> Q3: Choice of Band-limit\n\n1. The general strategy is to choose a bandlimit such that the loss of high-frequency information is minimal. However, similar to FNO, there is a trade-off between the bandlimit and computational efficiency. The universality results of FNO are also built upon a sufficient bandlimit (truncation modes) to ensure desired arbitrary small Fourier projection errors.\n\n\n> Q4: How does the computational complexity of CROP compare to standard FNO, especially for very high-resolution inputs?\n\n1. Overall, it is comparable to that of FNO. The computational complexity of CROP is depends on the band-limit; if the band-limit is kept the same, the computational time will barely increase.\n\n2. FNO has a complexity of $O(n^2 \\log n)$ for 2D examples due to 2D FFT. In general, it's $O(J \\log J)$ for a discretization of size $J$ (Assuming we can perform FFT).\n\n3. Training under high resolutions can be time-consuming to demonstrate; however, we have provided here a comparison of inference times for higher-resolution settings of the Navier-Stokes (NS) equation with a Reynolds number of 10,000. The results below were recorded on an NVIDIA RTX A6000 GPU with 48 GB GDDR6 over a batch of 20 samples. While the training speed of CROP is slower than that of FNO (as seen in Table 3, Sec. 5.2.1), CROP achieves a faster inference time. For high-resolution inputs, CROP is even much faster.\n\n|                       | Navier-Stokes ($R_e = 10,000$) |                     |\n|-----------------------|-------------------------------|---------------------|\n|                       | **256 x 256**                 | **64 x 64**    |\n| **FNO**               | 32.87ms                   | 3.01ms          |\n| **CROP**              | 2.83ms                   | 2.18ms         |"
            }
        },
        {
            "title": {
                "value": "Author Rebuttal to Reviewer z3xH: Part I"
            },
            "comment": {
                "value": "Thank you so much for your detailed and constructive comments! We appreciate your questions! We have revised the paper and also provide responses here.\n\n> W1: Variance in some results: Some empirical results, particularly in Figure 3b, show high variance. Maybe some explanation of the high variance?\n\n1. Figure 3b shows the super-resolution errors. **High variance can occur due to the accumulation of errors over sequential steps (layers).** In some cases, the discretization mismatch error in the next step can partially \"correct\" or offset errors from previous steps, effectively reducing the cumulative error.\n\n2. Moreover, the use of skip connections might also contribute to this effect. In certain runs, the later layers may simply be learning the identity map when the number of layers is overly sufficient. It is worth investigating further why this occurs and whether it can offer any new insights into designing more robust neural operators.\n\n3. If you find it necessary to include such discussions in the paper, please let us know, we will include under Appendix D.\n\n> W2: There's minimal discussion on how sensitive CROP is to hyperparameter choices, which is important for understanding its robustness and ease of application to new PDE problems\n\n1. In Appendix D5, we provide the implementation details. **All training hyperparameters (learning rates etc.) are general for CROP**; we only adjust some of them for the baselines, as we found that they perform better under those settings.\n\n2. Moreover, the backbone intermediate finite-dimensional operator is a standard U-Net, and **all network parameter choices are standard and commonly used in other works that employ U-Nets** in tasks such as segmentation (depth of 3, initial channel width of 16, kernel size of 3, 4 residual blocks in the middle layers, and 6 residual blocks in the bottleneck layer).\n\n3. We additionally perform ablation studies on the choices of network parameters on the Navier-Stokes equation with Reynolds number 10,000 (first one is used in the paper; results are averaged over 3 runs for this ablation study). The results reveal that CROP is stable. If you find it neccesary, please let us know, we will include it in Appendix D.5.\n\n| Layers           | Initial Channel Width | Residual Blocks (Middle Layer) | Residual Blocks (Bottleneck Layer) |Error(%)|\n|----|-----|--------|-------|------|\n| 3 |  16  |  4|    6     |4.06|\n| 2 |  16  |  2|    6     |4.54|\n| 2 |  16  |  2|    4     |4.45|\n| 2 |  16  |  4|    6     |3.96|\n| 2 |  32  |  2|    2     |4.77|\n| 2 |  32  |  2|    4     |4.49|\n| 3 |  16  |  2|    4     |4.69|\n| 3 |  8  |  2|    4     |4.24|\n| 3 |  8  |  4|    6     |4.14|\n\n> W3: Long-term stability in time-dependent problems\n\n1. **As CROP does not suffer from discretization mismatch errors for any time step, the error does not propogate through time.** CROP is stable for long-term roll-outs in terms of cross-resolution abilities. We have included this discussion in the paper (line 442)\n\n2. As shown in Table 2, CROP maintains consistent results over different resolutions of inference. This example has 40 rollout steps, which demonstates CROP's ability for long-term rollouts under cross-resolution tasks.\n\n3. If you are referring to the stability of long-time roll-out under the same resolution of inference, this is out of the scope of our work, as specific network and training designs must be implemented (e.g. [3]).\n\n> W4: Limited exploration of high-frequency information: While CROP addresses the issue of DMEs, the paper doesn't deeply explore how effectively it captures high-frequency information, especially in comparison to methods specifically designed for super-resolution tasks.\n\n1. We have included a spectral comparison between FNO and CROP in Appendix C.1 (Fig. 4). **The results show that CROP is at least on par with FNO in terms of learning higher frequencies beyond the band-limit under both same-resolution inference and high-resolution inference.**\n\n2. **The limitations of FNO extend beyond learning unseen high-frequency information; DMEs introduce large errors in the low-frequency components.** Similarly, we provide the spectral difference plot in Fig. 5 in Appendix C.1, which clearly reveals the superiority of CROP compared to FNO."
            }
        },
        {
            "title": {
                "value": "Author Rebuttal to Reviewer FHqG"
            },
            "comment": {
                "value": "Thank you so much for your efforts reviewing our work. We respond to your questions below.\n\n> W1.1: Contradiction between [1] and our findings\n\n1. There might be a misunderstanding here; **our work does not challenge [1]** but rather the terminology used FNO is indeed discretization-invariant as defined in [1] (or Definition 4.2 in our paper). However, **this definition does not enable FNOs to perform zero-shot super-resolution tasks (or generalize well to unseen high frequencies)**. By this definition, even U-Net can be considered discretization-invariant, as its local convolutions converge to a point-wise operator. Therefore, we propose the term 'convergence' for this definition.\n\n2. In summary, our work does not challenge [1], but rather **clarify a misconception in the community.** We discuss this throughout Sec. 4.1, e.g. lines 256-268.\n\n> W1.2: Section 7,ig. 8 and Table 2&3 of [1] Fig. 8 and Table 2&3.\n\n1. For the results in Fig. 8 and Table 2&3 in [1], **the training and testing resolutions are the same** (the caption of Fig. 8: \"Train and test on the same resolution\"), which is not what it means to perform super-resolution (cross-resolution) tasks.\n\n2. If your concern is on whether or not FNO really struggles with zero-shot super resolution tasks, **FNO-based models struggling with super resolution tasks can be found in several recent works, spanning both autoregressive rollout taks and one-to-one mapping tasks.** To name a few:\n\nIn [3], FNO has an relative $\\ell_2$ error of **$8.41$%** (from their Table 1 row 1) on the Navier Stokes equation with $\\nu=1 \\mathrm{e}-4$ and an super resolution test error of **$43.02$%** (from their Table 2 row 1).\n\nIn [3], an FNO trained on a resolution of $43\\times 43 (s=43)$ reported the following relative $\\ell_2$ errors (from their Table 3) on Darcy flow equations:\n\n| Resolution (s)      | Reported Error |\n|--------------------|----------------|\n| 61 x 61 (s=61)     | 0.1164         |\n| 85 x 85 (s=85)     | 0.1797         |\n| 141 x 141 (s=141)  | 0.2679         |\n| 211 x 211 (s=211)  | 0.3160         |\n| 421 x 421 (s=421)  | 0.3631         |\n\nIn [4], an FNO reported the following relative mean square errors (from their Table 5) on Darcy flow equations:\n\n| Resolution       | Reported Error           |\n|------------------|-------------------|\n| $ \\frac{1}{2} \\times $ | $ 1.475 \\cdot 10^{-1} $ |\n| $ 1 \\times $           | $ 5.867 \\cdot 10^{-2} $ |\n| $ 2 \\times $           | $ 8.646 \\cdot 10^{-2} $ |\n| $ 4 \\times $           | $ 7.731 \\cdot 10^{-2} $ |\n\nIn [4], an SFNO reported the following relative mean square errors (from their Table 5) on Spherical Shallow Water Equations:\n\n| Resolution       | Reported Error                      |\n|------------------|----------------------------|\n| $ \\frac{1}{2} \\times $ | $ 1.342 \\cdot 10^{-3} $ |\n| $ 1 \\times $           | $ 9.220 \\cdot 10^{-4} $ |\n| $ 2 \\times $           | $ 3.830 \\cdot 10^{-3} $ |\n| $ 4 \\times $           | $ 4.419 \\cdot 10^{-3} $ |\n\nReferences:\n\n[1] Neural operator: Learning maps between function spaces with applications to PDEs. Nikola Kovachki et al., JMLR 2023\n\n[2] Group Equivariant Fourier Neural Operators for Partial Differential Equations, Jacob Helwig et al., ICML 2023\n\n[3] Improved Operator Learning by Orthogonal Attention, Zipeng Xiao et al., ICLR 2024\n\n[4] Neural Operators with Localized Integral and Differential Kernels, Miguel Liu-Schiaffini el al., ICML 2024"
            }
        },
        {
            "title": {
                "value": "Author Rebuttal to Reviewer w3hk: Part III"
            },
            "comment": {
                "value": "> W4: Concerns about the numerical tests on the Darcy flow and the Poisson problemS\n\n1. These two PDEs are commonly used in many studies. For example, Darcy flow is tested in [2, 4, 5], and the Poisson equation is tested in [6, 7, 8]. Moreover, Darcy flow is a linear PDE; however, the operator itself is not linear (see [2], page 2, last sentence of the paragraph containing equation (2) of the ICLR 2020 version).\n\n2. The results in Table 7 do not demonstrate a significant advantage, further underscoring our concerns about zero-shot super-resolution tests discussed in line 1147. We could have only provided example in which we demonstrate significant advanges; however, we would like to be upfront and to motivate the community for further inverstigation and innovations. Additionally, in Appendix E, we provide more discussion on our vision and aspirations for the study and development of methods with insights from mathematics and physics for super-resolution tasks in the SciML field.\n\nReferences:\n\n[1] Fourier Neural Operator for Parametric Partial Differential Equations, Zongyi Li et al., ICLR 2021\n\n[2] Neural Operator: Graph Kernel Network for Partial Differential Equations, Zongyi Li et al., ICLR 2020\n\n[3] Group Equivariant Fourier Neural Operators for Partial Differential Equations, Jacob Helwig et al., ICML 2023\n\n[4] Improved Operator Learning by Orthogonal Attention, Zipeng Xiao et al., ICLR 2024\n\n[5] Neural Operators with Localized Integral and Differential Kernels, Miguel Liu-Schiaffini el al., ICML 2024\n\n[6] Convolutional Neural Operators for robust and accurate learning of PDEs,\nBogdan Raoni\u0107, et al., NeurIPS, 2023\n\n[7] BENO: Boundary-embedded Neural Operators for Elliptic PDEs, Haixin Wang et al., ICLR 2024\n\n[8] Learning the boundary-to-domain mapping using Lifting Product Fourier Neural Operators for partial differential equations, Aditya Kashi et al., ICML 2024"
            }
        },
        {
            "title": {
                "value": "Author Rebuttal to Reviewer w3hk: Part II"
            },
            "comment": {
                "value": "> W2.3: Derivation of Lemma B. 2 is relatively straightforward and could likely be generalized to other neural operator models as well.\n\n1. It\u2019s **subjective** to consider whether this proof is straightforward; while this is not a paper on numerical analysis, **we value the insights it offers into why FNO struggles with zero-shot super-resolution tasks**.\n\n2. You\u2019re absolutely correct that this could likely extend to other grid-based neural operators; in fact, line 215 (first paragraph under Sec. 4) of our paper states, \"the concepts and analysis can be extended to other mesh-based neural operators, such as the Wavelet neural operator (Gupta et al., 2021).\"\n\n3. Our intention isn\u2019t to single out FNO as the only model that falls short of acting as an infinite-dimensional operator; we demonstrate this issue with FNO simply because it is the most well-known and revolutionary framework. Instead, we aim to **highlight an overlooked aspect of discretization mismatch errors that can inspire the community to advance neural operator design**.\n\n\n> W3.1: Addtional experiments on training resolutions $128 \\times 128$ and $256 \\times 256$\n\n1. We provide only the results from training resolution of $64 \\times 64$ as **an example to demonstate this issue of discretization mismatch errors**.\n\n2. We provide the results from training resolutions of $128 \\times 128$ and $256 \\times 256$ here, which again demonstates this issue. **If you find it neccessary to include this in the paper, please let us know; we will include this result in the appendix.**\n\nTraining resolution $128 \\times 128$ (over $10$ runs):\n\n| Neural Operator        | **$256 \\times 256$**       | **$128 \\times 128$**       | **$64 \\times 64$** | **$32 \\times 32$**       |\n|------------------------|----------------------------|-----------------------------|---------------------------------|---------------------------|\n| FNO                    | $1.47 (\\pm 0.46)$          | $0.58 (\\pm 0.03)$           | $3.19 (\\pm 1.03)$               | $11.77 (\\pm 3.65)$        |\n| CROP (ours)            | **$0.54 (\\pm 0.05)$**      | **$0.54 (\\pm 0.05)$**       | **$0.54 (\\pm 0.05)$**           | **$0.54 (\\pm 0.05)$**    |\n\n\nTraining resolution $256 \\times 256$ (over $5$ runs as it takes ~16x training time for FNO compared to $s = 64$):\n\n| Neural Operator        | **$256 \\times 256$**       | **$128 \\times 128$**       | **$64 \\times 64$** | **$32 \\times 32$**       |\n|------------------------|----------------------------|-----------------------------|---------------------------------|---------------------------|\n| FNO                    | $0.61 (\\pm 0.05)$          | $3.41 (\\pm 0.57)$           | $7.73 (\\pm 1.97)$               | $13.43 (\\pm 3.92)$        |\n| CROP (ours)            | **$0.57 (\\pm 0.06)$**      | **$0.57 (\\pm 0.06)$**       | **$0.57 (\\pm 0.06)$**           | **$0.58 (\\pm 0.06)$**    |\n\n> W3.2: Better super-resolution results in other papers\n\n1. Different settings and datasets might lead to different results; we use the **exact same dataset and setup (including hyperparameters) from the original FNO paper [1]**. This is a time-dependent PDE with auto-regressive rollouts, and the discretization mismatch errors accumulate through time (see our Fig. 3 (a)).\n\n2. Moreover, **FNO-based models struggling with super resolution tasks can be found in several recent works, spanning both autoregressive rollout taks and one-to-one mapping tasks.** To name a few:\n\nIn [3], FNO has an relative $\\ell_2$ error of **$8.41$%** (from their Table 1 row 1) on the Navier Stokes equation with $\\nu=1 \\mathrm{e}-4$ and an super resolution test error of **$43.02$%** (from their Table 2 row 1).\n\nIn [4], an FNO trained on a resolution of $43\\times 43 (s=43)$ reported the following relative $\\ell_2$ errors (from their Table 3) on Darcy flow equations:\n\n| Resolution (s)      | Reported Error |\n|--------------------|----------------|\n| 61 x 61 (s=61)     | 0.1164         |\n| 85 x 85 (s=85)     | 0.1797         |\n| 141 x 141 (s=141)  | 0.2679         |\n| 211 x 211 (s=211)  | 0.3160         |\n| 421 x 421 (s=421)  | 0.3631         |\n\nIn [5], an FNO reported the following relative mean square errors (from their Table 5) on Darcy flow equations:\n\n| Resolution       | Reported Error           |\n|------------------|-------------------|\n| $ \\frac{1}{2} \\times $ | $ 1.475 \\cdot 10^{-1} $ |\n| $ 1 \\times $           | $ 5.867 \\cdot 10^{-2} $ |\n| $ 2 \\times $           | $ 8.646 \\cdot 10^{-2} $ |\n| $ 4 \\times $           | $ 7.731 \\cdot 10^{-2} $ |\n\nIn [5], an SFNO reported the following relative mean square errors (from their Table 5) on Spherical Shallow Water Equations:\n\n| Resolution       | Reported Error                      |\n|------------------|----------------------------|\n| $ \\frac{1}{2} \\times $ | $ 1.342 \\cdot 10^{-3} $ |\n| $ 1 \\times $           | $ 9.220 \\cdot 10^{-4} $ |\n| $ 2 \\times $           | $ 3.830 \\cdot 10^{-3} $ |\n| $ 4 \\times $           | $ 4.419 \\cdot 10^{-3} $ |"
            }
        },
        {
            "title": {
                "value": "Author Rebuttal to Reviewer w3hk: Part I"
            },
            "comment": {
                "value": "Thank you very much for your valuable feedback! We have revised the manuscript accordingly and provided our responses below.\n\n> W1: Infinitely fine resolution of CROP and comparison with CDE and Physics-informed constraints\n\n1. **CROP handles inifitely fine resolutions with a fully connected point-wise network, which aims to recover high frequencies lost due to the prior band-limit assumptions**. We provide theoretical support for this (see Remark 4.5), explaining why this network effectively recovers high frequencies.\n\n2. Moreover, we would like to note that **CDE cannot handle infinitely fine resolutions** nor can it enable the underlying network to perform cross-resolution tasks.\n\n3. Physics informed constraints (PICs) are difficuilt to train and is oftentimes rectricting. We focus on the data-learning perspective while PICs require explicit PDE forms, which can even be trained without data if we have the full governing PDE. **We do not think there is a fair comparison between our method and PIC.**\n\n4. Your question is very insightful and coincides with the message we aim to convey in our work, but we would rephrase it as, \"Can any existing grid-based operators truly handle infinitely fine resolutions?\" A common misconception in the field is that they simply can, as with FNO\u2019s purported ability to manage fine resolutions. **However, our work demonstrates that the answer is, in fact, NO; FNO and other grid-based operators don\u2019t achieve this.** We urge the community to pay more attention to this issue and propose new solutions. In our paper, we have also discussed the limitations of CROP in terms of handling infinite-dimensional learning in Sec. 6 and provide insights into super resolution for SciML in Appendix E.\n\n> W2.1: The upper bound does not necessarily imply that the \"discretization mismatch errors\" will vary in line with the trends of the upper bound.\n\n1. **This is why we included Section 5.1.1, which empirically demonstrates that this issue is present in FNOs**. In Fig. 3(a), we clearly show that **discretization errors rise significantly when the testing resolution differs from the training resolution**. Similarly, in Fig. 3(b), we show that **discretization errors increase with the number of layers**. These experiments provide clear evidence for our claims.\n\n2. Yes, while the results provide an upper bound\u2014meaning the trend doesn't always hold precisely, as evidenced by the high variance in Fig. 3(b)\u2014we want to emphasize that in practical super-resolution tasks, testing a model's super-resolution capability on specific test sets may not even be feasible. Therefore, **improving performance in the worst-case scenario is essential**. Moreover, it can be observed that, although it does not follow precisely, the overall error accumulation trend is clear.\n\n> W2.2: Omitted M in Lemma B.2.\n\n  1. What we have shown in Lemma B.2 is that, if for training at a fixed finer resolution $M$, **the discretization mismatch error increases as the inference resolution $N$ decreases** (i.e., as the testing resolution gets lower, the upper bound of the DMEs gets larger). This works the other way as well.\n\n\n  2. Basically, as long as the inference resolution differs from that of training, it will suffer from DMEs, whether it is super-resolution or sub-resolution (lower resolution), as demonstrated in the experimental results in Sec. 5.1.\n\n  3. We simplified the proof a little for the general audience, and along the way, this caused some confusion. **We have updated the descriptions in Proposition 4.4 and included the full proof without simplification to avoid any confusion.** The proof is similar to that of the previous one, especially in the treament of the terms $\\omega^L$ and $\\prod_{\\ell=1}^L C_{\\ell}$, although additional efforts are included on the Fourier analysis.\n\n  4. We have also added a sentence (line 288) to help readers better understand this. Additionally, we have included interpretations of the upper bound after the proof (lines 820 and 874)."
            }
        },
        {
            "summary": {
                "value": "This paper examines \"discretization mismatch errors\" in neural operators, like the Fourier Neural Operator (FNO), which learn mappings between different types of data. The authors find that training these models at low resolutions and applying them at high resolutions can lead to performance issues. They propose a new Cross-Resolution Operator-learning Pipeline to avoid these errors, improving accuracy in tasks requiring precise predictions across multiple resolutions, such as climate modeling and fluid dynamics."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The phenomenon of \"discretization mismatch errors\" is a significant issue in operator learning. This paper proposes the CROP framework to address (reduce) this error.\n\n2. The numerical examples in this paper show that CROP achieves improved performance over classic neural operator architectures, with the Fourier Neural Operator (FNO) used as an example."
            },
            "weaknesses": {
                "value": "1. Compared to approaches like \"physics-informed constraints\" and \"continuous-discrete equivalence (CDE),\" the CROP framework takes a more practice-oriented perspective. That is, the CROP framework is not inherently discretization-invariant. Specifically, it can only operate on fixed scales chosen *a priori*. This raises a question: how can the CROP framework be adapted to handle infinitely fine resolutions? Is there a formulation of CROP that accommodates an infinite range of scales?\n\n2. The theoretical treatment of \"discretization mismatch errors,\" particularly the results in Proposition 4.4, warrants closer examination:\n   1. The authors only provide an upper bound for the error $E_{MN}$. Mathematically, this does not necessarily imply that the \"discretization mismatch errors\" will vary in line with the trends of the upper bound.\n   2. Given the upper bounds in Lemma B.2, how should we interpret the conclusion that \"discretization mismatch errors\" increase as $M$ grows? Interestingly, it appears that the upper bound does not depend on $M$.\n   3. The derivation of Lemma B.2 is relatively straightforward, especially for the terms $\\omega^L$ and $\\prod_{\\ell=1}^L C_\\ell$, making the conclusion that \"discretization mismatch errors\" may increase with $L$ and $\\omega$ unsurprising. This could likely be generalized to other neural operator models as well.\n\n3. The numerical results in Table 2 could be expanded. Specifically, it would be beneficial to include results from training resolutions of $127 \\times 128$ or even $256 \\times 256$. Many recent studies on neural operators (such as papers from ICLR 2024) include similar numerical tests, but their results appear more favorable than those shown in Table 2 of this manuscript.\n\n4. I have concerns about the numerical tests in Table 4, as the operators used in the Darcy and Poisson problems are both linear, while the neural operators being tested (FNO, CNO, U-Net, ResNet, DeepONet) are nonlinear. Moreover, the results for the Poisson problem in Table 7 do not demonstrate a significant advantage of CROP over the standard FNO."
            },
            "questions": {
                "value": "Please refer to the questions in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper first pointed out a common misconception that FNO does not depend on the resolution of input. The authors proved that resolution DOES affect FNO and defined a metric, discretization mismatch error (DME), to quantify the effect of resolution changing. Then they proposed a solution to mitigate the issue that ensures FNO's high performance across different resolutions.\n\nThe author provided estimation of DME by mathematical analysis. \nLemma B.1. provides the estimation of difference between input functions of FNO layers at different resolution $N$ and $M$, which is bounded by $o(\\frac{1}{N^{s-1}})$ assuming $N<M$ and $s\\geq2$. Lemma B.2. extended the analysis of Lemma B.1. to neural operators with lifting and projection layers.\n\nThey proposed a solution, cross-resolution operating learning pipeline (CROP), which relies on 1x1 convolution as lifting and projection layers.\nThen the authors conducted experiments to show two superior aspects of CROP: cross-resolution tasks and learning capability."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "**Originality** Though the development of neural operators has been accelerating recently, some fundamental concepts need clarification. I really appreciate the contribution of this paper that addresses the particular misconception of cross-resolution ability of FNO and puts neural operators on solid ground. The solution proposed here is based on band-limited function spaces and implemented as 1x1 convolution.\n\n**Quality** The mathematical analysis is solid and experiment is persuasive. \n\n**Clarity** I find the paper clear to read and carefully composed.\n\n**Significance** is relatively high. Cross-resolution application is a favorable feature of neural operators like FNO. The price to pay for such feature should be reminded for researchers in this field."
            },
            "weaknesses": {
                "value": "Since your work is to some degree a direct challenge to part of the arguments in [1], can you provide some insights to their evidence of \"discretization invariance\" that contradicts your findings? Namely, in Section 7 of [1], several experiments support the argument \"the error of FNO is independent of resolution or any specific discretization\", see Fig. 8 and Table 2&3. How would you explain such results?\n\n\n[1] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew\nStuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces\nwith applications to pdes. Journal of Machine Learning Research, 24(89):1\u201397, 2023."
            },
            "questions": {
                "value": "N.A."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenge of discretization mismatch errors (DMEs) in neural operators, particularly FNOs. The authors identify that while FNOs are theoretically designed to be discretization-invariant, they still exhibit performance inconsistencies across different resolutions due to DMEs. I like that they also provide a thorough mathematical analysis of how these errors accumulate through the layers of the neural network. To address this issue, they propose a novel Cross-Resolution Operator-learning Pipeline (CROP). CROP introduces lifting and projection layers that map input and output functions to and from a band-limited function space, allowing the use of fixed discretization's in the latent space. This approach effectively minimizes DMEs and enables consistent performance across different resolutions. The authors demonstrate CROP's effectiveness through extensive experiments on various partial differential equations, including the Navier-Stokes equation at high Reynolds numbers. Results show that CROP not only achieves robust cross-resolution performance but also outperforms baseline models in learning complex dynamics. Overall, this paper contributes significantly to the field of neural operators by addressing a critical limitation and proposing a solution that maintains the advantages of FNOs while overcoming their resolution-dependent weaknesses. As a person who works primarily on Neural Operators, I figured someday someone would finally do this work: ) and talk about it. I do have certain questions and concerns but overall, i really enjoyed the presentation of this paper."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Analysis: The paper  identifies and provides a rigorous theoretical analysis of discretization mismatch errors (DMEs) in neural operators, particularly FNOs, addressing a critical gap in the field. The results do make sense to me as it makes sense that although FNO does have the discretization invariance property, just training on low res - isnt guaranteed enough to get the high level features in high -res images.\n2. Unique solution: The proposed CROP method effectively mitigates DMEs through a clever use of lifting and projection layers, allowing for consistent cross-resolution performance and flexibility in choosing intermediate neural operators. \n3. Strong empirical validation: Extensive experiments across multiple PDEs, including challenging cases like high Reynolds number Navier-Stokes equations, demonstrate CROP's superior performance compared to state-of-the-art baselines.\n4. Clear exposition and reproducibility: I like the fact that the paper is well-structured, clearly written, and provides detailed information on experimental setups, enhancing understanding and reproducibility of the results as well as they don't just bash FNO's, they actually justify their reasons well."
            },
            "weaknesses": {
                "value": "Some weaknesses\n1. Variance in some results: Some empirical results, particularly in Figure 3b, show high variance. Maybe some explanation of the high variance?\n2. There's minimal discussion on how sensitive CROP is to hyperparameter choices, which is important for understanding its robustness and ease of application to new PDE problems\n3. Long-term stability in time-dependent problems: For time-dependent PDEs like Navier-Stokes, there's limited discussion on the long-term stability of CROP predictions over extended time horizons.\n4. Limited exploration of high-frequency information: While CROP addresses the issue of DMEs, the paper doesn't deeply explore how effectively it captures high-frequency information, especially in comparison to methods specifically designed for super-resolution tasks.\n5. It would be also great to have pseudocode or the full architecture."
            },
            "questions": {
                "value": "I have several questions so please bare with me:\n1  - So, is CROP still a universal approximation for operators? It seems like its not but I may be mistaken.\n2 - I do understand the concept of band limiting and then learning a NO in the intermediate representation, but to be more fair in comparison to FNO for high Reynolds tasks, you should use higher number of modes to capture those details or use something like an Incremental Fourier Neural Operator [1]. It would be great to do some ablation studies on that if you have the time. \n3 - How sensitive is CROP's performance to the choice of band-limit in the lifting and projection layers? Is there a systematic way to determine optimal band-limits for different types of problems?\n4 - How does the computational complexity of CROP compare to standard FNO, especially for very high-resolution inputs?\n5 - How does the choice of intermediate neural operator in CROP affect its performance and computational efficiency? Are there certain types of problems where specific architectures are preferable?\n6 - Also I do get you have additional experiments on deeper FNO's but it would be great to actually also use skip connections or other techniques to actually train them well. They do suffer from convergence issues."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper emphasizes that the colloquial definition of discretization invariance does not align with the formal definition. They also demonstrate both formally and empirically that popular operator learning architectures (such as FNO) do not have constant performance across resolutions for many problems. The authors propose the CROP method to address these issues with bandlimited lifting and projection operators (as well as some pointwise operators to capture high frequency details), and they show improved performance compared to popular neural operator architectures."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-written, especially for those who may not be experts in neural operator theory. I appreciate the formal definition of discretization invariance from Kovachki et al., 2021 and the discussion of the differences between this definition and the colloquial definition. The experimental results are also quite compelling and suggest that the proposed CROP method vastly outperforms other methods."
            },
            "weaknesses": {
                "value": "Although the paper has some strengths, there are still some areas of improvement. My first concern is with the novelty of the insights about discretization invariance. Previous works have included similar discussions, such as [1] (See Figure 1 and Section 2), which uses the term \u201cdiscretization convergence\u201d to refer to the formal definition by Kovachki et al., 2021. Prior works have also demonstrated empirically that FNO performs well in zero-shot super-resolution tasks on smooth problems. Given these results, it seems to me that the primary contribution of this work is some theoretical analysis of the discretization mismatch error (the Lemmas in the appendix) and the CROP method.\n\nIn regards to the proposed CROP method, it is unclear to me whether high frequencies are adequately being captured by the proposed pointwise neural network in the projection layer. For some problems, looking at the L2 error may not be sufficient to ensure that high frequencies are captured. As such, I would recommend the authors include additional evaluation metrics, such as plotting the spectrum of each and seeing how CROP vs. FNO perform on the higher frequencies (for reference, see Figure 1 in [1]).\n\nI also have some concerns about the statement of Proposition 4.4. I understand that it is meant to be written informally for a broad audience. However, I believe some aspects of it are potentially misleading: for instance, it is not necessarily always true that the discretization mismatch error must increase with the higher resolution, right? If the operator learns a constant function, for instance, then this would not strictly increase.\n\n**References:**\n1. \u201cNeural operators for accelerating scientific simulations and design\u201d (2024).\n\n**Minor notes and typos:**\n1. The authors write that \u201cThe FNO architecture is not limited to rectangular domains, periodic functions, or uniform grids.\u201d I think this might be a bit misleading, since typical instantiations of FNO are limited by these constraints: the FFT can only be applied on uniform grids, without Fourier continuation, the output of the FNO is periodic, etc. It would be ideal if the authors can add some context to this claim.\n2. Typo in line 234 \u201caN FNO.\u201d\n3. I see that the appendix contains proofs of rigorous statements. I would clearly denote that Proposition 4.4 is informal."
            },
            "questions": {
                "value": "1. In the introduction, the authors claim that \u201cIt is widely believed that training an FNO on one resolution allows inference on another without degrading its performance, since FNO operates and parameterizes the kernel on the Fourier space.\u201d Do you have references for this claim?\n2. What is L in the equation in Definition 4.1?\n3. In Proposition 4.4, it is stated that the discretization mismatch error increases with M, but M does not seem to appear in Lemma B.2. I may be missing something, but how does Proposition 4.4 follow from Lemma B.2?\n4. How much were the hyperparameters tuned for the baselines, particularly in the Navier-Stokes setting?\n5. What architectures were tested for the intermediate neural operator in CROP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}