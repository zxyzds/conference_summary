{
    "id": "lS2SGfWizd",
    "title": "Adversarial Score Identity Distillation: Rapidly Surpassing the Teacher in One Step",
    "abstract": "Score identity Distillation (SiD) is a data-free method that has achieved state-of-the-art image generation performance by leveraging only a pretrained diffusion model, without the need for any training data. In this paper, we introduce SiDA (SiD with Adversarial Loss), which not only enhances generation quality but also improves distillation efficiency by incorporating real images and adversarial loss. SiDA utilizes the encoder from the generator's score network as a discriminator, enhancing its ability to differentiate between real images and those generated by SiD. The adversarial loss is batch-normalized within each GPU and then combined with the original SiD loss, effectively integrating the average \"fakeness\" per GPU batch into the pixel-based SiD loss. This allows SiDA to distill a single-step generator either from scratch or by fine-tuning an existing one. SiDA converges significantly faster than its predecessor when trained from scratch and quickly surpasses the original model\u2019s performance after an initial warmup period when fine-tuning from a SiD checkpoint. This method has established new benchmarks for low FID scores when distilling EDM diffusion models pretrained on CIFAR-10 (32x32) and ImageNet (64x64), achieving FID scores of **1.499** on CIFAR-10 unconditional, **1.396** on CIFAR-10 conditional, and **1.110** on ImageNet 64x64.",
    "keywords": [
        "Diffusion distillation",
        "deep generative models"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=lS2SGfWizd",
    "pdf_link": "https://openreview.net/pdf?id=lS2SGfWizd",
    "comments": [
        {
            "title": {
                "value": "Response to Reviewer sPNq"
            },
            "comment": {
                "value": "We would like to thank the reviewer for providing a clear summary of the improvements of SiDA over previous methods. Regarding the weaknesses pointed out, we would like to provide the following clarifications:\n\n**W1:** We will include a comprehensive comparison on ImageNet 512x512 with a wide range of baselines, including DiT and MAR, in our revision. For instance, using a 481M model with 256 autoregressive steps, MAR (Li et al., 2024) achieves an FID of 2.74 without CFG and using 100 diffusion NFEs, and an FID of 1.73 with CFG and using 100*2 diffusion NFEs. SiD$^2$A on EDM2-S (280M) already outperforms this, achieving an FID of 1.669 without CFG and with only a single generation step. Could you clarify which paper you are referring to for RDM?\n\n\n\n**W2:** We will add ImageNet 512x512 results and detailed comparisons with existing methods in the revised paper. Please refer to our general response for the current results on this point.\n\n**W3:** The term \u201cOne-Step\u201d is defined differently in OSGAN and SiDA. In SiDA, we retain the alternating training between the discriminator (the encoder of the fake score network) and generator, referring to the single forward pass of the generator as \u201cOne-Step\u201d generation. OSGAN, however, modifies alternating training into \u201cOne-Step\u201d training, eliminating the need for alternation. We will clarify this distinction in the revision.\n\n**W4:** We commit to open-sourcing both the PyTorch code and model checkpoints. These are not provided at the moment to prevent any accidental violations of ICLR's anonymity requirements.\n\n**Q1:** A gap exists between the pretrained teacher's estimated score and the true score. The addition of an adversarial loss helps bridge this gap, guiding the generator toward a closer approximation of the true data distribution.\n\n**Q2**: In SiD, the generator is guided by a biased teacher network, while in SiDA, this bias can be corrected with the help of an adversarial loss that leverages real data. Even when using as little as 5% of the real data, this real-data-based guidance remains highly effective. Please refer to our response to Reviewer kYRQ's Q1 for further details."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer kYRQ"
            },
            "comment": {
                "value": "We appreciate the reviewer's positive feedback on our paper. \n\n- In response to the identified weaknesses, we provide the following clarifications:\n\n   - **W1:** We\u2019d like to clarify that data-free distillation, which does not require real or teacher-synthesized images, is a distinctive feature of methods like Diff-Instruct and SwiftBrush, which optimize KL divergence under diffusion, and SiD, which optimizes Fisher divergence under diffusion. In scenarios lacking real data, SiD stands as the state-of-the-art choice. Most other distillation methods, including consistency distillation, CTM, and DMD, necessitate access to real or teacher-synthesized data. SiDA not only surpasses the teacher model but also outperforms both data-free and non-data-free distillation methods.\n\n  - **W2:** We will include results on distilling EDM2 pretrained on ImageNet 512\u00d7512 in the revised paper.\n\n  - **W3:** The revised paper will detail our approach to model parameter selection. Given that SiDA demonstrates robustness to these settings, we believe the provided guidelines are sufficient. However, we are open to conducting additional ablation studies if the reviewer deems them necessary for a deeper understanding of our method.\n\n  - **W4:** A schematic illustration of the SiDA algorithm will be added to the paper.\n\n- Addressing the reviewer's specific questions:\n\n  - **Q1:** The VAE latents for the ImageNet (512\u00d7512) dataset total approximately 157 GB. In our experiments with SiDA for EDM2-XS, we tested utilizing only 5% of the images (7.8 GB). The Appendix includes a comparison of the FID trajectories between using 5% and the full dataset. The results indicate that, with just 5% of the data, the SiDA framework achieves an initial convergence rate closely following that of the full dataset. However, it reaches a plateau earlier and converges to a higher FID\u2014worse than using the full data but still clearly outperforming the teacher. This finding actually suggests potential applications of SiDA in domain adaptation, which we plan to explore in future research.\n\n  - **Q2:** We have implemented the suggested changes; please refer to our response in W2.\n\n  - **Q3:** Please see below for a separate response.\n\n  - **Q4:** The revised paper includes a description of our parameter selection process. The approach involves adjusting parameters so that the SiD loss and adversarial loss components have comparable scales for both the generator and fake score networks, maintaining values roughly between 1 and 10,000 to prevent underflow or overflow in fp16. Notably, setting the adversarial loss coefficient too low results in SiDA's performance aligning closely with SiD, as the adversarial loss becomes dominated by the SiD loss component.\n\n  - **Q5:** In our ablation study on CIFAR10 (unconditional), we observed that the current SiDA implementation with $\\alpha=1.2$ can diverge. This issue can be mitigated by extending the initial warmup period using the SiD loss alone before introducing the adversarial loss. For SiD$^2$A, which is essentially SiDA with the generator initialized from a much better starting point, the model performs well at $\\alpha=1.2$ and remains stable even when $\\alpha$ is increased up to 1.5.\n\nWe trust these clarifications address your concerns and welcome any further discussion."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer Ajww"
            },
            "comment": {
                "value": "We appreciate the reviewer\u2019s recognition of SiDA\u2019s SOTA performance.\n\n\n1. *Can SiDA be used to distill DiT-based models?*\n\n\n    **Response:** Our research primarily focuses on distilling U-Net-based EDM models, which are well-recognized and benchmarked in diffusion-based generation and distillation. We have now also added results for EDM2, which shows a significantly lower FID on ImageNet 512 x 512 compared to all existing DiT-based models, as detailed in Table 2 of the EDM2 paper and our forthcoming revision.\n\n\n     Despite this, we acknowledge the versatility of DiT-based architectures. SiDA is not inherently limited to U-Net backbones and shows potential for adapting to transformer-based architectures. For example, in the context of transformer-based diffusion models, we could divide the architecture into \u201cencoder\u201d and \u201cdecoder\u201d components. Insights from studies like REPA (Yu, Sihyun, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. \"Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think.\" arXiv preprint arXiv:2410.06940 (2024).) suggest that aligning self-supervised features from the encoder, particularly those from middle layers (6th to 8th in a 24-layer model), with a diffusion model could enhance DiT/SiT model training.\n\n\n    While integrating SiDA with transformer-based models is intriguing, this exploration is beyond the scope of our current research. We plan to pursue this direction in future projects and invite researchers with the necessary computing resources to adapt our SiDA code, which will be open-sourced, to distill DiT-based models and contribute their findings to the community.\n\n\n2. *Can SiDA be applied to high-resolution models?*\n\n\n     **Response:** Yes, SiDA excels in high-resolution settings, achieving remarkably low FIDs on ImageNet 512x512 without CFG and through one-step generation. We have documented our current results for distilling EDM2 and will provide further updates after revising our paper based on the reviewers' feedback.\n\n\n3. *Why is $L^1(\\theta)$ still necessary with an additional discriminator, i.e., why does SiDA still require tuning $\\alpha$?*\n\n\n     **Response:** Omitting $L^1(\\theta)$, or setting $\\alpha=0$, was feasible in SiD but resulted in unsatisfactory FID scores. With the inclusion of adversarial loss, this configuration in both SiDA and SiD$^2$A now yields satisfactory outcomes, yet still underperforms compared to the recommended settings of $\\alpha=1.0$ for SiDA and $\\alpha=1.0$ or $1.2$ for SiD$^2$A. This indicates that, despite the support from adversarial loss, maintaining $\\alpha>0$ is important to correct the approximation bias in the SiD gradient effectively."
            }
        },
        {
            "title": {
                "value": "Preliminary Response: Addressing Concerns on High-Resolution Image Results"
            },
            "comment": {
                "value": "Dear Reviewers, \n\nWe appreciate your valuable feedback. While we are preparing a comprehensive revision, we would like to promptly address the shared concern regarding the absence of results for high-resolution images. To this end, we have adapted the SiDA codebase to distill EDM2 models of various sizes pretrained on ImageNet 512\u00d7512, yielding the following results:\n\n| **Method** | **CFG** | **NFE** | **XS (125M)** | **S (280M)** | **M (498M)** | **L (777M)** | **XL (1.1B)** | **XXL (1.5B)** |\n|------------|---------|---------|---------------|--------------|--------------|--------------|---------------|----------------|\n| EDM2       | N       | 63      | 3.53          | 2.56         | 2.25         | 2.06         | 1.96          | 1.91           |\n| EDM2       | Y       | 63      | 2.91          | 2.23         | 2.01         | 1.88         | 1.85          | 1.81           |\n| sCT        | Y       | 1       | -             | 10.13        | 5.84         | 5.15         | 4.33          | 4.29           |\n| sCT        | Y       | 2       | -             | 9.86         | 5.53         | 4.65         | 3.73          | 3.76           |\n| sCD        | Y       | 1       | -             | 3.07         | 2.75         | 2.55         | 2.40          | 2.28           |\n| sCD        | Y       | 2       | -             | 2.50         | 2.26         | 2.04         | 1.93          | 1.88           |\n| SiD        | N       | 1       | 3.353 \u00b1 0.041 | 2.707 \u00b1 0.054| 2.060 \u00b1 0.038| 1.907 \u00b1 0.016| 1.888 \u00b1 0.030 | -              |\n| SiDA       | N       | 1       | 2.228 \u00b1 0.037 | 1.756 \u00b1 0.023| 1.546 \u00b1 0.023| 1.501 \u00b1 0.024| 1.428 \u00b1 0.018 | -              |\n| SiD\u00b2A      | N       | 1       | **2.156 \u00b1 0.028** | **1.669 \u00b1 0.019** | **1.488 \u00b1 0.038** | **1.413 \u00b1 0.022** | **1.379 \u00b1 0.017** | -              |\n\nThe results for EDM2 are sourced from Karras et al. (2024), while those for sCT and sCD are obtained from Lu and Song (2024). Although Lu and Song's work is concurrent with ours, we include their results here to highlight the superior performance of SiDA.\n\nWe observe that SiD\u00b2A achieves an FID of 1.379 on EDM2-XL, marking the lowest score reported to date. This result is notable as it is obtained without classifier-free guidance (CFG), post-hoc exponential moving average (EMA), and utilizes only a single generation step. Furthermore, both SiDA and SiD\u00b2A applied to EDM2-S (280M parameters) surpass the performance of the largest teacher model, EDM2-XXL (1.5B parameters), as well as sCD and sCT methods. We also note that SiD, the baseline model adapted by us for EDM2, achieves performance comparable to that of the teacher model. This observation aligns with previous findings on smaller datasets and EDM models, demonstrating SiD's effectiveness across varying scales. \n\n**References:**\n\n- Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. \"Analyzing and Improving the Training Dynamics of Diffusion Models.\" In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 24174\u201324184, 2024.\n\n- Cheng Lu and Yang Song. \"Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models.\" *arXiv preprint arXiv:2410.11081*, 2024.\n\nWe will provide a more detailed response and a revised paper in due course."
            }
        },
        {
            "summary": {
                "value": "The authors propose SiDA (Score identity Distillation with Adversarial Loss), an enhanced version of Score identity Distillation (SiD) that aims to improve diffusion model training efficiency. The key innovation is the introduction of an adversarial loss that helps distinguish between real and generated latents at different noise levels. By reusing the fake score network in the original SiD, the authors propose a discriminator design that utilizes the encoder feature map to obtain the discriminator score. The proposed method achieves SOTA results on CIFAR-10, ImageNet-64, and other datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "-\tThe authors repurpose the fake score network as an additional discriminator with direct using the average of the encoder features as the discriminator score. This design introduces the adversarial objective to the original SiD with minimal additional parameter cost.\n-\tThe proposed method SiDA significantly outperforms SiD and achieves superior 1-step generation performance on various datasets."
            },
            "weaknesses": {
                "value": "-\tThe discriminator design is heavily based on the U-Net encoder-decoder architecture. It is not straightforward to generalize to transformer-based diffusion models, e.g., DiT, where there is no clear partition of the encoder and decoder.\n-\tThe current submission lacks the evaluation of Diffusion models on higher resolution, e.g., ImageNet 256x256. The cost overhead from both discriminator training and maintaining an additional fake score network can be exaggerated on high-resolution data."
            },
            "questions": {
                "value": "-\tWhy $L^1(\\theta)$ is still needed with the additional discriminator, i.e., why SiDA still needs tuning $\\alpha$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Score Identity Distillation with Adversarial loss. The method aims to distill a teacher diffusion model to reduce the number of inference steps to only one. The primary contribution is the combination of SiD with the use of real images and adversarial objective, which not only reduces the number of required inference steps but also improves generation quality, achieving a lower FID score across Cifar10, ImageNet-64, FFHQ-64 and AFHQ-64."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Achieves state-of-the-art performance with only one inference step\n- The methods demonstrates improvement over the teacher network thanks to the real images\n- Training remains relatively stable despite the use of an adversarial loss"
            },
            "weaknesses": {
                "value": "- Requires real images for training, which is not necessary for other distillation methods\n- Experiments are demonstrated only on very small images resolution\n- Ablation studies are limited to the parameter $\\alpha$\n- A schematic of the framework would greatly improve the paper's readability."
            },
            "questions": {
                "value": "- How does the method perform as the number of available real images changes? What happen if you use only 10% of the training real images?\n- Why not apply latent diffusion for larger images? Using VAE to compress the image would help to see if the method can scale to bigger images, without increasing the training time\n- Does the approach generalize to teacher models other than VP-EDM?\n- The paper arbitrarily sets $\\lambda_{sid}$ = 100 and $\\lambda_{adv}$ = 0.01 without discussion or ablation, although these hyperparameters likely play an important role in the method. How do these values influence the performance metrics?\n- In Figure 1, for $\\alpha$=1.2 (the yellow line), after approximately 1000 iterations, the Inception Score (IS) drops and the FID increases sharply; additionally, SiDA with $\\alpha$=1.2 is absent from Table 1. Does this imply that the model fails to converge?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces an improved version of score identity distillation and proposes training SiD with an adversarial objective. \nThe main motivation is that the lack of real data in the original training of SiD can potentially lead to discrepancies to the original data distribution. The authors propose to reuse the encoder part of the fake score model as the backbone of discriminator, and use its predicted realness and fakeness scores to improve the generation results further, at no cost of additional model parameters. By combining SiD with adversarial training, together with additional techniques such as initializing the student model with the SiD pretrained model, the proposed method achieved improved one-step diffusion results on multiple datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The discussions in the experiment section are very comprehensive. The detailed analysis of the training statistics across the training steps is appreciated. \n\n- The paper presented the background such as SiD and adversarial loss in diffusion well."
            },
            "weaknesses": {
                "value": "- Presentation\n\nIn the technical parts of the paper, which is mainly the Section 4. I frequently find the presentations not clear enough. My biggest confusion arises from the 'Loss Construction that Respects the Weighting of Diffusion Distillation Loss' paragraph. I understand that balancing loss terms can be tricky, and I acknowledge that this is an important topic to discuss. But from what the author presents, I didn't find how the proposed weight 'honors the existing weighting mechanisms' and, more importantly, automatically balances the loss regardless of the datasets. I read the Eq 7 and Eq 8 and found nothing particularly new. How is Eq 7 fundamentally different from PatchGAN? \n\n- Novelty\n\nAlthough the paper targets a problem of SiD that is not previously discussed, the solution of adding an adversarial term is by no means a novelty under the context of diffusion distillation. The adversarial object has been proven very effective in many works, such as UFOGen and SD3-turbo. The proposed method here is a simple mixture of two existing methods. \n\n- Evaluation and performance\n\nMost of the experiment settings used in this paper are small-scale. And the authors only compared the proposed method with very few other methods in the 512x512 ImageNet experiment. Since efficiency is one of the main advantages the authors are trying to advocate, it is necessary to include larger-scale evaluations, such as text-to-image experiments, to allow for fair comparisons with recent methods. \n\nI also have a slight concern about the advantage of efficiency. It is true that, according to Figure 3, the FID of the proposed method decreases faster compared to the baseline SiD in the later half of the training; I'd like to point out that this is achieved with an additional loss term, meaning each iteration probably optimizes the model with a larger gradient magnitude. Also, the adversarial loss term will inevitably introduce more computation, so the model is not necessarily able to finish training faster compared to SiD."
            },
            "questions": {
                "value": "Please see the Weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an intuitive method, which mainly focuses on the improvement of score identity distillation method by introducing additional real images for adversarial training. The improvement is two fold. First, additional real images can boost generation performance over data-free score identity distillation by discrimating real and synthetic images. Second, it achieves faster convergence rate relative to the baselines. The experimental results on small-scale dataset support the superority of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+ This paper proposes an intuitive extension of score identity distillation and effectively improves the generation performance on small-scale datasets, such as CIFAR and ImageNet 64x64.\n+ The proposed method achieves better performance than the main baseline, SiD and other diffusion methods."
            },
            "weaknesses": {
                "value": "+ More state-of-the-art generative methods are missing, such as DiT, RDM and MAR.\n+ The method only conducts experiments on small-scale datasets or low-resolution images. I'm concern about the effectiveness of this method on large-scale dataset with high-resolution images. More results on large-scale datasets with high-resolution images consolidate the effectiveness of this method.\n+ OSGAN[R1] adopts an one-step adversarial training on generative task. The comparion with OSGAN is needed to distinguish their differences and highlight the contributions of this paper.\n+ This paper doesn't mention any plan about open source of code and trained weights. In spite of encouraging performance, I'm concern about the reproducibility of this paper. \n+ Some motivations are not well clarified in the paper. See Questions section.\n+ Some unnecessary equations, such as Eq between 149 and 151, can be removed.\n\n[R1] Training Generative Adversarial Networks in One Stage. CVPR 2021."
            },
            "questions": {
                "value": "+ Why can incorporating average \"fakeness\" within each GPU batch into each pixel's loss solve the gap between diffusion distillation and adversarial learning?\n+ Why does the proposed method can achieve faster convergence than baseline when trained from scratch?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}