{
    "id": "kDakBhOaBV",
    "title": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data",
    "abstract": "Current trends in pre-training Large Language Models (LLMs) primarily focus on the scaling of model and dataset size.\nWhile the \\textit{quality} of pre-training data is considered an important factor for training powerful LLMs, it remains a nebulous concept that has not been rigorously characterized.\nTo this end, we propose a formalization of one key aspect of data quality -- measuring the \\textit{variability} of natural language data -- specifically via a measure we call the diversity coefficient. \nOur empirical analysis shows that the proposed diversity coefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. \nThen, we measure the diversity coefficient of publicly available pre-training datasets and demonstrate that their formal diversity is high compared to theoretical lower and upper bounds.\nFinally, we conduct a comprehensive set of controlled \\textit{interventional} experiments with GPT-2 and LLaMAv2 that demonstrate the diversity coefficient of pre-training data characterizes useful aspects of downstream model evaluation performance---totaling 44 models of various sizes (51M  to 7B parameters).\nWe conclude that our formal notion of diversity is an important aspect of data quality that captures variability and causally leads to improved evaluation performance.",
    "keywords": [
        "data centric machine learning",
        "large language models",
        "nlp",
        "natural language processing",
        "diversity",
        "data metrics"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "we demonstrate quantitatively that pre-training models on more formal data leads to better performance",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kDakBhOaBV",
    "pdf_link": "https://openreview.net/pdf?id=kDakBhOaBV",
    "comments": [
        {
            "summary": {
                "value": "The authors present a measure of text corpus diversity called \"diversity coefficient\" based on the cosine distance between Task2Vec vectors constructed from random subsets of the data. Arguing that the variability of pretraining data partly explain in-context learning and generalizability of LLMs, the authors show a correlation between the diversity coefficient and various performance metrics. On synthetic data with controlled variability, the proposed diversity coefficient follows human intuition about variability in datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper tackles an important problem, potentially deepening our understanding of the effects of pretraining data quality particularly with respect to diversity.\nThe approach combines concepts from vision (task2vec) with language modeling to create an original measure for textual data diversity."
            },
            "weaknesses": {
                "value": "1. The use of cosine distance between task2vec embeddings as a diversity measure is not well motivated. Can we consider other textual embedding methods? What are the tradeoffs?\n2. The absolute diversity coefficient value does not convey much information about what it means - I understand the author's argument for conceptual lower and upper bounds but these do not capture any representation of real natural language. The work should present a better way to understand the value of the measure of diversity.\n3. The experiments on synthetic datasets are valuable, however the conclusions could be further strengthened by a study utilizing human annotation for diversity."
            },
            "questions": {
                "value": "In section 3.5, the authors use the GINC dataset generated from a mixture of hidden markov models - how natural is the resulting dataset? Does it come close to resembling text found on the web?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a metric to measure pre-training data diversity and analyses how data diversity affects model test time performance and model emergent capabilities. The diversity metric is based on an existing method to represent the data, Task2Vec. Textual data is encoded into a vector according to Task2Vec and data vectors' distance is used to estimate data diversity.  Task2Vec vectors are computed as the diagonal of the Fischer Information Matrix of the parameters \\phi of the language model head (only the rest of the network is fixed) when these \\phi parameters are fine-tuned on the target data to assess. On experiments, the authors show that higher data diversity relates to better test time performance, that mixed datasets give more diversity, and that the distribution of data distances can be used to characterise the datasets. \n\nWhile the rational behind the proposed method seems clear, I lack a bit of context on what previous work was done in relation to this issue (was diversity --and how measured-- use in data selection for known LLMs?) and existing approaches and baselines.  Perhaps this is discussed more in the Appendix sections, I suggest this should be made clearer in the main paper. Also, how this metric would help in the selection of data for pre-training?"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "A tool to measure pre-training data diversity."
            },
            "weaknesses": {
                "value": "- While the authors evaluate performance against pre-training data diversity (section 3.1), the metric for performance is cross-entropy loss in LM, I wonder whether specific task evaluation on NLP tasks would make sense here (many NLP benchmarks used to benchmark LLMs on tasks like question answering, GLUE, etc.).\n- The Vendi Score seems to be another approach to compute diversity, why was not included as a baseline in the main experiments (at least in main Table 1)?"
            },
            "questions": {
                "value": "1. Line 122, which t is used to compute the FIM matrix for sequences of a batch? Are all the steps from the sequence averaged? or is FIM only build based on the last step?\n2. I suggest the authors to rewrite some pieces of text. There are places where authors use phrases like \"by them\" , \"they ...\" and it is not clear what the referents are what makes the reading and understanding of the paper more involved.\n3. Task2Vec should be fully and better described in the main paper (including how was used before, its intuition, etc.).\n4. Section I.2 should mention the data used to pre-train GPT2.\n\nMinor comments:\n\n- Figure 1 does not seem to add much to the understanding of the approach, maybe an algorithm would be more useful?\n- All figures are too small to read.\n- Line 751, \"this more sophisticated aggregation method\" which aggregation method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a metric to measure the diversity of language modeling pre-training datasets. The proposed metric is the expected distance between Task2Vec embeddings of random batches sampled from a dataset. This metric is applied to several pre-training datasets, and compared to upper and lower bounds. The paper presents some experiments aiming to relate the proposed measure of diversity with downstream perplexity on C4 and OpenWebText2. In a synthetic setting, the proposed diversity metric increases with vocabulary size and number of latent concepts."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This work focuses on an important and potentially very impactful area of research. While the general belief of the community is that data diversity is an important factor for the performance of language models, the relationship between pre-training data diversity and downstream performance is currently poorly understood. The use of Task2Vec embeddings to measure data diversity is to the best of my knowledge novel. The diversity results presented in Table 1 align well with current intuitions in the community regarding CC-derived datasets being more diverse than non-CC Pile subsets."
            },
            "weaknesses": {
                "value": "> Claim: \u201cPRE-TRAINING IN HIGHER DIVERSITY LEADS TO BETTER EVALUATION PERFORMANCE\u201d (Section 3.1)\n\nThe experiments in Section 3.1, aiming to relate their proposed measure of data diversity with performance, which in my view would amount to the most substantial contribution of the paper, are highly unconvincing. \n\nOnly three datasets are considered: PubMed, USPTO, and PubMed+USPTO. Linear regressions on three (!) datapoints are presented as evidence to substantiate the authors claim. Clearly, the actual relationship between diversity and performance will neither be linear nor monotonic \u2014 pre-training on random tokens (e.g., the upper bound of perplexity considered by the authors) would certainly lead to very bad performance while having maximal diversity. For most of the GPT-2 experiments, PubMed+USPTO has similar or higher loss than PubMed while also having higher diversity.\nTo make matters worse, the choice of the 2 pre-training datasets are highly unusual and not representative of the pre-training corpora of current language models. More convincing choices would have been C4, OpenWebText, The Pile (in its entirety), RedPajama, SlimPajama, RefinedWeb, Dolma, FineWeb, DCLM, \u2026 \nThe experiments consist of many small-scale training runs, where the differentiating factor is scale (e.g., number of parameters and training tokens) and architecture (e.g., GPT-2 or Llama 2). Since what authors aim to study is the relationship between dataset diversity and performance, what should vary are the datasets. The results could be much more convincing if authors were to consider a single architecture and scale  (e.g., Chinchilla optimal 1B model), but as many pre-training datasets as possible.\nChoice of evaluation metric: it is unclear why PPL on OpenWebText2 and C4 is a meaningful evaluation metric. For example, does PubMed lead to lower PPL because of its higher diversity, or because it is simply more similar to C4/OpenWebText? To get around this problem, work on pre-training data quality has started to use \u201cearly-signal benchmarks\u201d \u2014 see the FineWeb or DataComp-LM papers.\n\n\n> Other empirical results are less significant\n\nIn addition to the empirical results of Section 3.1, authors also show that concatenating datasets leads to higher diversity under their proposed diversity metric (Section 3.3), that the proposed cross diversity coefficient is higher than a single dataset\u2019s diversity and leads to well-sepearted histogram distance clusters (Section 3.4), and that in a synthetic setting the proposed diversity metric increases with vocabulary size and number of latent concepts (Section 3.5). While these results are reasonable checks on the proposed diversity metric, they are not particularly compelling contributions. It would be interesting to know if other simpler embeddings approaches (e.g., N-gram, mean GPT-2 last layer embedding) fail such checks.\n\n\n> It is not well motivated why to use Task2Vec encodings\n\nMuch prior work has considered N-gram or vector embedding based similarity metrics for text. While this work novelly considers Task2Vec embeddings for text, it is unclear why Task2Vec embeddings are preferable. While some discussion is provided in Appendix C (which I would encourage authors to partly move to the main text \u2014or at least reference), it would be much more compelling if authors replicated the core empirical results of the paper (e.g., Figures 2, 3, and 4)  to show the limitations, if any, of simpler, previously proposed similarity measures.\n\nThe writing is sufficiently clear but could certainly be improved. The legibility of figures could also be improved."
            },
            "questions": {
                "value": "See the specific points made in the limitations section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel approach to using diversity as a metric for evaluating the quality of training data used in training language models. The authors propose that this diversity metric correlates with downstream performance and use it to intervene in data compositionality. They claim that their metric is closely tied to model evaluation outcomes.\n\nThe diversity metric is computed using Task2Vec, which generates a vector representation based on text batches. This method relies on a GPT-2 embedding model, and the diversity score is determined by calculating the expected cosine distance between different batches of data.\n\nThe authors conduct multiple experiments across various datasets to calculate diversity scores, establishing lower and upper bounds to provide context for their results. To demonstrate the utility of the metric, they create a new dataset by merging two datasets, controlling for size, and training several language models of different sizes and architectures. They claim to show a correlation between the diversity metric and model quality, as measured by cross-entropy.\n\nWhile the premise of the paper is clear and motivated, there are several flaws in the evaluation and claims. These shortcomings, along with suboptimal writing, suggest that the paper is not ready for publication in its current form. Additionally, several related works that should have been cited for comparison are missing. Below, I outline specific issues:\n\n\nValidity of Claims\n--------\n\n- The authors assert that their work represents a paradigm shift in data-centric machine learning through the introduction of data diversity (L. 59). This claim is factually incorrect, as many previous studies have explored data diversity. Several relevant papers, which I list at the end of this review, were overlooked.\n- The claim that their method is interpretable is problematic. The term \"interpretable\" can be subjective, and in this case, the proposed metric is as much a black-box as other metrics that output a single number. Describing the metric as interpretable seems like an overreach. Additionally, the range of values produced by the metric is not intuitive, with a lower bound around 0.05 and an upper bound of 0.4, which feels inelegant.\n- The authors claim that higher diversity leads to better performance (Figure 2), but this relationship does not hold consistently. In some cases, performance declines with increased diversity, which the authors fail to address. Moreover, the presentation of results in graphs rather than tables makes it difficult to investigate this trend thoroughly.\n- Importantly, the paper does not adequately account for potential confounding factors. When merging two datasets to create a more diverse one, the authors do not consider that other characteristics of the combined dataset may influence performance, aside from diversity.\n\nIn addition to these major concerns, there are minor inaccuracies in some claims:\n- L.111: The assertion that the probe network\u2019s parameters are the most important for solving the task may be incorrect. This point doesn\u2019t heavily impact the paper\u2019s argument but should either be substantiated or revised.\n- L.266+269: The bounds discussed are described as theoretical, but they are actually empirical.\n\n\nEvaluation\n-----------\n\nThe evaluation is based solely on cross-entropy scores from some evaluation data, which is not a strong or convincing measure. Even if all results showed a consistent trend (which they do not), the claim that increased diversity leads to better downstream performance is weakly supported.\n\nFormatting and Clarity\n--------------\n\nWhile relatively minor, the following formatting and clarity issues should be addressed in future revisions:\n- L.33: Missing parentheses around citations (use \\citep).\n- L.66: The meaning of \u201clatent concepts\u201d is unclear and should be explained.\n- L.69: There is a grammatical issue with \u201cby them to.\u201d\n- Figure 1 is too small and would benefit from a size increase for readability.\n- L.122: In the formula, \u201ct-1:1\u201d should be \u201c1.\u201d\n- L.151: The phrase \u201cpre-trained on the English language\u201d is awkward. The model is trained on a corpus in English, not on the language itself.\n- Captions for tables and figures are unnecessarily long.\n- L.191-192: The argument about the lower bound is unclear.\n- L.197: The term \u201cnon-special\u201d token needs clarification.\n- L.209: The meaning of \u201cformal diversity\u201d is unclear.\n- Figure 3: Breaking it into subfigures (a-d) would make it easier to refer to each subfigure separately.\n- L.317: The notation \u201c+0.03-0.05\u201d is unclear and needs to be clarified.\n- Section 3.4: This section seems out of place. It should be moved to the beginning to help establish the validity of the metric early on.\n- L.360: It seems that \u201cright/left\u201d was meant instead of \u201ctop.\u201d\n\nRelevant Previous Work\n---------------\n\n- https://arxiv.org/pdf/2307.12532\n- https://arxiv.org/pdf/2205.06253\n- https://arxiv.org/pdf/2407.15724\n- https://arxiv.org/pdf/2103.03399v2\n- https://arxiv.org/abs/2311.08695\n- https://proceedings.mlr.press/v162/fang22a/fang22a.pdf\n- https://arxiv.org/pdf/2403.00553"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The introduction of a metric aimed at capturing the diversity of textual data is a valuable contribution. This could be useful for further research into how data composition impacts model training and performance.\n- The authors provide some evidence supporting the idea that diversity influences model performance, specifically in terms of perplexity. This demonstrates a potential relationship between diversity and model generalization capabilities."
            },
            "weaknesses": {
                "value": "- Several claims made by the authors raise concerns regarding their validity. In particular, some of their key assertions seem overstated or lack sufficient evidence.\n- The evaluation provided is not convincing. The results do not strongly support the paper\u2019s main claims, and the authors fail to address important confounding factors in their experiments.\n- The paper\u2019s structure and writing quality leave much room for improvement. Greater clarity in both the organization of ideas and the precision of language would significantly enhance the paper's readability and overall impact."
            },
            "questions": {
                "value": "Suggestions are listed in the main review."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}