{
    "id": "YaBiGjuDiC",
    "title": "Common Pitfalls of Margin-based Preference Optimization in Language Model Alignment",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become the predominant approach for aligning language models (LMs) to be more helpful and less harmful. At its core, RLHF uses a margin-based loss for preference optimization, which specifies the ideal LM behavior only in terms of the difference between preferred and dispreferred responses. \nThis under-specification of ideal behavior for each response individually leads to two unintended consequences as the margin increases:\n(1) The probability of dispreferred (e.g., unsafe) responses may increase, resulting in potential safety alignment failures.\n(2) When the probability of dispreferred responses is reduced, this often coincides with a decrease in the probability of preferred responses, even when these responses are ideal.\nIn this paper, we identify the fundamental issue: margin-based preference optimization loss under-specifies ideal LM behaviors. \nWe derive key conditions under which the probabilities of both preferred and dispreferred responses increase or decrease together. \nThese conditions occur when the inner products between the gradients of the log-probabilities of preferred and dispreferred responses are large. \nWe theoretically analyze when such inner products are large and empirically validate our findings. \nOur framework also reveals important differences in the training dynamics of various preference optimization algorithms and suggests new directions for developing better algorithms for language model alignment.",
    "keywords": [
        "Alignment",
        "Preference Optimization",
        "Large Language Model"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=YaBiGjuDiC",
    "pdf_link": "https://openreview.net/pdf?id=YaBiGjuDiC",
    "comments": [
        {
            "title": {
                "value": "typos"
            },
            "comment": {
                "value": "Really nice paper. The theoretical results are appealing. I want to point out the typos (they do not influence the correctness of the paper but may improve readability\uff09:\n\n1. **Lines 197 to 198 and 262 to 263**: The first `\u03b7` in each line should include a negative sign. This adjustment aligns with the negative sign in the loss function. The specific reason is:\n\n   ```\n   f(\u03b8 - \u03b7 x) \u2248 f(\u03b8) + f'(\u03b8)(-\u03b7 x) then  f(\u03b8 - \u03b7 x) - f(\u03b8) \u2248  f'(\u03b8)(-\u03b7 x)\n   ```\n\n2. **Lines 211, Case 3**: The expression should be `log \u03c0_w increases more`.\n\nThank you for the hard work on this paper. It has truly inspired me."
            }
        },
        {
            "summary": {
                "value": "This paper considers a common failure mode of RLHF algorithms like DPO, in which, although the difference between preferred and dispreferred outputs is guaranteed to increase, there is no guarantee that the log likelihood of the preferred option will increase, or that the log likelihood of the dispreferred option will decrease.  This paper derives a general criterion that can be used to identify pairs in which either both will increase, or both will decrease."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The theoretical analysis was enjoyable to read. The examples demonstrate the proposed criterion quite clearly."
            },
            "weaknesses": {
                "value": "The provided examples are tremendously simplified, which is useful to demonstrate the applicability of the theoretical analysis, but which reduces confidence that the analysis will have much impact on real LLM practice."
            },
            "questions": {
                "value": "Is there any strong argument that the problematic behavior addressed by this paper (both go up, or both go down) is important in real-world settings?\n\nI was intrigued by the claim that the log likelihood of a dispreferred output can go up, but this seems to not be a problem that has concerned any previous authors.  The alternative approaches discussed in the paper all address the problem that the log probability of the preferred option might go down, and that outcome is the outcome suggested by the last example analyzed in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates issues in margin-based preference optimization for language model alignment, with a focus on the Direct Preference Optimization (DPO) algorithm. It highlights that margin-based loss specifies only the ideal margin behavior, overlooking individual term behaviors, which often results in synchronized log probability changes between chosen and rejected responses. The authors theoretically derive conditions for this phenomenon starting with DPO, and extend their insights to other margin-based algorithms. They examine cases where the derived condition holds, noting that higher token similarity between chosen and rejected responses can intensify synchronized movement, and validate these findings empirically."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper stands out in its originality by addressing the under-explored limitations of margin-based preference optimization in RLHF, offering fresh insights into its impact on language model alignment. The theoretical contributions, specifically the gradient inner product conditions, demonstrate high quality, providing a clear, mathematically grounded framework that identifies when alignment failures may occur under existing loss structures. \nIn addition to an in-depth focus on the DPO algorithm, the paper comprehensively reviews existing methods in Table 2 and extends its findings in Section 3.2, which gives additional quality to the paper.\nThe suggested condition in DPO is further substantiated with its practical implication, with experimental validation in Section 4."
            },
            "weaknesses": {
                "value": "The paper centers primarily on DPO in Sections 3.1 and 4, with limited coverage of other margin-based optimization algorithms. To better align with reader expectations, it would be beneficial to introduce their main focus of DPO explicitly in the introduction and outline how subsequent sections will compare DPO against alternative approaches. Alternatively, devoting more space to a detailed examination of other algorithms could create a more balanced narrative.\n\nThe experimental results in Section 3.3 primarily observe phenomena regarding preferred and dispreferred response probabilities increasing or decreasing together\u2014findings that have already been discussed in previous work, such as Pal et al. (2024), and does not fully investigate the conditions under which these probability trends occur. Furthermore, the experiment relies on a single dataset, limiting the depth of empirical validation. \n\nThe empirical observations are concise but lack important methodological details. For instance, it would be valuable to clarify how log probabilities are computed in Figure 1 (e.g., whether they are averaged over the training samples), and to include additional measures such as standard deviations or confidence intervals to improve the figure's reliability. Since Figure 1 is intended as a motivating visualization, providing a additional figure for dedicated to empirical results (Section 3.3) would allow for a more thorough presentation of the experimental findings without cluttering the introductory material.\n\nFor minor clarification, it would be better to change the interval of the grid to clearly show the trend of margin in Figure2."
            },
            "questions": {
                "value": "1. The paper presents a clear delineation between preferred and dispreferred samples. \nHowever, some papers focus on distributional framework. This approaches insists the model to encounter a broader range of sample types to cover the distribution's full support, supporting the original RLHF sampling method.\n- Go, Dongyoung, et al. \"Aligning language models with preferences through f-divergence minimization.\"\u00a0arXiv preprint arXiv:2302.08215\u00a0(2023).\n- Korbak, Tomasz, et al. \"On reinforcement learning and distribution matching for fine-tuning language models with no catastrophic forgetting.\"\u00a0Advances in Neural Information Processing Systems\u00a035 (2022): 16203-16220.\n\n Could the authors discuss the potential impacts of their idealized setup compared to a distributional approach? This clarification would help assess the implications of choosing one framework over the other in alignment training.\n\n2. Training with both high- and moderate-quality responses, where one is better but another may serve as a hard negative example, can push the model to better distinguish between fine-grained preference levels. The ideal condition that assume clear difference of the quality  between preferred and dispreferred samples, seems to take a different approach. Does this work incorporate hard negative samples, or is it constrained to cases with explicit distinctions?\n\n3. The findings on token-level gradients provide an intriguing direction for refining preference optimization. Could the authors elaborate on any potential methods to leverage token-level distinctions more effectively within their model, perhaps as a future research direction or via fine-grained adjustments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper identifies and analyzes a fundamental problem with margin-based preference optimization in language model alignment (like RLHF): these methods only specify how much better preferred responses should be compared to dispreferred ones, without specifying how the individual probabilities should behave. This leads to two unintended consequences: (1) sometimes the probability of dispreferred (e.g., unsafe) responses increases, and (2) when reducing dispreferred response probabilities, the probability of preferred responses often decreases too, even when they are ideal."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper addresses a fundamental issue in language model alignment through RLHF by analyzing the under-specification problem in margin-based preference optimization - this is an important contribution given how widely RLHF is used."
            },
            "weaknesses": {
                "value": "1  Presentation Issues:\nThe notation becomes dense and potentially confusing in Section 3.2, particularly around the general form analysis\nSome figures (like Figure 4b) would benefit from more detailed explanations of their implications\nThe connection between the theoretical results and practical implications could be more clearly articulated\n\n2  Limited Experimental Validation:  \nExperiments are conducted on only one main dataset (TL;DR) and a simple sentiment classification task\nOnly tested on two main models (Mistral 7B and Llama-3 8B)\nLacks validation across diverse preference optimization scenarios and task types\nMore extensive empirical validation across different datasets and model architectures would strengthen the claims\n\n3  Theoretical Assumptions:\nThe theoretical analysis in Section 4.1 relies on somewhat simplified model settings (learnable last linear layer, learnable logits)\nSome assumptions about token probability relationships between chosen/rejected responses may not hold in practice\nThe extension of single-token results to multi-token cases could be more rigorously justified"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper criticizes margin-based preference optimization for LM alignment, which is a core method in RLHF. While increasing preference margins can reduce unsafe responses, it also often reduces preferred response probabilities, due to synchronized probability shifts between chosen and rejected responses. The authors reveal when these shifts occur and suggest that the under-specification of behaviour in current margin-based methods limits model safety and effectiveness. The work proposes fine-grained, token-level approaches as potential improvements, though this is not really pursued very deeply in this work."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper provides a fairly nuanced and theoretically grounded critique of margin-based preference optimization, which is an increasingly relevant task (especially  in RL). Identifying conditions under which preference optimization can backfire is important for real-world use and may provide a relatively underdeveloped area of research\n- The proposal to address preference optimization problems at the token level is interesting (though perhaps not surprising)."
            },
            "weaknesses": {
                "value": "- this is not meant to be a primarily empirical paper (apparently, it leans more on its theoretical aspects) but this work of course calls for broader empirical validation than it has. The proposed method may be evaluated across diverse tasks such as summarization, dialogue, and instruction-following. Other variants of the given LLMs (e.g., LLaMa) could have been explored, to demonstrate generalizability. \n- The analysis of Sec 4 focuses on synthetic, controlled scenarios. The kinds of complexities exist that in actual, diverse language data should be addressed."
            },
            "questions": {
                "value": "- While the theoretical analysis of gradient inner products and synchronized probability shifts is fairly rigorous, what actionable insights for real-world RLHF applications are achievable?\n- How would you evaluate the \u2018safety-critical alignment tasks\u2019 you mention in Sec 5 as being particularly ill-suited for margin-based preference optimization?\n- Can you clarify more directly how token-level gradient inner products between chosen and rejected responses provide a reliable indicator for alignment behaviour?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}