{
    "id": "YWtT5OGOAI",
    "title": "Practical Epistemic Uncertainty Quantification for View Synthesis",
    "abstract": "View synthesis using Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) has demonstrated impressive fidelity in rendering real-world scenarios. However, practical methods for accurate and efficient epistemic Uncertainty Quantification (UQ) in view synthesis are lacking. Existing approaches for NeRF either introduce significant computational overhead (e.g., \"10x increase in training time\" or \"10x repeated training\") or are limited to specific uncertainty conditions or models. Notably, GS models lack any systematic approach for comprehensive epistemic UQ. This capability is crucial for improving the robustness and scalability of neural view synthesis, enabling active model updates, error estimation, and scalable ensemble modeling based on uncertainty. In this paper, we revisit NeRF and GS-based methods from a function approximation perspective, identifying key differences and connections in 3D representation learning. Building on these insights, we introduce PH-Dropout, the first real-time and accurate method for epistemic uncertainty estimation that operates directly on pre-trained NeRF and GS models. Extensive evaluations validate our theoretical findings and demonstrate the effectiveness of PH-Dropout.",
    "keywords": [
        "Deep Learning",
        "Epistemic Uncertainty Estimation",
        "View Synthesis"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "A Simple, Training-Free, yet effective method for Epistemic Uncertainty Estimation in View Synthesis, working for both NeRF and Gaussian Splatting",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=YWtT5OGOAI",
    "pdf_link": "https://openreview.net/pdf?id=YWtT5OGOAI",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a fast post-hoc uncertainty quantification method for both NeRF and Gaussian Splatting Models. The method is based on test-time drop out of neurons/Gaussians in the model and estimating per-pixel uncertainty based on the error caused in the test views. The proposed method is orders of magnitude faster than other baselines and has competitive performance to them."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Explores how drop-out as a main UQ approach in ML, fits into the radiance field framework.\n- It is applicable to both NeRF-based and 3DGS-based models.\n- The proposed method is very fast, and is done post-hoc which makes it a useful method for downstream applications."
            },
            "weaknesses": {
                "value": "Main:\n- The rendering function in test time (the pre-trained radiance field) is not perfect. I do not see how comparing the rederings from the drop-out version of the model to the renderings of the model can specify uncertainty. Moreover, how are the test-views selected? Is there an assumption on the distribution of the cameras? If you select your camera far enough from the training ddistribution, the dropout rate would drop to zero , as the error from that view would be high anyway?\n- The  volume rendering function itself (through its integration) hides the ambiguity and uncertainty in depth, so using the rendered image error as a source for identifying uncertainty can be less robust than a spatial method, unless queried densly from many test views.\n- The authors claim this method is not architecture-dependant, a discussion about how drop-out in MLP-based NeRFs vs voxel-based or K-Plane ones results in the same metric would be useful. My confusion comes from the fact that some of these representations have more geometric meaning (like voxel or K-Planes or 3DGS) and in these cases dropping out a cell would directly affect a point in space while dropping out a node/layer from MLP might affect different places simultaneously.\n- Qualitative results on uncertainty primarily reflect color uncertainty, as seen in flat regions like the train body in figure 4, where low-opacity, cloudy Gaussians exhibit low uncertainty. However, the paper should clarify whether the reported uncertainty correlates more with color error, depth error, or both, and include qualitative evidence to support this claim. This helps readers understand what use cases this uncertainty is useful for.\nMinor:\n- What are the PSNRs reported in Table 1? Are they PSNR after noise removal the same way explored in the baseline, if so the coverage % should also be reported alongside this metric.\n- A few results on NGP-style NeRFs which are the main NeRF models used, and most results on MLP NeRFs.\n- The proof sketches in the main paper can be more detailed."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces PH-DROPOUT, an efficient method for epistemic uncertainty quantification in view synthesis, applicable to pre-trained NeRF and GS models. It leverages model redundancy to estimate uncertainty without retraining, showing strong performance across datasets. Despite its efficiency, it faces challenges with hash encoding and sparse scenarios."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The introduction of PH-DROPOUT as a post hoc epistemic uncertainty quantification (UQ) method is novel and addresses a critical gap in view synthesis research.\n* The authors present a strong theoretical foundation, including proofs (e.g., Theorem 3.1) that justify the redundancy in NeRF and GS models, enabling effective dropout-based UQ.\n* PH-DROPOUT achieves real-time performance, outperforming prior methods in computational efficiency.\n* The method shows strong correlations between UQ metrics and prediction errors (e.g., RMSE), validating its reliability."
            },
            "weaknesses": {
                "value": "* The method struggles with hash encoding-based NeRF models, which restricts its applicability to sparse or complex scenes where hash collisions are common.\n* The results on 2DGS show limitations in few-view scenarios. \n* The method is specifically designed for view synthesis tasks and may not generalize to other domains."
            },
            "questions": {
                "value": "* Can the authors provide a workaround or mitigation strategy for handling hash collisions in NeRF models? This would address a key limitation of the method.\n* The inclusion of more real-world datasets, particularly in unbounded scenarios, could strengthen the empirical evaluation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a simple yet effective and extremely fast ad-hoc method for epistemic uncertainty estimation that operates directly on pre-trained NeRF and GS models in the task of novel view synthesis. At its core, the method proposes to use dropout at test time with the maximal drop rate that maintains an epsilon fit to the training views. The method is validated through proxy metrics like Spearman\u2019s correlation of the uncertainty estimates with the RMSE from the GT, and the trend as a function of the number of training views. In addition, the authors demonstrate improved ensembling based on optimal ensemble member selection using their uncertainty estimates."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The authors considered multiple quantitative proxy metrics to benchmark their method, showcasing extensive evaluations.  \n* The method is very simple to implement and adopt, yet it is extremely effective and efficient in terms of runtime.  \n* Overall the paper is structured well and is easy to follow.  \n* Related work is acknowledged, and the contribution of this paper is put in proper context."
            },
            "weaknesses": {
                "value": "* Perhaps the major weakness of this paper is the relatively incremental contribution compared to Ledda et al 2023, which practically proposed the same technique up to the differences mentioned by the authors in L169-175. While I sincerely appreciate the honest citing of this related work by the authors, I find the contribution of PH-DROPOUT to be relatively small.    \n* The English writing of the paper can be significantly improved in certain spots (see small list below), although for the most part it is not hard to understand the authors intention based on the context.  \n* The proofs in the paper need to be more rigorous. For example, L192-207 are very hard to follow. Similarly, L235-243 seem to have mathematical inaccuracies such as calculating the KL divergence between two parameter instances instead of between two distributions. This overall proof sketch is not very clear and requires significant editing. The same applies to Theorem 4.2 and its proof sketch. More rigorous mathematical definitions and notations are needed. In the full proof from the appendix L777-782 seem to have a mistake in the variance formula. Where did $N\\_F$ come from?\n\nMy rating is mainly due to the relatively limited contribution compared to Ledda et al. and the lack of mathematical rigor in the proofs. Nonetheless, I\u2019m willing to increase my score if the authors' rebuttal can alleviate my concerns with respect to these two issues, as I still think this work does have the potential for practical value in quantifying the uncertainty of novel view synthesis. \n\nA few caught (minor) typos/english corrections:\n\n* L103 \\- hence hard \\-\\> hence **it is** hard  \n* L104 \\- find \\-\\> found  \n* L128 \\- in **ad** network \\-\\> in **a** network  \n* L129 \\- inject \\-\\> inject**ing**  \n* L130 \\- **non-**trivial \\-\\> **not** trivial  \n* L149 \\- step 3 in the algorithm is better split into 2 lines/formatted differently  \n* L162 \\- need \\-\\> need**ed**  \n* L184 \\- a common features \\-\\> common features/a common feature  \n* L185 \\- method \\-\\> method**s**  \n* L198 \\- signal \\-\\> **the** signal  \n* L201 \\- what does a \u201cfunction with nearly discrete pattern\u201d mean?  \n* L208 of \\-\\> of **a**  \n* L225 \\- spa**c**ial \\-\\> spa**t**ial  \n* L229 \\- same structur**al** \\-\\> same **structure**  \n* L229 \\- number of parameter \\-\\> number of parameter**s**  \n* L229 \\- have similar distribution of parameter \\-\\> have **a** similar distribution of parameter**s**  \n* L235 \\- continuou**sity** \\-\\> continuity  \n* \u2026."
            },
            "questions": {
                "value": "* What is the number of dropout samples N in your experiments?  \n* Did you check the calibration of your uncertainty estimates (e.g. using metrics like ECE)?   \n* Did you check the trend of the RMSE from the GT compared to thresholding out pixels with increasing uncertainty levels? Is this expected to behave similarly to a NeRF model predicting both the mean and the standard deviation of the RGB value along each ray?  \n* In L262-264 you mention that your uncertainty estimate is biased. Is this bias not important? How does this affect your uncertainty calibration?  \n* When you write down $\\\\rho\\_p$ in the results tables you are referring to $\\\\rho\\_{PE}$?  \n* What does the acronym \u201cAUSE\u201d refer to in Table 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a Post Hoc Epistemic UQ scheme, referred as PH-DROPOUT, to make uncertainty estimation directly on pre-trained Multi view Scene Reconstruction models (NeRF and Gaussian Splatting in particular). The algorithm estimates the variance of a well-trained model by introducing binary dropout mask into model parameters and greedily select drop out ratio to perturb the output within tolerance, and validate over test views. Authors explore the usage of the measurement from their PH-DROPOUT algorithm by conducting thorough analysis over different scenarios (active learning, correlation analysis, uncertainty driven model ensembles)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Overall it is a good discovery and reflection on how to effectively and efficiently justify and analyze the performance of NeRF or GS model quantitatively. Authors justify their measurements is sound by extensive experiments and they indicate that most NeRF or GS models tend to have more redundant parameters as training views increase. This is a bold yet very inspiring claim. It implicitly conveys an intuitive thought: the information attained from more input views shall reduce the dimension of the model itself.  \n\nMostly the paper context is well written and well organized, and the paper itself answers most of my concerns while I was reading it."
            },
            "weaknesses": {
                "value": "The math claim in this paper can be improved. There are too many \"colloquial\" proof rather than a mathematical modeling of phenomena. These claims seems to be redundant with respect to the integrity of the paper. For instance, to represent the significant redundancy in model parameters, authors state the following in line 190:\n \n$$ \\exists 0\\ll r<1\\rightarrow \\forall x\\in \\mathcal{D}_{train}, \\lvert F(x;\\theta)- F(x;D(\\theta,r))\\rvert<\\epsilon. $$\n\nFirstly, greater than $\\ll$ is not a rigorous notation. Secondly, $\\mathcal{D}_{train}$ is not defined. Is it a point set or a multi-view image set? Lastly, the proof of line 190, i.e. line 192-210, is all text and there is no anchor point to refer to appendix for a comprehensive mathematical proof, and I failed to find the corresponding complete proof of theorem 3.1 in appendix. One cannot call statement in line 192-210 a proof of theorem. My suggestion is to make it as a conjecture or rewrite this theorem. Same question for lemma 4.1, thm 4.2, thm 4.3 and thm 4.4."
            },
            "questions": {
                "value": "There are some clarification questions I wish to hear from authors:\n\n*    In line 141, the model function $F(x;\\theta)$ is introduced. Is $x$ a 3D point? This confuses me when I checked line 190.\n*    In Algorithm 1 (line 143-159), does $M\\cdot \\theta$ refer the element-wise product? Where does subscript $i,j$ come from?\n*    In line 173-175, it claims dropout only happens in one of the middle layes in NeRF-based models? Any ablation study over this choice of selective layer dropout?\n*    To show performance in active learning, the paper evaluates the performance starting from sparse view training (as low as 2 views). Nevertheless, PH-Dropout has an assumption that the pre-trained model needs to be well trained or over-fitted. I understand that some NeRF-based alrogithm can achieve sparse-view reconstruction. Is there any quantitative assessment on judging whether or not PH-Dropout is applicable in these sparse-view training results?\n*    What do AUSE RMSE, AUSE MSE and AUSE MAE, mean in Table 1 (line 443-450). Besides, layout of Table 1 and Figure 6 is bad as title and floatings are not properly aligned."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose using post-hoc dropout as a tool for epistemic uncertainty quantification in novel view synthesis frameworks like NeRF and Gaussian Splatting. Starting from a model with trained parameters, they uniformly increase the dropout rate of every parameter as long as the training loss is unharmed. Sampling from the distribution defined by this dropout rate can be used to compute spatially-varying variances in each novel view, which is used as a proxy for uncertainty, which is validated experimentally."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "I congratulate the authors for their submission. The algorithm is simple, elegant and efficient. Previous attempts at using Dropout in novel view synthesis have failed: the authors have correctly identified the promise in the recent theoretically developed dropout injection methods and ported it to the Computer Vision community and the novel view synthesis problem in particular. Regardless of how exactly it compares against prior work, it surely advances the state of the art in uncertainty quantification for Vision and will inspire future work by the community (it has inspired *me* just by reading it!)."
            },
            "weaknesses": {
                "value": "The main weakness of the current manuscript is the quality of the technical exposition. Given that what is proposed is a fairly simple extension of the Ledda et al. 2023 work, this poor quality of technical exposition is not enough to worry me about the soundness of the methods, but nonetheless the authors should strive to improve it in a revision, and this (together with the evaluation concerns below) is the reason why my score is not higher (I will gladly increase it if these questions are addressed). For example:\n\n- The pseudocode in Page 3 has several mistakes. Line 4 should be before Line 1, and Lines 1-5 should be indented. The line starting with \u201cEnsure\u201d is redundant.\n- The wording around \\sigma_max  (L164-168) is confusing. See questions below\n- The theorems and proofs in section 3.2 and 4 are incredibly vague, to the extent that I would be more comfortable if they were described as \u201cintuitions\u201d rather than \u201cproofs\u201d or \u201csketch of proofs\u201d. A proof sketch is an abbreviated version of a proof for which one can be confident a motivated student reader can fill out the details. This is very much not that, as evidenced by my questions below. I would propose the authors rewrite these sections, abandon the pretence of mathematical correctness, explain their intuition and refer the reader to experiments for validation.\n- After reading Lemma 4.1 and its proof 10 times, I still do not understand what it means. See questions below.\n\nI would have also appreciated some visual examples of the uncertainty predicted in novel views by this method and its competitors that go beyond the numerical evaluations. By looking at numbers on a table, it is hard to understand what makes this method more precise than others. It claims to be \u201cmodeling all epistemic sources of uncertainty\u201d, but this could be said of, e.g., the Goli et al. work as well, which is philosophically very similar (\u201chow much can one modify network weights without harming the loss?\u201d). In which image regions is this method doing a better work? Is this method better in floater removal? The text claims \u201cBayes Rays fails to correlate depth uncertainty with high prediction error on the LF dataset\u201d, which seems to contradict the results in Goli et al.\u2019s work. I would appreciate a deeper dive into this. This method is theoretically interesting and fast enough that I do not think its acceptance hinges on the evaluation being flattering, but I would expect a scientific paper to be more transparent in this regard and include a more exhaustive evaluation of where its results stand in relation to previous work, as well as its flaws and the flaws of previous works.\n\nThe manuscript would also benefit from some English proofreading (e.g., \u201ccontinuosity\u201d should be \u201ccontinuity\u201d), even if this has no bearing on my recommendation."
            },
            "questions": {
                "value": "I would love if the authors answered these questions in their revision:\n\n- Why does the dropout mask in this method not need scaling the amplitude of the rest of the weights? I cannot think of a theoretical reason why one would do this, am I missing something? If not, is this due to an empirically observed advantage? I would be very curious to see this explored further, since it appears to go against the common wisdom of Dropout methods.\n- About sigma_max (L164-168). Is the average being done over the training set only, or also the novel test view(s)? If the first one, isn\u2019t the whole point of the algorithm that the training rendered RGB should not change after dropout injection, in which case sigma_max should be zero (or epsilon)? If the second one, how is something \u201cquantifying the uncertainty of a model\u201d if it depends on the specific view?\n- What happens if the network is not perfectly overfit, as happens in almost every case? If the training is ended before the loss goes to zero, the network does not have sufficient resolution power, or it does but it is stuck in a local minima? The theoretical intuition seems to very deeply rely on this fact for the existence of redundancies: how much do the experiments suffer?\n- The argument in Theorem 3.1 hinges on the conjecture that redundancy in the function space (e.g., one could modify the low power/high frequency Fourier components of this network without affecting the output much) translates into redundancy in the network parameters (i.e., one could turn network weights on and off without affecting the output much). I cannot immediately say that this conjecture is true (though it may very well be). Do the authors have any broader intuition or justification for this?\n- On Lemma 4.1 and its proof: What is this D_KL in the space of weights? What does \u201cp(a)\u201d and \u201cp(b)\u201d mean? Are these conditioned on anything? I similarly do not understand the proof. Is all this lemma is doing saying that \u201cif two weight sets are close, their Bayesian likelihoods are similar\u201d? If so, this seems to me at least contestable. An argument about continuity may be made if the weights are changed only slightly, but if the observed redundancy is 20-30%, how can one state to be in a similar space in the Bayesian weight distribution?\n- See \u201cweakness\u201d for questions on evaluation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}