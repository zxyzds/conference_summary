{
    "id": "kFoJXqiGKz",
    "title": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind",
    "abstract": "We propose Decrypto, a novel interactive benchmark for evaluating coordination, competition, and theory of mind (ToM) reasoning capabilities in agentic, foundational AI models. Existing benchmarks often suffer from data leakage, saturation, and lack of interactivity, making it hard to measure the ability of intelligent systems to model other agents' reasoning. To overcome these limitations, we introduce Decrypto, a multi-agent benchmark based on a popular, language-based board game and designed to be future-proof for large language models (LLMs). We validate Decrypto's effectiveness through comprehensive empirical evaluations of frontier LLMs, ablation studies, and human-AI cross-play experiments. We show that LLMs do not coordinate well with other LLMs or humans and perform strictly worse than the latter. Specifically, LLMs struggle to reason about the choices of others, even if they use the same underlying model, pointing to a fundamental limitation of current systems.",
    "keywords": [
        "theory of mind",
        "multi-agent reasoning",
        "LLM benchmark",
        "zero-shot coordination"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We introduce Decrypto, a language-based benchmark for LLMs to assess multi-agent reasoning and theory of mind.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kFoJXqiGKz",
    "pdf_link": "https://openreview.net/pdf?id=kFoJXqiGKz",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces Decrypto, an interactive benchmark for evaluating multi-agent reasoning and theory of mind capabilities in foundational AI models. It involves a language-based board game to test coordination, competition, and strategic reasoning. Results show that large language models (LLMs) struggle to coordinate with both humans and other LLMs, indicating significant limitations in theory of mind capabilities compared to humans."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper introduces a novel benchmark that specifically targets theory of mind capabilities, providing a unique approach to multi-agent reasoning evaluation.\n\n2. It presents a benchmark designed to assess the performance of LLMs in both cooperative and competitive multi-agent environments, which is significant for advancing the field. The benchmark isolates language-based reasoning, offering a focused evaluation of LLMs without requiring complex symbolic or spatial reasoning, enhancing clarity.\n\n3. The proposed benchmark allows for adaptability in difficulty levels, addressing saturation issues in other benchmarks and ensuring scalability for future models."
            },
            "weaknesses": {
                "value": "1. The method lacks rigor in controlling for potential confounding variables, which casts doubt on the validity of the observed differences in model performance across experimental settings.\n\n2. The experimental design and descriptions lack precision, particularly in detailing how varying LLM architectures impact theory of mind capabilities, leaving key assumptions and decisions unaddressed.\n\n3. Evaluation metrics appear inadequate, as they rely heavily on average turn length without addressing the potential noise introduced by differing prompt conditions, which may skew the results.\n\n4. The ablation studies are limited in scope and fail to explore the interaction between vocabulary size and hint similarity thoroughly, leaving significant gaps in understanding model robustness. And the comparative analysis with baseline models is poorly justified, as it assumes without evidence that the chosen baselines are representative of the broader capabilities of specialist versus generalist agents."
            },
            "questions": {
                "value": "1. In the methodology section, could you clarify how you account for confounding variables when comparing the theory of mind performance across different LLM architectures? \n\n2. Your evaluation relies significantly on average turn length as a performance metric. Could you elaborate on the rationale behind choosing this metric and explain how you mitigate potential noise introduced by prompt variations? \n\n3. The scope of the ablation studies appears limited, particularly regarding the interaction between vocabulary size and hint similarity. Could you provide more detailed experiments that explore how these factors interact and influence model robustness? \n\n4. The selection of baseline models to represent \u201cspecialist\u201d versus \u201cgeneralist\u201d agents seems insufficiently justified. How did you determine that these baselines are representative of broader capabilities within these categories?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes to use a game Decrypto, to evaluate coordination, competition, and theory of mind (ToM) reasoning capabilities in agentic, foundational AI models. They show experiments on LLMs, and human-AI cross-play."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Decrypto introduces a unique  framework for studying language understanding and communication. Unlike traditional benchmarks that rely on static inputs, Decrypto involves dynamic clue-based exchanges where the meaning evolves over time. This setup offers a more naturalistic challenge for AI models, better reflecting the way humans adapt language in interactive and often ambiguous contexts."
            },
            "weaknesses": {
                "value": "Looking at the design of the task and its inspiration from Piagets three mountains task , I think this paper falls short as a theory of mind benchmark. The original Piagets three mountains task, primarily tests spatial perspective taking. Likewise in Decrypto, the task only requires players to think about shifting between \"clues\" or coded language without attributing beliefs or knowledge to others. Since Eve and Alice know each others codes at the end of the round, it could become more of a pattern matching game rather than ToM. Can you please clarify this. The claim is central to the paper.\n\nAlso, Decrypto does not seem to require players to reason about or infer others' beliefs\u2014particularly beliefs that could be mistaken or based on incomplete information. I mean, in Decrypto, players decode and interpret clues, but if the gameplay primarily revolves around interpreting words or phrases without attributing or understanding others' mental states (e.g., \u201cthey think that we think X\u201d), it does not measure ToM in the way Gopnik and Astington define. Without this, Decrypto would align more closely with tasks focused on communication or strategy rather than Theory of Mind. \n\n\nReferences :\n\nFlavell, J. H. (1992). Perspectives on perspective-taking\nGopniks Children's Understanding of Representational Change and Its Relation to the Understanding of False Belief and the Appearance-Reality Distinction\nWellman, H. M., Cross, D., & Watson, J. (2001). Meta-analysis of theory-of-mind development: The truth about false belief"
            },
            "questions": {
                "value": "Comments \nThe paper is a bit hard to read, please try to improve writing\nTable 1 is not referred anywhere in the paper!\nCouldn't find ablation even though it is referred in the paper\nAbstract ends abruptly!\nL83: Decrypto isolates language-based reasoning and association, directly leveraging LLMs\u2019 core training objective. Where is this explained?\nline 191 values of GPT-4o in the left column are doesn't end, incomplete  sentence\nThere is no baseline where Humans are intercepted by Humans to know how difficult or easy is this task. I think this is important for the claim about the task evaluating ToM."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose the Decrypto benchmark, a novel interactive evaluation framework for testing multi-agent coordination, competition, and ToM capabilities in AI systems.  Decrypto a popular (and very fun!) boardgame requires agents to reason about other agents' knowledge and decision-making while engaging in both cooperative and competitive gameplay. The benchmark is designed to be future-proof for large language models (LLMs) while avoiding common pitfalls of existing benchmarks like data leakage and lack of interactivity."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- I think it's fantastic and increasingly fruitful when studies leverage approaches from cognitive science to study AI, and in particular when they use boardgames to test (human and) AI capabilities because it provides a controlled environment, but one where you can arbitrarily scale complexity as needed. Decrypto in particular is a great game for testing cooperation, ToM, adherence to RSA, etc. (and is also a great game in general, though no bonus review points for that :-) ). Kelsey Allen et al. beautifully lay out the case for these kinds of studies in their NHB paper \"Using games to understand the mind\"\n- The authors make a compelling case for using games as benchmarks, particularly for evaluating theory of mind capabilities, by drawing clear connections to foundational cognitive science work like the Three Mountain Problem\n- The benchmark design thoughtfully eliminates common LLM failure modes (like numerical computation and tokenization issues) to focus specifically on reasoning and coordination abilities\n- The empirical evaluation is comprehensive, including ablation studies, human-AI cross-play experiments, and detailed analysis of model performance across different roles\n-The authors demonstrate that even state-of-the-art LLMs struggle with this benchmark despite its focus on language-based reasoning, revealing important limitations in current systems"
            },
            "weaknesses": {
                "value": "- The theoretical analysis of the benchmark's properties could be strengthened, particularly regarding what specific aspects of theory of mind and coordination are being tested. In particular, this may benefit from leaning on the Rational Speech Act literature\n- The authors could expand on how the benchmark's difficulty scales with agent capabilities; while they mention it cannot be saturated, more formal analysis would be valuable\n - The human baseline data collection (9 games) seems limited given the importance of human comparisons in the results section. Crucially, I also don't see important details about the human data collection in the manuscript (what were the demographics of the human players, what was their prior experience with Decrypto, were they compensated for their participation, was the study cleared by IRB, what was the interface that humans saw, etc.)\n- The authors could provide more detailed analysis of failure modes; what specific types of reasoning or coordination break down when LLMs perform poorly?\n - A discussion of potential gaming or adversarial strategies would strengthen the benchmark's robustness claims\n\n*Note: I think the paper is great and would be very happy to re-evaluate my score if the authors address my concerns.*"
            },
            "questions": {
                "value": "- How do the authors ensure that the keyword corpus provides consistent difficulty across different games?\n- Were any metrics considered beyond win rates and game length for evaluating performance?\n- Could the authors elaborate on how the benchmark tests different levels of ToM reasoning?\n- How sensitive is agent performance to the specific choice of prompts and system messages?\n- See weaknesses above for some additional questions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not sure if ethics flag is needed or not, but I don't see important details about the human subjects in the manuscript including whether the study received IRB approval, how participants were chosen, how they were compensated, etc."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}