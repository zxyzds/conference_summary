{
    "id": "skHPtDnYGa",
    "title": "Understanding the Role of LLMs in Multimodal Evaluation Benchmarks",
    "abstract": "The rapid advancement of Multimodal Large Language Models (MLLMs) has been accompanied by the development of various benchmarks to evaluate their capabilities.  However, the true nature of these evaluations and the extent to which they assess multimodal reasoning versus merely leveraging the underlying Large Language Model (LLM) backbone remain unclear. This paper presents a comprehensive investigation into the role of LLM backbones in MLLM evaluation, focusing on two critical aspects: the degree to which current benchmarks truly assess multimodal reasoning and the influence of LLM prior knowledge on performance. Specifically, we introduce a modified evaluation protocol to disentangle the contributions of the LLM backbone from multimodal integration, and an automatic knowledge identification technique for diagnosing whether LLMs equip the necessary knowledge for corresponding multimodal questions. Our study encompasses four diverse MLLM benchmarks and eight state-of-the-art MLLMs. Key findings reveal that some benchmarks allow high performance even without visual inputs and up to 50\\% of error rates can be attributed to insufficient world knowledge in the LLM backbone, indicating a heavy reliance on language capabilities. To address knowledge deficiencies, we propose a knowledge augmentation pipeline that achieves significant performance gains, with improvements of up to 60\\% on certain datasets, resulting in a approximately 4x increase in performance. Our work provides crucial insights into the role of the LLM backbone in MLLMs, and highlights the need for more nuanced benchmarking approaches.",
    "keywords": [
        "Multi-modal Language Models",
        "benchmark purification"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=skHPtDnYGa",
    "pdf_link": "https://openreview.net/pdf?id=skHPtDnYGa",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes an examination of issues within multi-modal evaluation from two aspects:\n\n\t1.\tTo what extent answers rely solely on textual information\n\t2.\tHow much external knowledge can boost the performance of multimodal questions\n\nThe paper finds that in some current benchmarks, up to 50% of questions can be solved using only textual queries. The authors introduce a retrieval-augmented generation (RAG) method to improve the language model\u2019s performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The idea is intuitive and easy to understand."
            },
            "weaknesses": {
                "value": "* The issue that multimodal questions can often be solved with textual queries alone is not new; many prior works either study or are motivated by this observation (e.g., [1,2]). The paper lacks a more extensive discussion of related works that have studied this phenomenon.\n* Equation 3 is confusing. From an intuitive standpoint, it doesn\u2019t seem to hold because: 1) even without the correct knowledge, the model may still provide the correct answer, and 2) in many cases, explicit knowledge is not required (e.g., inferring relative distances between two objects in an image), so such decomposition may not apply.\n* It's expected that additional information retrieved by RAG should boost the performance. It would be beneficial to discuss when RAG provides the most advantage. For instance, what types of VQA questions benefit most from RAG, and under what conditions does it perform best?\n* It would be nice to provide some results on the OK-VQA dataset as well, as it also requires external knowledge which should benefit from rag. \n\nReferences:\n[1] Fu, X., Hu, Y., Li, B., Feng, Y., Wang, H., Lin, X., \u2026 & Krishna, R. (2024). Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390.\n\n[2] Zhang, J., Mishra, A., Patwardhan, S., & Agarwal, S. (2022). Can open-domain question answering systems answer visual knowledge questions? arXiv preprint arXiv:2202.04306.\n\n[3] Schwenk, D., Khandelwal, A., Clark, C., Marino, K., & Mottaghi, R. (2022, October). A-okvqa: A benchmark for visual question answering using world knowledge. In European Conference on Computer Vision (pp. 146-162). Cham: Springer Nature Switzerland."
            },
            "questions": {
                "value": "1. In Section 3.3, Table 2, how do you conclude that the performance cap is due to insufficient knowledge? Could it not also be attributed to limited reasoning capabilities, among other factors?\n2. Additionally, the writing and notation used in the paper are unclear and need improvement, e.g.\n* The connection between some proposed concepts is unclear. There is a missing link between the sections \u201cIS VISUAL CAPABILITY NECESSARY?\u201d and \u201cDO MLLMS HAVE SUFFICIENT PRIOR KNOWLEDGE.\u201d\n* The indicator function $\\mathbb{1}\\left[ P(I, Q) \\sim K \\right]$ lacks a clear definition of its true conditions. The notation is poorly defined, as  $P(I, Q) \\sim K$  is usually interpreted as drawing  K  from the joint distribution of  I  and  Q."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores the impact of LLM backbones on the evaluation of MLLMs, concentrating on two primary aspects: the extent to which existing benchmarks accurately measure multimodal reasoning, and the effect of LLMs' prior knowledge on performance. The paper introduces several metrics to disentangle visual perception capabilities from language prior knowledge. The study is conducted on four public MLLM benchmarks and eight open-source MLLM models. Furthermore, the authors introduce a RAG pipeline to enhance the performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ Rethinking the impact of the LLM backbone in MLLM evaluation is insightful and may advance the development of MLLM.\n+ The paper is generally easy to read and follow.\n+ The introduced metrics and RAG pipeline are sound."
            },
            "weaknesses": {
                "value": "- According to lines 52-55, the reviewer understands that the author aims to focus on two aspects: the influence of language knowledge on the benchmark and the MLLM models, respectively. However, the metrics introduced are all dependent on specific MLLM models, making it impossible to objectively assess the language effects on the benchmark itself. The reviewer suggests that it would be more effective to use ground-truth (human annotation) to address the authors' first question: to what extent do current benchmarks truly assess multimodal reasoning versus relying solely on language capabilities?\n\n- While it is important to disentangle the language prior from MLLM evaluation, the task has already been considered in the construction of existing MLLM benchmarks, for example, SEED-Bench [A] (Sec. 3.3, \"... For verifying question/answer pairs, we filter out questions that can be answered correctly by multiple LLMs without resorting to visual information ...\"). However, such works have not been discussed in the paper.\n[A] SEED-Bench: Benchmarking Multimodal Large Language Models. CVPR 2024.\n\n- The reviewer acknowledges the empirical contributions of this work. However, it offers limited new knowledge to the community. Specifically, the impact of language prior on MLLM evaluation has already been addressed in existing benchmarks (see Weaknesses #2). Additionally, the introduced metrics rely on specific MLLM models without incorporating new ground-truth annotations (see Weaknesses #1), and the RAG pipeline presented is an established technique. The quality of the paper would be significantly enhanced if the authors could reconstruct existing popular benchmarks to disentangle them from language prior and place greater emphasis on multimodal reasoning."
            },
            "questions": {
                "value": "Please see the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the role of LLM backbones for MLLM in the perspective of which current benchmarks truly evaluate multimodal reasoning and the effect of LLM prior knowledge.  To achieve this, this paper introduces a modified evaluation protocol to elucidate the contribution of the LLM's backbone and an automatic knowledge identification technique to diagnose whether the LLM has the necessary knowledge for the question. This paper reveals that some benchmarks allow high performance without image inputs and many error rates can be attributed to insufficient LLM capabilities. Furthermore, this paper proposes a RAG pipeline for knowledge-based VQA tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- S1. The analysis of the contribution of the Language side in MLLM is of great community interest and is one of the key topics\n\n\n- S2. Incorporating metrics such as SR, GR, SuRD, and NeRD enables more detailed analysis. It will be an important analytical tool."
            },
            "weaknesses": {
                "value": "- W1. The author's investigation into image-agnostic questions within MLLM benchmarks confirms findings from previous research [1]. Although this analysis provides a more in-depth view, the results that some benchmarks allow high performance without image inputs are not unexpected. Similarly, it is widely recognized that an MLLM\u2019s performance depends on LLM knowledge capacity and generally improves with LLM\u2019s size. Therefore, the finding that \u201cmany error rates can be attributed to insufficient LLM capabilities\u201d is not surprising.\n\n\n- W2. It would be beneficial to propose how to construct the image-dependent questions. For example, this paper could create a benchmark from existing datasets that requires the use of visual information to answer the questions correctly. It would be more beneficial to the community if such a benchmark could be created along with Analysis.\n\n- W3. The evaluation dataset is not new. I would like to know how the proposed metric works on a more recent dataset such as MMStar [1].  MMStar consists of image-dependent questions. Therefore if new issues are identified using the analysis tools in this study, new findings can be given in terms of how to create image-dependent problems. \n\n\n- W4. The solution for the LLM's knowledge lacking is also just using RAGs, so there is limited novelty.\n\n\n[1] Chen+, Are We on the Right Way for Evaluating Large Vision-Language Models?, NeurIPS2024"
            },
            "questions": {
                "value": "The role of LLM in MLLM is a very important topic, and this paper provides an analytical tool on that topic. On the other hand, as noted in Weakness, what this paper examines is merely a more comprehensive examination of what is already known, and the findings are not surprising.\n\nI would like to know why existing evaluations are not enough and what are the novel findings through this analysis. And, also, I would like to know what would the results be if the author used newer benchmarks for evaluation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Not ethics concern"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies the contribution of language models in multi-modal evaluation benchmarks. The authors aim to quantify the importance of language in the performance on such datasets in two ways: (i) quantify shortcut reasoning (i.e., how much these models look at the image/how many of these questions do not require image understanding), and (ii) quantify the role of prior knowledge in answering these questions (i.e., understand whether more knowledgeable backbones provides better performance). The authors select four benchmark datasets and eight models to investigate these two aspects. As for prior works, they confirm that language models tend to exploit shortcuts, i.e., do not always use visual information to answer. Moreover, they discovered that the performance on the selected benchmarks heavily depends on prior knowledge of the language model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The work is well-presented and motivated, and the writing is clear.\n- The experiments cover four multi-modal datasets and eight multimodal language models.\n- The authors introduced some changes to previous evaluation protocols (i.e., options shuffling, open-ended conversion) to minimize biases.\n- The two main discoveries present valuable insights into how to design new benchmarks and what to focus on when training multi-modal language models."
            },
            "weaknesses": {
                "value": "- The analysis of visual capability necessity was already studied in the literature (as also noted by the authors), leaving only the second discovery (i.e., more language knowledge implies better multimodal performance) as the main discovery of the work. Moreover, one could argue that this second point is somewhat intuitive and is more of a confirmation of it.\n- The work acknowledges using only a subset of the MMMU benchmark due to limitations in the models' support for multi-image input and the complexity of converting certain questions into corresponding knowledge reasoning tasks.\n- Experiments regarding different retrieval approaches are missing, e.g., using different retrieval approaches and potentially exploiting multi-modal information."
            },
            "questions": {
                "value": "- As I am not fully convinced of the novelty of this work, I would like to ask the authors how their discoveries regarding the necessity of visual information differ from prior works.\n- Moreover, I would like to ask the authors whether one could infer the second discovery by evaluating the performance of different pre-trained language models with vision capabilities at different scales (e.g., LLAMA 7B, 30B, etc) without relying on retrieval.\n- I expect that adding vision to a language model after pretraining shouldn't diminish its existing knowledge. This means we could evaluate the sufficiency of prior language knowledge for multimodal tasks as if they were language-only tasks. I ask the authors to discuss this and include tables linking knowledge sufficiency results from multimodal benchmarks to language model performance on language tasks, i.e., for models adding vision post-pretraining, report their performance on language benchmarks, and check for correlations between them and multimodal knowledge sufficiency."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}