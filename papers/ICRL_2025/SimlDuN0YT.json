{
    "id": "SimlDuN0YT",
    "title": "Logical Consistency of Large Language Models in Fact-Checking",
    "abstract": "In recent years, large language model (LLM) has demonstrated significant success in performing varied natural language tasks such as language translation, question-answering, summarising, fact-checking, etc. Despite LLM\u2019s impressive ability to generate human-like texts, LLMs are infamous for their inconsistent responses \u2013 a meaning-preserving change in the input query results in an inconsistent response and attributes to vulnerabilities of LLMs such as hallucination, jail breaking, etc. Consequently, existing research focuses on simple paraphrasing-based consistency assessment of LLMs, and ignores complex queries that necessitates an even better understanding of logical reasoning by an LLM. Our work therefore addresses the logical inconsistency of LLMs under complex logical queries with primitive logical operators, e.g., negation, conjunction, and disjunction. As a test bed, we consider retrieval-augmented LLMs on a fact-checking task involving propositional logic queries from real-world knowledge graphs (KG). Our contributions are three-fold. Benchmark: We introduce three logical fact-checking datasets over KGs for community development towards logically consistent LLMs. Assessment: We propose consistency measures of LLMs on propositional logic queries as input and demonstrate that existing LLMs lack in logical consistency, specially on complex queries. Improvement: We employ supervised fine-tuning to improve the logical consistency of LLMs on the complex\nfact-checking task with KG contexts.",
    "keywords": [
        "LLM",
        "Logical consistency",
        "Fact-checking in Knowledge graphs"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "Assessing and improving the logical consistency of large language models on fact-checking queries in the knowledge graph",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=SimlDuN0YT",
    "pdf_link": "https://openreview.net/pdf?id=SimlDuN0YT",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a method for bench marking LLMs for logical consistency and ways to improve the logical consistency of LLMs through a supervised fine tuning using the fine tuning data. Paper shows how LLMs perform on different logical consistency checks with triples given a context to answer some logical queries and show the effectiveness of improving them with SFT. They propose three benchmark datasets based on three different knowledge bases and show how SFT improves across benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Overall benchmarks for checking logical consistency for a given triple context and logical query are good and can help in evaluating logical consistency of LLMs.\nEmpirical results showing zero shot/prompt and fine tuning results show the improvements across benchmarks."
            },
            "weaknesses": {
                "value": "Experiments are more on the benchmark created and not sure how well they translate to real world situations or places where queries are in textual nature and does he supervised model still be able to generalize to textual data as opposed to triple context?"
            },
            "questions": {
                "value": "After SFT, the models still perform as good on the original language benchmarks as the original model?\nWas the goal of the peper to produce a model that does logical consistency in KG RAG kind of setting or in general to improve the logical consistency of LLMs ? if its later I don't see any experiments around other evaluation to show that."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper discusses the topic of the logical consistency of LLM when answering complex queries that can be expressed by propositional logic and knowledge extracted from the knowledge graph. The logical consistency is defined by whether the model can answer the logical equivalent query with the same answer. To this end, the author first showcases some examples of non-consistency in query answering of LLM and then uses finetuning instead of prompting to improve the logical consistency of LLM."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe definition of logical consistency is strict and the paper constructs three new datasets for finetuning and evaluating the consistency of LLM.\n2.\tThe construction of the LLMQUERY seems interesting and the author proposes many optimizations towards that.\n3.\tThe experiments show that the logical consistency can be improved by finetuning but not prompting."
            },
            "weaknesses": {
                "value": "1.\tThough being logical consistent is a desirable attribute of LLM, we are also interested in whether it retrieve the correct answer. The logical consistency is only computed by whether the model gives the consistent answer with itself, but whether the answers are true or false are neglected in the experiment.\n2.\tThe writing of this paper can be further improved. For example, the Proposition 3 is trivial and kind of redundant. There are also some mistakes like in Line 346 ``more computationally cheaper\u2019\u2019."
            },
            "questions": {
                "value": "Similar with weakness, I wonder whether the logical consistency helps with the reasoning ability of LLM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tackles the significant issue of logical consistency in responses generated by large language models (LLMs). It emphasizes a gap in current research, where consistency assessments focus primarily on simple paraphrasing rather than complex logical reasoning. The authors address this by examining how LLMs handle propositional logic (involving operators such as negation, conjunction, and disjunction) within a fact-checking framework supported by knowledge graphs (KGs)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "There are three main contributions of this paper.\n1) Novel Dataset and Benchmarking: Three logical fact-checking datasets FreebaseLFC, NELLLFC, and WikiLFC, derived from knowledge graphs.\n2) Evaluating LLMs on these new fact-checking datasets. The paper includes a variety of experimental setups, including comparisons of zero-shot instruction prompting and supervised fine-tuning, adding depth to the evaluation of LLMs' logical consistency. Results show that existing LLMs are not consistent when considering\nlogical equivalents or variants of these facts.\n3) Improving the consistency of LLMs via supervised fine-tuning"
            },
            "weaknesses": {
                "value": "1)  While the paper presents methods for extracting relevant KG context through BFS and embedding-based retrieval, it is unclear how effective these methods are in dynamically varying real-world contexts. Expanding this section could strengthen the applicability of the approach.\n2) The paper suggests fine-tuning as a solution to improve logical consistency but does not deeply discuss the associated computational overheads and limitations, which may impact the scalability of this approach for large datasets or diverse domains.\n3) The paper mainly focuses on binary responses, which may limit the generalizability of the method in complex domains. Exploring extensions to multi-class logic and non-binary answer consistency could open avenues for broader applications. Note that a fact might not be binary, e.g., Crimean is a part of Russian. The answer is nether yes or no, but depends on who claims it."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on the logical consistency of LLMs in fact-checking using KGs. The authors first formalize the definition of logical consistency for LLMs and concentrate on retrieval-augmented LLMs within a fact-checking task. They propose three benchmarks, FreebaseLFC, NELLLFC, and WikiLFC to demonstrate that several existing LLMs lack logical consistency when handling complex queries, and employ supervised fine-tuning to enhance the logical consistency of these models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to follow.\n\n2. The logical consistency evaluation of LLMs is rational and interesting.\n\n3. The employed supervised fine-tuning method on LLMs has enhanced the logical consistency performance."
            },
            "weaknesses": {
                "value": "1. The authors may want to provide more results across various scales of LLMs (including both smaller models such as Vicuna-68M and larger models like LLaMA2-70B) to comprehensively evaluate the logical consistency issue.\n\n2. The authors may want to include more results from existing fact-checking methods using KGs, such as MiniCheck [1] or RefChecker [2], on the proposed benchmarks.\n\n[1] Minicheck: Efficient fact-checking of llms on grounding documents. EMNLP, 2024.\n\n\n[2] Refchecker: Reference-based fine-grained hallucination checker and benchmark for large language models. 2024.\n\n3. The third contribution, \"Improvement of Logical Consistency,\" appears somewhat underwhelming. In Table 1, Vanilla LLaMA-2-13B already achieved strong results before fine-tuning, which leads me to wonder whether models like GPT-4o, GPT-o1 exhibit lower levels of logical inconsistency which may be hardly noticeable.\n\n4. The authors may want to discuss and summarize more LLMs with KG contexts methods in related work including [3] to provide a more comprehensive insight.\n\n[3] Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models. ICML2024."
            },
            "questions": {
                "value": "1. Have the authors considered more complex logical expressions or some higher-order Horn rules?\n\n2. The tables in the paper are not formatted as three-line tables, which appears somewhat unusual."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}