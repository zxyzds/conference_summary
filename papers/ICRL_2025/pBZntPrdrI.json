{
    "id": "pBZntPrdrI",
    "title": "Rethinking Fair Representation Learning for Performance-Sensitive Tasks",
    "abstract": "We investigate the prominent class of fair representation learning methods for bias mitigation. Using causal reasoning to define and formalise different sources of dataset bias, we reveal important implicit assumptions inherent to these methods. We prove fundamental limitations on fair representation learning when evaluation data is drawn from the same distribution as training data and run experiments across a range of medical modalities to examine the performance of fair representation learning under distribution shifts. Our results explain apparent contradictions in the existing literature and reveal how rarely considered causal and statistical aspects of the underlying data affect the validity of fair representation learning. We raise doubts about current evaluation practices and the applicability of fair representation learning methods in performance-sensitive settings. We argue that fine-grained analysis of dataset biases should play a key role in the field moving forward.",
    "keywords": [
        "Fairness",
        "Causality",
        "Representation Learning",
        "Distribution Shift"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We provide theoretical proofs and empirical evidence to reveal fundamental limitations of fair representation learning methods.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=pBZntPrdrI",
    "pdf_link": "https://openreview.net/pdf?id=pBZntPrdrI",
    "comments": [
        {
            "summary": {
                "value": "This paper tackles an important challenge in explainable and reliable AI by examining fair representation learning methods focused on achieving group fairness. The authors critically explore the limitations of existing approaches. They present three main fairness paradigm: enforcing group parity, maximizing subgroup IID performance, and generalizing to unbiased distributions. Their key theoretical insight is that achieving group fairness cannot be both effective and harmless (relative to ERM) within the IID setting. Furthermore, the paper suggests that the success of fairness learning methods depends heavily on shifts in test data distribution, hypothesizing that performance is also influenced by the underlying causal structure and the amount of sensitive information available in the training dataset."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper is self-contained and easy to follow, and the notation is clear. It is a joyful experience to read a manuscript like this. The theory is simple yet insightful. The experiment supports the theoretical as well."
            },
            "weaknesses": {
                "value": "1. One major concern is the lack of results in the distributed shift setting. While the negative result in the IID is sound, it would be good to have some results for out-of-distribution, or connects the out-of-distribution fairness to some existed theoretical analysis results in the area of  distribuiton shift.\n\n2. Another concern is that the trade-off between fairness and accuracy has been explored by previous methods (Zhao et al., 2022). Why and how this paper is different than previous methods on analyzing the trade-off between fairness and accuracy, \n\nI would like to raise my score if these concerns are addressed.\n\n[1] Zhao, H., & Gordon, G. J. (2022). Inherent tradeoffs in learning fair representations. Journal of Machine Learning Research, 23(57), 1-26."
            },
            "questions": {
                "value": "1. This paper mainly talks about group fairness, what about counterfactual fairness? Is the counterfactual fairness also impossible in the iid setting?\n\n2. This work shows that distribution shift is closely related to group fairness, and there are some results in the area of domain generalization / adaptation. Can you elaborate some results in that area? Is there any method closely related to the group fairness?\n\n3. How does different types of distribution shift (class imbalance, covariate shift, concept shift ...) in the dataset related to the three types of disparities?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper makes the argument that fair representation learning (FRL) is not a useful approach in the class of cases where model accuracy (or analogous metrics) is key. They provide an overview of several paradigms of fair ML, and highlighting which are \u201cperformance-sensitive\u201d. They then apply a causal framework to discuss the situations in which FRL might be helpful, using this to claim that FRL is not helpful to achieve performance-sensitive objectives when test data is IID. They show experiments to support this, in particular highlighting that even when test data is IID, the causal structure of the problem matters."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- Note: I think I may have reviewed a previous version of this paper at another conference (also double blind, no anonymity issues). I have not gone back to look at my past comments so this review is with fresh eyes, and I believe that a number of improvements have been made to this draft. However, apologies if there are overlapping comments anywhere\n- Really enjoyed the framing in Sec 2 around group parity, IID performance, and generalization to unbiased OOD, I think this sets up the paper quite well and gives a nice overview of the relevant parts of the fairness literature\n- does a nice job of synthetizing causal reasoning with the FRL literature. In particular, the experiments around FRL effectiveness and causal structure are quite interesting\n- in general I think the main point made here is correct and correctly-scoped, and useful for a specific segment of the community interested in these things"
            },
            "weaknesses": {
                "value": "- I\u2019m a little confused by the X_Z and X_A framework, what about features that are impacted by both Z and A? It\u2019s not clear if this framework allows for that overlap\n- It would be good at some point to discuss whether any of this applies in an anti-causal setup - the \u201cfutility\u201d framing is quite strong and yet I feel as though the large anti-causal class of problems are not really touched on here\n- Quibble with Def 4.5 - technically, I would phrase this as \u201chave an equal amount of information\u201d rather than \u201cdo not discard relevant information\u201d. The question of whether information is discarded I think gets more into implementation and I\u2019m not sure how \u201cdiscarding information\u201d is defined technically.\n- Quibble with Def 4.4/Lemma 4.6 - I find something a little odd here which I think is around the corner case where ERM representations have no sensitive information. I think the important part of Def 4.4 (the \u201ceffectiveness\u201d) of FRL is the equality to 0 - this is whether they are truly \u201cfair\u201d in this definition. However, Lemma 4.6 relies heavily on the first part of the def\u2019n (having strictly less information) - if the ERM reprs have 0 sensitive information then trivially all FRL will violate effectiveness, but this seems wrong as that is not a property of the FRL method, but rather a property of the ERM representations which are given. Anyways, this is not a massive issue with the general point but I do think there needs to be some more careful wording with how the intuition is formalized here (e.g. maybe the I(ERM, A)=0 case needs to be separated out as \u201ctrivial\u201d)\n- The results in Fig 2 on subgroup separability go in the opposite direction of what I would expect - the authors note this and give some thoughts on why this might be. However, it suggests something troubling - that at separability=0.5 (i.e. no information at all about A, trivial case), the fair representation methods will yield low-accuracy representations, when in fact this should be the easiest case (just return the representations as given). This suggests to me that the implemented methods may not be working as intended. It would be good to have a supplementary experiment showing that the methods, are in fact, working as intended - otherwise the empirical results are a little harder to parse due to the chance of experimental failure."
            },
            "questions": {
                "value": "- The characterization in Fig 1 is interesting but a couple contextual questions - it\u2019s not clear to me as a reader if this is a) a complete characterization of the types of causal structures that can produce this phenomenon, or b) if this is a novel contribution on the part of this paper, or something pre-existing in the literature"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper defines causal structures representing realistic scenarios of dataset bias and discuss how the bias mechanisms may affect the performance and fairness of predictive models. The authors then prove fundamental limitations on fair representation learning when evaluation data is drawn from the same distribution as training data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Originality and novelty: the authors propose a novel causal framework to categorize the bias in the data.\n\n2. Supportive theoretical results: the authors prove the limitations of the FRL in IID settings.\n\n3. Clear structure: the writing and organization is clear, making the paper easy to follow."
            },
            "weaknesses": {
                "value": "1. The causality-based notion in fair ML literature has been well discussed, and there have been numerous causal structures proposed. Can the authors provide a comparative analysis with one or two to further prove the novelty?\n\n2. Limited experiments: it seems the experiments only focus on medical imaging scenarios. The results would be more convincing if datasets from other performance-sensitive areas are considered."
            },
            "questions": {
                "value": "1. The same as weakness 1, is there more comparison between the causal structures proposed by the authors and other literature?\n\n2. Are there more experiments from another dataset in performance-sensitive areas other than just medical imaging?\n\n3. Typo: in the contribution part (page 2), shouldn't the orderings begin with 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper summarizes the field of fair representation learning into three main categories and then illustrates from a causal perspective that the underlying problem that people have been trying to solve in fair representation learning is in fact somewhat ill-defined. The paper argues that if the test and training distributions are the same, fair representation learning is \"futile\". In addition, the paper also argues that if there is a distribution shift at test time then there is hope as well as when the features are somewhat separable.\nThe paper establishes theoretical foundations and well as experimentally illustrates the theoretical findings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper puts into perspective the whole fair representation learning field, using causal language.\n- The paper critically establishes a key problem in the field and how researchers benchmark their own methods.\n- The paper also shows experimentally that their theoretical findings have practical consequences which is very much appreciated. In particular, they show that if there is indeed a test=train time distribution, fair representation learning does not help much. and vice versa."
            },
            "weaknesses": {
                "value": "- The paper requires somewhat of a graphical model/ causal background which might be less beginner-friendly\n- The paper is very dense and requires multiple passes to fully understand the paper. I would recommend adding further intuitions to the paper.\n- Lastly, i might have missed this. Do the authors have any comments on the relations between FRL to standard fairness learning schemes that are not representation-based? I would be curious to hear of these problems persist necessarily."
            },
            "questions": {
                "value": "see above for the questions"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}