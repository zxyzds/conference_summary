{
    "id": "8w8d8j2FCy",
    "title": "MENTOR: Mixture-of-Experts Network with Task-Oriented Perturbation for Visual Reinforcement Learning",
    "abstract": "Visual deep reinforcement learning (RL) enables robots to acquire skills from visual input for unstructured tasks. However, current algorithms suffer from low sample efficiency, limiting their practical applicability. In this work, we present MENTOR, a method that improves both the architecture and optimization of RL agents. Specifically, MENTOR replaces the standard multi-layer perceptron (MLP) with a mixture-of-experts (MoE) backbone, enhancing the agent's ability to handle complex tasks by leveraging modular expert learning to avoid gradient conflicts. Furthermore, MENTOR introduces a task-oriented perturbation mechanism, which heuristically samples perturbation candidates containing task-relevant information, leading to more targeted and effective optimization. MENTOR outperforms state-of-the-art methods across three simulation domains---DeepMind Control Suite, Meta-World, and Adroit. Additionally, MENTOR achieves an average of 83% success rate on three challenging real-world robotic manipulation tasks including peg insertion, cable routing, and tabletop golf, which significantly surpasses the success rate of 32% from the current strongest model-free visual RL algorithm. These results underscore the importance of sample efficiency in advancing visual RL for real-world robotics. Experimental videos are available at https://mentor-vrl.github.io/.",
    "keywords": [
        "Visual Reinforcement Learning",
        "Robotics",
        "Mixture-of-Experts"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "MENTOR is a highly efficient visual RL algorithm that excels in both simulation and real-world complex robotic learning tasks.",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=8w8d8j2FCy",
    "pdf_link": "https://openreview.net/pdf?id=8w8d8j2FCy",
    "comments": [
        {
            "summary": {
                "value": "The authors introduce MENTOR, a visual deep RL algorithm designed to improve sample efficiency in robotic tasks. MENTOR enhances RL agents by replacing traditional MLPs with a MoE architecture. Additionally, the authors introduce a task-oriented perturbation mechanism that heuristically samples task-relevant perturbations. Their experiments show MENTOR can get good performance over the diverse tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-structured and easy to understand, with a clear presentation of the proposed method.\n2. The authors conduct extensive experiments in both simulated and real environments, effectively demonstrating the method\u2019s efficacy."
            },
            "weaknesses": {
                "value": "1. The proposed MoE architecture is not evaluated over multi-task environments, especially ones that need different strategies for the different tasks in the environments.\n2. The benefit of the MoE and the task-oriented exploration strategies are coupled. The authors need to decouple this two components and show the effectiveness of the MoE.\n3. The authors need to compare with other techniques that can handle the multi-modality like transformers, diffusion-based policy."
            },
            "questions": {
                "value": "The authors need to address my concerns in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents MENTOR, an innovative approach to enhance sample efficiency in visual deep reinforcement learning (RL) for robotics. By replacing the standard multi-layer perceptron with a mixture-of-experts (MoE) architecture and introducing a task-oriented perturbation mechanism, MENTOR improves the agent's performance in complex tasks and facilitates more effective optimization. The method demonstrates superior results across three simulation domains and achieves an impressive 83% success rate on challenging real-world robotic tasks, significantly outperforming the current best model-free visual RL algorithm, which only achieves 32%."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. MENTOR introduces a mixture-of-experts (MoE) architecture that enhances learning efficiency by dynamically allocating gradients to modular experts, effectively mitigating gradient conflicts in complex scenarios.\n2. The evaluation extends beyond simulations to real-world robotic manipulation tasks, demonstrating MENTOR\u2019s practical value and sample efficiency, which are crucial for advancing reinforcement learning applications in robotics."
            },
            "weaknesses": {
                "value": "While MENTOR demonstrates impressive performance in both simulation and real-world tasks, the paper could benefit from a more detailed analysis of the limitations of the proposed approach, particularly in terms of scalability and generalization across diverse robotic platforms and environments. This would provide a clearer understanding of the framework's applicability in broader contexts."
            },
            "questions": {
                "value": "1. Are the experimental results in the real-world obtained through sim2real transfer of models trained in simulation, or are they trained from scratch entirely in a real environment?  \n2. Why are external disturbance experiments not conducted in the simulation environment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a sample-efficient visual reinforcement learning approach called MENTOR, which utilizes a mixture-of-experts network instead of the traditional MLP network to mitigate gradient conflicts, along with a task-oriented perturbation method to enhance exploration. Evaluation results in multiple simulation environments show that MENTOR is sample efficient. Further, MENTOR can be successfully used for real-world reinforcement learning, which facilitates the application of reinforcement learning to real-world scenarios."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) Attempts to alleviate the burden of shared parameters by introducing MoE architectures into reinforcement learning\n\n2) A simple and effective perturbation method is proposed that can better guide the policy learning\n\n3) The proposed method achieves an improvement in sample efficiency compared to DrM\n\n4) Validates the effectiveness of the method on real-world robotics tasks, providing a valuable reference for the community"
            },
            "weaknesses": {
                "value": "1) Lack of persuasion and ablation in the use of MoE. MoE has been widely used in the field of multi-task learning, and it can effectively alleviate the conflict problem due to multi-objective optimization. However, policy optimization in a single robot manipulation task often has only one optimization objective, which does not fit the context of multi-task learning. Although it is claimed in the paper that the architecture advantage can be propagated to a single task to alleviate the burden of shared parameters, there is no further analysis and ablation experiments on this.\n\n2) Lack of correlation between the two main improvements. MENTOR makes improvements in both architecture and optimization, yet there seems to be no necessary connection between the two. This makes the improvements in the paper appear as if they are just a combination of two tricks. In fact, optimization is often related to architecture, and it remains uncertain whether the use of MoE will introduce new challenges for policy optimization.\n\n3) Lack of ablation of the two improvements. The paper only provides performance curves for MENTOR in simulation tasks, lacking ablation studies on architecture and optimization, which makes the reasons for the final performance improvement unclear. Although incremental comparisons are made in real-robot experiments, comparisons in simulation tasks will be more convincing and fairer."
            },
            "questions": {
                "value": "1) Whether it can be shown that the multi-stage property of the task in single-task learning leads to the gradient conflict problem or the existence of a shared parameter burden in policy optimization?\n\n2) Is MoE more prone to dormancy than MLP or can it mitigate dormancy to some extent?\n\n3) In Figure 6, why MENTOR performs worse on hammer than on hammer (sparse)?\n\n4) Does the performance improvement in Fig. 6 arise mainly from the task-oriented perturbations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenge of reinforcement learning with visual observations, where learning an efficient policy from high-dimensional image data is difficult. The authors propose a novel approach by incorporating a mixture-of-experts (MoE) architecture in the policy and applying task-oriented perturbation to optimize learning efficiency. The method, called MENTOR, is tested on several reinforcement learning benchmarks, including DeepMind Control Suite, Meta-World, and Adroit, as well as real-world experiments. MENTOR demonstrates superior performance compared to prior state-of-the-art (SOTA) methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The paper is clearly written and easy to follow.  \n* The proposed approach\u2014integrating a mixture-of-experts in the policy architecture and applying task-oriented perturbation\u2014is well-motivated and empirically supported, as demonstrated in Figures 3 and 4\\.  \n* MENTOR shows significant empirical improvements over baseline methods in both simulated environments and real-world experiments."
            },
            "weaknesses": {
                "value": "* The paper lacks a discussion of its limitations and possible future directions for addressing them.  \n* Several clarifications could improve the writing and presentation of the work.  \n* A more detailed analysis of hyperparameter sensitivity would be beneficial. It would be helpful to understand how MENTOR's performance is affected by hyperparameters such as the number of experts, the number of top-k experts, the perturbation rate, and the size of the set S\\_{top}\u200b."
            },
            "questions": {
                "value": "1. **Ablation study**: If the method only used the MoE component and random perturbation (similar to DrM), what would the performance be? It would be valuable to analyze whether the mixture-of-experts or task-oriented perturbation contributes more to the success of MENTOR.  \n2. **Task-oriented perturbation and self-imitation learning**: The task-oriented perturbation shares similar intuition with self-imitation learning (https://arxiv.org/abs/1806.05635), where agents benefit from their own past high-rewarding network weight or trajectories. Citing relevant work on self-imitation learning would strengthen the paper. Additionally, a discussion comparing the advantages and disadvantages of task-oriented perturbation versus self-imitation learning would enhance the contribution.  \n3. **Expert output architecture (Line 199\\)**: The paper mentions that expert i produces output a\\_i\u200b, but it is unclear how this output is derived from the latent vector z. Could you provide more details about the architecture of the feedforward network FFN\\_i\u200b and its role in generating the expert output?  \n4. **Clarification on MW (Line 215\\)**: The paper refers to the \"Assembly task from MW,\" but MW is not defined in the text. Does MW refer to Meta-World? A clear definition would improve readability."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}