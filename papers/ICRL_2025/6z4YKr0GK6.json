{
    "id": "6z4YKr0GK6",
    "title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
    "abstract": "The advancements of language language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about the true capabilities of such agents. In this work, we argue that for an agent to fully automate scientific discovery, it must be able to complete all essential tasks in the workflow. Thus, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using our benchmark, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. These results underscore the limited capacities of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research. In the long run, ScienceAgentBench will serve as a benchmark for rigorously measuring progress toward developing language agents to assist human scientists in data-driven scientific discovery.",
    "keywords": [
        "Benchmark",
        "Evaluation",
        "Large Language Model",
        "Language Agent",
        "AI for Science",
        "Code Generation",
        "Task Automation"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We present ScienceAgentBench, a new benchmark for rigorously measuring progress towards developing language agents to assist human scientists in data-driven scientific discovery.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=6z4YKr0GK6",
    "pdf_link": "https://openreview.net/pdf?id=6z4YKr0GK6",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces ScienceAgentBench, a new benchmark designed to test how well language agents can handle tasks in data-driven scientific discovery. The authors collected 102 tasks from 44 peer-reviewed papers across four scientific fields: Bioinformatics, Computational Chemistry, Geographical Information Science, and Psychology & Cognitive Neuroscience. Each task asks the agents to write self-contained Python programs to perform specific scientific activities like data processing, model development, analysis, and visualization.\n\nTo ensure the tasks are authentic and to prevent issues with data contamination, the authors involved experts from the respective fields and modified datasets so agents couldn't rely on memorized code. They evaluated five large language models using three different frameworks: direct prompting, OpenHands, and self-debug. The results showed that the best-performing agent could complete only about one-third of the tasks. This highlights the current limitations of language agents in fully automating data-driven scientific discovery and suggests that more advancements are needed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper introduces ScienceAgentBench, a novel benchmark for evaluating language agents in data-driven scientific discovery tasks. By incorporating tasks from four diverse scientific disciplines\u2014Bioinformatics, Computational Chemistry, Geographical Information Science, and Psychology & Cognitive Neuroscience\u2014it creatively applies language agents to new domains, filling a gap where existing benchmarks fall short.\n\nThe benchmark is rigorously developed with input from nine subject matter experts, ensuring tasks are authentic and challenging. The authors proactively mitigate data contamination by modifying datasets, enhancing the reliability of their evaluation. They use comprehensive evaluation metrics\u2014including Valid Execution Rate (VER), Success Rate (SR), CodeBERTScore (CBS), and computational costs\u2014to provide a holistic assessment of agent performance.\n\nThe paper is well-organized and written, utilizing figures and tables to enhance understanding. The authors provide insightful analyses of experimental results, highlighting why current language agents struggle with these tasks. By releasing all code and data, they promote open science and collaboration, significantly contributing to the advancement of AI in scientific research."
            },
            "weaknesses": {
                "value": "1. The paper evaluates agents using three frameworks but doesn't justify these choices or explore advanced architectures like ReAct or Toolformer. Without including state-of-the-art frameworks that offer advanced reasoning and tool-use capabilities, the study may not fully assess the agents' potential to handle complex scientific tasks. Incorporating such frameworks could provide deeper insights into their capabilities and limitations.\n2. Human evaluators who also participated in data collection may introduce bias due to familiarity with the tasks, affecting the objectivity of the assessments. Additionally, the error analysis lacks depth, as specific failure modes are not thoroughly examined. Involving independent evaluators and conducting a detailed error analysis would improve objectivity and help identify areas where agents struggle.\n3. The paper doesn't compare the agents' performance with traditional methods or domain-specific tools, making it difficult to assess their practical utility relative to existing solutions. Including such comparisons would provide valuable context to evaluate the agents' real-world usefulness and guide future improvements.\n4. Providing expert domain knowledge doesn't consistently improve agent performance and sometimes even decreases it, suggesting agents struggle to integrate this information effectively. Exploring why agents fail to benefit from expert knowledge could lead to better integration strategies and enhance their overall performance."
            },
            "questions": {
                "value": "1. Have you considered evaluating state-of-the-art frameworks like ReAct or Toolformer incorporating advanced reasoning and tool-use capabilities? Including these could offer deeper insights into the agents' performance on complex tasks.\n2. Since evaluators were also involved in data collection, how did you mitigate potential assessment bias? Would involving independent evaluators improve objectivity?\n3. Could you provide a more detailed analysis of the standard failure modes encountered by the agents? Understanding specific errors might help identify areas for improvement.\n4. Have you compared the agents' performance with traditional methods or domain-specific tools? Including such comparisons could help assess their practical utility relative to existing solutions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The ScienceAgentBench framework is introduced in this paper to assess the data-driven scientific discovery capabilities of LLM models. The framework offers both end-to-end and fine-grained metrics in evaluations. Significant room for improvement in scientific tasks was confirmed by implementing the benchmark on various sota models. The benchmark has the potential to serve as a long-term progress indicator for LLM models on scientific reasoning capabilities."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The study involved extensive data curation and human annotation, demonstrating the authors' dedication and thoroughness. The inclusion of both end-to-end and fine-grained metrics allows for a comprehensive evaluation of models, particularly when the models can only partially solve a problem. Additionally, the exploration and discussion of various interaction methods with the local environment provides valuable insights."
            },
            "weaknesses": {
                "value": "Coding generation-related tasks may not be representative of some other scientific domains. While recent research has focused on such tasks, the authors could briefly acknowledge this limitations, especially since the benchmark's name suggests a more comprehensive evaluation of broader scientific capabilities."
            },
            "questions": {
                "value": "Why was VER chosen over CBS when ranking models? High VER but low CBS could still indicate good context understanding, though poor execution. Was it considered to use heuristics / weighted sum to combine all metrics in the final evaluation?\n\nWill setting CBS to 1.0 when SR is 1 introduce bias into the metric? Some argue that this specific treatment can skew the metric's results. While CBS may not be ideal when the model employs a different approach than annotation but still arrives at the correct answer, setting it to 1.0 could lead to inconsistent score interpretations. Additionally, if the ranking is order-based, this specific treatment might not have a significant impact."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel benchmark, ScienceAgentBench, designed to assess language agents' performance in data-driven scientific exploration. It meticulously curates 102 diverse tasks sourced from 44 peer-reviewed publications spanning four disciplines (Bio, Chem, Information Sci, Psy & Cog Neuroscience), subsequently validated by nine subject matter experts. Employing a variety of evaluation metrics, the study examines the efficacy of generated programs, their execution outcomes, and associated costs. By evaluating five LLMs, including both open-weight and proprietary models, across three frameworks\u2014direct prompting, OpenHands, and self-debug\u2014the findings underscore the current limitations of language agents in generating code for data-driven discovery."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "(1) Writing: The clarity of this paper make it well-written and easy to comprehend.\n\n(2) Benchmark: This paper introduces ScienceAgentBench, a framework tailored for assessing language agents in the realm of data-driven scientific exploration. It emphasizes scientific authenticity through collaboration with subject matter experts, establishes rigorous evaluation criteria, and maintains meticulous control over multi-stage quality assurance.\n\n(3) Experiments: The paper evaluates three open-source models and two API-based models, conducting detailed assessments and in-depth analyses to provide comprehensive insights."
            },
            "weaknesses": {
                "value": "(1) It appears that the emphasis of this paper leans more towards Data Science or data-driven discovery rather than scientific discovery.\n\n(2) Task Annotation in Section 2.2 seems labor-intensive and time-consuming due to the involvement of identifying code, preprocessing data, implementing code, and writing dataset information. Are there any automated annotation or data collection methods available?\n\n(3) How is the ground truth for each task defined and generated? Are there any automated validation methods that could streamline this process instead of relying solely on multiple rounds of manual validation by annotators?\n\n(4) Could you elaborate on how the evaluation criteria outlined in Table 1 were established?\n\n(5) Regarding the validation of generated Python programs during inference and the utilization of CodeBERTScore to assess token-level embeddings, have you considered employing a self-consistency strategy to validate multiple outputs over time?\n\n(6) How is the validity of outputs generated by GPT-4o for the four heterogeneous datasets depicted in Figure 1 verified?\n\n(7) Given the focus on code generation for data science, have you considered evaluating or providing the performance of code generation models like Codellama and DeepSeek-Coder?\n\n(8) There appears to be inconsistency in the citation format, as observed in instances such as line 249 and line 251. Would it be possible to ensure uniformity in citation formatting throughout the paper?"
            },
            "questions": {
                "value": "Please see the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "A significant area of concern regarding this research pertains to the safety implications associated with the proposed language agent. Particularly in fields like bio or chemistry, there is a potential risk that the language agent could inadvertently synthesize toxic or dangerous chemicals. It is recommended that the authors address these safety considerations by conducting a thorough analysis of the potential risks associated with the language agent presented in this study."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}