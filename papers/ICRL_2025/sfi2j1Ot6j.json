{
    "id": "sfi2j1Ot6j",
    "title": "FreeCG: Free the Design Space of Clebsch-Gordan Transform for Machine Learning Force Fields",
    "abstract": "Machine Learning Force Fields (MLFFs) are of great importance for chemistry, physics, materials science, and many other related fields. The Clebsch\u2013Gordan transform (CG transform) effectively encodes many-body interactions and is thus an important building block for many models of MLFFs. However, the permutation-equivariance requirement of MLFFs limits the design space of CG transform, that is, intensive CG transform has to be conducted for each neighboring edge and the operations should be performed in the same manner for all edges. Freeing up the design space can greatly improve the model's expressiveness while simultaneously decreasing computational demands.\nTo reach this goal, we utilize a mathematical proposition, invariance transitivity, to show that implementing the CG transform layer on the permutation-invariant abstract edges allows complete freedom in the design of the layer without compromising the overall permutation equivariance. Developing on this free design space, we further propose group CG transform with sparse path, abstract edges shuffling, and attention enhancer to form a powerful and efficient CG transform layer. Our method, known as **FreeCG**, achieves state-of-the-art (SOTA) results in force prediction for MD17, rMD17, MD22, and is well extended to property prediction in QM9 datasets with several improvements greater than 15% and the maximum beyond 20%. The extensive real-world applications showcase high practicality. FreeCG introduces a novel paradigm for carrying out efficient and expressive CG transform in future geometric network designs. To demonstrate this, the recent SOTA, QuinNet, is also enhanced under our paradigm. Code and checkpoints will be publicly available.",
    "keywords": [
        "Machine Learning Force Fields",
        "Equivariant Graph Neural Network",
        "Clebsch-Gordan Transform"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "We present a novel SoTA MLFFs model with high practicality through freeing the design space of CG transform, together with a new paradigm for guiding the future designs of EGNNs.",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=sfi2j1Ot6j",
    "pdf_link": "https://openreview.net/pdf?id=sfi2j1Ot6j",
    "comments": [
        {
            "comment": {
                "value": "Dear Reviewer,\n\nWe sincerely appreciate your comments on our paper.\n\nBefore the full response to your review, we would like to first address one of your concern, which we believe is very important to be clarified.\n\n>The paper does not adequately address SO(3)-equivariance, a critical concept in equivariant GNNs. Typically, in Clebsch-Gordan (CG) transformations, a radial distance-based kernel function is used to ensure SO(3)-equivariance. However, FreeCG may lack SO(3)-equivariance, as the aggregation of abstract edges is based on a weighted summation over components of each irreps. While this aspect is unclear, explicitly showing how the model achieves SO(3)-equivariance would enhance understanding. If the model does not fully satisfy SO(3)-equivariance, it would be helpful to justify using abstract edges within the CG transformation layer.\n\nWe respectfully suggest that this is a misunderstanding. The weighted sum of spherical irreps under the same $l$ is clearly SO(3)-equivariant (and if you add parity it is O(3)-equivariant, as we did in the paper). Here is the proof.\n\n*Proof. Let $v_i$ be the spherical irreps with order $l$, $g\\in SO(3)$, $w_i$ the weight for $i$-th vector in the weighted sum, and $\\rho_V$ the group homomorphism. We need to prove $\\rho_V(g)\\sum_i w_i v_i = \\sum_i w_i \\rho_{V_i}(g) v_i$. It holds if we can prove the following conditions 1) $\\rho_{V_i}(g) w_i v_i = w_i \\rho_{V_i}(g) v_i$ which is trivially true as $w_i$ is a scalar; 2) $\\rho_{V_i} = \\rho_{V_j}$ for arbitrary $i$ and $j$.*\n\n*To show 2) is golden, we need to show a) $V_i$ is an invariant subspace of SO(3), since the $\\rho_{V_i}$ would be ill-defined if it is not the case, which is also trivially true because that is how spherical spaces are generated; b) $V_i = V_j$ for arbitrary $i$ and $j$. Here, $v_i$ are spherical irreps with same $l$. Spherical space of order $l$ is the same vector space spanned by spherical harmonics $Y^l_m$s. Spherical irreps with order $l$ are the vectors in the same order $l$ spherical space, so $V_i = V_j$ for arbitrary $i$ and $j$. Since all $V_i$s are the same we can simply denote each of them as $V$. Summarize over $i$ for both sides of $\\rho_{V_i}(g) w_i v_i = w_i \\rho_{V_i}(g) v_i$, we get $\\rho_V(g)\\sum_i w_i v_i = \\sum_i w_i \\rho_V(g) v_i$. This concludes our proof.  QED.*\n\nThis proof can be well extended to other spaces if they satisfy the above arguments (eg. Cartesian space).\n\nThis conclusion is taken for granted in most papers [1,2,3] in MLFFs and related areas without the explicit proof we have shown above. However, we are happy to add the proof to the revised paper per your suggestion to further enhance clarity (see Sec. A.5 in the current version). The only possible concern about equivariance/invariance is the permutation invariance as we generate abstract edges, which is introduced in Section A.4 in the appendix.\n\n\nTo empirically verify this property, we provide the following code snippet:\n\n```\nimport e3nn.o3\nfrom e3nn.util.test import equivariance_error\n\ntp = e3nn.o3.FullyConnectedTensorProduct(\"2x0e + 3x1o\", \"2x0e + 3x1o\", \"2x1o\")\n\n\nequivariance_error(\n    tp,\n    args_in=[tp.irreps_in1.randn(1, -1), tp.irreps_in2.randn(1, -1)],\n    irreps_in=[tp.irreps_in1, tp.irreps_in2],\n    irreps_out=[tp.irreps_out]\n)\n```\nThis snippet of code weighted sums the spherical irreps under the same $l$, and obtains the error of equivariance test. You could run this code, and you would see that the error is around $10^{-8}$, where the only source of error is from computational stability (floating-point precision). This level of error is common and acceptable for equivariant neural networks. We have also empirically tested the equivariance error of our model. It is around $1.41\\times 10^{-6}$ for energy prediction, and around $4.92\\times 10^{-6}$ for force prediction. \n\nWe hope this can address your concern about the SO(3)-equivariance. If you have any further questions on this point, please feel free to ask. We are happy to provide clarification.\n\n[1] Batzner, S., Musaelian, A., Sun, L., Geiger, M., Mailoa, J. P., Kornbluth, M., ... & Kozinsky, B. (2022). E (3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Nature communications, 13(1), 2453.\n\n[2] Simeon, G., & De Fabritiis, G. (2024). Tensornet: Cartesian tensor representations for efficient learning of molecular potentials. Advances in Neural Information Processing Systems, 36.\n\n[3] Musaelian, A., Batzner, S., Johansson, A., Sun, L., Owen, C. J., Kornbluth, M., & Kozinsky, B. (2023). Learning local equivariant representations for large-scale atomistic dynamics. Nature Communications, 14(1), 579."
            },
            "title": {
                "value": "Response to Reviewer o4xZ (1/2)"
            }
        },
        {
            "comment": {
                "value": "Dear Reviewer,\n\nThank you for your time in reviewing our paper and providing valuable comments.\n\n> Although the author claimed that this work presented a new paradigm for CG transform that can be combined with other models, such as QuinNet, to achieve better performance, the experimental results did not demonstrate this. Simply analyzing the training curves did not allow readers to determine the impact of adding or removing FreeCG on QuinNet. Furthermore, while other models were trained for 3000 epochs, QuinNet was only trained for 1000 epochs. At this point, there is no clear advantage for QuinNet+FreeCG, and readers cannot ascertain whether extended training would improve QuinNet's performance.\n\nWe appreciate your thoughtful comments. In fact, the results show that QuinNet+FreeCG trained for 1500 epochs already surpasses vanilla QuinNet. We provide the detailed numbers on the benchmarks as follows:\n\n| Model | Metric | Aspirin | Ethanol | Malonaldehyde | Naphthalene | Salicylic acid | Toluene | Uracil |\n|-------|---------|----------|----------|---------------|-------------|----------------|----------|---------|\n| QuinNet | Energy | 0.119 | 0.050 | 0.078 | 0.101 | **0.101** | 0.080 | 0.096 |\n| (from paper) | Force | 0.145 | 0.060 | 0.097 | 0.039 | 0.080 | 0.039 | 0.062 |\n| QuinNet | Energy | 0.132 | 0.052 | 0.076 | 0.109 | 0.106 | 0.081 | 0.099 |\n| (1500 epochs)| Force | 0.152 | 0.065 | 0.113 | 0.043 | 0.080 | 0.039 | 0.061 |\n| QuinNet + FreeCG | Energy | **0.113** | **0.048** | **0.073** | **0.094** | 0.103 | **0.078** | **0.094** |\n| (1500 epochs)| Force | **0.127** | **0.056** | **0.095** | **0.034** | **0.073** | **0.037** | **0.057** |\n\n\nIt is noteworthy that FreeCG demonstrates its benefits to QuinNet by achieving superior performance with only 1/2 of the training time. We have added this comparative table to the revised paper. Thank you again for bringing this point to our attention.\n\n>For MILP, both model execution speed and memory usage are critically important. The proposed method seems to significantly increase the computational complexity of the model. It is crucial to include a comparison of the number of parameters and training speed.\n\nThank you for this valuable suggestion! Here are the results:\n\n| | QuinNet+FreeCG | QuinNet | FreeCG | VisNet |\n|------------------|----------------|---------|---------|---------|\n| Number of parameters (M) | 9.4 | 9.1 | 10.4 | 9.8 |\n| Iterations per second (bs=32) | 4.19 | 4.45 | 3.62 | 3.95 |\n\n\nAs shown above, FreeCG adds only a minor number of parameters compared to its baseline in both cases. Additionally, the training speed is comparable to the baselines. We would also like to point out that the other computational overhead is sufficiently low, as demonstrated in Figure 3, which displays the memory occupation and inference speed. Proposed Sparse Path and Group CG Transform reduce the computational overhead to minimum.\n\nWe have incorporated all these results into our revised paper, which can be found in Tables 9 and 10. We sincerely hope that we have successfully addressed all your concerns. Should you have any further questions, we would be happy to discuss them. Again, thank you for your valuable feedback!"
            },
            "title": {
                "value": "Response to Reviewer h6Dk"
            }
        },
        {
            "summary": {
                "value": "This paper proposes FreeCG, a method that implements the CG transform layer on the permutation-invariant abstract edges, which allows complete freedom in the design of the layer without compromising the overall permutation equivariance. This can greatly improve the model\u2019s expressiveness and decrease the computational demands."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper uses invariance transitivity with permutation-invariant abstract edges to solve the narrowness design space of CG transform.\n- This work further proposes a FreeCG model that contains Group CG transform with sparse path, abstract edges shuffling, and Attention enhancer to improve the representation power and efficiency.\n- The model shows good performance on several small molecule datasets."
            },
            "weaknesses": {
                "value": "- In Table 2, FreeCG is not effective in energy prediction on the rMD17 dataset. Could authors elaborate more on this?\n- In Table 4, other competitive baselines such as equiformerV2 should be included for a comprehensive comparison. \n- The datasets used in this paper are quite small and the results are sometimes not robust to evaluate the model performance, such as QM9. I would like to see the force and energy prediction performance on a large dataset such as OC20, to better understand the effective and efficiency of the FreeCG."
            },
            "questions": {
                "value": "- Please refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an approach to address the computational inefficiency of Clebsch-Gordan (CG) transforms in rotation-translation equivariant graph neural networks (EGNNs). Leveraging the invariance transitivity property, the proposed method, FreeCG, implements the CG transform layer on permutation-invariant abstract edges, enabling a more flexible layer design without compromising permutation equivariance. Additional architectural modifications are introduced to enhance model efficiency, with extensive empirical results demonstrating FreeCG\u2019s performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. FreeCG could reduce the computational overhead of CG transforms and enhance expressivity by incorporating several architectural improvements.\n2. FreeCG achieves competitive or superior performance across multiple benchmarks and demonstrates compatibility with various EGNN architectures."
            },
            "weaknesses": {
                "value": "1. FreeCG\u2019s approach to freeing CG transform space with abstract edges resembles the customized tensor product mechanism used in Allegro (see Eq. 13 in [1]). Allegro applies CG transforms to geometric features of a pair (similar to FreeCG\u2019s node features) and environment embeddings of the pair (similar to FreeCG\u2019s abstract edges from neighbors). While FreeCG focuses on atom-wise message passing with greater interaction between atom features and abstract edges, further clarification on the advantages of FreeCG over Allegro would strengthen the technical contribution.\n2. Introducing sparse paths may come with a trade-off in expressivity. The authors should provide an ablation study to analyze the impact of sparse paths on computational efficiency and performance.\n3. Results in Table 5 are somewhat unclear. If the final FreeCG model uses the best-performing modules, it should have a group number of 32, which conflicts with the number 8 reported in Table 7. Additionally, the ablations indicate that the primary performance improvement is due to increased group numbers, suggesting that the other modules\u2019 contributions may be limited.\n4. Certain aspects of the methodology require additional clarity: a) Some notations are under-defined, e.g., $N$ in line 227, $\\bar{E_i^L}$, and $d\\bar{E_i^{L+1}}$ in line 277. b) The relationship between $d\\bar{E_i^{L+1}}$ and $\\bar{E_i^{L+1}}$ in line 277 is unclear, and more details are needed on how $\\bar{E_i^L}$ is constructed in the first layer.\n\n[1] Musaelian, A., Batzner, S., Johansson, A. et al. Learning local equivariant representations for large-scale atomistic dynamics. Nat Commun 14, 579 (2023)."
            },
            "questions": {
                "value": "1. For abstract edge shuffling, did the authors try using a linear layer to mix different channels of $\\hat{E}_i^L$? Linear mixing could enhance information exchange effectively in this case.\n2. Did the authors consider applying FreeCG to more representative tensor product-based EGNNs? This could further validate the method\u2019s efficacy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel approach to Machine Learning Force Fields (MLFFs) that leverages permutation-invariant hidden features for efficient computation. These permutation-invariant features are aggregated over high-irreducible (high-irrep) edge features with attention weights, forming what is referred to as the \u201cabstract edge\u201d in the paper. The Clebsch-Gordan (CG) transformation is applied within each layer, and these permutation-invariant hidden features serve as the input of the kernel function for the CG transformation layer.\n\nTo improve the efficiency of the CG product, the authors propose a grouped CG transformation, which operates similarly to a group convolution. Additionally, to enhance the utility of the abstract edge, they introduce abstract edge shuffling and an Attention Enhancer mechanism."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper includes extensive experiments in the MLFF domain, providing empirical solid support for the model\u2019s performance.\n\n- By aggregating high-irreducible (high-irrep) edge features with attention-weighted, permutation-invariant hidden features (abstract edges), the model achieves robust and flexible representations."
            },
            "weaknesses": {
                "value": "- The paper\u2019s organization and readability could be improved. In particular, modified margins between paragraphs, figures, titles, and subtitles reduce overall readability.\n\n- The paper does not adequately address  $\\text{SO}(3)$ -equivariance, a critical concept in equivariant GNNs. Typically, in Clebsch-Gordan (CG) transformations, a radial distance-based kernel function is used to ensure  $\\text{SO}(3)$ -equivariance. However, FreeCG may lack  $\\text{SO}(3)$ -equivariance, as the aggregation of abstract edges is based on a weighted summation over components of each irreps. While this aspect is unclear, explicitly showing how the model achieves  $\\text{SO}(3)$ -equivariance would enhance understanding. If the model does not fully satisfy  $\\text{SO}(3)$ -equivariance, it would be helpful to justify using abstract edges within the CG transformation layer.\n\n- The paper should include recent results in Table 2 to demonstrate the model\u2019s state-of-the-art (SOTA) status. Specifically, adding comparisons with Graph ACE [1] and PONITA would strengthen the claim.\n\n[1] Bochkarev, Anton, Yury Lysogorskiy, and Ralf Drautz. \"Graph Atomic Cluster Expansion for Semilocal Interactions beyond Equivariant Message Passing.\" Physical Review X 14.2 (2024): 021036.\n\n[2] Bekkers, Erik J., et al. \"Fast, Expressive $\\mathrm {SE}(n) $ Equivariant Networks through Weight-Sharing in Position-Orientation Space.\" The Twelfth International Conference on Learning Representations.\n\n- In Equation (26), the model employs a summation over all permutations, which may constrain the model complexity to  O(N \\times N!) . To fully discuss the model\u2019s efficiency, scalability with respect to the number of atoms should also be considered.\n\n### Minor Issues (Typos, Formatting, and Readability, etc..)\n- Information about what kinds of loss are used is omitted.\n- On page 5,  $N$  appears undefined and may be a typo; perhaps it should be  $Z$ .\n- In Figure 3, the shaded regions impair readability; using a transparent background or adjusting the shading would improve clarity.\n- Unnecessary line breaks are present in Appendix equations (7) and (15).\n- Table 8 is unnecessarily expanded to fill the entire line width."
            },
            "questions": {
                "value": "- How does the model ensure the SO(3) equivariance?\n- Why was the AIMD-Chig dataset chosen for memory usage and inference speed benchmarks?\n- Why does abstract edge shuffling improve the model? A brief discussion on the role of shuffling is needed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "While this paper addresses machine learning force fields (MLFF), which are highly valuable in chemistry, physics, and materials science, the author does not mention any concerns about the potential misuse of this technology. For instance, there could be risks of malicious applications, such as creating chemical weapons. Although such scenarios are unlikely based on current understanding, I believe all researchers working in AI for scientific applications should remain vigilant about these possibilities, given that AI tools can be used without specialized knowledge."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduced an informative and efficient model utilizing high-order irreps and CG transform. This model achieves state-of-the-art (SOTA) results in force prediction for MD17, rMD17, and MD22. However, while the authors asserted that this work represented a new paradigm for CG transform that can be integrated with other models, such as QuinNet, to enhance performance, the experimental results do not support this claim."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper proposed an informative and efficient model with high-order irreps and CG transform.\n\nThe model achieved state-of-the-art (SOTA) results in force prediction for MD17, rMD17, MD22."
            },
            "weaknesses": {
                "value": "Although the author claimed that this work presented a new paradigm for CG transform that can be combined with other models, such as QuinNet, to achieve better performance, the experimental results did not demonstrate this. Simply analyzing the training curves did not allow readers to determine the impact of adding or removing FreeCG on QuinNet. Furthermore, while other models were trained for 3000 epochs, QuinNet was only trained for 1000 epochs. At this point, there is no clear advantage for QuinNet+FreeCG, and readers cannot ascertain whether extended training would improve QuinNet's performance."
            },
            "questions": {
                "value": "For MILP, both model execution speed and memory usage are critically important. The proposed method seems to significantly increase the computational complexity of the model. It is crucial to include a comparison of the number of parameters and training speed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}