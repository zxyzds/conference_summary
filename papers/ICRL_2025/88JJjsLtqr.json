{
    "id": "88JJjsLtqr",
    "title": "Less is More: Masking Elements in Image Condition Features Avoids Content Leakages in Style Transfer Diffusion Models",
    "abstract": "Given a style-reference image as the additional image condition, text-to-image diffusion models have demonstrated impressive capabilities in generating images that possess the content of text prompts while adopting the visual style of the reference image. However, current state-of-the-art methods often struggle to disentangle content and style from style-reference images, leading to issues such as content leakages. To address this issue, we propose a masking-based method that efficiently decouples content from style without the need of tuning any model parameters. By simply masking specific elements in the style reference's image features, we uncover a critical yet under-explored principle: guiding with appropriately-selected fewer conditions (e.g., dropping several image feature elements) can efficiently avoid unwanted content flowing into the diffusion models, enhancing the style transfer performances of text-to-image diffusion models. In this paper, we validate this finding both theoretically and experimentally. Extensive experiments across various styles demonstrate the effectiveness of our masking-based method and support our theoretical results.",
    "keywords": [
        "Text-to-Image Diffusion Models",
        "Style Transfer",
        "Content Leakage"
    ],
    "primary_area": "generative models",
    "TLDR": "We uncover a critical yet under-explored principle: guiding with appropriately-selected fewer conditions can efficiently avoid unwanted content flowing into the diffusion model,  enhancing the style transfer performances.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=88JJjsLtqr",
    "pdf_link": "https://openreview.net/pdf?id=88JJjsLtqr",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a style transfer method that can preserve style in the style reference image while ignored the content injection for the final style transfer result.  The key idea of the proposed method is based on IP-adpator while masked out the some image tokens from the reference image.  The masking strategy is first clustering the product feature of style image and content image and then filtering out the feature tokens of high means in style features.   To approve the masked strategy is effective, the authors also provided several theoretical justifications."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The key strengths in this paper are the insights of analyzing different style transfer methods including IP-adpator and Instant Style. With those observations, the proposed method provide a masking solution to demonstrate that removing certain tokens especially the token with high correlations between content and style will result a high fidelity stylized images."
            },
            "weaknesses": {
                "value": "There are several weaknesses in this paper:\n\n1. The masked image feature is questionable. Although the proof demonstrate the divergency in a theoretical way, it is clear that it only demonstrate the divergence by comparing with InstantStyle rather than the method itself.   Image token selection is based on the product between content and style feature, such computation is more likely filtering about the foreground style feature.  Thus why not only encode the background  or non-object related style patches? \n\n2. The visual comparison is not in a fair comparison.  Lots of the results are in a cherry pick manner.  For example, StyledDrop and StyleShot focus more in tradition painting like style transfer, while the experiments showing more like photo as a reference image, which not makes much sense.  Moreover, the Figure 8 compare the results with InstantSyle but not StyleShot is also not fair since the setting is more like the traditional style transfer."
            },
            "questions": {
                "value": "Please double check the experiments. For example, I screenshot one style reference image and use the official styleshot demo, I could generate better results shown in the paper.   \n\nAnother fair comparison is leveraging existing benchmark (the images used in styledrop and styleshot) and shown more proposed results on that. \n\nThere are also some questions on the visual results. For example, the Figure 1 shows that the proposed method also could not generate visual plausible results especially preserving the style in style reference image. In Figure 8, it is clear to see that some cases InstantStyle gives better results.  As it preserves the content while generates less artifacts.  Thus more results on the existing benchmark maybe a good justification."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper clearly showing some weaknesses but lack of such discussion,  especially on human related stylization."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper focuses on the content leakage issues in the text-to-image diffusion model for style transfer, which aims to distangle the content and style characteristics of the style-reference images for generating outputs combining text content and visual styles. It proposes a simple and training-free method to decouple the content from style in style-reference images. By masking specific content-related elements within the image features, the proposed method prevents unwanted content information from influencing the output. The proposed method was evaluated on CIFAR-10 dataset and demonstrates good results."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The motivation of this paper is well elaborated, and the limitations of previous methods are clearly described. Therefore, potential readers can easily understand the core problem in style transfer. \n2. The structure of the paper is well-organized and the presentation is easy to follow. \n3. It proposes a masking-based technique to decouple the content and style in the reference images. Fig.3 clearly demonstrates the difference between the proposed method and previous methods."
            },
            "weaknesses": {
                "value": "1. The novelty of the proposed method is limited. 1). Compared with IP-adaptor and InstantStyle, the contribution of sampling masking features in the feature space is not significant. 2). Introducing a masking mechanism is effective for manually synthesizing high-quality images, which has been demonstrated in previous studies [1-3]. \n\n2. The proposed method aims to decouple the content and style characteristics of the reference images, but this problem is not formally formulated in the paper. Therefore, it is hard to understand why the masking mechanism can achieve this goal.\n\n3. The proposed method needs to carefully select specific features for generating desirable styles, but the selection criteria are not clearly described.\n\n4. The paper claims that it proposes an efficient method to decouple the content and style of the reference images in the introduction part, but the paper does not show significant evidence to demonstrate its efficiency compared with the previous methods.\n\n5. The proposed method is only evaluated on CIFAR-10 dataset, and measured with subjective metrics that are not defined clearly. Therefore, existing experiment results are insufficient to demonstrate the proposed method's advantages.\n\n[1] Gao, Shanghua, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. \"Masked diffusion transformer is a strong image synthesizer.\" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 23164-23173. 2023.\n[2] Xiao, Changming, Qi Yang, Feng Zhou, and Changshui Zhang. \"From text to mask: Localizing entities using the attention of text-to-image diffusion models.\" Neurocomputing 610 (2024): 128437.\n[3] Couairon, Guillaume, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. \"Diffedit: Diffusion-based semantic image editing with mask guidance.\" ICLR, 2023."
            },
            "questions": {
                "value": "1. It is better to elaborate on the advantages and significant contributions of the proposed method for style transfer compared to previous approaches?\n\n2. What is the formal definition of content leakages? Additionally, how does the proposed method effectively address this issue?\n\n3. How does the proposed method decouple the content and style characteristics of the reference images?\n\n4. How are features selected during the style transfer process? Please provide detailed information on the selection criteria. Are the same selection criteria, including hyperparameters, applied consistently to all output images?\n\n5. What makes the proposed method efficient? Compared to previous approaches, does it require less inference time or fewer GPU resources?\n6. It would be beneficial to include more experiments on different datasets. Specifically, how does the proposed method perform when processing high-resolution images?\n\n7. Including additional objective evaluation metrics, such as FID, LPIPS, and CLIP score, would be valuable. Since the fidelity score highly depends on the chosen classifier, how does the performance change when a different classifier is used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a simple but effective method to avoid content leakages and achieve better performance in style transfer. The main contribution is its innovative masking strategy, and extensive experiments demonstrate its superiority."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow. \n2. The proposed masking strategy is novel and effective. \n3. The authors provide both theoretical and experimental evidence to support their claims."
            },
            "weaknesses": {
                "value": "1. It remains unclear why clustering is performed on the element-wise product of $e_1$ and $e_2$.  Is there a relationship between $e_1\\cdot e_2$ and the energy function? \n2. The inference speed is slower than other methods, likely due to the additional time consumption introduced by the clustering algorithm. What is your inference time in practice? Is there a solution that avoids this additional time cost, or could the clustering algorithm be replaced to improve efficiency?"
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a feature masking method to control the content leakage for the stylization task. It requires an additional content description of the image and decouples this content by masking it out in the style features. The authors also provided theoretical proofs to demonstrate the motivation of their method. Both qualitative and quantitative results are superior to the alternatives."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed method is intuitive and works well.\n2. Theoretical proofs are valid and well support the experiments.\n3. Generally the writing is good and the story is complete, though there is some information missing as I mentioned in the following section.\n4. Experiment design is comprehensive and the performance looks good."
            },
            "weaknesses": {
                "value": "1. The authors should compare with more recent stronger baselines that are proposed to alleviate the content leakage problem like RB-Modulation, which uses attention feature aggregation and different descriptors to decouple content and style. Since it is also training-free and mentioned to outperform InstantStyle, it would serve as a good baseline for comparison. CSGO is another recent work that uses a separately trained style projection layer to avoid content leakage. Though it\u2019s pretty new (released a month before deadline), some qualitative results would help demonstrate the strengths of your method.\n2. In L229, should m^i be 1 or 0?\n3. Figure 3(b) is not referred to but seems to be mentioned in the experiments. I thought this should be part of the method you want to introduce. Can you explain how this is used with your proposed method? And how does the linear layer learn the content feature to be subtracted?\n4. Theorem 1 indicates that the proposed method archives a smaller divergence. Does \u201csmaller divergence\u201d define better style alignment? I\u2019m asking because there might be several factors that can lead to smaller divergence, like same background/or elements in the images, content leakage, etc. I\u2019d like to get your insights on what is the style in an image?\n5. In L215, it seems cluster number K controls how many tokens are masked. Is there any analysis to show how K affects the performance?\n6. Can you add more details on how you do binary classification as eval in L369?\n7. Are you using style descriptions in the prompt?\n8. Why does image alignment keep dropping in figure 5(a)?\n9. Can you provide more details on how you conduct user study? Like instructions to the raters and how you present the images to the raters.\n\nThe answers can be kept short and concise since there are many questions. Thanks!"
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}