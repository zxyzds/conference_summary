{
    "id": "RqJ0px8osW",
    "title": "A unified lightweight complex scenes-oriented network for infrared and visible image fusion",
    "abstract": "Existing infrared and visible image fusion (IVIF) techniques typically integrate the useful information from different modalities within the ideal conditions. Nevertheless, current state-of-the-art IVIF methods are ineffective when facing complex scene interferences such as bad weather, low light, and high noise, and they typically need to be used in conjunction with other de-interference baselines, which inevitably resulting in the high memory costs and error accumulation, thus yielding sub-optimal fusion results. To address these challenges, We propose a unified lightweight real-time IVIF network for multiple complex scenes. We conducted a theoretically thorough analysis of modal degradations in the frequency domain, leveraging the complementary strengths of both modalities to enhance network learning. Our method facilitates the extraction of critical features even amidst significant pixel interference. For reconstructing fusion results, we introduce a spatial domain branching strategy which significantly improves the local detail resolution, thereby mitigating potential omissions from frequency domain analysis. Extensive qualitative and quantitative experiments demonstrate that our framework excels in handling multiple complex scenes, while maintaining real-time computational efficiency for prompt image processing applications.",
    "keywords": [
        "Infrared and visible image fusion",
        "Complex Scenes",
        "Unified Network",
        "Frequency domain",
        "Real time"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "We propose a unified lightweight network for infrared and visible image fusion designed for real-time processing in complex scenes, such as adverse weather, low-light, overexposure, and noise conditions.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=RqJ0px8osW",
    "pdf_link": "https://openreview.net/pdf?id=RqJ0px8osW",
    "comments": [
        {
            "title": {
                "value": "Response to Reviewer 21oJ (W3-W5)"
            },
            "comment": {
                "value": "**W3** `On the Rationale of Directly Combining Source Image Information with Feature Maps`\n\n***Perspective of Previous Experience:***\n\nThis approach is common in image restoration tasks, and the final output result is obtained by adding the feature map generated by the network to the source image. For example, Restormer [4], PromptIR [5], PromptRestorer [6] and DRSformer [7], among others.\n\n***Rationale for This Approach:***\n\nThe primary reason for this design is to facilitate better network convergence. In image restoration tasks, the network does not need to reconstruct all pixel information in the scene, as the original image typically retains clear, non-degraded regions. The degraded information in such cases is usually localized to specific areas, rather than being distributed across the entire image.\n\nAs a result, this practice allows the network to focus on replacing the degraded pixels and restoring the missing details, while preserving the intact regions of the source image. By combining the source image with the feature maps, we effectively \"patch\" the degraded areas rather than unnecessarily reconstructing the entire image. This simplifies the fitting process for the network, which is especially important for lightweight models like ours.\n\n**W4-W5** `About modifications to Images and Text`\n\nThank you for pointing out these issues. We have carefully reviewed the manuscript and made the necessary corrections:\n\n***(1) Clarification of \u201cDWT\u201d in Figure 5:***\n\nThe label \u201cDWT: Discrete Wavelet Transform\u201d was mistakenly included in the flowchart under \u201cFFT Domain\u201d in Figure 5. Since DWT is not part of our method and is not discussed in the manuscript, we have removed this label to avoid confusion.\n\n***(2) Minor Issues:***\n\nThe typo in Line 018 (\u201c, We.\u201d) has been corrected.\n\nIn Table 2, we have highlighted the value for LRRNet (\u201c0.8050\u201d) in the \u201cRain\u201d column in red.\n\nWe have also conducted a comprehensive review of the manuscript to ensure accuracy and clarity throughout the text. Thank you for your careful review and valuable feedback, which helped us improve the quality of the paper.\n\n---\n[4] Zamir et al. Restormer: Efficient transformer for high-resolution image restoration. CVPR 2022.\n\n[5] Potlapalli et al. PromptIR: Prompting for All-in-One Blind Image Restoration. NIPS 2023.\n\n[6] Wang et al. PromptRestorer: A Prompting Image Restoration Method with Degradation Perception. NIPS 2023.\n\n[7] Chen et al. Learning A Sparse Transformer Network for Effective Image Deraining. CVPR 2023."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer 21oJ (W1-W2)"
            },
            "comment": {
                "value": "*Thank you for your thoughtful comments on  our work.  Below, we provide a detailed point-by-point response to each of your observations and suggestions.*\n\n**W1** `How does the proposed method improve upon or differ from these existing methods`\n\nThank you for your valuable comments. While frequency domain learning has indeed been widely adopted in low-level vision tasks, such as low-light enhancement (e.g., \u201cFourLLIE: Boosting Low-Light Image Enhancement by Fourier Frequency Information\u201d), dehazing [1], and deraining [2], our proposed method significantly differs in its design, application, and focus. Below, we outline these distinctions:\n\n***Design Motivation and Application:***\n\nThe primary difference lies in the application domain and algorithmic focus. The aforementioned methods address single-modality image restoration tasks, such as enhancing low-light images or removing specific degradations like haze or rain. In contrast, our proposed algorithm is designed specifically for the infrared and visible image fusion task in complex scenes. This task not only involves pixel restoration but also requires the effective interaction and integration of multi-modal information, which adds significant complexity compared to single-modal restoration.\n\n***Rationale for Frequency Domain Learning in IVIF Tasks:***\n\nWe thoroughly analyze the rationale for adopting frequency domain components to handle IVIF tasks in complex environments. Leveraging the unique characteristics of infrared and visible light modalities, we propose a novel multi-modal interactive guidance mechanism. This mechanism not only enhances the complementary relationship between modalities but also adapts frequency domain learning specifically to the requirements of IVIF. Our experiments validate the effectiveness of this approach, and we further provide theoretical insights into the applicability of frequency domain learning in IVIF, setting our work apart from other applications that primarily focus on single-modality restoration.\n\n***Algorithm Efficiency and Real-Time Performance:***\n\nMany existing frequency domain-based image restoration algorithms do not consider computational efficiency and are therefore unsuitable for real-time applications. While these methods highlight the ability of frequency domain learning to capture degraded information, they often rely on deeper networks or longer training times to process high-frequency components effectively. In contrast, our proposed algorithm emphasizes both performance and efficiency. Specifically:\n\n(1) Previous study [3] has shown that convolution operations typically learn low-frequency components first, gradually capturing higher frequencies.   However, fully learning high frequencies often requires deeper networks or longer training.   We use FFT to efficiently separate frequency information and accelerate the learning of high-frequency components through the phase.\n\n(2) Our approach not only enhances task performance but also supports a lightweight model design capable of real-time processing. For example, we achieve a processing speed of 30 frames per second on 640\u00d7480 images, making our method practical for real-world applications.\n\n**W2** `(1) About the Application of Spatial Domain Module in  Proposed Method.`\n\nWhile it is true that the Spatial Domain Module utilizes convolution operations, it is important to emphasize that its performance depends not only on the module itself but also on the overall network design.  Unlike traditional image processing algorithms, where feature operators extract all information indiscriminately, deep learning frameworks are driven by loss constraints during training.  In our method, the parameters of the convolution operations are continuously updated, enabling the model to adaptively focus on extracting valuable features while suppressing degradation artifacts.\n\n**W2** `(1) About the Application of Max pooling in  Spatial Domain Module.`\n\nFurthermore, max pooling in the Spatial Domain Module is not a standalone feature extraction mechanism;  instead, it operates as a supplementary component within the broader context of our network.  The Spatial Domain Module itself is designed to compensate for detail loss in the frequency domain, rather than serve as the primary mechanism for image restoration.  While max pooling alone may not fully address the issue of high-intensity degradation artifacts, it provides a fast and efficient means of selecting locally significant features.  This simplicity aligns with our core objective of achieving real-time and lightweight performance. We have incorporated the above into the revised manuscript and provided an explanation.\n\n---\n[1] Yu et al. Frequency and Spatial Dual Guidance for Image Dehazing. ECCV 2022.\n\n[2] Zhou et al. Fourmer: An Efficient Global Modeling Paradigm for Image Restoration. ICML 2024.\n\n[3] Tang et al. Defects of Convolutional Decoder Networks in Frequency Representation. CVPR 2023."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer Pge1"
            },
            "comment": {
                "value": "*Thank you for your insightful comments regarding the theoretical foundation and comparative performance of our approach. Below is our detailed, point-by-point response to each of your observations and suggestions.*\n\n**W1** \n*We analyze the reasons for the superior performance of the proposed algorithm from the following two perspectives:*\n\n`(1) Distinctive Approach to Joint Image Fusion and Restoration Tasks`\n\nIt is important to note the fundamental difference between image fusion and image restoration tasks. ***Image fusion aims to retain only the most salient features from each modality by selectively removing redundant information. On the other hand, image restoration seeks pixel-perfect recovery, often without discarding any features.*** In our framework, we prioritize extracting critical scene information from disturbances and then reconstructing based on these features, rather than attempting full scene recovery first. This design reduces the need for extensive network parameters and avoids pixel redundancy, allowing our method to capture the most essential features efficiently. This approach not only enhances fusion performance in complex scenes but also demonstrates the viability of end-to-end direct learning. We have further discussed this in lines 89-107 of the manuscript.\n\n`(2) Limitations of Sequential \"Image Restoration + Image Fusion\" for Complex Scenes`\n\n**Error accumulation can occur:** In our study, we analyze the drawbacks of traditional sequential methods in complex environments. First, these methods are vulnerable to error accumulation, as the quality of the fusion output depends heavily on the preceding restoration step. If the initial restoration does not fully address image degradation, the remaining interference can propagate and even amplify in the fusion stage.\n\n**Adding irrelevant or erroneous features:**\nSecond, sequential methods risk introducing irrelevant or erroneous features during fusion. As we illustrate in the manuscript (lines 76-88), blindly applying enhancement in low-light images can inadvertently amplify noise or irrelevant weak features. In such cases, low-light enhancement algorithms cannot reliably reproduce scene details as they would appear in normal lighting, and their enhancement of weak features remains incomplete. However, infrared imaging can provide comprehensive information about the target. As a result, the information from the visible image, with its incompletely enhanced weak features, may interfere with the distribution of thermal radiation information in the infrared image, thereby reducing the quality of the fusion results.\n\n**W2** `About the Redundant Information and Non-Critical Pixels`\n\n**Network module aspect:** In the design of our algorithm\u2019s spatial domain module, our objective is for spatial domain information to compensate for any detail lost during frequency domain learning. We focus on capturing sparse but significant pixel information, using a maximum pooling strategy to prioritize prominent spatial features. This approach encourages the model to focus on high-value, significant features, ensuring that only key spatial details are retained.\n\n**Overall network aspect:** Additionally, since our model is designed to be real-time and lightweight, it is intentionally shallower with fewer layers. In deep learning, redundancy often arises from models with many layers and parameters, which can capture numerous similar or irrelevant features. In contrast, our lightweight model, with limited parameter capacity, prioritizes the learning of essential features and naturally reduces the chance of generating redundant information. This constrained parameter setting compels the model to emphasize the compression and representation of critical features, thereby helping to limit feature redundancy. In simple terms, the small model learns important features preferentially with a limited number of parameters to satisfy the most basic task requirements.\n\n**W3** `Balancing Frequency and Spatial Domain Information`\n\nOur algorithm balances information from both frequency and spatial domains, as discussed in lines 253-259 of the manuscript and illustrated in Fig. 4. In our design, ***frequency domain learning plays a key role in enhancing feature extraction under interference conditions, while spatial domain information supplements this by providing essential structural details.*** Furthermore, we have validated the importance of the spatial domain module through ablation experiments in Table 5, which show that removing the spatial component leads to noticeable degradation in both image features and energy information. These results underscore the necessity of integrating spatial domain information to maintain fusion quality in complex scenes."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer Eiih (W3 (3)-W5)"
            },
            "comment": {
                "value": "**W3** `(3) About the use of MSEC`\n\nFor the overexposure scene, we chose the MSEC structure to reconstruct overexposed visible images. MSEC was selected due to its ability to reconstruct both color and detail in overexposed images, addressing overexposure and underexposure without introducing excessive enhancement. This makes it a more balanced approach for comparison in our image fusion experiments. We have added an explanation of our use of MSEC in the experimental setup section of the revised manuscript.\n\n**W4** `(1) Application Scenes for the Comparison Method`\n\nThank you for raising this important point about the training setup for the comparison models. Indeed, most of the image fusion methods we compare against were designed for ideal conditions and are not directly applicable to the four challenging scenarios in our study (e.g., noise, rain, low light, and overexposure). This lack of adaptability is one of the motivations behind our proposed algorithm, which aims to overcome the limitations of current fusion methods in complex environment. \n\n**W4** `(2) About the Setting of Comparison Method`\n\nTo ensure fair comparisons, we preprocess the degraded source images using existing image restoration models before feeding them into the comparison image fusion models. ***This \"image restoration + image fusion\" approach is used to eliminate interference from degraded pixel information, allowing each comparison method to function in a way that approximates its ideal setup as closely as possible.*** This procedure ensures that the evaluation metrics accurately reflect the performance of each method under challenging conditions, thus maintaining the credibility of our experimental results. We have also provided a more detailed description of this comparison approach in the experimental setup section of the revised manuscript.\n\n**W5** `(1) About the Multi-Degradation Processing Capability of the Comparison Method`\n\nThank you for your insightful question regarding the handling of multiple degradations with a single model. In our study, ***we observed that no our comparison fusion method can seamlessly address all four scenes (noise, rain, low light, and overexposure) with a single model.*** Current multimodal fusion methods are typically limited to ideal scenes, and they often rely on integrating separate image restoration algorithms to handle specific degradations. For example, CoCoNet requires preprocessing with an image denoising algorithm in noisy scenes or with an exposure correction algorithm in overexposed scenes. Similarly, Restormer, an image restoration model used in our comparisons, must be individually trained for each type of degradation.\n\nIn practical applications, it is true that multiple degradations (such as rain and overexposure) can co-occur. While this is a significant challenge for single-model fusion, our proposed algorithm offers a lightweight and efficient solution, achieving a processing speed of 30 frames per second in real-time applications. Although some recent unified models, like DA-CLIP [1] and MPerceiver [2], aim to address multi-degradation issues, ***they require substantial computational resources and are not designed to handle complex combinations like de-raining and overexposure simultaneously.***\n\n**W5** `(2) About the Multi-Degradation Processing Capability of Proposed Method`\n\nIn summary, while we recognize the importance of handling multiple degradations, it remains a challenging area that requires further exploration. ***Our current work focuses on providing a real-time, multi-modality fusion solution across complex scenarios with minimal parameters, which we believe represents a valuable contribution to the field.*** Addressing multiple degradations continues to be a challenge for the proposed real-time and lightweight model.\n\n---\n[1] Luo Z,  et al. Controlling Vision-Language Models for Multi-Task Image Restoration. ICLR 2024.\n\n[2] Ai Y, et al. Multimodal Prompt Perceiver: Empower Adaptiveness Generalizability and Fidelity for All-in-One Image Restoration. CVPR 2024."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer Eiih (W1-W3 (2))"
            },
            "comment": {
                "value": "*Thank you for your thorough review and insightful comments, which capture the core contributions and objectives of our work. Below is our detailed, point-by-point response to each of your observations and suggestions.*\n\n**W1** `(1) About the Mechanism of Mutual Guidance`\n\nAs noted in lines 246-249 of the manuscript, our multimodal image guidance mechanism does not completely replace the amplitude or phase information from either modality. Instead, ***it selectively extracts and enhances relevant details to compensate for potential deficiencies caused by low signal quality***, aiming to minimize the Impact of weak data on the overall fusion.\n\n**W1** `(2) Adaptability to Varying Image Quality`\n\n**Network Design Perspective:**  Our guidance approach operates on the frequency components in the frequency domain, rather than directly manipulating pixel values in the spatial domain. This design mitigates the limitations of individual modality data by focusing on frequency components, making it less susceptible to issues arising from local intensity variations or lower-quality data in one channel.\n\n**Experimental Perspective:** To further support our claim, we have included comparative results for our method on the M3FD and LLVIP datasets in Tables 7 and 8, available in the Appendix form the manuscript. These datasets encompass a wide range of scenes with varying quality in infrared or visible images. Our method consistently achieved top scores across five metrics, surpassing state-of-the-art fusion methods. This demonstrates the robustness and generalizability of our proposed frequency-domain interaction guidance mechanism, even in scenes with differing image quality across channels.\n\n**W2** ` About the MSRS Training Dataset`\n\nThank you for your insightful comments on our use of training data from the MSRS dataset. We agree that consistency in dataset usage is essential for scientific rigor. In our study, we used 1,000 images from the MSRS dataset for experiments on noise and overexposure scenes, even though the full MSRS training set includes 1,083 images. This choice was made to align the experimental setup with the AWMM-100k dataset used for rain scenes, ***allowing us to harmonize key training parameters, such as training time, epochs, and learning rate, across conditions.*** This approach ensures consistency and comparability across diverse scenes. Additionally, when we open-source our processed MSRS images, we will provide the complete dataset to facilitate comprehensive replication in future studies.\n\n**W3** `(1) About the lack of Information Theory-based Non-Reference Metric`\n\nThank you for your valuable feedback. In response, we have  provided a quantitative comparison of entropy across four challenging scenes: noise, rain, low light, and overexposure (As shown in the table below). As discussed in lines 419-424 of the manuscript, CoCoNet\u2019s approach indiscriminately enhances pixel information, leading to higher entropy scores due to pixel redundancy. Our algorithm ranks second in three scenes and third in the overexposed scene, aligning with the AG and SF rankings in Table 1 of the manuscript. These results collectively further validate the effectiveness of our algorithm.\n\n**Tab 1.** Results on $EN$ Metric\n\n| Methods   | Baseline | Noise    | Rain    | Low     | Over    |\n|-----------|----------|----------|---------|---------|---------|\n| CoCoNet   | \u221a        | **7.6254** | **7.7282** | **7.7741** | 7.5184  |\n| Text-IF   | \u221a        | 6.8695   | 6.5085  | 7.3416  | 7.5609  |\n| CDDFuse   | \u221a        | 6.9370   | 6.5558  | 7.3748  | 7.5834  |\n| DeFusion  | \u221a        | 6.6459   | 6.2333  | 7.2054  | 7.3961  |\n| IGNet     | \u221a        | 6.0996   | 6.1322  | 6.2321  | 6.5712  |\n| LRRNet    | \u221a        | 6.4986   | 6.4630  | 6.9203  | 7.0452  |\n| TGFuse    | \u221a        | 6.9035   | 6.4646  | 7.3591  | **7.5895** |\n| Proposed  | \u00d7        | *6.9414(2)* | *6.5770(2)* | *7.4453(2)* | *7.5786(3)*  |\n\n**W3** `(2) About SSIM as a Metric`\n\nRegarding the use of SSIM in Table 2 of the manuscript, we acknowledge your point about potential bias since SSIM is also part of the loss function during training. To address this, ***we have replaced SSIM with the Image Fusion Metric Based on Phase Congruency ($Q_{P}$) for the comparison.*** The revised quantitative comparison results using $Q_{P}$ are provided in the table below.\n\n**Tab 2.** Results on $Q_{P}$ Metric\n\n| Methods   | Baseline | Noise | Rain | Over |\n|-----------|----------|----------|---------|---------|\n| CoCoNet   | \u221a        | 0.1230   | 0.1112  | 0.2205  |\n| Text-IF   | \u221a        | *0.1939* | **0.1740** | 0.3910 |\n| CDDFuse   | \u221a        | 0.1680   | 0.1384  | *0.3942* |\n| DeFusion  | \u221a        | 0.1659   | 0.1342  | 0.3189  |\n| IGNet     | \u221a        | 0.1714   | 0.1450  | 0.2486  |\n| LRRNet    | \u221a        | 0.1618   | 0.1265  | 0.3254  |\n| TGFuse    | \u221a        | 0.1862   | 0.1612  | 0.3871  |\n| Proposed  | \u00d7        | **0.2158** | *0.1622(2)* | **0.4027** |"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the limitations of current infrared and visible image fusion (IVIF) methods when applied to complex scenes with interferences such as rain, noise, and low-light conditions. The authors propose a unified, lightweight IVIF framework that integrates image restoration and fusion in real time, enabling high-quality fusion without the need for pre-processing. Using Fourier domain techniques, the framework captures amplitude information from infrared and visible images and combines these with spatial domain data to maintain clarity and reduce interference. The paper includes extensive qualitative and quantitative experiments to demonstrate the framework\u2019s efficiency and robustness, with results showing improvements in both computational efficiency and image quality across diverse scenes."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe method combines frequency and spatial domain information, effectively separating critical scene features and suppressing interference by processing amplitude and phase components in the frequency domain. For example, in low-light or adverse weather conditions, the amplitude information of images is often disrupted (e.g., reduced brightness or increased reflections), while the phase information, reflecting structural aspects of the image, remains relatively stable. By using the amplitude of infrared images to guide the restoration of visible images and leveraging the phase information from visible images to enhance infrared images, the method achieves an optimal balance between the two modalities, ensuring clear detail retrieval in noisy scenarios.\n2.\tThe lightweight design of the method allows for real-time processing with minimal computational resources, which is critical for practical applications. The paper highlights that the model processes a 640x480 image in just 0.033 seconds, demonstrating exceptional efficiency. This efficiency makes it suitable for systems requiring real-time processing, such as autonomous vehicles or industrial monitoring, further broadening its potential application scope."
            },
            "weaknesses": {
                "value": "1.\tThe mutual guidance mechanism relies on the complementary characteristics of infrared and visible light, such as using the infrared amplitude to guide the visible amplitude, and vice versa. However, when the data quality of one channel is poor (for instance, when the thermal radiation information of the infrared image is weak), the effectiveness of this guidance mechanism may be significantly limited. In such situations, the original amplitude or phase information might not provide sufficient valuable features to complement the other channel, resulting in suboptimal fusion performance in certain scenes.\n2.\tThe data used in the experiments section was selected from existing datasets (MSRS, AWMM-100k). However, the MSRS training set contains only 1,083 pairs of images, so randomly selecting 1,000 pairs from this limited set seems unnecessary. If the authors intend to augment MSRS, they should uniformly apply Gaussian noise and pixel intensity scaling across the entire MSRS training set.\n3.\tIn Table 1, the authors list non-reference evaluation metrics; however, both SF and AG are gradient-based metrics, which are typically highly correlated. The lack of an information theory-based non-reference metric, such as entropy, is notable. In Table 2, the reference-based metrics include SSIM, but SSIM is already incorporated into the loss function during training for all four tasks, which I believe makes the comparison less fair. Additionally, there is insufficient explanation for why the authors used MSEC\u2019s reconstruction structure in the overexposure scenario.\n4.\tDid the models of the other comparison methods undergo specific training on the dataset proposed by the authors? In fact, other methods are typically trained only on the original MSRS training set. If the evaluation metrics for these comparison methods are obtained by testing their open-source models on the data across these four scenarios, while the authors\u2019 proposed model was specifically trained for each scenario, then I believe the experimental results lack sufficient credibility.\n5.\tThe comparison methods listed in the experiments section require only a single model to handle all four scenarios, while the proposed method requires separate training for each scenario. In real-world applications, however, rain and overexposure could occur simultaneously. In such cases, the other methods can generalize directly using their models, but which model output would the proposed method select as the result? If the model trained for rain is chosen, would it affect the handling of the overexposed areas?"
            },
            "questions": {
                "value": "Please rebuttal according to the weaknesses item by item."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a unified lightweight network designed for infrared and visible image fusion (IVIF) that aims to address the limitations of existing techniques in complex scene conditions such as bad weather, low light, and high noise. The authors conduct a frequency domain analysis of modal degradations, leveraging the complementary strengths of both infrared and visible modalities. A novel spatial domain branching strategy is introduced to enhance local detail resolution in the fusion results. The paper claims extensive qualitative and quantitative improvements in handling complex scenes while maintaining real-time computational efficiency."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The writing of this paper is easy to understand.\n2. The description of the methodology is detailed.\n3. The motivation of the methodology is detailed."
            },
            "weaknesses": {
                "value": "1. The proposed method based on exchanging amplitude and phase lacks novelty.\n2. The advantages of the paper are not significant when considering the overall computational cost of the results.\n3. The paper should also be compared with other SOTA methods, such as MURF (Xu et al., 2023), SegMiF (Liu et al., 2023), DDFM (Zhao et al., 2023), and EMMA (Zhao et al., 2024)."
            },
            "questions": {
                "value": "What specific types of degradation were most challenging for previous methods, and how does DSPFusion overcome these issues?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This model adopts a new frequency-domain perspective to solve the IVIF problem in complex scenes, and uses a multimodal information interaction guidance module that not only utilizes frequency-domain information but also integrates spatial information to more comprehensively and effectively extract image details, compensating for the loss of details that may be caused by relying solely on frequency-domain information. The model has been extensively experimentally validated in four complex scenarios, including noise, rainy weather, overexposure, and low lighting, achieving excellent image fusion quality. Its framework integrates image restoration and fusion, avoiding the problems of error accumulation and irrelevant feature introduction that may occur in traditional two-stage processing methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper proposes a multi-modal interactive guidance mechanism that combines frequency domain and spatial domain learning, which effectively enhances the effect of infrared and visible light image fusion in complex scenes. Through mutual guidance, the amplitude of the infrared image and the phase information of the visible light image are used to achieve richer feature extraction and fusion between different modalities.\n\n2. Through a large number of qualitative and quantitative experiments, the article demonstrates the significant performance improvement of this method in removing interference information and restoring image details in complex scenes such as noise, rain, overexposure, and low light. Outperforms existing state-of-the-art methods in multiple metrics.\n\n3. The network design emphasizes lightweight and efficient computing, and can achieve real-time image fusion under limited computing resources, which is a very important advantage in practical applications. Experimental data shows that this method only takes 0.033 seconds for each fusion when processing images of 640\u00d7480 size."
            },
            "weaknesses": {
                "value": "Although the motivation of this article is relatively clear and attempts to solve the problem of image fusion in harsh environments, there are still some shortcomings as follows:\n1. First of all, the method proposed by the author does not seem to have any theoretical innovation. Why does the method in the article achieve better performance than image restoration?\n2. Although a combination of frequency domain and spatial domain is proposed to extract key features, this method does not have a clear optimization strategy for how to reduce unnecessary redundant information. In complex scenes, this may cause non-critical pixels to be mistaken for valuable information, thereby affecting the fusion effect.\n3. The motivation of the article emphasizes the use of frequency domain learning to enhance information extraction under interference conditions, but does not fully consider the important balance of frequency domain and spatial domain information. In the IVIF task, relying solely on the frequency domain may lead to inaccurate processing of spatial details in complex scenes, thereby affecting the fusion quality."
            },
            "questions": {
                "value": "My questions is already stated in the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a unified and lightweight framework designed for real-time infrared and visible image fusion in environments characterized by complex interferences from a frequency domain perspective. It tackles the issue of complex scenes fusion problems, such as adverse weather, low-light environments, and noisy fusion. Authors introduce a multi-modality information interaction guidance module for multi-modality feature interaction and extraction. Extensive fusion experiments in four complex conditions: noise, rain, overexposure, and low-light, verified the effectiveness of the proposed method in dealing with interfering information."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "(1) This paper introduces a unified framework for real-time infrared and visible image fusion in different complex scenes, this is the first work of addressing complex scenes image fusion problems in frequency domain. \n\n(2) The paper proposes a multi-modality interactive guidance mechanism within the Fourier domain, which efficiently extracts and restores useful features from degraded pixels by leveraging the complementary strengths of different modalities.\n\n(3) The fusion performance of this work is very impressive. Extensive complex scenes fusion experiments cover rain, overexposure, low-light, and noisy demonstrate this method outperforms the state-of-the-art fusion methods in both subject and object evaluations."
            },
            "weaknesses": {
                "value": "(1) In the caption of figure 5, it is recommended to add the words \u201ccomplex scenes\u201d.\n\n(2) Section 4.4 and 4.5 could be combined as one part.\n\n(3) The source code is suggested to be public."
            },
            "questions": {
                "value": "n/a"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new IVIF method designed to be robust in complex scenes, including rain, noise, and under-exposed conditions. The core concept is that the amplitude component of infrared images and the phase component of visible images provide complementary modal information for IVIF, and interactively fusing these components enhances fusion quality in challenging scenarios. Specifically, the method decomposes the infrared and visible images into amplitude and phase components by applying the Fourier Transform, thus transferring them into the frequency domain. Based on this, a \u201cmulti-modality interactive guidance mechanism\u201d is introduced to fuse the frequency-domain information."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The idea of using frequency domain information to address IVIF in complex scenes is interesting. \n+ The paper is well-organized, with a logical flow between sections. The language is clear, and the content is presented in an easily understandable manner. \n+ The proposed methods are grounded in theoretical explanations, and the experiments conducted are fair and comprehensive."
            },
            "weaknesses": {
                "value": "- Despite the demonstrated effectiveness, similar methods have been widely applied in other fields, such as low-light image enhancement and image denoising. For example, \u201cFourLLIE: Boosting Low-Light Image Enhancement by Fourier Frequency Information\u201d transforms the image into the Fourier domain and utilizes an estimated amplitude map for enhancement. How does the proposed method improve upon or differ from these existing methods in the IVIF task?\n- The authors claim that existing IVIF methods fail in complex scenes because they may misinterpret interference features as valuable. However, the proposed Spatial Domain Module also fails to avoid erroneous features. The convolution operations in this block extract all local features from the input image, including both degradation artifacts and valuable features. This could lead to the mistaken interpretation of degradation artifacts as valuable features, resulting in error accumulation or the introduction of incorrect features during the fusion stage. Max pooling cannot fully address the issue of misextracting degradation artifacts, especially when these artifacts have high intensity, as max pooling may still retain these distracting features as significant ones. More explanation is needed.\n- The final fusion result is obtained by directly combining the source image information with the feature maps. Wouldn't this affect the fusion results, as interference features might also be added to the final image? \n- In Figure 5 (\u201cFFT Domain\u201d), \u201cDWT: Discrete Wavelet Transform\u201d is labeled, but I could not locate it in the figure. Furthermore, there seems to be no explanation of DWT in the paper. Please clarify it.\n- Minor issues: There is a typo in Line 018: \u201c, We.\u201d Additionally, in the second column of the \u201cRain\u201d table in Table 2, the value for LRRNet (\u201c0.8050\u201d) should also be highlighted in red."
            },
            "questions": {
                "value": "- Same as the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}