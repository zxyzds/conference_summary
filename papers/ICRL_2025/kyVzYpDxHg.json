{
    "id": "kyVzYpDxHg",
    "title": "Learning equivariant tensor functions with applications to sparse vector recovery",
    "abstract": "This work characterizes equivariant polynomial functions from tuples of tensor inputs to tensor outputs. Loosely motivated by physics, we focus on equivariant functions with respect to the diagonal action of the orthogonal group on tensors. We show how to extend this characterization to other linear algebraic groups, including the Lorentz and symplectic groups. \n\nOur goal behind these characterizations is to define equivariant machine learning models. In particular, we focus on the sparse vector estimation problem. This problem has been broadly studied in the theoretical computer science literature, and explicit spectral methods, derived by techniques from sum-of-squares, can be shown to recover sparse vectors under certain assumptions. Our numerical results show that the proposed equivariant machine learning models can learn spectral methods that outperform the best theoretically known spectral methods in some regimes. The experiments also suggest that learned spectral methods can solve the problem in settings that have yet to be theoretically analyzed.\n\nThis is an example of a promising direction in which theory can inform machine learning models and machine learning models can inform theory.",
    "keywords": [
        "equivariant machine learning",
        "tensors",
        "sparse vector recovery"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "We provide a characterization for equivariant tensor polynomials and use that for a machine learning approach to solving the sparse vector recovery problem in new settings.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kyVzYpDxHg",
    "pdf_link": "https://openreview.net/pdf?id=kyVzYpDxHg",
    "comments": [
        {
            "summary": {
                "value": "This work characterizes equivariant polynomial functions from tuples of tensor inputs to tensor outputs. The goal behind these characterizations is to define equivariant machine learning models. In particular, they focus on the sparse vector estimation problem. They try to recover a sparse vector from the given an orthonormal basis of subspace spanned by $d$ vectors, where one is the sparse vector to be recovered and the other $d-1$ are just vectors. The recovered sparse vector is stated in (25) and the contributed is how to construct $h$."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "They construct a equivariant function $h$."
            },
            "weaknesses": {
                "value": "There are some question you need to solve:\n\n1 Are there some applications about your proposed problem to recover a sparse vector?\n\n2. The representation of $h$ is only related to matrices, but not a complex tensor. Your definitions and analysis are all based on tensors. \n\n3. Can you give a details how to get (28) from Corollary 1?\n\n4. Why you set $h$ to be a equivariant function? What is the advantages?\n\n5. In the numerical experiments about mnist, do you get the similar results if you choose $v_1,\\cdots, v_{d-1}$ to be some other vectors which is not so related to the sparse vector?\n\n6. How to get the orthonormal basis for the spanned subspace? Do you get the similar results if you choose different orthonormal basis?"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies polynomial functions from tensor spaces to tensor spaces that are preserved by the action of the Orthogonal group O(d), the indefinite orthogonal group O(s, k-s) or the symplectic group Sp(d). The authors provide a characterization of such polynomial functions in the form of a linear combination of \"elementary functions\".\n\nAs a main application of such result authors propose an algorithm for planted vector recovery, dictionary learning, and experimentally demonstrated that the proposed algorithm outperforms earlier proposed Sum-of-Squares based methods.\n\nStudy of such polynomial function is also tangentially motivated by physics applications."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The characterization of equivariant polynomial functions studied in this paper is demonstrated to have applications to a planted sparse vector recovery problem which has several practical applications and received a lot of attention over the past decade. In this problem, one is given a linear space $L$ defined by a sparse planted vector $v_1$ and randomly selected complement basis $v_2, ... v_n$. The goal is to recover vector $v_1$ from $L$. The Authors observe that the function $h: L \\rightarrow v_1$ is equivariant so can be parametrized using the main characterization proven in this paper and they learn parameters of h by training NN over the training dataset.\nI think that this is a new interesting approach to the sparse vector recovery problem.\n\nThe proposed characterizations of equivariant polynomial functions may find applications in other areas and may be of its own interest."
            },
            "weaknesses": {
                "value": "If I understand the proposed approach to the sparse vector recovery problem, for the approach to work one actually need to observe many data samples to be able to train neural networks that define coefficients for the unknown polynomial $h: L \\rightarrow v_1$. This may be a strong assumption in many applications and is different from SOS-based methods that can recover $v_1$ from observing only one subspace L and does not require to go through the learning process. \n\nThe proof of the main theorem follows standard steps for this sort of problems: problem is easily reduced to homogenous degree r monomials and for homogeneous degree monomials the characterization is obtained by using a somewhat standard group averaging technique. In this sense, the proof of the main result is somewhat \"standard\" for the literature."
            },
            "questions": {
                "value": "1. Can you provide a bit more insight into what are the requirements for the training set for your method to be able to train the neural networks defining coefficients in the parametrization of h in Eq 28? Do you need to assume that all training samples have the same planted sparse vector $v_1$? Or is it sufficient to assume that all such vectors $v_1$ are coming from some distribution with nice properties?\n2. How many training samples do you anticipate needing to recover $n$-dimensional planted sparse vector? \n3. Are you aware of any immediate applications of the provided characterizations for the indefinite orthogonal group O(s, k-s) or the symplectic group Sp(d)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper develop a generic framework for defining functions that are equiv-\nariant under the action of classical Lie groups acting diagonally on tensors.\nThe groups considered include the orthogonal group O(d), the indefinite orthogonal group O(s,k\u2212 s), and the symplectic group Sp(d) and other general group actions. The main theoretical contribution is a characterization of\nO(d)-equivariant polynomial functions mapping multiple tensor inputs to tensor outputs. The authors prove that any such functions can be expressed as linear\ncombination of tensor products of the inputs with O(d) isotropic tensors. An\nimportant result is Theorem 1, which provides an explicit parameterization\nof O(d)-equivariant polynomial functions. The authors also present a practical\ncorollary (Corollary 1) for the case where the inputs are vectors and the out-\nputs are vector spaces, showing that the equivariant functions can be written\nas linear combinations of basis elements formed by permutations of the input\nvectors. Furthermore, the author also present a generalization form of general\ntensors in Theorem 2 and Corollary 2.\n\nAs a proof of concept, They consider the challenge of recovering a planted\nsparse vector from a set of vectors forming an orthonormal basis of a sub-\nspace. By designing a machine learning model that learns an equivariant 2-\ntensor (which is the covariance matrix) from data, they use the top eigenvector\nof this tensor as an estimator for the sparse vector. The numerical experiments\ndemonstrate that the learned algorithms outperform state-of-the-art methods.\nThe models adapt effectively to various noise structures and data sampling\nmethods.\n\nIn physics, Lorentz group O(1,3) is a special case of the equivariant group.\nParticularly in general relativity, tensors are used to describe physical quantities such as the curvature of spacetime, energy-momentum distributions. The\nauthors\u2019 work generally present one of the proof for how to comprehend this\nstructure in the regime of mathematical tensor products. they also provide a\nframework for building physics informed machine learning models that inherently respect Lorentz symmetry and applying for advanced physics studies."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "## Originality\nThe paper presents a novel and significant advancement in the field of equiv-\nariant machine learning. Additionally, it uniquely incorporates the indefinite\northogonal group O(s,k\u2212 s) and the symplectic group Sp(d), which are funda-\nmental in physics and other scientific domains but have been less explored in the\ncontext of physics informed machine learning. The application of these theo-\nretical developments to the sparse vector estimation problem further showcases\noriginality by demonstrating how algorithms can outperform state-of-the-art\nmethods in regimes not previously addressed.\n\n## Quality\nThe paper is of high quality, offering rigorous mathematical formulations and\nproofs that underpin the proposed methods. The experiments are well-designed,\nand the empirical results convincingly demonstrate the effectiveness.\n\n## Significance\nThe significance of the work is substantial. By providing explicit parameteriza-\ntions for equivariant functions of tensor inputs and outputs, the paper equips\nresearchers and practitioners with powerful tools. The successful application to\nsparse vector es"
            },
            "weaknesses": {
                "value": "## Limited Scope of Experiments\nOne of the main weaknesses of the paper is the limited scope of the experimental\nevaluation. The experiments are primarily focused on the sparse vector estima-\ntion problem, which, while important, represents a rather narrow application\ndomain. The method should have broader applications, such as in physics and\ngeneral relativity, but these areas are not sufficiently discussed or explored.\n\n## Clarity\nThe paper is difficult to follow due to disorganized notation and unclear variable\ndefinitions. The use of coordinates is somewhat messy, which makes the math-\nematical developments hard to track. Variables are often introduced without\nproper explanation or context, causing confusion. For example:\n\n\u2022 In Definition 4, the indices need more explanation.\n\n\u2022 In Theorem 1, how each tensor alk contracts with certain indices (dimen-\nsions) of cl1 ,l2 ,...,lr should be explained more clearly.\n\n\u2022 In Example 1, there should be more explanation of why only the generic\nelements of the G4 group are used while others are contracted.\n\n\u2022 In Lemma 1, the meaning and constraints of \u03b1\u03c3 need clarification.\nMoreover, the progression from theoretical concepts to experimental appli-\ncations lacks smoothness. The presentation could be improved by organizing\nnotation more systematically and clearly defining all variables.\n\n## Comparative Analysis\nThere are other contemporary techniques for sparse vector recovery and tensor\nanalysis that are not considered. The lack of comparison with a wider array\nof methods makes it difficult to fully assess the advantages of the proposed\napproach."
            },
            "questions": {
                "value": "\u2022 The statement of Theorem 1 introduces a complex expression involving\nlinear combinations over isotropic tensors. Could the authors provide more\nintuitive explanations or mathematical nature of this theorem?\n\n\u2022 The authors state that parameterizing all permutation-invariant polyno-\nmial functions may be as challenging as solving the graph isomorphism\nproblem. Could the authors provide an estimate of the computational\ncomplexity or approximate methods for tackling this problem? can we\npush this into high-dimensional settings?\n\n\u2022 While MLPs are popular in approximating the polynomial functions, is\nthere a risk that they might not fully capture the necessary polynomial\nstructures or equivariance properties?\n\n\u2022 The learned SVH models perform better when stringent data assumptions\nare not met. Could the authors elaborate on why their models are more\nrobust in these scenarios? Please provide valuable insights.\n\n\u2022 The paper focuses on equivariance. Could you derive some examples for\nother groups, such as unitary groups or affine transformations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a framework for constructing equivariant tensor functions to facilitate sparse vector recovery in distributions with specific symmetry properties, such as those invariant under orthogonal transformations. The key concepts introduced include the $ k(p)$-tensor space, which captures parity characteristics in tensor components, and the construction of equivariant polynomial functions that remain consistent under orthogonal group actions. The framework also incorporates tensor operations such as outer product and $ k $-contraction to enable flexible tensor combinations while preserving equivariance. Furthermore, the paper extends Haar averaging to non-compact groups, allowing the model to maintain equivariance in settings with broader symmetry requirements, like the indefinite orthogonal group."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- To the best of my knowledge, this paper introduces a new approach to equivariant tensor functions, expanding to cases with mixed symmetry properties through the $k(p)$-tensor space concept.\n\n- The paper provides a well-grounded method with precise definitions, such as outer product and $k$-contraction, supported by theoretical proofs.\n\n- Extending Haar averaging to non-compact groups (such as the indefinite orthogonal group $O(s, d-s)$ and symplectic group $Sp(d)$) broadens the versatility."
            },
            "weaknesses": {
                "value": "1. **Lack of Clear Theoretical Motivation and Practical Relevance**\n\n   - **W1.1:** The paper focuses heavily on mathematical formulas and definitions but lacks intuitive explanations to clarify the need for equivariant tensor functions and their practical motivations.\n   - **Suggestion**: Consider adding a high-level overview section that explains why equivariance is essential in certain applications and provides examples that demonstrate the demand for equivariant tensor functions in real-world scenarios.\n\n   - **W1.2:** Key theoretical challenges and the novel aspects of the proposed method are not thoroughly discussed, leaving readers uncertain about the primary obstacles and contributions within the framework.\n   - **Suggestion**: Consider adding a dedicated \u201cContributions\u201d section that explicitly outlines how this method differs from existing approaches, helping readers understand the unique contributions of the framework.\n\n   - **W1.3:** The paper does not clearly explain the necessity of \"learning equivariant tensor functions,\" lacking specific application scenarios that showcase the unique benefits and suitability of these functions. It remains unclear why certain types of tensors in applications require equivariance.\n   - **Suggestion**: Include examples of practical applications where specific types of tensors need to be equivariant and clarify how these scenarios validate the advantages of using equivariant tensor functions.\n\n2. **Limitations in Sparse Vector Recovery Application**\n\n   - **W2.1:** The paper centers its applications on sparse vector recovery, a problem that is primarily theoretical and rooted in computer science, with limited overlap with current mainstream machine learning applications, such as computer vision or natural language processing.\n   - **Suggestion**: Discuss the potential connections between sparse vector recovery and mainstream machine learning applications, such as compressed sensing, feature selection, or dimensionality reduction in deep learning, to enhance the method\u2019s appeal.\n\n   - **W2.2:** The paper does not demonstrate if or how sparse vector recovery can extend to practical uses in deep learning or other prominent areas within machine learning, lacking case studies or examples of its effectiveness in real-world tasks.\n   - **Suggestion**: Consider adding relevant cases or discussions on how the method could be integrated into neural network architectures or applied to tasks such as network pruning, compression, or efficient inference, to illustrate its practicality.\n\n   - **W2.3:** The real-world significance of the proposed method is unclear, potentially making it seem disconnected from practical contributions to machine learning, which could limit its appeal to a broader audience.\n   - **Suggestion**: To increase its appeal, the authors could discuss the broader implications of their work, such as how advances in equivariant functions might contribute to more efficient or interpretable machine learning models over the long term.\n\n3. **Limited Relevance to Machine Learning**\n\n   - **W3.1:** While the method shows mathematical novelty, its applications in machine learning are not well-established, which may restrict its potential readership. Machine learning researchers tend to focus on methods with clear, practical applicability and accessibility.\n   - **Suggestion**: Strengthen the connection to machine learning demands by discussing how equivariant tensor functions could enhance model interpretability or efficiency, demonstrating its contributions to machine learning research.\n\n   - **W3.2:** The paper extensively uses group theory terms and concepts, such as orthogonal groups and indefinite orthogonal groups, which may be unfamiliar and challenging for machine learning readers without a strong mathematics background.\n   - **Suggestion**: Consider including a brief primer on the relevant group theory concepts or offering more intuitive explanations for these mathematical objects. Providing references to introductory resources may also help readers better understand these terms.\n\n\n**Additional Suggestions:**\n\n- **Add Visual Aids and Case Studies**: Introduce diagrams in appropriate sections to help readers visually understand the basic principles of the method, or add case studies to illustrate its practical applications. This could help engage a wider range of machine learning researchers."
            },
            "questions": {
                "value": "Please refer to each weakness and suggestion section to help improve the readability and applicability of the work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}