{
    "id": "zkGxROm7D3",
    "title": "State & Image Guidance: Teaching Old Text-to-Video Diffusion Models New Tricks",
    "abstract": "Current text-to-video (T2V) models have made significant progress in generating high-quality video. However, these models are limited when it comes to generating dynamic video scenes where the description per frame can vary dramatically. Changing the color, shape, position and state of objects in the scene is a challenge that current video models cannot handle. In addition, the lack of a cheap image-based conditioning mechanism limits their creative application. To address these challenges and extend the applicability of T2V models, we propose two innovative approaches: **State Guidance** and **Image Guidance**. **State Guidance** uses advanced guidance mechanisms to control motion dynamics and scene transformation smoothness by navigating the diffusion process between a state triplet <initial state, transition state, final state>. This mechanism enables the generation of dynamic video scenes (Dynamic Scene T2V) and allows to control the speed and the expressiveness of the scene transformation by introducing temporal dynamics via a guidance weight schedule across video frames. **Image Guidance** enables Zero-Shot Image-to-Video generation (Zero-Shot I2V) by injecting reference image into the initial diffusion steps noise predictions. Furthermore, the combination of **State Guidance** and **Image Guidance** allows for zero-shot transitions between two input reference frames of a video (Zero-Shot II2V). Finally, we introduce the novel **Dynamic Scene Benchmark** to evaluate the ability of the models to generate dynamic video scenes. Extensive experiments show that **State Guidance** and **Image Guidance** successfully address the aforementioned challenges and significantly improve the generation capabilities of existing T2V architectures.",
    "keywords": [
        "Text-to-Video Generation",
        "Diffusion Models",
        "Diffusion Guidance",
        "Zero-shot Image-to-Video Generation"
    ],
    "primary_area": "generative models",
    "TLDR": "We present two new sampling methods for Text-to-Video (T2V) diffusion models that enhance pre-trained models, allowing for dynamic scene generation and zero-shot image-to-video and image-image-to-video generation (based on the first and last frames).",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zkGxROm7D3",
    "pdf_link": "https://openreview.net/pdf?id=zkGxROm7D3",
    "comments": [
        {
            "comment": {
                "value": "Dear Reviewer NpS9,\n\nThank you for your thoughtful review and for recognizing the strengths of our work. We appreciate your specific feedback and are committed to addressing each of your concerns to improve the clarity and rigor of our paper.\n\n### **Weakness 1**\n\n**Reviewer's Comment:** *\"The paper feels rushed. (The caption of the teaser figure misplaced the text for sub-figures (B) and (C)).\"*\n\n**Response:** We apologize for the typo in the caption of the teaser figure, which misplaced the text for subfigures (B) and (C). We will correct this in the revised version. If there are other areas where the presentation could be improved, we welcome your specific suggestions to improve the clarity of our paper.\n\n### **Weakness 2**\n\n**Reviewer's Comment:** *\u201cWhat is the novelty of the proposed method given the \"Make Pixels Dance\" (CVPR'24) paper\u201d*\n\n**Response:** We understand your concern about the novelty of our method compared to the paper \"Make Pixels Dance\" [1] (PixelDance). Although both papers use state triplets, approach is significantly different:\n\n- **PixelDance:** Utilizes \u27e8first frame, text prompt, last frame\u27e9 triplets in a custom architecture **requiring training from scratch**, aimed at **enhancing T2V model training and improving video dynamics**.\n- **State & Image Guidance:** Employs a **training-free scheme** with novel inference mechanics for existing T2V models like VideoCrafter2 [2]:\n    - *State Guidance*: Conditions on \u27e8first state prompt, transition state prompt, last state prompt\u27e9 for **dynamic video scene generation**.\n    - *Image Guidance*: Conditions on \u27e8prompt, reference\u27e9 duplet for **Zero-Shot I2V generation**.\n    - *Combination of State Guidance and Image Guidance*: Conditions on \u27e8first frame, text prompt, last frame\u27e9 triplet for **Zero-Shot II2V generation**.\n\nWhile our Zero-Shot II2V pipeline is comparable to the II2V regime of PixelDance, the motivations and implementations differ significantly. For a detailed comparison with *\"Make Pixels Dance\",* please refer to **Appendix A.5 (lines 892-896 and Figure 8)**.\n\n### **Weakness 3**\n\n**Reviewer's Comment:** *\u201cIt is extremely difficult to follow the experimental results and appreciate the contributions.\u201d*\n\n**Response:** We understand the importance of clarity in the presentation of experimental results and would like to address your concerns:\n\n1. **References to different methods**: Due to space limitations in the tables, we have included references to the methods in the text preceding each table. Specifically:\n    - **Table 3**: The methods are listed on lines 395-396 and 403-405.\n    - **Table 5**: Methods are listed on lines 462-463.\n    - **Table 6**: Methods are listed on lines 484-485.\n2. **Ablation studies in main text:** Including ablation studies in the main text is standard practice to highlight the impact of different components of the method. For example, recent papers such as \"Ground-A-Video\" [3] (ICLR'2024), \"ControlVideo\" [4] (ICLR'2024), and the aforementioned \"Make Pixels Dance\" [1] (CVPR'2024) include ablation studies in their experimental sections. We believe that this placement helps readers to better understand the papers.\n3. **Supplementary Material Navigation**: Guidance for navigating the Supplementary Files is provided in **Appendix A.7**. We aim to make it as user-friendly as possible and will consider further improvements.\n4. **The bold numbers in Table 3** highlight the performance improvements when using State Guidance (SG) over standard inference. Other methods are included for reference to show that even commercial models and models with multi-prompt inference may not perform as well.\n5. **Table 5** shows the performance of our method with different \"thresholds\" (hyperparameter values) introduced in formula (6). We compare these variations with each other and with TI2V-Zero to show the flexibility and effectiveness of our approach (with the best results in bold).\n\nWe hope these clarifications have addressed your concerns. We are committed to improving the presentation and clarity of our work and look forward to your response. Please don't hesitate to contact us if you have any further questions.\n\nThank you again for your valuable feedback.\n\n---\n\n[1] Zeng, Yan, et al. \"Make Pixels Dance: High-Dynamic Video Generation.\" (CVPR\u20192024).\n\n[2] Chen H. et al. \"VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models.\" (CVPR\u20192024).\n\n[3] Jeong H., Ye J. C. \"Ground-a-video: Zero-shot grounded video editing using text-to-image diffusion models.\" (ICLR\u20192024).\n\n[4] Zhang Y. et al. \"ControlVideo: Training-free Controllable Text-to-Video Generation.\" (ICLR\u20192024)."
            }
        },
        {
            "comment": {
                "value": "Dear Reviewer sBeJ,\n\nThank you for your valuable feedback. We would like to offer a brief response to your concerns regarding our submission and to initiate a discussion. Note that the Weakness 4 and Weakness 5 responses will be further supplemented by the results of the experiments we are currently working on.\n\n### **Weakness 1**\n\n**Reviewer's Comment:**\u00a0*\u201c*In Sec. 3, the definition of diffusion models might be incorrect*\u201d*\n\n**Response:** Unfortunately, it is difficult to localize the incorrectness of the definition. Could you please specify this comment?\n\n### **Weakness 2**\n\n**Reviewer's Comment:**\u00a0*\u201c*The idea of state guidance seems similar to the deforum-like technique used in the stable diffusion user community*\u201d*\n\n**Response:** State Guidance differs from Deforum in the following ways:\n\n1. Models and Objectives:\n\n- *Deforum*: Uses Image Diffusion models to animate single static images.\n- *State Guidance*: Utilizes Video Diffusion models specifically designed for generating dynamic video scenes.\n\n2. Generation Process:\n\n- *Deforum*: Iteratively transforms frames by adding and removing noise with Text-to-Image models.\n- *State Guidance*: Applies a concept guidance strength schedule over time to ensure smooth transitions between initial, transition, and final states.\n\nAdditionally, our method incorporates Image Guidance and combines it with State Guidance to enhance video quality and consistency (see Lines 100-107).\n\n### **Weakness 3**\n\n**Reviewer's Comment:**\u00a0*\u201c*Entries in the proposed dynamic scenes benchmark consist of three states tailored for the proposed framework\u2026*\u201d*\n\n**Response:** Our Dynamic Scenes Benchmark comprises 106 **single prompts** describing video scenes with significant changes, compatible with all standard Text-to-Video (T2V) models. The state prompt triplets are specifically designed for our State Guidance approach, utilizing pre-trained models and requiring structured input.\n\nRegarding experimental consistency, as shown in Table 3, we evaluated standard pre-trained T2V models using their typical inference method and our State Guidance approach without altering their architectures or weights. State Guidance enables models to efficiently incorporate state prompt triplets, significantly enhancing their ability to generate dynamic video scenes.\n\nAdditionally, Table 3 includes results from two closed-source commercial T2V models and two models that use multiple prompts from large language models (LLMs), which also struggle with dynamic scene generation, confirming the reliability and relevance of our benchmark.\n\n### **Weakness 4**\n\n**Reviewer's Comment:**\u00a0*\u201c*The paper does not mention how generated results were selected\u2026*\u201d*\n\n**Response:**  We generated all videos using a fixed random seed to ensure consistency, and preliminary tests showed negligible metric variation with different seeds. While reporting mean and standard deviation is ideal, the computational demands of diffusion-based video generation make it challenging and it is uncommon in the current literature.\n\nTo address your concern, we will include the mean, standard deviation, and success rates for key results in Table 3 for both LaVie and VideoCrafter2 with and without State Guidance. We hope this addition meets your expectations.\n\n### **Weakness 5**\n\n**Reviewer's Comment:**\u00a0*\u201c*For II2V experiments, the paper does not compare with SEINE*\u201d*\n\n**Response:** Due to page limitations, we were initially unable to include this comparison. We recognize its importance and will incorporate it in the revised manuscript. Below is the comparison, where M stands for Metamorphosis, A for Animation, and O for Overall performance.\n\n| Method | FID (M) | PPL (M) | FID (A) | PPL (A) | FID (O) | PPL (O) |\n| --- | --- | --- | --- | --- | --- | --- |\n| SEINE | 82.03 | 47.72 | 48.25 | 16.26 | 67.60 | 39.33 |\n| S&IG (Ours) | **35.46** | **12.26** | **31.44** | **6.58** | **30.15** | **10.75** |\n\n### **Weakness 6**\n\n**Reviewer's Comment:**\u00a0*\u201c*The scale of user study seems relatively limited and the design of user study seems flawed*\u201d*\n\n**Response:** Thank you for your feedback. We are revising the study to improve its robustness and will include the updated design in our revised submission next week.\n\n### **Comment 1**\n\n**Reviewer's Comment:**\u00a0*\u201c*The paper coined a new term II2V, which is actually frame inbetweening/interpolation*\u201d*\n\n**Response:** We define II2V as generating transitions between two input reference frames (see Lines 49-50). This term aligns with related concepts like I2V and Dynamic T2V, ensuring consistency and clarity within our framework.\n\n----\nPlease let us know if our answers meet your expectations or if there are other areas that need clarification. We look forward to your continued feedback. Thank you again and we hope to hear from you soon."
            }
        },
        {
            "summary": {
                "value": "# Summary\n\nThe paper proposes a training-free framework for generating better motion dynamics and adding image conditions with existing pre-trained T2V models. Extensive experiments demonstrate the effectiveness of the proposed framework"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "# Strengths\n\n- The proposed framework is training-free, and indeed achieves better motion dynamics for the mentioned types of prompts\n- The proposed framework outperforms the mentioned baselines"
            },
            "weaknesses": {
                "value": "# Weaknesses\n\n- In Sec. 3, the definition of diffusion models might be incorrect\n- The idea of state guidance seems similar to the deforum-like technique used in the stable diffusion user community\n- While the guidance schedule seems reasonable, the paper does not mention how it was designed/selected\n- Entries in the proposed dynamic scenes benchmark consist of three states tailored for the proposed framework, resulting in inconsistent experiments settings when compared with other baselines which does not support this type of inputs. In that case, is unclear how reliable the proposed benchmark is\n- The paper does not mention how generated results were selected. Considering diffusion models can generate various of results from the same input conditions from different seeds, it would be better to report mean+std for each metric and report the success rate of each generation\n- For II2V experiments, the paper does not compare with SEINE\n- The scale of user study seems relatively limited and the design of user study seems flawed: \"more changes\" in question 2 does not necessarily indicate the result is better in terms of visual quality. The results with flickering artifacts and incorrect/unfavourable color changes could also be considered as \"more changes\"\n\n# Other comments (not weaknesses)\n\n- The paper coined a new term II2V, which is actually frame inbetweening/interpolation"
            },
            "questions": {
                "value": "Please refer to the weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel video generation guidance method capable of producing videos with drastically different cross-frame descriptions, such as texture changes, morph transformations, and large motions. The core innovation of the method is the state triplet, which decomposes the video into different phases, each with its own distinct description. The state triplet can be generated either from a large language model (LLM) or manually. The proposed method is training-free and has been evaluated on a new video benchmark, achieving performance comparable to methods that require task-specific training."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed method is practically useful given that:\n\n* It enables the transformations that would be very difficult to model with pretrained models, such as morph transformation, drastic texture changes over frames. \n\n* The method is training-free and does not require text-video pairs with the target motion patterns, which is hard to collect in scale. The training free method achieves comparable performance as the compared training-based method."
            },
            "weaknesses": {
                "value": "- The T2V limitation mentioned in L202 is not convincing enough. The prompts used in the paper are relatively simple, consisting of only one or two sentences. Some related works, such as CogVideoX [1], utilize DiT-based structures and demonstrate that detailed prompts significantly improve video generation quality, both in appearance and motion. Therefore, if we consider a scenario with both detailed video captions and a DiT-based model that relies less on explicit per-frame modeling due to (1) temporal compression in the tokenizer, and (2) stronger spatial-temporal modeling capabilities (i.e., cross-frame modeling rather than per-frame), the limitation highlighted for the T2V model becomes less relevant. This is because we would not be restricted by a limited T2I model (point 2) and would benefit from enhanced spatial-temporal modeling (point 1).  \n\n- The paper lacks key information on how the transition order is maintained. While Eq. 1 models the joint conditional distribution given the prompt triplet, it does not specify how the generated images are constrained to follow the prompt order: initial -> transition -> final stage.  Ensuring this sequential alignment is crucial for achieving controllability and realism in the generated video.\n\n- As mentioned in the limitation section in the supplementary, the method introduces additional hyper-parameters, such as the guidance scale at for the triplet states. Tweaking those hyper-parameter would be a case-specific effort and paper does not propose a principled approach for estimating/optimizing those hyper-parameters. \n\n- As mentioned in the first item, there are strong models taking much more descriptive prompt as input for video generation. However, the paper does not include the comparison with those methods. The lack of this comparison makes the claim about the T2V limitation and the proposed method less convincing.\n\n[1] CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others. arXiv preprint arXiv:2408.06072"
            },
            "questions": {
                "value": "In addition to the key information missing mentioned above, I have several questions related to the details of the paper:\n\n1. How to handle different combinations of text and image prompt? For example, what if we have the triplet description and only the end frame of the generated image? How is this case different from the case where we have the triplet description and only the first frame of the generated image?\n\n2. How is the method able to handle the morph transform even although the pretrained model is rarely trained on videos with morphism since it is not common? More discussion on this would be appreciated.\n\n3. Have the authors try a more detailed caption vs. the simple ones used in the paper? Will that lead to better motion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper describes a method for Text To Video (T2V) generation.\nThe method proposes two extensions: State and Image guidance.\nState Guidance uses state triplets (initial, current, last) to help T2V generate the proper video frame.\nImage guidance injects noise in the early stages of the diffusion model to steer it in the right direction.\nThis is then used to: generate more dynamic video sequences, as well as zero shot video generation from a single image, and a video interpolating between two images."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The extensions are reasonable and the various experiments does show nice videos produced by the system.\n\nIn addition, a new dataset is introduced that, hopefully, will help future contributions to the field."
            },
            "weaknesses": {
                "value": "The paper feels rushed. (The caption of the teaser figure misplaced the text for sub-figures (B) and (C)).\n\nI wonder what is the novelty of the proposed method given the \"Make Pixels Dance\" (CVPR'24) paper.\nThey, too, use a triplet state representation to encourage better video synthesis. \nYet, I could not find a direct comparison. Can the authors explain why?\n\nThere are many results and one must appreciate the work done by the authors, but it is extremely difficult to follow the experimental results and appreciate the contributions. For example, \n\n1. Please add a reference to the different methods shown in the various tables.\n2. I'm not sure the ablation experiments should appear in the main text. \n3. The supplemental material is difficult to navigate. There are many folders and no easy way to navigate and compare the different results presented there.\n4. Table 3: The boldface numbers are confusing as they only refer to the method without SG. Yet, in almost each column there is a better alternative, so it's difficult to judge the overall quality of the results.\n5. Table 5: It is confusing to compare VC2+IG with three thresholds to TI2V-Zero and then highlight in bold different measures for different thresholds."
            },
            "questions": {
                "value": "Can the authors elaborate on the comparison to \"Make Pixels Dance\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}