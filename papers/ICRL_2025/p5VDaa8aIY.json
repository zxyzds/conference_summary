{
    "id": "p5VDaa8aIY",
    "title": "Small Molecule Optimization with Large Language Models",
    "abstract": "Recent advancements in large language models (LLMs) have opened new possibilities for generative molecular drug design. In molecular optimization, LLMs are promising candidates to augment traditional modeling and rule-based approaches for refining molecular structures toward design criteria. We present a novel approach to molecular optimization using LLMs trained on a hand-crafted corpus of over 100 million molecules and their properties. We trained three new models, Chemlactica-125M, Chemlactica-1.3B, and Chemma-2B, with a demonstrated ability to generate molecules with specified properties and learn new molecular characteristics from limited samples, competitive with the state-of-the-art (SOTA) in property prediction tasks on experimental data. Our optimization method, elucidated by these capabilities, combines the models' generative power with concepts from prompt optimization, evolutionary algorithms, and rejection sampling to solve molecular optimization problems more efficiently. The approach surpasses previous SOTA results on the Practical Molecular Optimization (PMO) benchmark and exceeds or is competitive with the SOTA in multi-property optimization tasks involving docking simulations. We release the training data, language models, and optimization algorithm to facilitate further research and reproducibility.",
    "keywords": [
        "Large Language Models",
        "Molecule Generation",
        "Transfer Learning",
        "Drug Discovery",
        "Evolutionary Algorithms"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=p5VDaa8aIY",
    "pdf_link": "https://openreview.net/pdf?id=p5VDaa8aIY",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a novel approach to molecular optimization for drug discovery by leveraging the large language models. The contribution are included as follows, LLM-based molecular design, new molecular corpus, optimization algorithm and this paper also claims achieves the state of art performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The strengths of the paper lie in several key areas.\n1. comprehensive dataset, this authors created a custom molecular corpus with over 100 million molecules from PubChem, incorporating detailed chemical properties. \n2. The combination of LLMs with a genetic algorithm, prompt optimization, and rejection sampling allows the paper\u2019s method to effectively explore chemical space and optimize for multiple properties at once.\n3. Versatility: The models demonstrate adaptability, achieving high performance even with minimal fine-tuning data, which underscores their capability with limited datasets. \n4. Open Access: The authors prioritize reproducibility by openly sharing their training data, models, and optimization algorithms with the research community."
            },
            "weaknesses": {
                "value": "The weakness of this paper includes:\n1. The model only consider the smile representation, it lacks explicit consideration of 3D conformation\n2. The proposed optimization algorithm, while efficient, still relies on a high number of oracle evaluations. This paper could further improve reduce oracle calls, especially for applications where computationally intensive evaluations may be costly.\n3. Limited experimental validation: while the paper demonstrates strong results on computational benchmarks, it would benefit from additional validation in real-world experimental settings, such as testing generated molecules in biological assays.\n4. The performance of the models appears sensitive to hyperparameter choices, especially during the optimization process.\n5. The molecule optimization algorithm lacks of novelty.\n6. Missing references to recent studies."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores small molecule optimization using Large Language Models (LLMs) based on evolutionary strategies. The authors introduce pair tokens to represent molecules and utilize these tokens to express various properties and SMILES strings. They fine-tune a pre-trained model using this token-based approach and perform molecule optimization and property prediction. The study demonstrates state-of-the-art performance in molecule property prediction on specific datasets through supervised fine-tuning. Additionally, the authors propose a novel SMILES generation method inspired by genetic algorithms using their token system. Finally, they present a molecule optimization algorithm based on dynamic fine-tuning, which achieves state-of-the-art results on molecule optimization benchmarks."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The study successfully demonstrates the feasibility of molecule optimization using LLMs and a special token system.\n2. The innovative approach of emulating genetic algorithms through the token system and Chain of Thought reasoning is particularly noteworthy."
            },
            "weaknesses": {
                "value": "### Lack of Computational Efficiency Comparison\n1. The paper does not provide a comparison of overall processing times between methods.\n\n### Exploration of Efficient Training Methods\n1. Have the authors considered more efficient learning methods beyond fine-tuning the entire model?\n2. It would be interesting to explore the effects of techniques such as freezing specific layers, layer skipping, or parameter-efficient fine-tuning.\n\n### Limited Exploration of LLM Capabilities for Multi-Property Optimization (MPO)\n1. Additional experiments demonstrating the potential of LLMs for MPO would be beneficial."
            },
            "questions": {
                "value": "### Lack of Computational Efficiency Comparison\n1. Given that LLM inference can be computationally expensive, it would be valuable to know the time required for molecule optimization. Is it possible to compare this with other models?\n\n### Exploration of Efficient Training Methods\n1. Can you explore the effects of techniques such as freezing specific layers, layer skipping, or parameter-efficient fine-tuning?\n\n### Limited Exploration of LLM Capabilities for Multi-Property Optimization (MPO)\n1. How would the performance change if target properties were enumerated rather than produced as a product?\n\nThese points could be addressed to further strengthen the paper and provide a more comprehensive understanding of the proposed method's capabilities and limitations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces 3 models Chemlactica-125M, Chemlactica-1.3B, and Chemma-2B which fine-tunes Galactica. The new series of models are trained on a custom prepared dataset based on PubChem data. The final models were used for property prediction and molecular optimisation tasks. The models show strong empirical performance, outperforming or matching recent existing methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The authors release the training corpus and model checkpoints\n* Table 1 shows the benefit of transfer learning\n* Property prediction experiments show strong performance\n* Molecular optimisation experiments are thorough and compared to strong baselines \n* The Appendix is detailed and the transparency around hyperparameter tuning, information around floating point precision is interesting"
            },
            "weaknesses": {
                "value": "Generally, descriptions of the model and pre-training are thorough but there are important metrics and discrepancies in the pre-training dataset that should at least be discussed. I will combine the specific points and related questions in the Questions section."
            },
            "questions": {
                "value": "1. From the main text results, Chemlactica-125M, Chemlactica-1.3B, and Chemma-2B show strong empirical performance, matching or outperforming recent strong baselines. It is clear that the models work, but it is unclear how much of the benefit is from the pre-training data itself and leveraging the base pre-trained Galactica. I will focus my discussion here on the molecule optimisation experiments. On the PMO benchmark, the authors outperform all compared models by a wide margin. However, the PMO benchmark was designed with ZINC 250k as the pre-training data. All models in the benchmark were pre-trained with this data. How would the performance differ if all the existing models in PMO were pre-trained with PubChem and/or Chemlactica/Chemma were further pre-trained on ZINC 250k and not PubChem? Based on this, I had also checked the Augmented Memory (ChEMBL) [1], GEAM (ZINC 250k) [2], and Saturn (ZINC 250k) [3] papers cited by the authors and their pre-training data. There is existing literature suggesting that changing from ChEMBL to PubChem pre-training data alone improves performance considerably [4]. Pre-training data is expected to have a notable impact on optimisation performance since the models fit this data. While I can appreciate that part of the point of the models released by the authors is leveraging \"big data\", an ablation on fine-tuning Galactica with just ZINC 250k and/or ChEMBL and/or pre-training the comparison models with PubChem would enable a more thorough understanding of which component of the proposed models leads to the biggest performance improvement.\n\n2. Table 9 shows high variability and seemingly unpredictable behavior when tuning for conditional generation. This section was shown for molecular weight but is this behavior also observed for other tasks? The authors state that the optimal hyperparameters are the same across the models but is this consistent with other tasks? Molecular weight is inexpensive to compute and if this tuning behavior is very sporadic, it might be difficult to control the behavior with other more expensive properties. It would be informative to show the tuning statistics on other properties.\n\n3. Figure 11 shows the docking scores of molecules for the DRD2 experiment. It looks like there is high variance even towards the end of the run.  An advantage of generative models that are fine-tuned is focused modeling of a good distribution. I would have expected that the variance decreases as good molecules are found since the model is being fine-tuned with only the best molecules. Is this variability present on the other docking targets? I can appreciate that the models generate molecules with good docking scores and that this does not take away from that fact, but I am interested in hearing the author's thoughts on potential reasons for the variability. A potentially interesting experiment would be to purposely fine-tune the models with sets of very similar molecules. Does this still lead to high variability?\n\n4. In Table 5/6, GEAM reports diversity with #Circles [5]. Do the authors also have these metrics?\n\n5. Are there statistics on how many times fine-tuning was performed and how long this takes?\n\n6. How much memory and time does it take to deploy and inference the models?\n\n7. Minor comment: typo in number of valid candidate molecules 10e60?\n\nOverall, the transparency in the paper and the strong empirical performance are positive points. The main questions I have are centered around how much benefit is from the pre-training data compared to the specific workflow introduced (Galactica fine-tuning on PubChem-derived dataset). I am happy to engage in discussions with the authors.\n\n\n[1] Augmented Memory: https://pubs.acs.org/doi/10.1021/jacsau.4c00066\n\n[2] GEAM: https://arxiv.org/abs/2310.00841\n\n[3] Saturn: https://arxiv.org/abs/2405.17066\n\n[4] REINVENT with Transformer: https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00887-0\n\n[5] #Circles: https://openreview.net/forum?id=Yo06F8kfMa1"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a unique framework for optimizing small molecules using large language models (LLMs), specifically Chemlactica-125M, Chemlactica-1.3B, and Chemma-2B. These models, trained on over 100 million molecules extracted from PubChem, offer both generative and predictive capabilities tailored to molecular property prediction tasks. The primary contributions are as follows:\n- Development of a Specialized Molecular Dataset: The authors construct a dataset rich in molecular properties, incorporating known structures, experimental properties, and optimized SMILES-based representations.\n- Novel Optimization Algorithm: The paper introduces an innovative molecular optimization framework combining LLM-based generation with evolutionary strategies, specifically integrating genetic algorithms and prompt optimization techniques to enhance the molecular design pipeline.\n- Benchmark Performance and Evaluation: By assessing performance on tasks such as Practical Molecular Optimization (PMO) and docking-based multi-property objectives, the authors claim state-of-the-art (SOTA) results across several key metrics, such as sample efficiency and the generative yield of viable molecules."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Comprehensive Experimental Validation: The authors present a well-rounded suite of experiments, rigorously evaluating their models on various molecular design benchmarks, such as PMO, and providing comprehensive comparisons to current SOTA techniques across a diverse range of metrics.\n- Innovative Optimization Framework: The integration of evolutionary strategies within a language-model-driven molecular generation pipeline is novel, blending genetic search concepts with LLM-specific prompt engineering. This hybridization is particularly suited to address the combinatorial complexity of chemical space."
            },
            "weaknesses": {
                "value": "- Inclusion of Baseline Comparison with Similar Methods:\nThe paper would benefit from a direct comparison with Wang et al.\u2019s 2024 study, \"Efficient Evolutionary Search over Chemical Space with Large Language Models,\" as this work also applies genetic algorithms with LLMs, albeit without fine-tuning. Since the authors acknowledge Wang et al.\u2019s work as the most similar, a side-by-side comparison would strengthen the argument for the advantages of the current approach and illustrate any tangible benefits from fine-tuning.\n\n- Model Selection and Justification:\nWhile Chemlactica and Chemma are based on Galactica and Gemma models, it remains unclear why these were chosen over other more established and widely benchmarked models like Llama or GPT. Galactica and Gemma are comparatively limited in LLM applications, so examining the performance difference with a model like Llama or GPT, which are better validated, would be beneficial. This could help address concerns about model architecture suitability and provide insights into optimizing architectures for molecular tasks.\n\n- Risk of Data Leakage and Benchmark Validity:\nA significant limitation arises from the potential data leakage inherent in using PubChem-derived training data. Given that PMO and docking benchmarks aim to rediscover known drugs as a proxy for drug discovery, these molecules may already exist within PubChem. This overlap risks inflating performance metrics by providing the model with information it may have encountered during training. A clearer methodology or additional validation on a benchmark explicitly excluding known molecules from PubChem could address this confounder and validate the robustness of the framework.\n\n- Task Selection and Generalizability:\nThe paper reports results on a subset of benchmark tasks (5 of 23 in PMO and 3 of 17 in MoleculeNet). This limited selection raises questions about the generalizability of the results, especially given that the ADMET prediction results in the appendix were less successful. Expanding the evaluation across additional tasks, or clarifying the criteria for task selection, could help establish confidence in the model\u2019s consistency and overall applicability to various molecular optimization challenges.\n\n### Reference\n[1] Wang, Haorui, et al. \"Efficient evolutionary search over chemical space with large language models.\" arXiv preprint arXiv:2406.16976 (2024)."
            },
            "questions": {
                "value": "How does the method perform on tasks beyond what is shown in the main text?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}