{
    "id": "hlijRgXTDK",
    "title": "Pathologies of Out-of-Distribution Detection",
    "abstract": "There is a proliferation of out-of-distribution (OOD) detection methods in deep learning which aim to detect distribution shifts and improve model safety. These methods often rely on supervised learning to train models with in-distribution data and then use the models\u2019 predictive uncertainty or features to identify OOD points. In this paper, we critically re-examine this popular family of OOD detection procedures, revealing deep-seated pathologies. In contrast to prior work, we argue that these procedures are fundamentally answering the wrong question for OOD detection, with no easy fix. Uncertainty-based methods incorrectly conflate high uncertainty with being OOD, and feature-based methods incorrectly conflate far feature-space distance with being OOD. Moreover, there is no reason\nto expect a classifier trained only on in-distribution classes to be able to identify OOD points; for example, we should not necessarily expect a cat-dog classifier to be uncertain about the label of an airplane, which may share features with a cat that help distinguish cats from dogs, despite generally appearing nothing alike. We show how these pathologies manifest as irreducible errors in OOD detection and identify common settings where these methods are ineffective. Additionally, interventions to improve OOD detection such as feature-logit hybrid methods, scaling of model and data size, Bayesian (epistemic) uncertainty representation, and outlier exposure also fail to address the fundamental misspecification.",
    "keywords": [
        "Out-of-distribution detection",
        "robustness"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "We examine the premise of OOD detection and expose deep-seated pathologies and failure modes.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=hlijRgXTDK",
    "pdf_link": "https://openreview.net/pdf?id=hlijRgXTDK",
    "comments": [
        {
            "comment": {
                "value": "Thank you for your review. We would like to clarify a few important misunderstandings.\n\nWhile many OOD detection methods are effective for smaller datasets like CIFAR-10, our paper demonstrates why these fundamental limitations prevent robust detection on more challenging problems like ImageNet-1k. Yang et al. [1] benchmark the performance of many SoTA OOD detection methods such as ViM, Max-Cosine, KNN, ASH-B, ReAct, etc, and find that the best of these methods only achieve an AUROC of 0.807 on ImageNet-1k vs ImageNet-OOD, with some methods scoring an AUROC of 0.593. These methods perform even more poorly under covariate shift, often achieving worse than random chance. To demonstrate that OOD detection cannot easily be solved by scaling up model and data size, we show in Figure 4 that even a ViT-G/14 DINOv2 pre-trained on internet-scale data, indicated by the rightmost points, still has significant irreducible error. Therefore, OOD detection methods still have considerable room for improvement, and optimal performance can only be achieved by rethinking the approach to OOD detection and moving away from using supervised models which are mis-specified for the problem.\n\n[1] Yang et al, ImageNet-OOD: Deciphering Modern Out-of-Distribution Detection Algorithms\n\nWe would also like to clarify that our claims in the paper do not rely on empirical examples; instead, we provide a conceptual understanding of the fundamental limitations of how these methods operate, and we use illustrative examples to show how these conceptual pathologies lead to suboptimal performance in practice. Furthermore, we actually do provide extensive benchmarks to support each of our claims:\n- To demonstrate the pathologies of feature-based methods (section 4.1), we provide experiment results using four different models: ResNet-18, ResNet-50, ViT-S/16, and ViT-B/16. We use ImageNet-1k as our ID data, and we test on three different OOD datasets: ImageNet-OOD, Textures, and iNaturalist. We also use a ResNet-18 trained on CIFAR-10 for visualization purposes, since smaller feature dimensions and fewer classes lead to clearer 2D plots; furthermore, we also visualize failure modes for ResNet-50 on ImageNet-1k.\n- To understand how these feature-based pathologies scale with model and dataset size (Section 5.1 and Figure 4), we benchmark 12 different models of varying sizes and training methods, as listed in Appendix B.3.\n- For pathologies of logit-based methods (Section 4.2 and Figure 3), we show results for ResNet-18, ResNet-50, ViT-S/16, ViT-B/16, trained on ImageNet, and ViT-G/14 trained with DINOv2. We also use a LeNet-5 trained on CIFAR-10 for visualization purposes.\n- To understand the scaling behaviors of logit-based methods (Section A.4 and Figure A.7), we evaluate 54 models from timm and torchvision, with 9 different architectures and 6 different training/pre-training setups, listed in Appendix B.2.\n\nWhile these experimental setups were included in the original paper, we have revised the text to highlight the diversity of the models and datasets we benchmarked. We hope this addresses your concerns about generalizability.\n\nWe do not propose yet another method to benchmark; instead, our work explicitly identifies the problem misspecification of the existing approaches which utilize supervised models trained on ID data. We do find that this misspecification is slightly alleviated when the model is first pretrained on very diverse datasets, as the model is then incentivized to learn robust distinguishing features. As such, we encourage practitioners to think more deeply about the data that the model has been exposed to, and we recommend using datasets that are as large and diverse as possible for general OOD detection. However, the goals remain fundamentally unaligned and there are still irreducible sources of error from irrelevant features as shown in Figure 4, so we conclude that more work on OOD detection is needed to develop procedures which directly address the problem. \n\nPrescriptive measures are not necessary for a paper to have scientific merit, and we believe that deepening understanding, critically examining limitations, and prompting further research are also important contributions. Many papers of this variety have been very successful at ICLR, ICML, and NeurIPS. To name a few:\\\n\u201cThe marginal value of adaptive gradient methods in machine learning\u201d (NeurIPS 2017, 1278 citations)\\\n\u201cUnderstanding deep learning requires rethinking generalization\u201d (ICLR 2016 best paper)\\\n\u201cHow Good is the Bayes Posterior in Deep Neural Networks Really?\u201d (ICML 2020, 373 citations)\\\n\u201cBayesian Model Selection, the Marginal Likelihood, and Generalization\u201d (ICML 2022 best paper)\\\n\u201cIf Influence Functions are the Answer, Then What is the Question?\u201d (NeurIPS 2022)\n\nWe would appreciate it if you would consider re-evaluating our paper in light of these clarifications, and we would be happy to answer any further questions."
            }
        },
        {
            "title": {
                "value": "Author Response to 7Rbf"
            },
            "comment": {
                "value": "Thank you for your review! We are glad that you recognize our work \u201cgoes beyond incremental improvements,\" and we address your concerns below.\n\nWe explore the impacts of scaling model size and pre-training data in Figure 4 and Figure A.7. We see in Figure 4 that even for the rightmost point representing ViT-G/14 DINOv2 pre-trained on internet-scale data, there is still significant irreducible error (the difference between the perfect performance of 1.0 and the purple triangle) for feature-based methods. This irreducible error can be further broken down into \u201cindistinguishable features\u201d (the difference between perfect 1.0 and the green star), as well as \u201cirrelevant features\u201d (the gap between the blue triangle and the purple triangle). These results demonstrate that the crux of the pathology remains unchanged even for SoTA models and pre-training procedures, although we do see that the larger models do have fewer instances of indistinguishable features compared to smaller ones.\n\nWhile there have been many OOD detection methods which do not purely rely on uncertainty through MSP, we show that logit-based methods also suffer from the pathologies we described. In Figure A.5, we demonstrate that max-logit, energy score, and entropy all fall victim to the same failure mode where the OOD scores for ID and OOD data entirely overlap. Because a supervised model trained only on ID data is not incentivized towards any behavior for OOD data, there is no guarantee that the logits can be used to distinguish ID and OOD.\n\nFurthermore, in Section 5.2, we also explore SoTA methods, like ViM, which combine feature and logit-based methods to compute the OOD score. We find that these hybrid methods also fail to address the fundamental pathologies caused by the model misspecification. For instance, in the cases where ID and OOD features are indistinguishable, hybrid methods would not be able to provide any benefits.\n\nWe hope this has addressed your concerns, and we are happy to engage with any further questions."
            }
        },
        {
            "title": {
                "value": "Author Response to NHyc"
            },
            "comment": {
                "value": "Thank you for your review and references. We have updated the text to fix the minor issues you pointed out, and we address your concerns below.\n\nOur paper focuses on the problem of OOD detection of semantic shift, where new and unseen classes appear at test-time. We specifically address methods which use the features or logits of supervised models to differentiate between ID and OOD data. Therefore, while problems like OOD generalization for covariate shifts addressed by Teney et al [1] are closely related, they are outside the scope of this paper.\n\n[1] Teney et al, ID and OOD Performance Are Sometimes Inversely Correlated on Real-world Datasets\n\nWe demonstrate significant research contributions by being the first to explore why entire families of OOD detection approaches, such as logit-based and feature-based methods, are fundamentally flawed. Because there are many works which criticize specific OOD detection procedures, it may seem that our work lacks novelty. However, many SoTA OOD detection methods focus on adaptations within these existing families, often implying the opposing stance that improved methodology can address these innate limitations. \n\nFor instance, in the prior work you referenced by Tajwar et al [2], the authors do address limitations of feature and logit-based methods. However, they focus on understanding the tradeoffs between the methods, and they propose a new feature-based method for low-data regimes, suggesting that choosing the best feature or logit-based method is sufficient.  In contrast, we discover that even these novel methods, or any other logit-based or feature-based methods, will have fundamental limitations that arise from the misspecification of supervised training. Moreover, their results utilize toy datasets and low-resolution datasets like CIFAR-10, while we demonstrate that these pathologies exist even with sophisticated models and large datasets. Thank you for the reference; we have added this discussion to our paper. Other OOD detection papers [3, 4, 5] attribute the failures of OOD detection to a weakness in the specific method and propose a novel method within the same family of approaches. No prior work has explored the inherent pathologies of OOD detection methods which cannot be mitigated by improved methodology or method selection. \n\n[2] Tajwar et al, No True State-of-the-Art? OOD Detection Methods are Inconsistent across Datasets\\\n[3] Sun et al, Out-of-Distribution Detection with Deep Nearest Neighbors\\\n[4] Wei et al, Mitigating Neural Network Overconfidence with Logit Normalization\\\n[5] Wang et al, ViM: Out-Of-Distribution with Virtual-logit Matching\\\n\nTo this extent, we provide many research contributions which demonstrate the limitations of these methods. We demonstrate the existence of irreducible error for all feature-space methods regardless of methodology, which was previously unexplored and unquantified. Furthermore, we illustrate that this error does not disappear even as we increase model size and data size, disapproving the effectiveness of a commonly proposed fix for OOD detection. Similarly, we also identify sources of irreducible error for logit-based methods, even at scale. We further illuminate the pathologies of potential remedies such as outlier exposure, Bayesian inference, introducing an OOD class, and generative modeling. Although our findings may not seem surprising at first glance (e.g. it seems obvious that feature-based methods will not work for indistinguishable features), our paper provides the important contribution of understanding the extent to which these pathologies occur, even for large models trained on internet-scale data.\n\nPrescriptive measures are not necessary for a paper to have scientific merit, and we believe that deepening understanding, critically examining limitations, and prompting further research are also important contributions. Many papers of this variety have been very successful at ICLR, ICML, and NeurIPS. To name a few:\\\n\u201cThe marginal value of adaptive gradient methods in machine learning\u201d (NeurIPS 2017, 1278 citations)\\\n\u201cUnderstanding deep learning requires rethinking generalization\u201d (ICLR 2016 best paper)\\\n\u201cHow Good is the Bayes Posterior in Deep Neural Networks Really?\u201d (ICML 2020, 373 citations)\\\n\u201cBayesian Model Selection, the Marginal Likelihood, and Generalization\u201d (ICML 2022 best paper)\\\n\u201cIf Influence Functions are the Answer, Then What is the Question?\u201d (NeurIPS 2022)\n\nWe hope this has addressed your concerns, and we are happy to engage with any further questions."
            }
        },
        {
            "title": {
                "value": "Author Response to K5mb"
            },
            "comment": {
                "value": "Thank you for your review. We would like to clarify a few important misunderstandings.\n\nBecause there are many works which in some way appear critical of OOD detection procedures, it may seem, superficially, as if our work, which is also critical of OOD detection, lacks novelty. But, diving beneath the surface, this characterization could not be further from the truth. These prior works are focused on adaptations of a family of approaches for better benchmark numbers, not on explaining why a paradigm for OOD detection is fundamentally flawed. For instance, there has been no prior analysis on the inevitable existence of indistinguishable and irrelevant features for feature-based methods, and many papers assume OOD examples will be far from the data without further exploration [1, 2, 3]. Therefore, although some of our findings may not seem surprising at first glance (e.g. it seems obvious that feature-based methods will not work for indistinguishable features), our paper provides the important contribution of understanding the extent to which these pathologies occur, even for large models trained on internet-scale data.\n\n[1] Sun et al, Out-of-Distribution Detection with Deep Nearest Neighbors\\\n[2] Ming et al, How to Exploit Hyperspherical Embeddings for Out-of-Distribution Detection?\\\n[3] Park et al, Understanding the Feature Norm for Out-of-Distribution Detection\n\nFurthermore, our claims in the paper do not rely on empirical examples; instead, we provide a conceptual understanding of the fundamental limitations of how these methods operate, and we use illustrative examples to show how these conceptual pathologies lead to suboptimal performance in practice. For instance, our section about generative models uses a simple example to explicitly demonstrate how the goals of generative modeling are fundamentally misaligned with the goals of OOD detection; therefore, merely improving the quality of the generative model does nothing to solve the pathology. Furthermore, we actually do provide extensive benchmarks to support each of our claims in the paper:\n- To demonstrate the pathologies of feature-based methods (Section 4.1 and Figure 2), we provide experiment results using four different models: ResNet-18, ResNet-50, ViT-S/16, and ViT-B/16. We use ImageNet-1k as our ID data, and we test on three different OOD datasets: ImageNet-OOD, Textures, and iNaturalist. We also use a ResNet-18 trained on CIFAR-10 for visualization purposes, since smaller feature dimensions and fewer classes lead to clearer plots when we only have 2D images; however, we also visualize failure modes for ResNet-50 and ImageNet-1k.\n- To understand how these feature-based pathologies scale with model and dataset size (Section 5.1 and Figure 4), we benchmark 12 different models of varying sizes and training methods, as listed in Appendix B.3.\n- For pathologies of logit-based methods (Section 4.2 and Figure 3), we show results for ResNet-18, ResNet-50, ViT-S/16, ViT-B/16, trained on ImageNet, and ViT-G/14 trained with DINOv2. We also use a LeNet-5 trained on CIFAR-10 for visualization purposes.\n- To understand the scaling behaviors of logit-based methods (Section A.4 and Figure A.7), we evaluate 54 models from timm and torchvision, with 9 different architectures and 6 different training/pre-training setups, listed in Appendix B.2.\n- For pathologies of generative models (Section 5.6 and Appendix A.5), we use normalizing flows of eight different sizes trained on CelebA. We also use Diffusion Transformers trained on ImageNet-1k to demonstrate the same pathologies.\n\nWhile these experimental setups were included in the original paper, we have revised the text to highlight the diversity of the models and datasets we benchmarked. We hope this addresses your concerns about scientific rigor.\n\nAlthough we demonstrate the impacts of model size, architecture, and training method throughout the paper with the experiments listed above, and we explicitly analyze the effect of pre-training in Appendix A.4, one of our key claims is that these pathologies cannot be resolved by simply improving the model or the training procedure. We see in Figure 4 that even for a ViT-G/14 DINOv2 pre-trained on internet-scale data, there is still significant irreducible error (the difference between the perfect performance of 1.0 and the purple triangle) for feature-based methods. This result demonstrates that the crux of the pathology remains unchanged even for SoTA models and pre-training procedures.\n\nWe would appreciate it if you would consider re-evaluating our paper in light of these clarifications, and we would be happy to engage with any further questions."
            }
        },
        {
            "title": {
                "value": "General Response"
            },
            "comment": {
                "value": "We would like to thank all of the reviewers for their feedback. We present a general response to the reviewers here, and we also address individual reviewers in separate posts below. \n\nOOD detection has been a vital component of AI safety and robustness, and many methods have been developed to address this problem. In our work, we are the first to highlight the fundamental pathologies of entire families of popular approaches to OOD detection, such as methods which utilize the features or logits of supervised models. We demonstrate that these approaches do not directly address OOD detection but instead answer entirely different questions, and we provide concrete failure modes across a diverse set of models and datasets. We do not propose yet another method which claims to improve OOD performance while following fundamentally pathological paradigms; instead, our research contribution lies in our novel demonstrations of the irresolvable failure modes when using supervised models trained on in-distribution data to detect out-of-distribution data.\n\nBecause there are many works which criticize specific OOD detection procedures, it may seem, superficially, as if our work lacks novelty. However, many SoTA OOD detection methods focus on adaptations within these existing families, often implying the opposing stance that improved methodology can address these innate limitations. For example, Sun et al [1] attribute the problem with feature-based approaches to the distributional parameterization and thus propose a feature-based approach. Wei et al [2] attribute the problem with logit-based approaches to model overconfidence and thus propose a novel logit-based approach. Wang et al [3] state that the performances of logit-based and feature-based methods \u201care limited by the singleness of their information source\u201d and thus propose a novel hybrid approach. In contrast, we argue that even these novel methods, or any other logit-based or feature-based methods, still have fundamental limitations that arise from the misspecification of supervised training. No prior work has explored the inherent pathologies of OOD detection methods which cannot be mitigated by improved methodology.\n\nWe would also like to clarify that we have extensive experiments over a wide variety of models and datasets, and our results are not limited to toy examples. In fact, our paper includes empirical results for hundreds of combinations of model architectures, training procedures, and ID and OOD datasets.\n- To demonstrate the pathologies of feature-based methods (section 4.1), we provide experiment results using four different models: ResNet-18, ResNet-50, ViT-S/16, and ViT-B/16. We use ImageNet-1k as our ID data, and we test on three different OOD datasets: ImageNet-OOD, Textures, and iNaturalist. We also use a ResNet-18 trained on CIFAR-10 for visualization purposes, since smaller feature dimensions and fewer classes lead to clearer 2D plots; furthermore, we also visualize failure modes for ResNet-50 on ImageNet-1k.\n- To understand how these feature-based pathologies scale with model and dataset size (Section 5.1), we benchmark 12 different models of varying sizes and training methods, as listed in Appendix B.3.\n- For pathologies of logit-based methods (Section 4.2), we show results for ResNet-18, ResNet-50, ViT-S/16, ViT-B/16, trained on ImageNet, and ViT-G/14 trained with DINOv2. We also use a LeNet-5 trained on CIFAR-10 for visualization.\n- To understand the scaling behaviors of logit-based methods, we evaluate 54 models from timm and torchvision, with 9 different architectures and 6 different pre-training setups, listed in Appendix B.2.\n\nWhile these experimental setups were included in the original paper, we have revised the text to highlight the diverse set of models and datasets benchmarked. \n\nEven though it has become popular to use out-of-the-box supervised predictive models for OOD detection, we make the case that this family of approaches is fundamentally misspecified for this purpose. Due to the profound significance of this undertaking and the broad relevance of our work, we believe our paper provides an urgent and needed contribution to the community. We hope these points, and our responses, can be accounted for in the final assessment, and that reviewers may be open minded about revising their understanding.\n\n[1] Sun et al, Out-of-Distribution Detection with Deep Nearest Neighbors\\\n[2] Wei et al, Mitigating Neural Network Overconfidence with Logit Normalization\\\n[3] Wang et al, ViM: Out-Of-Distribution with Virtual-logit Matching\\"
            }
        },
        {
            "summary": {
                "value": "The paper presents a critical analysis on prior approaches for out-of-distribution (OOD) detection. The central argument of the paper is that prior approaches cannot detect certain categories of OOD samples, due to limited information regarding OOD samples at test time. The paper demonstrates failure modes for common categories of OOD detection methods such as feature-based, logit-based, and uncertainty-based methods, with concrete empirical evidence and illustrations."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper is overall well written with clear figures and illustrations.\n2. The failure modes for different categories of OOD detection methods are clearly demonstrated with examples."
            },
            "weaknesses": {
                "value": "1. **Limited New Insights and Lack of Technical Depth**: The paper falls short in presenting new insights and lacks significant technical depth. For example:\n\n- Section 4.1: The paper states that \u201cno feature-based method can correctly detect these OOD inputs that have indistinguishable features from ID\u201d (L217) and demonstrates this claim with examples. However, this observation is already well-established in the literature and is almost self-evident given the definition of feature-based OOD detection. Instead of demonstrating the existence of OOD samples that share features with ID (which is an expected finding), a deeper analysis of the root causes\u2014such as the training method, model architecture, or the impact of pre-training\u2014would provide more value.\n- Section 4.2: The claim that \u201cOOD examples often have low uncertainty\u201d is supported by an experiment using only LeNet-5 to classify automobiles and trucks from CIFAR-10. The limited scope of this experiment is insufficient to support a generalized conclusion that OOD examples often exhibit low uncertainty.\n- Section 5.6: The failure mode of generative models is demonstrated solely with a simplified two-class Gaussian example and lacks accompanying experiments. The relevance of this simple demonstration to recent advancements in generative models remains unclear.\n\n2. **Lack of Scientific Rigor**: The paper lacks scientific rigor in most of its experimental sections. Each section follows a similar format where a failure mode is claimed (often an obvious one) and then demonstrated with a basic experiment (e.g., \u201cTo demonstrate feature overlap, we train a ResNet-18 on a subset of CIFAR-10 classes,\u201d L247). These conclusions are drawn from experiments involving only a single model and dataset, leaving open questions about the generalizability of the findings across different training and evaluation settings.\n\n\nDespite these issues, I acknowledge the educational value the paper provides by illustrating failure cases across different categories of OOD detection methods. I recommend that the authors consider submitting this work to a blog post track, where it could serve as a useful resource for educational purposes."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper argues that OOD detection is fundamentally misspecified. It provides a review of\nexisting OOD techniques and conducts mini-experiments to demonstrate that each suffers\nfrom fundamental flaws arising from the way OOD detection is framed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "A substantive assessment of the strengths of the paper, touching on each of the following\ndimensions: originality, quality, clarity, and significance. We\nencourage reviewers to be broad in their definitions of originality and significance. For\nexample, originality may arise from a new definition or problem\nformulation, creative combinations of existing ideas, application to a new domain, or\nremoving limitations from prior results. You can incorporate Markdown\nand Latex into your review. See https://openreview.net/faq (https://openreview.net/faq).\n\nThe paper is well presented, easy to follow, and makes a well argued case. The authors\nconduct mini-experiments to demonstrate pathologies across a wide range of OOD methods\nand intend to release code publicly to reproduce examples. The topic is relevant to the\ncommunity and raises awareness of the need to clarify the framing and purpose of OOD\ndetection."
            },
            "weaknesses": {
                "value": "A substantive assessment of the weaknesses of the paper. Focus on constructive and\nactionable insights on how the work could improve towards its stated\ngoals. Be specific, avoid generic remarks. For example, if you believe the contribution lacks\nnovelty, provide references and an explanation as evidence; if you\nbelieve experiments are insu&quot;cient, explain why and exactly what is missing, etc.\n\nWhile the paper is well presented and provides a compelling argument, it&#39;s not clear what\nthe new research contribution is, and as such I cannot recommend it for acceptance in the\nmain research track at ICLR. I&#39;d encourage submitting as a review article to a journal or as a\nposition paper to a workshop, but without a significant research contribution I can&#39;t\nrecommend it for the main research track.\nAdditional related work:\n* Fahim Tajwar, Ananya Kumar, Sang Michael Xie, Percy Liang, &quot;No True State-of-the-Art?\nOOD Detection Methods are Inconsistent across Datasets&quot; 2021\nhttps://arxiv.org/abs/2109.05554\n* Damien Teney, Yong Lin, Seong Joon Oh, Ehsan Abbasnejad &quot;ID and OOD Performance\nAre Sometimes Inversely Correlated on Real-world Datasets&quot; NeurIPS 2023\nhttps://arxiv.org/pdf/2209.00613\nMinor:\n* Figure 1 doesn&#39;t seem to be referenced in the main text\n* Line 89: If taking arg max of a function with respect to K, then I&#39;d expect K to appear\nsomewhere in the function. I believe the arg max should be with respect to index i \u2208 {1, .., K}\nand the index shown in the function as either y_i or y = i.\n* line 99: features or features -&gt; features of features?\n* Line 344: OO -&gt; OOD"
            },
            "questions": {
                "value": "Are the authors claiming a new finding or novel contribution in relation to any specific\nexperiment, or are these primarily intended to demonstrate known limitations as evidence to\nsupport the paper&#39;s main argument?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Authors claim that current OOD detection methods, often based on supervised models and predictive uncertainty, are fundamentally misguided, as they attempt to answer the wrong questions about whether a sample truly belongs to the training distribution.\n\nCurrently, common OOD detection methods involve training a supervised model on in-distribution data and subsequently using the model\u2019s predictive uncertainty or features to detect OOD instances. These approaches are typically divided into two main streams\n(1) Feature-based methods and (2) Logit-based methods.\n\nHowever, authors argue that these methods inherently answer the wrong question because they do not measure distribution membership but rather whether a sample leads to unexpected model representations. First, feature-based methods fail when features of OOD and in-distribution data overlap or are indistinguishable, leading to irreducible errors in OOD detection. Second, logit-based methods with high label uncertainty does not reliably indicate OOD samples, as in-distribution samples with high label ambiguity may appear as OOD. This conflation leads to missed detections or incorrect OOD classifications.\n\nAlso, alternative attempts to improve OOD detection through scaling, hybrid models, or outlier exposure fall short, as these interventions still rely on the flawed assumptions underlying feature- and logit-based detection.\n(1) Scaling model size: Increasing model and dataset size does not fundamentally address the limitations in feature separation for OOD detection.\n(2) Hybrid methods: Combining features and logits may show slight improvements but does not resolve the underlying misalignment with true OOD detection which adressed above.\n(3) Outlier Exposure: Training with outlier samples to simulate OOD data may improve detection but reduces generalization performance on covariate shifts, highlighting a trade-off between detection and generalization.\n(4) Methods based on epistemic (Bayesian) uncertainty, which should theoretically improve with increased data, fail as they conflate uncertainty about the model\u2019s knowledge with uncertainty over whether a sample is OOD. \n(5) Generative models, which compute likelihoods of training data, often fail in OOD detection due to their inability to differentiate low-likelihood in-distribution data from high-likelihood OOD data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper tried to adress its critical examination of the conceptual limitations of widely used OOD detection methods. By challenging the assumptions underlying feature- and logit-based OOD techniques, the authors reveal fundamental pathologies\u2014such as the conflation of high uncertainty with out-of-distribution status. This critique goes beyond incremental improvements and instead questions the foundational approach, which could reshape the field\u2019s direction. Overall the paper is well written in clear presentations with a topic should be high interest in the field."
            },
            "weaknesses": {
                "value": "Although, I agree that there is a fundamental pathologies in OOD detection which mentioned in the strength of the paper, I still have some concerns regarding the paper.\n\n1. OOD features that are indistinguishable from ID features may due to the small model representation space. Larger models with various classes may learn detailed representation that can distinguish.\n\n2. Recent OOD methods have devided definition of OOD score and uncertainty. It might seem simmilar, but OOD scores are for detecting whether the input is OOD or ID, not serving as the label is certain or not. This refers there can be a sample with low label uncertainty with high OOD scores (and this is the reason that most SOTA ood detection methods outperform MSP in tables)."
            },
            "questions": {
                "value": "Interesting paper with nice contributions. You may want to address the weaknesses I listed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper critically re-examines the popular family of OOD detection procedures, which rely on supervised learning to train models with in-distribution data and then use the models\u2019 predictive uncertainty or features to identify OOD points. The analysis reveals deep-seated pathologies. It argues that these procedures are fundamentally answering the wrong question for OOD detection, with no easy fix. Uncertainty-based methods incorrectly conflate high uncertainty with being OOD, and feature-based methods incorrectly conflate far feature-space distance with being OOD. Moreover, there is no reason to expect a classifier trained only on in-distribution classes to be able to identify OOD points. It shows how these pathologies manifest as irreducible errors in OOD detection and identifies common settings where these methods are ineffective. Additionally, it shows that interventions to improve OOD detection such as feature-logit hybrid methods, scaling of model and data size, Bayesian uncertainty representation, and outlier exposure also fail to address the fundamental misspecification."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. This paper introduces a fresh perspective by highlighting that many popular OOD detection methods \u2013 whether supervised or generative \u2013 are not effectively addressing the core question: Is this unlabeled point from a different distribution? \n\n2. This paper provides concrete examples illustrating how widely-used OOD detection techniques fundamentally miss the mark for true OOD detection. The paper further demonstrates that both feature-based and logit-based methods inherently suffer from irreducible errors, limiting their effectiveness in this context.\n\n3. This paper is well-structured, with clear explanations, making it easy to follow."
            },
            "weaknesses": {
                "value": "1. While the paper argues that popular OOD detection methods do not directly address the question of whether an unlabeled point is from a different distribution, these methods have demonstrated strong performance across established benchmarks. It remains unclear whether adopting the approach suggested in the paper would lead to meaningful improvements in performance, raising questions about the practical utility of the findings.\n\n2. The experiments are relatively simple and may not sufficiently support the claims. For instance, the study relies on a small-scale experiment using a LeNet-5 model trained to classify automobiles and trucks from CIFAR-10 to argue that OOD examples often exhibit low uncertainty. However, it is unclear whether these findings generalize to more complex architectures, such as ResNet, or to larger, real-world datasets.\n\n3. While the paper provides a critical analysis of existing OOD detection methods, it does not propose new approaches to address the identified limitations or offer concrete suggestions for overcoming the challenges it highlights. This limits the practical contribution and leaves open questions about how to improve OOD detection in light of the paper\u2019s critiques."
            },
            "questions": {
                "value": "1. Given that existing OOD detection methods perform well on benchmarks, how do the authors envision a method that directly addresses the question \u201cIs this unlabeled point from a different distribution?\u201d improving current performance? Could the authors provide further theoretical insights or experimental scenarios to clarify when and why their proposed perspective might offer an advantage?\n\n2. Have the authors considered evaluating their findings with more advanced architectures, such as ResNet, and on larger or more diverse datasets beyond CIFAR-10? Doing so could help demonstrate the generalizability of the claims. \n\n3. While the paper offers a valuable critique of existing methods, have the authors considered proposing potential approaches or heuristics to address the identified limitations? Even if speculative, some guidance or future directions could make the paper\u2019s contributions more actionable for researchers looking to build on the work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}