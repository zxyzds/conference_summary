{
    "id": "QrTvFCa4nX",
    "title": "Interpolating Video-LLMs:  Toward Longer-sequence LMMs in a Training-free Manner",
    "abstract": "Advancements in Large Language Models (LLMs) inspire various strategies for integrating video modalities. \nA key approach is Video-LLMs, which incorporate an optimizable interface linking sophisticated video encoders to LLMs. \nHowever, due to computation and data limitations, these Video-LLMs are typically pre-trained to process only short videos, limiting their broader application for understanding longer video content. Additionally, fine-tuning Video-LLMs to handle longer videos is cost-prohibitive.\nConsequently, it becomes essential to explore the interpolation of Video-LLMs under a completely training-free setting. In this paper, we first identify the primary challenges in interpolating Video-LLMs: (1) the video encoder and modality alignment projector are fixed, preventing the integration of additional frames into Video-LLMs, and (2) the LLM backbone is limited in its content length capabilities, which complicates the processing of an increased number of video tokens.\nTo address these challenges, we propose a specific INTerPolation method for Video-LLMs (INTP-Video-LLMs). We introduce an alternative video token rearrangement technique that circumvents limitations imposed by the fixed video encoder and alignment projector. Furthermore, we introduce a training-free LLM context window extension method to enable Video-LLMs to understand a correspondingly increased number of visual tokens.",
    "keywords": [
        "Video Understanding"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "Obtain a Video-LLM which can process more frames in a totally training-free manner.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=QrTvFCa4nX",
    "pdf_link": "https://openreview.net/pdf?id=QrTvFCa4nX",
    "comments": [
        {
            "summary": {
                "value": "This paper examines the task of adapting a Video-LLM, pre-trained on short videos, to handle long videos without additional training. It identifies key challenges related to the fixed video encoder, modality projector, and the limited context window size of the LLM. To address these issues, the authors propose a video token rearrangement technique and an extension method for the LLM's context window to accommodate a greater number of visual tokens. Additionally, they introduce a KV-cache compression mechanism to minimize memory usage during inference. These innovations enable the proposed INTP-Video-LLaVA model to process videos with up to 32 frames."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. **Innovation and Relevance:** The paper introduces a novel training-free approach to extend the input video length compatibility for Video-LLMs. This contribution is significant and addresses a timely issue in the field. \n\n2. **Clarity and Coherence:** The paper is commendably well-structured and it is easy to follow.\n\n3. **Insightful Analysis:** The paper offers a thorough explanation of the challenges associated with video encoder limitations, LLM context window size constraints, and KV-cache management during inference in current Video-LLMs. This analysis is particularly valuable for the community, providing insights that can guide future research and development."
            },
            "weaknesses": {
                "value": "1. The applicability of the proposed training-free techniques across a range of Video-LLMs is a critical aspect to assess. The paper, however, presents experimental results solely for Video-LLaVA. It is recommended that the authors expand their experiments to include additional Video-LLMs to demonstrate the broader applicability of the techniques.\n\n2. The paper lacks certain crucial experiments. An ablation study examining the impact of the video token rearrangement techniques and the efficacy of every design choice of the RoPE interpolation methods is recommended. Such studies would provide a more comprehensive understanding of the contributions of these specific aspects to the overall performance."
            },
            "questions": {
                "value": "My primary concern is the scope of the experimental section. It is recommended that the authors expand their experimental analysis to encompass a variety of Video-LLMs and conduct ablation studies for each design decision. This would offer a more thorough understanding of the findings for the readers. Please find more details in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Overall, this papre presents INTP, an training-free method aiming to improve video LMMs on longer video evaluation. In general, two strategies are proposed in this paper:\n(1) With a fixed length video encoder, it sends alternative frames to bypass the frame length limit for the video encoder. With this, it allows up to 128 frames as input for the `INTP`-VideoLLaVA.\n(2) It uses off-the-shelf NTK extrapolation to allow the short context LLM to allow more frames.\n\nThough this paper is technically solid, it does not provide enough contribution to this field, and sadly I cannot recommend acceptance to this paper in its current form. Please see the weaknesses part for more details and try to improve it."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. This paper is well-written.\n2. The experiments are abundant and solid, providing results from 8 to 128 frames.\n3. The presentation is well."
            },
            "weaknesses": {
                "value": "1. For performance, the improvement is marginal. Though we should not expect huge improvements on training-free methods, VideoLLaVA is already a very poor performer\n\n2. NTK interpolation is an off-the-shelf strategy for LLM context extension (almost default for most). Therefore, the part discussing rope and NTK seems a bit trivial with limited contributions. Similarly, the strategy for video encoder is SPECIFIC to video models with a video encoder, which is not common for now.\n\n3. As the extension has already gone to 128 frames, I would advise to evaluate on some longer video benchmarks, e.g. VideoMME-Long, MLVU, LongVideoBench. Maybe this may better emphasize the effect of the method."
            },
            "questions": {
                "value": "Please see my weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tackles the problem of long sequence LLMs. The authors point out that the challenge lies in : video encoder and modality alignment projector are fixed, preventing the integration of additional frames into Video-LLMs, and the LLM backbone is limited in its content length capabilities, which complicates the processing of an increased number of video tokens. The authors introduce a video token rearrangement technique that circumvents limitations imposed by the fixed video encoder and alignment projector. Furthermore, a training-free LLM context window extension method is proposed to enable Video-LLMs to understand a correspondingly increased number of visual tokens."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper proposes a video token rearrangement technique that bypasses the restrictions imposed by the fixed video encoder and alignment projector. \n2. A training-free Video-LLM context window extension method is proposed to ensure that the interpolated Video-LLM can handle any number of video frames.\n3. The presentations are good."
            },
            "weaknesses": {
                "value": "1. The chosed baselines are not complete. For example, PLLaVA, Video-LLaMA 2, Flash-VStream are not included.\n2. Table 2 should include the #frames of each model.\n3. Some others works about LLM sequence extension should be discussed and analysised. For example, LongVA: Long Context Transfer from Language to Vision.\n4. Lack some benchmark evaluations: VideoMME, MoVQA, MVBench, etc.\n\n[1] PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning\n[2] VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs\n[3] Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams\n[4] MoVQA: A Benchmark of Versatile Question-Answering for Long-Form Movie Understanding."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a interpolation method for extending Video-LLMs to process longer video sequences under training-free setting, called INTP-Video-LLMs. The approach leverages a video token rearrangement technique and a training-free LLM context window extension method to bypass the limitations of existing Video-LLMs, which typically only process short video clips. Furthermore, a training-free key-value (KV) cache compression mechanism is introduced to optimize memory usage during inference. The proposed INTP-Video-LLMs can comprehend longer video sequences (up to 32 frames) without additional training, and experimental results indicate that this approach provides a significant improvement in both video processing capabilities and inference efficiency."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The motivation for the proposed INTP-Video-LLM is clear and fundamental. The Introduction is well-crafted, making it easy to grasp the paper's concept.\n- The proposed methods allow existing Video-LLMs to process longer video (i.e. more video frames) **in a training-free manner**, effectively addressing computational constraints.\n- The paper comprehensively considers and analyzes whether the proposed **Video Tokens Rearrangement** and **Interpolating Video-LLM Backbone** will lead to additional computational overhead.\n- The **memory optimization** via KV-cache compression ensures that extended video sequences can be processed with minimal memory overhead, making the approach feasible for practical deployment."
            },
            "weaknesses": {
                "value": "- The paper points out the existing limitations in the temporal consistency of encoders and projectors, but the description of the video token rearrangement method is unclear. It is not explained why rearrangement would help maintain consistency, and a more thorough explanation is needed.\n- Near Figure 2, at line 234, it describes \u201cwe obtain two subsequences, $X_{v,1}$ and $X_{v,1}$,\u201d where $X_{v,1}$ appears twice. Is this a typographical error?\n- In Section 3.4.1, the authors analyze the inference cost and propose 2-bit quantization of the KV cache to reduce storage overhead during inference. However, the potential performance degradation due to quantization is not discussed in detail. It would be beneficial to include an analysis of the trade-offs between reduced storage and potential accuracy loss.\n- In the ablation study (Section 4.3), the authors compare the performance of using different numbers of frames in Video QA tasks. However, it is unclear what the individual contributions of each module are to the final performance. A detailed breakdown of the impact of each module would provide more insight into their effectiveness."
            },
            "questions": {
                "value": "Q1. The authors perform 2-bit quantization of the KV cache to reduce storage overhead during the inference process of the video LLM. However, does quantizing the stored KV result in performance degradation?\n\nQ2. Could you provide a more detailed description of the video tokens rearrangement process (e.g., in the form of pseudocode) as well as an analysis of the effectiveness of this method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}