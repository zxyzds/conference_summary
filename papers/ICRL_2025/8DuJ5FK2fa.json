{
    "id": "8DuJ5FK2fa",
    "title": "Trained Models Tell Us How to Make Them Robust to Spurious Correlation without Group Annotation",
    "abstract": "Classifiers trained with Empirical Risk Minimization (ERM) tend to rely on attributes that have high spurious correlation with the target. This can degrade the performance on underrepresented (or 'minority') groups that lack these attributes, posing significant challenges for both out-of-distribution generalization and fairness objectives. Many studies aim to enhance robustness to spurious correlation, but they sometimes depend on group annotations for training. Additionally, a common limitation in previous research is the reliance on group-annotated validation datasets for model selection. This constrains their applicability in situations where the nature of the spurious correlation is not known, or when group labels for certain spurious attributes are not available. To enhance model robustness with minimal group annotation assumptions, we propose Environment-based Validation and Loss-based Sampling (EVaLS). It uses the losses from an ERM-trained model to construct a balanced dataset of high-loss and low-loss samples, mitigating group imbalance in data. This significantly enhances robustness to group shifts when equipped with a simple post-training last layer retraining. By using environment inference methods to create diverse environments with correlation shifts, EVaLS can potentially eliminate the need for group annotation in validation data. In this context, the worst environment accuracy acts as a reliable surrogate throughout the retraining process for tuning hyperparameters and finding a model that performs well across diverse group shifts. EVaLS effectively achieves group robustness, showing that group annotation is not necessary even for validation. It is a fast, straightforward, and effective approach that reaches near-optimal worst group accuracy without needing group annotations, marking a new chapter in the robustness of trained models against spurious correlation.",
    "keywords": [
        "Spurious Correlation",
        "Group Robustness",
        "Zero Group Annotation",
        "Distribution Shift",
        "Out-of-Distribution Generalization"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We provide a scheme for making trained models robust to spurious correlation with zero group annotation using environment-based validation and loss-based sampling.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=8DuJ5FK2fa",
    "pdf_link": "https://openreview.net/pdf?id=8DuJ5FK2fa",
    "comments": [
        {
            "summary": {
                "value": "The paper EVaLS, a method to improve model robustness against spurious correlations without requiring group annotations. EVaLS balances high- and low-loss samples from an ERM-trained model and applies a simple last-layer retraining (on a loss-based sampled dataset), thus enhancing group robustness. The approach also uses worst environment accuracy to for model selection. Experimental results on diverse dataset shows competitive performance to baseline methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed approach EValS, using environment inference and Loss-based sampling, is novel and interesting. \n2. EValS has competitive performance with other methods across diverse datasets."
            },
            "weaknesses": {
                "value": "1. The assumption that \"minority samples are more prevalent among high-loss samples, while majority samples dominate the low-loss category\" is questionable. It is easy to construct distributions that does not satisfy this assumption. \n2. The performance of EValS seems to rely on the EIIL to find the correct environment. However, how to find the environments may be a challenging problem itself. \n3. The peformance of EValS does not seem consistently better than other baseline methods across all datasets. It is not clear whether the better performance in specific dataset is by chance."
            },
            "questions": {
                "value": "See weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Many studies that enhance robustness to spurious correlation require group annotations for training. This paper aims to enhance the robustness with minimal group annotation assumptions. Specifically, the losses from an ERM-trained model are used to construct a balanced dataset of high-loss and low-loss samples, mitigating group imbalance in data. Moreover, using environment inference methods to create diverse environments has been shown to potentially eliminate the need for group annotation in model selection. Experiments demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow.\n\n- The paper proposes a practical method to mitigate the reliance on spurious correlations without any group annotations. \n\n- A new dataset is constructed that demonstrates the effectiveness of the proposed method in mitigating unknown shortcuts."
            },
            "weaknesses": {
                "value": "- The proposed method is incremental and has limited technical contributions. Retraining the last-layer using group-balanced validation data to mitigate the reliance on spurious correlations has been used in [1,2]. Inferring environment is a direct follow-up of [3].\n\n- The theoretical analysis does not really explain why loss-based sampling within a class can be used to create a group-balanced dataset. The analysis assumes that the losses on the majority and minority samples follow Gaussian distributions. Under this assumption, it is obvious that the loss-based sampling could create two group-balanced sets of data. However, whether this assumption holds in practice is questionable. Moreover, a previous study [2] has found that that model disagreement may effectively upsample worst-group data, or in other words, may create a more group-balanced dataset. Thus, the loss-based sampling may not be as effective as proved in the paper.\n\n- The comparison in Table 1 isn't fair for some methods. EVaLS uses new data, i.e., a part of validation data, for retraining, while methods including GDRO + EIIL, JTT, and ERM do not have access to the new data. Moreover, the existing work [4] also propose a method that aims to mitigate spurious correlations without group annotations. It would be beneficial to compare with this method under the same setting. \n\n- There are some model selection methods that do not require group annotations, such as minimum class difference [4] and worst-class accuracy [5]. It would be helpful to analyze the effectiveness of the proposed worst environment accuracy in comparison with these techniques.\n\n[1] Kirichenko et al., Last layer re-training is sufficient for robustness to spurious correlations, ICLR 2023.\\\n[2] LaBonte et al., Towards last-layer retraining for group robustness with fewer annotations, NIPS, 2023.\\\n[3] Creager et al., Environment inference for invariant learning, ICML, 2021.\\\n[4] Li et al., Bias Amplification Enhances Minority Group Performance, TMLR, 2024.\\\n[5] Yang et al., Change is hard: A closer look at subpopulation shift, ICML, 2023."
            },
            "questions": {
                "value": "- See the weaknesses.\n- In Table 1, why the experiments on the CivilComments and MultiNLI datasets are out of the scope of the method?\n- In L189, the authors mention that they randomly divide the validation set into $\\mathcal{D}^{LL}$ and $\\mathcal{D}^{MS}$. What if the random division results in a poor set of $\\mathcal{D}^{MS}$ which does not have sufficient samples to represent a minority group of samples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a method called EVaLS, which trains a classifier that is robust to spurious correlations without requiring group labels. The method constructs a balanced dataset based on loss values and utilizes environments for hyperparameter tuning. Experiments on several datasets demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The method of selecting high-loss and low-loss data proposed in this paper effectively mitigated the problem of group imbalance, with both experiments and theoretical analysis effectively validating this point."
            },
            "weaknesses": {
                "value": "- The authors may need to provide a more detailed description of the advantages of loss-based sampling over other sampling methods. For instance, it would be beneficial to compare it specifically with methods like SELF, highlighting the unique benefits or efficiencies achieved by the proposed method.\n\n- The authors' claim that their method 'completely eliminates the need for group annotations' as a primary contribution seems somewhat tenuous. The methodology presented in the paper can be categorized into two main components: Environment-based Validation (EV) and Loss-based Sampling (LS). Notably, LS appears to require group labels for hyperparameter tuning, while EV utilizes environment labels generated through Environment Inference for Invariant Learning (EIIL) as a stand-in for group labels. However, EIIL was originally proposed by Creager et al., 2021, and it was inherently designed to address scenarios where group labels are not available. Although Creager et al., 2021 used true group labels for model selection within GroupDRO in their experiments, it appears that environment labels could also be suitably employed for this step."
            },
            "questions": {
                "value": "- From lines 452-457, the authors state that annotation-free methods can mitigate the impact of both labeled and unlabeled shortcut features more effectively. However, EVaLS-GL, which utilizes group labels, achieved better results than EVaLS. Could the authors provide their insights on this phenomenon?\n- \nIn the experiments, the performance of EVaLS and EVaLS-GL varied, with each having its strengths and weaknesses. Could the authors discuss the advantages and disadvantages of using inferenced environments versus group labels based on these findings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper works in the sub-population shift setting, in which the data consists in samples of different groups that shares a property. The goal is to learn a model robust to sub-population shifts, such as class imbalance, attribute imbalance, and spurious correlations. They propose a method (EVaLS) that improves the robustness of models trained using ERM without using any group annotation data. The method is well motivated both empirically and theoretically, and they show that the model has competitive performance with and without using group annotation. Moreover, they show that EVaLS is robust to scenarios where there is an unknown spurious attribute in comparison to the state-of-the-art."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Simple method, that works in multiple scenarios (with and without group annotations).  \n- The method do not make strong assumptions about the trained model. The only requirement is a model trained by ERM (the training data or any other training information is not necessary), while the method acts in a post-training phase.  \n- It shows competitive performance in comparison to the literature, while it has a less strict set of requirements in comparison to most of them.  \n- EVaLS outperforms the DFR (one of the state-of-the-art models) in cases with multiple spurious attributes"
            },
            "weaknesses": {
                "value": "- Make a comparison of EVaLS with more methods (e.g. AFR, since it also doesn't depend on ERM training) in the multiple spurious attributes scenario.\n- Have more evidence that EVaLS outperforms the DFR/other methods in cases with multiple spurious attributes (e.g. using more datasets). I believe that this is the strongest part of the results, and it is a clear advantage for EVaLS (besides cases in which group annotation is not available)."
            },
            "questions": {
                "value": "Some questions:\n1) Is it feasible to add AFR results to the multiple spurious attributes experiment?   \n2) Is it feasible to add an extra dataset to the multiple spurious attributes experiment?\n\nAdding these extra results will address the points mentioned in the weakness, showing stronger empirical evidence about EVaLS advantage in cases with multiple spurious attributes. \n\nMinor points:\n- There are references to Figure 1 (Line 83) and Figure 3 (Lines 172-177) that come before Figure 2 and were a bit confusing to me while I was reading the paper for the first time. In my opinion, removing these references will improve the readability of the paper.  \n- Figure 2 instead of figure 2 (Line 205).  \n- Have the oracle results as a reference in Table 6 to follow Figure 4 (b).  \n- Figure 4 (b) could include the standard deviation (such as in Table 6).  \n- Sometimes you use sub-population (Line 131) or subpopulation (Line 139). Keep it consistent across the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}