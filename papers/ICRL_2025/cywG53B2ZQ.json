{
    "id": "cywG53B2ZQ",
    "title": "Negative-Prompt-driven Alignment for Generative Language Model",
    "abstract": "Large language models have achieved remarkable capabilities, but aligning their outputs with human values and preferences remains a significant challenge. Existing alignment methods primarily focus on positive examples while overlooking the importance of negative responses in guiding models away from undesirable behaviors. For instance, the widely-used alignment datasets reveals a scarcity of explicit negative examples that contradict human values, hindering its ability to discourage harmful or biased outputs during training. To address this limitation, we propose NEAT, i.e., NEgative-prompt-driven AlignmenT, to introduce negative prompts to generate undesirable responses alongside positive examples during the optimization process.  NEAT explicitly penalizes the model for producing harmful outputs, guiding it not only toward desirable behaviors but also steering it away from generating undesirable, biased responses. This dual feedback mechanism enables better alignment with human preferences, crucial in contexts where avoiding harm is paramount. Starting from a pre-trained language model,  NEAT performs online alignment by incorporating a ranking loss derived from an expanded preference dataset containing both positive and negative examples. Extensive experiments validate  NEAT's effectiveness in significantly enhancing language models' alignment with human values and preferences.",
    "keywords": [
        "AI Alignment",
        "Prompt-driven Online Sampling"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cywG53B2ZQ",
    "pdf_link": "https://openreview.net/pdf?id=cywG53B2ZQ",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a straightforward method to align the LLMs via penalizing the negative responses. Specifically, this paper uses negative prompts to persuade the LLM to generate poor quality (harmful & helpless) responses. By simply combining ranking loss and language modeling loss, this paper trains the LLM towards alignment."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. This paper is easy to follow though the writing needs to be improved. Some content arrangements are not appropriate, for example, NEAT-PP variant is not well explained.\n2. The proposed method is easy to implement and the results show that it can help the LLM align better."
            },
            "weaknesses": {
                "value": "1. Lack of the novelty. The claimed \"focusing on penalizing negative responses\" has been already explored[1]. The claimed \"online sampling\" is not really \"online\", it is with two stages: sampling and training, which is similar to many alignment methods[2,3,4]. The combined training loss contributes little to this field.\n2. Experimental results are very limited. This paper only uses one dataset. This paper only involves two metrics: PPL and reward score. Please refer to existing papers to conduct more experiments.\n3. This paper does not offers insights into this field. Considering the current state of this paper, I encourage the authors to take a thorough revision and refine their ideas.\n\n[1] Liu, Xiao, et al. \"Extensive Self-Contrast Enables Feedback-Free Language Model Alignment.\" *arXiv preprint arXiv:2404.00604* (2024).\n\n[2] Yuan, Weizhe, et al. \"Self-rewarding language models.\" *arXiv preprint arXiv:2401.10020* (2024).\n\n[3] Guo, Shangmin, et al. \"Direct language model alignment from online ai feedback.\" *arXiv preprint arXiv:2402.04792* (2024).\n\n[4] Wu, Yue, et al. \"Self-play preference optimization for language model alignment.\" *arXiv preprint arXiv:2405.00675* (2024)."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method to align LLMs with human preferences by introducing negative prompts to penalize undesirable outputs during training, aiming to steers LLMs away from generating harmful or biased responses."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper writing is fluent."
            },
            "weaknesses": {
                "value": "1. There is a highly relevant work: \"Negating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization\". This paper also uses negative responses to guide LLMs to avoid generating harmful responses while maintaining helpfulness. The idea is highly similar with this paper, but about one year ahead.\n2. Experiments conducted in this paper is limited, making the effectiveness of the proposed approach not convincing."
            },
            "questions": {
                "value": "What are the differences between your work and \"Negating Negatives: Alignment with Human Negative Samples via Distributional Dispreference Optimization\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents NEAT (NEgative-prompt-driven AlignmenT), a method to align large language models with human values by addressing the lack of negative examples in training. Unlike existing methods that focus on positive examples, NEAT introduces negative prompts and explicitly penalizes harmful outputs, ensuring models are steered away from undesirable behaviors. Using a preference dataset with both positive and negative examples, NEAT enhances model alignment through a dual feedback mechanism. Experimental results show that NEAT effectively improves alignment with human preferences, especially in contexts where avoiding harm is crucial."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper presentation is clear and effective, with a compelling problem statement that highlights gaps in existing alignment methods. NEAT is introduced through a well-explained, novel approach involving negative prompts and a dual feedback mechanism. The detailed methodology, structured experimental setup, and use of visual aids ensure clarity, making the proposed solution and findings accessible and easy to follow."
            },
            "weaknesses": {
                "value": "1. Limitation of Applied Scenario: The effectiveness of NEAT relies heavily on the quality of the reward function used for penalizing negative outputs, which can be challenging to train and optimize effectively. This dependency limits the robustness and general applicability of the approach.\n\n2. Lack of Benchmark Dataset: All experiments in the paper are conducted solely on the HH-RLHF dataset. This limits the generalizability of the findings and raises concerns about whether NEAT can outperform other alignment methods across different datasets and diverse tasks.\n\n3. Unclear Performance Metrics: The metrics used in the evaluation are not clearly indicative of the model\u2019s alignment performance on the HH-RLHF dataset. The authors could use accuracy as a more straightforward metric to demonstrate the model\u2019s effectiveness, rather than relying on Perplexity (PPL), which does not directly reflect alignment quality."
            },
            "questions": {
                "value": "1. The effectiveness of NEAT appears to rely heavily on the quality of the reward function used for penalizing negative outputs. Given the challenges in training and optimizing this reward function, how do you ensure robustness across different scenarios?\n\n2. All experiments are conducted using the HH-RLHF dataset. Have you considered evaluating NEAT on other datasets to demonstrate its generalizability and its ability to outperform existing methods in different contexts?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces negative prompts to generate undesirable responses to the process of preference optimization. Similar to most existing LLM alignment approaches, the method aligns models with human preferences by penalizing undesirable responses and encouraging desirable responses. Experiments are conducted to evaluate the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe problem of aligning LLMs with human preferences is important.\n2.\tThe authors provide figures and a pseudo-code to illustrate the proposed method."
            },
            "weaknesses": {
                "value": "My concerns are as follows.\n1. The motivation of this paper is unclear. In Lines 13-15 of Abstract, the authors claim that \u201cexisting alignment methods primarily focus on positive examples while overlooking the importance of negative responses in guiding models away from undesirable behaviors.\u201d However, many preference alignment methods [1,2,3,4,5,6,7,8] use ranking-based loss to guide the model away from undesirable behaviors.\n2. This paper uses several important concepts in a confusing way. I am not sure whether the authors can distinguish between the concepts of \u201cexample\u201d, \u201cresponse\u201d, \u201csample\u201d, and \u201cprompt\u201d. This seriously undermines the professionalism and reliability of the paper.\n3. The technical contribution is incremental and the paper lacks novelty. I am well aware that reviewers should not hastily question the novelty of a work, but the so-called \u201cintroduction of negative examples\u201d and the final loss in Eq. (8) seem to provide extremely limited contributions to the field of preference alignment.\n4. Many important details about experiments are missing, such as learning rate, batch size, and the reward model used in the experiments.\n5. The authors may want to use GPT-4 as evaluators, which is a standard experimental setting in the area of preference alignment [5,6,9,10].\n6. There are lots of grammatical errors in the paper. A few minor errors in a scientific paper may be acceptable, but the unsatisfactory writing of this paper seriously affects the presentation of its method and brings great difficulty to readers to follow. Some examples of the errors (they are a small fraction of all errors) are as follows.\n- In Lines 73 and 265, the quotation marks are used incorrectly.\n- In Line 162, what do the authors mean by \u201cscore the dialogue with a constant\u201d?\n- In Line 198, \u201cthe training language model\u201d should be \u201cthe model being trained\u201d.\n- In Line 269, \u201cleading reward hacking\u201d should be \u201cleading to reward hacking\u201d.\n- In Line 269, the period at the end of the sentence is missing.\n\n[1] Direct Preference Optimization: Your Language Model is Secretly a Reward Model.\n\n[2] KTO: Model Alignment as Prospect Theoretic Optimization\n\n[3] A General Theoretical Paradigm to Understand Learning from Human Preferences\n\n[4] ORPO: Monolithic Preference Optimization without Reference Model\n\n[5] SimPO: Simple Preference Optimization with a Reference-Free Reward\n\n[6] Zephyr: Direct Distillation of LM Alignment\n\n[7] Qwen2 Technical Report\n\n[8] The Llama 3 Herd of Models\n\n[9] Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators\n\n[10] Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"
            },
            "questions": {
                "value": "Please check the Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}