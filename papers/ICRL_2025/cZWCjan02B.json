{
    "id": "cZWCjan02B",
    "title": "Flash Inference: Near Linear Time Inference for Long Convolution Sequence Models and Beyond",
    "abstract": "While transformers have been at the core of most recent advancements in sequence generative models, their computational cost remains quadratic in sequence length.\nSeveral subquadratic architectures have been proposed to address this computational issue. Some of them, including long convolution sequence models (LCSMs), such as Hyena, address this issue at training time but remain quadratic during inference. We propose a method for speeding up LCSMs' exact inference to quasilinear time, identify the key properties that make this possible, and propose a general framework that exploits these. Our approach, inspired by previous work on relaxed polynomial interpolation, is based on a tiling which helps decrease memory movement and share computation. It has the added benefit of allowing for almost complete parallelization across layers of the position-mixing part of the architecture. Empirically, we provide a proof of concept implementation for Hyena, which gets up to $1.6\\times$ end-to-end improvement over standard inference by improving $50\\times$ within the position-mixing part.",
    "keywords": [
        "computational efficiency",
        "inference",
        "sequence generative models",
        "long convolution sequence models",
        "hyena"
    ],
    "primary_area": "generative models",
    "TLDR": "We introduce a general method for speeding up exact inference in some classes of generative sequence models",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cZWCjan02B",
    "pdf_link": "https://openreview.net/pdf?id=cZWCjan02B",
    "comments": [
        {
            "comment": {
                "value": "Regarding the converse, yes we are on the same page that all LTI SSMs are also LCSMs, and the paper does say this in its current form in paragraph 2:\n> while any linear-time invariant (LTI) SSM has an equivalent convolutional filter\n\nAnd yes, you are right, that is a mistake on our end in the rebuttal (primarily driven by most of those models using the recurrent view for inference). The convolutional view does have to do with the state dimension in that choosing the parallel scan or FFT for training is a function of how large the state is. But we do agree that all SSMs are also LCSMs. And implicitly, any (LTI) SSM could use the convolutional view if that is preferable. We will more thoroughly go through the enumerated papers in our discussion, but feel free to proceed with responding to the rest of the rebuttal since we are on the same page."
            }
        },
        {
            "title": {
                "value": "clarification"
            },
            "comment": {
                "value": "I think there is a fundamental confusion here about definitions. Do we agree or disagree: by your definition of LCSM (section 2.3), all SSMs (henceforth assumed to be LTI and SISO unless otherwise qualified) are LCSMs?\n\n> the only SSM that makes use of the convolutional view (because it has a meaningfully large state dimension) is S4 - all the others exploit a small state dimension which LCSMs do not assume.\n\nFirst, all SSMs mentioned in my original review (\"LSSL, S4, DSS/S4D, H3, MEGA/Megalodon, and many more\") do use the convolutional view. This has nothing to do with state size.\n\n> but as stated in paragraph 2, we acknowledge that any convolution has an equivalent LTI SSM - it\u2019s the dimension of that state we emphasize as being the core difference\n\nSecond, my claim is not about any convolution having an equivalent LTI SSM, but the converse: all SSMs are by definition LCSMs.\n\nThese claims have nothing to do with the state size of the SSMs, for now. Let's get on the same page about this before I respond to the rest."
            }
        },
        {
            "comment": {
                "value": "Thank you for the thoughtful comments!\n\nWe are happy to try and integrate them, but before doing so we have a few questions to make sure we address the right issues:\n- Regarding the first weakness, we do provide Algorithm 2 which is the complete algorithm for an end-to-end LCSM model as well as Algorithm 1 which deals solely with one convolution layer. We could, if you find this to address your problem, add a python implementation for one layer (it would be mostly the same as Algorithm 1 but with an extra MLP and the $\\tau$ function implementation based on FFT). Otherwise, could you please pinpoint which parts of Algorithm 2 are too abstract? Could you also please detail what design decisions you have in mind? We are not aware of any implicit decisions, but are happy to add more details about any part.\n- For the second weakness\n   - The function $\\tau$ has first been defined at lines 261-262 (Eq. 3) for the case of relaxed polynomial interpolation and then overwritten for the general case at line 338. We intentionally use the same symbol since it has the same function, just across more dimensions, so that it is easier to follow the transition from Algorithm 1 to Algorithm 2. It is true that the first definition is as part of Lemma 1's statement: we are happy to define it before we state the Lemma to avoid any confusions.\n   - We will add in the Appendix the other 3 implementations of $\\tau$, but regarding how we choose between them: this is covered at lines 478-479. Specifically, we profile each implementation for each batch dimension, embedding dimensions, number of layers and tile size and automatically choose the best one for each iteration. There was indeed a typo at the end of the sentence: we should've referenced Figure 3a which shows the runtimes for the four implementations for varying tile sizes. We can provide the specific implementation choice for each (B, D, M, U) quadruple in the Appendix if that helps, but generally the pattern is as showed in Figure 3a: the quadratic implementations are better for small tiles and then the FFT ones dominate with the torch vs Flash implementations sometimes flipping.\n\nRegarding the first question, thank you for the suggestion, we will add it. As for the second question, what we meant is that van der Hoeven's approach could be used as well, but since $\\rho$ is known ahead of time we can do better. We hope the following restatement is clearer:\n> Whereas one could simply use the more general approach of (van der Hoeven, 1997), we can further take advantage of \u03c1 being known ahead of time to get a twice-faster and slightly simpler algorithm\n\nThe reason for the slightly more abstract view is simply to cover all LCSMs at once. We are happy to address any specific design choice you may have in mind and implement any actionable advice."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for the thorough comments and for acknowledging our technical contribution!\n\nRegarding the weaknesses:\n- While we are happy to add more context (and thank you for the pointers, have missed FlexConv), the only SSM that makes use of the convolutional view (because it has a meaningfully large state dimension) is S4 - all the others exploit a small state dimension which LCSMs do not assume. We will add a section in the appendix on the connection between LCSMs and SSMs, but as stated in paragraph 2, we acknowledge that any convolution has an equivalent LTI SSM - it\u2019s the dimension of that state we emphasize as being the core difference. The reason we did not delve deeper into this is simply because the purpose of the paper is not to argue for the use of LCSMs, but merely to provide an efficient way of serving them - an open problem that was also approached in Laughing Hyena Distillery.\n\n- Regarding the anchoring around Hyena, we will drop (or broaden) the references at lines 17, 43, 213 and 530. Thank you for the pointer - we thought Hyena is the most well known of the LCSMs and wanted to make explicit which parts apply to it as well. Moreover, the closest work to ours in terms of problem setup is Laughing Hyena Distillery.\nDo you mind elaborating what performance you would find meaningful to evaluate? Since we do exact inference, we only claim a certain speed-up: anything else related to the statistical quality of the model stays unchanged.\nRegarding why we tried only Hyena: is there any architecture you believe running extra experiments on would add any value? The reasoning was that the synthetic setting should cover all LCSMs and Hyena is just an extra example. Your last point about the method being generic is true and our synthetic setting, together with Algorithm 2 are not specific to Hyena in any way. \n\n- Firstly, Fig.1 of the linked paper does not seem to have any data on H3/Hyena. Secondly, and more importantly, it is very challenging to claim complete superiority of an architecture over another, especially in the opposite direction of representational power, simply because one cannot consider all the ways that module could be used: performance could vary a lot depending on use case and integration. Our method speeds up a long convolution layer, which is generic enough to be considered a useful building block for yet-unexplored scenarios. It could be that H3>Hyena by themselves and hybrids of attention with H3 are worse than hybrids of attention with Hyena. To give a precise example, EVO (which is based on Striped Hyena) is the current state-of-the-art in DNA modeling. Moreover, one could always change the biases of the convolutional filters: the main issue is that of training stability, which is itself an open problem. Our method serves as a tool readily usable for any such method. We believe it is useful for the community to be aware of a fast inference alternative for them to not constrain their research to low-dimensional SSMs because of computational considerations.\n\nRegarding the questions:\n- EVO (based on striped hyena) is an example of a long convolution-based model that is more performant than the equivalent SSMs (EVO tried mamba which failed due to numerical instability). We unfortunately cannot easily test how much of the accuracy is lost when distilling in due time, but the model is trained in the convolutional, not SSM view. S4, although primarily viewed as an SSM, is still SOTA on the Image section of the Long Range Arena tasks and requires a dimension equal to the sequence length.\n- Orchid and Zoology specifically show the representational advantages of data-dependent convolutions to synthetic tasks of interest - both of which suggest non-causal architectures which outperform SSM-based counterparts. It is an open problem to find a good *causal* architectural choice that is stable to training, but it is clear that the added representation power closes a gap. \n\nRegarding the broader positioning, we believe it is important to not overlook one main contribution: that of defining exact properties that allow for similar improvements to be applied to other architectures. We consider this is a good reason for the paper to be published at a conference such as ICLR since people can take these considerations into account when designing the next generations of architectures. While this is touched just briefly in the main paper due to limited space, we have a complete analysis in the appendix. Furthermore, we hope that the technical part of our method inspires people to think differently about similar problems that may arise in their practical research. Finally, we would also appreciate if you detailed what you meant through \u201creal models\u201d, since we do have a realistic Hyena implementation which is a real model."
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel linear-time inference algorithm for long convolution architectures like SGConv and Hyena. These models typically have a quadratic inference complexity with respect to the sequence length N, which can be prohibitive for large-scale applications.\n\nThe key insight behind the proposed approach is a clever partitioning and precomputation strategy for the contributions to the convolution of future outputs. This allows the inference complexity to be reduced to O(N log^2 N). The paper provides extensive experimental evidence demonstrating the acceleration of inference achieved by this new algorithm."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* As far as  I know, the interpolation perspective presented in this paper is original and inspiring. The writing is exceptionally clear, and Figure 1 has been extremely helpful in understanding the proposed method.\n\n* The algorithm introduced in the paper largely solves the long-standing problem of quadratic inference complexity for long convolution models like SGConv and Hyena. This has been a significant bottleneck for the practical deployment of these architectures (note that there are also other long conv architectures that do not suffer from this, please see below)."
            },
            "weaknesses": {
                "value": "This is a good paper and there is no much weakness to say about its methodology. However, I find the significance of the work depends on a line of work on long convolution architectures that the authors unfortunately have not discussed or compared.\n\nLong convolution kernels can be contructed from smaller convolutions in a tree style hierarchical dilations such as those in WaveNet [1]. Recently, people have shown that these architectures, with nonlinearities removed and weight sharing, can be interpreted as having a wavelet based state and are competitive with SSM and long convs on sequence modeling benchmarks [2].\n\nCrucially, these dilated convolution architectures also support linear-time inference, as detailed in [3], by maintaining a cache per layer. This allows for efficient inference without the need for techniques like FFT that are required for standard long convolutions.\n\nGiven these relevant prior results, I believe a comparison of the proposed approach to these dilated convolution models and their linear-time inference capabilities would greatly strengthen the paper. These alternative architectures offer a potentially simpler approach. As a reader of this paper, it would be nice to know when it is preferable to have a less structured long conv combined with the inference acceleration presented here.\n\nReferences:\n\n[1] Van Den Oord, Aaron, et al. \"Wavenet: A generative model for raw audio.\" arXiv preprint arXiv:1609.03499 12 (2016).\n\n[2] Shi, Jiaxin, Ke Alexander Wang, and Emily Fox. \"Sequence modeling with multiresolution convolutional memory.\" International Conference on Machine Learning. PMLR, 2023.\n\n[3] Paine, Tom Le, et al. \"Fast wavenet generation algorithm.\" arXiv preprint arXiv:1611.09482 (2016)."
            },
            "questions": {
                "value": "Please see my question above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes more efficient algorithms for auto-aggressive inference of long convolutional sequence models (LCSMs). The aggregate running time on a sequence of length $L$ is reduced from $O(L^2)$ to $O(L \\log^2 L)$, and the actual wall clock time of the implemented algorithm reflects substantial increases in efficiency."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper's main strength is that it is technically sound and novel, and addresses the problem it aims to solve.\n- The technical writing is clear and is helped by the inclusion of helpful graphics and rigorous algorithm boxes.\n- Many considerations and variants of the core algorithm are proposed.\n- An actual implementation is provided and all variants and baselines are benchmarked empirically.\n\nThere is a conception that long convolutions cannot be implemented efficiently in autoregressive inference settings, and so I do think that this paper presents an original algorithmic contribution."
            },
            "weaknesses": {
                "value": "While the paper provides a technical contribution, the paper's main weakness is that of significance and direction with respect to the broader field; it aims to solve a problem that I believe does not need solving. Correspondingly, the papers writing (in terms of positioning and related works / baselines) could also use improvement.\n\n- The paper's related work is sparse and I think it is important to present the lineage of these models more carefully. The original (depth separable) LCSMs were independently developed by two lines of work: the implicit convolution (CKConv and FlexConv) line of work, and the SSM line of work (LSSL, S4, DSS/S4D, H3, MEGA/Megalodon, and many more). Even though I understand why the paper deliberately puts emphasis on LCSMs that are not SSMs, because this is where its results are most applicable to, the positioning is at times misleading (e.g. in paragraph 2 of the introduction, where LCSMs are implicitly defined as non-SSM models, even though LTI SSMs are in fact the original models that popularized LCSMs).\n- It is odd that the paper is heavily anchored around the Hyena architecture, even though it does not actually do anything Hyena-specific. The experiments don't use an actual trained model or look at empirical performance, only the speed of an architecture, for which any model with a LCSM convolution (e.g. H3, to which Hyena is equivalent except for the choice of convolution kernel) could equivalently be substituted into the writing without changing the algorithmic results.\n- A distinction is made that low-dimensional LTI SSMs cannot represent general LCSMs, which is true and where the paper's potential benefits can come from. However, current understanding of LCSMs is that they need to be defined using certain priors (e.g. baking in exponential decay as in SGConv and Hyena) that essentially are similar to the priors imposed by low-dimensional SSMs. Empirically, it is known that there is little difference between these models (e.g. https://arxiv.org/abs/2312.00678v1 Fig 1, which claims that H3 in fact performs better than Hyena, where the only difference is the choice of convolution kernel parameterization). Thus a major weakness of this paper is that it is currently only applicable to models that have better alternatives empirically."
            },
            "questions": {
                "value": "I think the paper needs to position with respect to other types of LCSMs with more nuance. For example, it could benchmark how the proposed algorithm for general LCSMs compares to the inference speed of recurrent LCSMs; e.g. for a given sequence length $L$, at what recurrent state size does the speed of a recurrent SSM cross over the speed of the proposed algorithm? This would at least provide some more useful context for the reader interested in the pure algorithmic aspects.\n\nHowever, overall, in order for this paper to be valuable to the machine learning community, it should be applied to actual models, and current understanding of LCSMs is that general LCSMs essentially do not benefit over low-dimensional LTI SSMs. The contributions are interesting from a purely algorithmic perspective, and the paper does suggest potential extensions beyond time-invariant convolutions. However, it is not clear whether this has any chance of being extended into performant models - no actual downstream model is proposed. Thus, I think that a conference such as ICLR is not the most appropriate venue for such a submission without anchoring the algorithmic contributions to real models. In order to increase my score substantially, I think the paper needs to show a practical utility, for example either\n- Showing that there exist classes of models that the algorithm can be applied to that are not dominated by others (e.g. showing that some non-SSM LCSM model is actually faster or more performant than an equivalent SSM)\n- Showing that there exist data-dependent extensions that outperform data-independent convolutions in modeling performance"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work introduces a method to speed-up the autoregressive inference of long convolution sequence models (LCSMs) to near linear time. The approach is based on findings from relaxed polynomial interpolation\u2019s literature, which is gratefully adapted to convolutions.\nThe resulting algorithm results in important speeds-up both for the position-mixing components and end-do-end inference."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The method is novel and offers interesting improvements in the inference speed of LCSMs.\n\n- The paper offers interesting perspectives that could be used for the design of more efficient (causal, input-dependent) LCSMs in the future."
            },
            "weaknesses": {
                "value": "- The main weakness of the paper is that the presentation, design decisions and final implementation of the method remains quite abstract, even after reading the paper multiple times. Given that the paper presents an inference strategy, it should be feasible to have an stand-alone implementation (at least for one layer) incorporated in the Appendix of the paper. This would give clarity to the final, concrete version of the algorithm.\n\n- Next, I feel that the presentation of the paper could be improved. For example, the function $\\tau$ \u2013which is crucial for the method\u2013 remains undefined through the whole body of the paper up to Sec. 4.2, where it is briefly and loosely defined. It is mentioned that 7 possible versions were tested, but only 4 were mention-worthy. Then, it is mentioned that some are used, but it is not specified in which ranges and under which parameters one is preferred over the other. Given that this is core to the method, this should definitely be improved."
            },
            "questions": {
                "value": "### Additional questions and observations\n\n- Contribution 1. This is true, however, for clarity, I would recommend that the authors mention Laughing Hyena before making this claim to put in context that that method is not exact.\n\n- Line 233. There\u2019s a typo here that changes the whole meaning of the sentence. Please fix.\n\n ### Conclusion\n\nWhile I believe that the core contributions of this paper are very valuable, I am not really convinced by the current presentation of the paper. I would recommend the authors to improve readability and make specific the different design decisions that make the paper. \n\nDo note that the contribution of this paper is rather an algorithmic one. Yet, the current very abstract depiction of the method makes it unnecessarily difficult to implement and reproduce. I am, therefore, only able to provide this paper a \u201cweak acceptance\u201d at this time. However, that if the authors were to improve over the weaknesses outlined here, I would be more than happy to increase my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method for speeding up Long Convolution Sequence Models (LCSM)'s exact inference to quasilinear time, identifies the key properties that make this possible, and proposes a general framework that exploits these. The proposed approach is inspired by relaxed polynomial interpolation and uses a tiling method that minimizes memory movement and enhances computation sharing, allowing near-complete parallelization across layers in the architecture\u2019s position-mixing component. Through a proof-of-concept implementation on Hyena, we demonstrate up to a 1.6\u00d7 improvement in end-to-end inference time and a 50\u00d7 speedup in the position-mixing part."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper addresses a fundamental issue in sequence models, particularly long convolution sequence models (LCSMs) like Hyena, where inference time scales quadratically with sequence length. The proposed framework reduces this to quasilinear time, achieving significant improvements by leveraging a tiling-based approach inspired by relaxed polynomial interpolation.\n\n2. Besides speed, the paper focuses on reducing memory movement, a bottleneck in handling large models. It suggests methods for activation storage optimization and discusses adjustments for memory-constrained hardware, making the work relevant for deployment.\n\n3. The paper presents thorough empirical results that demonstrate the proposed method's effectiveness. It reports up to 1.6x end-to-end improvement in speed for Hyena and a 50x speedup within the convolutional mixer component, which provides concrete evidence of practical impact."
            },
            "weaknesses": {
                "value": "1. While the study is well-documented, the experiments focus primarily on synthetic setups and Hyena. Additional tests on a broader range of sequence models or with real-world tasks could further validate the framework\u2019s generalizability.\n\n2. The framework's efficiency relies on data-independent filters for optimal performance. Although data-dependent filters can be supported, doing so may lead to higher complexity or additional constraints. This could limit the application in cases where data-dependent filters are essential.\n\n3. The framework assumes autoregressive causality, which could restrict its application to non-causal architectures. This point is mentioned briefly, but it would benefit from further exploration of how the framework could adapt to different model constraints."
            },
            "questions": {
                "value": "1. The paper suggests applicability beyond LCSMs. Could the authors provide additional empirical results or theoretical discussion on adapting the method to different architectures, such as transformers, beyond high-level discussion?\n\n2. While data-independent filters optimize the framework's efficiency, data-dependent filters could require modifications. Could the authors elaborate on practical adaptations to support data-dependent filters effectively, and whether the framework maintains similar efficiency in such cases?\n\n3. The work compares primarily with the eager and lazy approaches. How does the proposed framework perform against other efficient models or architectures, such as those using efficient transformer variants?\n\n4. The paper assumes causality in model design. Are there potential modifications to the framework that could apply it to non-causal architectures? This could expand the impact and applicability of the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces \"Flash Inference\", a novel framework for efficient inference in long convolution sequence models (LCSMs) employing a tiling strategy inspired by relaxed polynomial interpolation . The authors propose an algorithm that achieves quasilinear $O(L \\log^2_2 L)$ time complexity for exact inference, improving upon the quadratic complexity of standard approaches without resorting to any approximation techniques. The work specifically demonstrates the method's effectiveness on Hyena architectures, achieving significant end-to-end speedup and even more significant improvement within the position-mixing component of the model. Beyond LCSMs, the authors identify key properties that enable such speedups and propose a general framework to guide future architecture design. The proposed approach further reduces memory movement and enables parallel computation across layers."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Technical Innovation and Soundness:\n- The paper presents a mathematically rigorous approach to improving inference efficiency, with clear proofs and careful analysis of complexity bounds\n- The implementation details are thoroughly considered, including memory optimization and parallelization strategies\n2. Practical Impact:\n- The achieved speedups (1.6\u00d7 end-to-end, 50\u00d7 for position-mixing) are significant and well-documented\n- The method is exact rather than approximate, maintaining model fidelity while improving performance\n- The framework extends beyond just LCSMs, providing valuable insights for future architecture design\n3. Experimental Validation:\n- Comprehensive empirical evaluation across different hyper parameters parameters ($B$, $M$, $D$)\n- Careful ablation studies of different $\\tau$ implementations\n- Clear breakdown of performance improvements and their sources"
            },
            "weaknesses": {
                "value": "1. Technical Presentation:\n- The notation could be improved for clarity, particularly in handling subscripts that simultaneously indicate sequence position, feature dimension, and layer. For example, you could consider using superscripts to indicate features (channels) and/or paranthesis around the subscripts/superscripts for the layers. \n- the use of \"$\\mapsto$\" notation on line 159 is imprecise; it should be $y\\mapsto z$.\n- Algorithm 1 should reference equation (3) for the definition of $\\tau$ for better clarity.\n\n2. Contextual Discussion:\n- The discussion of approximate inference methods could be more nuanced. The statement about \"defeating the purpose of using LCSMs instead of LTI SSMs\" is overly strong, given that Hyena filters are intentionally underparameterized and some *implicit regularization* (convergence of the filters to ones that can be effectively represented by a truncated basis of complex exponential functions) is to be expected. Te reviewer suggests that the authors provide a more balanced discussion of the trade-offs between exact and approximate methods, considering the intentional underparameterization of Hyena filters and the potential benefits of implicit regularization.\n- The introduction would benefit from a clearer distinction between prefilling and autoregressive generation phases in the complexity analysis. Elaborating on this could strengthen the clarity of how the method affects the performance of the system.\n- The claim about \"popular misconception\" (line 109) regarding the realization theory of LTI systems could be better supported with appropriate citations. It is the reviewer belief that the this is not a misconception in the control theory literature as the problem of realization of LTI system has bee studied for more than 60 years  (e.g., Kalman and Ho 1960). Perhaps the authors could clarify on that.\n\n3. Technical Specifics:\n- The asymptotic complexity expressions should consistently specify the base of the logarithm ($log_2$)\n- The parallelization discussion could benefit from more detailed analysis of the memory bandwidth implications\n- The interesting extension to data-dependent filters, while covered in the appendix, could deserve more attention in the main text if supported by quality experiments (do we need input-dependent filters in LCSMs?)"
            },
            "questions": {
                "value": "1. How does the performance of Flash Inference compare to existing methods when dealing with very long sequences (e.g., $L>$32K) in non synthetic scenarios? Are there specific challenges (other than memory) or additional optimizations possible at these scales?\n2. Could you elaborate on how the tiling strategy might be adapted for architectures with more complex dependencies between layers? This seems particularly relevant for hybrid architectures that combine convolution and attention mechanisms.\n2. The main limitation of the proposed method is certainly the linear growth of cache size with sequence lenght. How does this affect the practical benefits of exact vs approximate inference methods? In the approximate method, the cost of distillation can be amortized offline for a subsequent constant memory autoregressive generation. Moreover, one can trade additional memory (number of states in the SSM) for additional precision in representing the filters. Using a pre-trained LCSM, could you verify the accuracy-memory trade-off of the approximate vs exact method for practical sequence lengths? \n3. Have you considered how this framework might be extended to handle structured sparsity patterns in the filters? This could potentially lead to additional efficiency gains while maintaining the theoretical guarantees."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper develops a computation framework for efficient inference of long convolution sequence models (LCSMs). The proposed approach is based on relaxed polynomial interpolation, and can speed up LCSMs\u2019 exact inference to the quasilinear time, as is shown by both complexity analysis and numerical verifications."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow. \n2. The complexity analysis regarding both the computation and memory is detailed. \n3. The experiments are also provided to numerically verify the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. The main concern is about the novelty of this paper, particularly compared to the work by van der Hoeven and \"dynamic FFT\". It would be clearer if authors can provide more details about the differences or improvements of this work compared to former references. \n2. The applied setting seems somewhat limited, since it currently works only for Hyena-related architectures. Does the proposed method have the potential to inspire further extensions for general state-space models (SSMs; as with an equivalent convolutional filter) and Transformers (despite reasons stated in Sec. 3.4)? \n3. The numerical experiments part can be strengthened by e.g. adding tests for multiple configurations of hyper-parameters."
            },
            "questions": {
                "value": "1. Please kindly provide more details to the questions raised in the \"Weaknesses\" section. \n2. How about the case when the non-mixers\u2019 runtime is dominant, e.g. the large MLP module is common in practice (such as Transformers in applications). Do the improvements shown in Fig. 2(a) & 3(c) become marginal? \n3. For Fig. 3(c), why is the non-mixers\u2019 runtime less for Eager (NP) & Lazy (NP)? In addition, it seems that there is no reduction of mixers\u2019 runtime of Hybrid compared to (Flash) FFT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There are no ethics concerns."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}