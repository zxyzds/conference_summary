{
    "id": "uuXPWRtwvK",
    "title": "Graph-based Confidence Calibration for Large Language Models",
    "abstract": "One important approach to improving the reliability of large language models (LLMs) is to provide accurate confidence estimations regarding the correctness of their answers. However, developing a well-calibrated confidence estimation model is challenging, as mistakes made by LLMs can be difficult to detect. We propose a novel method combining the LLM's self-consistency with labeled data and training an auxiliary model to estimate the correctness of its responses to questions. This auxiliary model predicts the correctness of responses based solely on their consistent information. To set up the learning problem, we use a weighted graph to represent the consistency among the LLM's multiple responses to a question. Correctness labels are assigned to these responses based on their similarity to the correct answer. We then train a graph neural network to estimate the probability of correct responses. Experiments demonstrate that the proposed approach substantially outperforms several of the most recent methods in confidence calibration across multiple widely adopted benchmark datasets. Furthermore, the proposed approach significantly improves the generalization capability of confidence calibration on out-of-domain (OOD) data.",
    "keywords": [
        "Language Models; Uncertainty Calibration"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=uuXPWRtwvK",
    "pdf_link": "https://openreview.net/pdf?id=uuXPWRtwvK",
    "comments": [
        {
            "summary": {
                "value": "To enhance the calibration performance of LLMs, this paper proposes to combine the LLM\u2019s self-consistency with labeled data and train an auxiliary model to estimate the correctness of its responses to questions. Experiments demonstrate that the proposed method outperforms baselines in confidence calibration on two datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe paper presents a novel method to confidence calibration in large language models by leveraging graph neural networks and the consistency among multiple model responses. \n\n2. The manuscript is well-organized and the experiments are comprehensive."
            },
            "weaknesses": {
                "value": "1. The premise proposed in this paper is if LLMs give similar response. Then there is less uncertainty, and these responses tend to have a high probability of being correct.  It is common to use the idea of self-consistency to guide the selection of the most appropriate answer from multiple answers, but it may not be reasonable to use it as a probability to evaluate the correctness of an answer. For example, the LLM may exhibits generate the same wrong answer when sampling multiple answers. In this case, is the proposed method still effective? \n\n2. This paper trains another model to calibrate the correctness of the answers generated by LLM. What is the difference between this auxiliary model and the \"reward model\" in the LLM evaluation work, which is used to evaluate the quality of responses? If there is a difference, the author should point out the difference with this type of work; if it is similar, the author should add a description of these works and performance comparison in related work and experiments.\n\n3. This work uses semantic similarity to cluster different answers. Even so, answers within the same cluster may still be completely different answers. Because they just exhibit the similar reasoning paths but gives the different final answers. This phenomenon is also mentioned in paper [1]. But according to Equation 3, the author calculates the correctness probability of each cluster using the ratio of numbers. Why? How to explain the reasonability?\n[1] SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales\n\n4. In Method Section, the details how the training dataset was created are unclear. For example, for the same question, will the confidence information of each cluster be input into the GNN? It is recommended that the author add a paragraph to describe the complete process of constructing GNN training data.\n\n5. This paper constructs training data based on the results of self-consistency. The author should add a comparison with the performance of the simplest self-consistency method to estimate confidence in the baselines.\n\n6. Section 4.3 lacks analysis of DDL results."
            },
            "questions": {
                "value": "1.\tAre the confidence values of answers in the same cluster the same?\n\n2.\tWhen verifying the ability of the proposed method on OOD in Section 4.3, I recommend that the author add other datasets of tasks that are quite different from the training data to verify the generalization ability, such as Math"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper considers the problem of producing meaningful uncertainty estimates given samples from a large language model. The approach relies on repeated sampling the LLM and forming semantically equivalent clusters of responses. These clusters are converted to a fully-connected similarity graph based on SBERT similarity between the responses contained in those clusters, which is the input to a supervised post-hoc calibration graph neural network (GNN). The node classification problem is to predict for each response if it is correct. The GNN does not take as input the SBERT embedding vectors to avoid dependency on textual information, but uses the cluster IDs and the similarity between each response. The clustering is obtained using the k-means algorithm. The appendix says that 30 responses are sampled and that K is set to 3 for k-means. The robustness of the approach is further (marginally) improved by forming multiple prompts for each question to increase the variety of response; further variety is associated with greater uncertainty. The proposed approach is compared to a diverse set of baseline methods on two standard QA tasks and is found to perform favourably in terms of calibration performance. The approach performs well with only small amounts of training samples."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The paper considers an important problem (LLM confidence estimation).\n* A diverse set of baseline methods are included (although I would have liked more information about how they were tuned compared to the baseline).\n* The approach is applicable to \u201cblack box\u201d models since it only relies on being able to sample from the model, rather than requiring access to the activations or full predictive distribution.\n* The idea of using a GNN to calibrate a set of responses given a measure of semantic similarity between them is appealing."
            },
            "weaknesses": {
                "value": "* The use of a fixed number of samples and K for K-means seems like it could introduce some issues. If there\u2019s no/little ambiguity won\u2019t this still return three different clusters? At least some more sensitivity analysis should be done for K but this seems like a fundamental issue with the approach since choosing the number of clusters is underspecified in general.\n* The quality of the approach seems like it depends strongly on the semantic similarity used. SBERT is quite an old approach and it\u2019s unclear how well it generalises to various settings. I would have liked to see more analysis of the choice of semantic similarity. \n* The evaluation is conducted on two public QA benchmark datasets from 2017 and 2019, which are almost certainly in the training dataset for many LLMs. How does this impact performance?\n* There is no meaningful discussion of the limitations.\n* Regarding the writing, it should be made more apparent what data is treated as \"calibration data\" vs. \"validation data\" vs. \"test data\"\n* Overall, my biggest concern with the paper that the evaluation is limited to two fairly \"easy\" QA datasets. This makes it difficult to gauge how general the proposed approach is."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a graph-based method for confidence calibration for LLMs, aiming to improve reliability by estimating response correctness through consistency. A graph neural network (GNN) is trained on a similarity graph of multiple LLM responses to a question, predicting correctness based on response consistency without processing language directly. Experimental results show substantial improvements over baseline methods in calibration performance and out-of-domain generalization."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe topic of the paper is important. A well calibrated confidence of LLM outputs can benefit a lot of domains.\n2.\tThe model exhibits strong generalizability across different datasets and LLMs, showing robustness against domain shifts, which is valuable for real-world applications.\n3.\tThe paper includes sensitivity analyses and comparisons across various configurations, demonstrating the stability and effectiveness of the method under different setups and highlighting the performance benefits over established baselines."
            },
            "weaknesses": {
                "value": "1.\tThe requirement for sampling multiple responses and constructing similarity graphs for each query can introduce substantial computational costs, limiting scalability for real-time or resource-constrained applications.\n2.\tThe motivation is not clear. First, how to choose the value of \\tau in Eqn. is an extra challenge. Second, after having the correctness label, why do we need to leverage a GNN for classification? I think it is a typical text classification task. The motivation of choosing GNN is not clear. \n3.\tThe reliance on Sentence-BERT embeddings for similarity computations could introduce biases and inconsistencies if embeddings do not fully capture semantic equivalence, potentially affecting calibration accuracy."
            },
            "questions": {
                "value": "refer to the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes a GNN-based approach to calibrate confidence scores of LLM responses. The proposed method is an extension of the Semantic Uncertainty,  a sampling-based method that estimates uncertainty by using the semantic distribution.\nThe main distinction of the proposed method is that it treats each response as a node and applies GNNs to obtain the logit. Next, cross-entropy loss is introduced to calibrate the node score. Experimental results on short-text datasets CoQA and TriviaQA show that the new method can outperform other baselines and lead to a better generality in OOD settings."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. the paper is easy to follow \n2. This work develops a graph-based method to determine the semantic uncertainty, which is interesting\n3. better performance in OOD settings"
            },
            "weaknesses": {
                "value": "* **Main concern**:  the scientific contribution of this work is limited. The proposed method is an extended work of the semantic uncertainty with GNNs\n* writing and presentation should be improved. Some claims appear too arbitrarily without any proof. Figure 1 is not very informative. (see questions below)\n* the proposed method requires labels, but other baselines such as self-checkgpt and semantic uncertainty require no further training\n* the use of the ROUGE to evaluate semantic equivalence is problematic since it does not capture semantic meaning well; in the experimental part, only short-answer datasets are used. It is important to include long-form generation dataset"
            },
            "questions": {
                "value": "1. Figure 1 is not very informative, it does not provide the intuition why the proposed method chose to use GNNs\n2. In line 163-164, it is supposed to provide proof here: theoretical studies, empirical studies, or examples \n3. the use of the ROUGE metric for judging the correctness is problematic. ROUGE does not account for semantic meanings and is sensitive to length and word orders. Therefore, it is not an appropriate metric to evaluate whether two responses are semantically equivalent \n4. CoQA and TriviaQA are short-answer datasets. It is useful to test on datasets with long-form answers such as TruthfulQA, which can show the generality of the proposed method\n5. in line 63-64, \" A response consistent with more answers tends to have a higher likelihood of being correct\". A high consistency is an indicator of high confidence rather than high correctness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}