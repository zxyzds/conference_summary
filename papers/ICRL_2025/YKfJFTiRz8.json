{
    "id": "YKfJFTiRz8",
    "title": "Pre-Training Graph Contrastive Masked Autoencoders are Strong Distillers for EEG",
    "abstract": "Effectively utilizing extensive unlabeled high-density EEG data to improve performance in scenarios with limited labeled low-density EEG data presents a significant challenge. In this paper, we address this by framing it as a graph transfer learning and knowledge distillation problem. We propose a Unified Pre-trained Graph Contrastive Masked Autoencoder Distiller, named EEG-DisGCMAE, to bridge the gap between unlabeled/labeled and high/low-density EEG data. To fully leverage the abundant unlabeled EEG data, we introduce a novel unified graph self-supervised pre-training paradigm, which seamlessly integrates Graph Contrastive Pre-training and Graph Masked Autoencoder Pre-training. This approach synergistically combines contrastive and generative pre-training techniques by reconstructing contrastive samples and contrasting the reconstructions. For knowledge distillation from high-density to low-density EEG data, we propose a Graph Topology Distillation loss function, allowing a lightweight student model trained on low-density data to learn from a teacher model trained on high-density data, effectively handling missing electrodes through contrastive distillation. To integrate transfer learning and distillation, we jointly pre-train the teacher and student models by contrasting their queries and keys during pre-training, enabling robust distillers for downstream tasks. We demonstrate the effectiveness of our method on four classification tasks across two clinical EEG datasets with abundant unlabeled data and limited labeled data. The experimental results show that our approach significantly outperforms contemporary methods in both efficiency and accuracy.",
    "keywords": [
        "EEG Signals Recognition",
        "Graph Learning",
        "Self-supervised Graph Pre-training",
        "Graph Knowledge Distillation"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "We proposed a model that leverages self-supervised  graph-based transfer learning and knowledge distillation to address high-density to low-density EEG recognition.",
    "creation_date": "2024-09-14",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=YKfJFTiRz8",
    "pdf_link": "https://openreview.net/pdf?id=YKfJFTiRz8",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a graph self-supervised learning framework, *EEG-DisGCMAE*, designed for pre-training models and transferring knowledge from high-density EEG data to improve classification performance on low-density EEG data. The framework combines *Graph Contrastive Learning(GCL)* and *Graph Masked Autoencoders(GMAE)* in a pre-training scheme that reconstructs contrastive samples and contrasts these reconstructions. With a novel *Graph Topology Distillation(GTD)* loss in the fine-tuning stage, the framework effectively transfers spatial knowledge from high- to low-density settings. This enables a lightweight student model to achieve enhanced classification performance with limited labeled data, showing potential for efficient and effective analysis in low-density EEG scenarios."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. **Integration of Pre-train Methods from Original Perspective:** This paper proposes a novel approach that combines GCL and GMAE with the interpretation in relationships between node dropping and node masking, leading out a synergy of both self-supervised approach with unlabeled EEG data. This integration allows the model to achieve enhanced performance in scenarios with limited labeled data.\n\n2. **Effective Knowledge Distillation in Spatial Connectivity:** By leveraging Graph Topology Distillation (GTD) Loss with contrastive distillation, the framework effectively distills the knowledge of teacher model from hard-to-obtain HD EEG data into a lightweight student model. This enables the student model to achieve robust performance even with only LD EEG data, making the approach more practical and accessible.\n\n3. **Comprehensive Evaluation across Clinical EEG datasets:** The paper provides extensive experimental validation on four classification tasks across two datasets, demonstrating superior performance of EEG-DisGCMAE over existing methods and various baselines."
            },
            "weaknesses": {
                "value": "1. **Insufficient Evidence for Hypotheses:** The method relies on two key hypotheses: (1) that combining GCL and GMAE methods improves robustness in distillation, and (2) that jointly pre-training teacher and student models via mutual contrasting enhances distillation performance. However, these claims are not adequately validated in the current results. The paper would benefit from additional ablation studies and comparative evidence to clarify each component\u2019s impact and substantiate these hypotheses.\n\n2. **Lack of Clinical Interpretation:** While the paper includes visual assessments of EEG pattern reconstructions under various masking ratios, it lacks an analysis linking these patterns to clinically meaningful EEG features. Examining which EEG regions or connections contribute to classification tasks, and comparing these patterns with known clinical findings, would strengthen the study\u2019s practical relevance.\n\n3. **Limited Comparison with Recent SSL Methods:** The comparison primarily includes pre-2023 graph SSL methods, omitting recent graph SSL approaches [1, 2, 3, 4] and other EEG-specific SSL [5, 6] advancements that incorporate contrastive, generative, or both types of methods. Analyzing how the proposed approach differs from these recent methods would provide a clearer benchmark and better contextualize its contributions.\n\nOverall, the proposed method introduces novel approaches, such as GTD, that have the potential to deliver robust performance in practical scenarios. However, the study would benefit from more comprehensive ablation studies and tighter quantitative and qualitative validations to more effectively substantiate the hypotheses and demonstrate the method\u2019s impact. Addressing these aspects could significantly enhance the contribution of this work to the EEG clinical practice community.\n\n---\n```\n[1] Tan, Qiaoyu, et al. \"S2gae: Self-supervised graph autoencoders are generalizable learners with graph masking.\", ACM WSDM, 2023.\n[2] Hou, Zhenyu, et al. \"Graphmae2: A decoding-enhanced masked self-supervised graph learner.\", ACM web conference, 2023.\n[3] Yang, Weiwu, and Liang Zhou. \"CMGAE: Enhancing Graph Masked Autoencoders through the Use of Contrastive Learning.\", IEEE MLCR, 2023.\n[4] Wang, Yuxiang, et al. \"Generative and contrastive paradigms are complementary for graph self-supervised learning.\", IEEE ICDE, 2024.\n[5] Ho, Thi Kieu Khanh, and Narges Armanfard. \"Self-supervised learning for anomalous channel detection in EEG graphs: Application to seizure analysis.\", AAAI, 2023.\n[6] Peng, Ruimin, et al. \"Wavelet2vec: a filter bank masked autoencoder for EEG-based seizure subtype classification.\", IEEE ICASSP, 2023.\n```"
            },
            "questions": {
                "value": "1. Hypothesis 1 suggests that combining GCL and GMAE results in a more robust distiller than using each method independently. However, it seems that when GCL or GMAE is used alone for pre-training, knowledge distillation is not applied during fine-tuning. Is this correct? To validate this hypothesis, ablation studies similar to Table 4 that compare the fine-tuning performance of GCL and GMAE with and without GTD in distillation cases would be beneficial. Furthermore, if there are prior works supporting this hypothesis, referencing them would strengthen the argument.\n\n2. In Table 3, does \"Seq. Comb.\" refer to first performing GCL, then GMAE? If so, what is the performance when GCL and GMAE are jointly trained without mutual contrasting using teacher and student keys, followed by distillation fine-tuning? If this setup improves performance over single pre-training methods and further enhances distillation fine-tuning with mutual contrasting, it could provide validation for both Hypotheses 1 and 2.\n\n3. Is there a particular reason for using DGCNN as the backbone for the student model instead of Tiny G-Former? Given that the Tiny DGCNN  and the Tiny G-Former  have similar parameter counts (Table 7), and that G-Former generally performs better (Figure 2), G-Former might yield better results as the student model.\n\n4. According to Appendix A, a learning rate of 0.0002 was used uniformly across pre-training methods during fine-tuning. While a common learning rate may work well for similar model architectures, if GCL and GMAE were validated only on G-Former, it\u2019s possible that the selected learning rate might favor DGCNN. Additionally, each SSL pre-trained model may require an optimal learning rate suited to the specific starting representation, suggesting a need for more extensive hyperparameter ablation study in fine-tuning.\n\n5. Using the same samples from the EMBARC and HBN dataset for both pre-training and downstream tasks raises potential data leakage concerns. Even with different window sizes, the model may still encounter similar information, which could lead it to memorize dataset-specific features instead of learning generalizable patterns. To better evaluate generalizability, pre-training on the larger HBN dataset and fine-tuning on EMBARC would provide a clearer measure of robustness across datasets.\n\nMinor Comments:\n1. Figure 1 appears to need revision. Although \u201cFrozen\u201d and \u201cTuned\u201d are noted, the corresponding symbols are not visible. Additionally, the \u201cGNN2GNN Distill Loss\u201d in the figure is not referenced in the main text; does this term refer to the Graph Topology Distillation Loss? Consistent terminology would improve clarity.\n2. In Equation 11, what is the meaning of the second $\u2223\u2223$ following the KL divergence? Does it function as a conditional mask using an indicator? If this notation is standard, could a reference be provided?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Discrimination / bias / fairness concerns"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The dataset comprises de-identified human brain EEG data, publicly available and ethically approved, minimizing privacy concerns. However, observed imbalances in gender and patient group distributions may introduce bias, leading the model to favor more represented groups and impacting fairness. Addressing these imbalances through balanced representation or debiasing techniques could help improve generalization and accuracy across clinical settings."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a graph-based pretraining method for EEG data using contrastive learning and masked autoencoder. The pretrained model can be combined with distillation to improve the downstream performances on both low-density and high-density data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed pretraining and fine-tuning method improves the preformance on multiple tasks of EEG data.\n\n- The experiments are comprehensive. The paper compares with multiple baseline methods and conducts ablation studies."
            },
            "weaknesses": {
                "value": "- The goal and contributions of the paper are ambiguous. The paper proposes two things. First, pretraining combines contrastive learning and reconstruction. Second, the large model can used for training a tiny model from distilluation. The first part is not quite novel. For the second part, it's also unclear how does distillation help the model than just using the large model?\n\n- Equation (11) and (12) are confusing. What does \"||\" before P_ij means in Equ (11). In Eq (12), it is also not clear that why the distillation loss is defined like this. Is there any maths interpretation on what a sum of KL div over a sum of some other KL div means?  How are L^{logits}_{Dis} and L^{CE}_{Dis} defined and how are they implemented together with GTD loss? I believe the sample used in each batch are different. so it's unclear how these two losses can be summed during trainiing for a batch?\n\n- What is the different between GTD loss and some self-distillation method for SSL like DINO? Why it is not used in PT part but the FT part?\n\n- Figure 4 shows that training may not be enough after 400 epoch. How about validation loss?\n\n- Figure 1 is unclear. What is shown in (a), HD or LD performance? In (b), how can a student model be large? plots AUC v.s. ACC, but they are highly correlated. Instead of size of circle, it might be more straight forward to put size on y-axis. In the caption, not sure what does it mean by \"\u2019L\u2019 denotes large-size models.\" How was large-size models defined? What does ours-tiny/large in (a) coresponds to in (b)?\n\n- The experiments lack important details on other baselines - are they trained on HD or LD or both datasets? The comparison might not be fair if they are trained with different data from the proposed method. \n\n- More explanations are necessary to help understanding what the colors and shapes in Figure 3 mean."
            },
            "questions": {
                "value": "See weakness. Please considering clarify the main contribution of the paper, as the goal of the paper is hard to follow in the version. \nAlso, more details are need for method details. Please decribe the pipeline of the method - what data is used at which stage for both baseline and the proposed method? What is the purpose of some design choices? These will be helpful for the reasoning of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors integrate contrastive learning and knowledge distillation (KD) for EEG classification tasks. Specifically, they utilize contrastive learning between high-density EEG graphs and low-density EEG graphs to improve performance on limited labeled data. Additionally, they propose a Graph Knowledge Distillation approach with a Graph Topology Distillation loss to boost the performance of low-density EEG models when applied to high-density EEG data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The description of the method is clear and easy to understand.\n2. The authors' proposed method is technically sound.\n3. Experiments are comprehensive."
            },
            "weaknesses": {
                "value": "1. The experimental results suggest that the proposed method offers only a marginal improvement, and due to the typically small size of medical datasets, the performance gains are not convincing for me.\n2. The methods of comparison experiment are outdated, e.g.,  GMAE(Hou et al., 2022), GPT-GNN (Hu et al., 2020).\n3. The method is incremental. The method designed in this paper is primarily a combination of existing approaches. In graph learning, both contrastive learning and knowledge distillation have been extensively studied, and contrastive learning across different data views, such as high-level and low-level representations, is also a common practice."
            },
            "questions": {
                "value": "1. Assumption1 and Assumption 2 seem too strong and lack explanations and empirical support.\n2. As for Assumption 2, the assumption fails to specify how positive and negative pairs are selected.\n3. As for Assumption 1, there may be instability in the training process or architectural differences between GCL and GMAE that could affect the effectiveness of joint pre-training."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new approach, EEG-DisGCMAE, to address the challenge of enhancing low-density EEG analysis by leveraging high density EEG data. The authors used a graph-based transfer learning approach and considered this as a knowledge distillation problem. They introduced a unified pre-training framework that combines Graph Contrastive Learning (GCL) and Graph Masked Autoencoder (GMAE) methodologies. Their hybrid design captured robust features by reconstructing contrastive samples and contrasting the reconstructions, which enables both generative and contrastive pre-training. Their main contributions are as follows:\n\n1.\tEEG-DisGCMAE integrates GCL and GMAE pre-training to better leverage unlabeled data, optimizing the learning of robust features by combining generative and contrastive methods.\n2.\tThe Graph Topology Distillation (GTD) loss function facilitates knowledge distillation from a complex HD EEG model (teacher) to a lightweight LD EEG model (student), enabling the student to handle missing electrodes through contrastive distillation.\n3.\tBy jointly pre-training teacher and student models through contrastive querying, the framework enhances the distillation robustness, enabling strong transfer performance across downstream tasks.\n4.\tThe authors validated EEG-DisGCMAE on two clinical EEG datasets, demonstrating its ability to outperform existing models in both accuracy and efficiency, even with reduced parameter sizes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. The paper introduces a novel unified framework combining Graph Contrastive Learning and Graph Masked Autoencoders, along with a specialized Graph Topology Distillation (GTD) loss for HD to LD EEG data distillation.\n\n2. The methodology is solidly backed by theoretical foundations, with comprehensive experiments and ablations on two EEG datasets validating each component\u2019s effectiveness.\n\n3. The paper clearly explains complex ideas with well-structured sections, helpful figures, and strong contextualization within EEG and graph learning research.\n\n4. The framework advances portable EEG diagnostics and has broader implications for graph-based learning, offering lightweight, high-performance solutions."
            },
            "weaknesses": {
                "value": "1. While the paper demonstrates effectiveness on specific EEG classification tasks, testing on a broader range of EEG applications (e.g., seizure detection, cognitive state classification) could further validate generalizability.\n\n2. The paper focuses on graph-based methods, but comparing with non-graph EEG models (e.g., CNNs or RNNs used for EEG analysis) could provide a fuller picture of the proposed approach\u2019s advantages and trade-offs.\n\n3. The explanation of the GTD loss function is complex and could benefit from additional breakdown or intuitive examples to clarify how positive and negative pairs are selected.\n\n4. While low-density EEG is addressed, more evidence on the performance of the model at very low-density settings (e.g., <16 channels) would strengthen the claim that the model is effective for portable and affordable EEG setups.\n\n5. Although ablations cover essential components, including more details on varying the depth of GCL-GMAE integration (e.g., testing contrastive-only or generative-only pre-training) would offer deeper insights into the contributions of each part of the pre-training framework."
            },
            "questions": {
                "value": "1. How does the framework perform on EEG tasks beyond classification, such as seizure detection?\n\n2. Have you compared your model against non-graph EEG methods like CNNs or RNNs?\n\n3. Can you clarify the selection process for positive and negative pairs in the GTD loss?\n\n4. Have you tested your model at very low-density EEG settings (e.g., <16 channels)?\n\n5. Could you provide more ablation results showing the impact of using only GCL or only GMAE in pre-training?\n\n6. What is the potential of your model for real-time EEG applications in terms of computational efficiency?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a pre-training framework for resting-state Electroencephalography (EEG), a crucial tool for understanding neural dysfunctions. First, EEG data is represented as graph data, and Graph Neural Networks (GNNs) are employed to capture both the intricate features and topological structures of the EEG data. Due to the difficulty of acquiring labeled EEG data, the authors propose leveraging large amounts of unlabeled EEG data to improve performance on tasks with limited labeled data. Specifically, they introduce a self-supervised pre-training strategy, which combines Graph Contrastive Pre-training (GCL-PT) and Graph Masked Autoencoder Pre-training (GMAE-PT), allowing for efficient learning from the unlabeled data. To further enhance the model's performance on high-density EEG data, the authors design a Graph Topology Distillation (GTD) loss function, enabling a lightweight student model to learn from a more complex teacher model trained on high-density EEG data. The proposed framework is evaluated on real-world EEG datasets, and the contributions of each component are analyzed through extensive ablation studies."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "\u00b7 The paper proposes a framework to capture intricate features in EEG data, which is crucial for diagnosing clinical brain disorders.\n\n\u00b7 Given the scarcity of labeled EEG data, the authors formalize an effective transfer learning strategy that pre-trains on large unlabeled datasets and fine-tunes on limited labeled data.\n\n\u00b7 The paper introduces a novel knowledge distillation objective function that allows models trained on low-density EEG data to handle missing electrodes, effectively enhancing model performance."
            },
            "weaknesses": {
                "value": "\u00b7 The rationale behind the different components is unclear. It is not evident how the model handles the heterogeneous unlabeled EEG graphs.\n\n\n\u00b7 The authors do not discuss when Assumption 1 is valid and do not provide sufficient evidence to support the claim that combining GCL and GMAE results in a more robust distillation process.\n\n\u00b7 The strategy of randomly dropping nodes and edges to generate query and key graphs raises concerns. As mentioned in line 037, EEG data has complex inherent structures, and randomly removing nodes or edges could disrupt the graph's topology, potentially undermining graph learning and making contrastive learning less effective.\n\n\n\u00b7 The experimental setup is limited. The authors do not provide sufficient details about the graph data (e.g., the number of nodes and edges), making it difficult to assess the model\u2019s generalization capabilities.\n\n\n\u00b7 The paper is not well-written and is difficult to follow. For instance, the motivation behind proposing graph topological distillation is not clearly explained, leading to confusion.\n\n\n\u00b7 The paper is not well-written and hard to following. For example, it is quiet confusing that why the graph topological distillation is proposed"
            },
            "questions": {
                "value": "\u00b7 Could the authors provide more discussion on when Assumption 1 is valid?\n\n\n\u00b7 Would randomly dropping nodes and edges destroy important structures in the input EEG graphs?\n\n\n\u00b7 Regarding the contrastive learning framework, what is the rationale for using query and key graphs during contrastive learning? How does this approach aid in model pre-training? Why is the key sample pool beneficial?\n\n\n\u00b7 Could the authors provide more details on how topological information is preserved within the knowledge distillation framework?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "To fully leverage the extensive unsupervised data, this paper proposes a pretraining and fine-tuning paradigm for EEG representation learning, where GNN is applied as the backbone. To further enhance the learning of low-density data, this paper proposes a teacher-student distillation method to enrich the information from high-density data. Experiments are conducted on two datasets and four classification tasks, and the results demonstrate performance improvements over conventional GNN-based learning frameworks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper formulates two important questions in EEG representation learning, 1) how to leverage unsupervised data, 2) how to distill knowledge from rich (high-density) EEG data. With incorporation of those external knowledge, the performance on various datasets and tasks has shown significant improvements. Overall, this paper is well structured. The insights are straightforward and the solution is clear."
            },
            "weaknesses": {
                "value": "Though the questions tackled are significant, the technique designed lacks some novelty. [1] tackles the similar problem in EMR representation learning for clinical prediction, and the solution is also very similar (using pretraining on large data volume and fine-tuning on small targets, and distill knowledge from datasets with rich features). There are some differences in detailed design according to the problem setting, but the core ideas seem identical.\n\n```\n[1] Ma L, Ma X, Gao J, et al. Distilling knowledge from publicly available online EMR data to emerging epidemic for prognosis[C]//Proceedings of the Web Conference 2021. 2021: 3558-3568.\n```\n\nBelow are some technical issues:\n\n0. In graph construction stage: There seems too much information loss during node representation construction. Why not consider using a time-series embedding model (transformer-based or lightweight RNN-based) to capture more high-order information in the EEG time series of every single channel as the node representation?\n\n1. In GNN pretraining stage: How to ensure the representation spaces of teacher and student models can be inherently \u201calignable\u201d when using different model architectures (Graph transformers v.s. GCN)? As GCNs are spectral-based GNN, while Graph transformers are spatial-based. The basic hypothesis behind those architectures is different, thus the alignment of their representation spaces cannot be directly ensured. It is more reasonable to use models in different sizes under the same architecture.\n\n2. In GNN pretraining stage: The adjacency matrix A is dynamic during training. How to ensure the training stability? Moreover, GCN is a transductive GNN method, how to make it suitable for dynamic adjacency matrices?\n\n3. In Fine-tuning stage: Z in Eq.(11) seems like a likelihood score according to Eq.(8). Then how to conduct KL between two scores in Eq.(11) , as they are not distributions?\n\n4. In experiments: Maybe incorporate some other transfer learning/distillation methods for comparison?\n\nFinally, I suggest supplementing a notation table of all the variables introduced in Chapter 3, as there are too many symbols and readers may easily get lost."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}