{
    "id": "88hh5GtLBJ",
    "title": "MetaAdapter: Leveraging Meta-Learning for Expandable Representation in Few-Shot Class Incremental Learning",
    "abstract": "Few-shot class incremental learning (FSCIL) aims to enable  models to learn new tasks from few labeled samples while retaining knowledge of previously ones. This  scenario typically involves an offline base session with sufficient data for pre-training, followed by online incremental sessions where new classes are learned from limited samples. Existing methods either rely on a frozen feature extractor or meta-testing simulation to address overfitting issues in online sessions. However, they primarily learn feature representations using only the base session data, which significantly compromises the model's plasticity in feature representations. To enhance plasticity and reduce overfitting, we propose the MetaAdapter framework, which makes use of meta-learning for expandable representation. During the base session, we expand the network with pre-trained weights by inserting parallel adapters and employ meta-learning to encode generalizable knowledge into these modules. Then, the backbone is further trained on abundant data from the base classes to acquire fundamental classification ability.  In each online session, the adapters are first initialized with parameters from meta-training, and subsequently tuned to adapt to the new classes. Leveraging  meta-learning to produce initial adapters, MetaAdapter enables the feature extractor to effectively adapt to few-shot new classes, thus improving the generalization  of the model.  Experimental results on the mini-ImageNet, CUB200, and CIFAR100 datasets demonstrate that our proposed framework achieves the state-of-the-art performance.",
    "keywords": [
        "few-shot class incremental learning",
        "meta-learning",
        "feature representation",
        "residual adapter"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=88hh5GtLBJ",
    "pdf_link": "https://openreview.net/pdf?id=88hh5GtLBJ",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces MetaAdapter, a framework designed to address challenges in Few-Shot Class Incremental Learning (FSCIL). By employing meta-learning, MetaAdapter initializes adapters that encode general knowledge, aiming to balance stability and plasticity during incremental learning. The training process involves three phases: meta-training adapters, applying a feature compactness loss to reserve space for future classes, and utilizing knowledge distillation during incremental sessions. The framework demonstrates state-of-the-art performance on benchmarks such as mini-ImageNet, CIFAR100, and CUB200."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. **State-of-the-Art Performance:** MetaAdapter achieves competitive results on multiple FSCIL benchmarks, indicating its effectiveness in adapting to new classes with limited data.\n\n2. **Comprehensive Framework:** The integration of meta-learning with feature compactness and knowledge distillation offers a holistic approach to incremental learning challenges.\n\n3. **Well-Structured Presentation:** The paper is organized and clearly articulates the methodology, facilitating understanding."
            },
            "weaknesses": {
                "value": "1. **Limited Novelty in Meta-Learning Approach:** The application of meta-learning for adapter initialization resembles existing methods. Clarification on how this approach differs from established frameworks would strengthen the contribution.\n\n2. **Training Complexity:** The three-phase training process, including feature compactness loss and sharpness-aware minimization, adds complexity. This may pose challenges for implementation in resource-constrained environments.\n\n3. **Base Task Performance:** The framework appears to underperform in base classification tasks compared to other methods using the same backbone. Understanding the reasons for this discrepancy is crucial, as base task accuracy is vital for incremental learning stability.\n\n4. **Typographical Errors:** Minor errors, such as \u201crataining\u201d in Line 012 and \u201cminImageNet\u201d in Line 448, detract from the paper's professionalism. A thorough review to correct these is recommended.\n\n5. **Limited Comparison with Recent Methods:** The paper compares MetaAdapter with only one method from 2024. Including a broader range of recent methods would provide a more comprehensive evaluation of its performance."
            },
            "questions": {
                "value": "Please refer to weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to use meta-adapter to address the few-shot class incremental learning problem. Additionally, a feature compactness loss is introduced to help the accommodation of new categories. By leveraging the generalization capabilities of meta-learning training methods, the proposed method enhances the performance of few-shot class incremental tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n2. The figures in the paper are concise and clear.\n3. The idea of feature compactness loss and adapter integration is impressive.\n4. The ablation studies in Table 2 and Figure 4 are extensive."
            },
            "weaknesses": {
                "value": "1. Two highly related works are not compared [1,2], which have the same task setting.\n2. Although the feature compactness loss (FCL) is impressive and shows good performance improvement, the analysis of FCL is insufficient:\n    - Why don't more similar inter-class feature representations affect classification?\n    - Need further proof or experiment to demonstrate that more similar inter-class feature representations reserve embedding space. Additionally, will more similar inter-class feature representations possibly lead to the overall embedding space shrinking, similar to collapse in contrastive learning?\n    - Need to prove that it is the reservation space that helps future tasks and not others. For example, reducing the number of novel categories in the few-shot adaptation task can also reserve space. Will this also improve performance?\n    - The results using only FCL are missing in Table 2.\n    - FCL is an interesting idea that I think needs further discussion.\n3. It would be better to state how to expand $W_n^{t-1}$, Figure 2(b) only shows $W_n^{t}$.\n4. typo in L95: \"using this knowledge to improve leanring efficiency\"\n\n\n[1] Improved Continually Evolved Classifiers for Few-Shot Class-Incremental Learning. TCSVT2023. \\\n[2] Rethinking Few-shot Class-incremental Learning: Learning from Yourself. ECCV2024."
            },
            "questions": {
                "value": "1. Please address the Weaknesses.\n2. Since the ViT backbones are receiving more attention, I wonder if meta-adapter and FCL could be migrated to methods with ViT backbone?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel framework for few-shot class incremental learning (FSCIL) that addresses the challenges of plasticity and overfitting through meta-initialized adapters. The approach consists of three phases: meta-training adapters during the base session to obtain generalizable initial parameters, backbone pretraining with feature compactness loss to prevent feature space dispersion, and few-shot adaptation in incremental sessions where adapters are fine-tuned while preserving backbone knowledge. The framework demonstrates state-of-the-art performance across multiple benchmark datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Novel architectural design that combines meta-learning with adapter modules in a way that enhances both plasticity and stability. \n\n2. Three-phase training strategy that systematically addresses key FSCIL challenges. The strategy integrates meta-learning for initialization, feature space management during pretraining, and adaptation during incremental sessions.\n\n3. Thorough empirical validation across multiple benchmark datasets with ablation studies. The experimental results demonstrate consistent performance improvements across different scenarios and provide insights into the contribution of each component."
            },
            "weaknesses": {
                "value": "1. While the paper presents reproducible results across three datasets, some existing techniques implemented in the program are not mentioned in the manuscript. For example, the rotation technique utilized in the implementation appears to draw from previous work [1]. The authors should include all the implementation details in the paper and illustrate how these designs facilitate their method.\n\n[1] Learning with Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning\n\n2. A more detailed discussion of the novelty in the existing techniques combination should be made (meta-learning [2], SAM [3], and feature compactness loss in FSCIL [4][5][6]...). The paper should elaborate on how the specific combination and adaptation of these techniques contribute to improved performance and reduced forgetting in the FSCIL context, for example, highlighting any unique modifications or interactions between these designs.\n\n[2] On First-Order Meta-Learning Algorithms;\n[3] Sharpness-Aware Minimization for Efficiently Improving Generalization;\n[4] Forward Compatible Few-Shot Class-Incremental Learning;\n[5] Dycr: a dynamic clustering and recovering network for few-shot class-incremental learning;\n[6] Few-Shot Class-Incremental Learning from an Open-Set Perspective;\n...\n\n3. The benchmark comparisons should include detailed network architecture specifications for all compared methods. In addition, the model parameter size should also be listed since additional parameters are introduced in MetaAdapter. This additional information would facilitate a better understanding of the relative complexity and computational requirements across different approaches."
            },
            "questions": {
                "value": "Please refer to the weaknesses of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Nil"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Pointing out the heavy reliance on a feature extractor trained only on the base session, this paper leverages meta-learning to effectively adapt to new classes. Specifically, the proposed method first constructs a meta-learning scenario using the dataset from the base session and trains an adapter with Reptile, one of the meta-learning algorithms. Then, MetaAdapter trains a backbone network with a feature compactness loss to reserve feature space for future new classes. Finally, MetaAdapter updates the adapter using few-shot new-class data. The authors evaluate the proposed method on the CIFAR-100, miniImageNet, and CUB-200 datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors address one of the key challenges in Few-Shot Class Incremental Learning (FSCIL): the lack of plasticity caused by the heavy reliance on the encoder trained during the base session. To overcome this, they propose leveraging a meta-learning approach and tackle several challenges that arise when applying meta-learning in the context of FSCIL."
            },
            "weaknesses": {
                "value": "1) Unclear descriptions of the proposed method.\n\nThe meaning of 'c_pseudo,' mentioned in lines L223-L224, is difficult to understand, and a formal definition would be helpful. Additionally, in Equation 4, the dimension of 'p_concat' is unclear. It appears that 'c_batch' has a shape of B x C x d, while 'p' has a shape of B x d, where B, C, and d represent the batch size, the number of base classes, and the feature dimension, respectively. If this is the case, concatenation would be impossible; if not, further clarification from the authors on the structure of 'p_concat' is necessary.\n\nFurthermore, Section 3.5 is challenging to interpret. Figure 2 is particularly difficult to follow, especially in relation to the adapter\u2019s structure. It seems that the adapter may share convolutional layers with the backbone model, as indicated by the gray and sky-blue colors. However, the gray coloring appears to make this unclear. Additionally, the number of channels between the adapter convolutional layer (shown in red) and the backbone convolutional layer (in sky-blue) seems to differ. Yet, in Equation 13, these two layers are simply added, which would not be feasible with different channel numbers.\n\nThese issues make it challenging to understand the few-shot adaptation phase. A more explicit explanation of the adapter architectures would be beneficial.\n\n2) Motivation of the feature compactness loss (FCL)\n\nIn L211-L215, the authors argue that traditional optimization during the base session results in a dispersed embedding space that does not accommodate future new classes. To address this, they propose Feature Compactness Loss (FCL), which compacts the feature space to reserve space for future new classes.\nWith similar motivation, many existing works on Few-Shot Class Incremental Learning (FSCIL) [1, 2] have aimed to maximize the margin between classes in the feature space. While FCL appears to share this motivation, it takes the opposite approach: rather than maximizing the margin, it reduces the overall feature space. To validate FCL, the authors should provide additional analysis to explain why compacting the feature space is more effective than maximizing class margins in preserving space for new classes.\nThe reviewer encourages the authors to refer to [3], which proposes reducing inter-class distance to improve representation learning in FSCIL and provides an analysis on its implications.\n\n[1] Yang et al, \"Neural collapse inspired feature-classifier alignment for few-shot class incremental learning\", in ICLR 2023.\n\n[2] Zhou et al, \"Forward compatible few-shot class-incremental learning\", in CVPR2022.\n\n[3] Oh et al, \"CLOSER: Towards Better Representation Learning for Few-Shot Class-Incremental Learning\", in ECCV2024.\n\n3) Fairness issue\n\nIn Appendix A, the authors state that they use ResNet-12 for both mini-Imagenet and CIFAR-100 experiments.\nHowever, several existing methods like FACT and ALICE adopt ResNet-18 for miniImageNet experiments.\nThus, the comparison with these methods is unfair and may not demonstrate the effectiveness of the proposed method."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tackles the problem of Few-Shot Class Incremental Learning (FSCIL). The authors propose enhancing model plasticity during incremental learning stages by integrating and updating adapters within the backbone network. To simulate the testing scenario, the Reptile algorithm is employed to meta-learn the adapter, facilitating better initialization using data from the base session. Afterward, the adapters are frozen, and the backbone is fine-tuned on the base classes using a novel Feature Compactness Loss (FCL), complemented by a strategy to promote Flat Local Minima (FLM). The objectives of FCL and FLM are to reduce inter-class distances within the base classes, thereby preserving feature space capacity for future incremental classes. During incremental sessions, the adapters are updated and merged into the backbone through a running average. The approach demonstrates superior performance across three standard benchmarks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1.\tEnhancing model plasticity through the insertion of adapters while maintaining stability by freezing the backbone is a sound technical approach for FSCIL.\n2.\tPreserving feature space for future incremental classes is an effective strategy for improving overall performance.\n3.\tThe proposed method demonstrates superior performance across three standard benchmarks."
            },
            "weaknesses": {
                "value": "Major:\n\n1. The primary motivation of this paper is that previous methods do not update the learned representation during incremental sessions, thereby compromising model plasticity. However, the authors have not sufficiently explored related work in the field of continual learning. There is a considerable body of research that focuses on balancing model stability and plasticity during incremental sessions while also updating the backbone, not limited to FSCIL. The absence of a discussion on these relevant works is a notable gap.\nEven within FSCIL, prior works such as MetaFSCIL provide a relevant comparison. MetaFSCIL is a meta-learning-based method that not only learns meta-representations using base session data but also updates the backbone representation during incremental sessions. The meta-learning strategy employed in offline training mimics the meta-testing scenario while also balancing stability and plasticity as the backbone is updated. Conceptually, MetaFSCIL is closely aligned with the idea of meta-learning adapters proposed in this paper. However, the authors have misinterpreted MetaFSCIL\u2019s approach in L60-63 and L113-115.\n2. The concept of feature compactness loss, aimed at reducing excessive dispersion among base classes, is conceptually similar to the forward compatibility strategy introduced in FACT, which ensures sufficient feature space is reserved for future classes. However, the authors did not provide a discussion or comparison with FACT, despite the conceptual overlap. Including such a comparison would have strengthened the paper\u2019s positioning and clarified its contributions relative to existing approaches.\n\n3. The training sequence, where adapters are trained before fine-tuning the backbone, appears non-intuitive. In the first phase, the adapters are trained while the backbone remains frozen, causing the adapters to rely heavily on the backbone\u2019s fixed knowledge. As a result, the adapters are meta-learned to operate on top of this frozen representation. However, in the second phase, the backbone undergoes fine-tuning, altering its parameters. This shift may create incompatibility between the previously trained adapters and the newly updated backbone, potentially undermining the synergy between the two components.\n\n\nMinor:\n\n1.\tIt is inaccurate to state \u201crandomly initialize the adapter parameters for the j-th task,\u201d as mentioned in L197-198. This phrasing implies that the adapters are randomly initialized for each task, which is misleading. In reality, the adapters should be updated iteratively using Eqs. (1) and (2) to build on previously learned knowledge rather than restarting with random parameters for every task.\n2.\tEq. (4) appears somewhat unclear. If the goal is to bring feature vectors closer together, it would imply that Eq. (4) encourages a more uniform probability distribution. However, it is unclear how the information is concatenated into P_{\\text{concat}} . What is the dimensionality of P_{\\text{concat} ? If the concatenation occurs along the embedding dimension, the resulting output of the cosine similarity would be a scalar. Applying softmax to a scalar value does not seem meaningful, so additional clarification on the concatenation process is needed.\n3.\tThe objective of the feature compactness loss and sharpness-aware minimization is to reduce the distances among base classes. However, it is unclear whether this operation could negatively impact the model\u2019s performance on the base classes. If such degradation occurs, it is important to discuss how this issue could be mitigated to maintain performance on the base classes."
            },
            "questions": {
                "value": "1. How is the knowledge encoded in the adapters ensured to be task-agnostic, as claimed in L67? This concept is introduced in the paper\u2019s introduction but is not elaborated upon in subsequent sections. A more detailed explanation is necessary to clarify how the adapters generalize across tasks without being biased toward specific ones.\n2. How are pseudo-targets generated? The paper lacks details on the process used to obtain these pseudo-targets. Providing a clear description of the method for generating pseudo-targets is essential for understanding the approach and evaluating its effectiveness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}