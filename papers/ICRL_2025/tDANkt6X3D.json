{
    "id": "tDANkt6X3D",
    "title": "Horizon-Length Prediction: Advancing Fill-in-the-Middle Capabilities for Code Generation with Lookahead Planning",
    "abstract": "Fill-in-the-Middle (FIM) has become integral to code language models, enabling generation of missing code given both left and right contexts. However, the current FIM training paradigm, which reorders original training sequences and then performs regular next-token prediction (NTP), often leads to models struggling to generate content that aligns smoothly with the surrounding context. Crucially, while existing works rely on rule-based post-processing to circumvent this weakness, such methods are not practically usable in open-domain code completion tasks as they depend on restrictive, dataset-specific assumptions (e.g., generating the same number of lines as in the ground truth). Moreover, model performance on FIM tasks deteriorates significantly without these unrealistic assumptions.\n\nWe hypothesize that NTP alone is insufficient for models to learn effective planning conditioned on the distant right context, a critical factor for successful code infilling. To overcome this, we propose Horizon-Length Prediction (HLP), a novel training objective that teaches models to predict the number of remaining middle tokens (i.e., horizon length) at each step. HLP advances FIM with lookahead planning, enabling models to inherently learn infilling boundaries for arbitrary left and right contexts without relying on dataset-specific post-processing. Our evaluation across different models and sizes shows that HLP significantly improves FIM performance by up to 24% relatively on diverse benchmarks, across file-level and repository-level, and without resorting to unrealistic post-processing methods. Furthermore, the enhanced planning capability gained through HLP boosts model performance on code reasoning. Importantly, HLP only incurs negligible training overhead and no additional inference cost, ensuring its practicality for real-world scenarios.",
    "keywords": [
        "Language Models",
        "Fill-in-the-Middle",
        "Planning",
        "Training"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=tDANkt6X3D",
    "pdf_link": "https://openreview.net/pdf?id=tDANkt6X3D",
    "comments": [
        {
            "summary": {
                "value": "The paper highlights the current limitations of Fill-in-Middle (FIM) evaluations of popular coding benchmarks due to the post-process that takes places, which often boost evaluation performance artificially. The main contribution of the paper is introducing a new auxiliary loss that predicts the \"horizon\" length (i.e. the portion of tokens left to predict) on top of the typical Prefix-Suffix-Middle next token prediction loss to perform FIM tasks. With this training objective, LLMs perform better at FIM tasks, notably, the repository-level cross-fill code tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper soundly points out a glaring problem with current evaluation of FIM tasks that convincingly concludes that the post-processing only artificially boosts model scores while not providing additional insights into performance towards practical settings\n- The paper provides a simple but novel idea that shows model improvements in the more rigorous evaluation (without any post-processing) of FIM tasks that would be of interest for the research community to apply to other domains\n- The paper explores a breadth of different evaluation tasks for assessing their methods, including direct FIM tasks, repository-level cross-file FIM tasks, code fixing tasks, and reasoning tasks, demonstrating the effectiveness of their method\n- That paper is well written and easy to understand/follow"
            },
            "weaknesses": {
                "value": "- Lack of rigorous confidence interval analysis - all the of experimental results lack statistical significance numbers, making it hard to judge if the performance improvements are due to noise or if they are statistically significant.\n- Lack of theoretical/empirical evidence for why HLP works - it is not clear to me why this method works (assuming the experimental results are statistically significant). I believe the authors should add a section explaining (at least intuitively) why this method should work.\n- Lack of additional baselines + ablations - adding some other strong baseline results would further validate this method. For instance, authors mention multi-token prediction in their related works. This methods performance should be reported as a strong baseline. For ablations, one idea could be explore the affect of increasing the complexity of the $hlp$_$head$ (e.g. using a MLP of increasing layers)."
            },
            "questions": {
                "value": "- Could you report the statistical significance for each of the evaluations?\n- I found Section 5 particularly interesting and believe a natural question to ask is if \"Horizon Awareness\" under NTP increases with model parameters?\n- Why is $HLP_{L2R} + HLP_{FIM}$ not presented as the standard approach?\n- Is there a reason that during training the HLP objective is applied for each token instead of just the first token? My concern is that remaining tokens after the first token do not really provide any \"additional\" signal for the horizon length."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper considers the task Fill-in-the-Middle (FIM) in code generation. Instead of completing the code, which is a common task in code generation, the authors consider the task of completing missing code given the left and right contexts.\n\nThey consider adding an auxiliary objective, predicting the number of missing lines, to the training of a language model. The model is trained to optimize next-token prediction as well as the number of missing lines. Empirically, this method improves performance on file-level and repository-level benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "FIM is an important problem that is relatively underexplored in the literature. The method proposed in this paper is simple and straightforward. It shows significant improvements on diverse benchmarks.\n\nThe paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "**Contributions.**\n\nGeneralization: The proposed method appears to target at the FIM task. It limits its generalization to other code generation tasks.\n\nCost: The method requires fine-tuning a model specifically for FIM, which could be costly. Whenever a code model or a generalized model is released, it needs to be finetuned and maintained solely for this task.\n\n**Evaluation metrics.** Table 4 uses Exact Match (EM) and Edit Similarity (ES) as evaluation metrics, which are not standard in code generation. This choice seems to be consistent with prior work. Is it possible to evaluate using pass@1 / pass@k? Or is it justifiable to measure EM and ES for codes?"
            },
            "questions": {
                "value": "My questions are in the weaknesses section above. The authors are welcome to correct any possible misunderstandings."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper discusses the problem of fill-in-the-middle capabilities of code generation. The next-token-prediction way tends to make the model fail in long-length code generation to combine with both the left and right code sides. The paper proposes the method Horizon-Length Prediction (HLP) to tackle this issue. During the training, the model is also required to predict the ratio of the left token length to be generated to the total token length to be generated. This method is supposed to enhance the ability of LLMs to take care of the code generation length.\n\nThe experiments on extensive settings prove the effectiveness. The authors also use probing method to show that HLP trained model tends to have better capability on code generation length prediction. I am not an expert in code LLMs. Hence, I am not sure whether the proposed question and method are novel. Based on the current experiments, I think the results are effective and the paper is clear. However, the paper lacks extensive analysis on the proposed method like why should set like M-T/M for length prediction not other settings. I am giving a borderline in this case and will surely look through other reviews and authors' comments for final evaluation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The issue and the method are clearly explained, the experiments are carried out extensively and achieved notably better performance. Meanwhile, the method will nearly not increase the burden of LLM training. The method is intuitively reasonable."
            },
            "weaknesses": {
                "value": "1) The techniques used are relatively simple. Most contents in the paper are well-known. However, it makes sense.\n\n2) The setting of target as M-t/M is not such solid. In this equation, the target y also depends on the total length M. Whether it is optimal setting should be discussed. For example, ablation studies on other settings.\n\n3) The paper also lacks the thorough analysis like why enhancing the ability to predict code generation length is such effective, especially in fill-in-the-middle problem. Will it also work in uni-directional code generation?\n\ntypo: Line 132 are are -> are"
            },
            "questions": {
                "value": "As discussed in the above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}