{
    "id": "GM7cmQfk2F",
    "title": "Rethinking Neural Multi-Objective Combinatorial Optimization via Neat Weight Embedding",
    "abstract": "Recent decomposition-based neural multi-objective combinatorial optimization (MOCO) methods struggle to achieve desirable performance. Even equipped with complex learning techniques, they often suffer from significant optimality gaps in weight-specific subproblems. To address this challenge, we propose a neat weight embedding method to learn weight-specific representations, which captures weight-instance interaction for the subproblems and was overlooked by most current methods. We demonstrate the potentials of our method in two instantiations. First, we introduce a succinct addition model to learn weight-specific node embeddings, which surpassed most existing neural methods. Second, we design an enhanced conditional attention model to simultaneously learn the weight embedding and node embeddings, which yielded new state-of-the-art performance. Experimental results on classic MOCO problems verified the superiority of our method. Remarkably, our method also exhibits favorable generalization performance across problem sizes, even outperforming the neural method specialized for boosting size generalization.",
    "keywords": [
        "Neural Multi-Objective Combinatorial Optimization",
        "Weight Embedding",
        "Conditional Attention"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We propose a neat weight embedding method for neural multi-objective combinatorial optimization",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=GM7cmQfk2F",
    "pdf_link": "https://openreview.net/pdf?id=GM7cmQfk2F",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces a novel but simple neural multi-objective combinatorial optimization (MOCO) method. Specifically, the paper proposes a single-model method which can effectively solve MOCO problems (such as the multi-objective variants of the Traveling Salesman Problem or Capacitated Vehicle Routing Problem). This model is capable of learning the interaction of the problem instances with the weight vectors that are provided to decompose the problem into smaller, scalarized subproblems. At inference time, this allows the user to specify N weight vectors along with the problem instance, thereby producing a Pareto front of solutions. The authors introduce two variations of their method. In the first approach, named WE-Add, the interaction of the weight vectors and the node features is captured by simply adding their linear projections to get the node embeddings in the encoder of the model. In the second approach, WE-CA, the authors leverage a conditional attention model to capture the interaction of the instance and the weight vector. First, node embeddings conditioned on the weight vectors are derived through feature-wise affine transformations of the linear projections of the node features and weight vectors. Then, these embeddings are passed through standard transformer encoder layers, with multi-headed attention, instance normalization and feed forward networks. The authors demonstrate that this model not only reduces the optimality gaps of the subproblems but can also generalize well to problems of different sizes."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "### Originality\nThe method of deploying \"conditional attention\" as proposed in the paper is simple and novel.\n\n### Quality\nWith the exception of the points discussed in the Weaknesses Section, the paper is of good quality.\n1. The paper features a comprehensive list of experiments. It discusses variations of several important problems, such as 20, 50, and 100 node variants of the bi- and tri-objective Traveling Salesman Problem (Bi-TSP and Tri-TSP), bi-objective Capacitated Vehicular Routing Problem (Bi-CVRP), and bi-objective Knapsack Problem (Bi-KP). The paper also demonstrates the out-of-distribution generalization for 150 and 200 node variants of Bi-TSP.\n2. The authors justify their method which uses *conditional attention* by running ablation studies for its important components, such as *conditional embeddings* and *attention*. The experiments show that the combination of both these ideas work better than either one in isolation.\n\n\n### Clarity\n\nThe paper is well-written. The ideas are communicated clearly. For example, Section 4.1 explains the base model that is used, and then builds on it in Section 4.2 to explain the model with conditional attention, making it easy to follow.\n\n### Significance\nThe contributions of the paper are significant:\n1. The simplicity of the method is commendable.\n2. The proposed method shows strong performance compared to the baselines, showing smaller optimality gaps for the subproblems and higher hypervolumes, with comparable or faster solving times.\n3. Also interesting is the finding that a unified model trained this way generally performs better than models trained for problems of specific sizes."
            },
            "weaknesses": {
                "value": "The choice of reference points for evaluating the authors' methods and the baselines raises a concern. In the paper, it is mentioned that CNH [1] bears some similarities to the authors' approach. The CNH paper also evaluates the bi- and tri-objective Traveling Salesman Problem as well as the bi-objective Capacitated Vehicle Routing Problem. Additionally, six out of the twelve baseline methods listed in Table 2 of this paper are also present in Table II of the CNH paper: MOED/D, NSGA-II, MOGLS, DRL-MOA, PMOCO, and PMOCO-Aug. However, the reference points for calculating hypervolume (HV) in this paper differ from those in the CNH paper, making direct comparisons with their results impossible. For reference, please see the table below.\n\n| Problem   | Size | Reference Point (this paper)          \t| Reference Point (CNH paper)  \t|\n|-----------|------|----------------|----------------------|\n| Bi-TSP\t| 20   | (20, 20)   \t| (15, 15)        \t|\n|       \t| 50   | (35, 35)   \t| (30, 30)        \t|\n|       \t| 100  | (65, 65)   \t| (60, 60)        \t|\n|       \t| 150  | (85, 85)   \t| (90, 90)                \t|\n|       \t| 200  | (115, 115) \t| (120, 120)                \t|\n|       \t|  \t|            \t|                  \t|\n| Bi-CVRP   | 20   | (30, 4)    \t| (15, 3)         \t|\n|       \t| 50   | (45, 4)    \t| (40, 3)         \t|\n|       \t| 100  | (80, 4)    \t| (60, 3)         \t|\n|       \t|  \t|            \t|                  \t|\n| Tri-TSP   | 20   | (20, 20, 20)   | (15, 15, 15)    \t|\n|       \t| 50   | (35, 35, 35)   | (30, 30, 30)    \t|\n|       \t| 100  | (65, 65, 65)   | (60, 60, 60)    \t|\n\nEmploying the same reference points as the CNH paper would enable a direct comparison and lend additional credence to the findings if they align with the established results.  Without this alignment, and in the absence of publicly available code, it is difficult to verify the results. Addressing this issue would greatly enhance the rigor and transparency of the paper.\n\nI would be glad to reconsider my review if the authors could either provide results using the same reference points as the CNH paper or offer a clear justification for the reference points chosen in this study. This is the concern that has informed my rating for Soundness.\n\n[1] Mingfeng Fan, Yaoxin Wu, Zhiguang Cao, Wen Song, Guillaume Sartoretti, Huan Liu, and Guohua Wu. Conditional neural heuristic for multiobjective vehicle routing problems. IEEE Transactions on Neural Networks and Learning Systems, 2024.\n\nOther concerns about the paper are thus:\n1. Figure 2 is intended to show the Pareto fronts, but it doesn\u2019t. For instance, in the figure for KroAB200 (right), the results for CNH-Aug, represented by green circles, show that the left-most point appears to Pareto dominate all other CNH-Aug points. There are more examples across the three figures.\n2. Section 3.1 and parts of 3.2 bear significant resemblance to the Section 3.1 and 3.2 of the paper on PMOCO [2]. However, I have overlooked this as they simply discuss the problem formulation.\n3. The authors' use of subjective language, such as \"the weight embedding are ingeniously incorporated into node embeddings\" and referring to alternative approaches as \"clumsy multi-model methods\" detracts from the objectivity of the paper. A more impartial tone that directly outlines the methods would enhance the clarity and professionalism of the writing.\n\n[2] Xi Lin, Zhiyuan Yang, and Qingfu Zhang. Pareto set learning for neural multi-objective combinatorial optimization. In International Conference on Learning Representations, 2022a.\n\nA few other suggestions to improve the clarity and ease of reading the paper:\n1. It would benefit the reader if it is explained that the solution, represented as a sequence $\\( \\pi = \\{ \\pi_1, \\dots, \\pi_T \\} \\)$, is simply a permutation of the nodes for the Traveling Salesman Problem.\n2. $P(\\pi | \\lambda, s)$ is introduced in Section 3.2, but $s$ is only described in 4.1.\n3. \u201cIN\u201d (instance normalization) used in Eq (2) is never explicitly stated anywhere.\n4. Discussing the venue of publication in Table 1 is not necessary, in my opinion. Neither is the description of baseline methods as \"complex\" and the authors' method as \"neat\". Parameters is misspelled in the column header."
            },
            "questions": {
                "value": "Can the authors justify the reference points that were chosen for the experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduced a new way for directly learning weight-specific representations, thereby improving the handling of decomposed subproblems. The authors designed two models for weight embedding: one is an additive embedding model, which performs embedding through simple addition operations; the other is a conditional attention model, which more accurately captures the interaction between weights and instance information through a conditional attention mechanism."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The weight embedding method proposed in this paper directly learns weight-specific representations, avoiding tedious adjustments and high computational costs, while improving performance without increasing model complexity.\n\nThe weight embedding method not only performs well across various problem scales but also shows strong generalization across different scales (such as varying numbers of nodes or task complexity). This capability allows the model to maintain good optimization performance when encountering problems of different scales or new challenges, demonstrating high adaptability. The additive weight embedding and conditional attention weight embedding models designed in the paper are not only straightforward but also adaptable to various MOCO tasks.\n\nThe authors also provides a lot of experiments for validation, showing the superority for their performance."
            },
            "weaknesses": {
                "value": "I think it would be beneficial to include more theoretical discussions. For example, the paper mentions that the weighted approach can improve generalization; adding a proof for the generalization bound would make the results more convincing. Additionally, when the number of classes approaches infinity, will this weighting approach converge to the average weight?"
            },
            "questions": {
                "value": "For larger-scale problems, such as those with a large number of objective functions or high dimensions, how efficient is weight embedding? If the number of variables is quite large, could this impact the precision of weight learning? Will it still lead to improvements in training results?\n\nDo the authors plan to release the code for validation? I believe that such a detailed comparison could be a great contribution to the community."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a method for solving Neural Multi-Objective Combinatorial Optimization (MOCO) using a \"neat\" weight embedding approach. The authors argue that existing MOCO models are limited in their ability to effectively optimize weight-specific subproblems due to complex learning techniques and significant optimality gaps. Their proposed method learns weight-specific representations through a simpler weight embedding technique, capturing weight-instance interactions. Two models instantiate this approach: one with addition-based weight embedding and another with conditional attention. Experimental results demonstrate the method\u2019s performance on benchmark MOCO problems, showing significant improvements in generalization across different problem sizes."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This work has extensive experiments that show the effectiveness of the proposed method and show strong cross-size generalization capabilities."
            },
            "weaknesses": {
                "value": "1. The authors could a clear definition of what they mean by \"neat\" in the context of their work, and highlight specific sections where they could elaborate on how their method contrasts with the complexity of existing approaches.\n2. In the Methods section, while the structure of the proposed architecture is described, I would like to see a more detailed explanation of why each component is expected to improve performance. Specifically, theoretical justifications for key components such as the addition-based weight embedding and the conditional attention mechanism would be helpful. Some discussion of how the proposed components address specific limitations of previous approaches would also strengthen the work.\n3. It would be valuable to see more ablation studies that isolate the contributions of each architectural component. For example, a fair experiment design that solely isolates the effect of the addition-based weight embedding."
            },
            "questions": {
                "value": "1. In Fig. 1, what is the input to get $h_c$ when $t=1$ in the decoder?\n2. In Table 2, the proposed method appears to be slower than PMOCO. Does this indicate that the efficiency is worse compared to the baseline (given the 'neatness')?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a novel weight embedding approach for neural multi-objective combinatorial optimization (MOCO) to address the optimality challenges observed in current decomposition-based methods. It focuses on capturing weight-instance interaction through a weight-specific representation learned directly within the neural model. Two model variations\u2014Weight Embedding with Addition (WE-Add) and Weight Embedding with Conditional Attention (WE-CA)\u2014are introduced. These models simplify MOCO by avoiding complex auxiliary techniques and showcase state-of-the-art performance across several MOCO problems, specifically the multi-objective traveling salesman, capacitated vehicle routing, and knapsack problems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- Direct weight embedding is a fresh perspective in MOCO, addressing a gap in existing neural approaches that often require complex multi-model techniques.\n- The paper provides thorough experimental results, comparing its models with state-of-the-art baselines (including multi-model, single-model, and heuristic-based methods) on three classic MOCO problems across different scales.\n- The proposed WE-CA model achieves superior performance in terms of hypervolume (HV) and execution time, particularly highlighting its generalization capabilities across problem sizes.\n- The models eliminate the need for size-aware embedding mechanisms, thus simplifying the optimization process."
            },
            "weaknesses": {
                "value": "- Real-world applications with complex constraints are acknowledged as challenging for this approach. Further exploration into handling such constraints would enhance the paper's practical relevance.\n\n- The paper\u2019s unified training model, WE-CA-U, provides promising generalization across problem sizes, but more discussion on its failure cases (where applicable) would improve understanding of its limitations."
            },
            "questions": {
                "value": "1-  In cases where the unified model fails to generalize effectively to certain sizes, could you provide more details on why this happens? Are there specific problem characteristics or settings that lead to these limitations?\n\n2- Could you elaborate on how the conditional attention layer facilitates weight-instance interaction? While the paper describes the feature-wise linear projection mechanism, more insight into its layer-wise influence on embeddings would clarify how it improves optimality across subproblems."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}