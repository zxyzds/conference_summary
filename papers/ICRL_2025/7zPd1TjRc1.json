{
    "id": "7zPd1TjRc1",
    "title": "SANIA: Polyak-type Optimization Framework Leads to Scale Invariant Stochastic Algorithms",
    "abstract": "Adaptive optimization methods are widely recognized as among the most popular approaches for training Deep Neural Networks (DNNs). Techniques such as Adam, AdaGrad, and AdaHessian utilize a preconditioner that modifies the search direction by incorporating information about the curvature of the objective function. However, despite their adaptive characteristics, these methods still require manual fine-tuning of the step-size. This, in turn, impacts the time required to solve a particular problem. This paper presents an optimization framework named SANIA to tackle these challenges. Beyond eliminating the need for manual step-size hyperparameter settings, SANIA incorporates techniques to address poorly scaled or ill-conditioned problems. We also explore several preconditioning methods, including Hutchinson's method, which approximates the Hessian diagonal of the loss function. We conclude with an extensive empirical examination of the proposed techniques across classification tasks, covering both convex and non-convex contexts.",
    "keywords": [
        "optimization",
        "learning rate free",
        "adaptive optimizers",
        "polyak step-size"
    ],
    "primary_area": "optimization",
    "TLDR": "Polyak step-size based optimization framework SANIA generalizes some of the most popular adaptive optimization methods while also making them scale invariant.",
    "creation_date": "2024-09-16",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=7zPd1TjRc1",
    "pdf_link": "https://openreview.net/pdf?id=7zPd1TjRc1",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a training algorithm generalizing the \"Polyak step-size\" to second-order algorithms, such as cubic Newton or quasi-Newton. \n\nLet $f$ be a function to minimize and $w\\_t$ the current estimate of the argmin.\nFor an order-1 method, the Polyak step-size comes down to choosing the step-size such that we would attain the minimum $w^*$ in one step if $f$ was affine between $w\\_t$ and $w^*$. For order-2 methods, the authors distinguish the metric used for the parameter space, denoted by $B_t$, and the metric related to the curvature of the local approximation of $f$, represented by a matrix $D_t$.\n\nThen, the authors propose variations of existing algorithms based on their generalization of Polyak step-size."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "# Originality\n\nAs far as I know, the work presented in this paper is original.\n\n\n# Clarity\n\nOverall, the paper is easy to follow.\n\n\n# Quality\n\nI am very grateful to the authors for providing additional experiments (Appendix E), which show not only test accuracy, but also test loss. I would have been even better to provide the training loss, since the proposed algorithms have been designed to improve the optimization process (not the generalization).\n\nOverall, the paper seems to be correct.\n\n# Significance\n\nThis paper provides practical uses of *SP2: A Second Order Stochastic Polyak*, Li et al., 2023. \nSo the present paper is relevant for the community."
            },
            "weaknesses": {
                "value": "# Originality\n\nThis paper can be seen as a practical application of *SP2: A Second Order Stochastic Polyak*, Li et al., 2023.\n\n\n# Clarity\n\nAt the end of Section 2, the authors use the notations: \"SANIA $I_d$\", \"SANIA $(V^{-1})^2$\" and \"SANIA $\\mathrm{diag}(H^{-1})$\", while writing that there is no preconditioning. Thus, I understand that \"SANIA $A$\" refers to Eqn. (6) with $D_t = A$. But there is no formal definition of \"SANIA $A$\". It should appear somewhere.\n\nJust above Eqn. (9), one can read: $m_t = m_t$, which should be \"$m_t = g_t$\" (?).\n\nSeveral mistakes, Figure 1, first plot: \n * the legend is difficult to understand, since several curves have the same label;\n * some curves do not seem to correspond to their label: apparently, light green should be \"SANIA $(V^{-1})^2$\";\n * the purple curve is dotted, while it is not dotted in the legend."
            },
            "questions": {
                "value": "Could the authors provide a formal link between scale invariance and using SANIA? It seems that there is some overlap between the two, but SANIA does not ensure scale invariance by itself. It would be interesting for potential users of SANIA to provide conditions for their algorithm to be scale-invariant.\n\nAddition to Fig. 5: how does SANIA compare to concurrent methods in terms of training loss?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces cubic Newton method with adaptive Polyak step-size, enhancing robustness and efficiency in non-convex tasks. It also proposes scale-invariant variants of AdaGrad and Adam, which improve optimizer performance on poorly scaled data. Extensive experiments validate the effectiveness of these methods across diverse optimization scenarios."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Thorough theoretical analysis.\n2. Clear writing"
            },
            "weaknesses": {
                "value": "1. It would be great to have theoretical/practical comparisons with Sophia [1], which also uses Hutchinson's based method to compute their preconditioner.\n2. KATE [2] removes square root to ensure scale invariance of Adagrad. It would be great to have theoretical/empirical comparison with KATE. \n\nI am willing to improve the score if the above are addressed.\n\n[1] Liu, Hong, et al. \"Sophia: A scalable stochastic second-order optimizer for language model pre-training.\" arXiv preprint arXiv:2305.14342 (2023).\n[2] Choudhury, Sayantan, et al. \"Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad.\" arXiv preprint arXiv:2403.02648 (2024)."
            },
            "questions": {
                "value": "What are your thoughts on how learning rate schedules such as cosine decay etc compose with $\\lambda_t$ schedule defined in (19)?\n\nHow does $\\lambda_t$ change during training?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The work tries to incorporate a Polyak-type optimization framework with existing optimizers for machine learning. In the new method, the optimization direction and step size are spontaneously determined by a simple local optimization for each step. The new method is not difficult in realization and appears to work. From the numerical results, the test accuracy of network trained with the new method is comparable to existing methods (but not better than them)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The new method can be naturally incorporated with existing methods and the realization is not difficult."
            },
            "weaknesses": {
                "value": "From the numerical methods, the new method does not show improvements compared to previous methods."
            },
            "questions": {
                "value": "1. Instability of the Euler method and noises are believed to be helpful for generalization (see Ref[1]). Relatively large step size is helpful for the training process to jump out of bad local minima. In the new framework, when the steps are spontaneously obtained, the instability are also almost inhibited spontaneously. This may be a severe problem for the new frame work. \n\n2. From the numerical results, it seems that the test accuracy of Sania is slightly worse than existing methods. The author should also compare the results with SGD, since it usually has competitive generalization ability.\n\n3. In Line 209, what is the meaning of \"Otherwise, we replaced step-size parameter \u03b3t to parameter fi\u2217?\". The authors should explain more clearly.\n\n4. Due to the anisotropic property of the loss landscape, the estimate f* may not be a good guess for the local optimization. How will this affect the performance of the new method?\n\n[1] The Implicit Regularization of Dynamical Stability in Stochastic Gradient Descent, Lei Wu, Weijie J. Su, ICML 2023;"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents SANIA, which is a method that doesn't require manual fine-tuning of the learning rate in commonly used stochastic optimization algorithms, leading to faster optimization. The authors present a framework which generalises common stochastic optimization algorithms. The authors also consider affine and scale invariance which seeks to address poorly scaled or ill-conditioned problems. The authors compare performance of SANIA with commonly used algorithms (which have had fine-tuned parameters) for training classifiers for MNIST, FashionMNIST, CIFAR10 and SVHN."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Provides a novel formulation of stochastic optimization algorithms and a novel method that does not require fine-tuning of the learning rate parameter. Investigation into scale invariance is novel. Theorems, equations and ideas are presented clearly. Numerical experiments show that the SANIA methodology achieves similar performance to algorithms with fine-tuned learning rates, and improved robustness as the training curves fluctuate less."
            },
            "weaknesses": {
                "value": "Numerical experiments lack a comparison to other options for tuning-free methods.\n\nMinor comments:\n\nOccasional incorrect grammar, including after equation (7): \"This leads us to Stochastic Polyak step-size method.\" should read \"This leads us to the Stochastic Polyak step-size method.\" Also bad grammar in the statement of Theorem 1. \"Another way to derive this formulation is by solving\" (4), I think it may be better to reference appendix B.2 to make it clear why this holds. \"interpolation condition\" isn't very clearly defined in my opinion, it may be better to be more clear. \"in practice it displays better convergence to the true Hessian than other\nsimilar methods like BFGS\" - I think this needs a reference."
            },
            "questions": {
                "value": "In Page 2, SPS has been derived, why not just cite it? The difference between g_t and m_t is unclear to me, these seem to often mean the same thing? Why not keep the notation consistent? In (6) you have written \\| w \\|_{B_t} is a Euclidean norm, would it be better to say \" \\| \\cdot \\|_{B_t} is a Euclidean norm\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}