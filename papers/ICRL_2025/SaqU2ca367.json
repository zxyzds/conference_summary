{
    "id": "SaqU2ca367",
    "title": "Explaining Hypergraph Neural Networks: From Local Explanations to Global Concepts",
    "abstract": "Hypergraph neural networks are a class of powerful models that leverage the message passing paradigm to learn over hypergraphs,\na generalization of graphs well-suited to describing relational data with higher-order interactions. However, such models are not naturally interpretable, and their explainability has received very limited attention. We introduce SHypX, the first model-agnostic post-hoc explainer for hypergraph neural networks that provides both local and global explanations. At the instance-level, it performs input attribution by discretely sampling explanation subhypergraphs optimized to be faithful and concise. At the model-level, it produces global explanation subhypergraphs using unsupervised concept extraction. Extensive experiments across four real-world and four novel, synthetic hypergraph datasets demonstrate that our method finds high-quality explanations which can target a user-specified balance between faithfulness and concision, improving over baselines by 25 percent points in fidelity on average.",
    "keywords": [
        "graph neural nets",
        "hypergraph neural nets",
        "explainability",
        "concepts"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We develop a model-agnostic post hoc explainer for hypergraph neural networks.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=SaqU2ca367",
    "pdf_link": "https://openreview.net/pdf?id=SaqU2ca367",
    "comments": [
        {
            "summary": {
                "value": "The paper studies a relatively unexplored problem of hypergraphs neural networks explanation for the task of node classification, both from the local and global perspective. The authors describe two methods they use for local and global explanations. They\nthen benchmark the methods on synthetic and real-world datasets against current baselines, notably with an updated fidelity metric."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The topic is interesting, and still in unexplored territory.\n- The paper is well written, the experiments seem to yield good results and the experimental design looks solid."
            },
            "weaknesses": {
                "value": "- There is not a lot of theory in this paper, and the methods used are pretty straightforward. \n- The literature could be a bit more developed (the one other baseline on the problem, global explanations of GNNs, mean field approximation in the hypergraphs context).\n- The local and global explanations techniques are completely decoupled, with the global\nexplanation in particular being transposed from a method already studied for GNNs. This is a liitle surprising, as the titled implied a deeper relationship.\n- There should be more local and global explanations derived from this work and derived\nfrom the GNNs explanation literature: benchmark different global explainer heuristic\nbased on the local explanation (as opposed to just one); for the local explanation, an\napproximation technique with a more refined method, or at least explain why other\nmethods would not work.\n\nOverall the paper needs a major revision."
            },
            "questions": {
                "value": "- Line 212 you use a mean-field approximation, can you justify this more, add some\nliterature that justifies such approximations for hypergraphs? Why can you do that\n(specially in the context of hypergraphs)? Are there other approximations you explored\nor considered? This is a pretty strong assumption.\n \n- Please define a concept line 251; it seems to be specific to a GNN, and to be a cluster\nof points in the GNN embedding space, and its \u201drepresentative\u201d is the node closest to\nthe geometric center in the embedding space, is that correct?\n\n- Line 249: Can you specify what the latent embedding space is for a node? Is it the\nfinal layer\u2019s output before the softmax?\n\n- The local and global explainer seem to be two completely decoupled methods; please\npoint this out in the paper. How are they related?\n \n- Have you tried other types of global explanations (e.g., GLGex-\nplainer [1]) based on this local explainer? \n\n- Have you tried other local explanations techniques (there are plenty of work [2])?\n\n- Has a similar sampling method for local explanations been used in GNN explanations?\n\nSome minor comments:\n\n\u2013 Equation (3) line 209 is not clear, I would write it differently.\n\n\u2013 Some typos: \u201cexplainiability\u201d line 75, \u201cP r(v \u2208 esub = 0)\u201d line 206\n\n\u2013 \u201ccoherent explanations\u201d line 80: please clarify what this means.\n\n\u2013 What is \u201cInfoNCE\u201d line 106? add context about noise-contrastive estimation.\n\n\u2013 Line 259, GCExplainer is mentioned in the method, but there has not been a\nreference since the introduction. It should be added on line 244 along with the\nMagister paper.\n\n\u2013 For the results, it would be helpful to separate the local from the global explanations.\n\n\u2013 The paragraph starting line 259 compares GCExplainer with the method of the\npaper, this is not clear, as it is never explained what GCExplainer does.\n\n[1] Azzolin, Steve, Antonio Longa, Pietro Barbiero, Pietro Li\u00f2, and Andrea Passerini. \"Global explainability of gnns via logic combination of learned concepts.\" arXiv preprint arXiv:2210.07147 (2022).\n\n[2] Kakkad, Jaykumar, Jaspal Jannu, Kartik Sharma, Charu Aggarwal, and Sourav Medya. \"A survey on explainability of graph neural networks.\" arXiv preprint arXiv:2306.01958 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces SHypX, the first model-agnostic post-hoc explainer for hyperGNNs that provides both local and global explanations. \nAt the local level, it finds salient subhypergraphs to explain individual predictions using Gumbel-Softmax sampling, while balancing faithfulness and concision. \nFor global explanations, it extracts concepts by clustering network representations and visualizing representative examples.\n\nIn experiments, the paper introduces four synthetic hypergraph datasets and generalized fidelity metrics for proper evaluation. \nThrough experiments on both synthetic and real datasets, SHypX demonstrates superior performance over existing baselines while maintaining architecture independence, making it a significant contribution to hypergraph machine learning interpretability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The paper presents multiple novel contributions: \n1. The first to provide both local and global explanations for hyperGNNs. \n2. Introduces a new sampling-based approach for local explanations that avoids using attention mechanisms. \n3. It develops techniques specifically designed for hypergraph structures. \n4. The synthetic datasets also represent an original contribution by creating structure-dependent tasks for evaluating hypergraph explainability.\n5. This work addresses an important gap in making hyperGNNs more interpretable."
            },
            "weaknesses": {
                "value": "**Insufficient Analysis of Graph-to-Hypergraph Explainability**\n\n**Constrained Real-World Evaluation**"
            },
            "questions": {
                "value": "**1.Insufficient Analysis of Graph-to-Hypergraph Explainability**\nThe paper provides some reasons why hypergraphs need specialized explainers, such as larger search spaces and structural differences. However, it lacks an in-depth analysis of what specifically would fail if traditional GNN explainers were used on constructed hypergraphs, which can be represented as graphs. Including a comparative study that illustrates concrete failure cases when applying regular GNN explainers to hypergraphs would significantly strengthen the motivation for developing specialized hypergraph explainers.\n\n**2. Constrained Real-World Evaluation**\n- The real-world datasets used in the paper are relatively small, which limits the ability to generalize the results. \n- Moreover, the paper acknowledges that the selected datasets may not sufficiently test the model's ability to understand hypergraph structure, given that even simple MLPs achieve competitive performance. While the inclusion of synthetic datasets partially addresses this limitation, it would be more compelling to include more complex real-world hypergraph datasets that present a greater challenge. Such datasets would better demonstrate the practical utility and robustness of the proposed method. - Furthermore, the constrained real-world evaluation raises the question: does the limited complexity and availability of these datasets indicate that the problem lacks substantial practical applications?\n\nMinor Comment: In Figure 1, what does the \"concept-to-class decision tree\" icon represent? Providing a brief explanation would help improve clarity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the post-hoc explainability of hypergraph neural networks (HyperGNNs) for the task of node classification in hypergraphs. The authors propose a method for providing both instance-level and global-level explanations for these models. The core idea of their approach is to sample node-hyperedge pairs in the computation graph of a specific node in a way that minimizes the loss function. Additionally, they introduce a set of synthetic hypergraphs to evaluate their method. Their experimental results demonstrate that the proposed method outperforms existing baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-written and clearly presented.\n- The authors address both instance-level and global-level explainability for HyperGNNs while latter has not been addressed previously."
            },
            "weaknesses": {
                "value": "- My main concern is that this paper only addresses explainability for the task of node classification in hypergraphs, which is a less interesting and important problem compared to (hyper)graph classification.\n- The baselines used in this paper are not sufficient. While there might not be many related works on explainability in hypergraphs, explainability methods for graph neural networks could be applied and compared with the proposed method.\n- The quantitative evaluation on real-world datasets shows very low performance, and the differences between the various baselines are minimal, making it difficult to determine if the proposed method performs significantly better than the other baselines.\n- While having synthetic datasets for an explainability method is crucial, it is not clear how the proposed synthetic datasets are particularly useful in this setting."
            },
            "questions": {
                "value": "- For the task of node classification, what would happen if a method returns only the first immediate neighbors of a node as the most important sub-hypergraph? Specifically, how would the fidelity metrics mentioned in this paper perform in this scenario? My main point is that having only an explainability model for node classification may not be of significant importance.\n\n- Could you present a hypergraph as a bipartite graph and apply graph-based explainability models to compare their performance with your method? It would be particularly useful to try GNNExplainer, as this method is very intuitive, and it is not immediately clear why it would not perform well in your setting.\n\n- For the global explainer, you are introducing yet another black-box, despite mentioning that SHypX \"doesn\u2019t rely on additional black-box networks.\" Could you clarify this contradiction?\n\n- It is necessary to visualize the results of the explanations for at least the synthetic datasets to visually assess which sub-hypergraphs SHypX identifies as important.\n\n- Figure 4 is very confusing, and the results you are trying to convey are not clear. Is there an alternative way to evaluate global explanations that might be more effective?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an explainer for hyperGNNs at both instance-level and global-level, which is called SHypX. At both instance-level and global-level, they extract salient subhypergraphs as the explanation. Four synthetic datasets were introduced in this paper, although none of them are open-sourced."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper is the second work on hyperGNN explainability. \n2. They introduce a new benchmark that contains four synthetic datasets to evaluate hyperGNN explanations."
            },
            "weaknesses": {
                "value": "1. Related work is not thoroughly reviewed, and more recent studies should be discussed. The limitations of existing approaches for GNN explainability in the context of hyperGNNs are not clearly addressed. Since most existing methods are model-agnostic, they should be able to provide explanations for hyperGNNs too. \n2. The newly introduced datasets are not provided. \n3. The presentation is not very good. For example, the caption of Fig 2 is confusing. \n4. The performance improvements on real world datasets seems minimal. \n5. The evaluation on the global-level explainability is too few. It doesn't seem that this explainer actually produce global-level explanations.  \n6. Some concerns about motivation and metrics. See questions."
            },
            "questions": {
                "value": "1. In Line 171, it says \"our goal is to produce an explanation subhypergraph\" for the local explainer. However, a single subhypergraph may not be enough to fully explain the prediction. Why do you only consider to produce a single explanation subhypergraph for each graph sample? \n2. The proposed method doesn't look to be specifically designed for hyperGNNs. For example, modules like Gumbel-Softmax samplers and global clustering. Why don't apply your method also to traditional GNNs? And why not use the explainers for those traditional GNNs, (which didn't use Gumbel-Softmax samplers, but some other methods to pick crucial subgraphs) to explain hyperGNNs? \n3. For the Fidelity evaluation, why do you only evaluate the Fidelity-, how about Fidelity+?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}