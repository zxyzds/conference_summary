{
    "id": "3LFR5N2uv8",
    "title": "Younger: The First Dataset for Artificial Intelligence-Generated Neural Network Architecture",
    "abstract": "Designing and optimizing neural network architectures typically require extensive expertise, starting from handcrafted designs followed by manual or automated refinement, which significantly hinders rapid innovation. To address these challenges, Younger is introduced as a comprehensive dataset derived from over 174K real-world models across more than 30 tasks from various public model hubs. After extensive processing and filtering, Younger includes 7,629 unique architectures, each represented as a directed acyclic graph with detailed operator-level information based on ONNX operator definitions, enabling compatibility across different deep learning frameworks. The dataset is designed to support the emerging research area of Artificial Intelligence-Generated Neural Network Architecture (AIGNNA), which aims to automate their generation and refinement. Comprehensive statistical analysis, including architecture component analyses, highlights the diversity and complexity of architectures in Younger, revealing the potential for future research in this domain. Initial experiments, including operator and dataflow predictions, demonstrate the dataset's utility for architecture exploration and evaluation, and highlight its potential as a benchmark for graph neural networks. Furthermore, an online platform ensures continuous maintenance and expansion of the dataset, supporting global researchers in their endeavors. The dataset and source code are publicly available to encourage further research and lower entry barriers in this challenging domain.",
    "keywords": [
        "Artificial Intelligence-Generated Neural Network Architecture",
        "Neural Architecture Design",
        "Graph Neural Network",
        "Benchmark",
        "Dataset"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=3LFR5N2uv8",
    "pdf_link": "https://openreview.net/pdf?id=3LFR5N2uv8",
    "comments": [
        {
            "summary": {
                "value": "This study produced a dataset of 7k unique models, Younger, from 174k publicly available models and 30 tasks. After processing and filtering, architectures are stored as acyclic graphs based on ONNX definitions.  The study aims to automate architecture generation and refinement.  It offers a range of statistical analyses to illustrate the diversity of Younger.  Several experiments are also included to show the potential of Younger, especially as a benchmark for GNN."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The study is highly relevant and timely for NAS-related fields, with the potential to generate a high impact on the community.  \n\nThe effort invested in this work seems quite substantial.\n\nThe writing and the presentation are clear and easy to follow."
            },
            "weaknesses": {
                "value": "The goal of this study is quite ambitious, allowing people to search for good architectures using Younger for a wide range of tasks.  The experimental section shall match that ambition, e.g. demonstrating the applicability and benefits of Younger on a wide range of tasks.  This would help illustrate the practical application of the dataset.\n* Provide a concrete example, or case study showing how Younger could be used for a specific task like CIFAR-10 classification and ImageNet classification.\n* Other possible cases can be performing time series prediction\n* Or creating generative models.\n* Or multimodal models for a specific task e.g. text-to-video or description or caption generation task.\n\n---\n\nAlso the paper claims Younger is advantageous in comparison with benchmarks like DARTS, NAS-Bench-201.  Other than the difference between them, as shown in Table 1, how would they compare in a specific task, e.g. using different sets for the same task?  The study should show Younger's advantages clearly, e.g. leading to better performance, reducing search costs etc.\n* Conduct a comparative experiment using Younger and a benchmark like NAS-Bench-201 on a common task, measuring metrics like search efficiency and final model performance.\n\n---\n  \nClearly state whether AIGNNA is a novel term introduced in this paper.  If not, provide the appropriate citation.\n\n---\n\nThe code and the dataset link cannot be found in the submission.\n* Provide direct links to the code repository and dataset, perhaps in a dedicated \"Resources\" section."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel dataset for AI-generated neural network architectures. Specifically, the construction process contains four core steps, i.e., retrieving NN models, converting the models to ONNX format, extracting DAGs from the ONNX models, and filtering out isomorphic DAGs to ensure the uniqueness of the architectures. Some experimental results support the statistical analysis and present some distributions of the whole dataset."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1) Constructing the dataset for neural architectures in the AI-generated manner is novel and interesting. \n2) The paper is nicely written and well organized. The details for the dataset and construction processes are clearly presented. \n3) The AI-generated architectures in different types are very helpful to several research directions."
            },
            "weaknesses": {
                "value": "1) More details in terms of the use of the proposed YOUNGER dataset are expected. For example, it is possible to provide at least one case using the YOUNGER dataset, such as performing some NAS algorithms to search for architectures in this dataset? \n2) The experimental results are mainly focus on GNNs. However, it seems that there can be other types of architectures contained by YOUNGER. Could the authors provide additional experimental results in terms of other types of architectures beyond GNNs?\n3) More details for the distribution of the performance of architectures are needed."
            },
            "questions": {
                "value": "Please see the weaknesses. Overall, this paper is interesting and the proposed YOUNGER dataset seems to be useful in several directions. Giving all these, I\u2019d like to recommend the score 6 temporarily."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a dataset for neural network architectures extracted from public model sources. Then convert these models to intermediate representation operators for further research purpose.  The dataset incorporates richer operator than existing neural architecture datasets, to facilitate more research in the field."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper presents a good contribution to the community by offering a dataset that can potentially faciliate the research in the neural architecture search field.\n\n2. The dataset published can overcome challenges of previous related datasets in terms of operator scope and scale."
            },
            "weaknesses": {
                "value": "One complaint I have regarding this dataset comparing with existing NASbench dataset is that the proposed dataset doesn't seem to have the associated training performance on a standardized dataset.  In comparison, the NASbench dataset is evaluated on a standard task (Cifar10) on different settings.  Because of this, user can only perform unsupervised analysis like shown in figure 3-6."
            },
            "questions": {
                "value": "in the paper it is stated in line 284 that \"less than 1% of these models represent heterogeneous and effective architectures. This notably low proportion of heterogeneous architectures highlights the limitations of current neural network design methods, both manual and NAS-based, in fostering architectural innovation\".   \n\nHow did the author decide whether an architecture is heterogeneous?  and what does it mean by effective architecture?   In addition, all architectures are from online sources should be mostly manually built right?    It is really surprising that out of all models available online, less than 1% are really valid."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This manuscript proposes Younger, a dataset comprising of neural network ONNX graph representations collected from various online repositories. Each ONNX graph is annotated with some kind of task performance. The goal of Younger is to fuel Artificial Intelligence-Generated Neural Network Architecture (AIGNNA) generation, in order to break the confines of pre-established search spaces. The manuscript consists of tables and figures tabulating/illustrating features of the Younger dataset, as well as some limited experiments on AIGNNA Exploration."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Strengths:\n- The dataset consists of many diverse architectures from different tasks.\n- Breaking the confines of predefined search spaces is a good step forward for NAS."
            },
            "weaknesses": {
                "value": "Weaknesses:\n- Primary weakness of this paper is that its key contribution is not novel at all. While it is true that predefined search spaces pose a large problem for NAS, there are existing works that break the confines of predefined search spaces, enabling generalizable prediction across different micro, cell-based NAS search spaces [1] and even between micro and macro search spaces [2], using ONNX as the generalizable representation [3, 4, 5] or otherwise. This paper does not seem to recognize such existing works.\n- AIGNNA in Section 4.3, the authors illustration is flawed for Fig. 7 left. You would not have a choice between 'Add' and 'ReLU' since ReLU is an activation applied to one single input, while 'Add' combines several inputs into one. A better choice would be to ask 'ReLU' or 'SiLU' or 'Add' vs. 'Concat'. Here still, there is existing work on generating new architectures outside of pre-existing search spaces [6, 7]. \n- Experimental results in Section 4.3.2 is not very convincing as you are focusing on testing different GNN designs (which govern the message passing rule, not necessarily graph feature design), while there are numerous neural predictors in the literature [8] which are better compared to. Also, ACC, F1, Prec. and Recall are generally not evaluation metrics for neural predictors; rather, rank correlation (Kendall's Tau or Spearman Rho) and regression error (L1) are more common [8]. \n- Overall presentation in the paper is very lacking. Table and Figure captions are too short and do not adequately convey enough information, while most figures (2 - 6) should be re-tweaked with larger fonts."
            },
            "questions": {
                "value": "Section 4.3.2: different tasks have different evaluation metrics and ranges for different datasets [9], so how do you handle that?\nSection 4.3.3: Can you provide more information on exactly what this is, in terms of framing it through citations and potential tables/figures presenting the results? That would be a better use of page real estate than Figs 2-6 which are more appendix details.\n\nReferences:\n\n[1] Liu, Yuqiao, et al. \"Bridge the gap between architecture spaces via a cross-domain predictor.\" Advances in Neural Information Processing Systems 35 (2022): 13355-13366.\n\n[2] Mills, Keith G., et al. \"Gennape: Towards generalized neural architecture performance estimators.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 8. 2023.\n\n[3] Yang, Yichen, et al. \"Equality saturation for tensor graph superoptimization.\" Proceedings of Machine Learning and Systems 3 (2021): 255-268.\n\n[4] Zhang, Chenhao, et al. \"Towards better generalization for neural network-based sat solvers.\" Pacific-Asia Conference on Knowledge Discovery and Data Mining. Cham: Springer International Publishing, 2022.\n\n[5] Mills, Keith G., et al. \"Building Optimal Neural Architectures using Interpretable Knowledge.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\n[6] Schrodi, Simon, et al. \"Construction of hierarchical neural architecture search spaces based on context-free grammars.\" Advances in Neural Information Processing Systems 36 (2024).\n\n[7] Salameh, Mohammad, et al. \"AutoGO: automated computation graph optimization for neural network evolution.\" Advances in Neural Information Processing Systems 36 (2024).\n\n[8] White, Colin, et al. \"How powerful are performance predictors in neural architecture search?.\" Advances in Neural Information Processing Systems 34 (2021): 28454-28469.\n\n[9] Mills, Keith G., et al. \"Aio-p: Expanding neural performance predictors beyond image classification.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 8. 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}