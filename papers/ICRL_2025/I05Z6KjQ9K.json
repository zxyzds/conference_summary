{
    "id": "I05Z6KjQ9K",
    "title": "Gradient Regularization-based Cross-Prompt Attacks on Vision Language Models",
    "abstract": "Recent large vision language models (VLMs) have gained significant attention for their superior performance in various visual understanding tasks using textual instructions, also known as prompts.\nHowever, existing research shows that VLMs are vulnerable to adversarial examples, where imperceptible perturbations added to images can lead to malicious outputs, posing security risks during deployment.\nUnlike single-modal models, VLMs process both images and text simultaneously, making the creation of visual adversarial examples dependent on specific prompts.\nConsequently, the same adversarial example may become ineffective when different prompts are used, which is common as users often input diverse prompts.\nOur experiments reveal severe non-stationarity when directly optimizing adversarial example generation using multiple prompts, resulting in examples specific to a single prompt with poor transferability.\nTo address this issue, we propose the Gradient Regularized-based Cross-Prompt Attack (GrCPA), which leverages gradient regularization to generate more robust adversarial attacks, thereby improving the assessment of model robustness.\nBy exploiting the structural characteristics of the Transformer, GrCPA reduces the variance of back-propagated gradients in the Attention and MLP components, utilizing regularized gradients to produce more effective adversarial examples.\nExtensive experiments on models such as Flamingo, BLIP-2, LLaVA and InstructBLIP demonstrate the effectiveness of GrCPA in enhancing the transferability of adversarial attacks across different prompts.",
    "keywords": [
        "Adversarial attacks",
        "vision language models",
        "cross-prompt"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "Using gradient regularization to enhance cross-prompt adversarial attacks on vision language models.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=I05Z6KjQ9K",
    "pdf_link": "https://openreview.net/pdf?id=I05Z6KjQ9K",
    "comments": [
        {
            "summary": {
                "value": "The authors proposed a method termed Gradient Regularized-based Cross-Prompt Attack (GrCPA) that creates adversarial images that transfer across prompts. The GrCPA method extends the previous cross-prompt framework by applying gradient regularisation. The effectiveness of the GrCPA is evaluated with Flamingo, BLIP-2, LLaVA, and InstructBLIP on different tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper conducts extensive experiments on various VLMs to prove the effectiveness of GrCPA.\n- The paper is easy to follow."
            },
            "weaknesses": {
                "value": "- **The novelty is limited**: As detailed in Section A.2 (line 878), the only difference between GrCPA and a recent work termed CroPA [1] is the addition of Gradient Regularization. The pipeline of GrCPA is highly similar to that of CroPA.\n\n- **Practical applicability to the real world is limited**: As shown in Table 11, GrCPA does not demonstrate strong transferability across different models, with the average ASR remaining below 10%.\n\n[1] Luo, Haochen, Jindong Gu, Fengyuan Liu, and Philip Torr. \"An image is worth 1000 lies: Transferability of adversarial images across prompts on vision-language models.\" In The Twelfth International Conference on Learning Representations. 2023."
            },
            "questions": {
                "value": "- What is the impact of using different extrema K?\n\n- Why the top k largest values of the gradient vectors are clipped directly to 0 instead of other constant values?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenge of creating transferable adversarial attacks across different prompts for vision language models (VLMs). The authors propose GrCPA (Gradient Regularized-based Cross-Prompt Attack), which utilizes gradient regularization to generate more robust adversarial examples."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1.\tThe experiments show the consistent better performance.\n2.\tThe writing is easy to follow."
            },
            "weaknesses": {
                "value": "1.\tThe novelty and contribution are marginal. It only modifies the training loss in a very simple way.\n2.\tThe logic is unconvincing to me. It is claimed that large gradients can lead to local optima and trigger overfitting issues. However, the Gradient Regularization simply sets the largest and the lowest gradients to zero. This raises two questions: (1) Why do you set the lowest gradient to zero? (2) Does the largest gradient represent a \u2018large\u2019 gradient? For example, in some cases, the largest gradient could be lower than the lowest gradient in another sample or batch. How do you define \u2018large\u2019 and \u2018small\u2019?\""
            },
            "questions": {
                "value": "1.\tDuring training and testing, do you use the same text prompts? If so, it seems that cross-prompt is just overfit on several prompts rather than one."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The similarities between this work and [1] in problem motivation, paper organization, experimental design, and writing text raise concerns about originality and potential plagiarism. Although the proposed approach does present some differences, the extent of overlap indicates that the authors may not have adequately distinguished their work from [1], published in ICLR 2024.\n\nHere are some specific instances that suggest potential plagiarism:\n\n1. The paper introduces a new problem, \"cross-prompt transferability,\" which was first proposed in [1]. Notably, the main text, abstract, and introduction do not reference this prior work.\n\n2. The organisation of this paper closely mirrors that of [1], with some tables being directly copied and merely modified to add an additional row.\n\n3. Several paragraphs in this manuscript appear to be simple paraphrases of corresponding sections in [1].\n\n4. In the experimental design, instead of acknowledging [1] as a basis, the authors claim to have independently designed the experiment, even though the design and details align precisely with those in [1].\n\nThere are additional similar issues present in the manuscript. Overall, this paper clearly does not adhere to accepted scientific writing standards.\n\n[1] Luo, Haochen, et al. \"An image is worth 1000 lies: Transferability of adversarial images across prompts on vision-language models.\" In The Twelfth International Conference on Learning Representations. 2023. Url: https://openreview.net/pdf?id=nc5GgFAvtk"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "N/A"
            },
            "weaknesses": {
                "value": "N/A"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The similarities between this work and [1] in problem motivation, paper organization, experimental design, and writing text raise concerns about originality and potential plagiarism. Although the proposed approach does present some differences, the extent of overlap indicates that the authors may not have adequately distinguished their work from [1], published in ICLR 2024.\n\nHere are some specific instances that suggest potential plagiarism:\n\n1. The paper introduces a new problem, \"cross-prompt transferability,\" which was first proposed in [1]. Notably, the main text, abstract, and introduction do not reference this prior work.\n\n2. The organisation of this paper closely mirrors that of [1], with some tables being directly copied and merely modified to add an additional row.\n\n3. Several paragraphs in this manuscript appear to be simple paraphrases of corresponding sections in [1].\n\n4. In the experimental design, instead of acknowledging [1] as a basis, the authors claim to have independently designed the experiment, even though the design and details align precisely with those in [1].\n\nThere are additional similar issues present in the manuscript. Overall, this paper clearly does not adhere to accepted scientific writing standards.\n\n[1] Luo, Haochen, et al. \"An image is worth 1000 lies: Transferability of adversarial images across prompts on vision-language models.\" In The Twelfth International Conference on Learning Representations. 2023. Url: https://openreview.net/pdf?id=nc5GgFAvtk"
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a novel Gradient Regularization-based Cross-Prompt Attack (GrCPA) targeting Vision-Language Models (VLMs), which addresses the issue of adversarial non-stationarity across diverse prompts. By leveraging gradient regularization, GrCPA mitigates the variability in adversarial success when multiple prompts are used. This approach enhances the robustness of adversarial examples, improving their transferability across prompts. Experiments on models such as Flamingo, BLIP-2, LLaVA, and InstructBLIP validate GrCPA\u2019s effectiveness, showing superior attack stability and transferability compared to existing methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The method introduced is original. GrCPA\u2019s use of gradient regularization for adversarial robustness across prompts introduces an effective method for enhancing VLM attack transferability.\n\n2. The extensive experimental analysis across models and tasks (e.g., image captioning, VQA) confirms the soundness of the approach.\n\n3. The method\u2019s formulation and rationale are clearly articulated, supported by structured experiments that compare GrCPA with established baselines."
            },
            "weaknesses": {
                "value": "1. The technical depth of this paper is somewhat limited. Adversarial attacks are really not something that is surprisingly new in machine learning models, even in VLM. Incremental improvement in this area does not contribute much to this community. The method only introduces gradient normalization to stabilize the adversarial optimization, which is more like a trick for attack implementation.\n\n2. I would expect some black-box transferability analysis to demonstrate the effectiveness of this attack."
            },
            "questions": {
                "value": "See the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}