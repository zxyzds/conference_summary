{
    "id": "ReKWjKvkJE",
    "title": "Structure-Guided Large Language Models for Text-to-SQL Generation",
    "abstract": "Recent advancements in large language models (LLMs) have shown promise in bridging the gap between natural language queries and database management systems, enabling users to interact with databases without the background of SQL. However, LLMs often struggle to fully exploit and comprehend the user intention and complex structures of databases. Decomposition-based methods have been proposed to enhance the performance of LLMs on complex tasks, but decomposing SQL generation into subtasks is non-trivial due to the declarative structure of SQL syntax and the intricate connections between query concepts and database elements. In this paper, we propose a novel $\\textbf{S}$tructure $\\textbf{GU}$ided text-to-$\\textbf{SQL}$ framework ($\\textbf{SGU-SQL}$) that incorporates syntax-based prompting to enhance the SQL generation capabilities of LLMs. Specifically, SGU-SQL establishes structure-aware links between user queries and database schema and recursively decomposes the complex generation task using syntax-based prompting to guide LLMs in incrementally constructing target SQLs. Extensive experiments on two benchmark datasets demonstrate that SGU-SQL consistently outperforms state-of-the-art text-to-SQL baselines. These results highlight the importance of incorporating structural syntax information for effective text-to-SQL generation and pave the way for more robust and reliable interfaces to databases in the era of artificial intelligence.",
    "keywords": [
        "Text-to-SQL",
        "large language model",
        "structure learning"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ReKWjKvkJE",
    "pdf_link": "https://openreview.net/pdf?id=ReKWjKvkJE",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a method SGU-SQL, which represents user queries and databases into unified and structured graphs and employs a tailored structure-learning model to establish a connection between the user queries and the databases. The linked structure is then decomposed into sub-syntax trees, guiding the LLMs to generate the SQL query incrementally"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper provides interesting ideas for text-to-sql, such as graph representation of query & data schema, query schema linkage, syntax tree based decomposition with LLMs, etc.  Although these concepts are not brand new, it is interesting to see how authors leverage on them with graph + LLMs"
            },
            "weaknesses": {
                "value": "**1 Method writing should be clearer**\n\nIt is not very clear how does schema-linkage is used in SQL generation by decomposition (sec 3.1)\n\nHow is score eq21 used in your algorithm? \n\nminor: \n\nPRELIMINARIES has detailed equations, etc, however this section seems not help introduce the method, may confuse readers (training GNN? prompting?)\n\n\n**2. Performance is not competitive on more complex dataset like BIRD** \nTo measure the effectiveness of the method, we need to show competitive results on complex realistic dataset. \n\nTable 2 is BIRD performance (exe acc 61), which is lower than SOTA 74 https://bird-bench.github.io/\nSGU-SQL focus on leveraging structure-aware links between queries and database schema, so to prove effectiveness of the method, the method need to be tested on more challenging database, like BIRD. \n\nTable 1 Spider.  Many of the baseline methods are not Text-to-SQL specific method, therefore performance is not competitive. \nadditionally only show method is good on Spider is not enough.  \n\nideally test performance on test split by submitting to leaderboard for generation ability\n\n\n\n**3. need convincing ablation study to show each component designs is helpful**\n\nNeed ablation study on effectiveness of graph representation of query (sec 3.1) i.e.say compared with standard natural language representation without graph representation\n\nNeed ablation study on effectiveness of graph representation of database, compared with code representation \u201cCREATE TABLE (...)\u201d or serialized representation: table1: c1,c2 .. | table 2: c1, c2 \u2026 \n\nNeed convincing ablation qualitative and qualitative results on the effectiveness of  \u201cstructure-aware linking\u201d: effectiveness of DUAL GRAPH ENCODING(i.e. linking the query to the appropriate tables and columns in the database ). Since link query and database structure is one of key contribution, there should be clear ablation study\n\nNeed convincing ablation study to show \u201csyntax-based decomposition\u201d is effective. what if we do not do the \u201cDECOMPOSING QUERY WITH SYNTAX TREE\u201d (3.2.1), only directly generation SQL out. \nTable 3 compares with other basic decomposition approach. relatively simple prompt decomposition method can get on par text-to-sql performance \u200b\u200bhttps://arxiv.org/pdf/2312.11242 (86.75 vs 87.5)"
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "+ Identify the limitations of LLM-based Text-to-SQL models and introduce SGU-SQL, which\nleverages structural syntax information to improve SQL generation capabilities of LLMs.\n+ SGU-SQL proposes graph-based structure construction to comprehend user query and database\nstructure and then link query and database structure with dual-graph encoding.\n+ SGU-SQL introduces tailored structure-decomposed generation strategies to decompose queries with\nsyntax trees and then incrementally generate accurate SQL with LLM.\n+ Experiments on two benchmarks verify that SGU-SQL outperforms state-of-the-art baselines, including 11 fine-tuning models, 7 structure learning models, and 11 in-context learning models"
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "+ The article is quite neat and meticulous, from giving mathematical formulas and reasoning why to do so and also the evaluation steps, ablation study, the article has introduced a completely new method in text2sql which is syntax-based prompting, avoiding traditional classical methods such as FewShot and CoT.\n+ The method of the author group is quite new and very well implemented, the evaluation of the method is also on both open-source and closed source to provide an objective view.\n+ The novelty of the method lies in the ability to build a good enough graph structure to link from user queries and information in the database, thereby helping to reduce the input of LLM to some extent, the selected information is carefully filtered to help the generated query be of better quality."
            },
            "weaknesses": {
                "value": "Overall I found the article quite good and have no comments on weaknesses."
            },
            "questions": {
                "value": "+ Have the authors compared the current method with the traditional CoT or FewShot methods?\n+ What Lora rank does the author team use for fine-tuning? Have they tested it on multiple ranks?\n+ Nh\u00f3m t\u00e1c gi\u1ea3 c\u00f3 th\u1ec3 gi\u1ea3i th\u00edch th\u00eam v\u1ec1 c\u00e1ch training GNN, c\u00e1c th\u1eed nghi\u1ec7m v\u00e0 k\u1ebft qu\u1ea3 \u0111\u00e1nh gi\u00e1 v\u1edbi GNN c\u1ee7a b\u1ea1n?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 10
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper focuses on tackling the challenge of generating robust and reliable SQL for a given natural language question. The authors propose SGU-SQL which is a novel framework leveraging the structural relationship between entities in user questions and the database tables for solving Text to SQL tasks using LLMs. There are three main challenges that the authors focus on:\n1. Ambiguous user intents which lead to LLM's making uninformed decisions about SQL generation\n2. Complex database schemas which may lead the LLM to confuse between different columns or tables\n3. Complexity of SQL - LLMs are trained mainly for text generation whereas SQL is complex like programming language where only well-defined syntax must be used.\n\nThe authors tackle these issues by creating this framework where entities in natural language questions and the database are represented using graphs. These entities from questions to database are linked using a structure-linking model forming connections. Then, finally a prompt based method decomposes the complex question into subquestions and tackle these subquestions separately while aggregating them later. This method has been experimented with GPT-4 and a comparison has been reported using several models including SOTA from the leaderboard. Overall, the framework is proved to be successful while also achieving SOTA results on"
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. The paper focuses on one of the critical issues with LLMs for SQL generation which requires the LLMs to tackle them in a unique way where reliability and robustness are a strong focus.\n2. The framework is novel, intuitive and well designed to bridge the gap between an LLM which is tuned effectively for text generation and the SQL generation task which requires a structured understanding where the user queries can be ambiguous and answers can be complex.\n3. The paper demonstrates strong results on the most popular SQL datasets SPIDER and BIRD which certifies the potential of the approach.\n4. Well articulated work and not complex to follow from beginning to end. Conducted experiments including ablation studies are designed with thorough understanding.\n5. The real world applicability and potential of this framework can be very high and effective when tackling large production grade databases in industrial applications."
            },
            "weaknesses": {
                "value": "1. The framework is novel and intuitive but because of SPIDER not being the best representation of production grade SQL, it would also beneficial to probably test them using SPIDER-2. BIRD is somewhat closer to a real-world SQL but SPIDER-2 has large-scale and diverse databases which evaluates the proposal on a much better dataset revealing how the proposal works on large scale datasets.\n2. There's not much information provided on the future direction of this work which might hurt the significance/potential of the work a little. I'm mainly looking for how can this work be extended as it can provides opportunities for other researchers trying to explore in the same direction.\n3. Specific discussion on the model used for structure linking seem insufficient. Like what training methodology and architecture choices.\n4. When we discuss about constructing graphs and linking nodes, it may be important to learn about the complexities at a high level."
            },
            "questions": {
                "value": "1. In a real world, the databases are changed very often and there may be challenges where the proposed framework might not work same as proposed. In such cases, what components of the framework can still be relevant and what components must be changed is particularly industrial practitioners care about. Atleast including it in future directions might suffice.\n2. How does this framework vary for dialects other than SQL? Not a super important question to clarify for the purpose of this conference, but having information about this can be helpful for practitioners."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Recent text-to-SQL methods using LLMs encounter challenges in accurately interpreting user intent and navigating the complex structures of databases. Existing decomposition-based approaches to improve LLM performance on text-to-SQL tasks often struggle with SQL's declarative syntax and the intricate links between queries and database elements. This paper introduces Structure Guided text-to-SQL (SGU-SQL), a novel framework that integrates syntax-based prompting to enhance LLM-driven SQL generation. SGU-SQL incorporates structure-aware linking between user queries and database schemas and decomposes complex SQL generation tasks into manageable subtasks through syntax-based guidance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The experiments provide a comprehensive comparison with most of the previous works, showcasing that their approach outperforms all included baselines, although some recent approaches were not covered.\n\n2) The novel prompting strategy significantly enhanced model performance across different LLM families, achieving substantial improvements over naive few-shot prompting and chain-of-thought (CoT) methods.\n\n3) The paper presents an effective strategy for linking the structural elements of user queries to the database schema, facilitating more accurate SQL generation."
            },
            "weaknesses": {
                "value": "The primary limitation of this approach lies in the experimental section, where some of the most recent state-of-the-art methods, such as CHESS, Distillery, and CHASE-SQL, are not included. This omission is significant, as approaches like Distillery suggest that LLMs already demonstrate strong performance in handling complex schemas without the need for schema linking or query decomposition, challenging the findings of this paper. Additionally, as shown by CHASE-SQL, LLMs can achieve approximately 80% performance (compared to the 62% reported in this paper on BIRD) through enhanced sampling strategies, indicating that the main challenge in the text-to-SQL domain now lies in verifying the outputs of LLMs.\n\nFurthermore, crucial tables, such as Table 2 and Table 3, have been relegated to the appendix. These tables contain valuable data and should be included in the main body of the paper for better accessibility and understanding."
            },
            "questions": {
                "value": "NA."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}