{
    "id": "lCasyP21Bf",
    "title": "Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?",
    "abstract": "Vision and language model (VLM) decoders are currently the best-performing architectures on multimodal tasks. Next to predictions, they are able to produce natural language explanations, either in post-hoc or CoT settings. However, it is not clear to what extent they are using the input vision and text modalities when generating answers or explanations.\nIn this work, we investigate if VLMs rely on their input modalities differently when they produce explanations as opposed to answers.\nWe also evaluate the self-consistency of VLM decoders in both post-hoc and CoT explanation settings, by extending existing unimodal tests and measures to VLM decoders. We find that VLMs are less self-consistent than LLMs. Text contributions in VL decoders are more important than image contributions in all examined tasks. However, when comparing explanation generation to answer generation, the contributions of images are significantly stronger for generating explanations compared to answers. This difference is even larger in CoT compared to post-hoc explanations.\nLastly, we provide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE benchmark, which before was restricted to\nVL encoders. We find that VL decoders still struggle with most phenomena tested by VALSE.",
    "keywords": [
        "interpretability",
        "multimodality",
        "natural language explanations",
        "vision and language",
        "benchmarking"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We (1) measure how much VLMs use text and images when generating predictions or explanations, (2) evaluate their self-consistency in post-hoc and CoT explanation, (3) provide an update of state-of-the-art VL decoder accuracy on the VALSE benchmark.",
    "creation_date": "2024-09-21",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=lCasyP21Bf",
    "pdf_link": "https://openreview.net/pdf?id=lCasyP21Bf",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates how VLMs balance text and image data when generating answers and explanations, evaluating whether their reliance on each modality shifts depending on the task. Additionally, the authors measure the self-consistency of VLMs by comparing answers and explanations generated in both post-hoc and Chain-of-Thought settings. The results show that VLMs are less consistent than LLMs, with visual information playing a more significant role in explanations than in answer generation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This paper is well-motivated and shows interesting findings of VLM. I believe that the result is welcome and beneficial to the community.\n- The paper is overall well-written.\n- The experimental results are extensive, including various datasets (VQA, GQA, Foi1It, MSCOCO, VALSE) in a wide range of tasks."
            },
            "weaknesses": {
                "value": "- The main technical contribution is limited since the proposed method is heavily based on MM-SHAP and CC-SHAP but applies to VL decoders instead of encoders.\n- Only evaluated on LLaVA-based models, missing baselines such as CogVLM."
            },
            "questions": {
                "value": "- Please refer to the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper conducts a study on how the inputs to an autoregressive vision-language model (VLM) influence the outputs. Mainly the paper compares the influence of textual and visual input tokens on the output answer and explanation, evaluates the faithfulness of explanations using controlled modifications of the input. The paper studies three open-weight models: BakLLaVA, LLaVA-Mistral, and LLaVA-Vicuna. The methods of the analyses are adapted from prior work on interpreting LMs and encoder VLMs. The results show, among other things, that textual inputs influence the answer more than visual inputs, but that the influence of visual inputs increases when looking at influence on generated explanations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Interesting findings regarding interpretability of VLM decoders. These are the main qualitative findings: text input is more influential than visual input for answer prediction, visual input is more influential for explanation generation than for answer prediction, the contributions from each modality are different in answer prediction and explanation generation\n- Evaluated on 3 models and 3+ datasets"
            },
            "weaknesses": {
                "value": "- Methods are essentially the same as those from prior work\n- The takeaways from the experiments with perturbed inputs seem unclear. Lines 471-472 seem to suggest that the results from the edit-based tests are hard to interpret: \"However, generative tasks require semantic evaluation to determine if the output remains consistent\".\n- The paper does not explain how CC-Shap works (page 4) -- how are the values computed?"
            },
            "questions": {
                "value": "- Lines 357-358 say: \"The large difference between overall higher accr and lower acc results suggests that VL decoders rely on linguistic priors to solve VALSE\" - isn't this also because acc is a harder metric? How do you tease those two issues apart?\n- What do you view as the main takeaway(s) from the edit-based tests?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates how Vision-Language Models use image and text modalities in generating predictions and explanations. It studies whether VLMs rely more on visual or textual inputs, and also studies to what extent these models are self-consistent when providing explanations. The paper found that generally, LLM is more reliable than VLMs and text contributes more in VL decoders compared to image. And the effect is even larger with CoT explanations included. The paper also provided the first  benchmarking of state-of-the-art VL decoders on the VALSE benchmark."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper identifies and addresses a gap of previous works. Previous works only studies encoder while this work provides a study of the decoder on their multi-modal degree and the consistency in their self-explanation.\n2. The paper found that VLMs use text more than image, and the gap is larger with CoT included.\n3. The paper provides an benchmarking of state-of-the-art VL decoders on VALSE, which has previously only focused on encoders."
            },
            "weaknesses": {
                "value": "1. The paper primarily uses existing techniques (e.g., MM-SHAP and CC-SHAP) and the main contribution is applying these metrics to VLM decoders. This adaptation is incremental.\n2. The writing is unclear and needs to be improved. It would be useful to provide more details on the Shapley values and the computation."
            },
            "questions": {
                "value": "1. The results show that text is more significantly used compared to image for answer generation. Is it possible that this is a result of dataset biases or limitations in the MM-SHAP metric\u2019s rather than image/text contributions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work aims to answer an important research question - Do VLM-generated responses (and self-explanations) rely more on images or text?\"\n\nBy extending methods from VL and LLM research, the authors propose MM-SHAP to measure the contribution of individual tokens to model predictions in order to explore the multimodal degree of autoregressive VLM under different tasks. Their findings are quite interesting and inspiring for future VLM works."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This work aims to explore an important research question that is well-motivated. The three findings presented are all very interesting. \n- The approach is clearly presented and reasonable. \n- Comprehensively experiments and analysis."
            },
            "weaknesses": {
                "value": "- This work claims the scope as \u201call decoder VLMs,\u201d yet all experiments are based on llava-based models. I agree with the statement in line 108 that these models (e.g. llava, miniGPT-4, blip2, Otter, etc) share similar high-level ideas; however, do their subtle differences in design lead to changes in the results/conclusions?\n- The discussion section lacks in-depth analysis and exploration of the root causes of the observation. In particular, each paragraph in Section 4.4 reveals some interesting phenomena (for example, for CoT ability, VLM is clearly worse than that of LLM in self-consistency) but lacks a deeper discussion of the underlying reason. That being said, this is still a good finding paper."
            },
            "questions": {
                "value": "As mentioned in the Weaknesses section, do you think/have found your conclusions can be applied to all decoder VLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies the degree to which VLMs use image or text modalities more in different contexts. The paper does a good job in extending established measures like MM-SHAP to decoder models and evaluating self-consistency of decoders in post-hoc and CoT with CC-SHAP. The authors do an extensive evaluation across 12 datasets with multiple answer formats. \n\nThe papers main contribution include:\n1) Benchmarks VLMs on VALSE.\n2) VL decoders are more text-centric compared to VL encoders.\n3) VLMs are less self-consistent than LLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1) Extending proven measures for VL Encoders to Decoders. \n2) Provides robust evidence about how VLMs rely on different modalities unequally, by evaluating extensively across 12 datasets."
            },
            "weaknesses": {
                "value": "1) The paper evaluates only on LLaVA based open-source models. Could the authors evaluate on mPLUG-Owl2 or CogVLM or other non-LLaVA based VLMs as it would be ideal to see the approach work on a different architecture and with different pre-training data?"
            },
            "questions": {
                "value": "Please refer to the weakness section to answer questions about the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}