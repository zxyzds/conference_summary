{
    "id": "JDm7oIcx4Y",
    "title": "Accelerated training through iterative gradient propagation along the residual path",
    "abstract": "Despite being the cornerstone of deep learning, backpropagation is criticized for its inherent sequentiality, which can limit the scalability of very deep models.\nSuch models faced convergence issues due to vanishing gradient, later resolved using residual connections. Variants of these are now widely used in modern architectures.\nHowever, the computational cost of backpropagation remains a major burden, accounting for most of the training time.\nTaking advantage of residual-like architectural designs, we introduce Highway backpropagation, a parallelizable iterative algorithm that approximates backpropagation, by alternatively i) accumulating the gradient estimates along the residual path, and ii) backpropagating them through every layer in parallel. This algorithm is naturally derived from a decomposition of the gradient as the sum of gradients flowing through all paths, and is adaptable to a diverse set of common architectures, ranging from ResNets and Transformers to recurrent neural networks.\nThrough an extensive empirical study on a large selection of tasks and models, we evaluate Highway-BP and show that major speedups can be achieved with minimal performance degradation.",
    "keywords": [
        "optimization",
        "efficient training"
    ],
    "primary_area": "optimization",
    "TLDR": "We propose Highway backpropagation, a parallelizable algorithm that accelerates training by leveraging residual connections, achieving significant speedups with minimal performance loss across various architectures.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=JDm7oIcx4Y",
    "pdf_link": "https://openreview.net/pdf?id=JDm7oIcx4Y",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a method to accelerate the backward pass of backpropagation through parallelization of the gradient calculation in deep neural networks with residual-like architecture. This is done by decomposing the gradient into a sum of terms, where the k-th term is the sum of all gradients passing through at most k blocks which are not a residual (i.e. skip-like) connection. By parallelizing this computation, an acceleration is achieved as long as the maximal k we compute is not too large (i.e., we need to drop values of k above some threshold). It is shown empirically that k=5 is approximately enough for good accuracy on several (small) benchmarks, while still achieving significant acceleration."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The problem of accelerating Backprop is important.\n\n2. The idea is novel, original, and quite interesting.\n\n3. The empirical results are promising.\n\n4. The presentation is mostly clear."
            },
            "weaknesses": {
                "value": "1. The main issue is the scale of the experiments, which is rather small (e.g., no ImageNet). It is not clear to me that a small k would be enough to get both high accuracy and acceleration on more complicated problems than those shown in this paper. \n\n2. The acceleration is only relevant for the backward pass, which is roughly a third of the total time of Backprop (which includes the forward pass, backward pass, and parameter gradient) and so this limits the overall benefit of this method."
            },
            "questions": {
                "value": "3. I think there are some small mistakes in the math. For example, in eq. 8: \n\n$\\quad$ a) The transition to eq. 8 from eq. 6 is not clear. Specifically, why did $w_{i}^{0}$ disappear?\n\n$\\quad$ b) The right side of eq. 8 seems different from the middle part of the equation, since $v_{i}^{k+1}$ is multiplied by $K_{i+1}$ in the middle part of the equation, but not in the right side of the equation.\n\nCan the authors please check these?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Highway backpropagation (Highway-BP), an algorithm that speeds up training of deep neural networks by approximating traditional backpropagation using parallelizable iterative methods. It leverages residual connections to propagate gradients efficiently and is adaptable to various architectures like ResNets and Transformers. Empirical studies show that Highway-BP achieves significant speedups with minimal performance loss, making it a promising method for accelerating deep learning model training."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Parallelization: The algorithm is designed to be parallelizable, which can significantly reduce training time, especially for very deep models or large-scale problems.\nArchitecture Flexibility: Highway-BP is adaptable to a variety of common neural network architectures, making it a versatile tool for different applications.\nTheoretical Foundation: The paper is grounded in mathematical theorems that provide a theoretical foundation for the proposed method."
            },
            "weaknesses": {
                "value": "Generalization to All Models: Although Highway-BP is adaptable to many architectures, there may be specific models or scenarios where it does not perform as well as traditional backpropagation or other optimization methods. For example, experiments on Graph neural network, spiking neural network and mamba.\nHyperparameter Tuning: The algorithm introduces a new hyperparameter (the number of iterations, k), which requires tuning and may lead to different optimal values depending on the model and task. This can add complexity to the training process. A ablation on how to tune the parameter should be included.\nMissing baselines: The paper compares Highway-BP with backpropagation and fixed-point iteration but may not fully address how it stacks up against other state-of-the-art optimization techniques. Here are some papers authors should include in their baseline:\nHuang, Kai, et al. \"Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation.\" arXiv preprint arXiv:2309.13192 (2023).\nWang, Ziteng, Jianfei Chen, and Jun Zhu. \"Efficient Backpropagation with Variance-Controlled Adaptive Sampling.\" arXiv preprint arXiv:2402.17227 (2024).\nYang, Yuedong, et al. \"Efficient low-rank backpropagation for vision transformer adaptation.\" Advances in Neural Information Processing Systems 36 (2024).\nScalability to Distributed Settings: The paper mentions the potential for distributed training but does not provide empirical results or a detailed discussion on how Highway-BP would perform in a distributed setting."
            },
            "questions": {
                "value": "How does Highway-BP compare with other advanced optimization techniques, especially in terms of convergence speed and final model accuracy?\nWhat are the performance implications of Highway-BP when scaling to models with billions of parameters, and how does it handle memory and computational constraints?\nCan the authors provide more theoretical analysis or proofs regarding the convergence properties of Highway-BP compared to traditional backpropagation? For example, give a lower bound for the improvement."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper considers gradient computation in network architectures with multiple residual connections. The authors propose the Highway-BP method to improve the efficiency of gradient computation/estimation by decoupling and rearranging multiple computational paths in vanilla BP. The proposed algorithm leverages the fact that computational burdens differ between the two paths within each block and iteratively approximates the gradient from easier to more challenging paths, allowing trade-offs between accuracy and efficiency through adjustable iteration steps. Numerical results indicate that the approximate gradient, with a certain number of iterations, achieves comparable results to the true gradient, demonstrating its potential applicability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The authors present an interesting idea for improving the sequential computation in BP, utilizing the shared structural characteristics of mainstream models, which may facilitate further refined explorations in the future. The experimental results in the paper are comprehensive, demonstrating potential advantages in terms of efficiency."
            },
            "weaknesses": {
                "value": "My main concern is that unrolling the recursive computation in vanilla BP requires storing a significant number of additional variables, thereby increasing the algorithm\u2019s storage demands. The authors do not appear to provide test results for memory consumption, which is an important consideration when developing algorithms for large-scale models. From the results in Table 3, the proposed algorithm shows a higher time overhead than the baseline, and with only 20 iterations, the time consumption is nearly equivalent to that of vanilla BP, indirectly suggesting that the additional read/write overhead may not be insignificant.\n\nThe efficiency of the proposed algorithm lacks theoretical guarantees, and the experimental results have minor flaws. Please see the question part for details. The comparison includes only the fixed-point iteration algorithm. Algorithms mentioned in the literature and other approximation-based acceleration methods for BP are not included."
            },
            "questions": {
                "value": "- The authors should provide a more detailed experimental setup in the paper. In Figure 2, the final performance of vanilla BP appears to differ significantly from its usual training outcomes. Might this be due to insufficient training? As observed in Figure 3, the effectiveness of Highway-BP's approximation seems limited to the relatively easy early stages of training, suggesting that the accuracy results in Figure 2 may need to be presented at multiple time points.\n\n- The paper includes empirical tests of the algorithm, but could the authors provide a theoretical analysis of how the approximation error varies with $k$? Alternatively, could a more rigorous boundary be identified for the types of network structures where this approximation applies? Additionally, conducting a comprehensive analysis of the time and space complexity of Highway-BP is also worthwhile and important.\n\n- Since the paper aims to alleviate the sequentiality in BP, which is a challenge that typically arises in large models, could the authors evaluate whether the complex computation of Highway-BP might hinder the application of other crucial techniques in this area, such as data parallelism or distributed training? Furthermore, how might these structural changes to gradient computation impact communication and read/write overhead?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Summary: This paper proposes Highway Backpropagation, an algorithm for accelerating the training of neural nets with residual connections (such as ResNets, RNNs, and Transformers) by skipping some of the expensive backpropagation steps. In a nutshell, backpropagating a gradient through a layer consisting of a weight matrix plus a residual connection consists of a \u2018cheap\u2019 part (the residual) and an expensive part (the weight matrix), and the total gradient can be represented as the sum of all combinations of going through either. By omitting some of the terms of that summation - specifically those that have more than $k$ \u2018expensive\u2019 components, the gradient calculation can be sped up. The authors address the practical concerns of the implementation and provide experimental results validating their setup."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper tackles a crucially important problem, which is the enormous cumulative expense of training machine learning models, especially in the world of transformers. The idea seems to be quite original, and makes a great deal of intuitive sense. It is also straightforward to implement.\n\nOverall, the main body of the paper is generally well-written and lays out its argument in a logical and even exciting way. The mathematical argument is very clearly presented. Further, the practical considerations of creating an implementation with real-world speedups are addressed.\n\nThe training dynamics section is nicely presented and interesting."
            },
            "weaknesses": {
                "value": "### Main weaknesses:\n\nThe main weakness of the paper is the experimental section. While the method is clever, the real value of the idea is in whether it is actually useful, and unfortunately the experimental section does not sufficiently cover this point. Of course, we cannot expect a finely optimized implementation in a research work. However, this reviewer doesn\u2019t see a reason that a basic, workable implementation couldn\u2019t have been built for standard architectures (e.g., ResNets) or something much closer to them. Because of the authors\u2019 use of these nonstandard implementations, at least some of the actual numerical results are very far below what could be expected.\n\nFor instance, for CIFAR100, a standard ResNet50 architecture would have an accuracy of over 85%; the 50-layer architecture used in the paper (albeit with about a quarter of the total parameters of a RN50)  has an accuracy of 40%. A ResNet20, which has far fewer parameters, should still be able to get something like 70-75%. The authors do not address or justify this gap.\n\nLikewise, real-world speedup numbers are not presented. Again, the point regarding the practical difficulty of creating a CUDA-optimal distributed setup is well taken, but the question remains unanswered. Perhaps it may have been possible to create a less-optimized standard implementation of backpropagation (e.g., on CPU) and compare this to the proposed one.\n\nFinally, there is no guidance for selecting the right $k$ for a new task. This omission makes it difficult to use Highway Backpropagation in practice, even if an efficient implementation is available. \n\n### Smaller issues:\n\nThe writing style of the introduction/related work sections is often vague or confusing.\n* Like 36: what is meant by \u201ctransformers only defer the problem\u201d?\n* Line 41: What is meant by \u201coften involve trade-offs between speed and task performance\u201d? Aren\u2019t these tradeoffs always involved?\n* Line 42: What is meant by \u201cleverage the recent layers\u201d?\n* Line 45: What is meant by \u201csignificantly improves optimization\u201d? Can vague statements like this be made more precise?\n* Line 51: What is meant by \u201cderived from an original derivation\u201d?\n* Line 75: \u201cwhere gradients can either diminish or grow uncontrollably\u201d is completely redundant with line 73.\n* Line 92: \u201cWhile this seems attractive, the reality differs\u201d - can this be made more clear and precise?\n* Line 93: \u201ccomputing the Jacobian matrix of all layers\u201d: this is misleading, since the proposed Highway-BP algorithm also requires this computation.\n\nLine 192 typo provie -> provide\n\nLine 231 typo $K$ -> $k$"
            },
            "questions": {
                "value": "* Why is the CIFAR-10 and CIFAR-100 model accuracy so low for the chosen architectures, or, conversely, why were architectures with such low accuracy chosen?\n\n* Especially for image classification, why was this method not tested in the most standard setups, for instance Imagenet-1K on a standard ResNet50? Admittedly, in a standard RN50 architecture, the residual layer spans several weight matrices, but this seems like a straightforward extension of the proposed method. For the architectures chosen, why is the number of parameters much lower than that of the standard models (for instance, a traditional ResNet30 would have about 25M parameters, versus this paper\u2019s 4.1M)? What is meant in line 317 by \u201cstandard ResNet\u201d?\n\n* In line 367, what is meant by \u201cvery reasonable performances\u201d? \n\n* Is there an advantage to varying $k$ during the training process?\n\n* Why are the intermediate losses necessary for the theoretical framework to be effective?   Likewise, why is the proposed algorithm restricted to generative and classification tasks (liNE 153)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new algorithm for approximating backpropagation in deep sequential neural network models: Highway Backpropagation (Highway-BP)\n\nMotivation: Backprop is computationally expensive, and has sequential dependence in depth making it hard to parallelize.\nKey idea: Highway-BP leverages residual connections in modern architectures to decompose the gradient into paths of different lengths. It then iteratively computes an approximation of the gradient. Pruning some of the gradient paths enables Highway-BP to compute the approximate backprop in parallel. Doing this iteratively allows the approximate gradient to approach the true gradient over iterations.\n\n\n\n\nI really like this paper. In my head I'm drawing a parallel with the \"Residual Networks Behave Like Ensembles of Relatively Shallow Networks\" paper. In Figure 1 from that paper, they \"unravel\" the residual connection of an N=3 block network.\nWhen I read your paper, I immediately drew a parallel with that paper where, a view of your method, is that you only take backprop thru the right diagonal portion of the \"unraveled\" representation of Figure 1(b) of the \"Residual Networks Behave Like Ensembles of Relatively Shallow Networks\" paper.\nIts brilliant!\n(I am not an author of that paper :) )"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "(1) Flexibility: Can be applied to various architectures like ResNets, Transformers, and RNNs. \n\n(1.5) Experiments: Tested on image classification (CIFAR10/100) and language modeling (Wikitext103) tasks. Compared against standard backpropagation and fixed-point iteration (for RNNs). Achieved comparable performance to backpropagation with fewer iterations (k \u2264 5 in most cases). Showed consistent speedups (2x-4x) over standard backpropagation.\n\n(2) Paper presents a tradeoff frontier: Number of iterations (k) controls the trade-off between speed and accuracy\n\nEdit:\n(3) The parallelism unlocked by this method can greatly accelerate training speeds producing more efficient and faster training setups. The fact that it generalizes to a lot of settings makes this paper :chefs-kiss:\n(see Appendix C for potential speedup wins (but this could be better presented in the main body))"
            },
            "weaknesses": {
                "value": "This proposes a way to accelerate training at the cost of model performance\nThe true cost of AI is inference NOT training. I'd pay more for training if it produced a better model (and therefore made TCO cheaper)...\n\nThe results obviously show that the using the method produces worse models\n\n\nI'll be the echo chamber (somebody has gotta do it): I want to see this applied to large scale autoregressive LLMs. Show me LLaMa training or I don't care! \n\\s"
            },
            "questions": {
                "value": "The results obviously show that the using the method produces worse models.\nHave you looked at methods for recovering performance?\nHave you tried increasing k over training?\nIn figure 3, I can see a schedule in which, for the first 500 steps, you use k=1, then for the next 500 steps, you use k=2, then for the next 1000 steps, you use k=5, then eventually you use backprop a the end of training to fully recover performance.\n\n\nEdit:\nCan Figure 3 be redone but with training time on the x-axis (instead of time)? Appendix C shows that your method will look more favorable when this is done."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}