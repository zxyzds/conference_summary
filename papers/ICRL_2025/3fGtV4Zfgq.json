{
    "id": "3fGtV4Zfgq",
    "title": "Fast training and sampling of Restricted Boltzmann Machines",
    "abstract": "Restricted Boltzmann Machines (RBMs) are effective tools for modeling complex systems and deriving insights from data. However, training these models with highly structured data presents significant challenges due to the slow mixing characteristics of Markov Chain Monte Carlo (MCMC) processes. In this study, we build upon recent theoretical advancements in RBM training, focusing on the gradual encoding of data patterns into singular vectors of the coupling matrix, to significantly reduce the computational cost of training (in very clustered datasets) and evaluating and sampling in RBMs in general.  The learning process is analogous to thermodynamic continuous phase transitions observed in ferromagnetic models, where new modes in the probability measure emerge in a continuous manner. Such continuous transitions are associated with the critical slowdown effect, which adversely affects the accuracy of gradient estimates, particularly during the initial stages of training with clustered data. To mitigate this issue, we propose a pre-training phase that encodes the principal components into a low-rank RBM through a convex optimization process. This approach facilitates efficient static Monte Carlo sampling and accurate computation of the partition function. Furthermore, we exploit the continuous and smooth nature of the parameter annealing trajectory to achieve reliable and computationally efficient log-likelihood estimations, enabling online assessment during the training process, and to propose an novel sampling strategy termed parallel trajectory tempering that outperforms previously optimized MCMC methods.\nOur results demonstrate that this innovative training strategy enables RBMs to effectively address highly structured datasets that conventional methods struggle with. Additionally, we provide evidence that our log-likelihood estimation is more accurate than traditional, more computationally intensive approaches in controlled scenarios. Moreover, the parallel trajectory tempering algorithm significantly accelerates MCMC processes compared to existing and conventional methods.",
    "keywords": [
        "Restricted Boltzmann Machine",
        "Fast Sampling",
        "structured data learning",
        "training algorithm"
    ],
    "primary_area": "generative models",
    "TLDR": "We design a new training pipeline for the Restricted Boltzmann Machine, capable of training quickly and accurately on hard multimodal dataset.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=3fGtV4Zfgq",
    "pdf_link": "https://openreview.net/pdf?id=3fGtV4Zfgq",
    "comments": [
        {
            "title": {
                "value": "Answer to the weaknesses and questions (part 2/2)"
            },
            "comment": {
                "value": "**Weaknesses**\n\n5. We are unsure if we fully understand the reviewer's comment. Calculating the exact log-likelihood is only feasible for a small number of hidden or visible nodes, as it requires an exhaustive enumeration of all $2^{\\mathrm{min}(N_v,N_h)}$ possible states. Since this count grows exponentially, even modest increases in $\\mathrm{min}(N_v,N_h)$ quickly make exact calculations infeasible, regardless of computer advances over the past 15 years. For larger RBMs, as shown in Fig. 2, we can estimate the log-likelihood; however, these estimates can only be compared to other approximate methods rather than exact values. Figure 3 illustrates a comparison with exact values, which is possible only when the full enumeration is computationally feasible. This approach of validating approximate methods with exact values where possible is common practice in papers introducing new techniques for estimating the partition function. We would be grateful if the reviewer could share any specific types of analysis they had in mind.\n\n\n**Questions**\n\n  1. We apply it to CelebA $N_v=32^2$ and $N_h=500$, how large should we go ?\n  2. The problem in general came from numerical issue rather than from the method. We observed that when doing the projection in the low-dimensional space, if part of the dataset lied on the border of the domain, there can be convergence issue, mainly due to the saturation of the hyperbolic tangent. Otherwise if the discretization step is excessively large, it may lead to the appearance of spurious probability modes that can't be detected, and some data clusters might be overlooked.\n  3. Basically yes, you need to have several machines over the learning trajectory to sample."
            }
        },
        {
            "title": {
                "value": "Answer to the weaknesses and questions (part 1/2)"
            },
            "comment": {
                "value": "**Weaknesses**\n  1. We appreciate the reviewer\u2019s feedback on the accessibility of our paper. We know how important it is to make our work as understandable as possible for a wide audience. Therefore, we have included an appendix to explain the definitions and concepts from phase transition theory. We are happy to provide further explanation if there are particular terms or sections that could benefit from additional context. Although our work contains elements from the field of statistical physics, we believe that it also makes an important contribution to the fields of computer science and machine learning by providing a practical algorithm for training, evaluating, and sampling Restricted Boltzmann Machines that has numerous applications.\n\n\n  2. We appreciate the reviewer\u2019s comments on the term \"highly structured dataset\". While there is no precise mathematical definition, we would like to clarify what we mean by this. By \"highly structured\" we refer to datasets that exhibit certain notable characteristics: (i) the presence of visible and well-separated clusters in PCA projections and (ii) the difficulty Monte Carlo methods have in jumping between isolated clusters, often due to excessively long mixing times. As for \"unstructured\" datasets, we show that our method is effective on these as well, although it does not offer much advantage for long training times, since PCD has difficulty thermalizing both with and without pre-training, rendering pre-training mostly useless for long training times. Fig. 15 shows the training of the full MNIST dataset, where the pre-training becomes useless (in terms of matching log-likelihoods) very early in the training, from $10^3$ updates. Fig. 15 illustrates the training of RBMs on the full MNIST dataset, where the advantage of pre-training diminishes as early as $10^3$ updates, making it ineffective in terms of matching log-likelihoods beyond this point. In contrast, for the MNIST 0-1 subset shown in Figure 2 (top), the pretrained model consistently outperforms the standard PCD model in terms of log-likelihood and in a proper balance of the different peaks of the histogram of the projected generated data. For the CelebA dataset (Figure 2), the pre-trained RBM also reaches a higher log-likelihood than the standard model, but both approaches seem to converge toward similar values over time. \n\n  3. Our paper focuses on the sampling problems that highly structured datasets pose when training and evaluating RBMs, and suggests strategies to overcome these problems. The long introduction focuses on both reviewing previous work and explaining the physical reason why many of these sampling strategies may fail at the different training stages. Based on these conclusions, we propose a more appropriate training and sampling strategy as well as a new method to compute log-likelihood during training. We want to stress that while  Monte Carlo is an easy method to implement, in many cases, like in RBMs, it is extremely difficult to implement and control it correctly. While we do not believe that pre-training is the main contribution of our work (we rather think that it is the trajectory AIS measure of log-likelihood or the PTT), we have made innovations that are necessary to make the mapping of D&F 2021 usable in real datasets, which we present on page 5. We can try to make them clearer.\n\n  4. The work D&F 2021 propose a rather theoretical setting to learn a low-rank RBM and it is tested only on very simple, low-dimensional synthetic data sets with specific modes and regular features to cover the low-dimensional space, and mainly focuses on theoretical aspects. The first added value of the present work is to move from theory to practice, where many details need to be specified to make the technique work for arbitrary data. In this previous work, the algorithm was tested up to only 2 constrained directions. In our construction, we reach 4 constrained dimension thanks to including the possibility of adding a trainable bias, which also turns out to be crucial to train decent low-rank RBMs with image data. Moreover, D&F's work never controlled the quality of the low-rank RBM generated samples but only that of the low-rank Coulomb machines. It turned out that when dealing with real data, one needs to carefully correct the entropy term to ensure that true RBM equilibrium configurations can be obtained by the static Monte Carlo procedure, which is crucial to sample fast the trained machines, but also to properly train the low-rank RBMs. We will present these improvements in more detail in the final version."
            }
        },
        {
            "summary": {
                "value": "The paper discusses approximations to train a restricted Boltzmann machine (RBM). The first is to pre-train the RBM by fitting a constrained (low-rank) form of the RBM to the low-dimensional PCA space of the data. This can help with finding a good initial solution. After this various MCMC approaches are considered to continue training."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "RBMs are an important model and finding appropriate ways to train them is a topic of significant interest. The paper highlights the phenomenon of critical slowing down and how pre-training the model with a low-rank approximation of the parameter matrix can help the model overcome some of the slowing down effects."
            },
            "weaknesses": {
                "value": "The paper suffers from a lack of clarity of presentation and lack of clarity of novelty.\n\nThe paper mentions that the idea of a low-rank approach has already been used by others and it's unclear to me what novelty there is in any of the sampling approaches used after the pre-training phase.\n\nIn terms of presentation, there are notational inconsistencies and a general lack of clarity in terms of the main ideas. Fundamentally the approach of fitting a constrained model seems straightforward and indeed I believe there is a simple way to compute the projected distribution in the PCA space (using the Fourier integral representation of the Dirac delta function) which the authors do not discuss."
            },
            "questions": {
                "value": "*** introduction\n\nWhilst the RBM is well known, it would be helpful I feel for a reader to have the definition of the model earlier in the text. It currently isn't defined until near the end of page 4. Please introduce the RBM formally earlier in the text.\n\nNotation: inconsistent use of $N_v$ and $N_{\\text{v}}$ throughout, similarly for $N_h$.\n\nEquation 1: it might be better to write W_{i\\alpha}, rather than w_{i\\alpha} since w is used later for the \"singular values\".\n\n*** page 2\n\nFigure 1 isn't very easy to parse. For example the panel on race is placed more in the Mickey column than the human genome column.\n\n*** page 5\n\nPlease clarify the difference between \"model averages\" and \"observable averages\" and the difference between using N_s independent MCMC processes and R parallel chains.\n\nPlease clarify for the reader the meaning of <v_ih_a>_D\n\nSection 4: It is not correct that it is possible to train \"exactly\" an RBM with a reduced number of modes. Approximations are required, as explained in the supplementary material.\n\nPlease state what the free parameters to learn are in equation 3. If u and \\bar{u} are the singular directions, then the free parameters would be w_\\alpha? \n\nIn general I found the description of the low-rank approach unclear and this important section needs work to make it simpler and more clear to the reader.\n\nFor figure 14 it would be useful to show the distribution of the PCA projected data to see how well the RBM matches the projected data distribution.\n\nIt's unclear to me what contribution the authors are claiming to make. They state that the learning of the low rank parameterisation of W has been done before. Please clarify what the contributions of the paper are.\n\n\n*** Section 5\n\nI find it hard to follow why the authors are considering different sampling schemes and therefore what the aim of this section is. I presume this is considering alternative sampling approaches after the low-rank pre-training has been applied. However, I struggle to follow a clear recommendation or conclusion as to which method might be more suitable.\n\n*** Section 6\n\nIn the conclusion the authors claim to have introduced a method that enables \"precise computation of log-likelihood\". I cannot see anything in the main text that relates to this. There is no experiment I can see that measures the quality of the log-likelihood approximation. Please give some evidence to support this assertion.\n\n\n*** Supplementary material\n\nThe use of the term \"mode\" isn't very clear. The phrasing suggests that the first d modes of the maximum likelihood trained RBM should correspond to the d \"modes\" of the PCA solution. I'm not sure I know what this means. What are modes of a PCA solution?\n\nThe notation \\hat{u} is confused with \\bar{u}.\n\nWhy use $w$ here whereas $W$ is used in the main text?\n\nThe derivation is quite confusing. For example the dependence on \\bar{u} in equation 7 disappears without explanation. Indeed \\bar{u} seems to be never properly defined.\n\nPlease state clearly what are the parameters of the model that are being learned.\n\nSection A.2. The claim as before of exact training is incorrectly made here.\n\nThe notation in equation 20 is confusing, such as w_{\\alpha,a}=\\sum_i w_{ia}u_{i\\alpha} -- are arabic and latin indices meant to indicate referencing a different entity, even though both objects are labelled w?\n\nIn general I find the supplementary material confusing. I believe it is trying to fit an RBM projected to the d-dimensional subspace defined by PCA of the data to the empirical data distribution in that same subspace. However, approximations are clearly required in order to compute the projected RBM distribution. Given that, for a very low dimension d then one can easily discretise the model and carry out a simple maximum likelihood fit. If that is what is being done, it is not well explained and rather misleading (since this requires approximations itself).\n\nAn alternative (and standard) way to compute the marginal p(m) is to use the integral (Fourier) representation of the Dirac delta function. This means that the summation over v can be then carried out exactly, leaving only a d-dimensional integral to exactly compute p(m). This can also be carried out using discretisation for small d. The authors are (as I can understand) also using discretised integrals, so I'm unclear why they don't employ the standard Fourier Delta representation approach to compute p(m) -- this would seem to involve less approximations that the approach the authors consider."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The claimed novelties of this work are twofold. \nFirst, this paper proposes low-ranking training of RBMs by directly encoding the principal components throughout a convex-optimization process. This pre-training component proves to be very efficient when data are particularly clustered. In such cases, target densities are highly multimodal, and the model struggles to \"discover\"all the modes from scratch during training without the pre-training phase. This autonomous discovery of new modes is often associated with second-order phase transitions, similar to systems from statistical mechanics, where critical slowing down prevents the discovery of all modes in finite time efficiently. \n\nAs a second contribution, the paper also investigates how to use a variation of parallel tempering (PT) algorithms, termed parallel trajectory tempering, to sample more efficiently and obtain log-likelihoods estimates. In simple terms, parallel trajectory tempering (PTT) essentially relies on the same idea of parallel tempering of swapping between models at different temperatures using the Metropolis rule (and therefore retaining detailed balance). However, differently from PT, PTT swaps a full set of parameters $\\Theta^t$ instead of the temperature $\\beta$ only. In that sense, it can be thought of as a generalization of PT. \n\nNumerical experiments in Fig. 2 prove the pre-trained low-rank RBM to be more capable of identifying all modes in highly clustered data, while Figs. 3-4 show that PTT allows more accurate loglikelihood estimation and faster yet more efficient sampling from all modes of distribution compared to standard alternate Gibbs sampling (AGS)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow. \n- It represents a pleasant read that is accessible to a broad audience. \n- The literature review and related work section read well and are exhaustive.\n- The idea of pre-training the RBM to encode the principal components is simple yet very effective. \n- Leveraging the analogy between critical slowing down and the struggle of RBM during training to be ergodic and discovering all modes of the distributions is elegant and intuitive (though I suppose this is not a novelty of this paper, it is very nicely pictured in the introduction). \n- The numerical experiments look solid and aligned with the theoretical insights given in the main text. \n- I have not thoroughly checked the mathematical details in the appendix, but at first glance, they look good."
            },
            "weaknesses": {
                "value": "- I find it a bit challenging to identify the two main contributions in the paper as those are totally disentangled in their presentation between Sec. 4 and Sec. 5.2. I strongly recommend adding a list of bullet points at the end of section 1 to clearly list the contributions of work and crossref to the corresponding point in the paper. This would substantially help navigate the paper.  \n- I find that the structure of sections 5.2 and 5.2.1 can be improved. In particular, I find it confusing that Parallel Trajectory Tempering is introduced in section 5.2, and Parallel Tempering approaches are discussed in section 5.2.1. I find this logically inefficient as I believe that a more natural yet easier-to-follow flow would be to first introduce Parallel Tempering approaches and then explain what makes PTT different compared to existing approaches from the literature. As this is one fundamental contribution of this work I believe it is crucial to rework these sections such that the actual novelty emerges more clearly from the discussion. \n- The discussion around eq. (4) is rather crucial for the paper as it represents one of the main contributions of this work. Currently, the novelty with respect to Decelle and Furtlehner (2021a) is not very clear to me, and I would appreciate it if the authors could elaborate more on this. Moreover, what's the intuition behind the \"magnetizations\" along each of the singular vectors? Is there any correspondence with the magnetization as a physical observable? As far as I understand, those should be the projections along the unitary vectors of the visible variable. Is that correct? If all my understanding is correct, then the new contribution of this work is to use a bias initialization along a direction $\\boldsymbol{u}_0$, which augments the dimensionality of the system by one in the bias direction. If all above is still all correct, I wonder the following:\n    - How beneficial is to have such an augmented direction for the bias compared to the naive approach proposed in Decelle and Furtlehner (2021a)?\n    - Have the authors conducted any ablation studies to compare the differences in performances between Decelle and Furtlehner (2021a) and their new approach from an empirical standpoint?\n\nThis latter point is crucial in assessing the effective novelty of this work. At the moment the reason for the lower score is primarily due to my perception of limited novelty. I am more than happy to discuss this with the authors during the rebuttal and revisit my score upon clarification of my concerns above (and below, see, e.g., the first bullet point in the **Questions** cell)."
            },
            "questions": {
                "value": "## Questions, Small comments and typos\n- Would it be possible for the authors to provide a sketch and pseudocode for their PTT algorithm as a standalone and in comparison to PT? This would be very helpful to get a better understanding of the contribution of this work. \n- Is there any intuition behind the bump observed in Figure 3 at around $10^3$ gradient updates (left and middle plot).\n- Layout: there's a problem with Figure 2. The x-axis is sometimes completely or partly cut. I strongly recommend carefully checking this, aligning the plots, and making sure such problems are removed. \n- In general the authors often refer to the Appendix as SI (I assume Supplement Information). I guess this acronym has not been defined anywhere. I identify its first occurrence in line 96. Perhaps the authors can define what SI is or, alternatively just all it appendix. \n- Line 235: I'd recommend adding a reference for critical slowing down. This comment applies to earlier occurrences of this concept.\n- Line 459: grew -> grey\n- Line 512: Banos et al. (2010) might need to be wrapped in parenthesis \\cite -> \\citep"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This research proposes an efficient training approach for structured data in RBMs by employing pre-training based on simple convex optimization, which significantly facilitates learning for structured datasets. Furthermore, the study introduces a novel sampling and log-likelihood evaluation method that leverages the model's learning process, differing from conventional Parallel Tempering."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper offers a novel contribution by proposing a pre-training technique and a new sampling approach for RBMs inspired by their thermodynamic properties. This builds on the existing theoretical analyses of RBMs.\n- To my knowledge, extending replica Monte Carlo methods to a learning trajectory is original and intriguing.\n- Including a specialized physics background in the Appendix makes the paper accessible even to readers without a physics background."
            },
            "weaknesses": {
                "value": "The distinction between theoretical claims and empirical findings is not clear. It would be beneficial for the authors to clarify which parts of the study are based on theoretical analysis and which are supported by numerical experiments, particularly in the context of related work. For instance, the first- and second-order phase transition claims pertain to equilibrium properties. However, it is unclear how these phase transitions are justified when updating parameters with limited samples. \n\n- In Section 4, the paper introduces pre-training for low-rank RBMs with singular value decomposition (SVD)--based weights, aiming to avoid continuous phase transitions (second-order transitions) as structural patterns gradually emerge. It is further claimed that training can proceed quickly using the PCD method after post-pre-training. Could the authors provide a more detailed explanation for this intuition? Even if second-order transitions are avoided, if there are multiple stable clustered states, capturing multiple modes with the PCD method may be challenging and could introduce bias in the estimation. However, the paper claims, \"Once the main directions are incorporated, training can efficiently continue with standard algorithms like PCD, as the mixing times of pre-trained machines tend to be much shorter than at the transitions.\" I believe that simulating clustered models with simple PCD often results in impractically long mixing times. Indeed, in Section 5.2, it is argued that mixing is very slow for AGS in clustered data.\n\n- The statement \"It\u2019s also often ineffective with highly clustered data due to first-order phase transitions in EBMs, where modes disappear abruptly at certain temperatures, as discussed by Decelle & Furtlehner (2021a)\" suggests that using PT becomes challenging because the learned RBM exhibits a first-order transition at specific temperatures. However, does the existence of a first-order transition in the learned RBM typically occur regardless of the statistical model being learned? For example, if learning a model without a first-order transition, such as the Ising model without a local field, does a first-order transition still arise in the learned RBM? This seems somewhat nontrivial.\n\n- In the phase diagram of A. Decelle\u2019s Thermodynamics of Restricted Boltzmann Machine and Related Learning Dynamics does not appear to be a first-order transition, and the AT line may suggests continuous phase transitions dominated by Full-step RSB. Thus, the claim regarding first-order transitions requires further elaboration. If a first-order transition is present, it would be essential to validate this by examining the free energy from the equilibrium state of the learned model, which could likely be accomplished by evaluating the partition function using the proposed method.\n- If a first-order transition does exist, then the exchange probability in PT would approach zero near the transition. Has this phenomenon been observed? Additionally, it would be helpful to evaluate the round-trip rate of PT and PTT.\n- While it is argued that preparing models at different temperatures is challenging for PT, it should be noted that the proposed approach also requires storing models during the learning process.\n- The CelebA data in Figure 2 appears to be truncated.\n\nBecause the high performance has been verified numerically, the score can be raised if the above statement is cleared."
            },
            "questions": {
                "value": "- Does critical slowing down occur in the energy-based model when the hidden variables are traced out, or does it occur in the joint distribution that includes the hidden variables? If the phase transition occurs in the joint measure, does the traced-out distribution also exhibit a phase transition?\n- What is the definition of $\\bar{u}$?\n- Could the authors provide a detailed derivation of Equation (4)? The terms $\\bar{u}_{a}$ and $\\eta_{a}$ are currently undefined.\n- The phrase \"a direction $\\bar{u}_0$ is used for the magnetization $m_0$ that is only present in the bias term\" is unclear. Could you explain this in more detail?\n- Is it possible to learn DBM without pre-training using the pre-training with weights introduced by [1] ?\n\n[1] Yuma Ichikawa and Koji Hukushima, Statistical-mechanical Study of Deep Boltzmann Machine Given Weight Parameters after Training by Singular Value Decomposition."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies algorithms for the training of Restricted Boltzmann Machines (RBMs).  It argues that \"highly structured\" data require different algorithms than those that have been successful for, e.g., image datasets.  There are three algorithmic ideas that are discussed: 1) Pre-training an RBM using an \"exact\" procedure that produces low-rank weight matrices; 2) Estimating log-likelihoods using annealed importance sampling across steps of a training run; and 3) Using parallel tempering for sampling, again using different steps of training. Evidence for the efficacy of these procedures is provided in the form of curves from training runs on a few small datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The idea of low-rank pre-training is interesting and seems like it could be useful if it scaled up.\n\n2. The idea of doing AIS across the training run is creative and clever.\n\n3. Parallel tempering across training steps seems new."
            },
            "weaknesses": {
                "value": "1. I think this paper has a somewhat limited audience.  It mostly builds upon work from a small group of authors, using language most familiar to that community.  (For example, one person's work is cited thirteen times in the references.)  A significant amount of jargon is used that keeps this from being a readable stand-alone paper.  This is coupled with heuristic explanations for things that appear to rely on sharing the particular statistical mechanical point of view of this subcommunity.\n\n2. Much of the motivation for the work centers on \"highly structured\" data, which is not defined clearly.  The authors indicate that this corresponds to the existence of clusters.  The paper does not show examples of the methods succeeding or failing in the presence of this structure.  For example, the Celeb-A dataset is given as an example of a dataset in which there are not clusters and so it is not \"highly structured\".  However, Figure 2 does not seem to show us that this matters for the pre-training procedure.  Figure 15 is\nsimilar.  Why does one conclude that the bottom row of Fig 2 and Fig 15 are significantly different from what we see in the top row of Fig 2?\n\n3. The main text is highly verbose, with most of the actual concrete content being in the appendices.  I don't think anything novel is introduced until page six.\n\n4. I find it difficult to appreciate precisely what the contribution of Section 4 is.  As I understand it, the insight is \"do Decelle & Furtlehner (2021a) before you do PCD\".  This is useful information, but between this section and Appendix A, I'm not sure where the boundary is between this and D&F (2021a).\n\n5. While the ideas of section 5 are interesting and Figure 3 is intriguing, the empirical results are at the level of \"preliminary findings\" on a single small problem.  Even with the vastly smaller compute resources of 15 years ago, RBM researchers were studying larger problems.\n\n6. The title is too broad relative to what the paper delivers.\n\nTypos:\n - L161-162: \"two slow\"\n - L478: \"exchanges parameters\" but I think you mean \"exchanges configuration\".\n - L775-776: \\bar{u} vs \\hat{u}.\n - L836-837: \"gradient is convex\" -- surely you mean the training objective is convex in the parameters."
            },
            "questions": {
                "value": "1. Why didn't you apply this to larger problems?\n\n2. What are situations where the pre-training fails?\n\n3. Is PTT useful for generating samples during training, using only earlier parts of the training run?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}