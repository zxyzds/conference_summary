{
    "id": "Y4iaDU4yMi",
    "title": "InteractiveCOT: Aligning Dynamic Chain-of-Thought Planning for Embodied Decision-Making",
    "abstract": "Vision-Language Models (VLMs) are increasingly being employed as the decision-making \"brains\" of embodied agents. Effectively harnessing their powerful generalization capabilities in dynamic, context-specific tasks remains a significant challenge. Chain-of-Thought (CoT) prompting is often utilized for complex task execution, but existing methods either rely on static strategies that fail to adapt to changing environments or fine-tune on offline datasets, which are insufficient for optimizing agent decision-making through interaction.\nIn this paper, we propose a novel approach that focuses on optimizing the CoT reasoning process rather than just the final action tokens. By aligning the CoT process through preference-based reinforcement learning, specifically Direct Preference Optimization (DPO), we enhance the agent's ability to make accurate decisions in dynamic environments while mitigating model degradation during fine-tuning. Our method models the environment as a Markov decision process, requiring the agent to reflect on the current state in real time to generate adaptive plans and actions.\nBy prioritizing the optimization of the CoT process over the final actions, we enhance the agent's reasoning adaptability while effectively mitigating model degradation during fine-tuning.\nExperiments in the ALFWorld environment demonstrate an average success rate of \\textbf 26.67%, which is a 6\\% improvement over RL4VLM, and show that our method effectively mitigates model degradation post fine-tuning. These results highlight the potential of integrating preference-based reinforcement learning techniques with CoT processes to enhance the decision-making capabilities of vision-language models in embodied agents.",
    "keywords": [
        "Embodied Agent",
        "Multi-modal Large Model",
        "Chain-of-Thought",
        "Planning",
        "Reinforcement Learning",
        "Preference Learning"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Y4iaDU4yMi",
    "pdf_link": "https://openreview.net/pdf?id=Y4iaDU4yMi",
    "comments": [
        {
            "summary": {
                "value": "This paper used Direct Preference Optimization (DPO) with a Vision Language model that uses Chain-of-Thought (COT) prompting for task planning of embodied agents. The proposed method optimized the COT process instead of the final action as in PPO. The experiment showed the proposed method outperformed RL4VLM in terms of the average success rate on ALFWorld dataset."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Replacing PPO in RL4VLM with DPO can increase average success rate."
            },
            "weaknesses": {
                "value": "- The contribution is incremental. The main contribution of this paper is replacing PPO in RL4VLM (which also has COT) [1] by DPO [2] with necessary adaptions.\n\n- The motivation that the authors want to use DPO is that it could be beneficial in the cases of long action sequences, partial occulsion and multi-tasking. However the results do not show any of these scenarios. It's not clear whether DPO performs better thanPPO in these scenarios.\n\n- More analyses are needed to show the performance gain on ALFWorld, details see comments below.\n\n\n[1] Zhai, Yuexiang, et al. \"Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning.\" arXiv preprint arXiv:2405.10292 (2024).\n[2] Rafailov, Rafael, et al. \"Direct preference optimization: Your language model is secretly a reward model.\" Advances in Neural Information Processing Systems 36 (2024)."
            },
            "questions": {
                "value": "- I'm wondering why the proposed method works much better on the task Heat & Place and Examine in Light, while worse on Pick & Place. Can authors provide more insights on these?\n\n- What will the proposed model perform without COT prompting? Since the proposed method re-plan the task at interactive steps, is COT helpful in this case? I'm interested to see the ablation study with and without COT prompting.\n\n- Intuitively, re-planning should work better when the action sequence is long, and the closer towards the end of the action sequence, the more accurate and less uncertain the planned actions should be. Does the proposed method behave like this? \n\n- There are typos and grammar mistakes in the text and figure, please check the spelling."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a preference based fine-tuning method for a VLM on sequential decision making tasks. They look at Alfworld tasks and show a 6% improvement over the baseline RL4VLM (which looks at RL based finetuning of VLMs). They add auxiliary losses (APW and APC) to encourage COT - action consistency in the output form the VLM."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "How to benefit from and optimise chain of thoughts in LLM-derived models for decision-making is an interesting question which the paper considers."
            },
            "weaknesses": {
                "value": "The paper is very poorly written:\n1) Many implementation details are left to the imagination, there isn\u2019t a code implementation provided either. \n2) The supplementary material only contains a single page derivation and no experimental details. \n3) Lots of typos and confusing wording throughout the paper\n4) Lines 214, 215, what is meant by \u201c\u03c0ref refers to the policy generated by the unrefined output of VLM? Donate \u2192 Denote \n5) \u03c0\u03b8 and \u03c0ref donate the policy generated by VLM - not clear enough\n6) Line186  - typos? - For once(?) planning instance, planner output the \u2026\n7) Fig 1 : dicision -> decision\n8) In many places there is mention of a \u201cPreference action\u201d - I suppose the authors mean to say \u201cpreferred action\u201d\n9) Fig 1 is not very clear - what is the circle trying to outline in the \u201cpreference action\u201d and \u201cnot preference action\u201d? \n10) The subscripts in the equations are not defined anywhere\n11) Line 222 typo in the section heading\n12) The contributions claim interactiveCOT supports both PPO and DPO- where is this validated? Isn\u2019t the paper only presenting DPO results?\n13) Are their pairs action level or trajectory level\n14) Point 2 in contributions: \u201ctailored to the interaction characteristics of embodied agents, \u201c - what does this mean?\n\nLack of thoroughness in experiments:\n1) The paper doesn\u2019t mention what exactly is the reward function used by the PPO baseline? The paper mentions using partial task progress signal (which is much denser than a 0/1 success signal indicating whether the episode was successful) to construct preference labels - did the baseline (PPO) also benefit from this extra information? If not, why? It should be possible to convert the partial progress to rescaled rewards.\n2) Fig 6a both variants seem the same in terms of the max performance resached yet the text describes one variant (w action weighting) as better than the other\n3) No information about the hyperparameters is provided, no information on how many seeds were used, fig 4b does not even have error bars - was only one seed used per task??\n4) How was the checkpoint used to report the final score chosen? Fig 4a has a larger gap between the two methods at 2k steps (6% as claimed by the authors), but the gap would be smaller (by around 2%) if you used the ckpt for PPO at 1k training steps - which would reduce the improvement to 4%. \n5) How exactly were the trajectories collected?\n6) No other baselines are studied apart from RL4VLM:  the RL4VLM paper cited studies a CNN+RL baseline, as well as a prompting-only LLM baseline. It would also make sense to compare to the baseline where the authors train a VLM with DPO directly for action selection (as done for the VLM-RL case in [1]).\n7) What is the reason/motivation for using the particular preference score definition in eqn 10? Why 50 * progress  specifically?\n8) 454 line - unclear, and where is this demonstrated?\n9) Line 426 - extremely vague description of scores computation (\u201cIn practice, considering the achievement of long-term goals, we calculate preference scores using a method similar to discount factor weighting in reinforcement learning returns\u201d)\n\n[1] Large Language Models as Generalizable Policies for Embodied Tasks"
            },
            "questions": {
                "value": "I have listed the questions in the bullet points of the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces InteractiveCOT, a method for fine-tuning a vision-language model (LLaVA-7B) for embodied AI tasks. The authors aim to enhance RL4VLM by optimizing the chain-of-thought (CoT) process rather than focusing solely on final action decisions. Using the Direct Preference Optimization (DPO) algorithm, they fine-tune the vision-language model with online-collected episodes. The proposed method is evaluated in the ALFWorld environment, demonstrating improvements over the PPO baseline."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Utilizing foundation models as a starting point for embodied agents is an interesting approach, as it can enhance sample efficiency and reduce redundant exploration by leveraging the prior knowledge embedded in these models.\n- InteractiveCOT achieves a 6% improvement in the final success rate by focusing more on optimizing the CoT process during VLM fine-tuning."
            },
            "weaknesses": {
                "value": "- The paper's presentation is difficult to follow, with numerous typos and inconsistencies in notation. For example:\n    - Line 84, \"In summery\" -> \"In summary\".\n    - In Figure 1, \"LLaVA as dicision model\" -> \"LLaVA as decision model.\"\n    - Line 215, \"donate\" should be \"denote\"; additionally, $\\pi_{ref}$ is duplicated.\n    - The definitions of subscripts and superscripts for action (i.e., $a_t^1$ and $a_t^2$) in line 245 and in Equations (4), (6), (7), (8), and (9) are inconsistent.\n    - Line 213 references the tuple with $o_t$, but it is unclear where $s_{1 \\sim t-1}$ originates.\n\n- The authors should include a background section to introduce the basic RL framework, including elements of the MDP, trajectories, and policy, to clarify the RL context being considered. Without this, it is difficult to follow the subsequent sections. Additionally, a brief overview of the original DPO algorithm should be provided so that modifications proposed in the methods section are clearly distinguishable. \n\n- In Section 3.1, the authors state that the VLM is used as a planner; however, it is unclear how the plan is generated. It appears that the VLM functions directly as a policy, outputting final actions to step into the environment, as illustrated in Figure 1. Thus, it may be misleading to frame the proposed method as a \"re-planning framework\" (line 197). Can author clarify this point?\n\n- What type of action space does the paper consider? Is it continuous or discrete? If it is discrete, how is the MSE calculated in Eq. (7)?\n\n- In line 201, what does \"well-fine-tuned model\" refer to? Is this the VLM fine-tuned by the proposed method?\n\n- Throughout the paper, what does $\\tau_t^{t-1}$ represent?"
            },
            "questions": {
                "value": "See the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}