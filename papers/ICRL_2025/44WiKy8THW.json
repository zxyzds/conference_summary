{
    "id": "44WiKy8THW",
    "title": "Integrating Geodesic Interpolation and Flow Matching for Non-Autoregressive Text Generation in Logit Space",
    "abstract": "Non-autoregressive language models are emerging as effective alternatives to autoregressive models in natural language processing, enabling simultaneous token generation. This study presents a novel flow matching approach using Kullback-Leibler (KL) divergence geodesics to interpolate between initial and target distributions for discrete sequences. We establish a loss function that maximizes the conditional likelihood of discrete tokens, demonstrating that its maximizer corresponds to the flow matching velocity under logit interpolation. While initial tests on the TinyStories dataset yielded unsatisfactory results, we introduce an empirical sampling scheme based on a pretrained denoiser, which significantly improves performance.",
    "keywords": [
        "Flow Matching",
        "Non-autoregressive text generation"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=44WiKy8THW",
    "pdf_link": "https://openreview.net/pdf?id=44WiKy8THW",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a novel approach for non-autoregressive text generation in logit space. It uses Kullback-Leibler (KL) divergence geodesics for flow matching between initial and target distributions of discrete sequences. A loss function is defined to maximize the conditional likelihood of discrete tokens, and its theoretical properties are explored. Despite initial poor results on the TinyStories dataset, an empirical sampling scheme based on a pretrained denoiser is proposed, which significantly improves performance. The method is also applied to image generation tasks for comparison."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) Novel theoretical approach: The use of KL-divergence geodesics for flow matching in discrete sequence modeling is a novel concept. The theoretical justification provided for the likelihood function and its relation to the flow matching velocity adds to the rigor of the method."
            },
            "weaknesses": {
                "value": "1) Extremely low writing quality: The writing and presentation of this article are extremely poor and unreasonable.\n\n2) Limited dataset evaluation: The evaluation is conducted on two uncommon datasets."
            },
            "questions": {
                "value": "Given the low quality of presentation, I have no further questions. I hope that the authors can make full preparations and improvements before the next submission."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel method for non-autoregressive text generation using KL-divergence geodesics and flow matching in logit space. The authors propose a conditional flow matching approach to address the challenges of discrete sequence modeling, demonstrating theoretical alignment between the loss function and flow matching velocity. To enhance performance, they implement an empirical sampling scheme based on a pretrained denoiser. Experiments on both text and image datasets show that the method outperforms traditional autoregressive models. Despite promising results, the sampling technique lacks full theoretical justification."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper introduces a novel application of KL-divergence geodesics for text generation, addressing limitations in linear interpolation commonly encountered in discrete sequence modeling.\n\n- The use of a pretrained denoiser-based empirical sampling scheme demonstrates ingenuity, compensating for initial performance shortcomings and achieving improved generation results."
            },
            "weaknesses": {
                "value": "- The paper seems to have been written in a hurry and lacks proper polish, with numerous missing references that make it difficult for me to follow. For example, references are missing at lines 32, 33, 39, 53, and 90, which disrupts the flow of the paper.\n- I find the experimental section quite limited, as it only includes a single experiment for both text and image generation. A detailed ablation study is missing, making it hard to understand the impact of different components.\n- I believe the evaluation metric for text generation is too restricted, relying almost exclusively on perplexity. While perplexity is useful for understanding how well the generated text fits the probable distribution, it can fail to capture semantic richness. I would recommend adding metrics like BLEU, ROUGE, or exploring newer evaluation methods for a more comprehensive assessment.\n- After reading the introduction, I still do not fully understand why flow matching is necessary for generation models. The motivation for choosing this specific approach remains unclear to me."
            },
            "questions": {
                "value": "See the Weaknesses part"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work presents a flow matching approach for generating discrete sequences. This approach treats discrete tokens as one-hot vectors and constructs a flow by interpolation on the logit space. Randomized top-k sampling is proposed for inference."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "N/A"
            },
            "weaknesses": {
                "value": "This paper is only half-baked and needs substantial refinements before resubmission. For example, the presentation is poor (many variables are not explained, Figure 1/2 have the same caption, and some references are placeholders), experiments are only conducted on toy datasets (Tiny Stories, MNIST), and evaluation metrics are not sound (only use generative perplexity for language modeling)."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}