{
    "id": "vf8iou7FNF",
    "title": "RLSF: Reinforcement Learning via Symbolic Feedback",
    "abstract": "Reinforcement Learning with Human Feedback (RLHF) is considered a standard approach to fine-tuning Large Language Models (LLMs). However, such methods often face limitations such as unsound black-box reward models, difficulties in collecting human preference data, and the reliance on sparse scalar rewards. These methods often fall short when applied to tasks that require complex domain-specific understanding.\n\nTo address these challenges, we propose a new fine-tuning paradigm we refer to as Reinforcement Learning via Symbolic Feedback (RLSF), which aims to improve domain-specific understanding of LLMs more effectively than traditional reward signals. In the RLSF setting, the LLM being fine-tuned is considered an RL agent, while the environment is allowed access to reasoning or domain knowledge tools (e.g., solvers, provers, algebra systems, or knowledge bases). Crucially, in RLSF, these reasoning tools can provide feedback to the LLMs via poly-sized certificates (e.g., proofs), that characterize errors in the LLM-generated object with respect to some correctness specification. As a bonus, our RLSF approach does not require the reasoning systems we use to be differentiable. The ability of RLSF-based fine-tuning to leverage certificate-generating symbolic tools enables sound fine-grained (token-level) reward signals to LLMs, and thus addresses the limitations of traditional reward models mentioned above.\n\nVia extensive evaluations, we show that our RLSF-based fine-tuning of LLMs outperforms traditional approaches on five different applications (that have some associated logical or domain constraints), namely, program synthesis from natural language pseudo-code to programming language (+31.43\\% in functional correctness for Google's CodeGemma-2b compared to supervised fine-tuning, +17.01\\% in functional correctness compared to GPT-3.5 -- 100$\\boldsymbol\\times$ larger), three chemistry tasks (+5.5\\% exact match for molecule generation, +19.4\\% exact match for forward synthesis, +33.7\\% exact match for retrosynthesis, using Meta's Galactica-1.3b, compared to GPT-4 -- 1000$\\boldsymbol\\times$ larger), and solving the Game of 24 (+25\\% success rate using Meta's Llama2-7b compared to traditional methods, and +7\\% success rate compared to GPT-3.5 -- 25$\\boldsymbol\\times$ larger). A takeaway is that fine-tuning via RLSF enables relatively smaller LLMs to significantly outperform closed-source models that are orders of magnitude larger (e.g., GPT-4).",
    "keywords": [
        "Symbolic Feedback",
        "Reinforcement Learning",
        "Large Language Models",
        "Program Synthesis"
    ],
    "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)",
    "TLDR": "RLSF is a new fine-tuning paradigm that improves domain-specific understanding in LLMs by using symbolic tools for fine-grained feedback, surpassing traditional methods and enabling smaller models to outperform much larger closed-source models.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vf8iou7FNF",
    "pdf_link": "https://openreview.net/pdf?id=vf8iou7FNF",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes RLSF, a fine-tuning approach that uses symbolic tools to provide token-level feedback for LLMs. The authors evaluate RLSF on five tasks across three domains: code generation, chemistry, and math problem solving (Game of 24)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "# Implementation Details\n- Clear description of the RLSF approach\n- Good reproducibility through detailed experimental setup\n- Comprehensive evaluation across multiple domains\n# Results\n- Shows consistent improvements over baselines across different tasks\n- Provides detailed metrics and comparisons\n- Demonstrates potential practical utility"
            },
            "weaknesses": {
                "value": "# Limited Technical Novelty\n- The core idea of using symbolic tools for feedback is not new - the paper acknowledges similar approaches in code generation and verification\n- The main contribution appears to be applying token-level feedback in RL fine-tuning, which is an incremental advance\n- The approach is essentially a straightforward combination of existing techniques (RL, symbolic verification, token-level feedback)\n# Experimental Focus\n- The paper is primarily focused on empirical results across three domains\n- Limited theoretical analysis or justification for why this approach works better\n- No significant algorithmic innovations beyond combining known components"
            },
            "questions": {
                "value": "1. How does the approach scale to more complex tasks requiring deeper reasoning?\n2. Why focus on these particular domains/tasks? How generalizable is the approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose a new fine-tuning paradigm for LLM post-training called RLSF, where the reasoning tools can provide feedback to the LLMs via poly-sized certificates characterizing errors in the LLM-generated object with respect to some correctness specification.\nRLSF is evaluated across five different tasks, demonstrating superior performance compared to traditional fine-tuning methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Innovativeness: RLSF is the first reinforcement learning paradigm that utilizes multi-dimensional certificates generated by symbolic reasoning tools to provide fine-grained feedback, addressing the limitations of traditional reward models.\n\n* Efficiency: RLSF significantly enhances the performance of LLMs across multiple tasks, particularly excelling in the conversion of natural language pseudocode to C++ and in chemical tasks.\n\n* Practicality: RLSF does not require a differentiable symbolic reasoning system, which increases its flexibility and applicability in real-world scenarios.\n\n* Experimental Validation: The article validates the effectiveness and superiority of RLSF through experiments on five different tasks, showcasing its potential in specific domain tasks."
            },
            "weaknesses": {
                "value": "* The proposed RLSF relies on symbolic reasoning tools to generate feedback, and the performance and availability of these tools may influence the effectiveness of RLSF. While the article mentions that these tools perform well in practice, it does not provide a detailed discussion of their limitations and possible alternatives.\n* The abstract section is overly lengthy and could benefit from a more concise phrasing."
            },
            "questions": {
                "value": "* The RLSF method demonstrates exceptional performance on specific tasks, However, its applicability to a broader range of reasoning tasks or general tasks remains unclear. Is there theoretical or empirical evidence supporting its effectiveness across different tasks?\n* Does the performance of symbolic tools decline when handling complex or large-scale problems? Are there alternative solutions or improvements that can be considered?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes RLSF or RL with symbolic feedback. RLSF's main difference with RLHF are two components. 1) A symbolic feedback generator that produces poly-sized certificates which basically means the feedback is not too unreasonably large compared to the input, .i.e is polynomial in size of the input. An examples of these symbolic feedback generator is the output of a compiler when compiling a program. 2) A function that takes in this feedback, the policy-sized certificate, and translates it to per-token rewards, e.g. assigning negative rewards to the lines of code that had syntax problems. Learning from this symbolic feedback improves RL post-training of LLMs on various tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "It is not clear how to integrate symbolic feedbacks into a RL post-trainng of LLMs. Therefore, it seems RL post-training is missing out those feedbacks and finding a way to integrate them into the training loop should have advantages. This work is a first attempt at doing these which is valuable by trying to translate this feedbacks to environment rewards. It is different from other works that want to give the LLM a way to use symbolic tools.  Also, the work tests the idea on diverse tasks: Translation of Pseudo-Code to Code, Modelling chemical reactions, and game of 24 as a toy example. The work shows token-level rewards given by this system improves performance especially in terms of lexical or sytax errors."
            },
            "weaknesses": {
                "value": "In terms of novelty, the fact that fine-grained feedback works better is well-known in RL community. For examples, There is a plethora of works on reward shaping. However, I know that in NLP, people know RLHF with its boolean feedback. Also in RL for reasoning we have papers that try to incorporate process based rewards instead of scalar rewards. Therefore, showing fine-grained signal works better is not considered very novel. However, I have not seen another work that tries to somehow extract and translate token-level feedback from these symbolic feedbacks. That is the the novelty of the paper. I think this paper has an interesting message for people who are doing RL post-training to try to incorporate rewards of previously symbolic systems which I think is important and is a strength. \n\nIn terms of the experimentation, I think there is a big problem in the coding experiments. It may be that I misunderstood but I want to double check. After the SFT is done on the models, it seems the actual comparison is between training with the binary feedback, and the then the symbolic feedback. The main problem is that the symbolic feedback also contains the actual rewrad of whether the test cases are passed or not while the binary feedback is just whether the program compiles or not. That is not the correct comparison as the second option has the access to the actual success reward on the tasks in top of the integration of the symbolic feedbacks. The comparison that I think shows the significance of the inclusion of the symbolic feedback is to compare A) training with the success reward which can include the compilability reward as well  B) training with the success reward augmented with symbolic rewards.\n\nI don\u2019t have an expertise in the tasks involving chemical reactions. I have asked a few questions about them to make sure the signficant of the contribution.\n\nIn terms of writing, I think the paper is not written in terms with its actual message. I think the paper is written from the angle that its message is that \u201cfine-grained signal is better\u201d. However, this is a well-known message and not surprising. The actual hard task is how to transform a symbolic feedback to a fine-grained signal. If the paper was written about the details and challenges of this conversion I think its contribution would be much more apparent. I think the authors have actually done the hard work of the details and nitty-gritty of this conversion but decided to put the focus on the RLSF rather than on those details. There are many questions about how-to of this extraction of signal from symbolic feedback to fine-grained signal that I think it deserves a well written paper to discuss them. I have asked them in the questions section and I would like a lot if the authors include a section about that in their paper."
            },
            "questions": {
                "value": "1-In the pseudo-code to code translation experiments, is there a reason that you did not have a run with the success reward (maybe alongside compilability reward)? What happens in that experiments and is there still benefit to integrate the symbolic feedback from the compiler?\n\n2-What are the challenges and details of the choices for converting the symbolic feedback to per-token level rewards? I understand when a line does not compile, we can put negative rewards on that line. But is it always correct? If I am not wrong, there are cases in which a line does not compile because of a line before is not written. For example, a variable is not defined or a space or newline is forgotten. Isn\u2019t it possible that with this the conversion actually misses the actual source of the problem and keep it untouched? I think there is some degree of credit assignment going on and the assignment is based on best guesses. While I am not familiar with the chemistry tasks, I am wondering what the output of the symbolic feedback generators look like and how easy it is to transform to per-token rewards? Also, I did not follow the explanation of the paper on how they integrated the algebraic system to the game of 24. Can you explain this in detail? I think the details of how to cover these signal is the actual contribution of the paper. \n\n3-For modelling the chemical reactions, you take out a pre-trained language model and then fine-tune it. Is that correct? I am just wondering if this is standard practice because these reactions are very far from written human texts and I was wondering if the standard practice is to just train from scratch? Can you cite other papers that also employ the same strategy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a paradigm for finetuning large language models. In tasks where outputs are expected to belong to a certain formal language, symbolic programs can be used to check the validity / correctness and can be used as rewards to finetune an LLM. The authors demonstrate this approach on three problems - code generation, molecular generation / rethrosynthesis, and the so called game of 24."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The paper is written with good clarity. I am unsure about how original this work is because I am not aware of the literature in this area. But, I think the approach is fairly straightforward. \n2) The results, especially in code generation tasks seem better than baselines like GPT-3.5."
            },
            "weaknesses": {
                "value": "1) Although there seems to be no glaring weakness to me, I am unsure about the significance of the chemistry problems that are used. I do not know how significant is Fingerprint Tanimoto Similarity or ~35%.\n2) I am also not sure how novel this approach is. Can the authors point towards similar related work? Right now there is only one citation in the, \"Neurosymbolic Reinforcement Learning (NRL)\"  paragraph. \n\nBut both points are speculations rather than weaknesses, I would just appreciate an answer."
            },
            "questions": {
                "value": "Please look at the weakness section for questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}