{
    "id": "gyHoR6uFhU",
    "title": "PortLLM: Personalizing Evolving Large Language Models with Training-Free and Portable Model Patches",
    "abstract": "As large language models (LLMs) increasingly shape the AI landscape, fine-tuning pretrained models has become more popular than in the pre-LLM erafor achieving optimal performance in domain-specific tasks. However, pretrained LLMs such as ChatGPT are periodically evolved (i.e., model parameters are frequently updated), making it challenging for downstream users with limited resources to keep up with fine-tuning the newest LLMs for their domain application. Even though fine-tuning costs have nowadays been reduced thanks to the innovations of parameter-efficient fine-tuning such as LoRA, not all downstream users have adequate computing for frequent personalization. Moreover, access to fine-tuning datasets, particularly in sensitive domains such as healthcare, could be time-restrictive, making it crucial to retain the knowledge encoded in earlier fine-tuned rounds for future adaptation. In this paper, we present PORTLLM, a training-free framework that (i) creates an initial lightweight model update patch to capture domain-specific knowledge, and (ii) allows a subsequent seamless plugging for the continual personalization of evolved LLM at minimal cost. Our extensive experiments cover seven representative datasets, from easier question-answering tasks {BoolQ, SST2} to harder reasoning tasks {WinoGrande, GSM8K}, and models including {Mistral-7B,Llama2, Llama3.1, and Gemma2}, validating the portability of our designed model patches and showcasing the effectiveness of our proposed framework. For instance, PORTLLM achieves comparable performance to LoRA fine-tuning with reductions of up to 12.2\u00d7 in GPU memory usage. Finally, we provide theoretical justifications to understand the portability of our model update patches, which offers new insights into the theoretical dimension of LLMs\u2019 personalization.",
    "keywords": [
        "Large Language Models",
        "NLP",
        "Efficiency",
        "Fine-tuning",
        "Efficient Fine-tuning",
        "Portability"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=gyHoR6uFhU",
    "pdf_link": "https://openreview.net/pdf?id=gyHoR6uFhU",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a training-free personalization approach for evolving LLMs. Specifically, the proposed PortLLM utilizes LoRA parameters from finetuning older models to approximate updated models. Extensive experiments validate the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The proposed method is based on the observation that residual parameter updates are negligible compared to the naive update, which is both intriguing and empirically proven effective."
            },
            "weaknesses": {
                "value": "The paper\u2019s core claim is that the residual matrix $\\Delta \\theta_i\u2019 - \\Delta \\theta_i$ is negligible in value compared to the naive update $\\theta\u2019 + \\Delta \\theta_i$. Although the authors provide both theoretical and empirical justifications for this, the Lemma (lines 266-269) only establishes that $\\text{rank}(\\Delta \\theta_i\u2019 - \\Delta \\theta_i) \\ll \\text{rank}(\\theta\u2019 + \\Delta \\theta_i)$, which does not directly imply that the former is negligible in value relative to the latter. While this statement is supported by empirical results in Table 1, a theoretical proof or an estimated error bound would strengthen the justification, as this could represent a significant proportion of the error for this method."
            },
            "questions": {
                "value": "- Theoretically, the proposed method $\\theta\u2019 + \\Delta \\theta_i$ introduces error compared to $\\theta\u2019 + \\Delta \\theta_i\u2019$, potentially leading to worse performance. However, in Table 2, a large proportion of $\\theta\u2019 + \\Delta \\theta_i$ results outperform $\\theta\u2019 + \\Delta \\theta_i\u2019$ (the fine-tuned updated LLM). Could the authors elaborate on potential reasons contributing to this improvement?\n\n- What does \u201c# Trainable Parameters\u201d signify in Table 6? Why does the fine-tuning model have zero trainable parameters, while the proposed training-free approach has a large number of trainable parameters?\n\n- In line with the discussion in Weaknesses, could the authors investigate the impact of the approximation error introduced by the proposed method? For instance, providing metrics similar to those in Table 1 for all datasets, along with the corresponding performance gains, could offer a clearer understanding of how this factor contributes to overall performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces PORTLLM, a framework designed to maintain the effectiveness of domain-specific adaptations in Large Language Models (LLMs) across updates without requiring further fine-tuning. The framework leverages task-specific model patches, derived as the difference between a fine-tuned model and its base (pre-trained) version, to transfer knowledge from one version of an LLM to the next. Experiments demonstrate the method\u2019s effectiveness in enhancing the performance of updated LLMs on various tasks. The approach is claimed to offer computational efficiency and portability across model architectures."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The framework proposes a training-free alternative to recurrent fine-tuning, which could be useful for applications with limited resources.\n2. The portability across model versions and architectures has potential, especially in scenarios where repeated fine-tuning is computationally prohibitive."
            },
            "weaknesses": {
                "value": "1. The core idea of leveraging parameter differences between fine-tuned and pre-trained models (task vectors) has been explored in prior work, such as Ilharco et al. (2022) on task arithmetic. PORTLLM builds on this concept, but without substantial theoretical or empirical advancements, the framework\u2019s contribution may be seen as incremental rather than groundbreaking. In light of the established work on task vectors, PORTLLM\u2019s innovation could be questioned.\n\nIlharco, Gabriel, et al. \u201cEditing models with task arithmetic.\u201d arXiv preprint arXiv:2212.04089 (2022).\n\n2. From the results in Tables 2 and 4, it is evident that the PortLLM method is effective in enhancing the performance of the Updated LLM. However, in most cases, it does not surpass the performance of the Fine-tuned LLM. Moreover, since the PortLLM approach requires access to the parameters of the Fine-tuned LLM to calculate $\\Delta \\theta$ (task vector), applying PortLLM depends on the existence of a Fine-tuned model. In this setup, if PortLLM cannot consistently outperform the Fine-tuned LLM in terms of performance, its practical application value becomes limited."
            },
            "questions": {
                "value": "1.\tIn Table 2, why does your method show such a large improvement on the WNLI task compared to the Fine-tuned LLM and Fine-tuned Updated LLM?\n\n2.\tLine 344: There is a notation error for the fine-tuned model, it should be $\\theta_i$.\n\n3.\tAre the two columns of values in Table 6 reversed?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper hypothesizes that the rank of adapters trained with LoRA for a specific downstream task is significantly lower than that of adapters obtained through continued pre-training of large language models using LoRA. The authors experimentally validate this hypothesis and further demonstrate that for pre-trained large language models, using the original adapter fine-tuned for a specific downstream task with LoRA does not lead to significant performance degradation after continued pre-training; in fact, there may even be an improvement in performance on downstream tasks. The performance is comparable to that achieved by training new adapters on the updated large language model. This conclusion suggests that after continued pre-training, the original adapters can still be effectively utilized, significantly reducing training costs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The training-free framework proposes the reuse of existing adapters after updating large language models and validates its feasibility, effectively reducing training costs.\n\n2. Extensive experiments across multiple datasets and architectures provide strong evidence for the method\u2019s effectiveness. The paper encompasses a broad range of tasks and models, demonstrating its generalizability and robustness across diverse scenarios."
            },
            "weaknesses": {
                "value": "1. The background and related work sections contain excessive irrelevant content.\n\n2. In the experimental setup, the fine-tuning for downstream tasks is limited to a rank of 8 without further experimentation.\n\n3. In the experimental setup, the continued pre-training involves only the use of different pre-training datasets for training, without addressing the scenario claimed by the authors: that large language models are updated periodically. In such updates, there may be conflicts between new knowledge that develops over time and existing knowledge, potentially leading to significant changes in the model. The authors did not consider this situation.\n\n4. In the theoretical analysis section, it is better to check the statement that $ \\( \\theta' = \\theta + \\Delta\\theta \\)$ ensures $ \\( \\text{rank}(\\theta') \\geq \\text{rank}(\\theta) \\) $."
            },
            "questions": {
                "value": "Hope the authors can expand the experiments based on the identified weaknesses and revise the writing and theoretical analysis accordingly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents PORTLLM, a novel training-free framework that enables the transfer of personalized knowledge between different versions of evolving LLMs. The key innovation is the creation of lightweight model patches that can be seamlessly applied to updated LLMs without requiring fine-tuning. The framework is extensively evaluated across seven tasks and multiple model architectures, demonstrating comparable or superior performance to LoRA fine-tuning while significantly reducing computational costs. The authors also provide theoretical justification for the portability of their model patches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- A training-free framework for transferring domain-specific knowledge between evolving LLM versions through portable model patches\nTheoretical analysis justifying why model patches work effectively, showing certain terms can be neglected to create simplified, training-free patches\n- Comprehensive empirical validation across multiple tasks ranging from QA to complex reasoning and different model architectures\n- Significant efficiency gains compared to LoRA fine-tuning"
            },
            "weaknesses": {
                "value": "I don't identify major weakness, but I do have a few concern regarding the experimental setting:\n- In the experiments, seems the authors only focus on small-scale open-source LLMs. However, from my understanding, it could also be more beneficial for large-scale commercial LLMs. \n- Following the previous setting, a few related works adaptation related works might be considered as related works besides LoRA, LMaaS (Sun et al., 2022), kNN-Adapter (Huang et al., 2023), CombLM (Ormazabal et al., 2023), IPA (Lu et al., 2023), Proxy-Tuning (Liu et al., 2024), and BBox-Adaptor (Sun et al., 2024).\n- It is interesting to observe that in some cases it even performs better than fine-tuned models. More explanations on potential reasons could benefit this work.\n- Echo my previous questions on experimental settings, I think a better evaluation benchmarks lie in (1) personalization tasks, like LaMP; (2) domain-specific adaptation tasks.\n- Any thoughts on rigorous theoretical bounds on performance guarantees?"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}