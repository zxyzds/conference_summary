{
    "id": "fk4czNKXPC",
    "title": "Transformers meet Neural Algorithmic Reasoners",
    "abstract": "Transformers have revolutionized machine learning with their simple yet effective architecture. Pre-training Transformers on massive text datasets from the Internet has led to unmatched generalization for natural language understanding (NLU) tasks. However, such language models remain fragile when tasked with algorithmic forms of reasoning, where computations must be precise and robust. To address this limitation, we propose a novel approach that combines the Transformer's language understanding with the robustness of graph neural network (GNN)-based neural algorithmic reasoners (NARs). Such NARs proved effective as generic solvers for algorithmic tasks, when specified in graph form. To make their embeddings accessible to a Transformer, we propose a hybrid architecture with a two-phase training procedure, allowing the tokens in the language model to cross-attend to the node embeddings from the NAR. We evaluate our resulting TransNAR model on CLRS-Text, the text-based version of the CLRS-30 benchmark, and demonstrate significant gains over Transformer-only models for algorithmic reasoning, both in and out of distribution. Finally, we empirically show that Transformer-only models distilled from TransNAR models also exhibit improved out-of-distribution generalization capabilities.",
    "keywords": [
        "Graph Neural Network",
        "Transformer",
        "Neural Algorithmic Reasoning",
        "Length Generalization"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=fk4czNKXPC",
    "pdf_link": "https://openreview.net/pdf?id=fk4czNKXPC",
    "comments": [
        {
            "title": {
                "value": "(First) Reply to Reviewer 6nT4"
            },
            "comment": {
                "value": "Thank you for the kind words about the motivation of our work and recognising the strength of our comparative evaluation!\n\nAs we have done with Reviewer mvbM previously, we would like to initiate the discussion with you early, as we are not certain we completely comprehended all your suggestions. Some of our experiments may take up to 4\u20135 days to complete, and we want to make sure that we have understood well while there\u2019s still plenty of time to gather additional results!\n\nNote that this is not a complete response; we will be preparing updates to the paper in parallel while we are awaiting your thoughts.\n\n> The novelty is somewhat limited, as combining text-based transformers with GNNs via cross-attention is well-explored in previous works, such as [1] and [2].\n\nThank you for pointing out these references to us, and we will certainly cite and explore them in our related work discussion! We were not aware of these specific works. \n\nTo be clear: we are not claiming to be the first to combine GNNs and Transformers in general, but we do claim to be the first to combine them in the context of (algorithmic) reasoning and out-of-distribution generalisation, which is a setting that is particularly harmful for decoder-only Transformers.\n\n> Both Figures 4, 5 and 6 are not clear enough. I cannot clearly read them in a print version. Please consider to use at least a table to show the results.\n\nWe appreciate your concern about the figures\u2019 readability, and commit to tabulating these results in our next revision! We will let you know once the paper has been updated.\n\n> Only the Chinchilla base transformer is used in experiments. Testing with open-source pretrained LLMs like LLaMa and Mixtral could provide broader insights.\n\nWe appreciate your remark on this! \n\nWhile it would certainly be possible to include additional base Transformers to the mix, we are averse to it for two reasons:\n\n* It would take a significant amount of time and resources, which we would rather dedicate to other experiments at this time.\n* It is _highly unlikely_ to lead to a different conclusion to what we already observed with Chinchilla.\n\nTo provide direct evidence towards the second point, we would like to point you to the results in the CLRS-Text paper (Markeeva et al., 2024), where results for fine-tuning a modern, open-source, Gemma 2B model on this data are provided.\n\nGenerally, it does not seem that scale or algorithmic improvements of the open Gemma 2B model have made a significant difference to the models\u2019 OOD capability compared to Chinchilla: on most algorithms, we observe that Gemma (much like Chinchilla) can learn to fit the algorithms well in-distribution, and out-of-distribution, performance on most algorithms promptly collapses (also like Chinchilla).\n\nAs such, it is our expectation that, were we to e.g. repeat our experiments with Gemma 2B, we\u2019d still observe significant collapse out-of-distribution (as Markeeva et al. did), and TransNAR would be able to supplement that.\n\nWe have no strong reason to expect any other open language model to behave differently.\n\n> Experiments are limited to a single dataset with pre-constructed input graphs, leaving questions about generalizability. Testing on automatically constructed graph datasets, which may be less accurate but useful, could strengthen the study\u2019s applicability.\n\nWe appreciate the Reviewer\u2019s concern!\n\nFirstly, we\u2019d like to note that CLRS-Text should not be considered a \u201csingle dataset\u201d \u2013 realistically it encompasses _thirty_ different algorithmic skills that are learned and evaluated simultaneously.\n\nSecond, the focus of our work is on out-of-distribution generalisation \u2013 specifically, length generalisation \u2013 of language models. It is absolutely standard to leverage carefully constructed synthetic tasks (where outputs can be computed at arbitrary input lengths) for this purpose; see, e.g., \u201cExploring Length Generalization in Large Language Models\u201d from Anil et al. (NeurIPS\u201922), which uses two crafted tasks (Parity and Variable Assignment). \n\nTaken together, the tasks in CLRS-Text are arguably more challenging than most tasks typically seen in length generalisation papers, encompassing more generic polynomial time procedures beyond simple arithmetic \u2013 spanning path-finding, geometric algorithms, string matching, sorting, minimum spanning tree finding, and more.\n\nIt is because of this that we believe our collection of datasets is sufficient for the purpose we aim to show. Please let us know what you think!\n\nAll that being said, we are always open to suggestions for strengthening our work\u2019s experimental section \u2013 please, let us know if there are any specific datasets you would like us to explore, and we can attempt to pursue them in the remainder of the discussion period."
            }
        },
        {
            "summary": {
                "value": "This paper proposes combining transformers with graph neural networks (GNNs) to build enhanced neural algorithmic reasoners. The core approach involves a cross-attention module, enabling language transformers to leverage information from a pretrained GNN. Experiments on the CLRS-Text benchmark indicate that the proposed method improves both in- and out-of-distribution performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The TransNAR approach is well-motivated, and the core concept is clearly articulated.  \n\n2. The method achieves competitive performance compared to a vanilla Transformer.  \n\n3. The study explores a distilled version of the TransNAR transformer, demonstrating improved out-of-distribution robustness over a transformer trained from scratch without GNN support."
            },
            "weaknesses": {
                "value": "1. The novelty is somewhat limited, as combining text-based transformers with GNNs via cross-attention is well-explored in previous works, such as [1] and [2].  \n[1] https://aclanthology.org/Q19-1002.pdf. Semantic Neural Machine Translation Using AMR.   \n [2] https://aclanthology.org/2020.tacl-1.2.pdf. AMR-To-Text Generation with Graph Transformer.   \n\n2.Both Figures 4, 5 and 6 are not clear enough. I cannot clearly read them in a print version. Please consider to use at least a table to show the results. \n\n3. Only the Chinchilla base transformer is used in experiments. Testing with open-source pretrained LLMs like LLaMa and Mixtral could provide broader insights. \n\n4. Experiments are limited to a single dataset with pre-constructed input graphs, leaving questions about generalizability. Testing on automatically constructed graph datasets, which may be less accurate but useful, could strengthen the study\u2019s applicability."
            },
            "questions": {
                "value": "Please refer to Point 3 and Point 4 in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "title": {
                "value": "Reply to Reviewer 79uu"
            },
            "comment": {
                "value": "Thank you for your encouraging comments and for supporting our work!\n\n> The proposed method is domain-specific. It's hard to imagine how this method would benefit broader domains, as the NAR requires a specific type of input.\n\nIt is indeed the case that TransNAR as presented is not applicable to problems where graphs are not provided or derived. The motivation of our work was to show that:\n\n* The knowledge stored in NARs _can_ be meaningfully leveraged by Transformers (via TransNAR).\n* The knowledge captured within TransNAR can be _distilled_ back into a general-purpose Transformer, even though they are purely autoregressive.\n\nWe believe our distillation result to be the proof-of-concept that can be used as a blueprint for empowering broader domain learning in the future!\n\n> How can we combine this method with a pretrained language model? So that we can leverage the math reasoning ability in state-of-the-art language models.\n\nThe most direct application of our work here would include fine-tuning the language model using our distillation loss, potentially leveraging LoRA-style techniques for efficiency. We could also use TransNAR-style models to generate additional training data for fine-tuning.\n\n> Have you compared with using GNN-based NAR alone?\n\nWe are very upfront about NARs\u2019 performance in the early stages of our paper:\n\n> _Provided appropriate inductive biases are used, NARs are capable of holding perfect generalisation even on 6\u00d7 larger inputs than ones seen in the training set, for highly complex algorithmic tasks with long rollouts (J\u00fcr\u00df et al., 2023)._\n\nIt is well-known that GNN-based NARs can generalise much better than LLMs on these tasks. However, their lack of auto-regression and causal masking would make them much harder to train at scale over general purpose text data. The aim of our paper is to see whether some of these benefits can be upstreamed into LLMs, which _can_ be trained at scale more easily and efficiently.\n\n> How does a pretrained language model perform on these tasks?\n\nWe refer the Reviewer to the CLRS-Text paper by Markeeva et al., where two-shot performance for Gemini 1.5 Flash is reported. Generally speaking, pre-trained LLMs are quite poor at executing these algorithms even across small problem sizes, with sorting perhaps being the only semi-notable exception (up to length ~14)."
            }
        },
        {
            "title": {
                "value": "(First) Reply to Reviewer mvbM"
            },
            "comment": {
                "value": "We would like to thank you for commending the clarity of the paper and the simplicity of the approach, as well as proposing multiple interesting avenues for improvement! \n\nWe would like to initiate the discussion early, as we are not certain we completely comprehended all your suggestions \u2013 keeping in mind that our experiments may take up to 4--5 days to complete, we want to make sure that we have understood well while there\u2019s still plenty of time to gather additional results!\n\nNote that this is not a complete response; we will be preparing updates to the paper in parallel while we are awaiting your thoughts.\n\n> Of course a hybrid of a general-purpose model (transformer) and a task-specific model (NAR) will perform better on the specific task. Especially when the model requires special graph-structured data, which is the case for TransNAR.\n\nWe are in agreement here, though we are afraid you may not be considering our message in its entirety. Indeed, we agree it should be obvious that a hybrid model should have a better performance on this task. \n\nWe find it significantly less obvious that these improvements can be distilled back into a standard text-based Transformer, and this is also a key result of our work. This is because:\n\n* Distillation is a procedure originally aimed at transferring knowledge from a larger model to a smaller one, with both assumed to share the same input and output spaces. Here we adapt it to upstreaming the OOD generalization capability of a hybrid model with multiple modalities, back into a unimodal LLM.\n* We are distilling knowledge gained from a hybrid of a non-autoregressive (GNN) and autoregressive (LLM) model into a purely autoregressive (LLM) model, and it is well known that generalisation can be harmed by autoregression (see, e.g., Barbero et al. (NeurIPS\u201924)). The fact that distillation allows us to convey these benefits in spite of that is not obvious to us!\n\nWould you be in agreement with these points? We are very happy to discuss them further!\n\n> The distillation results were hard for me to process and verify. The error bars (constructed on just 3 samples!) are very large.\n\nWe appreciate this remark, and ask: would reducing the variance through increasing the number of seeds be sufficient to address your concern? \n\nWe have already kicked off these experiments; the reason why we ask if this is sufficient _now_ is because these experiments are fairly memory intensive and slow to run. They require caching logits of the teacher model, which are quite expensive given the large vocabulary size of the underlying language model (~32k).\n\n> An obvious baseline is missing: distilling a NAR model to Transformer.\n\nCould you please clarify how this baseline could be constructed?\n\nFrom our understanding of distillation, it is mainly applicable between two models when their output spaces match, so that the student can be trained to mimic either the outputs or the logits of the teacher.\n\nThe NAR and Transformer do not have compatible output spaces, however\u2014the Transformer produces language tokens while the NAR produces output signals on a graph.\n\nAre you suggesting that we manually convert the NAR outputs into text and then supervise the Transformer on this via next-token prediction?\n\n> Give more context to the reader (CLRS-30, NAR, tabulation and analysis)\n\nWe appreciate the reviewer\u2019s remark and commit to add a brief introduction to both benchmarks and illustrate with one sample, as well as adding a tabular version of the results and elaborating on the analysis \u2013 this will be added to a revision during the discussion period, and we will let you know once we\u2019ve done so.\n\n> Did you look into how the distillation data that TransNAR produces is different from the groundtruth?\n\nThe distillation data produced by the TransNAR are logits \u2013 not final predictions. They can only really be compared with ground-truth solutions after decoding into text. We can get a feel for how accurate the decoded data is (compared to the ground-truth) via the TransNAR performance metrics in our paper. \n\nWould you like us to do any particular qualitative study on top of this?"
            }
        },
        {
            "title": {
                "value": "Reply to Reviewer gZZh (Part III)"
            },
            "comment": {
                "value": "Continuing our answers from Part II:\n\n> Could you comment why you chose to call Q as Key and K as Query in equations (1) and (3)?\n\nThanks for the remark, this was an omission on our part \u2013 we will correct it and properly name these parameters in our next revision.\n\n> How would you pronounce NAR? Please make sure it is consistent in the paper (for example in line 065 as \"a NAR-\" and as \"an NAR\" in many other places).\n\nThank you for pointing this out! We would pronounce NAR as [en-eh-ar] (spelled-out) and therefore \u201can NAR\u201d is the version we would go for. We will harmonise this across the paper in the next revision.\n\nThis concludes our responses -- please let us know if there is anything you would like to discuss further! We thank you again for your supportive assessment of our work."
            }
        },
        {
            "title": {
                "value": "Reply to Reviewer gZZh (Part II)"
            },
            "comment": {
                "value": "Continuing our answers from Part I:\n\n> Is every iteration the same as illustrated in Figure 3? It seems it is not as the final output is only from the last layer; for other layers, the outputs from both modules feed to the next layer as inputs. Could you revise Figure 3 to show the entire process accurately?\n\nYou are correct, and we will revise Figure 3 to showcase multiple layers instead of just one!\n\n> Given that the proposed transNAR fails to improve on some of the problems, could you please comment on the limits of the NAR approach? While it can generalize over a larger range of lengths, NAR and transNAR would still lack the generalization and robustness of correct algorithms.\n\nOf course, we will gladly comment on the limits of NARs in this regard in the paper. \n\nWhile NARs do offer superior generalisation on algorithmic tasks compared to most other neural network approaches, they are still not exact algorithms, and will make mistakes provided a sufficient distribution shift is encountered at test time.\n\nAccordingly, we will emphasise that our aim here is to improve the algorithmic generalisation capabilities of LLMs, so that they can be applicable in the wider range of scenarios for algorithmic problems, while acknowledging that out-of-distribution generalisation is a very hard problem for neural networks in general, and we do not think of these models as a pure drop-in replacement for algorithmic computation.\n\n> Reasoning is inherently sequantial but it seems the graphs are not. Could you comment on how the graphs emulate the sequential nature of reasoning steps?\n\nThe graphs themselves are not sequential (in fact they are assumed permutation-symmetric); the reasoning steps emerge through the temporal axis of repeatedly iterating a recurrent GNN layer (the NAR) over such a graph structure.\n\n> One of the known issues with graphs is the oversmoothing issue. Could you comment how that affects the NAR and transNAR?\n\nAs per our reply above, we follow standard NAR practices and use the max aggregation function in our NAR\u2014this significantly reduces vulnerability to oversmoothing due to sharp decisions over neighbourhoods.\n\n> While the improvements over the baseline seem impressive, could ypu give the specific contributions beyond combining NAR and transformers?\n\nWe believe that the key additional contribution beyond combining the two models is distilling these capabilities back into simple Transformers (through our experiments in Section 4.2.).\n\nThe successful distillation allows us to transfer some of the knowledge gathered within TransNAR back into more standard Transformers, making this knowledge accessible in models that can operate in settings where graphs are not provided.\n\n> Could you provide references for transformers' natural langaue UNDERSTANDING properties? The paper mentioned that transformers have unrivaled such properties (Section 1) and at the same time they are limited in generalization due to their autoregressive, causally-masked objective (Section 2). These two do not seem to go together well.\n\nThank you for pointing out this dichotomy; this is something we are well-aware and are happy to explicitly provide references.\n\nThe language understanding capabilities are unrivaled in decoder-only Transformers because they are able to leverage training at scale to make impressive inferences from relatively unstructured textual corpora\u2014most other specialised models (including our NAR) require the data to be prepared in a very specific format before they will show meaningful returns. A standard reference for these abilities, and their emergence at scale, could be \u201cEmergent Abilities of Large Language Models\u201d (Wei et al., TMLR\u201922).\n\nThat being said, the impressive levels of natural language-based predictions tend to only appear _within the distribution_ the decoder-only Transformer has been trained on, and tend to suffer when pushed outside of this distribution. As a simple example, in our algorithmic tasks, the Transformers perform quite impressively on the problem sizes they\u2019ve been fine-tuned on, yet their performance quickly collapses after we add even 1\u20132 nodes to the problems.\n\n>  Would you open-source the code code for your transNAR and baseline models? As many of the papers are from one group due to the unfair advantages created, it is not acceptable to the research community.\n\nYes, we commit to open-sourcing a reference implementation of TransNAR as well as the relevant baseline models.\n\n> More generally, would you be committed to the principle that scientific research should be for the common good and accessible to all?\n\nYes, we are strongly committed to that principle and intend to uphold it.\n\n_Answers to be continued in the next message..._"
            }
        },
        {
            "title": {
                "value": "Reply to Reviewer gZZh (Part I)"
            },
            "comment": {
                "value": "Thank you very much for the positive and supportive review, and for the high degree of detail in your remarks!\n\n> The paper is sketchy in covering important technical details.\n\nWe will address the specific points you raised below, but if there are any other points you would like us to address that aren\u2019t already covered by our rebuttal response, please let us know!\n\nWe intend to upstream any relevant changes into the paper later during the discussion period, and we will notify you when we\u2019ve done so.\n\n> For example, the abstract states a two-phase training procedure is used; yet the main paper does not mention the two-phase training procedure even once.\n\nWe apologise for not explicitly addressing the \u201ctwo-phase\u201d remark beyond the abstract, this was an oversight on our side and it will be fixed.\n\nFor completeness, we remark here that the two phases are referring to \n(1) pre-training the NAR (following Ibarz et al., LoG\u201922), and \n(2) fine-tuning the TransNAR using the NAR\u2019s embeddings (following our paper).\n\n> Related to this, the paper describes the system wide issues only from lines 213 to line 223 without mentioning some known issues.\n\nThank you for remarking this, we are very happy to supplement this discussion with additional issues that may arise in training such models.\n\n> For example, the particular implementation of the transNAR uses 6 layers and it is well known beyond two to three layers oversmoothing becomes an issue in graph neural networks.\n\nThe GNN layers used in NARs (including ours!) prefer the max aggregation function \u2013 a standard choice since the NEGA paper (Veli\u010dkovi\u0107 et al., ICLR\u201920) \u2013 and since max leverages a sharp decision across its neighbourhood, it is not vulnerable to over-smoothing, even over very long horizons.\n\n> The paper freezes the weights for graph embeddings, but multimodal models rely on a shared embedding space; the differences between the two embedding spaces should be a factor, yet there is no mention about that.\n\nThank you for remarking this; our approach mimics that of Flamingo, which also freezes the weights for its image embedding model. We acknowledge that this is not the only way to perform multimodality successfully, and will mention this explicitly in our discussion, along with a note about the potential challenges in differing embedding spaces.\n\n> One naive way to combine NAR and transformers for CLRS-text is to use start of the art transformers to convert the problems into the graphic form required by NAR and then apply NAR. \n\nThis is indeed a very interesting approach and we will explicitly note it in our discussion! \n\nIn the meantime, we prompted a few state-of-the-art models to extract feature matrices that could be used as NAR inputs from the textual data. \n\nWhile they were able to extract some interesting graph-style featurisations of the text data, we note that our NARs require highly bespoke data, including the detailed internal states of the algorithms considered, which no LLM was able to extract at this time. As such, we will not be able to feed such outputs into NAR.\n\n> Also the transformer model used in the paper is far from the state of the art; as a baseline, the authors should use the performance of a fine-tuned state-of-the art transformer model on the CLRS-text dataset. \n\nWe acknowledge that our Chinchilla-style base model is not present state-of-the-art at such a model scale; however we do believe our comparison is fair, because the TransNAR model also uses that same Chinchilla-style model as the starting point.\n\nTo address your question, we would like to point you to the CLRS-Text paper (Markeeva et al., 2024), where results for fine-tuning a modern Gemma 2B model on this data are provided. \n\nGenerally it does not seem that scale or algorithmic improvements of the Gemma 2B model have made a significant difference to the models\u2019 OOD capability compared to Chinchilla: on most algorithms, we observe that Gemma (much like Chinchilla) can learn to fit the algorithms well in-distribution, and out-of-distribution most algorithms promptly collapse.\n\n> In addition, it seems the input to the transformer model is only the text but the transNAR has dual inputs; the impact of the dual inputs should be factored in when stating the improvements.\n\nWe agree with you that the duality of its inputs are providing TransNAR with an advantage and are happy to directly address this. \n\nThat being said, we also wish to point you to our distillation experiments (Section 4.2.) where these capabilities are transferred back into simple Transformers, leading to improved OOD performance while no longer requiring dual inputs.\n\n_Answers to be continued in next message..._"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a transNAR architecture by having both texts and graphs as input and using cross-attention to combine the strengths of both. Systematic results on CLRS-text demonstrate significant improvements in a number of problems, even though no improvements are achieved in some of them."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper uses graphs and texts so that the new system can use the natural language inputs as well. The combination leads to significant performance improvements over the baseline systems. While the integration uses cross attention very similar to those in multimodal models, the benefits seem significant."
            },
            "weaknesses": {
                "value": "The paper is sketchy in covering important technical details. For example, the abstract states a two-phase training procedure is used; yet the main paper does not mention the two-phase training procedure even once. Related to this, the paper describes the system wide issues only from lines 213 to line 223 without mentioning some known issues. For example, the particular implementation of the transNAR uses 6 layers and it is well known beyond two to three layers oversmoothing becomes an issue in graph neural networks. The paper freezes the weights for graph embeddings, but multimodal models rely on a shared embedding space; the differences between the two embedding spaces should be a factor, yet there is no mention about that.\n\nA major strength of the paper is the improvements as highlighted in Figure 1. However, the baselines do not reflect the actual state of the art. One naive way to combine NAR and transformers for CLRS-text is to use start of the art transformers to convert the problems into the graphic form required by NAR and then apply NAR. Also the transformer model used in the paper is far from the state of the art; as a baseline, the authors should use the performance of a fine-tuned state-of-the art transformer model on the CLRS-text dataset. In addition, it seems the input to the transformer model is only the text but the transNAR has dual inputs; the impact of the dual inputs should be factored in when stating the improvements."
            },
            "questions": {
                "value": "- Is every iteration the same as illustrated in Figure 3? It seems it is not as the final output is only from the last layer; for other layers, the outputs from both modules feed to the next layer as inputs. Could you revise Figure 3 to show the entire process accurately?\n\n- Given that the proposed transNAR fails to improve on some of the problems, could you please comment on the limits of the NAR approach? While it can generalize over a larger range of lengths, NAR and transNAR would still lack the generalization and robustness of correct algorithms.\n\n- Reasoning is inherently sequantial but it seems the graphs are not. Could you comment on how the graphs emulate the sequential nature of reasoning steps?\n\n- One of the known issues with graphs is the oversmoothing issue. Could you comment how that affects the NAR and transNAR?\n\n- While the improvements over the baseline seem impressive, could ypu give the specific contributions beyond combining NAR and transformers? It seems to me a fair comparison is to use a straightforward combination as the baseline rather than the transformers alone, whose limitations are known in this area.\n\n- Could you provide references for transformers' natural langaue UNDERSTANDING properties? The paper mentioned that transformers have unrivaled such properties (Section 1)  and at the same time they are limited in generalization due to their autoregressive, causally-masked objective (Section 2). These two do not seem to go together well.\n\n- Would you opens-source the code code for your transNAR and baseline models? As many of the papers are from one group due to the unfair advantages created, it is not acceptable to the research community.\n\n- More generally, would you be committed to the principle that scientific research should be for the common good and accessible to all?\n\n- Could you comment why you chose to call Q as Key and K as Query in equations (1) and (3)?\n\n- How would you pronounce NAR? Please make sure it is consistent in the paper (for example in line 065 as \"a NAR-\" and as \"an NAR\" in many other places)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a TransNAR neural architecture that is a hybrid of Transformer and Neural Algorithmic Reasoner (NAR) architectures. The Transformer part of the model is allowed to cross-attend to NAR states. The model requires a hybrid input: the Transformer part expects text and the NAR part consumes a graph. The paper tests the proposed architecture on CLRS-Text, which is a text version of an algorithmic benchmark called CLRS-30. The model is trained on both textual inputs and the corresponding graph. The paper\u2019s main claims are that (1) TransNAR performs better than Transformer. (2) that TransNAR teacher can produce extra training data for the Transformer student, and that this data increases the generalization of the latter."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper was clear enough for me to understand the main idea. I appreciate that the paper contained a \u201cLimitations\u201d section."
            },
            "weaknesses": {
                "value": "The paper proposes a very straight-forward idea and presents unsurprising results. Of course a hybrid of a general-purpose model (transformer) and a task-specific model (NAR) will perform better on the specific task. Especially when the model requires special graph-structured data, which is the case for TransNAR. \n\nThe distillation results were hard for me to process and verify. The error bars (constructed on just 3 samples!) are very large. An obvious baseline is missing: distilling a NAR model to Transformer.\n\nOn the writing side, the paper could benefit from the following improvements: \n- Give more context to the reader:\n   - better explain CLRS-30 benchmarks\n   - better explain NAR architecture\n- Include a rigorous textual analysis of performance numbers (e.g.\" in 3 cases out of 7 our model is significantly better than the baseline (p-value = 0.01)\"), in addition to presenting all numbers on all tasks in large overwhelming figures\n- Include a tabular version of the results"
            },
            "questions": {
                "value": "Did you look into how the distillation data that TransNAR produces is different from the groundtruth?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new model architecture, TransNAR, that combines a Transformer model with a Neural Algorithmic Reasoner via a cross-attention mechanism. The motivation is to leverage the reasoning and generalization ability in NAR representation to improve the performance of the transformer model. The experiment shows promising results in the out-of-distribution regime and over 20% absolute improvement in several classes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The method is straightforward and well-motivated. Experiment results also show that it performs well on the targeted tasks compared to standard transformer architecture."
            },
            "weaknesses": {
                "value": "The proposed method is domain-specific. It's hard to imagine how this method would benefit broader domains, as the NAR requires a specific type of input. Combining this method with existing large language models would also require further experiments. Figure 5 and 6 are hard to read."
            },
            "questions": {
                "value": "1. How can we combine this method with a pretrained language model? So that we can leverage the math reasoning ability in state-of-the-art language models.\n2. Have you compared with using GNN-based NAR alone?\n3. How does a pretrained language model perform on these tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}