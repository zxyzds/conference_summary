{
    "id": "K3KrOsR6y9",
    "title": "LLMs Can Plan Only If We Tell Them",
    "abstract": "Large language models (LLMs) have demonstrated significant capabilities in natural language processing and reasoning, yet their effectiveness in autonomous planning has been under debate. While existing studies have utilized LLMs with external feedback mechanisms or in controlled environments for planning, these approaches often involve substantial computational and development resources due to the requirement for careful design and iterative backprompting. Moreover, even the most advanced LLMs like GPT-4 struggle to match human performance on standard planning benchmarks, such as the Blocksworld, without additional support. This paper investigates whether LLMs can independently generate long-horizon plans that rival human baselines. Our novel enhancements help achieve state-of-the-art results in planning benchmarks out-competing prior methods and human baselines all autonomously.",
    "keywords": [
        "large language models",
        "decision-making",
        "planning"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "Investigating what enables autonomous planning in large language models",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=K3KrOsR6y9",
    "pdf_link": "https://openreview.net/pdf?id=K3KrOsR6y9",
    "comments": [
        {
            "title": {
                "value": "New Experiments and Clarifications On Our Method's Assumptions"
            },
            "comment": {
                "value": "We would like to thank the reviewer for taking the time read our work, and we are excited to hear our work to bring \"a novel perspective\" and to have conducted \"comprehensive empirical evaluation across multiple challenging benchmarks\". We have acted on their suggestions and introduced new results to fully isolate the impact of each innovation and have more insightful error analysis for AoT+.\n\nThe reviewer's main concern is whether the assumption of having a PDDL instance of the problem limits our methods scalability to general domains.\n\n**Assumption of having a PDDL instance**\n\nWe do not make such an assumption. In the benchmarks we tested, only the Blocksworld and Logistics are given as PDDL instances, and the others (game of 24, crossword puzzle, creative writing, list functions and ACRE) do not use PDDL. We utilized PDDL as an additional challenge since we view outputting exact PDDL solution files instead of natural language actions as a more challenging setting. Therefore, we'd like to say repeat that AoT+ does not assume a PDDL instance of a problem, and it can be used in more general environments with even almost infinite number of actions (crossword puzzle) and infinite actions (creative writing, list functions and ACRE). The reason behind providing our wide range of benchmark results together with testing our method with many LLMs with wide variety of open and propietary LLMs was to show the generality and scalability of our method.\n\n**Isolating the impact of each innovation and further error analysis on AoT+**\n\nThank you for your request to see further error analysis together with the impact of each innovation. For the former, we went ahead and analyzed the state hallucinations for AoT and AoT+ on the logistics benchmark for puzzles with a depth of at least 20. For each depth, we randomly sampled 20 states to check whether it represents a correct state if the actions were taken according to the LLM (we chose LLaMA-3.1-70B model due to it being cheap and already having a score on the benchmark). We marked the states as errors if they had any mistakes in the states. **You can see the comparison in Appendix A.1 in Figure 5 (on page 13)**. Briefly, it shows that AoT tends to make a lot more mistakes in its beliefs about the states as the depth increases compared to AoT+. Furthermore, for AoT+, the cases where it is not able to reach a solution are mostly due to not being able to find a solution before reaching the max new token generation of 3072 set in our experiments.\n\nFor Llama-3.1-70B, out of 58 errors in 200 examples, in only 3 of them the model gave a solution step but was not executable (inadmissable action) or did not reach the goal, compared to 142 for AoT (out of 174 errors in 200 examples).\n\nFor isolating the impact of each innovation, we already have Table 1 and \"Figure 3\" (which is a table actually), showing relevant ablation studies for each innovation. However, we wanted to show a more fine-grained ablation study to show the impact of these on our main experiments as well. For this reason we added AoT+R for AoT with random solution traces, and AoT+M with memoization on all LLMs and benchmarks we tested. **Please check Appendix A.2 on the revised PDF of our paper (on page 14)**. Briefly, the results show that the random solution traces, do not affect the performance as much while making the generation of in-context examples much easier across the benchmarks and LLMs we tested. Furthermore, memoization does boost the performance to reach AoT+'s performance.\n\nWe hope we have both clarified and addressed your concerns. We kindly ask that you will reconsider your score based on our response. Feel free to let us know if you'd like us to address anything further. Thank you again for your thoughtful feedback!"
            }
        },
        {
            "summary": {
                "value": "This paper investigates whether Large Language Models (LLMs) can effectively generate long-horizon plans autonomously, without requiring external verification tools or complex frameworks. The authors introduce AoT+ (Algorithm-of-Thoughts Plus), an enhanced prompting technique that builds upon the original Algorithm of Thoughts (AoT) approach. The paper suggests that LLMs may possess latent planning capabilities that can be activated through appropriate structuring of the problem-solving process, without requiring external verification tools or complex frameworks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper presents a novel perspective challenging both overly pessimistic and optimistic views of LLMs' planning capabilities.\nThe AoT+ innovations are creative combinations of existing ideas since it uses periodic state regeneration to manage attention/cognitive load.\nThere is comprehensive empirical evaluation across multiple challenging benchmarks, such as clear ablation of components through comparison of AoT vs AoT+\nThe paper has well-structured progression of ideas from problem motivation to solution."
            },
            "weaknesses": {
                "value": "The paper focuses heavily on successful cases but lacks systematic analysis of where AoT+ fails.\nWhile the paper compares AoT vs AoT+, it doesn't fully isolate the impact of each innovation.\nThe AoT+ assumes we have a pddl instance of the problem, so I'm not sure if this method is scalable to general domain."
            },
            "questions": {
                "value": "Is there any scalable way to promote AoT+?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the planning capabilities of Large Language Models (LLMs) and presents an improved prompting technique which authors refer to as AoT+ because the method has been built on a previous method referred to as AoT. While current LLMs exhibit limited efficacy in complex planning tasks that demand multi-step, long-term reasoning, traditional methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) prompting encounter notable challenges due to lack of integrated error correction which limits their performance in planning scenarios. Both CoT and ToT lack the flexibility to backtrack, resulting in inadequate performance on benchmarks like Blocksworld. AoT that this paper has been built on enhances planning accuracy by incorporating human-like intuition and backtracking strategies. The proposed AoT+ technique introduces periodic structured state generation, which reiterates the problem state to help LLMs concentrate on pertinent information and reduce cognitive overload, along with random trajectory augmentation that uses random solution paths interspersed with correct steps, facilitating easier prompt creation while ensuring high accuracy. The effectiveness of AoT+ is validated through various experiments demonstrating its superior performance over existing methods across various tasks, including Blocksworld and Logistics, without the need for external verification tools. By utilizing structured prompts that help LLMs in state management and heuristic search, AoT+ also decreases token usage and computational time compared to other frameworks, enhancing its practicality for real-time applications."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Although the work has been built on existing AoT work, it still shows several strengths including: \n- Achieving state-of-the-art performance across complex planning benchmarks without the need for external verification tools, \n- Unlike approaches like Tree-of-Thought (ToT) that require extensive API requests and computational resources, AoT+ operates efficiently within a single-prompt framework, cutting down on token usage and latency. This improvement is also observed in AoT but token counts in AoT+ is more efficient.\n- The use of random solution trajectories in AoT+ (instead of rigid, human-crafted sequences) makes it easier to generate prompts and apply the method across various planning problems.\n- By leveraging memoization-inspired techniques for periodic state regeneration, AoT+ helps to improve issues around state hallucination (errors in tracking problem states), enhancing the model\u2019s ability to stay on track over multi-step tasks.\n- AoT+ demonstrates consistent performance improvements across various LLMs indicating its model agnostic nature."
            },
            "weaknesses": {
                "value": "Authors have done great work, and can potentially improve the paper more by addressing the following:\n- In terms of presentation I expect a more clear diagram explaining different stages of the proposed method. It took me some time to get a better sense of the proposed method by going through the details in methodology section. \n- While AoT+ performs well on the benchmarks reported in the paper, evaluation on real-world planning tasks like pathfinding for robotics would strengthen the work.\n- While AoT+ addresses state hallucination issues, the paper doesn\u2019t provide a detailed error analysis of where these hallucinations occur. Identifying specific failure points would offer valuable insights for refining state-tracking strategies."
            },
            "questions": {
                "value": "- Can you provide specific examples of common failure modes for AoT+, particularly regarding state hallucinations? Are there particular problem types or scenarios where these issues are more prevalent?\n- How interpretable are the decision paths generated by AoT+? Can users trace the model\u2019s reasoning steps and identify where an error may have occurred in the planning sequence?\n- Would incorporating intermediate reward structures within AoT+ improve long-horizon planning accuracy by incentivizing the model to reach sub-goals?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores the capabilities of LLM in autonomous planning, addressing their previous limitations when compared to human performance. Further, this paper proposes a new prompting method that enable LLMs to generate long-horizon plans autonomously."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- It presents a significant advancement in the autonomous planning capabilities of LLMs, demonstrating their potential to match or exceed human performance.\n- This paper proposes a prompting strategy to generate long-horizon plans."
            },
            "weaknesses": {
                "value": "- First, the identification of your performance gap has already been established[1].\n- However, several key baselines are missing. Although significant research addresses planning optimization strategies, much of it does not conduct experiments in the blocksworld domain [2-4]. Furthermore, baseline [5], which even operates in blocksworld, has not been directly compared.\n- Given that your method relies on search-based techniques, it would be beneficial to include comparisons with MCTS-Decoding or A*, as these are also search-based approaches. Please explain why these specific search-based approaches were not included. \n\nIf you can reasonably resolve my issue, I will reconsider my score.\n\n[1] Stechly et al. Chain of Thoughtlessness? An Analysis of CoT in Planning\n\n[2] Madaan et al. SELF-REFINE: ITERATIVE REFINEMENT WITH SELF-FEEDBACK\n\n[3] Shinn et al. Reflexion: Language Agents with Verbal Reinforcement Learning\n\n[4] Hu et al. Tree-Planner: Efficient Close-loop Task Planning with Large Language Models\n\n[5] Hao et al. Reasoning with Language Model is Planning with World Model"
            },
            "questions": {
                "value": "See Weakness for more details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes AoT+, a new prompting technique based on the previous work AoT, in research of LLM's ability for planning. The authors analysed the limitations of previous work, such as CoT, ToT, AoT. And then as a pre-study, it is showed that using random solution traces will not degrade the performance in comparison to AoT.\nThe authors further hypothesised that state hallucinations are due to continuous recomputation and tracking of the current state after each action. Based on this hypothesis, the authors introduced memoization in AoT+, inspired by dynamics programming. It is further shown that the attention weighs more on non-solution steps in AoT+.\nThe results demonstrate AoT has satisfactory performance improvement against other baselines and reduces the token usage.\n\nThe proposed method is based on previous AoT together with memoization and random traces. The method is simple yet effective. The paper shows promising results and improvement in comparison with baselines.  However, The paper itself lacks explanation in implementation of the proposed method. For example, it is unknown that how the authors interweave the correct solution path with random trajectories in 4.1 or how is memoization done in 4.2. Furthermore, the experiments can be done in a more consistent setting and more human performance data should be compared to support \"out-competing human baselines\". Until these concerns are resolved, this paper should be considered for a weak reject."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The paper conducted proper experiments on attention to support the hypothesis about state hallucination. \n* The proposed method shows promising results in higher performance and lower token usages."
            },
            "weaknesses": {
                "value": "* This paper would benefit from examples of AoT and AoT+, similar to how it benefits from examples of CoT.\n* The observation that random in-context examples does not hurt performance is not new. For instance Min 2022 (https://arxiv.org/abs/2202.12837) presents a study of what makes ICL work. I quote from their abstract: \"randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choice tasks, consistently over 12 different models including GPT-3\".\n* The authors can improve the illustration of the results by showing variance, confidences interval and other related statistical metrics, for example, in Table 3.\n* The author claims that the proposed method out-competes prior methods and human baselines. However, only human performance in Logistics is compared and only AoT+ with GPT4 can slightly outperform human performance.\n* Figure 1 and Figure 4 is not informative. Furthermore, the paper itself lacks descriptions of actual details of pipeline.\n* Though experiments to verify random solution traces and memoization are conducted, it is noticed they are done in different models and/or domains. It would be more convincing for extra ablation study under same settings in Section 5.2 main results.\n* The hypothesis in Section 4.2, \"that these hallucinations stem from the LLM\u2019s need to continuously recompute and track the current state after each action, potentially overwhelming its computational capacity as the solution trace grows longer\", though partially supported by a experiment in attention, is not fully explained or validated over the length of solution trace."
            },
            "questions": {
                "value": "* I think there might be a grammar error with the sentence: (L339) \"Table 3 demonstrates the more structured attention mechanism as a shift towards the visited states, resulting from our AoT+ approach with memoization.\" How is \"memoization\" implemented? What is the exact prompt modification? Why is this not presented in the main paper? Why does memoization prevent state identification?\n* What does CoT-SC mean in Table 1? It is not explained in the paper.\n* In Section 4.2: What specific version of LLaMA-13B did you use?\n* For random solution trajectories, what is the percentage of solution path? \n* How do you incorporate memoization into the proposed method?\n* Readability of figure 1 and figure 4 should be improved.\n* Figure 3 should be referred as a Table."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores various aspects of planning and problem-solving using language models, focusing on whether LLMs can independently generate long-horizon plans that rival human baselines. The authors introduce novel enhancements to the Algorithm-of-Thoughts (AoT), termed AoT+, which achieve state-of-the-art results in planning benchmarks, surpassing prior methods and human performance\u2014all autonomously. These enhancements, including Periodic Structured State Generation and Random Trajectory Augmentation, significantly improve LLM performance, suggesting that LLMs may have latent planning capabilities that can be unlocked through the right combination of context, structure, and guidance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The paper presents a highly original approach to addressing the limitations of large language models (LLMs) in autonomous planning. It introduces the Algorithm-of-Thoughts (AoT+), an enhanced prompting technique that builds upon the Algorithm of Thoughts (AoT) approach. By activating what the authors term \"System 3 thinking,\" a more deliberate decision-making process, the paper challenges the perceived boundaries of LLMs in complex planning tasks. The use of random solution traces and memoization to improve the performance of LLMs in planning further highlights the novelty of this work.\n\nThe paper maintains a high standard of quality in terms of its depth of analysis. It conducts experiments with a diverse set of language models and various prompting strategies, resulting in a comprehensive evaluation of the proposed methods. The authors effectively communicate complex concepts through clear figures and tables, and the structure is logical and easy to follow. The paper begins by identifying the limitations of prior work, proceeds to verify the efficacy of the proposed techniques through experiments, and concludes with an extensive evaluation of its findings.\n\nBy addressing LLMs\u2019 limitations in planning tasks and proposing novel enhancements, this paper significantly contributes to advancing the problem-solving and decision-making capabilities of language models."
            },
            "weaknesses": {
                "value": "AoT+ requires explicitly restating and caching the current problem state throughout the solution process, which introduces additional complexity. This can demand extra effort in crafting effective state representations, and may be particularly challenging in tasks where the state is difficult to define or capture, such as in ALFWorld.\n\nRegarding the token count comparison table, it would be more comprehensive to include other baselines, such as CoT and AoT, to provide a fuller comparison.\n\nThe paper does not specify which version of the Claude model is used in the main experiments (Claude 3.5, Sonnet?). Providing this information would enhance clarity and reproducibility.\n\nThe paper lacks details on the implementation of memoization, which makes it somewhat unclear."
            },
            "questions": {
                "value": "Why might using random trajectories, rather than carefully crafted ones, provide greater flexibility and generalizability as prompting strategies for planning problems? In the context of this paper, crafting examples for Blocksworld as well as other tasks appears relatively straightforward and requires minimal effort.\n\nWhat considerations should be made when representing complex states, such as those in environments like ALFWorld?\n\nHow is memoization actually incorporated in AoT+? Does it involve cache the tokens for problem definitions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}