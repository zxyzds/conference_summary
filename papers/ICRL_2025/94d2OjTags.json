{
    "id": "94d2OjTags",
    "title": "AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity",
    "abstract": "Recently, when dealing with high-resolution images, dominant large multimodal models (LMMs) usually divide them into multiple local images and one global image, which will lead to a large number of visual tokens. In this work, we introduce AVG-LLaVA, an LMM that can adaptively select the appropriate visual granularity based on the input image and instruction. This approach not only reduces the number of visual tokens and speeds up inference, but also improves the overall model performance. Specifically, we introduce the following modules based on LLaVA-NeXT: (a) a visual granularity scaler that includes multiple pooling layers to obtain visual tokens with different granularities; (b) a visual granularity router, which includes a Transformer layer, an MLP layer, and a voter layer, used to select the appropriate visual granularity based on the image and instruction. Furthermore, we propose RGLF, a novel training paradigm that aims at aligning the granularity predicted by the router with the preferences of the LMM, without the need for additional manually annotated data. Extensive experiments and analysis show that AVG-LLaVA achieves superior performance across 11 benchmarks, as well as significantly reduces the number of visual tokens and speeds up inference (e.g., an 85.3\\% reduction in visual tokens and a 2.53$\\times$ increase in inference speed on the AI2D benchmark).",
    "keywords": [
        "Large multimodal model",
        "multi-stage training",
        "adaptive visual granularity"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "AVG-LLaVA, an LMM that can adaptively select the appropriate visual granularity based on the input image and instruction.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=94d2OjTags",
    "pdf_link": "https://openreview.net/pdf?id=94d2OjTags",
    "comments": [
        {
            "summary": {
                "value": "The paper presents AVG-LLaVA, a large multimodal model capable of adaptively selecting the appropriate visual granularity based on input images and instructions, aiming to enhance model performance and reduce the number of visual tokens to expedite inference. AVG-LLaVA extends LLaVA-NeXT with the addition of a visual granularity scaler and a visual granularity router, along with a novel training paradigm called RGLF, which aligns the router's predicted probabilities of multiple granularities with the preferences of the LMM through a ranking loss."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper introduces a novel approach to handle high-res images by adaptively selecting the appropriate granularity based on the input image and instruction. Also, it conducted experiments to develop the appropriate tuning practice (the training state 3 & 4) to unlock the potential of the new paradigm.\n2. On multiple benchmarks, AVG-LLaVA demonstrates its efficacy. It can achieve better results compared to LLaVA-NeXT while consumes much less computations."
            },
            "weaknesses": {
                "value": "1. The training paradigm is complex. It incorporates two additional training stages, each requires extensive computation costs. The additional training cost may hinder this approach from being widely adopted.\n2. The framework is not thoroughly investigates and the ablation study is not sufficient (see Questions)."
            },
            "questions": {
                "value": "1. It's well known than finetuning VLMs on instruction tuning corpora with multiple epochs will typically improve the performance on benchmarks. The authors need to prove that the improvement cannot be simply attributed to 3x tuning epochs (corresponding to stage 2 to 4).\n2. Achieving better performance with fewer visual tokens is not a usual case. Would you please include more qualitative & quantitative examples & analysis and discuss under which circumstances the VLM can achieve this?\n3. The AVG-LLaVA framework can be easily extended to perform patch-wise granularity selection (for example, select different granularity for different patches). Would that be helpful to save more visual tokens under text-rich scenarios (the current AVG-LLaVA did not save much visual tokens for TextVQA and ChartQA). \n4. Recently, Qwen2-VL proposed to use native dynamic resolution visual encoders (no patchify) to generate visual embeddings. It would be beneficial to show that AVG-LLaVA also works for that kind of visual encoders."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a model that dynamically adjusts the granularity of visual tokens based on input images and instructions. This adaptive mechanism improves both efficiency and performance in multimodal tasks, reducing token usage and speeding up inference. The authors propose a novel training method, Ranking Granularity to Align LMM Feedback (RGLF), and test the model across 11 benchmarks. While the approach optimizes efficiency, concerns remain regarding scalability and performance trade-offs on certain tasks. The work offers promising advancements in multimodal learning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper introduces a visual granularity scaler and router, which adaptively selects the appropriate granularity for visual tokens based on the input image and instructions. This adaptive selection mechanism is a significant advancement over static high-resolution LMMs, potentially improving both efficiency and accuracy in multimodal tasks."
            },
            "weaknesses": {
                "value": "1.\tLack of novelty: The motivation of this paper is highly similar to Matryoshka model, which also employs hierarchical token merging for visual token reduction, akin to token pruning in this paper. It seems that the difference is that the authors design an router to allocate weights to several granularities, which is incremental in terms of novelty.\n2.\tInsufficient experiments: This paper does not fully explore alternative approaches for granularity selection, such as task-specific fine-tuning or manual selection for certain tasks that might further improve performance.\n3.\tWhile the model's adaptive granularity selection is a strength, the architecture of the visual granularity router (involving multiple pooling layers, Transformer layers, and a voter layer) adds significant complexity and a substantial computational cost.\n4.\tThe performance improvement is not superior across all benchmarks. For example, in GQA and ScienceQA, the proposed method underperforms slightly compared to some baselines, raising concerns about whether token reduction is always beneficial.\n5.\tRepeated Training Data: The training data for Stages 2, 3, and 4 are identical. Therefore, it is unclear whether the performance improvement is due to repeated training, akin to training for three epochs.\n6.\tPerformance on OCR Tasks: As shown in Table 5, the visual tokens for OCR tasks are almost entirely retained, rendering the filter ineffective. The improvement in OCR tasks may primarily stem from repeated training."
            },
            "questions": {
                "value": "1.\tThe ablation study in Section 4.5 suggests a strong reliance on instruction tokens for granularity selection. Could the model's robustness be affected in situations where instructions are ambiguous or noisy? This is more important to the industry from my perspective.\n2.\tThe benchmarks used are well-known public datasets. However, has the model been evaluated in real-world scenarios with less curated, noisier data? This would test its robustness in a more practical context.\n3.\tTraining Cost: Provide details of the training costs associated with each of the four training stages.\n4.\tComparative Experiment: Conduct a comparative experiment by training LLaVA-Next with repeated SFT data two or three times and present the detailed results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work aims to enhance the LMM LLAVA-NeXT through improved visual granularity selection. \nTo achieve this, we introduce AVG-LLAVA, which consists of a visual granularity scaler, \na visual granularity router, and the RGLF training paradigm. \nExperiments have been conducted to validate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The research focus  is intriguing, particularly the aspects of visual granularity selection and the Ranking Granularity to Align LMM Feedback.\n\n2. Experimental results demonstrate its effectiveness."
            },
            "weaknesses": {
                "value": "1. The training pipeline has become more complicated, moving from original two stages to four, which increases the training overhead despite the performance improvements.\n\n2. I think the description of the main contributions is not well-articulated; it should better to include an algorithm, especially the Visual Granularity Router.\n\n3. It would be beneficial to provide direct, rigorous evidence for the selection of granularity to illustrate the proposed method.\n\n4. Providing visual examples that highlight the need for granularity, such as attention maps of visual tokens in the LLM, would be advantageous.\n\n5. In Table 3, for ChartQA, the token per grid is 99.1%, while the speed is 0.97x without any increment."
            },
            "questions": {
                "value": "It should better to provide total token numbers of each method in main performance comparsion for each method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose an adaptive visual granularity mechanism dubbed AVG-LLaVA.  Based on this assumption, they employ the visual granularity scaler to generate visual tokens with various granularities, and the visual granularity router to select the appropriate visual granularity. Besides, the paper introduces a training paradigm RGLF to enhance the router.  Comprehensive experiments are performed on various visual benchmarks to validate the effectiveness of the method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The motivation is novel, and different prompts require information at different visual granularities. And the manuscript is explicit and well-organized.\n2. The authors solve the problem of training the router in VLM directly and utilize the ranking loss to supervise, which is impressive.\n3. Experimental validation is sufficient. The authors conduct comprehensive experiments on various tasks and show improvements, to validate the effectiveness of the method."
            },
            "weaknesses": {
                "value": "1. The method lacks novelty. (1) the multiple pooling operation in visual granularity scaler is very common, like the most classic SPPNet [1]. (2) the router operation has been proposed for many years.\n2. Although the method sounds simple, the overall pipeline is complex. The stage 2 and 3 cost more training resources and time, where the vision encoder and LLM both are trained.\n\n[1] He, K., Zhang, X., Ren, S. and Sun, J., 2015. Spatial pyramid pooling in deep convolutional networks for visual recognition.\u00a0*IEEE transactions on pattern analysis and machine intelligence*,\u00a0*37*(9), pp.1904-1916."
            },
            "questions": {
                "value": "1. Is it convenient to list their accuracy in Table 3 for further comparison? Besides, I want to know the absolute value of its actual speed.\n2. I would like to see a visualization of actual token clipping, such as the image in Figure 1, and what the router results would be for different prompts."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}