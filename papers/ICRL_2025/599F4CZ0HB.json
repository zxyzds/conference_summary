{
    "id": "599F4CZ0HB",
    "title": "Bench-O-Matic: Automating Benchmark Curation from Crowdsourced Data",
    "abstract": "The rapid evolution of Large Language Models (LLMs) has outpaced the development of model evaluation, highlighting the need for continuous curation of new,\nchallenging benchmarks. However, manual curation of high-quality, human-aligned\nbenchmarks is expensive and time-consuming. To address this, we introduce Bench-O-Matic, an automated pipeline that leverages LLMs to curate high-quality, open-\nended prompts from large, crowd-sourced datasets, enabling continuous benchmark\nupdates without human in the loop. We apply Bench-O-Matic to datasets such as\nChatbot Arena and WildChat-1M, extracting challenging prompts and utilizing\nLLM-as-a-Judge for automatic model evaluation. To validate benchmark quality,\nwe propose new metrics to measure a benchmark\u2019s alignment with human preferences and ability to separate models. We release Eval-O-Matic, a benchmark\nconsisting 500 challenging prompts curated by Bench-O-Matic. Eval-O-Matic\nprovides 3x higher separation of model performances compared to MT-Bench and\nachieves 98.6% correlation with human preference rankings, all at a cost of $20.\nOur work sets a new framework for the scalable curation of automated benchmarks\nfrom extensive data.",
    "keywords": [
        "LLM",
        "Evaluation"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "Scalable curation of high-quality, automated benchmarks from extensive data without human in the loop.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=599F4CZ0HB",
    "pdf_link": "https://openreview.net/pdf?id=599F4CZ0HB",
    "comments": [
        {
            "summary": {
                "value": "The work proposes Bench-O-Matic, a system for automatically curating high-quality, open-ended LLM benchmarks by using large-scale, crowd-sourced data. This tool addresses the need for evolving benchmarks that adapt to the rapid development of LLMs without requiring human intervention."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Bench-O-Matic efficiently creates high-quality benchmarks from crowd-sourced data without human input, the work addressed the scalability issue in benchmark curation.\n- The work Introduces novel metrics like Separability with Confidence and Pair Rank Brier Score, enhancing the robustness and reliability of benchmark assessments.\n- Eval-O-Matic achieves strong performance alignment with human preferences for only $20 per evaluation, and provides a cost-effective alternative to static benchmarks."
            },
            "weaknesses": {
                "value": "- Quality insurance. The seven quality criteria may not fully encompass the diversity of user tasks, potentially favoring specific types of prompts over others.\n- The synthesis of data is reliant on on LLMs as Judges. The LLM-as-a-Judge framework may introduce stylistic or self-bias, even with adjustments, which could influence benchmark objectivity in certain cases."
            },
            "questions": {
                "value": "Is there any estimation on the error/quality of the data generated? Or using some metrics to evaluate the similarity of the generated data with the real-world data?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces an automated pipeline, Bench-O-Matic, designed to curate prompts and create benchmarks for evaluating large language models (LLMs). The authors propose new metrics to assess benchmark quality, ensuring a clear separability of confidence scores and alignment with human preferences. The prompts are organized into topic clusters to ensure diversity, and an \"LLM-as-a-Judge\" approach is used to evaluate responses from various LLMs, fully automating the evaluation process. Additionally, the paper presents two novel benchmarks generated using this pipeline: Eval-O-Matic, based on Chatbot Arena, and Wild-O-Matic, derived from WildChat-1M."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- The problem statement is clearly defined.\n- The paper addresses a significant challenge highly relevant to the current state of AI and places well in the current literature.\n- The pipeline is flexible and open-ended, allowing for continuous improvements over time.\n- The experiments are comprehensive, demonstrating that the pipeline effectively creates benchmarks based on the metrics defined in the paper, with multiple LLMs evaluated on Eval-O-Matic.\n- The paper presents new ideas to evaluate benchmarks to overcome previous issues."
            },
            "weaknesses": {
                "value": "- Using an LLM to evaluate other LLMs\u2019 responses may limit the complexity of the benchmark prompts. While employing an ensemble of judges partially mitigates this issue, there is still an inherent limitation. However, the advantages of an automated pipeline outweigh this concern, and the authors have implemented techniques to reduce evaluation biases.\n\nI have a hard time finding weaknesses for the paper. It is a well-executed and solid paper, though not necessarily groundbreaking.\n\n**Minor Comments** \n- On line 80, \"achieve 98.6% correlation\" should be \"achieve**s** 98.6% correlation\".\n- On line 82, \"Our work**s** makes\" should be \"Our work makes\".\n- On lines 206 and 352, \"Section C\" should probably be changed for \"Appendix C\" for clarity.\n- On line 464, \"an regression based approach\" should be corrected to \"**a** regression-based approach.\""
            },
            "questions": {
                "value": "- Previous studies have shown that fine-tuning the LLM-as-a-Judge can significantly improve evaluation robustness. Has this been considered in the current work? This could help improve the quality of the judges, the main limitation of this benchmark.\n- In Section 4.2, it states, \"We also ensure the final dataset is free from personally identifiable information or offensive content.\" Could the authors elaborate on how this is achieved? Was this done manually or automatically with the help of an LLM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes approaches to automate the benchmark generation process via the prompting of LLMs. The proposals for different characteristics to establish the baselines are fair and the contributions are around the different scoring mechanisms to 1) evaluate the quality of prompts 2) LLM-based judging of prompt outputs to generate 1-5 score instead of binary preferences and 3) combining them with statistical aggregators to differentiate end evaluate different LLM outputs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The promise of the paper is excellent if delivered -- Reconfigurable automated benchmarks without humans in the loop and via crowd sourced data. With a series of prompting techniques in the pipeline, the approach is fair and well studied. Key innovations are in the design of metrics to separate various models and crux of thesis on generating evaluation data that is of high quality and can be separable."
            },
            "weaknesses": {
                "value": "The key weaknesses around this paper are the claims that the proposed approach is human-free and easily configurable as shown in the Table comparing the multiple methods. Given that the approach leverages use of ChatBotArena supplied queries  and even though the quality filter will remove the specific poor quality queries, it is not free from the input i.e., humans prompting the different LLMs on the arena and easily being configured to a use case that end users may have in mind. Discussing results on adapting the evaluation framework to beyond what is available in Chat bot Arena would be needed to support the claims of the paper.  Also it would be good to discuss potential biases introduced by using ChatBotArena queries as a starting point. The paper could be strengthened by providing concrete examples or experiments showing how their approach could be adapted to different domains or use cases beyond ChatBot Arena data\n\n\n\nAn additional area of concern is that almost every step of the pipeline involves a prompt engineering exercise including scoring the final models on a scale of 1-5. This is standard but the question emerges on the fidelity of the LLMs themselves and when they hallucinate themselves. As evidenced by the score sorted by topic cluster, the data does show that for exact answer situations like Python game coding versus loose open ended questions the LLM-judges are not very good.  To strengthen the paper - discuss potential failure modes or biases introduced by relying heavily on LLMs, provide more detailed analysis of how performance varies across different types of questions or topics and suggest ways to mitigate or detect potential hallucinations or errors introduced by LLMs in the pipeline\n\n\nThe details of human annotation were very unclear. See questions below."
            },
            "questions": {
                "value": "- Is the approach really adaptable/configurable ? Restate the claims if not. \n- Can the approach work irrespective of humans in the loop ? i.e., crowd-sourcer providing initial prompts. \n- human Annotation study; \n    How many human annotators were involved?\n    What was the inter-annotator agreement?\n    How were discrepancies between annotators resolved?\n    Were the human annotators experts in any particular domains?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}