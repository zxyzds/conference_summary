{
    "id": "wgRQ2WAORJ",
    "title": "Aligning Visual Contrastive learning models via Preference Optimization",
    "abstract": "Contrastive learning models have demonstrated impressive abilities to capture semantic similarities by aligning representations in the embedding space. However, their performance can be limited by the quality of the training data and its inherent biases. While techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have been applied to generative models to align them with human preferences, their use in contrastive learning is less explored.\nThis paper introduces a novel method for training contrastive learning models using Preference Optimization (PO) to break down complex concepts. Our method systematically aligns model behavior with desired preferences, enhancing performance on the targeted task. In particular, we focus on enhancing model robustness against typographic attacks, commonly seen in contrastive models like CLIP. We further apply our method to disentangle gender understanding and mitigate gender biases, offering a more nuanced control over these sensitive attributes. Our experiments\\footnote{Code available at: \\href{https://shorturl.at/FN1e8}{https://shorturl.at/FN1e8}} We demonstrate that models trained using PO outperform standard contrastive learning techniques while retaining their ability to handle adversarial challenges and maintain good accuracy on other downstream tasks. This makes our method well-suited for tasks requiring fairness, robustness, and alignment with specific preferences. We evaluate our method on several vision-language tasks, tackling challenges such as typographic attacks. Additionally, we explore the model's ability to disentangle gender concepts and mitigate gender bias, showcasing the versatility of our approach.",
    "keywords": [
        "contrastive learning",
        "preference optimization",
        "alignment",
        "reinforcement learning from human feedback",
        "robustness",
        "computer vision"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "In this work we propose preference-based optimization, to finetune and align contrastive learning-based models such as CLIP.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wgRQ2WAORJ",
    "pdf_link": "https://openreview.net/pdf?id=wgRQ2WAORJ",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces an alignment method designed for contrastive models, such as CLIP, using aligned and unaligned image-text pairs. In this setup, each image input has a preferred (or aligned) response and an dispreferred (or unaligned) output. The model is trained to differentiate between these two responses using preference optimisation designed as a one-step Markov decision process. Therefore, they use a preference dataset that pairs images with aligned and unaligned responses, and a regularisation dataset containing clean examples to maintain the model's ability to generalise to other downstream tasks. Importantly, they don't fine-tune the full model but train a single linear projection layer on top of the frozen text and image encoders. \n\nTo further control the model's behaviour, the authors modify the singular values of the learned linear transformation. Specifically, they apply a singular value decomposition (SVD) to the weight matrix of this layer and scale all singular values using a scaling parameter $t$. This intervention technique builds on the intuition that the linear transformation transforms the original similarity function between image and text spaces.\n\nThey evaluate the effectiveness of their method in two settings. First, they evaluate its effect on typographic robustness by comparing it against baseline models (incl. standard CLIP, PAINT, Defense-Prefix) across nine datasets (incl. ImageNet, Flowers102, and EuroSAT). The preference dataset is created by adding misleading text to the original images of each dataset. They find that their method performs on par or better than prior methods, with a few exceptions. Despite improving the robustness, some performance gaps between the original and typographic dataset remain; for example, a gap of around 20 % on StanfordCars. Using the intervention technique leveraging the SVD of the linear projection layer, they show that they can modify the trade-off between OCR and object detection performance. In the second setting, they explore the possibility to disentangle gender representations. They train the linear transformation using a dataset of images depicting men and women during activities, and show that by scaling the singular values they can reverse gender-specific representations, including a specific scaling factor where the gender information is effectively neutralised, without significant degradation on the downstream task."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper appears to be the first paper to apply preference optimisation to contrastive models, and presents an interesting use of SVD to control model behaviour. \n- Optimising robustness and mitigating (gender) biases are of significant interest, especially in high-risk domains. \n- The evaluation results suggest comparable and often better performance than alternative approaches in improving robustness while enabling a (to some degree) interpretable intervention technique. \n- The paper is well written and easy-to-follow."
            },
            "weaknesses": {
                "value": "- Despite improving robustness over baseline methods in some datasets, none of the methods consistently outperforms other methods (see Table 1). \n- The baseline methods, PAINT and Defense-prefix, and their differences to the proposed method are not explained in the paper.  \n\nMinor Comments: \n- Line 23: Incomplete sentence \u201eOur experiments We demonstrate\u201c. \n- Line 256: Comma instead of dot used. \n- Line 258: Comma should be a dot, and dot should be a comma. \n- Line 289: \u201ethis\u201c -> \u201eThis\u201c \n- The differences in Table 1 appear to computed inconsistently. While most of the time the differences are computed based on the best alternative method incl. the base model (e.g. OxfordPets), the difference for DTD O is computed with respect to PAINT, whereas CLIP seems to performs better. Overall, I think it would be easier to follow if all differences would be reported relative to the base CLIP model."
            },
            "questions": {
                "value": "- The role of the transformation scaling parameter $t$ in the results in Table 1 remains unclear to me. Is the parameter varied between all settings or kept constant?\n- The scaling range from -2 to 1.2 in Figure 2 has likely been chosen because the performance improves up until that point. But what happens if you scale between e.g. -4 and 4? It would be interesting to see even if performance starts to deteriorate after some threshold.\n- Have you tried training separate linear projection layers for the text and image decoders?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a method for aligning contrastive learning models (CLIP), with human preferences using preference optimization techniques such as DPO and IPO. By formulating contrastive learning as a one-step MDP and fine-tuning CLIP with these techniques, the authors enhance the model robustness of CLIP against typographic attacks and mitigate biases, particularly around gender. Experimental results show improvement on multiple datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "Originality: This is the first work to improve contrastive learning models through Preference Optimization. The idea of leveraging true labels and typographic labels for preferences, instead of curating a separate preference set from human annotation, is novel and interesting.\n\nClarity: This paper is well-written and has very clear motivations, backgrounds, methods, and experiments.\u00a0\n\nSignificance: The topic of aligning human preferences in contrastive learning is impactful, as models like CLIP are now used widely, yet many undesirable behaviors such as gender biases still exist."
            },
            "weaknesses": {
                "value": "Significance: this paper relies on a preference dataset, which requires heavy annotations and the preference set will be very small compared to the training set of CLIP. Also, the preference would be very task-specific (e.g., typographic or gender), limiting the generalizability of the approach to new, unseen attacks or biases.\n\nQuality: the inclusion of SVD makes it much slower to fine-tune on a larger scale. Also, the experiments focus on controlled, relatively smaller-scale datasets (the largest being ImageNet100), so the effectiveness of the approach is yet to be seen on diverse, complex large-scale datasets."
            },
            "questions": {
                "value": "Broadly speaking, for a general image-text task, e.g., VQA or retrieval, is there any guidance to design the preferences? The easiest way is following standard RLHF and curating a set with actual human preferences, but could the authors kindly suggest any other auxiliary information we can leverage?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper revisits well-known alignment techniques, such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO), in the representation space learned by CLIP. The idea is simple yet effective: reformulate the policy $\\pi$ in DPO and IPO by using the similarity scores between preference texts, $y_w$ (preferred) and $y_l$ (unpreferred), and the given adversarial image $x'$. The authors evaluate the proposed method on typographic attacks and show that it improves the CLIP model\u2019s robustness to these attacks while preserving performance on the original datasets (without typographic attacks). To mitigate the overfitting issue of training large models on small datasets, the authors propose training a linear layer (parameterized by $W$) appended to the visual encoder, with both the pre-trained visual and text encoders frozen. Additionally, the authors propose applying SVD decomposition over $W$ as $W=U\\Sigma^tV$, allowing the alignment magnitude to be controlled by $t\\in\\mathcal{R}$. The authors demonstrate that a learned alignment for gender bias can be effectively controlled by adjusting $t$."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed method is simple yet effective.\n2. The authors provide a new perspective on IPO and DPO concerning the representation space learned by CLIP.\n3. The alignment controllability through $t$ is effective.\n4. The background and motivation are well-organized."
            },
            "weaknesses": {
                "value": "1. Clarity needs improvement.\n    * $\\mathcal{L}_{pref}$ in (10) appears without a definition. In Corollary 3.2, it is assumed to be either the DPO loss or IPO loss, while the experiments further include the case of KTO loss.\n    * In (9), $\\mathcal{I}_{ref}$ is frozen and has no trainable parameters, contributing solely to per-example weighting when substituted in (5), (6), and (7). It is recommended to clarify this in advance.\n    * In Fig.1, $\\mathcal{L}_{pref}$ is computed with the given triplet $(y_w, y_l, x\u2019)$, where $x\u2019$ is an adversarial image. The presence of multiple negative text representations, such as $\\tau_1$, $\\tau_2$, and $\\tau_3$, is confusing without specifying either $y_w$ or $y_l$ as text inputs.\n    * The overall loss in (13) is iterated over two different datasets, $D_{pref}$ and $D_{reg}$, simultaneously. Further explanation is needed on how the inputs $(y_w, y_l, x\u2019)\\in D_{pref}$ and $x\\in D_{reg}$ are paired or sampled.\n    * The bottom row (differences) in Table 1 is confusing, and it cannot correctly demonstrate the trade-off between O (Original dataset) and T (Target dataset) for each variant of the proposed method. The reviewer recommends indicating the improvement or degradation alongside each accuracy as $\\color{green}{(+1.0)}$ or $\\color{red}{(-1.0)}$ relative to the base model, i.e., CLIP, for clarity.\n2. Lack of a concrete conclusion over comparisons with baselines. The results in Table 1 deserve more discussion. Examples are listed below.\n    * No method in Table 1 consistently outperforms the others. Is there a large domain gap between different datasets that prevents any method from generalizing well across all of them?\n    * PAINT significantly outperforms the proposed method (including all variants: DPO, IPO, and KTO) on both O and T in the ImageNet* column. Is the constraint of a single trainable linear layer in the proposed method too restrictive?\n    * A comparison between different variants of the proposed method would be valuable. For example, what types of inputs are weighted more in different variants according to (10)?"
            },
            "questions": {
                "value": "The reviewer appreciates the well-organized background of DPO, IPO, and KTO, as well as the new perspective on these methods with CLIP. The paper focuses on providing new insights into preference optimization with CLIP. However, the most significant difference between DPO, IPO, and KTO\u2014i.e., per-example weighting\u2014is not discussed sufficiently in the paper (KTO might require further discussion). Additionally, the lack of a thorough comparison and the absence of clear distinctions in Table 1 together weaken the contribution. Therefore, the reviewer\u2019s main questions are as follows:\n\n1. What are the differences among the proposed method variants? Which types of inputs or datasets are weighted more or preferred for each variant? \n2. Could these variants be unified into a single method that outperforms the other baselines in Table 1?\n\nThe reviewer is open to reconsidering the rating if the authors could address these questions (including those in Weakness section).\n\nSome typos:\n* In Ln. 023, \u201cOur experiments\u201d appears incorrectly inserted.\n* Table 2 lacks a reference. The possible related reference is in Ln. 1073 in the appendix.\n* Several papers in the references are duplicated: Ln. 551 and Ln. 554; Ln. 559 and Ln. 562; Ln. 681 and Ln. 687; Ln. 750 and Ln. 754."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Preference Optimization for training the contrastive learning model CLIP, aiming to enhance the model's robustness against typographic attacks and mitigate gender biases. This approach aligns the model with human preferences. Experimental results on datasets such as ImageNet, Caltech101, and OxfordPets demonstrate the effectiveness of this method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "It is significant to explore aligning non-generative model with human preferences using Preference Optimization.\n\nThis paper is well-motivated."
            },
            "weaknesses": {
                "value": "See questions."
            },
            "questions": {
                "value": "1. Could this method be applied to other tasks apart from enhancing robustness against typographic attacks and mitigating gender biases?\n2. Could you provide the ablation study results for components $\\mathcal{L}_{pref}$ and $\\mathcal{L}_{reg}$ in the loss function?\n3. I do not understand the image in the left part of Figure 1. What does the obscured dog in the left part of Figure 1 signify?\n4. What is the role of section 3.4 in your method? Why is fine-tuning the model mentioned in Section 3.4?\n5. Could you provide evidence for your claim \u201cthe overall matrix $W^TW$ remains close to the identity matrix\u201d in Section3.4\uff1f \n6. Section 3 does not clearly introduce the method. In the last part of Section 3, you should package and summarize your method to give readers an overall understanding.\n7. In Table 1, although the accuracy on the typographic dataset has increased compared to other methods, the accuracy on the original dataset has generally decreased. Therefore, the method has harmed the pretrained knowledge.\n8. Could you provide an analysis of the different performances of DPO, IPO, and KTO in your method in Section 4?\n9. What is the relationship between Section 4.4.1 and Section 4.1? I am not sure about the role of Section 4.4.1.\n10.  What is the meaning of transformation scaling t in Section 4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}