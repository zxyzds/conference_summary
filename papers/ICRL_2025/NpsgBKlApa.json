{
    "id": "NpsgBKlApa",
    "title": "Less is More: Adaptive Coverage for Synthetic Training Data",
    "abstract": "Synthetic training data generation with Large Language Models (LLMs) like Google's Gemma and OpenAI's GPT offer a promising solution to the challenge of obtaining large, labeled datasets for training classifiers, especially when rapid model deployment is critical, such as classifying emerging social media trends or combating new forms of online abuse tied to current events. While prior research has examined the comparability of synthetic data to human-labeled data, this study introduces a novel sampling algorithm based on the maximum coverage problem to select a representative subset from a synthetically generated dataset. Our results demonstrate that training a classifier on this contextually sampled subset achieves superior performance compared to training on the entire dataset. This ``less is more'' approach not only improves accuracy but also reduces the volume of data required, leading to potentially more efficient training.",
    "keywords": [
        "Large Language Models (LLMs)",
        "Synthetic Data Generation",
        "Sampling Algorithms",
        "Maximum Coverage Problem",
        "Data Efficiency"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "Train a better classifier with less synthetic data from LLMs by intelligently selecting the most informative examples.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=NpsgBKlApa",
    "pdf_link": "https://openreview.net/pdf?id=NpsgBKlApa",
    "comments": [
        {
            "summary": {
                "value": "This paper aims to take better usage of LLM generated synthetic training data, Specifically, how to downsample large synthetic datasets to select the most informative and diverse subset of data points for training machine learning models. The paper proposes a novel binary search algorithm that determines the optimal configuration for max coverage sampling. The main idea is to first embed the data into a latent space and construct a similarity graph where nodes represent data points and edges are weighted by pairwise cosine similarity. On this graph,   a greedy max-coverage approximation algorithm is applied to  prune edges through a binary search procedure to identify the best k \u201drepresentative\u201d samples for fine-tuning a model on various downstream tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "It is an important task for selecting LLM-generated samples."
            },
            "weaknesses": {
                "value": "W1: The selection process begins with constructing a similarity graph, which incurs significant computational cost due to its quadratic complexity relative to the number of data points. According to the evaluation results, this costly approach only marginally outperforms simpler methods like k-means clustering. This raises the question of whether the additional computational expense is justified, and if such a complex selection process is truly necessary.\n\nW2: The selection process operates independently of any machine learning model training, which means it does not necessarily guarantee an \u201coptimal\u201d subset for the intended learning tasks. This raises concerns about its effectiveness in selecting subsets that are truly beneficial for model performance.\n\nW3: The writing should be improved. There are a lot of minor mistakes, such as \u201ccu- rated dataset\u201d, \u201cChen et al.\u201d (without year in citation),\u201d (Algorithm ??)\u201d"
            },
            "questions": {
                "value": "Q1: It remains unclear how this selection process addresses key issues associated with LLM-generated synthetic data, such as deviations in distribution from real-world data and imbalanced class distributions. How does this method ensure that the selected subset effectively mitigates these potential issues in synthetic datasets? Further clarification is needed on this point.\n\nQ2: There is no comparison with the closely related work (Chen et al)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper claims an idea that training a classifier on the contextually sampled subset achieves superior performance compared to training on the entire  dataset and creates a novel sampling algorithm named Adaptive Coverage Sampling to select the representative subset from a synthetically generated dataset. In the paper, the author theoretically proves the effectiveness and correctness of ACS and demonstrates it through empirical experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "In order to select a good representative subset from the data set, the author uses a new binary search algorithm to determine the threshold of cosine similarity between data points. Two data points are connected if the similarity between them is greater than the threshold. At the same time, a concept  'coverage' is creatively proposed to represent the probability that the points on the graph are adjacent to or overlap with the selected points. The point selection method that maximizes this coverage makes the final selected data subset.\n\nThis method of choosing a representative subset is of great originality. Then, the author explains the process and effectiveness of ACS, which is logical and concise. What's more, the research on the selection of synthetic data is also very meaningful."
            },
            "weaknesses": {
                "value": "Firstly, i think it mains a question that how to determine the value of k (percentage of data) when trying to get a representative subset of the whole dataset. In another word, if i want to use this method to choose a representative subset of a synthetic dataset, what percent should i retain? \n\nSecondly, there lacks a legend to explain the meaning of the dotted lines in the figures on section 5, I read this section several times just to find your result of models trained on whole dataset."
            },
            "questions": {
                "value": "According to my understanding of your paper, the execution process of ACS is to first determine the value of coverage, then calculate the similarity threshold through binary search according to the value of coverage, then build the similarity graph, and finally select the data points by greedy method. But i can't see what the value of coverage you use for the experiment on section 5, can you explain it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose Adaptive Coverage Sampling (ACS), a novel method designed to optimize synthetic training data selection\uff0cwhich can identify a representative, diverse subset from large synthetic datasets, improving training efficiency and model accuracy. They use a max coverage sampling algorithm with binary search on similarity graphs to achieve an optimal coverage threshold. The experiments demonstrate that ACS can significantly improve performance on downstream tasks such as sentiment analysis and relation extraction, using only a fraction of the synthetic data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThis study addresses an important and common issue in synthetic data applications, optimizing synthetic data usage by focusing on selecting representative subsets.\n2.\tThe authors present a comprehensive empirical analysis of their ACS method.\n3.\tThe experiments are conducted extensively, spanning multiple tasks such as sentiment analysis and relation extraction, and analyzed in detail, which demonstrates the versatility of ACS."
            },
            "weaknesses": {
                "value": "1.\tIt would be beneficial for the paper to include an ablation study analyzing the impact of the two constraints used in ACS. This would help clarify the role each constraint plays in the sampling process and its contribution to overall model performance.\n2.\tThe paper only compares ACS with a few basic sampling approaches (e.g., random and k-means). Including more relevant baselines, such as Alpagasus, could provide a more comprehensive view of ACS\u2019s advantages and limitations. \n3.\tWhile the authors mention that ACS enhances data diversity, the experiments focus mainly on accuracy and F1 score. Providing qualitative or quantitative analyses of diversity would strengthen the paper, giving more insight into the diversity aspect of the sampled subsets. \n4.\tThe paper contains some minor typos, such as \u201cAlgorithm?? \u201d on line 341 and 'Pegasus' should be replaced with 'Alpagasus' on line 133."
            },
            "questions": {
                "value": "1.\tFor a new downstream task, how should one set key hyperparameters such as similarity threshold, maximum nearest neighbors, and coverage values? \n2.\tHow does the proposed method's computational complexity scale with larger datasets?\n3.\tSee the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel sampling strategy, called ACS, for selecting a subset of high-quality samples from a large set of synthetically generated dataset. The main motivation for the strategy is that there is often a lot of redundant samples when generating samples from the large language models, which causes problems for the training (e.g., overfitting on too obvious examples). To deal with this, the method is designed as a maximum coverage problem -- a similarity graph is constructed based on the sample embeddings, the edges are pruned according to a novel binary search procedure and the best k samples are generated. The authors show that this approaches outperforms other baselines and leads to comparable or higher performance than training on the full synthetic dataset."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors propose a graph-based solution using a maximum coverage, which is a novel solution for the problem of sampling a set of high-quality samples. In addition, the method is theoretically grounded.\n\nThe authors explicitly take randomness into consideration by repeating the fine-tuning of the models over multiple initialisations. This is also clearly shows the strength of the sampling strategy -- it leads to a significantly lower deviation in the fine-tuning results over the baselines, which may point to the fact that the samples are indeed high-quality and the performance is not simply a result of randomness.\n\nThe code of the full method is released, which allows for easy replication of the results."
            },
            "weaknesses": {
                "value": "There are two main weaknesses of the paper.\n\n1. **Insufficient comparison with existing baselines**\n\nThe proposed sampling strategy is compared with 2 baselines -- random selection and k-means clustering. However, there are many strategies for selecting a subset of high-quality samples, such as the core-set selection strategies [1], active learning strategies [2], or some specific that perform similar selection (e.g., identifying and removing noisy samples) such as dataset cartography [3] or datamodels selection [4]. Even though these strategies were designed for human-labelled datasets, I believe they could be used for LLM generated datasets as well -- but may lead to lower performance if not prioritising the specific of synthetic samples.\n\nComparing with these additional strategies would significantly increase the findings regarding the proposed ACS strategy -- for example by better understanding the benefits of the strategy (e.g., it focuses on dealing with the redundancy which the other strategies may not achieve).\n\n2. **Rather poor presentation/writing**\n\nThe writing in the paper is all over the place and would significantly benefit from a major revision of the writing, as there are many inconsistencies, redundant information and hard to follow parts.\n\nFirst, the experimental setup is discussed in multiple places (Section 3.3; 4.2 and again in 5.1.2), but is not consistent -- one Section mentions $BERT_{large}$ , while other mentions $BERT_{base}$ ; or one mentions that the experiments are repeated 25 times, while other mentions that they are repeated only 5 times.\n\nThe introduction of the ACS sampling strategy would benefit from being in its own Section -- currently it is introduced as part of the baselines.\n\nThe motivation of the paper is focused on LLM generated text samples and the methodology mentions the fine-tuning of BERT, but Section 4 deals with MNIST (image) dataset and introduces different model. In addition, there is no mention how the synthetic samples were generated for the MNIST dataset (or whether it was even done)\n\nThere is a reference to an Algorithm, but no algorithm is provided in the paper (line 341).\n\nThe results in Section 4.2 repeated a few times and could be rewritten to be more concise.\n\nThe legend in all figures is incomplete and makes the figures hard to interpret. For example, in Figure 2 there are multiple coloured lines, but there is no explanation (in the Figure caption or text) what they represent. Similarly, in Figures 3 and 4 there are dashed green and red lines, but no explanation is provided about what they represent -- I can only assume what they represent based on the results description.\n\nThe claims in some places (mainly motivation) are not supported by evidence -- for example line 81. I would suggest adding more references to such claims, mainly when comparing with existing works. In addition, there are parts that would benefit from more explanation -- for example line 287, where a similarity of 0.707 is used because it is a cosine of 45 degrees -- why is this relevant for designing what similarity to use?\n\nIt is not clear whether the authors generate the synthetic samples using GPT-3.5, or just use already pre-generated synthetic dataset from other works -- Section 5 mentions both using data from previous work but also reusing their methodology for generating the samples\n\n\nAlso related to the previous weaknesses, the related work is missing many of the existing works on sample selection (e.g., [1, 2, 3, 4]).\n\n**Additional weaknesses and suggestions**\n\nThe benefit of ACS strategy is evaluated using only 2 (or 3) rather simple datasets -- I would suggest to include more datasets that may be also more complex. For example, using the GLUE or SuperGLUE benchmark datasets or other commonly used datasets in the text domain.\n\n(minor impact) The synthetic samples are generated only from a single closed large language model (GPT3.5). I would suggest to evaluate the benefit of the ACS strategy on samples generated from open-source models (LLaMA, Mistral, Zephyr) -- also focusing on multiple models to show better generalisability of the results.\n\n\n\n**Summary of review and assigned score**\n\nBased on the two major weaknesses I believe the paper needs a major revision and is not ready for acceptance. The revision should focus on improving the overall writing of the paper (to be more concise, keep to the motivation in the introduction which is well written) and better grounding it in relation to existing works (comparing against more baselines)\n\n\n**References:**\n1. DeepCore: A Comprehensive Library for Coreset Selection in Deep Learning\n1. Active Learning is a Strong Baseline for Data Subset Selection\n1. Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics\n1. Datamodels: Predicting Predictions from Training Data"
            },
            "questions": {
                "value": "Did you generate the samples or just reusing the synthetic dataset from other works?\n\nWas there any effort on checking the quality of the generated samples? \n\nIs there any reason why the existing strategies for selecting a representative subset of samples cannot be used for subsampling the synthetic datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}