{
    "id": "PWtx9fJqM5",
    "title": "A Study of Necessity & Sufficiency of Linear Transformations in the Attention Mechanism",
    "abstract": "Scaled Dot Product Attention (SDPA) is the backbone of many modern\n  deep-learning models. It is so versatile that it has been used in\n  natural language, vision, and multi-modal domains with very little\n  change compared to its original formulation. This paper studies the linear transformations used in SDPA. To this end, we introduce three variants of the attention mechanism by removing consecutive linear transformations or adding an extra one. We name these variants Optimized ($W^V$ removed),\n  Efficient ($W^V$ and $W^K$ removed), and Super Attention ($W^V$ and $W^K$ removed and $W^A$ introduced) to simplify comparison when referring to them. In addition to providing the mathematical intuition behind these choices, we evaluate these variants on several datasets of varying size and complexity in vision and text modalities for predictive and generative tasks. Optimized and\n  Efficient variants have one and two matrix multiplications fewer\n  per head, respectively, and 25\\% and 50\\% fewer parameters,\n  respectively, than standard SDPA. However, the performance change compared to difference in parameter count is small. Super Attention introduces a new linear transformation\n  on the values, transforming them from the left. It outperforms\n  standard SPDA in both modalities by up to 10\\%\n  while having one fewer matrix multiplication per head and 25\\% fewer\n  parameters than standard SPDA. Consequently, it is also faster than standard SDPA.",
    "keywords": [
        "Transformers",
        "Attention",
        "Self-Attention"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We introduce three variants of attention mechanism based on removing existing linear transformations or adding extra ones to standard attention and study these algorithms in terms of perfofrmance and speed in varying scales.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=PWtx9fJqM5",
    "pdf_link": "https://openreview.net/pdf?id=PWtx9fJqM5",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces three alternative formulations for the self-attention mechanism with the objective of reducing the number of parameter of the standard attention mechanism while keeping similar level of performance. The three formulations are based on two principles: (i) composing linear transformations do not increase linear expressivity (ii) it should be beneficial to linearly align inputs along the sequence dimensions before applying self attention. The formulations are benchmarked on vision and languages tasks on different datasets, while keeping the scale of the models smaller for computational capability reasons."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper investigates on overlooked aspects of the self attention mechanism in mutliheaded attention architectures. In terms of expressivity the Optimized and Efficient alternatives are well posed alternatives, and the super attention one is an interesting alternative.\n\n- These observations may be impactful, given that transforms are the default choices in many branches of deep learning, and could help explaining some of the reasons behind of post hoc head pruning of attention networks [a] . \n\n    -  [a] Voita, Elena, et al. \"Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned.\" ACL\n\n\n- The paper is well written and clear."
            },
            "weaknesses": {
                "value": "- Experiments have not been performed at a larger scale to compare with standard performance of transformers on the datasets. This limits a lot the evaluation of the proposed alternatives implementations of attentions as the performance and scale of the problem is far from the original one. \n\n- The redundancy properties analyzed are proper of the self attention mechanism in the multi head setting. This should be specified in the introductory and method section, as methods as cross attention for example, should not be affected by the observations made.\n\n\n- While the efficient and optimized attention mechanism are well posed in terms of expressivity, the training dynamics of the network might actually benefit from the redundancy and overparametrization of the standard self attention mechanism, especially with large datasets. Given that this is to be verified, also in the small scale setting (LoRA) overparametrization has shown to be effective for training convergence and generalization, for example in: \n\n    - Yaras, Can, et al. \"Compressible Dynamics in Deep Overparameterized Low-Rank Learning & Adaptation.\", ICLR 2024\n\n\n- It might be useful to check and discuss the following paper which is related to the second principle and the Super Attention mechanism proposed :\n\n     - Cordonnier, Jean-Baptiste, Andreas Loukas, and Martin Jaggi. \"Multi-head attention: Collaborate instead of concatenate.\"\n\n\n*Minor*\n\nI spotted some typos: \n\nLine 323, Captions fo tables 1,2,3: \"Apendix\" -> \"Appendix\""
            },
            "questions": {
                "value": "- How are the generalization performances of the model affected by the reduction of parameters? For example its transferability performances (fine-tuning the model on other tasks/dataset):  with larger scale scale experiment available it might have been interesting to look at the generalization performance of the parameter reduced models on different tasks for example of ViTs on the VTAB benchmark [a]\n\n     - [a] Zhai, Xiaohua, et al. \"A large-scale study of representation learning with the visual task adaptation benchmark. arXiv 2019.\" arXiv \n\n- Why does accuracy on validation set on CIFAR and Imagenet is lower than test accuracy in Table 1? \n\n- Could you report standard deviation across the different seeds of training for CIFAR and MNIST datasets in Table 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents three alterations to the standard attention mechanism that is widely used in transformer-style neural network architectures across much of machine learning.  These variants are different methods to replace linear projections in the attention block with a slicing operation that groups different columns together as an analog of attention heads with improved efficiency.  The paper evaluates these three variants on several vision and language benchmarks, and shows improvements in efficiency with at least no worse performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well written, and the claims are appropriately scaled to the results demonstrated in the evaluation.  \n- The evaluations are thorough given constraints on computational budget, and cover several benchmarks where transformer-inspired architectures have been effective.  \n- The proposed changes to the attention mechanism are easily understood, and motivated with high-level principles that help explain the design process."
            },
            "weaknesses": {
                "value": "- The paper could improve with a better discussion of relevant related work.  Currently the related work section focuses heavily on quantization, which seems only distantly relevant to the proposed approach.  I would have appreciated a more thorough comparison of the proposed methods with relevant literature that seeks to modify the attention mechanism for better efficiency:  eg Performer, Linformer, RKWV. \n- Although the linear part of the attention block is under-studied, I would argue that this is partly because the attention itself (with its quadratic scaling with context length) is the primary bottleneck.\n- The runtime gains reported in smaller settings do not seem to hold up with larger models and longer sequence lengths. (Fig 5)"
            },
            "questions": {
                "value": "- How far can you push the efficiency gains with these different methods of attention?  For example, with the optimized variant, can you further reduce parameters/runtime by increasing the number of slices (or heads?) used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces three efficient variants of Scaled Dot Product Attention (SDPA) that reduce computational costs and parameters by adjusting the linear transformations in them. Tested across vision and NLP tasks, these variants maintain or enhance performance while achieving faster inference."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The approach is simple and well-explained\n2. Evaluation covers a broad range of tasks spanning different modalities. \n3. Complexity analysis is comprehensive"
            },
            "weaknesses": {
                "value": "I am not an NLP expert and am less familiar with NLP tasks, so my focus is primarily on the architecture and vision tasks.\n\n1. Lack of Strong Baselines: The paper primarily compares against standard attention, which is a highly under-optimized baseline. For vision tasks, it would be more informative to include stronger, more optimized baselines such as Swin, MetaFormer, MaxViT, or EfficientNet. Since your method modifies the attention mechanism, it could be integrated into these attention-based or hybrid architectures, which would reveal how your proposed changes perform in comparison to already optimized attention modules.\n\n2. Absence of End-to-End Training on ImageNet-1K: The goal of the proposed models seems to be achieving parameter efficiency without sacrificing expressiveness. However, fine-tuning on ImageNet-1K alone may not fully validate this. Superior performance on smaller datasets (e.g., CIFAR-100, MNIST) and fine-tuned ImageNet-1K can sometimes reflect lower expressiveness, as reduced architectures naturally avoid overfitting on simpler datasets. While removing components from the standard ViT could reduce redundancy, recent hybrid models like MetaFormer and NAT, which use standard ViT only in the later layers, demonstrate that the later layers often require more expressive power. Testing your method on such stronger baselines might show that similar gains are less achievable when the baseline model is already well-optimized and not overly redundant.\n\n3. Limited Analysis Beyond Complexity: While the paper focuses on the complexity benefits of the proposed method, it would be insightful to explore how the learned representations change. For instance, comparing attention maps and MLP activations between standard attention and your optimized attention variants could reveal additional insights into the effects of these architectural modifications."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}