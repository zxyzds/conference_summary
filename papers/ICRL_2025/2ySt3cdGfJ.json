{
    "id": "2ySt3cdGfJ",
    "title": "Distribution Backtracking Builds A Faster Convergence Trajectory for Diffusion Distillation",
    "abstract": "Accelerating the sampling speed of diffusion models remains a significant challenge. Recent score distillation methods distill a heavy teacher model into a student generator to achieve one-step generation, which is optimized by calculating the difference between two score functions on the samples generated by the student model.\nHowever, there is a score mismatch issue in the early stage of the score distillation process, since existing methods mainly focus on using the endpoint of pre-trained diffusion models as teacher models, overlooking the importance of the convergence trajectory between the student generator and the teacher model.\nTo address this issue, we extend the score distillation process by introducing the entire convergence trajectory of the teacher model and propose $\\textbf{Dis}$tribution $\\textbf{Back}$tracking Distillation ($\\textbf{DisBack}$). DisBask is composed of two stages: $\\textit{Degradation Recording}$ and $\\textit{Distribution Backtracking}$. \n$\\textit{Degradation Recording}$ is designed to obtain the convergence trajectory by recording the degradation path from the pre-trained teacher model to the untrained student generator.\nThe degradation path implicitly represents the intermediate distributions between the teacher and the student, and its reverse can be viewed as the convergence trajectory from the student generator to the teacher model.\nThen $\\textit{Distribution Backtracking}$ trains the student generator to backtrack the intermediate distributions along the path to approximate the convergence trajectory of the teacher model.\nExtensive experiments show that DisBack achieves faster and better convergence than the existing distillation method and achieves comparable or better generation performance, with an FID score of 1.38 on the ImageNet 64$\\times$64 dataset.\nDisBack is easy to implement and can be generalized to existing distillation methods to boost performance.",
    "keywords": [
        "Diffusion Model",
        "Diffusion Distillation",
        "One-step Generation"
    ],
    "primary_area": "generative models",
    "TLDR": "This paper proposes an efficient and fast distillation method for diffusion models by introducing the convergence trajectory.",
    "creation_date": "2024-09-15",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=2ySt3cdGfJ",
    "pdf_link": "https://openreview.net/pdf?id=2ySt3cdGfJ",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a novel approach to improve diffusion distillation. The key idea is to include a trajectory of teacher distributions for the student to match. This improves convergence and the final quality of the student model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper proposes a novel technique that is intuitive and is shown to work well. The experiments show a clear improvement in the convergence speed of the DisBack method and the qualitative results show high-quality single-step samples. The authors apply their method to a variety of teacher models over multiple datasets and demonstrate success and failure cases (in the appendix).\n\nThe paper is easy to follow. Technical content is presented clearly."
            },
            "weaknesses": {
                "value": "I felt that the paper writing in places suggested more than was provided. For example, the authors claim that they \"identified this issue arises because existing score distillation methods focus on using the endpoint\". However, the authors provide no such identification in their own work. They provided sufficient evidence that utilizing a trajectory of distributions improves the student but this has not been shown to be necessary. There may be alternative approaches that work well while using only the endpoint. This is present elsewhere in the work, e.g. \"the fat convergence speed is because constraining the convergence trajectory of the generator provides a clear optimization direction\", this is a strong technical claim that has not been adequately explored.\n\nThe authors could do more to explain why DisBack is successful. Some theoretical analysis could help to explain the improved convergence speed, or experiments designed to show the convergence more carefully. For instance, I'd be interested to see how quickly the student model converges to each point on the trajectory, and how closely it matches the distribution. Presumably, this behaviour must be better than linear for DisBack to succeed, which is interesting. Overall, I felt that the idea worked well and was intuitive, but I didn't understand why it worked so well after reading the paper.\n\nThe ablation study is minimal (and I would argue does not qualify as an ablation study). The authors only explore DisBack and the original method side-by-side. Instead, they could also investigate the effect of the number of degradation path checkpoints, different student initializations, different degradation schemes (like using the training trajectory), etc."
            },
            "questions": {
                "value": "The original motivation suggests that the teacher training trajectory could be used, but \"the convergence trajectory of most teacher models is inaccessible\". It's not clear to me that the training trajectory of the teacher would be useful for distillation, as it may not align well with the student distribution either. Did you explore DisBack using the training trajectory for trained diffusion models?\n\nDid you explore distillation into smaller student models? Mismatched architectures could be a useful application of DisBack too.\n\nDo you have any samples from the models along the intermediate teacher trajectory? Do these produce sensible samples at all?\n\nOverall, I like the proposed DisBack algorithm and feel that it is sufficiently novel and performant to justify publication. I would give the paper a higher score if the authors provided some more experimental investigation into why their method is successful.\n\n\nMinor:\n\nFig 2. includes the same sample twice (top middle, and middle)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, a novel approach is proposed to further improve existing diffusion distillation methods. The proposed approach leverages the convergence trajectory from teacher model to the initial state of student model to guide the training of student model backwards, which mitigates the score mismatching problem at the beginning of the distillation. Empirical results have demonstrated the superior performance of the proposed approach."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper has a clear motivation: to address the initial score mismatch problem in diffusion distillation. Specifically, the authors first identify the suboptimal performance of existing diffusion distillation methods as being due to the score mismatch issue at the beginning of student model training, and then propose a novel approach to resolve it. \n2. The proposed approach is intuitive and effective. It makes sense to follow the degradation path from the teacher model to the student model in reverse during distillation, providing a progressive learning signal for the student and mitigating the initial score mismatch problem. In practice, by introducing this backtracking strategy, the distillation process is shown to be significantly faster than its variant without this technique. \n3. The proposed approach is versatile, as it is orthogonal to other diffusion distillation approaches."
            },
            "weaknesses": {
                "value": "1. The state-of-the-art claim for the proposed method is misleading. According to the experimental setup in Appendix B.2, the proposed method trains for 500,000 **epochs** ($\\approx500K \\times \\frac{|D|}{|B|}$ **iterations or steps**, where $|D|$ is the data size and $|B|$ is the batch size). This number is significantly higher than for the baselines. For example, Diff-Instruct only trains for ${\\color{red}50K}$ **iterations** on ImageNet $64\\times 64$, while DisBack (this paper) uses about ${\\color{red}500K\\times 40K=20G}$ **iterations** ($|D|=1,281,167$ and $|B|=32$), which is approximately $40,0000$ times larger. Even if \"epochs\" actually refers to steps (if it is a typo), it still represents 10 times the training length compared with the Diff-Instruct baseline. Additionally, the result (${\\color{green}1.51}$) of DMD2 on ImageNet $64\\times 64$ is achieved by training for ${\\color{red}200K}$ **iterations**. With the extended training setup (${550K}$ **iterations** in total), DMD2 could achieve an FID of ${\\color{green}1.28}$, which is lower than DisBack's ${\\color{green}1.38}$. This raises concerns that the proposed strategy may not fully account for the state-of-the-art performance showcased. This is also supported by their ablation study and Table 3. The variant (\"w/o convergence trajectory\") is essentially Diff-Instruct, as noted in the main text on Lines 391-392. However, even this variant, when trained under the same setting, shows better performance on FFHQ (12.26) versus the original Diff-Instruct (19.93).\n\n2. The speedup shown in Figure 1 is only plotted for epochs 0 to 2000, which covers only the early stage of the distillation. More epochs, either until convergence or until training budgets are exhausted, are needed to better understand how the backtracking strategy behaves throughout training.\n\n3. Although the entire concept revolves around backtracking the degradation path, in actual training, each intermediate checkpoint is only trained for as few as $1000$ steps (for FFHQ, AFHQv2, and ImageNet at $64\\times64$ resolution), while the remaining steps are trained with the original teacher model. This means that the proposed backtracking is used for only a small fraction of the student model's training, which makes it even harder to attribute the superior performance to the proposed strategy."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose a DisBack, which is a new distillation method of diffusion models. On the top of the Diff-Instruct, this DisBack propose a better training algorithm. While Diff-Instruct only use the pre-trained diffusion teacher, DisBack makes a series of degraded teachers, and use that teachers iteratively. This makes the student models easy to learn a teacher distribution."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The idea is simple but effective. It is makes sense that distilled from degraded teacher make the distillation faster.\n- Degradation Recording algorithm looks reasonable. The degraded teacher finally converges at the initialized student distribution, which make the student easy to learn in the early stage.\n- The result compared to Diff-instruct seems the algorithm is effective."
            },
            "weaknesses": {
                "value": "- My major worry is \bthat I can not trust the performance. The paper distilled from EDM in ImageNet 64 which have 2.44 FID, but the distilled student has the performance of 1.38 FID. In my understanding, I think the upper bound performance of the student is teacher.\n\n- I also can not believe user preference study compared to the SDXL teacher. How can it better than teacher? \n\n- The ablation on the number of degraded teacher N is missing. I want to see progressive performance boosting from N=1 (equivalent to the Diff-Instruct) to N= large.\n\n- Is there any scheduling algorithm that changes the teacher in stage 2? It may requires lots of trials to find the schedule that determine when to change that target teacher from degraded to the original.\n\n- Figure 1 is a little bit over-claimed. This algorithm should contains the training costs of stage 1."
            },
            "questions": {
                "value": "My major worry is the reliability of performance and the laborious algorithm. Please respond to my worries."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Distribution Backtracking Distillation (DisBack), a method to accelerate sampling in diffusion models by addressing the \u201cscore mismatch\u201d issue common in traditional score distillation approaches. Unlike existing methods that rely solely on the endpoint of a pre-trained teacher model, DisBack captures the full convergence path between the teacher and student models. It does this through two stages: Degradation Recording, which records a degradation path from the teacher to the untrained student model, and Distribution Backtracking, where the student generator retraces this path to improve alignment with the teacher model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. DisBack addresses the common \u201cscore mismatch\u201d issue in score distillation by incorporating the entire convergence trajectory. DisBack enables the student generator to align more accurately with the teacher model, leading to faster convergence and better optimization paths.\n2. DisBack is designed to be easily integrated into current distillation frameworks, providing a versatile tool to further boost performance in generative model distillation."
            },
            "weaknesses": {
                "value": "**Major**:\n- While the authors claim DisBack is orthogonal to those of other distillation methods, there is no evidence to support this point. It would be valuable if the authors could provide further experiments to show it can be incorporated into other distillation methods, like consistency distillation or adversarial score distillation. \n- The paper aims to mitigate the score mismatch issue by employing degradation recording as convergence trajectory for distillation. The mismatch between the predicted score of generated samples and the model's prediction will be degraded but the mismatch between the model's prediction and the teacher's score prediction will be larger in degradation path.  This suggests a potential tradeoff between these two types of mismatches, which could impact the final model\u2019s performance. Providing further analysis or empirical results on this point would strengthen the motivation and effectiveness of this approach.\n\n**Minor**:\n- In Eq.(6), $\\partial x_t/\\partial \\eta$ should be included in expectation, same as (8). \n- Better to use bold $\\epsilon$ for noise and show the relationship between $\\epsilon$ and $x_t$. \n- In Algorithm 1&2, since the loss includes the expectation w.r.t. $t$ and $\\epsilon$, the line to calculate $x_t = x_0 + \\sigma_t \\epsilon$ is unnecessary and misleading. \n- Labels in Fig.7 are wrong."
            },
            "questions": {
                "value": "- The score estimation (7) is not general for all noising methods. For example, the score estimation of ddpm has a mean scale $\\alpha_t$. When do distillation, should the teacher and student noising methods keep consistent?\n- Compared with Diff-Instruct which only training student models to fit one teacher model, Algorithm 2 needs to fit $N-1$ intermediate checkpoints, what about the training overhead of this part? In Fig.1, did the epochs for DisBack in x-axis record from the degradation recording stage or from which other points?\n- Any experiments to show the influence of the number of degradation checkpoints and the number of degradation epochs? Will more checkpoints and epochs mitigate the mismatch better?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}