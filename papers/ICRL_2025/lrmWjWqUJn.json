{
    "id": "lrmWjWqUJn",
    "title": "Covariate-moderated Empirical Bayes Matrix Factorization",
    "abstract": "Matrix factorization is a fundamental method in statistics and machine learning for inferring and summarizing structure in multivariate data. Modern data sets often come with ``side information'' of various forms (images, text, graphs) that can be leveraged to improve estimation of the underlying structure. However, existing methods that leverage side information are limited in the types of data they can incorporate and rely on specific parametric models. Here, we introduce a novel method for this problem, covariate-moderated empirical Bayes matrix factorization (cEBMF). cEBMF is a modular framework that accepts any type of side information that is processable by a neural network. The cEBMF framework can also accommodate different constraints and assumptions about the factors through the use of different priors and takes an empirical Bayes approach to adapt the priors to the data. We demonstrate the benefits of cEBMF in simulations and in an analysis of spatial transcriptomics data.",
    "keywords": [
        "Matrix factorization",
        "Empirical Bayes",
        "spatial transcriptomics",
        "statistical genetics",
        "non-negative factorization"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "We introduce a novel matrix factorization method that can leverage row or colum information to guide the factorization (e.g. images or graphs) our work is going well beyond the current covariate-guided factorization methods.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=lrmWjWqUJn",
    "pdf_link": "https://openreview.net/pdf?id=lrmWjWqUJn",
    "comments": [
        {
            "summary": {
                "value": "The work deals with a Bayesian matrix factorization (BMF) framework that can accommodate the side information using a parametrized representation, e.g., neural network. Such side information allows the framework to handle any prior of the latent in flexible manner. The work extends the EBMF framework proposed in [Wang and Stephens, 2021; Willwersheid, 2021)] along with the classic normal means model. Experiments with both simulated data and real data are presented to support the claims."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Strengths:\n\n1.\tThe wok is an interesting combination of connecting classical models with more expressive models in deep learning.\n\n2.\tReal data experiments especially the genomic data is insightful and the performance of the proposed approach looks reasonable."
            },
            "weaknesses": {
                "value": "Weakness:\n\n1.\tThe discussions and sections could have been organized better. I think, many important technical details and discussions are moved supplementary, which makes hard to verify the technical soundness and the reasoning.\n\n2.\tSome relevant baselines are missing from discussion and empirical study."
            },
            "questions": {
                "value": "1.\tIn the introduction, a figure or a clear description could have helped what are the side information in the context of the described genomic data.\n\n2.\tIt is commented that \u201cFurther, there are sometimes benefits to not making strong assumptions about the spatial organization of the data even when we know the data are spatial.\u201d. This is a bit unclear statement as spatial PCA seems perform well in the real data experiments in Figure 4. Could you clarify this statement?\n\n3.\tFigure 1 is hard to understand the clustering performance of the different methods. Instead of PCs, it would have been more natural to compare the clustering of the original points in x-y domain. Also, how does different prior assumptions.\n\n4.\tWhat are the connections with deep matrix factorization frameworks? There exists some works by directly modeling the latent using the deep architectures with prior information, e.g.,\na.\tWang, Jianyu, and Xiao-Lei Zhang. \"Deep NMF topic modeling.\" Neurocomputing 515 (2023): 157-173.\nb.\tXue, Hong-Jian, et al. \"Deep matrix factorization models for recommender systems.\" IJCAI. Vol. 17. 2017.\nAnd the related references.\nThese types of frameworks are non-Bayesian, without making any distributional assumptions on the prior. I think, they are very related to the proposed method and could not find any discussion or empirical experiments in the paper.\n\n5.\tIt is unclear how does the constraints are handled in this case, for e.g., nonnegative constraints in Eq. (15) in the algorithm design. \n\n6.\tAlso, if there are missing side information (i.e., matrix entry of Z is present, but side information from X is missing), will the method be able to handle it?\n\n7.\t\u201cIn practice, the full posterior q is not needed; the first and second posterior moments are sufficient\u201d. However, looking at Eq. (12), it seems that the full posterior is needed (notations are also confusing here, $p$ vs $q$). It is unclear how does this translate to the moments of  $\\ell_{ik}$ and $f_{ik}$. Are there for a specific family of distributions?\n\n8.\tIn the real data experiments with collaborative filtering, there exists side information for only columns. In that case, how does the algorithm handle it? In spatial transcriptomics data, no side information is specified. Spatial PCA seems performing well in the real data. What about the runtime performance of the proposed algorithm and how does it compared to the competing baselines?\n\n9.\tI feel the series of equations 31-36 has some issues. I doubt if (31) and (32) are equal, which then questions the correctness of the remaining equations. Also, the distribution $q_{\\beta}$ is not defined properly. As the details are missing in the main paper, the soundness of algorithm is hard to verify. Please add more clarity to the algorithm design in the main text. \n\n10.\tAny insights about identifiability of this approach which is a key consideration in matrix factorization-based models?\n\n11.\tMinor comments:\n\na.\tSome typos: Notation in (7). Notation confusion: side information is notated using $\\bm x_i$ or $\\bm y_i$, but then $\\bm d_i$ is used in (9)\n\nb.\t\u201cD is an invertible diagonal matrix\u201d in Page 4. D can be any invertible matrix"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a covariate-moderated empirical Bayes matrix factorization, which borrows information from the side information. The side information is modeled as a prior information. Comparisons are made to some competitors through simulations and real data analysis."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Paper is well-written with detailed introduction on empirical bayes and clarifications on details. \n\nThe whole paper is easy to read and technical parts are easy to follow."
            },
            "weaknesses": {
                "value": "All the empirical studies are simple. The dimensions of the simulated data are not large. 1000*200 matrix Z is not enough to show the effectiveness. Even the real datasets are relatively small."
            },
            "questions": {
                "value": "The side information can be treated as other views for learning, which is multi-view learning. There are tons of papers about this. It can also be treated as source data in transfer learning. Authors need to discuss why the side information is modeled into the prior as a preferred way. \n\nAuthors mentioned on Line 183 that it can leverage many models. Can authors tell more about the details? Do authors mean that the prior information can be found via modeling? \n\nHow is the rank determined in the paper? It is very difficult to identify it. Also, in the empirical studies, K is treated as known in the simulation. In MovieLens, how all methods choose K based on complexity of data (Line 469)? In DLPFC, I see most people use K=15. If doing clustering, authors need to talk about choosing the number of clusters.....\n\nIn Figure 2, if I am right, the uninformative covariate should have worse results than sparsity-driven covariate, but they seem to have better results. Shifted tiled-clustering should have worse results, but MFAI got better. Can authors clarify these?\n\nI used the DLPFC dataset multiple times and it may not be a good way to present it here. Literature about spatial transcriptomics focuses a lot on incorporating the spatial information, e.g., the empirical Bayes model for spatial transcriptomics. How is the spatial information used in this method? People typically use it for clustering, so authors may show the clustering results, ARI etc. Many methods haven been demonstrate on this dataset as well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a modular framework for empirical Bayes matrix factorization that can leverage a large variety of models, and can use families of priors that are flexible in form to accommodate different assumptions and constraints, and allows automatic selection of the hyperparameters. Various experiments are conducted to illustrate the effectiveness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed framework is modular, flexible, and general, encompassing many previous studies as special cases. The presentation of the core ideas and algorithms is clear, supported by the appendix. The experimental results sufficiently demonstrate the method's effectiveness. The paper provides a well-balanced discussion of the approach's strengths and limitations."
            },
            "weaknesses": {
                "value": "The overall technical contribution appears moderate, as the generalization from MFAI seems straightforward. The key challenges in model formulation and algorithm design could be better articulated."
            },
            "questions": {
                "value": "It would be helpful if the authors could elaborate on the specific obstacles encountered in extending MFAI."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposed a new matrix factorization method with covariates for rows/columns included in the model. It generalizes previous studies on nonnegative matrix factorization and empirical Bayes matrix factorization. The proposed model can utilize any side information that can be included in a probabilistic model with the flexibility of little to no assumption on the factors."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Treating side information through empirical Bayes in the matrix factorization problem is novel. \n2. The paper provides clear derivations and algorithms for the proposed methodology. Part of the result is justified in the appendix."
            },
            "weaknesses": {
                "value": "My major concern is the parametric prior assumption in Eq. (4). It can easily overfit the model given the flexibility in Eq. (4). Although the authors mentioned the overfitting problem in the limitation section, it cannot be overlooked.  Details are provided in the Questions."
            },
            "questions": {
                "value": "1. The parametric assumption in Eq. (4)  is confusing --- it can easily lead to an overfitted model if the index of the distribution family is arbitrarily related to $\\mathbf{x}\\_i$. More constraints should be given to control the model complexity. \nFor example, it could be $\\ell_{ij}\\sim g_k^{(l)}(q_i)\\in \\mathcal G_{l,k}$ with $q_i=q_i(\\textbf{x}_i)\\in\\mathcal Q$, where restriction on the function family $\\mathcal Q$ helps control the complexity. Actually, the authors are doing this in later illustrations. For example, in the spike-and-slab prior example in Sec. 3.2.1, $q_i$ is a logistic function of $\\mathbf{x}_i$. Further discussion on the choices of the parametric families and the constraints on controlling the model complexity should be provided. \n2. Does the proposed model assume a known number of factors $K$? If not, how should $K$ be determined?\n3. If $\\mathbf{Z}$ is a symmetric matrix (e.g. covariance) such that $\\mathbf{L}=\\mathbf{F}$, is there any change to the current process?\n4. For the example in Sec 4.2, the side information $\\mathbf{X}$ is the genre of the movies. If every movie belongs to exactly one genre (correct me if I was wrong), then the empirical prior appears to be a hierarchical prior (19 priors with parameters from a common hyperprior). Then I don't see why the proposed model is needed. \n5. For the example in Sec 4.2, what is the side information for columns, i.e. $\\mathbf{Y}$?\n6. Other minor typo/writing issues:\n    1. In Eq. (3), $\\ell_{ik}$ and $f_{jk}$ are not defined.\n    2. In Eq. (5), $\\mathbf{\\omega}$ is not defined.\n    3. In Eqs. (14) and (16), should Eq. (14) be $\\overline{\\mathbf{R}}^k = \\overline{\\mathbf{R}} + \\overline{\\mathbf{\\ell}}_k\\overline{\\mathbf{f}}_k^T$ and Eq. (16) be the other way?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an extension of empirical Bayes matrix factorization (EBMF), by incorporating \"side information\" through an adaptive prior, resulting in a method called covariate-moderated EBMF, or cEBMF. One type of side information discussed in this paper is spatial information, motivated by the recent popular technology, spatial transcriptomics."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Matrix factorization is important to the community.\n\n2. Spatial transcriptomics is an interesting application.\n\n3. The cEBMF framework is broader than specific models, such as spatial models, or sparse models."
            },
            "weaknesses": {
                "value": "1. It seems that Section 3.2 is incomplete. My understanding is 3.2 discusses the general framework of cEBMF model, followed by some specific examples of side information. Then 3.2.1, as expected, discussed the case where the side information is factor sparsity. As a reader, I was expecting 3.2.2 to be another example, say when side information is spatial, as discussed in both the abstract and the introduction. However, there is no such an subsubsection, and actually 3.2.1 is the only subsubsection of 3.2. Did I miss anything here? \n\n2. Following the above comment, in 4.1, the simulation settings include sparsity-driven covariate, and tiled-clustering model. However, there is no such a discussion on tiled-clustering model in 3.2. This setting seems a spatial example, so I think it makes more sense to have a section 3.2.2 for it. \n\n3. The ST analysis need to be improved:\n\n3.1 There is no quantitative metrics comparing methods, for example, Adjusted Rand Index, among others. \n\n3.2 There are more competitors to be included, like those discussed in the introduction. One obvious example is NSF by Townes and Engelhardt. \n\n3.3 Further results on other slides in Maynard dataset, shown in Figure 10 and 11, don't seem to support the claim that \"cEBNMF tended to produce the largest improvements in accuracy\". I might be wrong since this is purely visualization, which again requires some more quantitative scores to better compare the performance. \n\n\nMinor issues:\n\nLine 112-113, the citation Gopalan et al. (2014) should be in (), say \\citep instead of \\cite. \n\nThe order of methods in Figure 4, 9, 10, 11 are different. \n\nIn ST, \"slide(s)\" are used more frequently than \"slice(s)\" in my understanding. I personally prefer \"slides\", but I don't have any strong opinion on this. However, at least it should be consistent within this manuscript. A simple search would find 10 \"slices\", and two \"slides\"."
            },
            "questions": {
                "value": "These are somewhat repetitive. \n\n1. Can the authors extend 3.2, by adding more settings under which side information can be used (and how).\n\n2. Since one major selling point of cEBMF is the flexibility in incorporating side information, is there any other examples in addition to sparsity and spatial information? If that's all, one potential argument why can't I just use sparse methods and/or spatial methods?\n\n3. Can the authors improve the ST analysis in section 4.2? I mean to include more competitors and quantitative scores. Once the scores are included, it is easier for the audience to compare the methods, over all slides in the Maynard dataset."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel approach, covariate-moderated Empirical Bayes Matrix Factorization (cEBMF), which is aimed at enhancing the prediction of missing values in matrix factorization problems where side information can be leveraged flexibly. Compared to existing models, cEBMF allows prior knowledge to be incorporated into its design as a modular component, similar to a neural network module. The parameter estimation in this framework is based on an Empirical Bayes approach, and an estimation algorithm is derived accordingly. Experimental results on three datasets demonstrate that the proposed method achieves better performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The cEBMF model introduces a novel methodology.\n- The paper is well-organized and written clearly, with a structured overview of related work that positions the proposed method effectively.\n- The code is provided, supporting reproducibility."
            },
            "weaknesses": {
                "value": "- There is a lack of comparison with neural network-based approaches. While several models capable of handling side information have been proposed in the literature [1-3], no comparison with these methods is provided.\n- The primary contribution of this paper lies in its novel approach to incorporating prior knowledge. While this is valuable within the Bayesian community, its broader impact on the ICLR community, which tends to be more focused on neural network approaches, might not be as apparent.\n\n\n\n1. Guo, H., Tang, R., Ye, Y., Li, Z., & He, X. (2017). DeepFM: A factorization-machine based neural network for CTR prediction. *Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI)*, 1725\u20131731. https://doi.org/10.24963/ijcai.2017/239\n\n2. Cheng, H.-T., Koc, L., Harmsen, J., Shaked, T., Chandra, T., Aradhye, H., ... & Anil, R. (2016). Wide & deep learning for recommender systems. *Proceedings of the 1st Workshop on Deep Learning for Recommender Systems*, 7\u201310. https://doi.org/10.1145/2988450.2988454\n\n3. Xiao, J., Ye, H., He, X., Zhang, H., Wu, F., & Chua, T.-S. (2017). Attentional factorization machines: Learning the weight of feature interactions via attention networks. *Proceedings of the 26th ACM International Conference on Information and Knowledge Management (CIKM '17)*, 1019\u20131028. https://doi.org/10.1145/3132847.3132953"
            },
            "questions": {
                "value": "- Can the insights gained from this paper be applied to neural network modeling or other machine learning frameworks rather than the Bayesian framework? If so, could you elaborate on potential methods for achieving this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}