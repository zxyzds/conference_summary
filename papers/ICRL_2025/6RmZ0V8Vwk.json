{
    "id": "6RmZ0V8Vwk",
    "title": "Language Guided Representation Learning",
    "abstract": "Deep neural networks have achieved notable success; however, they still encounter significant challenges compared to humans, particularly in areas such as shortcut learning, texture bias, susceptibility to noise, and catastrophic forgetting, all of which hinder their ability to generalize and adapt. Humans excel in learning high-level abstractions, attributed to various mechanisms in the brain, including reasoning, explanation, and the ability to share concepts verbally\u2014largely facilitated by natural language as a tool for abstraction and systematic generalization. Inspired by this, we investigate how language can be leveraged to guide representation learning. To this end, we explore two approaches to language guidance: Explicit Language Guidance, which introduces direct and verbalizable insights into the model, and Implicit Language Guidance, which provides more intuitive and indirect cues. Our extensive empirical analysis shows that, despite being trained exclusively on text, these methods provide supervision to vision encoders, resulting in improvements in generalization, robustness, and task adaptability in continual learning. These findings underscore the potential of language-guided learning to develop AI systems that can benefit from abstract, high-level concepts, similar to human cognitive abilities.",
    "keywords": [
        "representation learning",
        "generalization",
        "natural language",
        "shortcut learning",
        "continual learning",
        "language guidance"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "Exploring natural language guidance in vision models to improve representations, generalization, robustness and continual learning",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=6RmZ0V8Vwk",
    "pdf_link": "https://openreview.net/pdf?id=6RmZ0V8Vwk",
    "comments": [
        {
            "summary": {
                "value": "The paper focuses on the impact of using language model's representation on vision tasks. The paper uses two ways to use language model to \"guide\" the vision model, one is by explicitly align the vision model embedding and language model embedding, another is by implicitly use and freeze (part of) pretrained language model parameters as part of the model pipeline to make prediction on images. The authors demonstrated the usefulness of the two methods by showing that with language guidance, the model is more robust to out of distribution examples, texture bias, and adversarial attack, and it can do better on continual learning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper covers experiments in extensive aspects to illustrate the benefits of language model guidance. The setting of the experiment is comprehensive, and I believe it can be reproduced."
            },
            "weaknesses": {
                "value": "- The idea behind this paper is not very novel. Starting from CLIP, it is well known that the alignment between language and vision can bring benefits (for example, [1] can do zero shot generalization to image classifications with new labels). \n- The setting of the paper seems outdated. Nowadays, VLMs like LLaVA has been widely used, but the paper still focuses on ResNet and CIFAR-10. A good question here is what the implication of the results is, as the state-of-the-art models has already been using vision language alignment to achieve much more.\n- The presentation of the paper can be improved. For example, eq (2) and (3). Seem like $f_v$ and $f_l$ are defined across a set of datapoints, $S_v(i, j)$ is the cosine similarity between image embeddings of two data points, indexed as $i$ and $j$. The current presentation is very misleading.\n- Similarly, for Sec 4.2, the authors failed to clarify what is the input to classification head, and what is exactly the input to the language block. My concern here is that the paper uses ResNet-18, which is not natural to convert to input of language model block.\n- Figure 3 is hard to read and interpret, thus the implication is unclear to me.\n\n[1] Hanjie, Austin W., Ameet Deshpande, and Karthik Narasimhan. \"Semantic supervision: Enabling generalization over output spaces.\" arXiv preprint arXiv:2202.13100 (2022)."
            },
            "questions": {
                "value": "- In Figure 1, I don't understand why CKA can be applied here. What is $X$ and $Y$ in eq (4)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies language guided representation learning and its potential techniques for incorporating language guidance into vision representation learning. The paper considers two techniques for incorporating language guidance, one based on explicit guidance (ExLG) and the other based on implicit guidance (ImLG). The paper investigates the effect of language guidance on sample efficiency, OOD generalization, spurious feature learning, shortcut learning, and robustness. Generally, the paper finds ExLG improves on all aspects over traditional approaches for performing vision representation learning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "A thorough improvement from language guidance: I found the paper to do a reasonably good exploration of language guidance with strong results.\n\nConsistent and large amount of experimentation: The paper has many experiments comparing its two proposed methods. \n\nInteresting analysis: Figures 3, 4, and 5 show some interesting analyses from language guidance, showing feature maps on the Skewed-CelebA dataset, the effects of stylization, etc. These were pretty useful for understanding language guidance more deeply."
            },
            "weaknesses": {
                "value": "My main concern in the paper is related to novelty and clarity on its positioning. Given the large number of related papers in the field, I\u2019m finding it a bit difficult to describe the guiding question the paper aims to answer. This leads to the weaknesses I describe below. \n\n\u2022\tMotivation of the methods: Overall, I found the approach for ExLG and ImLG a bit difficult to motivate fully since I don\u2019t see how they map language guidance approaches for vision representation learning papers from the past. I don\u2019t see how these findings are interesting or relevant to the way people design language guidance for visual representation learning if ExLG or ImLG aren\u2019t well motivated methods themselves. In particular, with ImLG, could the authors give some methods that use something similar?\n\n\u2022\tDistinction with related papers: First, I strongly recommend that the authors write a related work section. This is necessary for positioning the paper in relation to other work that incorporates language guidance for visual representation learning. Overall, I found myself puzzled over the novelty of this paper. The paper finds many benefits from language guidance that has been found in prior papers that use language guidance [1, 2, 3]. The paper tries to differentiate itself from CLIP, which uses a joint language encoder by arguing that the approach uses a frozen language encoder. However, there are plenty of other papers that use a frozen text encoder [1, 2, 3] for language guidance. These papers also report similar findings of improvements over robustness, generalization, etc., although not all features are covered in the search I did. \n\n\u2022\tFocus on vision domain: For a paper that has the title on language guided representation learning, I would have expected a focus on more domains than just vision. Would this extend to the other modalities? I would prefer if the title just stated that this was focused on vision instead. \n\nOverall, in my opinion, the positioning and motivation of this paper need significant work. I would like to see a related work section added to the paper. If the authors can clarify their positioning in a satisfying manner, I may raise my score.\n\n[1] El Banani et. al. Learning Visual Representations via Language Guided Sampling. CVPR 2023.\n[2] Sariyildiz et. al. Learning Visual Representations with Caption Annotations. ECCV 2020.\n[3] Stroud et. al. Learning Video Representations from Textual Web Supervision. arXiv 2021."
            },
            "questions": {
                "value": "\u2022\tCan the authors more clearly explain what ImLG is doing? I found myself confused about the approach. \n\u2022\tI found it interesting to see cases where ImLG was worse than the baseline. For example, T4 in Figure 6. Would the authors mind providing some discussion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors investigate using natural language to enhance visual representations, and how this enhancement affects systematic generalization and catastrophic forgetting in neural networks. More specifically, inspired by human cognition, the authors propose that language, as a tool for abstraction and concept-sharing, can help guide DNNs to better, more abstract representation learning. The authors explore two main approaches: Explicit Language Guidance (ExLG), which aligns visual representations with high-level language descriptions, and Implicit Language Guidance (ImLG), where a pre-trained language model \u201cindirectly\u201d enhances the vision model. Both methods are tested extensively across diverse tasks such as generalization to new data (IID and OOD), among others. Perhaps unsurprisingly, the results show improvements over baseline models. ExLG performed better on generalization tasks, while ImLG showed advantages in robustness and shortcut learning. As seminal work in the past (e.g., CLIP), the study highlights language guidance as a powerful tool for creating models that generalize and retain knowledge more effectively."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper is well written and clearly explained\n2. Figures are clear and informative. \n3. The topic is timely and of extensive interest and applicability."
            },
            "weaknesses": {
                "value": "The main, and crucial weakness of this work is its novelty and scope. Although, as the authors point out, their method slightly differs from other VLMs whose representation \u201cfuse\u201d vision and language embeddings, both the added theoretical and empirical value of this paper is poor:\n\n1. Other papers already make the point that language can generate richer representations that have an impact on the issues highlighted by the authors.\n\n2. I would compare the proposed methods with other VLM models, in order to show concrete empirical value of this paper."
            },
            "questions": {
                "value": "*If you mention the Global Workspace Theory, I think it\u2019s only fair to cite at the very least Dehaene (1998) and I would also include Baars (1994).\n*Line 123, CKA is first presented without spelling the acronym.\n*I would define what a \u201cconventional classification model\u201d is.\n\nTypo:\nLine 036: \u201c\u2026is one of the aspects of human cognition is still a challenge for neural networks...\u201d -> that is still\nLine 049: \u201c...context of continual learning (?)...\u201d  issue with citation.\nLine 099: \u201c...System 2 (Explicit) processing (Daniel, 2017).\u201d Citation should be Kahneman."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studied the effect of additional language information in image tasks training. The research found out that language guidance in the form of explicit representation alignment and implicit access to the language model improved ResNet's\n- OOD generalization\n- shortcut learning (reduction of spurious correlations)\n- bias on textures\n- robustness against adversarial attacks\n- continual learning (reduction of catastrophic forgetting)"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Thoroughly studied the role of representation learning in language models.\n2. Identified that the guidance of language reduced many unwanted behaviors in image models training, such as catastrophic forgetting and the vulnerability against adversarial attacks."
            },
            "weaknesses": {
                "value": "1. The evaluation needs to be fair. For all the experiments, we should keep the total (frozen) parameter count the same, and (ideally) the tunable parameter count the same. Otherwise we may attribute the better performance of ExLG/ImLG comparing to the baseline models to the increase of the number of parameters, not language-guidance.\n2. You should add more baselines/ablations. For example, in the \"continual learning\" experiment, it is unclear whether the reduction of catastrophic forgetting from ER to ExLG/ImLG is due to the language guidance or access to a large bank of information (be it language/image/...).\n3. You should conduct a more thorough related work analysis. I haven't conducted a thorough literature review on the topic, but this paper should have a \"related work\" section that distinguishes it from other related concepts or frameworks, such as CLIP.\n4. There is a lack of explanation or insight into how the language guidance improved the image's performance on these tasks. Consider how the alignment loss improved the representation alignment by offering some interpretability analysis, such as probing the learned resnet's inner representations, https://arxiv.org/pdf/2410.06940 applies on diffusion models, but did several layer-by-layer analyses, which I think is valuable at improving the insights of your work."
            },
            "questions": {
                "value": "1. Could you clarify again what is the core difference between your approach with other language/vision representation alignment approaches, such as CLIP? Especially for the explicit guidance.\n2. In section 5.3 and figure 4, what is the \"stylization alpha\"? It seems that this is not explained.\n3. In figure 7, how did you calculate the \"plasticity\" and \"stability\"? How are these precisely defined mathematically?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper asks whether non-visual language models provide advantages when used to create an image representation, over a traditional end-to-end image classifier alone. It compares a traditional classifier with a proposed \u201cExLG\u201d vision encoder and an \u201cImLG\u201d encoder, where ExLG adds a tung-mori student-teacher setup to align the image representation with a language representation, and ImLG incorporates a frozen pretrained language model as the final stage of the vision encoder.  With this setup, they train on several standard classification problems and compare performance on low-data regimes, OOD generalization, strongly biased data, and adversarial robustness, and continual learning. They conclude that the incorporation of pretrained language models is helpful in all these settings."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The community has a lot of interest in the language models\u2019 capabilities to represent the visual world without ever having been exposed to an image during training. The paper poses natural questions, looking beyond basic classification performance to ask whether incorporating a language model improves the inductive biases for a model. The benchmark datasets used to investigate OOD and bias behavior are reasonable, and the robustness test is appropriate."
            },
            "weaknesses": {
                "value": "The paper as currently presented doesn\u2019t have enough evidence to support its broad claims. The claim is that several types of robustness improve when language modeling is incorporated (implying that there are benefits from having the \u201cknowledge\u201d derived from lots of text training), but there are several possible confounders that aren\u2019t investigated.\n\nFor example, the addition of extra loss terms (for ExLG) or extra layers (for ImLG) could have a regularizing effect regardless of the specific content of those extras, which could mean that there is nothing special about language knowledge.  The paper would be strengthened if it presented clearer evidence that the essential benefits come from the fact that the extra model involved is a language model trained on lots of text, as compared to e.g., a random neural network. Ablations are needed on: what type of text is used in ExG; how powerful the language model is; whether it matters if the language model is trained on natural text or some non-object related task, or left uninitialized. Ideally the hyperparameters can be held fixed while comparing the use of a language model to a baseline with the same computational form that doesn\u2019t have the benefit of large-scale text training.\n\nThe paper\u2019s investigations are related to the idea articulated in the recent paper \u201cThe Platonic representation hypothesis\u201d by Huh, and it would be nice to cite+connect it."
            },
            "questions": {
                "value": "In the ExG case, it is unclear what text is used for aligning the representation. Is it image-specific text, or class-generic text, or something else; how was it chosen, and how important is this choice? What would be the effect of some text-per-class that has nothing to do with the class?\n\nAs the language model becomes more powerful, does it improve OOD, resistance to bias, robustness, and CI behavior? Only one small model size comparison is done, and only on basic classification accuracy.\n\nThe claim is that incorporating a language model helps, but does it need to be a language model pretrained on real text?  Would performance benefits be obtained from a randomly-initialized language model? What about an early checkpoint of a language model with poor performance, or a model trained on non-visual-world-descriptive text such as a code LM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}