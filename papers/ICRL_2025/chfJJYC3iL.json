{
    "id": "chfJJYC3iL",
    "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code",
    "abstract": "Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEvla, MBPP) are no longer sufficient for assessing their capabilities suffering from data contamination, overfitting, saturation, and focus on merely code generation. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which collects new problems over time from contests across three competition platforms, Leetcode, Atcoder, and Codeforces. Notably, our benchmark also focuses on a broader range of code-related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts over six hundred coding problems that were published between May 2023 and Aug 2024. We evaluate over 50 LLMs on LiveCodeBench (LCB for brevity) presenting the largest evaluation study of code LLMs on competition problems. Based on the study, we present novel empirical findings on contamination, overfitting, and holistic evaluations. We demonstrate that time-segmented evaluations serve as a robust approach to evade contamination; they are successful at detecting contamination across a wide range of open and closed models including GPT-4O, Claude, Deepseek, and Codestral. Next, we highlight overfitting and saturation of traditional coding benchmarks like HumanEvla and demonstrate LCB allows more reliable evaluations. Finally, our holistic evaluation scenarios allow for measuring the different capabilities of programming agents in isolation.",
    "keywords": [
        "Code LLMs; Evaluation; Contaminationl; Overfitting"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We build evaluation of code LLMs on new problems highlighting challenges like contamination and overfitting",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=chfJJYC3iL",
    "pdf_link": "https://openreview.net/pdf?id=chfJJYC3iL",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces LiveCodeBench, a lively-updated dynamic benchmark that is designed to evaluate large language models (LLMs) for coding tasks with minimized data contamination risk.\n\nAside from being contamination-free, LiveCodeBench also seeks to improve the data quality, difficulty, and task diversity of the benchmark.\n\nThe paper also conducts thorough experiments exposing the significant data contamination and overfitting issues."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "## Significance\n\nThe paper addresses one of the most important issues: data contamination in the existing Code LLM evaluation process. The benchmark is highly practical and should be known and tried by the Code LLM community to encourage more rigorous evaluation of models. \n\n\n## Soundness\n\nThe paper demonstrates strong soundness through its detailed analyses of the models' performance over time, overfitting issues on HumanEval, and comparison of different types of models.\n\n\n## Effectiveness\n\nDynamic benchmarking is indeed an effective solution to mitigate data contamination."
            },
            "weaknesses": {
                "value": "## Novelty\n\nThe paper presents two key contributions:\n\n1. A comprehensive dynamic benchmark for coding takes\n\n2. Analyses exposing the contamination issues in coding tasks\n\nWhile these contributions are highly relevant and practical for the Code LLM community, it's important to note that dynamic benchmarking is a technique that has been utilized in various prior works [1,2,3,4] as early as 2021 [1]. Additionally, the issue of data contamination has been systematically examined in earlier studies [5,6], limiting the novel insights that the paper can offer to the broader ICRL community.\n\nTo enhance the paper's novelty, it may be beneficial to develop better and more efficient dynamic benchmarking techniques, or introduce new methods to alleviate more intricate contamination issues such as re-phrasing [6].\n\n\n\n\n[1] [Dynaboard: An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking](https://arxiv.org/abs/2106.06052)\n\n[2] [Competition-Level Problems are Effective LLM Evaluators](https://arxiv.org/abs/2312.02143)\n\n[3] [LatestEval: Addressing Data Contamination in Language Model Evaluation through Dynamic and Time-Sensitive Test Construction](https://arxiv.org/abs/2312.12343)\n\n[4] [NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes](https://arxiv.org/abs/2312.14890)\n\n[5] [To the Cutoff... and Beyond? A Longitudinal Perspective on LLM Data Contamination](https://openreview.net/forum?id=m2NVG4Htxs)\n\n[6] [Rethinking Benchmark and Contamination for Language Models with Rephrased Samples](https://arxiv.org/abs/2311.04850)"
            },
            "questions": {
                "value": "## Discussion Questions\n\nHave you considered addressing more intricate data contamination issues like re-phrasing within the scope of this work?\n\nI believe that many code-specific techniques can be developed to detect re-phrasing contamination such as checking the equivalence or similarity of the test cases and canonical solutions.\n\nI would be happy to raise my score if the authors could incorporate and address those intricate contamination issues in the revision."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Prior LLM coding benchmarks suffer from contamination and saturation issues. This paper introduces LiveCodeBench, a new benchmark that collects new problems from public programming contests (LeetCode, AtCoder, and Codeforces). Besides code completion, the benchmark also tests the self-repair and output prediction abilities of LLMs. The paper provides a comprehensive analysis on over 600 problems and over 50 LLMs, demonstrating that time-segmented evaluations are useful for detecting contamination."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper proposes a coding benchmark with live updates, ensuring fair model evaluation by testing only on new problems after each model\u2019s cutoff date. The time-segmented analysis also reveals a significant drop in performance of some models after release date, indicating notable contamination issues.\n- The paper is well-written and organized, with a thorough appendix including discussion on legal compliance and benchmark creation details."
            },
            "weaknesses": {
                "value": "- The novelty is limited. Using competitive programming problems for LLM evaluation has been well adopted. Additionally, the evaluation scenarios in this paper are also not new but borrowed or adapted from prior work (e.g., the test case output prediction can be seen as a chain-of-thought prompting solution of the code execution task). The main contribution of this paper is to scrape the problems with appropriate difficulty and analyze the results in a time-segmented way.\n- The relatively small number of problems (~40 LeetCode problems every two months) raises concern about the reliability of the results. I wonder if it\u2019s possible to estimate the variance of the pass@1 performance and the statistical significance of the comparisons."
            },
            "questions": {
                "value": "It seems that the drop in performance occurs only for the LeetCode problems and is smooth for other platforms like AtCoder. Do you have any explanation for this interesting phenomenon?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper worked on the evaluation of LLMs for code, proposing a benchmark named LiveCodeBench. The benchmark will be continuously updated by automatically collecting problems from online code contest sites to overcome data contamination. It contains four code-related tasks: code generation, self-repair, code execution, and test output prediction. A large-scale evaluation was conducted on 50 LLMs, which revealed the widespread contamination among LLMs, as well as overfitting and saturation of traditional coding benchmarks like HumanEval."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. **Benefiting the community**: The benchmark will be continuously updated and provide code ability evaluation with less contamination.\n2. **Large scale evaluation**: The authors evaluated 50 LLMs with carefulness about contamination, providing a valuable reference for the code-related capabilities of each model.\n3. **Having insights**: The experiments revealed the widespread contamination among LLMs, as well as overfitting and saturation of traditional coding benchmarks like HumanEval."
            },
            "weaknesses": {
                "value": "1. **Unproven data representative**: Besides contamination, we also need to know how many code problems we need and how representative they are, to conduct a comprehensive evaluation. However, they were not answered in this paper. Are there mechanisms to guarantee or prove these properties, in particular for new models?\n\n2. **Unevaluated workflow reliability**: While the workflow of benchmark construction is completely automated, this workflow's reliability was not evaluated. For example, the accuracy of the HTML extractor.\n\n3. **Unproven test case completeness**: Averaged 18 of the test case count basically is far fewer than test cases used inside the programming task websites; the completeness or sufficiency of these test cases was not analyzed in this paper.\n\n4. **Biased filtering**: In the code competition scenario that this work focused on, questions with multiple correct outputs for a single input would contain many unique features or have different problem-type distributions. However, they were all removed in this work, which might lead to a bias. Further analysis of the impact of this filtering should be conducted.\n\n5. **Multi-step track needed**: This work's designs of four tracks were motivated by AlphaCodium. Including AlphaCodium, state-of-the-art competition-level code generation works nowadays broadly applied multi-step workflow. However, besides one-step revision, multi-step code generation was not evaluated in this work."
            },
            "questions": {
                "value": "The contribution of this work to the community depends largely on your continued maintenance efforts. Do you have a long-term plan?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces LiveCodeBench, a new benchmark designed to evaluate LLMs for code-related tasks. \nLiveCodeBench addresses some key limitations of previous benchmarks, \nincluding issues like data contamination, overfitting, saturation, and limited application range. \nUnlike existing benchmarks, LiveCodeBench assesses not only code generation but also self-repair, code execution, and test output prediction. \nAlongside proposing the benchmark, \nthe authors perform a comprehensive evaluation and analysis of the contamination and overfitting problem in popular LLMs. \nThe paper also introduces a difficulty-guided problem curation strategy, \nwhich enhances the evaluation's effectiveness by creating clearer margins between model performances, \nallowing for a more meaningful comparison.\nThe evaluation covers 52 models, with sizes ranging from 1.3B to 70B, \nproviding a fairer and more reliable understanding of model performance in code-related tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* LiveCodeBench has a live update mechanism that mitigates data contamination and enables continuous growth. \n  This mechanism ensures that the benchmark remains useful for evaluating newer models, \n  even those with a knowledge cut-off date beyond its release.\n* With the timestamps on the tasks, the authors conduct a novel \"live\" evaluation of code generation to directly address data contamination issues. \n  This approach offers deeper insights into the models' actual coding capabilities, focusing on genuine problem-solving rather than mere memorization.\n* The difficulty guided problem curation is particularly effective in revealing performance differences between models of similar sizes, making it practical to use for meaningful evaluation.\n* The paper employs a clustering method to identify models that overfit HumanEval. \n  While overfitting to older benchmarks is a well-known issue, \n  the authors' method offers empirical evidence and analysis that specifically highlights the models affected by this problem."
            },
            "weaknesses": {
                "value": "* Novelty: The benchmark's construction primarily aims to create a newer and harder version of existing benchmarks. \n  However, a key issue with LLM benchmarks for code is that Olympiad in Informatics (OI) programs are not typically representative of real-world software engineering and programming languages. \n  OI programs often have a distinct style and differ in their learned representations compared to code from real software projects. \n  This has led to a shift in focus in this research area from OI competitive programming to open-source software projects, as seen in recent work like SWE-bench [1] and RepoBench [2].\n\n[1]: Jimenez, Carlos E., et al. \"Swe-bench: Can language models resolve real-world github issues?.\" ICLR 2024.\n\n[2]: Liu, Tianyang, et al. \"Repobench: Benchmarking repository-level code auto-completion systems.\" ICLR 2024."
            },
            "questions": {
                "value": "1. The tasks of *code execution* and *test case output prediction* appear to be quite similar. \n   According to the paper, the distinction lies in their prompt: \n   code execution is to predict the output based on inputs and functions in *programming language*, \n   while test case output prediction is to predict the output based on inputs and function descriptions in *natural language*. Given the similarity between these two tasks, is there any observable correlation in their results?\n2. Test case generation typically is designed as predicting the entire test case from a given function implementation [3,4], \n   which is more practical in real-world software engineering and testing applications, \n   such as in tools like Copilot. \n   I'm curious why the authors choose a different approach to evaluating test generation in the form of $(I, f_{\\text{NL}}) \\mapsto O$, \n   where the input and natural language function descriptions are used to predict the output.\n3. In the **Problem Difficulty** paragraph of Section 3.1, \n   the authors state that they collected problems of varying difficulty levels as labeled by competition platforms. \n   However, how did the authors address potential rating bias across these platforms? \n   Given that different competition platforms are designed for distinct user groups and, as the authors note, CodeForces problems are generally harder than those on other platforms, \n   there is likely a selection bias in difficulty ratings across different platforms.\n\n[3]: Nie, Pengyu, et al. \"Learning deep semantics for test completion.\" ICSE 2023.\n\n[4]: Rao, Nikitha, et al. \"CAT-LM training language models on aligned code and tests.\" ASE 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}