{
    "id": "WHtNc5kX1v",
    "title": "Latent Boost: Leveraging Latent Space Distance Metrics to Augment Classification Performance",
    "abstract": "The pursuit of boosting classification performance in Machine Learning has primarily focused on refining model architectures and hyperparameters through probabilistic loss optimization. However, such an approach often neglects the profound, untapped potential embedded in internal structural information, which can significantly elevate the training process. In this work, we introduce Latent Boost, a novel approach that incorporates the very definition of classification via latent representation distance metrics to enhance the conventional dataset-oriented classification training. Thus during training, the model is not only optimized for classification metrics of the discrete data points but also adheres to the rule that the collective representation zones of each class should be sharply clustered. By leveraging the rich structural insights of high-dimensional latent representations, Latent Boost not only improves classification metrics like F1-Scores but also brings additional benefits of improved interpretability with higher silhouette scores and steady-fast convergence with fewer training epochs. Latent Boost brings these performance and latent structural benefits with minimum additional cost and no data-specific requirements.",
    "keywords": [
        "Latent Space",
        "Distance Metrics",
        "Magnet Loss"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "Latent Boost enhances classification by optimizing latent representation distances, leading to better clustering, improved classification performance, and faster convergence.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=WHtNc5kX1v",
    "pdf_link": "https://openreview.net/pdf?id=WHtNc5kX1v",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces Latent Boost which levearges latent representation distance metrics to improve the classification training."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is clearly structured and easy to read.\n\n2. Figure 1 well presents the method pipeline."
            },
            "weaknesses": {
                "value": "1. No explanantion on the choice of model architecture for three experiments. Sometimes, VGG is used while sometimes ResNet-50 is deployed.\n\n2. All datasets considered are small scale. It's suggested to use more complex datasets.\n\n3. The novelty is somehow limited, by modifying existing magnet loss with PCA. However, the motivation of using PCA is not clearly stated nor ablated using experimental results to show the necessity. \n\n4. The importance of inter- and intra- variance is controled by $\\alpha$ and $\\beta$. However, they follow two distinct update rule (linearly and exponentially), which is not clearly explained.\n\n5. How is $\\lambda$ selected? Table 1 shows the $\\lambda$ ranging from [0.5, 0.9] are potentially good, but there is no consistently bset performant $\\lambda$. Results on selecting $\\lambda$ on the validation set are not presented.\n\n6. All results are conducted in ID test sets. It would be beneficial to show performance gain on OOD test sets."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes the Latent Boost method, which aims to enhance classification performance by incorporating distance metric learning in the latent space. The method uses Principal Component Analysis (PCA) for dimensionality reduction before calculating the loss, attempting to reduce noise in early training stages. Additionally, Latent Boost introduces dynamic variance calculation and adaptive hyperparameters for intra-cluster compactness and inter-cluster separation, respectively. Experimental results show some performance improvements over existing methods, highlighting the potential of structured latent representations for classification tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1 The method combines PCA with Magnet loss and introduces dynamic variance and adaptive hyperparameters.\n\n2 The paper introduces the concept of dynamically adjusting intra-cluster and inter-cluster factors during training, which is relatively uncommon in metric learning literature."
            },
            "weaknesses": {
                "value": "1 The core idea\u2014combining PCA with Magnet loss and adding dynamic parameters\u2014lacks substantial novelty. PCA is a basic technique, and the dynamic adjustments are incremental improvements rather than groundbreaking changes. This limits the theoretical contribution of the paper.\n\n2 The dynamic adjustments for intra-cluster and inter-cluster separation might result in overfitting to specific data characteristics. This could negatively impact the model's generalization ability, especially on datasets that significantly differ from those used in the experiments. The paper does not discuss this risk or explore methods to mitigate it.\n\n3 The limited performance gains further suggest that the method lacks the innovation needed to advance the field meaningfully and may merely represent incremental changes to established methods."
            },
            "questions": {
                "value": "1 How sensitive is the method\u2019s performance to the initial values of \u03b1 and \u03b2? Is extensive tuning required for different datasets, and if so, would this limit the method\u2019s usability?\n\n2 Given the dynamic intra-cluster and inter-cluster adjustments, has the potential for overfitting been explored? Would there be a way to regularize these adjustments to avoid excessive tailoring to specific data characteristics?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Latent Boost, a novel approach that enhances classification performance by leveraging latent space distance metrics. It optimizes models not only for classification metrics but also for sharp class cluster formation within the latent representation, leading to improved F1-Scores and interpretability with minimal additional cost."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1). This paper proposes a distance-based loss Latent Boost which is inspired by the Magnet loss, addressing previously overlooked nuisances with dynamic adaptation and discriminative information density.\n2). Latent Boost demonstrates a consistent improvement in classification metrics such as F1-Scores across different datasets and model architectures. \n3). Latent Boost is able to achieve these performance benefits with minimal additional computing costs and without requiring any data-specific tweaks."
            },
            "weaknesses": {
                "value": "1).In this paper, three datasets, namely Fashion MNIST, CIFAR-10 and CIFAR-100, are mainly tested. For other more complex datasets, the generalization ability and effect of Latent Boost are not verified in this paper.\n2).While Latent Boost is designed to reduce training cycles and increase computational efficiency, the additional computational burden it introduces, such as the amount of computation that can be added when calculating potential spatial distance measures, is not discussed in detail.\n3).This paper mainly compares with several traditional distance measurement learning methods, but does not compare with the latest classification methods, which limits the comprehensive evaluation of Latent Boost performance."
            },
            "questions": {
                "value": "See the above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies boosting classification performance via leveraging latent space distance metrics. Traditional data-driven classification training focuses on optimizing classification scores in a black-box manner. The authors argue that the internal structural information can significantly elevate training process, but is neglected in conventional probalistic approaches. They introduce Latent Boost, an approach that enhances the cluster separation in the latent space. Results show that Latent Boost not only improves classification metrics but also brings additional benefits of improved interpretatbility with higher silhouette scores and steady-fast covergence."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper proposes to enhance cluster separation through optimizing latent space distance metrics during classification training. The idea generally sounds reasonable. \n- Latent Boost is simple. Results show that it can enhance performance and interpretability while shrinking computation demand.\n- The paper compares Latent Boost with a few other clustering losses. The former shows better performances on three datasets. \n- Analyses with different $\\lambda$, TSNE plots and Solhouette score are relatively well-presented."
            },
            "weaknesses": {
                "value": "- Although the authors argue that 'Latent Boost is the first method to integrate distance metrics into the classification loss function', I do feel that it is unexpected that such clustering losses help as they have been widely validated in semi-supervised learning area.\n\n- Latent Boost is based on Magnet loss. A few modifications are designed, such as PCA dimensionality reduction, individual variance, dynamic update rules. However, the authors fail to validate the effectiveness of each component. \n\n- The three datasets experimented in the paper are small. It is unclear whether the proposed loss is effective when there are many training data, such as ImageNet, especially considering the full-supervised learning nature.\n\n- From the paper, the latent representations are extracted from only one layer. While it is reasonable to use the layer before classification head, knowing how to apply distance metrics on early layers seem to be more interesting."
            },
            "questions": {
                "value": "- In Eq.(9), will $\\beta$ become negative when $E>0.2E_{total}$? Figure 3 in Appendix C does not clearly show the situation when Epoch is larger than 100. \n\n- In Eq.(10), should $\\epsilon$ be placed in the logarithm or denominator?\n\n- In Ln 377, should it be 'As shown in Section 4.2'?\n\n- For Ln 392-399, could the authors plot out curves of number of principle components throughout training? The authors are suggested to validate the necessity of PCA, e.g., any evidences that (Ln 197) 'it might introduce noise' and the influences of different threshold $T$.\n\n-  In Ln 522, could the authors clarify 'the model is not directly trained on the latent representation data'? I do feel that the loss is directly applied on the latent feature representations to optimize the clustering structure, which is aligned with Silhouette Score.\n\n- What are the number of clusters and how to calcuate cluster means/cluster assignments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}