{
    "id": "trV41CpAK4",
    "title": "Accelerating Neural ODEs: A Variational Formulation-based Approach",
    "abstract": "Neural Ordinary Differential Equations (Neural ODEs or NODEs) excel at modeling continuous dynamical systems from observational data, especially when the data is irregularly sampled. However, existing training methods predominantly rely on numerical ODE solvers, which are time-consuming and prone to accumulating numerical errors over time due to autoregression. In this work, we propose the VF-NODE, a novel approach based on the variational formulation (VF) to accelerate the training of NODEs. Unlike existing training methods, the proposed VF-NODEs implement a series of global integrals, thus evaluating Deep Neural Network (DNN)--based vector fields only at specific observed data points. This strategy drastically reduces the number of function evaluations (NFEs). Moreover, our method eliminates the use of autoregression, thereby reducing error accumulations for modeling dynamical systems. Nevertheless, the VF loss introduces oscillatory terms into the integrals when using the Fourier basis. We incorporate Filon's method to address this issue. To further enhance the performance for noisy and incomplete data, we employ the natural cubic spline regression to estimate a closed-form approximation. We provide a fundamental analysis of how our approach minimizes computational costs. Extensive experiments demonstrate that our approach accelerates NODE training by 10 to 1000 times compared to existing NODE-based methods, while achieving higher or comparable accuracy in dynamical systems. The source code will be publicly available upon publication.",
    "keywords": [
        "Neural ordinary differential equations",
        "irregularly-sampled dynamical systems",
        "variational formulation",
        "acceleration"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "We propose a novel approach based on variational formulation (VF) to accelerate the training of Neural ODEs for dynamical systems.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=trV41CpAK4",
    "pdf_link": "https://openreview.net/pdf?id=trV41CpAK4",
    "comments": [
        {
            "summary": {
                "value": "The authors present a new method for training Neural ODEs (NODEs) based on a variational formulation of an ODE loss. The resulting method used spline regression and to interpolate noisy data which allows for a computation of a variational loss. The advantage of the method is many few function evaluations during training."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The strengths of the paper are:\n- the proposed method introduces a reasonable approach to training a NODE based on a variational formulation of the loss\n- The training method requires many fewer function evaluations, allowing for much faster training than existing methods.\n- The method maintains or at times outperms competing methods in terms of accuracy."
            },
            "weaknesses": {
                "value": "The weaknesses of the paper are:\n- Many of the examples seem somewhat toy. It is unclear how the method might extend to much noisier / less structured ODEs with more complex dynamics. In particular one would expect the  spline regression / interpolation to eventually fail on very long, non-smooth or noisy trajectories.\nFrom the paper it is difficult to get a sense of how much these attributes are present in the given benchmarks. \n- Implementation of the method is not straightforward from a practitioner's point of view.\n- The method introduces a number of hyper parameters required for accurate interpolation which must be chosen/tuned.\n- The benefit of reduced training, while convenient, is not broadly important for many of the problems considered."
            },
            "questions": {
                "value": "In the COVID-19 dataset example is the data gathered from real world observations? Or is the data generated from some parametric model of COVID-19 spread?\n\nDo the authors have a sense of the limits of the spline regression / interpolation? On what sorts of trajectories it might fail? \n\nIt would be very helpful if the authors could provide plots of training trajectories the ODEs in consideration so the reader could assess the noise levels / complexity of the trajectories."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new way to train NeuralODEs that is faster than current approaches. Current training strategies require numerically solving the differential equation which is very computationally expensive due to the high number of function evaluations at each time step. The approach introduced here relies on the variational formulation as a surrogate objective. However, the variational formulation still requires the value of the vector field and underlying time series at each time step, so the authors approximate this with cubic splines. To compute oscillatory integrals, the authors incorporate the Filon method. Finally, the authors demonstrate the performance of their method and show impressive computational time gains. Some prediction performance gains are also observed, due to the non-accumulation of errors in their training approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Speeding up the training of neural ODE methods is a very important challenge with potentially high impact. \n\nThe method proposed shows clear computational time improvements as well as some prediction performance gains. \n\nThe method makes sense, and the paper is well written and easy to read."
            },
            "weaknesses": {
                "value": "One significant weakness that I see with this approach is that it requires defining the trajectories of the time series a priori (here with cubic splines). As such, the model with learn a vector field that agrees with the cubic spline interpolation and will not figure out alternative dynamics. If this is correct, this is a signicant limitation as one important application of NeuralODEs is for data imputation. I would like the authors to discuss that limitation more clearly in the paper and/or to argue against the reasoning above. \n\nGiven that you use cubic splines to interpolate the dynamics (and vector field), another baseline can now be considered. That is, you can now use the same cubic spline acceleration to directly integrate Equation 1 and compute the MSE at the observation points. Is such an approach reasonable ? This should be considered as an additional baseline, that doesn't use the variational approach but the cubic spline interpolation."
            },
            "questions": {
                "value": "As stated above, I would like the authors to discuss the limitations of using an explicit interpolation of the dynamics before training.\n\n4.2 Step 4, the fact of interpolating the vector field using the values only at the observation points seems strange to me as the interpolation will not coincide between observations. That is, between observations, with $\\hat{f}$ the interpolation of the vector field:\n$ \\hat{f}(t) \\neq f(\\hat{x}(t))$. Can the authors give more details about this discrepancy and motivate why it makes sense ? \n\nDuring training, your method requires computing cubic splines coefficients for each time series every time (at least for the $b$ coefficients). Can the author elaborate on the compuational cost of such a procedure ? I would also like the authors to explain how the gradient with respect to $\\theta$ can still flow from the computation of the $b$ coefficients - how is this end-to-end differentiable ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces VF-NODE, a new method for accelerating the training of Neural ODEs by using a variational formulation that evaluates vector fields only at observed data points, reducing function evaluations (NFEs) and eliminating autoregression, thereby minimizing error accumulation. The approach integrates Filon\u2019s method to handle oscillations in the loss function and uses natural cubic spline regression to manage noisy or incomplete data. Experiments show VF-NODE is 10x to 1000x faster than traditional methods while maintaining or improving accuracy."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper introduces a novel approach using variational formulation (VF) to greatly accelerate Neural ODE training, reducing function evaluations and improving accuracy. By integrating Filon's method with cubic spline regression for handling oscillatory integrals, the method achieves 10x to 1000x faster training across various dynamical systems with higher or competitive accuracy, demonstrating originality, technical rigor, and real-world applicability."
            },
            "weaknesses": {
                "value": "The author accelerates the training of the original NODEs method by adopting the VF-NODEs approach. However, the necessity and rationality of transforming the training problem into the optimization problem of Equation 6 still require further elaboration.  See Questions for details."
            },
            "questions": {
                "value": "1. The author utilized natural spline interpolation to fit the orbit **$x$** as well as the vector field $f_\\theta$ in this process. Then, why not directly adopt the strategy of gradient flow matching for training (see literature [1]). This approach seems more straightforward and bypasses the high computational costs associated with traditional numerical integration.\n\n2. As the author did not present the trajectory plots in experiments, I am concerned about the performance boundaries of this method in handling time series data. Is this method only effective on data with simple behaviors, or can it still outperform traditional NODE methods for more complex systems (such as chaotic systems) ?\n\n3. When using natural spline interpolation, is there an overfitting issue, such as the Runge phenomenon?\n\n4. This method directly inputs the data from the original system, thus it cannot train through modeling latent variables (like latent ODE method), which may result in a loss of the method's flexibility.\n\n[1] Li X, Zhang J, Zhu Q, et al. From Fourier to Neural ODEs: Flow matching for modeling complex systems[J]. arXiv preprint arXiv:2405.11542, 2024.\n\nIf the authors can answer the above questions well, I would be happy to consider raising the score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new method for speeding up the training of Neural ODEs. The method removes the need for an ODE solve during the forward and backward pass of a Neural ODE.\n\nThe theory behind the approach is to rewrite integration by parts:\n\n$\\int_0^T x\\dot{\\phi} dt = [x\\phi]_{0}^{T} - \\int_0^T \\dot{x}\\phi dt$ \n\nIf $\\phi(0)=0$ and $\\phi(T)=0$, then $\\int_0^T x\\dot{\\phi} dt + \\int_0^T \\dot{x}\\phi dt = 0$.\n\nThe aim is to approximate $\\dot{x}$ using a neural network $f_\\theta(x, t)$. And so rather than solving the ODE for $f_\\theta$ and applying MSE on the observations, the method attempts to minimise $\\int_0^T x\\dot{\\phi_l} dt + \\int_0^T f_\\theta \\phi_l dt$, for a set of orthonormal basis function $\\phi_l$ that are zero at the boundaries of the solve.\n\nA fourier basis is used as a natural choice for $\\phi_l$, and cubic splines are used to approximate $x(t)$ in the integrals to give them analytical solutions.\n\nExperiments demonstrate good performance measured by MSE, and a speed up in training time."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The writing is strong\n- The solution is novel and interesting, it's particularly interesting to see an approach which does not solve the ODE in the forward pass\n- The theory behind the method is strong\n- The experiments demonstrate good MSE\n- The ablations on the basis functions demonstrate using a Fourier basis is a good choice"
            },
            "weaknesses": {
                "value": "- I found the evaluation confusing. As far as I can tell the only place where any wall clock timing is carried out is in Figure 2, on one dataset, the glycolytic model. All other tables seem to be about MSE. However, the main claims and messages of the paper are about accelerating neural ODEs. If the main claim is that this approach speeds up training then this should be shown more across all datasets. Are these numbers available, have I missed them?\n- Figure 2 shows no uncertainties on the time taken to train.\n- There is an error in line 137-138. If there is regularisation applied during training, the training time can also be reduced, since later iterations are made faster after regularisation is applied in earlier iterations of training.\n- Following on from this, the experiments would be made more convincing by testing some of these regularisation methods. For example minimising higher order derivatives. Currently the main baselines for speed are discretise-then-optimise, optimise-then-discretise and Seminorms, of which only Seminorms claim to speed up Neural ODEs. This point would not be as relevant if the main claims of the paper are about MSE rather than time to train.\n- It is not clear what ODE solver is used in the experiments, if adaptive solvers are used it is harder to make the argument that the forward solve is slow. The proposed method is linear in the number of observed points (to do cubic interpolation), however if there are many observed points this method will be slower compared to an adaptive solver if the trajectories are quite smooth. Have adaptive solvers been tested? The reverse is also true, if the dynamics are complex an adaptive solver can take smaller steps whereas the proposed method might suffer from accuracy issues as identified in the paper."
            },
            "questions": {
                "value": "- Is it possible to include two papers in the related work: 1) STEER: Simple Temporal Regularization For Neural\nODEs and 2) Interpolation Technique to Speed Up Gradients Propagation in Neural ODEs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}