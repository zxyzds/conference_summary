{
    "id": "C65Hpf02Ay",
    "title": "One-step Image-function Generation via Consistency Training",
    "abstract": "Consistency models aim to deliver a U-Net generator to map noise to images directly and enable swift inference with minimal steps, even trained in isolation with consistency training mode. However, the U-Net generator requires heavy feature extraction layers for multi-level resolutions and learning convolution kernels with specific receptive fields, resulting in the challenge that consistency models suffer from heavy training resources and fail to generate images with any user-specific resolutions. In this paper, we first validate that training the original consistency model with a small batch size via consistency training mode is pretty unstable, which motivates us to investigate efficient and flexible consistency models. To this end, we propose to use a novel Transformer-based generator to generate continuous image functions, which can then be differentially rendered as images with arbitrary resolutions. We adopt implicit neural representations (INRs) to form such continuous functions, which help to decouple the resolution of generated images and the total amount of the parameters generated from the neural network. Extensive experiments on one-step image generation demonstrate that our method greatly improves the performance of consistency models with low training resources and also provides an efficient any-resolution image sampling process.",
    "keywords": [
        "Image generation; Diffusion models; Consistency Models"
    ],
    "primary_area": "generative models",
    "TLDR": "We use Transformer-based generator to generate image functions via consistency training, and the image functions can be rendered as any-resolution images.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=C65Hpf02Ay",
    "pdf_link": "https://openreview.net/pdf?id=C65Hpf02Ay",
    "comments": [
        {
            "summary": {
                "value": "The authors observe that with low training resources and small batch size, the training of UNet-based consistency model is unstable, and proposed a Transformer-based generator that generates network parameters as INR for consistency training. The authors show better training stability and lower FID metric than the original UNet-based consistency model in the low-resource training setting."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Using consistency training for image function generation is an interesting direction to explore. Since INRs are any-resolution decoders, it is natural to compute the consistency objective in the rendered patches.\n2. The proposed reconstruction pre-training is simple and effective.\n3. The authors show improved FID and other metrics. The training stability is also improved compared to the baseline UNet on common datasets."
            },
            "weaknesses": {
                "value": "1. It is not very clear that why modeling as INR can help improve the stability in low-resource training. With the Transformer generator and INR representation, is the input noisy image / target in training at fixed resolution or varied resolutions? More discussions about the intuition for the improvement might be helpful. Can the reconstruction pre-training also be applied for the UNet consistency model?\n2. Despite showing many metrics, the FID values for both the baseline and proposed method are very high (though it is due to the training budget). The results will be more convincing and solid when the methods can achieve a generally better quality.\n3. The claim the advantage of any-resolution generation, it is better to discuss and compare to more recent works that specifically works on any-resolution image generation, for example [1, 2].\n\n[1] Any-resolution training for high-resolution image synthesis, ECCV 2022\n\n[2] Image Neural Field Diffusion Models, CVPR 2024"
            },
            "questions": {
                "value": "It is shown in the supplementary that the generated INR has better quality than bilinear interpolation when decoding to high resolutions. Is the high resolution higher than the resolution in training? If it is the case of resolution extrapolation, is any artifact observed in high resolutoins?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses two main issues: the training instability of consistency models with small batch sizes and the limitation of generating images at fixed resolutions. To tackle these challenges, the authors propose using a Transformer-based generator along with implicit neural representations. Additionally, to improve training stability, they introduce an auxiliary task before training the consistency model, which leads to faster convergence and enhanced image generation quality. Experimental results show that this approach improves performance in one-step image generation with reduced training requirements and enables efficient, any-resolution image sampling."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "A key strength of this paper lies in its innovative design of a consistency model that supports multi-resolution sampling, overcoming the fixed-resolution limitations of traditional models. The approach also effectively addresses training instability at low batch sizes, making it feasible to train with fewer resources."
            },
            "weaknesses": {
                "value": "While the paper presents improved training efficiency as a key contribution, there are two aspects that raise questions regarding this claim:\n\n1. In comparison with Song et al.\u2019s experimental setup, it seems expected that training with a smaller batch size would lead to lower performance. To convincingly demonstrate an improvement in training efficiency, comparing the proposed model with a consistency model trained on low batch sizes may be insufficient. Instead, it would strengthen the argument to show that the proposed method performs better than models trained with larger batch sizes.\n2. In Figure 8, it appears that pre-training is essential for reaching the convergence point of \u201cDenoising Distance.\u201d However, considering the overall training time, if an additional 30 epochs of pre-training are required compared to traditional methods, it may be worth questioning whether this approach can truly be considered efficient."
            },
            "questions": {
                "value": "1. Total Training Time: Could the authors clarify the total training time required for the model? While it is mentioned that 30 epochs were used for pre-training, it would be helpful to know the training duration for both CM-UNet and CM-Func models.\n\n2. Evaluation in Table 2: The evaluation process for Table 2 is unclear, particularly regarding how the multi-resolution sampling FPS was measured for CelebA 64. Additional explanation on the methodology used for this metric would be appreciated.\n\n3. Effectiveness with Larger Batch Sizes: It would be interesting to know if the proposed method continues to perform better than models trained with batch sizes larger than 32.\n\n4. Related Works : Adding the following reference to the related work section would enhance the context of the study: Zhuang, Peiye, et al. \"Diffusion probabilistic fields.\" The Eleventh International Conference on Learning Representations, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses the limitations of using a U-Net generator with consistency models, i.e., the substantial computational resources required and the difficulty in generating images at user-specified resolutions. To address these challenges, the researchers propose replacing the U-Net generator with an implicit neural representation (INR), which demonstrates potential in producing images with scalable resolutions. The proposed method reduces training costs relative to the U-Net generator while achieving superior image quality as quantified by common evaluation metrics."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed method is conceptually sound and effectively addresses the limitations of the U-Net-based consistency model.\n\nExperimental results support the efficacy of incorporating INR within consistency models for improved image generation."
            },
            "weaknesses": {
                "value": "(1) Novelty: while INR is applied here for high-resolution image generation within the context of consistency models, INR is already a widely-used technique in other 2D image generation frameworks. The contribution appears to be somewhat incremental.\n\n(2) Related work: the review of related work on INR-based methods is somewhat insufficient, particularly in the context of high-resolution image generation. Additional discussion on alternative high-resolution generation strategies would be beneficial.\n\n(3) Performance comparison: although the method shows reduced computational cost, its image quality appears less competitive compared to replacing UNet with DiT, as observed in Table 2.\n\n(4) Computation cost comparison: it would be helpful to include a broader computational cost comparison with other methods listed in Table 2, rather than restricting comparisons solely to the CM-UNet model.\n\n(5) It would be clearer and more concise to use \u201cEq.\u201d rather than \u201cEq. equation.\u201d when referring to equations."
            },
            "questions": {
                "value": "See the limitations above, which detail the questions concerned, and it is expected to address these issues."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel approach to image generation through consistency models, aiming to improve efficiency in generating high-quality, variable-resolution images. By adopting a Transformer-based generator that leverages implicit neural representations (INRs), the authors propose an architecture allowing flexible resolution generation with reduced resource demands. The method addresses challenges associated with traditional U-Net models by decoupling image resolution from model parameters and incorporating a pre-training phase for enhanced consistency training."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The introduction of a Transformer-based generator that produces image functions is an efficient approach that enables any-resolution sampling. This is a significant step forward from fixed-resolution U-Net generators.\n2. By decoupling image resolution from model parameters, the proposed method reduces computational overhead and GPU memory usage, allowing more accessible high-resolution image generation.\n3. The pre-training task effectively enhances the consistency model\u2019s performance, leading to faster convergence and better denoising capabilities compared to models trained from scratch."
            },
            "weaknesses": {
                "value": "1. The paper utilizes Transformers in a relatively straightforward way for image generation. While the INR-based function generator is effective, the paper could benefit from a clearer explanation of how it fundamentally diverges from other Transformer-based models in diffusion applications. \n2. The pre-training phase, while beneficial, adds additional complexity to the training pipeline. It would be helpful to compare the training cost between this method and other approaches.\n3. The comparisons with existing one-step diffusion methods are missing. In fact, there are a lot of one-step methods, including ADD and DMD.\n4. Given that the method proposed by the authors is capable of generating images of arbitrary resolution, in the selection of datasets in Section 4.1, the authors should consider including more datasets with various resolutions beyond the current 64 and 128 to facilitate a comprehensive comparison. In fact, a larger resolution has become more popular, e.g. 512 and 1024. It is hard to justify whether this method can actually accommodate arbitrary resolution without reporting the results of high resolution image synthesis.\n5. To evaluate the method, more metrics should be considered when comparing different methods, including NIQE, CLIPIQA, MUSIQ, LPIPS, MANIQA, DISTS."
            },
            "questions": {
                "value": "More results are required and more methods should be compared."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}