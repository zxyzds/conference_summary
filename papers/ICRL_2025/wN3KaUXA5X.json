{
    "id": "wN3KaUXA5X",
    "title": "Diffusion On Syntax Trees For Program Synthesis",
    "abstract": "Large language models generate code one token at a time. Their autoregressive generation process lacks the feedback of observing the program's output. Training LLMs to suggest edits directly can be challenging due to the scarcity of rich edit data. To address these problems, we propose neural diffusion models that operate on syntax trees of any context-free grammar. Similar to image diffusion models, our method also inverts \"noise\" applied to syntax trees. Rather than generating code sequentially, we iteratively edit it while preserving syntactic validity, which makes it easy to combine this neural model with search. We apply our approach to inverse graphics tasks, where our model learns to convert images into programs that produce those images. Combined with search, our model is able to write graphics programs, see the execution result, and debug them to meet the required specifications. We additionally show how our system can write graphics programs for hand-drawn sketches. Video results can be found at https://td-anon.github.io.",
    "keywords": [
        "neurosymbolic",
        "search",
        "programming languages",
        "inverse graphics"
    ],
    "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)",
    "TLDR": "We propose a diffusion based approach on syntax trees to do program synthesis for inverse graphics tasks",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wN3KaUXA5X",
    "pdf_link": "https://openreview.net/pdf?id=wN3KaUXA5X",
    "comments": [
        {
            "comment": {
                "value": "Thank you for your positive review, we are very encouraged by your positive feedback and comments. We also thank you for providing insightful discussion and thoughts about our work.\n\n> It is better to mention the size of the decoder model in the architecture section rather than in the appendix, so that readers with LLM background can quickly understand the edge of the model on this task.\n\nThis is a really good point, and we will state this in the main text. For reference, the model we are using is much smaller than typical LLMs, around ~10 million parameters.\n\n> It is better to discuss the number of steps in the diffusion procedure, and the model's potential ability limit in terms of output sequence length or number of symbols.\n\nWe wanted to ask for clarification to this point, is this asking us to measure performance conditioned on the number of symbols in the ground-truth test tasks? So, fewer symbols should be easier and more symbols should be harder? We discovered that increasing the number of symbols ended up making the test tasks easier than harder, since the probability we would subtract a big shape to make the image fully black was increased, so we went for the procedure in Appendix D.\n\n> Two highly related work should be cited and discussed \u2026\n\nBoth of these works are very relevant, and we will cite these works. Thank you for bringing these to our attention.\n\n---\n\nThank you for your feedback, your positive review, and insightful questions."
            }
        },
        {
            "comment": {
                "value": "Thank you for your encouraging review. We are glad that you see the removal of the partial compiler and having to use no reinforcement learning while still using the execution output as strengths of our approach. We will answer your insightful questions here.,\n\n> In the ablations, there is one about training only on the last mutation step. It's illustrated by Fig. 12 by only showing the transition from z_5 to z_4. Do I understand correctly that the other reverse transitions (e.g., z_4 -> z_3, ..., z_1 -> z_0) are not used for training?\n\nThis is correct, the other transitions are not used for training. We considered using all the transitions at once as a training signal, but we were not actually sure how to do that, since the network must receive the execution output of the \u201ccurrent\u201d program, which program would be the \u201ccurrent\u201d?\n\n> Can you define the \"Rollout\" method, and the differences with the \"Search\" one?\n\nThis is our mistake for not stating, we will edit to add this description. Tree Diffusion Rollout is using the \u201cpolicy\u201d network\u2019s actions (i.e. edit actions) being sampled from the model repeatedly, progressively editing an initial program, $z_0 \\to z_1 \\to \\ldots$. Tree Diffusion Search is using this same policy, but instead sampling multiple possible \u201cedits\u201d and ranking them using the value network, as in beam search, which we described in Section 3.3.\n\n> What's the relationship between \"Number of nodes expanded\" and the \"number of compilations needed\"?\n\nThis is indeed a typo in Line 372, we will update this to be consistent and say \u201cNumber of nodes expanded\u201d instead of \u201cnumber of compilations needed\u201d. Each node is a full program that is either edited through pure policy (rollouts), via search (policy+value), or by repeatedly re-sampling the node from the VLM baseline. We will fix this.\n\n---\n\nWe thank you for your review, your feedback, and thoughtful discussion about our work."
            }
        },
        {
            "comment": {
                "value": "(... continued)\n\n> If so, please state explicitly and explain how this metric is applied to the RGB images in TinySVG.\n\nWe accept an image if at least 99% of the pixels have every channel (RGB) within $1/256$ of the goal image. For each channel we have 256 possible values, and for a value to be considered the same they have to be within 1 step of each other. So an RGB tuple of $(76, 14, 243)$ is accepted with $(76, 13, 244)$ but not with $(88, 9, 10)$. We want at least 99% of the pixels to match in this way. Since these images are normalized between $0$ and $1$, this threshold is $0.005$.\n\n> The difference between tree diffusion search and tree diffusion rollouts is not explicitly stated or defined in section 4.\n\nAgain, this is our mistake for not stating, we will edit to add this description. Tree Diffusion Rollout is using the \u201cpolicy\u201d network\u2019s actions (i.e. edit actions) being sampled from the model repeatedly, progressively editing an initial program, $z_0 \\to _1 \\to \\ldots$. Tree Diffusion Search is using this same policy, but instead sampling multiple possible \u201cedits\u201d and ranking them using the value network, as in beam search, which we described in Section 3.3.\n\n> There were references to computational demand and efficiency, yet no time-related metrics were reported to demonstrate gains in this regard, despite claims about improving performance efficiency.\n\nAll methods (including baselines) use the same neural architecture. All methods also use the same renderer (compiler). The cost of expanding a node is the cost of a forward pass through the neural architecture + the cost of rendering the program and checking. Therefore the X-axis in Figure 4 represents the total computation needed. For instance, increasing the model parameter count would scale the time taken by each method identically.\n\n---\n\nWe hope we are able to clarify some of your concerns, and please let us know if we misunderstood any point. We really appreciate the feedback, both positive and negative, and the insightful discussions."
            }
        },
        {
            "comment": {
                "value": "Thank you for your insightful review, we really appreciate the thorough reading alongside your detailed and engaged questions. We are also encouraged that you found our neurosymbolic design to be a strength of our work. We will attempt to address your mentioned weaknesses and concerns here.\n\n>  the illustrative example shown in Figure 2, enabling the approach to modify node types rather than shape, the base syntax tree structure is governed by the initial generated program.\n\nWe will try to illustrate how our mutation scheme allows for not only changing node types, but also the shape and structure of the whole syntax tree here. Consider the tree in Figure 8, in Appendix A. Under our approach, all nodes in green are potential mutation targets. Node 44 is the width of the rectangle and has type `Number`. And it can be replaced with any other `Number`. This allows changing node types. For changing tree shape and structure, consider Node 33. Node 33 contains 2 primitives, the left Quad and the right Quad. A mutation target can replace this _entire_ node with a single node that contains only 1 primitive. Let\u2019s say we replace Node 33 with a Circle node. We have effectively changed the shape of the tree. With this Circle node, Node 32 is now also a potential mutation target, and can be replaced with a smaller subtree and so on. This way, we can actually delete the whole tree with our mutations. The reverse is also true, a node with just one primitive can be replaced with a node with two, which allows us to \u201cgrow\u201d any tree using a series of small mutations.\n\nWe encourage the reviewer to look at our interactive demo for this (https://td-anon.github.io/). If you scroll down on the page and click the \u201cAdd Noise\u201d button, it will run our code in the browser to show how our mutations can edit both the type _and_ shape of the tree.\n\nIf this makes it clearer, we will include an illustrative description such as above to make this important point clearer.\n\n> Specifically, it is not clear how replacing the \"(\" denoted by the edit position token and replacing it with the grammar constrained autoregressive decoding yield valid syntax \u2026\n\nThe figure is indeed confusing in this way. It looks like it\u2019s replacing the opening \u201c(\u201c with the whole expression, but your hunch is correct, every parenthesis actually uniquely refers to a node in the syntax tree (or alternatively every opening \u201c(\u201c is implicitly matched with a closed \u201c)\u201d and the entire sub-expression is replaced). These details are actually very nicely handled by the fact that we are using a context-free grammar and the excellent Lark library for parsing, which automatically maps the node in the syntax tree to tokens in the S-expression representation. Our implementation is flexible such that only the grammar needs to be changed and it would work with other formats.\n\n> How are varying input lengths handled? \u2026  In addition, it is not clear what the purpose of \"EOS\" is in this context.\n\nThe Transformer architecture we\u2019re using (akin to GPT-2) can handle varying input lengths (in the same way as large language models). The sequences are modeled autoregressively, and hence the `<EOS>` (i.e. \u201cend of sequence\u201d) token is used for the model to indicate that it has stopped generating. We could just stop decoding when all syntax constraints are met, however, using the `<EOS>` token allows us to re-use standard Python libraries that work with these Transformer models.\n\n> The fraction of problems solved by the method trained with \"no reverse path\" is nearly the same as that of the control after about 60 expanded nodes.\n\nAgain, great catch! We will remove the word \u201csignificantly\u201d here. It does improve the results, but as you mentioned, not by much. We actually think this is a strength, since computing the optimal reverse path is not always possible, and the experiment shows that it is not strictly needed. We will restructure the description of this to be more along the lines of \u201cit helps a little but not by much\u201d.\n\n> Overall the presentation of section 3, especially 3.4, requires careful rework.\n\nWe are grateful for your feedback on Section 3.4 (based on your previous weaknesses and questions), and will make the issues you bring up clearer.\n\n> However, access to this ground truth may not be readily available in most cases.\n\nThe ground-truth mutations are actually just the reverse of the noise mutations. Every mutation can be inverted, so if we add noise mutations, the ground-truth are the reverse of these mutations. We add a variable number of mutations, so the ground-truth distance is the number of mutations we added.\n\n(continued ...)"
            }
        },
        {
            "comment": {
                "value": "Thank you for your positive review, we are very encouraged that you thought our paper was simple but non-obvious, well-written, and easy to understand.\n\n> Fig 3 is confusing. v(x) is the value function or the pre-trained image encoder? if its pretrained, why is there a _phi subscript?\n\n\nYou are correct about Figure 3, this is a typo and we will make a correction to this. We were trying to use the same notation as in Tsimpoukelli et al. (2021), where they used $v_\\phi$ for the image encoder, but just made the mistake of re-using the notation as the value function. Thank you for pointing this out!\n\n> Where do the initial problems come from? It seems like they are generated randomly, but how?\nWe (very) briefly touch on this in Section 3.1, Line 183. The initial problems come from our `ConstrainedSample` function, which follows Luke (2000) and Zeller et al. (2023), which randomly samples programs from the context-free grammar. You are correct that we don\u2019t specify that this method is used to generate the problems, and we will definitely clarify this.\n\n> Do the fuzzing and edit algorithms generalize easily to non-context-free grammars (e.g. general programming languages)?\n\nThis is a really good question, and we believe it is an open research question for future work. Indeed, there have been fuzzers written for general programming languages (for instance, AFL can fuzz JavaScript code for systems like V8, etc.), but it is not obvious yet how to use that appropriately for an execution guided system like ours. We think this is an exciting future direction!\n\n> How many steps did you train for? I don't think this is covered in the appendix.\n\nWe trained all methods, including the baselines for 1 million steps. You are correct that it wasn't covered in the Appendix, and is also something we will add.\n\n---\n\nAgain, we thank you for your encouraging review, insightful questions, and we will take the feedback into account and make the appropriate clarity changes."
            }
        },
        {
            "summary": {
                "value": "The authors propose a program synthesis method based on \"tree diffusion\". They randomly corrupt programs (with some constraints) and learn to invert the corruptions, conditioned on the output of the corrupted program and the target output (an image rendering in their case)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The method is simple but non-obvious\nThe more general problem of program synthesis conditioned on desired outputs is very relevant\nThe authors use randomly generated programs as a dataset which sidesteps dataset curation in favor of just a specification of the language\nThe paper is well-written, easy to understand, and has nice and (mostly) clear figures"
            },
            "weaknesses": {
                "value": "The paper is somewhat limited in scope (simple problem setup) in ways that make it not entirely obvious how the method \"scales\" to more complex relevant tasks like code generation.\n\nSome minor things covered in Questions"
            },
            "questions": {
                "value": "Fig 3 is confusing. v(x) is the value function or the pre-trained image encoder? if its pretrained, why is there a _phi subscript?\n\nWhere do the initial problems come from? It seems like they are generated randomly, but how? \n\nDo the fuzzing and edit algorithms generalize easily to non-context-free grammars (e.g. general programming languages)?\n\nHow many steps did you train for? I don't think this is covered in the appendix."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces an innovative approach to inverse graphics tasks by combining diffusion models with transformers. The authors present the first application of diffusion to program synthesis using explicit syntax tree updates, validating their method on CSG2D and TinySVG environments."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Innovative Approach: The paper presents a novel combination of autoregressive, diffusion, and search methodologies, which, despite being applied to a specific domain, holds potential for broader applications. The reverse mutation path algorithm also provides an efficient way to generate training targets. \n- Clarity and Replicability: The manuscript is well-written and easy to follow, providing sufficient detail to enable replication of the experiments.\n- Comprehensive Ablation Studies: The authors conduct thorough ablation studies on key hyperparameters and the impact of integrating search, enhancing the understanding of their method's efficacy."
            },
            "weaknesses": {
                "value": "- Literature Coverage: The authors should consider citing \"Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation\" in the Neural program synthesis section since this work also takes multiple passes of the program and edits the program. \n- The value network (v\u03d5) training and effectiveness aren't thoroughly evaluated. Alternative approaches to edit distance estimation, including direct calculation from syntax trees, are not explored or compared."
            },
            "questions": {
                "value": "- In the limitations section, you mention that your syntax tree currently supports only a limited set of operators. What are the bottlenecks in expanding support to other operators and generalizing to broader coding problems?\n- What is the cost of training the value network that predicts the edit distance?\n- Given the recent advances in vision-language models, how does your approach compare against contemporary models like VILA or LLaMA? The current baselines only include older models (4+ years old), and evaluating against recent state-of-the-art would provide a more helpful comparison."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed a program synthesis framework using mutations on syntax trees via a neural diffusion model for inverse graphic tasks. These tasks aim to convert images that can contain free hand-sketches of shapes or a set of computer-generated colored shapes into images depicting a computer-generated rendering matching the input. The authors defend the claim that the approach presents the ability to edit trees generated using a base model as opposed to incrementally autoregressive approaches that fail to narrow down the search space. The authors apply their method to inverse graphics tasks and present results in two settings (CSG2D and TinySVG) and show improved performance in the number of problems solved compared to baselines (REPL VLM and VLM) along with an ablation study to investigate the individual contributions of constituent components."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The main strength of this paper is the design of a neurosymbolic framework to evaluate the automated (i.e. diffusion-based) conversion of images into context-free grammar. This formal evaluation ensures that the desired specifications are met through iterative observation of the execution results and verification.\n\n2. The authors extend the approach to accept hand-drawn sketches and illustrate examples in the appendix confirming the applicability of the approach in several real-world settings.\n\n3. The supplementary videos illustrate the overall problem that the authors are attempting to solve and showcases the \"edits\" made by the framework."
            },
            "weaknesses": {
                "value": "There are three main weaknesses I would like to bring up. The authors are encouraged to rebut and provide legitimate explanations, if any, against these and the review decision may be adjusted accordingly.\n\n1. A claim made by the author states that the proposed method focuses on editing the program synthesized from the image, unlike prior works that autoregressively generate programs that are incrementally better. In doing so, the authors propose adding random noise to modify a base syntax tree generated from CSG2D. Despite the illustrative example shown in Figure 2, enabling the approach to modify node types rather than shape, the base syntax tree structure is governed by the initial generated program. It remains unclear (at least it has not been proven) that diffusion + base tree always yields the optimal syntax tree (a statement regarding suboptimal steps in section 4.3 is thus not justified). An analysis and example to demonstrate this is lacking and should be included.\n\n2. The overall architecture presented in Figure 3 is difficult to understand at first glance. In addition, the descriptions provided in section 3.4 (the model architecture) do not present enough detail to understand Figure 3.  Specifically, it is not clear how replacing the \"(\" denoted by the edit position token and replacing it with the grammar constrained autoregressive decoding yield valid syntax (i.e. are there low-level implementations in play that ensure that entire blocks from \u201c(\u201c to \u201c)\u201d are parsed out during replacing? How are varying input lengths handled? ). Replacing \"(\" with \"(Quad 8...\" seems to break the pairing of parenthesis. In addition, it is not clear what the purpose of \"EOS\" is in this context.\n\n3. The fraction of problems solved by the method trained with \"no reverse path\" is nearly the same as that of the control after about 60 expanded nodes. The control reaches the same performance at about 50 nodes. Is this a \"significant\" efficiency gain when the maximum node expansion budget was two orders of magnitude higher (i.e. 5000)? There are no computational or time-related metrics presented which help put this into context.\n\n4. Overall the presentation of section 3, especially 3.4, requires careful rework."
            },
            "questions": {
                "value": "Here are some of the main points I would like to make:\n\n1. The edit distance introduced by Pawlik & Augsten is narrowed to allow only small changes. If there are big changes, the change that reduces the distance between the trees the most is chosen. While this can be used as a training signal, it is assumed and was mentioned in section 3.3.2 that access to the ground-truth mutations is available. However, access to this ground truth may not be readily available in most cases. How is this ground-truth data obtained? If not automated, the authors must comment on the limited scalability of the approach.\n\n2. In section 4.2 under evaluation, it is not clear what the criteria for considering a match between the synthesized and true plan is. Re: \u201cIn TinySVG, we accepted an image if 99% of the pixels were within 0.005 \u2248 $\\frac{1}{256}$.\u201d Is this \u201cwithin 0.005\u201d a tolerance on the 8-bit pixel color intensity? If so, please state explicitly and explain how this metric is applied to the RGB images in TinySVG.\n\n3. The difference between tree diffusion search and tree diffusion rollouts is not explicitly stated or defined in section 4.\n\n4. There were references to computational demand and efficiency, yet no time-related metrics were reported to demonstrate gains in this regard, despite claims about improving performance efficiency. It is unclear as to just how much improvement the proposed approach affords."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents an algorithm to train a diffusion model on the abstract syntax tree (AST) of programs written in a simple procedural language. That language produces 2D images by combining geometric shape primitives, and the resulting image is used to guide the denoising process.\n\nUsing an additional network estimating the distance from a rendered image to the target image, beam search is applied to generate a sequence of AST edits (node replacements) that produce a predicted program, approximating the target image. That approach transfers to generating geometric images from noisy sketches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Originality\n-------------\nThe paper takes inspiration from existing ideas and benchmarks, but they are clearly cited, and the novel aspects are well described. For instance, a backward edit path that's better than reversing the corruption path, removing the need of a partial renderer, and relying on beam search rather than full-fledged reinforcement learning.\n\nQuality\n----------\n\nExperiments demonstrate the advantages of the proposed approach, and properly ablate the different aspects and contributions.\n\nClarity\n---------\nThe paper is overall clear and straightforward to follow. With the additional details of the appendix, the approach should be re-implementable by a different team.\n\nSignificance\n-----------------\nUsing ML models to directly manipulate and modify programs, rather than either generate a whole program autoregressively, or emit edition instructions (which could be invalid or result in an invalid program) could make iterative program generation better or easier.\nThe fact that no reinforcement learning is required, but observation of the output of intermediate programs can simply be combined with beam search is also an interesting result."
            },
            "weaknesses": {
                "value": "Originality\n-------------\nNo major weakness here, the work is in the continuation of previous cited work.\n\nQuality\n----------\nComparison with baselines might have been more extensive, specifically the RL-based algorithms from previous work, which could have better shown how \"brittle\" they were.\n\nClarity\n---------\nA few things were not clearly defined in the experiments and ablation sections (see \"questions\" below).\n\nSignificance\n----------------\nOverall, the CSG2D and TinySVG languages are a small-scale benchmark, but it's unclear whether the proposed approach would scale to large, structured programs in general purpose languages.\nFor instance, it might not be possible to find a sequence of valid programs created by short mutations between two relatively close programs. For instance, going from recursion to a loop, from an implicit lamda to a declared function, or from a for loop to a list comprehension. Even splitting a function into smaller pieces may require either large edits, or intermediate unparseable states."
            },
            "questions": {
                "value": "1. In the ablations, there is one about training only on the last mutation step. It's illustrated by Fig. 12 by only showing the transition from z_5 to z_4. Do I understand correctly that the other reverse transitions (e.g., z_4 -> z_3, ..., z_1 -> z_0) are not used for training?\n2. If so, why not use that (training on all denoising steps) as a baseline?\n3. Can you define the \"Rollout\" method, and the differences with the \"Search\" one?\n4. What's the relationship between \"Number of nodes expanded\" and the \"number of compilations needed\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the edge of visual symbolic reasoning and code generation. It addresses the important task of generating code (symbolic sequence) to depict images with visual feedbacks. It applies diffusion model-like approaches to permute the program syntax tree and guarantee the correctness of the generated code. Through iterations, the model is able to recover the image with high fidelity using discrete preset symbols."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This paper proposed a novel solution to the reverse CG field, to synthesis programs for visual symbolic reasoning. The proposed method address the hard task through the unique lens of syntax tree, and achieves notably better results.\n\n- The idea of permuting on syntax tree allows for more efficient model, with better performance.\n\n- The efforts to make demo video makes the paper easier to understoand and spread."
            },
            "weaknesses": {
                "value": "This is a good paper, with minor weakness points below.\n\n- It is better to mention the size of the decoder model in the architecture section rather than in the appendix, so that readers with LLM background can quickly understand the edge of the model on this task.\n\n- It is better to discuss the number of steps in the diffusion procedure, and the model's potential ability limit in terms of output sequence length or number of symbols.\n\n- Two highly related work should be cited and discussed:\n\n\"Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation\" in ICML 2023, which explore the possibility of syntax tree to generate code, and via coarse-to-fine multi-round generation approach.\n\n\"Symbolic Visual Reinforcement Learning: A Scalable Framework with Object-Level Abstraction and Differentiable Expression Search\" in TPAMI, which also learns visual symbolic programs, not to depict the image but to interact with the environments. Rainbow environment is also leveraged in their experiments."
            },
            "questions": {
                "value": "See discussions above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}