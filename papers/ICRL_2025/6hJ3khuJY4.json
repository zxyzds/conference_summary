{
    "id": "6hJ3khuJY4",
    "title": "Learned Data Transformation: A Data-centric Plugin for Enhancing Time Series Forecasting",
    "abstract": "Data-centric approaches in Time Series Forecasting (TSF) often involve heuristic-based operations on data. This paper proposes to find a general end-to-end data transformation that serves as a plugin to enhance any arbitrary TSF model's performance. To achieve this, we propose the Proximal Transformation Network (PTN), which learns effective transformations while maintaining proximity to the raw data to ensure fidelity. When orthogonally integrated with popular TSF models, our method helps achieve state-of-the-art performance on seven real-world datasets. Additionally, we show that the proximal transformation process can be interpreted in terms of predictability and distribution alignment among channels, highlighting the potential of data-centric methods for future research. Our code is available at https://anonymous.4open.science/r/PTN-2FC6/.",
    "keywords": [
        "time series",
        "data-centric",
        "data transformation",
        "forecasting",
        "generalization",
        "deep learning"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=6hJ3khuJY4",
    "pdf_link": "https://openreview.net/pdf?id=6hJ3khuJY4",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a Proximal Transformation Network to learn effective transformations while maintaining proximity to the raw data to ensure fidelity. The model includes a convolution-based Encoder and an attention-based Encoder that provide transformation on different levels of proximity. The training involves a co-optimization of the proximity of the transformed data and forecasting accuracy. The method achieves state-of-the-art performance on seven real-world datasets. Additionally, the paper shows that the proximal transformation process can be interpreted in terms of predictability and distribution alignment among channels."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The traditional time series prediction task has been redefined as a \"two-step problem,\" the goal is to learn predictions on the transformed data and align them with the raw series, showcasing innovation.\n\n* The proposed method achieves state-of-the-art performance on seven real-world datasets. The ablation experiments are comprehensive.\n\n* The effectiveness of the proposed module is demonstrated through the distribution of data on the loss surface, revealing its ability to categorize time series into predictable and unpredictable groups in a self-supervised manner, with a particular focus on enhancing performance for the former."
            },
            "weaknesses": {
                "value": "* When the predictor is a linear model, the complexity of PTN seems to far exceed that of the predictor itself. It is suggested to provide the time and memory complexity analysis of the proposed module. Additionally, for predictors that include modules capturing channel-wise and patch-wise correlations, the proposed PTN appears redundant, which may affect its generalizability.\n\n* Based on the results in Table 10, the PTN module appears to enhance performance primarily for simple linear models, while its effectiveness on more complex models, such as iTransformer and PatchTST, varies against the dataset. Considering that the main objective of this paper is to propose a general plugin, it is essential to select a sufficient range of predictors for experimentation.\n\n* The article contains several errors that require careful proofreading. For example, \"Encoder\" in line 65 should be corrected to \"Decoder,\" and the shape of the matrix in line 129 needs clarification. Additionally, there are concerns regarding Figure 2(b), where $l_{\\text{raw}}$ decreases as $l_{\\text{pred}}$ increases, which seems counterintuitive and requires further explanation."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces the Proximal Transformation Network (PTN) as a data-centric plugin for enhancing time series forecasting. The proposed PTN aims to find optimal data transformations that improve model performance while preserving data fidelity. Extensive experiments demonstrate state-of-the-art results when the method is integrated with various forecasting models. The key contributions include a reformulation of the time series forecasting problem, the introduction of PTN, and successful performance on seven real-world datasets. The approach highlights the potential of data-centric methods in advancing time series forecasting research"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.The paper is well-organized and easy to understand.\n2.The proposed model can be widely applied.\n3.The proposed model achieves SOTA performance."
            },
            "weaknesses": {
                "value": "1.In section 3.2, while two losses are considered, what are the motivations/insights of the losses. The reason why they have an influence on the results should be explained.\n2.As a plug-and-play model, whether it is lightweight and easy to use is an important criterion, but the experiment does not analyze the time and space complexity of the proposed model.\n3.While the performance of the model is not promising enough, the authors don\u2019t analyze the results or explain the pattern."
            },
            "questions": {
                "value": "1.What is the motivation of losses in section 3.2?\n2.Please study the time and space complexity of the proposed model.\n3.Why the performance is not stable compared with baselines?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes the Proximal Transformation Network (PTN), a plugin for improving time series forecasting (TSF) by learning general, data-centric transformations that enhance model performance while preserving proximity to the original data. PTN, which combines a convolutional encoder and attention-based decoder, can integrate with any TSF model, optimizing both data fidelity and forecasting accuracy. Through experiments on seven real-world datasets, PTN achieves state-of-the-art results, showing its effectiveness across linear and non-linear models and its ability to adapt data distributions for better predictability. Additionally, PTN supports interpretability and transferability, offering potential applications in other time series tasks, like anomaly detection and classification."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. PTN offers a general, model-agnostic approach to improve time series forecasting across diverse datasets.\n\n2. It achieves state-of-the-art results, enhancing accuracy and robustness for both linear and non-linear models.\n\n3. This paper conducts too many experiments to show the effectiveness of their framework."
            },
            "weaknesses": {
                "value": "1. The paper\u2019s abstract and introduction do not clearly convey the overall research idea and process, making it difficult to understand the framework.\n\n2. The authors mention that PTN shows potential to make time series forecasting more interpretable. However, the enhanced embedding is latent, produced by deep learning. It is unclear how this actually enhances interpretability.\n\n3. The authors provide a caption for the framework, but it does not help clarify the framework\u2019s procedure. It's unclear how the transformed embedding is involved in enhancing the prediction task. I suggest that the authors include a framework overview to illustrate the entire process."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a data transformation model to support long-term time series forecasting. Specifically, it first obtains a transformation of the raw data using Proximal Transformation Networks (PTNs) and then uses the transformed data to train a predictor. Each PTN consists of a convolutional encoder and a decoder with intra-patch attention, channel-wise attention, and a point-wise linear head. Experiments on several benchmark datasets are conducted to evaluate the effectiveness of the proposed model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The idea is relatively novel and is supported by some theoretical insights.\n2. Extensive experiments from various perspectives are provided.\n3. The motivations behind the proposal are well introduced."
            },
            "weaknesses": {
                "value": "1. The comparison with baselines in Table 9 for look-back length 512 appears to be unfair. For instance, iTransformer should also use a look-back length of 512, and the results of PatchTST in this table are much worse than those reported in the PatchTST paper (PatchTST/64 in Table 3 of its paper).\n\n2. The proposed model does not seem to perform well on complex datasets, such as Traffic. It would be beneficial to provide results on more complex datasets, such as the PEMS datasets used in the iTransformer paper.\n\n3. It would be helpful to include Mean Squared Error (MSE) results in Table 4.\n\n4. It seems inappropriate to claim that MoE is used without a gating network. Additionally, the method for selecting an appropriate number of PTNs, as well as the specific values used in the paper, is unclear.\n\n5. There is no complexity analysis when adding the proposed model to the base models.\n\n6. There are also many unclear points and typos in the paper, such as:\n\n+\nFigure 1 is not well explained, e.g., the meaning of \"7/8\".\n\n+\nIt is unclear how the outputs of intra-patch attention and channel-wise attention are combined.\n\n+\nIt is unclear how the prediction process is conducted after training. Should the raw data be directly input to the trained predictor?\n\n+\n\"An attention-based Encoder\" should be \"An attention-based Decoder\" on Page 2.\n\n+\n\"Piece-wise Linear Head\" should be \"Point-wise Linear Head\" in Figure 3."
            },
            "questions": {
                "value": "Same as the weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}