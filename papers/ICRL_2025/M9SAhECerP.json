{
    "id": "M9SAhECerP",
    "title": "Long-Term Fairness in Reinforcement Learning with Bisimulation Metrics",
    "abstract": "Ensuring long-term fairness is crucial when developing automated decision making systems, specifically in dynamic and sequential environments. By maximizing their reward without consideration of fairness, AI agents can introduce disparities in their treatment of groups or individuals. In this paper, we establish the connection between bisimulation metrics and group fairness in reinforcement learning. We propose a novel approach that leverages bisimulation metrics to learn reward functions and observation dynamics, ensuring that learners treat groups fairly while reflecting the original problem. We demonstrate the effectiveness of our method in addressing disparities in sequential decision making problems through empirical evaluation on a standard fairness benchmark consisting of lending and college admission scenarios.",
    "keywords": [
        "fairness",
        "reinforcement learning",
        "bisimulation"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We use bisimulation metrics to optimize the reward function and observation dynamics in an MDP in order to achieve long-term fairness.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=M9SAhECerP",
    "pdf_link": "https://openreview.net/pdf?id=M9SAhECerP",
    "comments": [
        {
            "summary": {
                "value": "The paper studies fairness in reinforcement learning. The author(s) proposed bisimulation metrics to measure long-term fairness. These metrics were further minimized to learn the reward and transition function, based on which the estimated optimal policy is derived. The author(s) further conducted two numerical experiments, including a lending example and a college admission example to investigate the finite sample performance of their proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "I summarize the strengths of the paper as follows:\n\n* Although fairness has been widely studied in the machine learning literature, it has been less considered in RL, to my knowledge. In that sense, the problem formulation is relatively \"original\".\n* The approach to ensuring fairness through optimizing bisimulation metrics is \"original\", to my knowledge. Existing work in RL typically imposes constraints to guarantee fairness. \n* The proposed $\\pi$-bisimulation-guided reward shaping algorithm is \"original\", to the best of my knowledge."
            },
            "weaknesses": {
                "value": "I summarize the weaknesses of the paper as follows:\n\n* One of my major concern lies in the use of the $\\pi$-bisimulation-guided approach for ensuring fairness in RL. In particular, the $\\pi$-bisimulation metric in (3) is intended to identify equivalent states with same rewards and transition functions. Similarly, the revised $\\pi$-bisimulation metric in (7) is intended to identify equivalent state-group pairs with same rewards and transition functions. In other words, if two groups have similar rewards and transition functions, imposing such a constraint is unnecessary as they inherently would achieve similar rewards and transition functions. Conversely, if the rewards and transitions differ between two groups, enforcing such a constraint to align them might result in approximations that differ substantially from their true values. Are there scenarios where deliberately modifying the reward/transition functions could lead to fairer outcomes without significantly compromising performance.\n\n* Definition 5 considers a fixed s between two groups. However, the proposed metrics optimize over different state-group pairs. It makes more sense to minimize across different groups while holding the states constant. Please clarify your reasoning for optimizing over different state-group pairs rather than fixing the state across groups.\n\n* Theorems 1 and 2 appear to be direct extensions of existing results. Theorem 3, on the other hand, is ambiguous. Specifically, Definition 5 involves a constant $\\epsilon$. Please clarify which $\\epsilon$ value would be attained by minimizing the proposed metric.\n\n* The presentation needs to be enhanced. At several places, it remains unclear to me how the proposed methodology is indeed implemented. For instance, on Page 4, the author(s) mentioned their proposal is to minimize Equation (7). However, it remains unclear what are the parameters being optimized. Similarly, in Algorithm 1, it remains unclear what is the definition of the parameter $\\omega$ being optimized on Line 12 (see the Questions Section).\n\n* Current theories did not fully support the validity of the proposal. For instance, it can be seen from Algorithm 1 that the proposed algorithm is iterative in nature, which alternates between estimation of the reward and transition functions and learning of the optimal policy. Please clarify whether such an iterative algorithm would converge. \n\n* The numerical example, particularly the college admission, appears superficial. Can you consider more realistic examples to more effectively evaluate the various algorithms?\n\n* Should $j^{\\pi}$ in Definition 5 be $V^{\\pi}$? This seems a typo."
            },
            "questions": {
                "value": "1. What does the \"long-term fairness\" mean? Why do you want to emphasize \"long-term\" in the title?\n2. What parameters are being optimized when minimizing Equation (7)?\n3. What is the definition and role of parameter $\\omega$ in Algorithm 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work studies the problem of enforcing long-term fairness in reinforcement learning (RL). Unlike constrained optimization approaches, the authors propose an unconstrained policy optimization algorithm. This algorithm is inspired by the connection between bisimulation metrics and long-term DP fairness in RL. The authors analyze the proposed group-conditioned bisimulation metric and propose a practical algorithm to adjust the reward and observation dynamics to achieve long-term fairness. The effectiveness of the proposed algorithm is demonstrated through extensive numerical studies on two real-world case studies, comparing it with several baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe connection between bisimulation and long-term fairness in RL is a novel and insightful approach.\n2.\tThe authors preform thorough numerical studies on two real-world lending and college admission scenarios, comparing their method against up-to-date baselines."
            },
            "weaknesses": {
                "value": "1.\tThe clarity of the writing could be improved by providing more details and explanations for certain aspects of the proposed method. See Question section."
            },
            "questions": {
                "value": "1.\tIn Definition 5 of DP fairness in RL, the difference inexpected returns is considered for the same state $s$ and different group variables $g_i, g_j$, while in Equation (6), the difference is considered for different state-group pairs $(s_i,g_i)$ and $(s_j,g_j)$. Could you elaborate on the relationship between these two equations?\n2.\tPlease clarify whether the proposed method is applicable to offline or online RL settings. In Algorithm 1, is the dataset $\\mathcal{D}$ collected through environment interaction or by using a bisimulator?\n3.\tCould you explain the role of $\\omega$ in Line 12 of Algorithm 1?\n4.\tThe authors could consider mentioning potential limitations of the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a fairness-aware RL approach to satisfy long-term group fairness by capturing behavioral similarities across different groups. The authors show that tuning the observable MDP by minimizing a policy-dependent Bisimulation metric automatically achieves demographic parity. This modified MDP is then leveraged to learn a fair policy using standard RL algorithms."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper studies the important problem of long-term fairness, and proposes a novel and somewhat straightforward approach to address it. The authors establish a connection between Bisimulation relation/metric and demographic parity, and exploit this connection for unconstrained optimization of fair policies. Empirical results show better or on par performance of the approach compared to state-of-the-arts. Moreover, the availability of the code supports the reproducibility of these findings (disclaimer: I have not checked the implementation)."
            },
            "weaknesses": {
                "value": "The proposed approach targets a specific class of problems and it is not clear, nor discussed, how it can be generalized/adapted to other problems and settings. The framework relies on a single measure of group fairness which limits its applicability to broader contexts. Particularly that recent literature suggest that individual fairness measures are better suited for fairness-aware learning compared to group-based metrics that are more compliant with statistical analysis. \n\nIn addition, it is implicitly assumed that the original MDP is known by the approach which may be a strong requirement (e.g., that R^original(s,a) is given or the transitions are available to sample rollouts). This assumption and how the approach extends to problems with unknown MDPs are not fully discussed in the paper.\n\nThe authors claim that modifying the observable MDP occurs outside the RL training loop, so that the problem stays stationary. However, (batch) updates of the dynamics model are still in the main loop, making the policy dependent on the changing MDP, hence, suggesting an overall non-stationary setup.\n\nWhile the implementation is provided in the supplementary materials, not all parts of the framework are clear. For instance, it would be helpful to discuss how the quantile matching works or whether the expectation in Eq. 7 applies to all possible state and groups or the same state in different groups? i.e., (s,g_i) and (s,g_j)? Also some information about the architecture/etc. would improve clarity.\n\nThe main advantage mentioned for modifying the observable MDP over constrained optimization is the ability to use existing RL algorithms. However, it\u2019s not entirely evident how this is a significant benefit, especially given the approach\u2019s limited flexibility in terms of fairness metrics.\n\nMinor:\nThere are a few references to metrics that are adapted from supervised learning, which implies that metrics are algorithm-dependent. Why this is important?\n\nTypo (line 131):  \\tau_a(s\u2019|s,g) should be \\tau_a(s\u2019|s,a,g).\n\nThe term co-optimization seems to be over-used, e.g., co-optimization of reward function in Sec. 4.1 or co-optimizing J_rew and J_dyn that are learned separately.\n\nline(281-282): the approach is solely based on a single fairness measure, so it is NOT regardless of fairness measure used."
            },
            "questions": {
                "value": "Is it possible to extend this approach to accommodate different notion of fairness, and if yes, how?\n\nIt seems that the Bisimulation metric is defined for discrete state spaces; could you explain if that's the case and how it can be generalized to continuous spaces?\n\nWould it be more intuitive to incorporate the sensitive attribute g as part of the state representation by redefining the state as  S x G, leaving the rest of the MDP and group-conditional definitions unchanged? Would this adjustment change the technical approach?\n\nHow the choice of baselines is justified if some of them optimize a different measure of fairness (like EO)? Will the comparisons be fair then?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel method for learning group-fair solutions in reinforcement learning (RL), addressing a limitation of standard deep RL approaches, which prioritize reward maximization over fairness considerations. The proposed method learns group-specific reward models and observation dynamics alongside the RL policy, using bisimulation metrics without modifying the core RL optimization process. Experiments in lending and college admission scenarios are presented that show the proposed method is comparable with considered baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper presents a clear and well-structured approach to incorporating group-level fairness in RL.\n\nThe connection drawn between group fairness and bisimulation is both novel and thorough.\n\nThe paper introduces a practical implementation of their framework, incorporating algorithmic innovations that enable efficient computation.\n\nThe experimental results effectively demonstrate the method's ability to learn group fair solutions in two domains."
            },
            "weaknesses": {
                "value": "Despite the paper's compelling core idea, the empirical evaluation appears somewhat limited in scope, with only two relatively simple domains and discrete action spaces. I believe, incorporating more complex scenarios with multiple groups with conflicting nature would greatly improve the paper.\n\nA detailed and thorough theoretical analysis is also lacking, particularly in terms of rigorously examining the properties of the proposed reward models and observation dynamics and their relationship to bisimulation metrics.\n\nThe paper's organization could also be improved, with a formal definition of long-term fairness and clearer connections between bisimulation and fairness metrics."
            },
            "questions": {
                "value": "1. Could you provide a clear definition of long-term fairness in the context of your work? This definition is missing and should have been defined formally. \n2. How well would the proposed method perform in environments with more groups and actions? What are the theoretical and practical limitations in scaling the approach, and how might these be addressed?\n3. Current results show DQN performing comparably or better than PPO, which contradicts common findings in the field. This pattern appears in both standard implementations and when combined with bisimulation. Could you explain this unexpected behavior, and is it possibly related to hyperparameter tuning?\n4. How does your approach compare to welfare-based RL methods[1-5]? Specifically, wouldn't egalitarian or lexicographic egalitarian welfare functions, which maximize the minimum group utility, achieve similar goals in reducing recall and credit gaps?\n5. Regarding the implementation details, how do you handle the optimization ordering between reward and observation dynamics models (assuming they're represented by neural networks)? Does the optimization order impact performance? Could you specify which gradient-free optimization method was employed?\n\n[1] Cousins, Cyrus, Kavosh Asadi, and Michael L. Littman. \"Fair E3: Efficient welfare-centric fair reinforcement learning.\" 5th Multidisciplinary Conference on Reinforcement Learning and Decision Making. RLDM. 2022.\n\n[2]Siddique, Umer, Paul Weng, and Matthieu Zimmer. \"Learning fair policies in multi-objective (deep) reinforcement learning with average and discounted rewards.\" International Conference on Machine Learning. PMLR, 2020.\n\n[3] Zimmer, Matthieu, et al. \"Learning fair policies in decentralized cooperative multi-agent reinforcement learning.\" International Conference on Machine Learning. PMLR, 2021.\n\n[4] Fan, Zimeng, et al. \"Welfare and fairness in multi-objective reinforcement learning.\" arXiv preprint arXiv:2212.01382 (2022).\n\n[5] Cousins, Cyrus, et al. \"On Welfare-Centric Fair Reinforcement Learning.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}