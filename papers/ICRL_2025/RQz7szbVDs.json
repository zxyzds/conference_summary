{
    "id": "RQz7szbVDs",
    "title": "A Theory of Initialisation's Impact on Specialisation",
    "abstract": "Prior work has demonstrated a consistent tendency in neural networks engaged in continual learning tasks, wherein intermediate task similarity results in the highest levels of catastrophic interference. This phenomenon is attributed to the network's tendency to reuse learned features across tasks. However, this explanation heavily relies on the premise that neuron specialisation occurs, i.e. the emergence of localised representations. Our investigation challenges the validity of this assumption.\nUsing theoretical frameworks for the analysis of neural networks, we show a strong dependence of specialisation on the initial condition.\nMore precisely, we show that weight imbalance and high weight entropy can favour specialised solutions.\nWe then apply these insights in the context of continual learning, first showing the emergence of a monotonic relation between task-similarity and forgetting in non-specialised networks, and, finally, assessing the implications on the commonly employed elastic weight consolidation regularisation technique.",
    "keywords": [
        "machine learning theory",
        "teacher student setup",
        "initialisation",
        "specialisation",
        "statitistical mechanics of learning"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We examine in theoretical frameworks how different initialisation schemes influence specialisation in neural networks and explore their impact on downstream tasks in settings such as continual learning.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=RQz7szbVDs",
    "pdf_link": "https://openreview.net/pdf?id=RQz7szbVDs",
    "comments": [
        {
            "summary": {
                "value": "- The authors introduce a theoretical framework that demonstrates how initialization controls whether networks develop specialized or shared representations\n- This work contains a rigorous mathematical analysis demonstrating that weight imbalance between layers drives specialization\n- This paper presents a novel analysis of how initialization affects continual learning methods, particularly demonstrating the dependency of EWC on specialization"
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This work takes a new perspective on the class catastrophic forgetting problem and provides theoretical justification for why initialization is important\n- I believe continual learning researchers will be interested in this approach and in building on this work"
            },
            "weaknesses": {
                "value": "- The primary weakness is the lack of empirical experiments. I would like to see different initialization methods compared on several simple continual learning or class incremental learning benchmarks. Overall, I think this work still has value as a theory paper."
            },
            "questions": {
                "value": "How would your initialization schemes scale to modern architectures with skip connections?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies how the initialization of a network can affect its ability to \u201cspecialize\u201d where representations are localized and disentangled. The paper discusses a simple student-teacher framework with extensions to disentangled attribute learning and continual learning. I think the paper looks solid but it is highly likely that I didn\u2019t understand the paper."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is informative on the theoretical setup and its implication in empirical experiments such as disentangled learning and continual learning.\n- The conclusion about network initialization is insightful.\n- The figures in the paper are nicely organized."
            },
            "weaknesses": {
                "value": "- Clarity: I found the paper not very clear. First it builds off from some prior knowledge on \u201cspecialization\u201d and its definition of specialization is \u201cone pathway finish learning (reach it\u2019s hitting time t \u2217) before the other begins learning (reaches it\u2019s escaping time t\u02c6)\u201d. However, \u201chitting\u201d and \u201cescaping\u201d are two new concepts without definition. And why does the leading time matter for specialization? Can\u2019t a MoE like architecture also specialize? In other words, the two neurons could simultaneously specialize into two different aspects of the input data, where the lead time of one neuron does not matter. Perhaps this is because I am very new to the literature, but without properly understanding the concept of specialization, I cannot have a confident judgment on the paper.\n- Although the paper is theoretical, the main paper does not have any formal theorem/claim to explain the relation between initialization and specialization.\n- The paper lacks a core claim. The first part attributed specialization to the \u201cimbalance\u201d in initialization (I also didn\u2019t understand why h-ww^T is imbalance). The second part focuses on how \u201cgain\u201d of initialization affects disentanglement. And lastly, in continual learning, initialization was mentioned but unclear which part of initialization can cause the failure of continual learning. Basically, it seems that the whole paper is talking about initialization but the actual mechanism is quite different in each part and lacks a unifying explanation."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates how weight initialization, specifically imbalance, affect the arising of specialization in the network representation and the subsequent consequences on forgetting wrt. task similarity."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality: Although specialization and continual learning have been widely studied in the context of modularity, this paper attempts to take a more thorough investigation from the perspective of weight imbalance at initialization which is interesting and an important direction. \n\nQuality: Figure 3&4 provide good insights on how imbalance leads to specialization/disentanglement both theoretically and experimentally. \n\nClarity: Notations are clear and maths are largely clear (except Eq. 9). \n\nSignificance: I think weight imbalance is an interesting and important angle for continual learning thus the paper has great potential."
            },
            "weaknesses": {
                "value": "Quality: 1) As mentioned by the authors, simplified setting is a limitation of the paper: all theoretical results are on single-hidden layer networks (linear for learning dynamic, nonlinear for student-teacher); All experiments including the ones on continual learning are on toy examples except Figure 4. 2) I find the overall main message unclear - is specialization good or bad for continual learning?  (see question) \n\t\t\nClarity:  The current flow of the paper can be problematic, it required significant back-and-forth for me to understand the messages (example see questions).  The manuscript seems incomplete, for example the caption for Figure 4.  \n\t\t\nSignificance: Apart from the other mentioned comments, one key aspect the paper could gain on its significance is to make more clear conclusions (see questions)."
            },
            "questions": {
                "value": "a. Conflicting results? - Figure 6 orange is no specialization and with lowest forgetting (good?) but Figure 7 trying to say no specialization is not good? Is specialization good or bad for continual learning? \n\nb. Can the authors give any explicit conclusion on weight imbalance and their impact on continual learning, apart from initialization has an effect? This seems to be the most important message of the paper but I can't find any statement in the text. Eq 9 gives the generalization error but was not explained.  \n\nc. Fig 6 is very confusing. Two factors: layer-wise imbalance (r) vs. pathway imbalance (theta). But they are both changing across scenarios; without controls, I don't see how conclusions can be drawn. \n\nd. Figure 6 blue - why is it 'specialization after the first task' since theta=pi/4 and according to Figure 5 that's yellow = no specialization? \n\ne. Unclear conclusion: For example, Sec 4.3 on continual learning, its conclusion only mentions the concerned factors without making clear their consequences on CL so standing more like an introduction than a conclusion \"In a broader context, a rich diversity of behaviours can emerge, driven by factors such as the initialisation schemes, the scale of weights in the first layer, and the readout heads for both tasks.\"\n\nf. Could the authors somehow materialize the concept of 'specialization' in Figure 2? I could vaguely see how two pathways of different lambda could lead to specialization as one simply wins the race but it was after much back-and-forth comprehension.  \n\ng. Figure 1b main message = activation function is subsidiary to weight imbalance? If so, it seems to be disconnected from the rest of the paper and seems like better belonging to the appendix than as a killer figure. \n\nh. Flow problem: Q and R in Figure 1 are not really introduced until in Sec 4.1 but even then they are only defined without explained (l352)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents an analysis of specialization in neural networks without nonlinearities, with focus on the determinants of specialization and its influence on forgetting.\n\nThe analytical methods of the paper fall outside my expertise hence I cannot give a meaningful evaluation of the paper as a whole as they make up the majority of the argument. The below ratings and comments are limited to a superficial evaluation of the main points and should not be regarded as reliable."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The analysis and discussion about the relevance of specialziation to forgetting, as well as the determinants of specialization, is an interesting area on inquiry.\nThe analysis is provided with a rigorous mathematical framework."
            },
            "weaknesses": {
                "value": "The analysis is performed on linear networks, while nonlinearity is the defining feature of NNs. Authors are clear about that and apparently it is not an unprecedented way to conduct a similar analysis, but this would render the conclusions of the analysis questionable nonetheless. I would suggest the authors to include a more detailed discussion regarding what sort of guarantees can be given when transferring conclusions obtained from linear networks to conventional, nonlinear ones."
            },
            "questions": {
                "value": "Please see weaknesses point above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 1
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}