{
    "id": "mPdmDYIQ7f",
    "title": "AgentSquare: Automatic LLM Agent Search in Modular Design Space",
    "abstract": "Recent advancements in Large Language Models (LLMs) have led to a rapid growth of agentic systems capable of handling a wide range of complex tasks. However, current research largely relies on manual, task-specific design, limiting their adaptability to novel tasks. In this paper, we introduce a new research problem: Modularized LLM Agent Search (MoLAS). We propose a modular design space that abstracts existing LLM agent designs into four fundamental modules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory. Building on this design space, we present a novel LLM agent search framework called AgentSquare, which introduces two core mechanisms, i.e., module evolution and recombination, to efficiently search for optimized LLM agents. To further accelerate the process, we design a performance predictor that uses in-context surrogate models to skip unpromising agent designs. Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Moreover, AgentSquare can generate interpretable design insights, enabling a deeper understanding of agentic architecture and its impact on task performance. We believe that the modular design space and AgentSquare search framework offer a platform for fully exploiting the potential of prior successful designs and consolidate the collective efforts of research community. Code repo is available at https://github.com/ICLR-10021/AgentSquare.",
    "keywords": [
        "LLM agent",
        "Modular design space",
        "Agent search",
        "AutoML"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We propose a modular design space of LLM agents and an automatic agent search framework named AgentSquare.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=mPdmDYIQ7f",
    "pdf_link": "https://openreview.net/pdf?id=mPdmDYIQ7f",
    "comments": [
        {
            "title": {
                "value": "Response to Reviewer Dm8t"
            },
            "comment": {
                "value": "**About breaking the double-blind review protocol:**\n\n \n\nIn the end, we must emphasize that **revealing personal identifiers, including the reviewer\u2019s name and professional title, severely violates of ICLR\u2019s double-blind review policy**. Reviewer Dm8t also chose to include a list of big titles, which is totally unnecessary even for reporting misconduct. It imposes unfair influence and pressure on other reviewers, ACs and public readers. Reviewer Dm8t also has clear conflict-of-interest of reviewing this paper.\n\n \n\nWe stand by the originality of our work, the integrity of our research timeline, and our adherence to academic norms. We will fight for our work\u2019s undeniable originality, and expect a fair and objective evaluation based on these facts.\n\n \n\nSincerely, \n\nAnonymous Authors"
            }
        },
        {
            "title": {
                "value": "Response to Reviewer Dm8t"
            },
            "comment": {
                "value": "**Response to the so-called \u201cevidence\u201d:**\n\n \n\n***Open-source code reuse:*** We must point out the exaggeration and misrepresentation in Reviewer Dm8t\u2019s claim *\"The prompts from ADAS are largely copied verbatim and then modified in a few places.\"* The fact is that we only refer to a very small fraction of prompts from ADAS's open-source codes of querying LLMs for code writing. Completely replacing these prompts would not impact our method\u2019s functionality entirely. Unlike the one-sided prompt comparisons you show, we conduct a comprehensive comparison of codes between the two works via JPlag (https://github.com/jplag/JPlag), an advanced source code overlap detector. The result is presented in https://drive.google.com/drive/folders/15-uYc3Yv11sCSy-oWxp9q0uB3fYC6SNd?usp=sharing, revealing that **the overall code similarity with ADAS is only 1.8%**. Reusing parts of open-source codes and prompts is a standard practice in the community and should not be used as evidence for idea plagiarism. Additionally, we also find part of prompts in ADAS has overlap and is modified from previous projects like DiscoPop (https://arxiv.org/abs/2406.08414, released on June 12), and the overall idea is very similar to existing code/prompt optimization works such as DiscoPop and OPRO, all following a \"meta-prompt -> LLM -> output code/prompt -> evaluation\" pattern. Below are some examples of prompt reuse of ADAS from DiscoPOP: \n\n \n\nExample 1 (the output instruction prompts):\n\n \n\nADAS: *\"**The first key should be (\"thought\"), and it should capture your thought process for designing the next function.** In the \"thought\" section, first reason about what should be the next interesting agent to try, then describe your reasoning and the overall concept behind the agent design, and finally detail the implementation steps. **The second key (\"name\") corresponds to the name of your next agent architecture**. **Finally, the last key (\"code\") corresponds to the exact** **\u201cforward()\u201d function in Python code that you would like to try.**\"*\n\nDiscoPOP: *\"When you respond, output a JSON where **the first key (\"thought\") corresponds to your thought process when designing the next function**. **The second key (\"name\") corresponds to the name of your next function.** **Finally, the last key (\"code\") corresponds to the exact python code that you would like to try.**\"*\n\n \n\nExample 2 (the same output format prompts: \"thought\", \"name\", \"code\"): \n\n \n\nADAS: {***\"thought\":** \"Insights:\\nYour insights on what should be the next interesting agent.\\nOverall Idea:\\nyour reasoning and the overall concept behind the agent design.\\nImplementation:\\ndescribe the implementation step by step.\",*\n\n  ***\"name\":** \"Name of your proposed agent\",*\n\n  ***\"code\":** \"\"\"def forward(self, taskInfo):*\n\n  *# Your code here*\n\n  *return answer*\n\n*\"\"\"*\n\n*}*\n\nDiscoPOP: *{**\"thought\":** \"Based on the previous outputs, I should try the direct preference optimization algorithm.\",*\n\n***\"name\":** \"dpo\",*\n\n***\"code\":** \"def sigmoid_loss(...)\"*\n\n*}*\n\n \n\nThese all suggest that ADAS is built on prior frameworks, however, the ADAS paper does not mention their idea comes from DiscoPop or refer to DiscoPop when introducing their design. Does this imply reusing open-source code/prompt is a common practice of the community?\n\n \n\n***About the doubt of proposing a new research area:*** Our key innovation is to propose a modular design space of LLM agent, focusing on the compositional structure of LLM agents, which sets it apart from the research problems of prompt/code searching like ADAS, OPRO and Promptbreeder. It is only natural to propose a search algorithm in this new design space as technical contribution. It is impossible for Reviewer Dm8t to claim copyright for proposing new research problem.\n\n \n\n***About \"MoLAS is only simplification of ADAS\":*** MoLAS searches in a smaller space compared to the entire universe of possible agents defined in python code. But we will argue it is also a more fruitful space since search algorithms can swiftly recombine successful classic design, and allow open-endedness to explore new modules. One analogy will be fully-connected MLPs can theoretically approximate any function, but more structured architectures like CNNs are more successful in CV. Anyway, the difference in search space should be considered as a sign of our original contribution.\n\n \n\n***About the plot style similarity:*** Plotting search trajectory is very common in any paper about search problem, which can also be found in many works on \"LLM for search\" [1] [2] and AutoML [3]. \n\n [1] Lu, C., et al. Discovering Preference Optimization Algorithms with and for Large Language Models. *arXiv preprint arXiv:2406.08414*.\n\n[2] Shojaee, P., et al. Llm-sr: Scientific equation discovery via programming with large language models. arXiv preprint arXiv:2404.18400.\n\n[3] Real, E., et al. Automl-zero: Evolving machine learning algorithms from scratch. In ICML 2020."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer Dm8t"
            },
            "comment": {
                "value": "We would like to defend our work's originality and make the following response to Reviewer Dm8t\u2019s false accusation:\n\n\n\n**Against the false accusation of idea plagiarism:**\n\nThe research idea of our project was proposed on April 27th. It is entirely original, and well before the release of ADAS (paper on August 15th, code on August 19th) Our key innovation is to propose a modular design space and a search algorithm within it. It allows researchers easily combine classic agent designs like assembling computer parts and discover new modules can be directly reused by others. It took us a long time to finish the project because the huge workload of abstracting all the classic agent designs into a standardized modular design space. ADAS is a code/prompt optimization framework that has no such modular feature. \n\n \n\nWe provide the following **Direct Evidence** to support our statement:\n\n1. **A video of a public presentation of our work on August 22nd.** It is broadcasted, recorded and timestamped by commercial remote meeting service, and stored in their cloud server since that day. \n2. **A seminar notification email sent on August 20th.** It includes an abstract that clearly states the modular search idea and experiment results.\n3. **Extensive raw communication logs extracted from commercial Instant Message application.** There are organized as a timeline, showing our communication groups is created on April 27th, discussion about modular design space on the first day, first mention of modular search on May 7th.\n\n \n\nThese materials can be accessed at https://drive.google.com/drive/folders/15-uYc3Yv11sCSy-oWxp9q0uB3fYC6SNd?usp=sharing. They are direct and decisive evidence that refute the false accusation of idea plagiarism. Our key idea is to propose a modular design space of LLM agents, which is inspired by the agent formula of Lilian Weng as described in Line 138-139 in our paper, rather than those code/prompt optimization works like ADAS, Promptbreeder, OPRO, etc. This evidence shows that our research idea was conceived and developed independently from ADAS, and any claims that AgentSquare was \"copied verbatim\" are entirely false.\n\n\n\nIn addition to above evidence, we encourage ACs and other reviewers to further read the papers of ADAS and AgentSquare. We know it is a big ask for your valuable time, but we believe you can reach fair assessment after reading both papers in first hand.\n\n\n\nNote: To keep the integrity of blind review at this stage, we provide anonymized screenshots of recorded video and communication logs. The link to original video in cloud server has been sent to ACs, which can be available with their permission and made public after the review process. We can also provide the raw communication logs proving how we form and implement our idea of modular agent design starting from April 2024, should further evidence be required."
            }
        },
        {
            "title": {
                "value": "Do the other reviewers agree this  a form of plagiarism and likely involves intentional academic misconduct (or at least suspect behavior)?"
            },
            "comment": {
                "value": "Do the other reviewers agree with my assessment that this paper is a form of plagiarism and likely involves intentional academic misconduct (or at least suspect behavior)? \n\nI am sorry to ask since I know everyone is busy, but since you have already read the paper I am hoping reading my review and offering your reaction is not too large a time burden. Given the stakes I think it is worthwhile (but of course I understand we are all juggling many demands)."
            }
        },
        {
            "summary": {
                "value": "The paper introduces AgentSquare, a framework designed to automatically optimize LLM agent architectures within a modular design space. It proposes a novel approach, termed Modularized LLM Agent Search (MoLAS), by defining a modular architecture divided into Planning, Reasoning, Tool Use, and Memory modules. AgentSquare employs module evolution and recombination mechanisms, along with a performance predictor, to explore and identify optimal combinations within the design space efficiently. The framework is evaluated on six benchmarks, showing a 17.2% performance improvement over existing hand-crafted agent designs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Modularity and Reusability: The modular allows the reuse and recombination of components, which aligns well with LLM advancements in modularization and scalability. \n\n- Effective Search Mechanism: The combination of module evolution, recombination, and performance prediction seems to be a robust optimization strategy. The proposed performance predictor effectively reduces evaluation costs, addressing practical limitations in real-world deployments of LLM agents.\n\n- Comprehensive Evaluation: Benchmarks across domains such as web applications, embodied AI, and gaming provide evidence of AgentSquare\u2019s efficacy and generalizability.\n\n- Interpretable Insights: The framework\u2019s ability to provide human-interpretable design insights is a useful addition, potentially helping in  aiding the design and tuning of future LLM-based agents."
            },
            "weaknesses": {
                "value": "My main issue with the paper is that while the modular approach is beneficial in the short-term, it may limit flexibility by enforcing predefined components. Extending the modular design to allow more dynamic, task-specific modules could enhance its applicability.\n\nAdditionally, the framework\u2019s reliance on LLM-driven suggestions for module evolution and recombination could inherit biases or inefficiencies from the LLM models themselves, potentially limiting the quality of novel configurations.\n\nMinor comment: \n- Many citations miss parenthesis around them. Try using \\citep instead."
            },
            "questions": {
                "value": "The authors write that: \u201cIn contrast, module-level searching methods including random and Bayesian search lack a clear and insightful search direction.\" Where do we see this? When looking at the figures, it seems that random search is a pretty competitive baseline?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a modular design space for LLM agents named \u201cModularized LLM Agent Search (MoLAS)\u201d and proposes an evolutionary framework named \u201cAgentSquare\u201d for optimizing and recombining the various modules (covering planning, reasoning, tool use and memory) within this design space. It further introduces a performance prediction model for ruling out unpromising candidate solutions, hence rendering the search process more efficient. Through comprehensive evaluations it is demonstrated that their autonomous, modular design of LLM agents significantly outperforms manual designs and other search algorithms, and offers interpretable insights that complement human knowledge. This paper thus contributes to more standardized and scalable development of agentic systems that exploit prior successful experience, minimizing the reliance on human intervention."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.The paper is well-organized and uses language that is easy to understand. \n\n2.The paper is well motivated, addressing an interesting research question. It innovatively consolidates existing (and potentially upcoming) LLM agent designs into a unified framework, and effectively leverages their successful experience for better design. \n\n3.The experiments are conducted with both quantitative evaluations (including task performances, API costs and search trajectories), and qualitative analyses such as the specific module combinations of optimized agents and insights drawn from the newly discovered modules. \n \n4.The authors have made their code repository available on GitHub, which ensures reproducibility and promotes future work."
            },
            "weaknesses": {
                "value": "1.There are some inconsistencies within the paper that might cause confusion. For example, Equation (2) and (3) indicate that both module recombination and evolution take past experience as input, which is not the case in Figure 3. Besides, the random initialization (as mentioned in experimental setup) seems to contradict the arguments made at the beginning of Section 3.3. \n\n2. It seems more fair to also take into account the API cost incurred by search when you compare the performance-cost trade-offs of AgentSquare and manual design. However, the paper did not mention this information. \n\n3.The experimental results appear to be obtained from a single run. The authors are suggested to carry out repeated experiments, and report averaged results and standard deviations to demonstrate the robustness of their approach."
            },
            "questions": {
                "value": "1.Are there any specific criteria for selecting the 16 LLM agents when constructing module pools? Besides, human labor involved in standardizing these modules might become prohibitive if one hopes to leverage a larger number of existing agents. Are there any measures to tackle this problem? \n\n2.Could you elaborate a bit more on the evaluation of candidate solutions? Specifically, it is currently unclear when solutions are evaluated by the performance predictor and when they are tested in the real task environment. \n\nThank you!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed a framework for automatically selecting and optimizing for off the shelf LLM-based methods from existing literature. It proposes a new concept:MoLAS to enhance agent performance through modular evolution and recombination. The design includes four fundamental modules: Planning, Reasoning, Tool Use, and Memory and uses an in-context performance predictor to efficiently evaluate designs, outperforming manually crafted agents across six benchmarks by an average of 17.2%.  In sum up, AgentSquare provides a platform for researchers to reuse and extend prior successful LLM agent designs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This method shows it's originality in using evolutionary and recombined mechanisms to automatically explore optimal combinations, effectively consolidating prior research efforts. It also provides empirical testing across diverse benchmarks, showing improvement over handcrafted and prior methods. The research extends to its broad applications and the potential to unify efforts within the LLM agent community, reducing reliance on task-specific human design part and enabling a more systematic exploration of agent architectures."
            },
            "weaknesses": {
                "value": "The paper lacks clarity regarding the definition of certain components of the method, particularly the performance evaluation function mentioned in Section 3.1. It is unclear whether this refers to the API cost introduced later or another evaluation metric. Additionally, the method shows limited novelty, as it primarily focuses on leveraging LLMs to recombine and select existing components rather than introducing a fundamentally new application or capability for LLMs. Although the proposed approach does achieve better performance than existing handcrafted methods, it does not address or eliminate the core limitations of prior works, given that it relies heavily on reusing previous designs."
            },
            "questions": {
                "value": "1. What is the evaluation and metric function to optimize the searching process ?\n2. How does it scales if extending to multi-agent problem?\n3. Is it able to solve manipulation tasks ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "I am disheartened to report I believe this paper involves a form of plagiarism and likely involves intentional academic misconduct (or at least suspect behavior). For full transparency, and because I am an author of the plagiarized work and thus have a stake beyond a normal reviewer, I am making my review public and non-anonymous. \n\nWe posted a paper called \u201cAutomatic Design of Agentic Systems\u201d (ADAS) on August 15th to arXiv and shared it on X and LinkedIn. It was submitted for review at ICLR 2025. \n\nThis review is of AgentSquare: Automatic LLM agent search in modular design space. It was posted to arXiv on Oct 8 2024. \n\nThis AgentSquare paper clearly takes many components from the ADAS paper, including the main ideas (slightly modified), repackages them (after adding a bit of new work), and presents the paper as a completely new set of ideas, all without acknowledging the significant degree to which it has lifted many key elements from our paper. It does cite ADAS, but only as an afterthought in the last paragraph before the conclusion, rather than writing throughout the paper (including in the motivation) that the ADAS paper is an extremely relevant piece of prior work and from which they copied (and slightly adapted) core pieces: the main ideas, unique writing choices, data visualizations, code, and prompts (which have significant chunks directly copied without modification). \n\nIt is hard to believe this is an honest mistake. Instead, the evidence causes me to conclude it is an attempt to get an ICLR publication by hoping the reviewers are not aware of how similar this paper is to a previous arXiv paper. Given how similar this new piece of work is, I would expect (and feel scholarship demands) this new paper to mention ADAS early and often, including discussing how it inspired their work and how the works differ, and to acknowledge that this new paper uses many pieces of the ADAS paper. I would also expect AgentSquare to compare to ADAS, since a central claim is that it is an improvement over the approach ADAS took. Instead, the paper is written in such a way that a busy reviewer/reader might not know such a similar work exists, and thus mistakenly attribute credit for the big ideas, results, and novelty to this new work.\n\nIf the authors are somehow so unaware of what is appropriate and required in academic writing that this is all an honest mistake, then in my view they should let the community know that, apologize, and entirely rewrite the paper to properly inform readers. \n\nNote: this is almost certainly not a case of the work being done concurrently, because the paper copied so many pieces from ADAS (including prompts used in their experiments). \n\nTo be clear, there are some nice new innovations in the paper (if the data are to be believed, see below). Properly written (and with more careful comparisons to ADAS and ablations of what makes this paper different and if they help), this paper could have been accepted as a nice innovation on top of ADAS. Instead, I believe it should be rejected as plagiarism. There is not enough time and rounds of review in this conference's review process for reviewers to consider a full rewrite (including back and forth) at this stage. \n\nHere is evidence of untoward behavior:\n\n1. The prompts from ADAS are largely copied verbatim and then modified in a few places. The amount of overlap makes it certain they copied from our work, yet they did not acknowledge doing so. They should report using a prompt from another paper. But more importantly, it shows how similar the work is to, and was inspired by, ADAS, and thus that ADAS should have been mentioned throughout (as prior work, something to be compared to, something they built on and were inspired by, etc.). You can see the similarities here: https://drive.google.com/file/d/1vHPW2EXvx7LjFv-kDhQHyb_VGHTnndD8/view?usp=sharing\n\n2. ADAS did something rare: it said it was (A) recognizing and naming \u201ca new research area\u201d named Automatic Design of Agentic Systems, and (B) introducing a new search algorithm within that area named Meta Agent Search. AgentSquare clones that template, saying \u201cwe introduce a new research problem: Modularized LLM Agent Search (MoLAS)\u201d and \u201cbuilding on this design space, we present a novel LLM agent search framework called AgentSquare.\u201d It is uncommon for papers to name a new research area explicitly, let alone also introduce an instance within it. \n\n3. Moreover, MoLAS is not a new research area. It is a subset of ADAS. It is a more constrained subset of ADAS, at least the way it is initially presented, which is that MoLAS is constrained to combine pre-existing, human-designed agent modules. However, in a contradiction that is confusing and harms novelty, later the paper also says the modules can be changed, removing this constraint, meaning that MoLAS is thus just ADAS with a new name?\n\n4. Even if ADAS is not identical, it clearly is a very related prior work, yet it is not mentioned in the introduction, nor anywhere in the paper until the second-to-last paragraph. Yet other, much less related work is mentioned (line 053), suggesting an intentional choice to deceive the reader/reviewer by hiding work that hurts the novelty claims of this paper. Also, some of the claims in the introduction say effectively that things like ADAS do not exist or are rare (line 047): clearly ADAS should be mentioned in that context, but isn\u2019t. \n\n5. The plot style of ADAS Fig. 3a is very similar to Fig. 4, providing further evidence of the central role the ADAS paper played in inspiring and catalyzing the AgentSquare paper. \n\n6. One (somewhat, see * below) new thing in AgentSquare vs. ADAS is an explicit surrogate model that predicts an agent\u2019s performance, saving compute vs. running a real evaluation. However, I am very skeptical of this data. Even with much training and years of research, getting surrogate models to be highly predictive (e.g. in Neural Architecture Search) is very difficult. I have a hard time believing an LLM is even good zero shot, yet these data say it is nearly PERFECT zero shot. That raises an important question: why are there so few (5) points shown per run (e.g. versus the many more shown in many more per run in Fig. 4)? How did you select these 5? Can you provide code that replicates this experiment, so the community can run it independently and verify it? I do not mean the overall search, which is expensive, but just the zero-shot predictions and the evaluations for all agents produced per experiment. I have never asked for such a thing in review before, but extraordinary claims require extraordinary verification, in my mind, especially given the cloud of suspicion raised by all the other issues in this review.  If the surrogate model is indeed this good, that is a major discovery and worthy of publication in some form, and I recommend you share it in a properly written paper. \n\nThe net effect of all of this evidence forced me to conclude the paper is at best deeply flawed due to many innocent mistakes, and at worst intentionally dishonest, with the latter seeming far more likely. \n\nIf there were no plagiarism issues, I would also point out:\n\n- There is no ablation of memory, one of the key new things they claim they added, and thus no evidence it helps\n\n-  The paper should cite https://arxiv.org/abs/2206.08896\n\n- *Surrogate models arguably implicitly exist within ADAS since it asks for proposals the model thinks will be high-performing, meaning it could use its predictions to guide proposals. However, we know from things like Chain of Thought and Reflexion that one can coax better performance by asking an LLM in the right way and/or asking it to reflect post-hoc on a creation. \n\n- The paper claims that ADAS\u2019 search in the space of code is an over-simplification. While it is true that describing the search space is simple, searching in it is MUCH more complex. I think it is much more appropriate to say that MoLAS is simplifying search by constraining it to a simple, small search space vs. ADAS, the latter of which allows any possible agent. \n\n- The paper claims that ADAS is proposed without consideration of existing agent architectures. I disagree, since one can easily inject them as seeds in the search archive (and we did consider that). But we prefer that to be a choice: it likely speeds up search, but also biases it. \n\nIt saddens me to make these accusations. However, I deeply believe in the integrity of the scientific process, including peer review. It relies on trust, and I think we need to stay vigilant and hold people accountable so we can maintain that trust as much as possible. \n\nJeff Clune\n\nProfessor, Computer Science\nUniversity of British Columbia\nJeffClune.com\n\nCanada CIFAR AI Chair & Faculty Member\nVector Institute"
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "See main review."
            },
            "weaknesses": {
                "value": "See main review."
            },
            "questions": {
                "value": "See main review."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "See main review. Also note that because I have am a co-author on the paper I believe is plagiarized, and thus have a conflict-of-interest, I have added my name to my review. I hope that is alright, as it seems more ethical to me than otherwise. Please let me know if you have concerns and we can discuss the best path forward."
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}