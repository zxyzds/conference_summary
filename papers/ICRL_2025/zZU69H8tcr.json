{
    "id": "zZU69H8tcr",
    "title": "SparsitySolver: Efficient Reinforcement Learning-based Pruning for LLMs",
    "abstract": "Large Language Models (LLMs) have achieved significant success in the field of Natural Language Processing (NLP). However, due to their large model size and high inference costs, the application of LLMs is restricted. Pruning is regarded as an effective method to reduce the size of LLMs. Mainstream pruning methods for LLMs typically apply a uniform ratio to prune all the layers or determine layerwise sparsity based on simple criteria. Such manually or semi-manually designed pruning strategies often lead to suboptimal results, which makes reinforcement learning a feasible solution. However, current reinforcement learning-based pruning methods usually have redundant environment designs or multiple agents, rendering them ill-suited to massive LLMs. Hence, we propose SparsitySolver, which first incorporates reinforcement learning into the pruning of LLMs, supporting various pruning granularity. SparsitySolver employs an improved reinforcement learning environment, allowing for a rapid pruning strategy search with a small-scale agent. Moreover, to lessen the performance decline caused by structured pruning, we propose a compensation method capable of restoring performance without introducing additional parameters to the model. We evaluate our approach on LLaMA-V1/V2, Mistral, and the OPT families across multiple pruning granularities, achieving performances surpassing the state-of-the-art methods.",
    "keywords": [
        "Large Language Models",
        "Model Compression"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "a reinforcement learning-based pruning technique for LLMs, strong performance across various pruning granularities",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zZU69H8tcr",
    "pdf_link": "https://openreview.net/pdf?id=zZU69H8tcr",
    "comments": [
        {
            "summary": {
                "value": "The authors introduce RL into LLM pruning. I feel the paper is tough to understand as the authors always give some reverse statements."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Introduce RL into LLM pruning."
            },
            "weaknesses": {
                "value": "Bad Writing (why reinforcement learning is a good choice? Why you say the problem of previous research and say your work is the first one.)\n\n\"Suchmanuallyorsemi-manuallydesignedprun\ningstrategiesoftenleadtosuboptimalresults,whichmakesreinforcementlearn\ningafeasiblesolution. However, current reinforcement learning-basedpruning\n methodsusuallyhaveredundantenvironmentdesignsormultipleagents, render\ningthemill-suitedtomassiveLLMs. Hence,weproposeSparsitySolver,which\n first incorporates reinforcement learningintothepruningofLLMs, supporting\n variouspruninggranularity.\""
            },
            "questions": {
                "value": "RL is known unstable. How to reverse the parameters that you have pruned before?\nDoes this method require training or just forward pruning?\nFor the reward compensation, will that increase reduce sparsity as the sparse item for different channels are different.\nWhat is the meaning of without need for additional computation?\n \"Weproposea simpleandefficient reinforcement learningenvironment, improving the\n sparserewardenvironmentinexistingRLpruningmethodswithouttheneedforadditional\n computation.\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the shortcomings of current pruning methods that overlook the inner sparsity differences between layers in large language models (LLMs). The motivation is clear and straightforward, and the design of SparsitySolver integrates reinforcement learning to facilitate an intuitive exploration of various pruning granularities. Additionally, the implementation of performance compensation through a linear combination in the final linear layer is both simple and effective. Experiments demonstrate its effectiveness across both general perplexity (PPL) and LLM benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The illustrations for this work are easy to understand, and the method's description is clear and straightforward. \n2. The approach demonstrates its effectiveness across multiple large language models (LLMs) of varying scales when compared to current state-of-the-art (SOTA) methods. Moreover, the pruned performance shows improvements in both general metrics, such as perplexity (PPL), and current LLM benchmarks.\n3. Additionally, the proposed search strategy highlights the diverse sparsity within LLMs across layers and reflects the overall sparsity distribution within these models, presenting an interesting observation for further research."
            },
            "weaknesses": {
                "value": "1. To gain a comprehensive understanding of a pruning technique, it is essential to evaluate its performance on complex tasks such as machine translation and other capabilities of large language models (LLMs), including in-context learning (few-shot). \n2. One pressing application of pruning techniques is to reduce the operational costs of very large-scale language models, such as LLaMA2-70B and PaLM-540B. However, this paper does not provide support for these models."
            },
            "questions": {
                "value": "1. Quantization is a direct approach to reducing storage costs and inference time. Is this method compatible with existing quantization techniques?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents SparsitySolver, a reinforcement learning-based approach for pruning Large Language Models (LLMs). The method consists of two main technical components: a reinforcement learning framework that searches for optimal pruning strategies, and a reconstruction compensation method for recovering performance in structured pruning without introducing additional parameters. The authors propose a simplified reinforcement learning environment design where the state is represented by the total pruning ratio and the action determines layer-wise sparsity. They also introduce a parameter-free reconstruction compensation method that aims to restore the performance of structured-pruned models through linear combinations of preserved channels. The approach is evaluated on various LLM architectures (OPT, LLaMA-V1/V2, Mistral) across different model scales and pruning granularities (both structured and unstructured), with experiments on language modeling perplexity and zero-shot tasks. The method claims to achieve competitive performance compared to state-of-the-art pruning approaches while maintaining efficiency in the pruning strategy search process."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper proposes a novel direction by incorporating reinforcement learning into LLM pruning strategy search. The idea of using RL to automate pruning strategy discovery shows some originality.\n2. The reconstruction compensation method for structured pruning is interesting, as it aims to restore performance without introducing additional parameters, which could be practically valuable.\n3. The method demonstrates some level of versatility by supporting both structured and unstructured pruning across different model scales."
            },
            "weaknesses": {
                "value": "1. **Limited Technical Novelty and Theoretical Foundation:**\n   1.1. The motivation for using RL in pruning strategy search is insufficiently justified. The paper fails to establish why RL is particularly suitable for this task compared to other approaches.\n   1.2. The RL environment design appears overly simplistic. Using only the total pruning ratio as the state space lacks theoretical justification and seems naive.\n   1.3. The derivation of the reconstruction compensation method lacks mathematical rigor. Many assumptions and steps are not properly justified or explained.\n\n2. **Significant Experimental Deficiencies:**\n  2.1. The experimental evaluation lacks comprehensive comparisons with other RL-based pruning methods in the literature.\n  2.2. No statistical significance analysis is provided for the reported results.\n  2.3. The zero-shot evaluation is superficial and fails to demonstrate the method's advantages conclusively.\n  2.4. The ablation studies are incomplete and fail to validate the necessity of each component.\n\n3. **Poor Paper Presentation:**\n  3.1. The overall organization is chaotic with inconsistent paragraph spacing and formatting.\n  3.2. The figures, especially Figure 1, are of low quality and fail to effectively illustrate the proposed method.\n  3.3. Mathematical notations and equations lack proper explanations and context.\n  3.4. Citations are inconsistently formatted and poorly integrated into the text.\n\n4. **Insufficient Technical Details:**\n 4.1. Critical implementation details of the RL agent architecture and training process are missing.\n 4.2. The practical aspects of the reconstruction compensation method are unclear.\n 4.3. The computational overhead and deployment considerations are not adequately discussed."
            },
            "questions": {
                "value": "1. **Methodology:**\n  1.1. What is the justification for such a simplified RL state space design? Have you considered incorporating model structural information into the state representation?\n  1.2. How does the reconstruction compensation method handle different types of network layers? What guarantees its effectiveness?\n\n2. **Experiments:**\n  2.1. Can you provide detailed comparative experiments with existing RL-based pruning methods?\n  2.2. What are the results on larger-scale models (>70B parameters)?\n  2.3. Have you conducted stability analysis across different downstream tasks?\n\n3. **Practical Implementation:**\n 3.1. What is the end-to-end training time and computational resource requirement?\n 3.2. How do you balance the search cost versus performance improvement in practical deployments?\n 3.3. Are there any architectural limitations to your method's applicability?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper adapts the RL-based pruning framework for LLMs which has been previously shown to be effective in CNNs but performs poorly when applied to LLMs. It achieves it by densifying the sparse reward of the previous environment for the RL algorithm to efficiently utilize. Additionally, it introduces Compensation Through Reconstruction to alleviate the effect of the structured pruning. Lastly, experiments are conducted on popular LLMs like LLama V1-2 and OPT and results are competitive with other popular pruning methods."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Paper is well written and easy to follow\n* Its novelty is redesigning the reward function, which shows it significantly impacts performance, unlike the previous approach.\n* Proposed methodology supports both structured and unstructured pruning\n* It does not introduce additional parameters for structured pruning\n* Results are strong in terms of both perplexity and downstream task performance."
            },
            "weaknesses": {
                "value": "* The main issue with this paper is its evaluation setup and the lack of comprehensive experiments compared to previous pruning strategies.\n\n* The reason why its evaluation setup is biased is because of the following: unlike previous pruning strategies, this paper defines a proxy reward in terms of perplexity which at first glance makes quite a sense. However, perplexity does not always translate to better performance(i.e. FLAP vs wanda-sp from FLAP paper). Nevertheless, perplexity is still a valid metric for many papers but not for this one because pruning is determined by minimizing the perplexity of a certain dataset which is wikitext eval in this case.  Unsurprisingly, this strategy performs quite good perplexity but I think this leads to reward hacking (Skalse et al., 2022). OPT-6.7B is a good example of this phenomenon where the pruned model has better perplexity than the dense. To settle all these potential issues, I think downstream evaluation should be prioritized because of the sensitivity of the perplexity. However, throughout the paper perplexity is the main metric whereas downstream evaluation is only performed for LLama-V1 7B with only 20 percent sparsity for structured and 70 percent for unstructured pruning. Thus, downstream evaluation should be performed for different models, model sizes, and sparsity ratios. Lastly, the perplexity of a different dataset also can be reported instead of training one.i.e.  C4, PTB. \n\n\n* Moreover, some results are missing. For example, OWL with SparseGPT has a better perplexity score than the proposed methodology at 70 percent sparsity and is also as good as at downstream evaluation(SparseGPT: 48.03 vs 48.11 of the paper), which shows that the proposed methodology is not SOTA. One more note: even though I appreciate the reproduction, there are some discrepancies between reproduced scores and reported metrics from the original paper, i.e. BESA.  So I believe it is a good idea to include one from the paper and in addition your reproduction results.\n\n\nReferences\nSkalse, J., Howe, N.H., Krasheninnikov, D., & Krueger, D. (2022). Defining and Characterizing Reward Hacking. ArXiv, abs/2209.13085."
            },
            "questions": {
                "value": "Q1) What is the overall runtime against other pruning strategies?\n\nQ2) In Figure 1 reward is defined as $1/ppl$ whereas it is $10/ppl$ in the reward function which one is the correct one? If it is $10/ppl$, why do you scale the reward, does it provide any benefits?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}