{
    "id": "M4RhGr2lAy",
    "title": "Fairness-Aware Graph Learning: A Benchmark",
    "abstract": "Fairness-aware graph learning has gained increasing attention in recent years. Nevertheless, there lacks a comprehensive benchmark to evaluate and compare different fairness-aware graph learning methods, which blocks practitioners from choosing appropriate ones for broader real-world applications. In this paper, we present an extensive benchmark on ten representative fairness-aware graph learning methods. Specifically, we design a systematic evaluation protocol and conduct experiments on seven real-world datasets to evaluate these methods from multiple perspectives, including group fairness, individual fairness, the balance between different fairness criteria, and computational efficiency. Our in-depth analysis reveals key insights into the strengths and limitations of existing methods. Additionally, we provide practical guidance for applying fairness-aware graph learning methods in applications. To the best of our knowledge, this work serves as an initial step towards comprehensively understanding representative fairness-aware graph learning methods to facilitate future advancements in this area.",
    "keywords": [
        "Graph Learning Algorithms",
        "Algorithmic Fairness",
        "Performance Benchmark"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "This paper presents a comprehensive benchmark for the representative fairness-aware graph learning algorithms, which provides in-depth analysis and practical guidance to facilitate their broader applications and future advancements.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=M4RhGr2lAy",
    "pdf_link": "https://openreview.net/pdf?id=M4RhGr2lAy",
    "comments": [
        {
            "summary": {
                "value": "This paper provided a comprehensive protocol to evaluate the performance of fairness-aware graph learning methods. Extensive experiments on seven real-world attributed graph datasets with ten fairness-aware graph learning methods were conducted to induce in-depth analysis for the benchmark results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper provided a comprehensive benchmark over ten very recent fairness-aware graph learning methods, including both group fairness and individual fairness. A clear timeline and categorization were provided to help the presentation of the whole study.\n2. Extensive experiments on seven datasets and ten methods were conducted and were well organized to answer the four research questions. Various metrics in utility, group fairness and individual fairness were compared.\n3. In Section 4, the results in the tables were further partially compared in figures to discuss the limitations and strengths of different methods, which validated the findings of the authors.\n4. Practical guidance was provided for users to help choose the most appropriate fairness-aware graph learning methods."
            },
            "weaknesses": {
                "value": "1. The discussion mostly focused on stating the observations that different methods have different performances on different metrics. However, a more in-depth discussion of why these methods have different advantages could be added. E.g., how do different objective functions in the methods benefit certain fairness notions?\n2. For a benchmark study of fair graph learning, how do you choose the datasets for testing? What is the group density in the graphs? Are the groups balanced or not? These details could be clarified to justify the diverse choice of datasets for evaluation.\n3. What are the connections and the comparison within different group fairness metrics, within different individual fairness metrics, and between them? Can you provide more explanations in Section 4.3?\n4. Although the authors studied efficiency in Section 4.4, it is not well discussed as a factor in their practical guidance.\n\nIn all, despite the extensive experiments, more in-depth analysis could be added instead of simply presenting the results. Please also see the minor weaknesses in the questions."
            },
            "questions": {
                "value": "1. I did not find the newly constructed datasets AMiner-S and AMinder-L in the supplemental materials, which I think may count for part of the contributions of this work. Am I missing something?\n2. In lines 334-335 you mentioned \"neither DeepWalk nor GNN yields top-ranked performance under utility\". Where does this statement come from? DeepWalk achieves an even worse ranking in utility than fairness.\n3. In Figure 3, what do dots of different colors stand for?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work addresses the pressing issue of fairness in graph learning by providing a comprehensive evaluation of ten fairness-aware graph learning methods. The authors propose an experimental setup to evaluate these methods across seven real-world datasets, focusing on group fairness, individual fairness, the balance between fairness and utility, and computational efficiency. Their systematic approach aims to offer guidance to practitioners for selecting appropriate fairness-aware methods in real-world applications."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Originality: Although the motivation and the idea to do this kind of study have been attempted in the past, as pointed out by the authors, they also make a clear distinction of how their approach is different.\n\n2. Quality: The empirical evaluation is extensive, spanning several datasets and including multiple fairness and utility metrics. Common checkmarks for benchmark works like hyperparameter tuning, reporting across multiple runs, etc., are taken care of.\n\n3. Clarity: Key objectives and methods are introduced clearly, and the premise is set clearly without any confusion on the objective of the work. The work is very easy to read and follow, and the figures and the metrics reported are well explained.\n\n4. Significance: Fairness in machine learning on graphs is a growing area of importance, and this benchmark offers a much-needed reference point for comparing fairness-aware graph methods. While there are more definitions of fairness in this domain, this work covers the most widely used settings."
            },
            "weaknesses": {
                "value": "W1. The paper implies that this benchmark is a comprehensive evaluation of fairness-aware methods, yet it evaluates only a subset of fairness criteria. There are prior works that cover more inherent fairness issues in GNNs that do not depend upon the node characteristics [1] [2], and the evaluation of such methods would have also been a great addition. \n\nW2. While the benchmark includes a selection of ten methods, the paper does not sufficiently discuss recent advancements in fairness-aware graph learning, such as methods that address intersectional biases or temporal fairness. This limits the relevance of the benchmark for contemporary applications and makes it appear somewhat outdated in the rapidly evolving fairness landscape.\n\nW3. Experimental details are insufficiently described. Important aspects such as hyperparameter search ranges, validation criteria, and dataset preprocessing steps are either missing or vaguely specified. While Figure 3 conveys some parts of it, the exact ranges specified and the final hyperparameters used would be helpful for reproducibility.\n\nW4. The benchmarking focuses mainly on fairness-aware graph neural networks (GNNs) and shallow embeddings but lacks methods that might employ fairness-enhancing architectures or hybrid models. Expanding this range would make the benchmark more relevant and impactful.\n\nW5. The benchmark briefly touches on computational efficiency but does not thoroughly investigate the scalability of these fairness-aware methods on large-scale graph datasets. Given that scalability is a crucial consideration for real-world applications, particularly in social networks or knowledge graphs, this is a notable omission.\n\nW6. While the findings validate existing assumptions in the field, they don't offer novel insights beyond what practitioners would already understand through experience.\n\n[1] Liu, Zemin, Trung-Kien Nguyen, and Yuan Fang. \"On generalized degree fairness in graph neural networks.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 4. 2023.\n\n[2] Arun, Arvindh, et al. \"CAFIN: Centrality Aware Fairness Inducing IN-Processing for Unsupervised Representation Learning on Graphs.\" ECAI 2023."
            },
            "questions": {
                "value": "Q1. Can the authors clarify the hyperparameter tuning process? Specifically, were grid search ranges adjusted for each dataset, and how were optimal settings chosen? What were the search ranges, and how were they decided upon?\n\nQ2. The dataset selection includes only node classification tasks. Can the authors comment on the applicability of this benchmark for link prediction or other graph-based tasks?\n\nQ3. More realistic scenarios that often have hybrid fairness objectives have not been tested; what would be the performance of these methods in those settings?\n\nQ4. How does the benchmark account for potential variations in fairness definitions across different domains, especially when the impact of bias may differ (e.g., financial vs. social networks)?\n\nQ5. It would be interesting to see the GNN methods running without (or with random) node attributes since the claim that the \"absence of bias brought by node attributes\" is mentioned in two places in the text without any supporting evidence.\n\nQ6. The confidence intervals in Figure 2 overlap a lot, which is why I'm unsure about the statistical validity of the claims. Can the authors verify the significance of the results presented?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors aim to establish a benchmark for fairness-aware graph learning to guide practical applications by evaluating the trade-off between fairness and performance of multiple graph learning methods. To this end, the authors propose a set of evaluation protocols covering group fairness and individual fairness metrics, summarize and test the fairness and performance of ten representative fairness graph learning methods on seven real-world datasets, provide systematic experimental results and analyses demonstrating their trade-offs between fairness and utility, as well as constructing a reliable and quantitative basis for a benchmark for fairness graph learning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "\u25cf\tThe authors address an interesting and timely topic that is absolutely relevant to ICLR.\n\n\u25cf\tThe authors tested these methods on seven real-world datasets, including datasets from different domains such as social networks and finance."
            },
            "weaknesses": {
                "value": "\u25cf\tThe novelty of this paper is limited, as the systematic evaluation approach proposed does not significantly improve upon existing methods. The authors primarily use visual comparisons, i.e., plotting accuracy and fairness or presenting them separately in tables or bar charts. This approach is still unable to effectively illustrate the trade-off between performance and fairness for fair GNN.\n\u25cf\tThe comparison of fair graph methods are some of the early works. The authors omitting newer approaches such as Graphair [1] and FairSAD [2], which should be included for a more comprehensive comparison.\n[1] Ling, Hongyi, et al. \"Learning fair graph representations via automated data augmentations.\" International Conference on Learning Representations (ICLR). 2023.\n[2] Zhu, Yuchang, et al. \"Fair Graph Representation Learning via Sensitive Attribute Disentanglement.\" Proceedings of the ACM on Web Conference 2024. 2024.\n\u25cf\tThe discussion of experimental results lacks depth and context. The authors mostly describe the numerical scores shown in the result table without interpreting their significance. For example, in RO2, three individual fairness metrics are used to assess model fairness, yet the authors do not discuss the rationale for choosing multiple metrics, the potential conflicts among them, or how to interpret whether the metric values indicate good or poor performance."
            },
            "questions": {
                "value": "please refer the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors compare equity approaches up to 2022 through four dimensions: group fairness, individual fairness, fairness-performance trade-offs, and performance-efficiency trade-offs. They provide a more comprehensive comparison of the methods and an extensive overview of the approaches available prior to 2022."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors successfully reproduced the pre-2022 methods and conducted experiments using the corresponding performance indicators."
            },
            "weaknesses": {
                "value": "Defining the Balance Between Fairness Guidelines: The paper should clarify how the balance between different fairness guidelines is defined. If the balance is understood as a trade-off between fairness and performance, it is important to explain why existing related works are not utilized as baselines in the experiments [1], [2].\n\nUnique Evaluation Metrics: The paper needs to specify whether it proposes a unique evaluation rubric for assessing fairness. If it relies solely on original metrics, it should highlight how this approach differs from current research that employs multiple baseline comparisons.\n\nComprehensiveness and Novelty of Fairness Perception Methods: The proposed fairness perception graph learning methods must be evaluated for their comprehensiveness and novelty to determine their potential contributions to the field. Notably, the article only includes methods up to 2022 and lacks discussion of methods from 2023 and 2024 [3], [4], [5].\n\nSignificance of Comparing Fairness Types: The significance of comparing individual and group fairness across different approaches should be addressed, especially in light of the established trade-off between these two types of fairness.\n\nInterpretation of Figure 4: The paper should provide guidance on how to interpret the blurred trade-off represented in Figure 4. It should also discuss the insights this figure provides regarding the balance between different fairness metrics.\n\nInterpretation of Figure 5: As fairness-aware graph learning methods, the metrics used for the baselines in Figure 5 should include fairness metrics. The absence of these metrics needs to be justified.\n\n[1] Yuchang Zhu, et al. Fair Graph Representation Learning via Sensitive Attribute Disentanglement. ACM Web Conference 2024, 2024.\n\n[2]  Renqiang Luo, et al. FUGNN: Harmonizing Fairness and Utility in Graph Neural Networks. KDD, 2024.\n\n[3] Kamesh Munagala and Govind S. Sankar. Individual Fairness in Graph Decomposition. ICML, 2024.\n\n[4] Chen Yang, et al. FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization. AAAI, 2024.\n\n[5] Zhimeng Jiang, et al. Chasing Fairness in Graphs: A GNN Architecture Perspective. AAAI, 2024."
            },
            "questions": {
                "value": "How do the methods developed after 2023 perform on these benchmarks?\n\nThere has been substantial research on the trade-off between fairness and performance; why has this not been considered in the current study?\n\nWhat is the contribution of comparing efficiency? When evaluating efficiency, why is performance the only metric taken into account?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents benchmarking results for fairness corrections to graph learning algorithms. They investigate multiple aspects of 10 baseline methods on node classification tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is very well-written and well-organized. The paper tackles a useful problem, benchmarking fairness approaches for graph learning algorithms, which to my knowledge has not been done."
            },
            "weaknesses": {
                "value": "There are some key weaknesses:\n\n1. The paper only covers node classification, limiting its impact and informativeness.\n\n2. The paper claims to \"design a systematic evaluation protocol\", but this amounts only to running 10 baselines on standard fairness datasets for graph algorithms. As best I can tell, there is not a novel architecture/system design that the benchmarking experiments depend on.\n\n3. There are some confusions about the results that I point out in my questions."
            },
            "questions": {
                "value": "Small typo in RQ1: $\\Delta_{\\text{Utility}}$ should not be called a fairness metric.\n\nRQ3 is written somewhat confusingly -- its not clear what this RQ is investigating. I also don't know what \"Utility\" is in Figure 4. Can this be elaborated?\n\nFinding 1 (the answer to RQ1) reads \"This verifies the natural advantage of GNNs in achieving both accurate and fair predictions owing to their superior fitting ability\", suggesting that GNNs best balance group fairness and accuracy. But Finding 3 (the answer to RQ3) suggests DeepWalk-based methods balance all three metrics the best. Combined with the fact that \"Utility\" from Finding 3 does not seem to be AUC-ROC, this makes these two findings hard to reconcile and compare. How can a practitioner make decisions based on these findings?\n\nL482: This part should be specific that GNNs only exceed at balancing utility & *group* fairness, not overall fairness.\n\nL483: Where do you show that \"FairGNN maintains better trade-off between all three fairness metrics\"?\n\nS5 should have a disclaimer that these recommendations only hold for node classification."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}