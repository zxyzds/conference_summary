{
    "id": "6lB5qtdYAg",
    "title": "ProFI-Painter: Text-Guided Prompt-Faithful Image Inpainting with Diffusion Models",
    "abstract": "Recent progress in text-guided image inpainting, based on the unprecedented success of text-to-image diffusion models, has led to exceptionally realistic and visually plausible results.\nHowever, there is still significant potential for improvement in current text-to-image inpainting models, particularly in better aligning the inpainted area with user prompts.\nTherefore, we introduce $\\textit{ProFI-Painter}$, a $\\textbf{training-free}$ approach that $\\textbf{accurately follows prompts}$.\nTo this end, we design the $\\textit{Prompt-Aware Introverted Attention (PAIntA)}$ layer enhancing self-attention scores by prompt information resulting in better text aligned generations.\nTo further improve the prompt coherence we introduce the $\\textit{Reweighting Attention Score Guidance (RASG)}$ mechanism seamlessly integrating a post-hoc sampling strategy into the general form of DDIM to prevent out-of-distribution latent shifts.\nOur experiments demonstrate that ProFI-Painter surpasses existing state-of-the-art approaches quantitatively and qualitatively across multiple metrics and a user study. \nCode will be made public.",
    "keywords": [
        "text-guided image inpainting",
        "diffusion models",
        "high-resolution image inpainting"
    ],
    "primary_area": "generative models",
    "TLDR": "We present a text-guided image inpainting method which improves prompt alignment, prevents image quality loss and enables high-resolution inpainting",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=6lB5qtdYAg",
    "pdf_link": "https://openreview.net/pdf?id=6lB5qtdYAg",
    "comments": [
        {
            "summary": {
                "value": "The authors introduced Prompt-Aware Introverted Attention (PAIntA) block without any training or fine-tuning requirements, enhancing the self-attention scores according to the given textual condition aiming to decrease the impact of non-prompt-relevant information. They also proposed Reweighting Attention Score Guidance (RASG), a post-hoc mechanism seamlessly integrating the gradient component in the general form of DDIM process. This allows to simultaneously guide the sampling towards more prompt-aligned latents and keep them in their trained domain."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The writing is clear to understand with detailed formulations and figures.\n\n* The results are superior when compared to other methods.\n\n* The phenomena of Appendix B are very interesting, revealing that the original model maintains a similar visual pattern from other parts of images and the PAIntA would increase the probability to respond to the prompts."
            },
            "weaknesses": {
                "value": "Some questions below\n\n* Would this method be easily adapted to some modern models, for example, SDXL, SD3, or even FLUX?\n\n* The successful rate of one case with sufficient sampling of different seeds is not clear.\n\n* The experiments are conducted in cases with few instances, so if there are multiple instances (>5), what is the performance?\n\n* Could the method deal with the inpainting tasks with multiple masks in one inference?"
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a training-free approach to enhancing prompt-guided image inpainting with diffusion models. It proposes two key components: Prompt-Aware Introverted Attention (PAIntA) and Reweighting Attention Score Guidance (RASG), which improve alignment with text prompts. PAIntA adjusts self-attention layers to prioritize text-related regions, while RASG refines cross-attention scores for better prompt consistency. A specialized super-resolution technique ensures high-quality image scaling. Quantitative and qualitative results on MSCOCO confirm the method\u2019s superiority."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The discussion about prompt neglect is promising. \n\nThe proposed solution achieves strong results on evaluation metrics."
            },
            "weaknesses": {
                "value": "Some discussion and analysis should be included, see the question part."
            },
            "questions": {
                "value": "The inference time cost should be reported and compared.\n\nHow to derive Claim 1? How to define high-quality images?\n\nWill the proposed method work on transformer-based models like SD3 and FLUX?\n\nIn Table 2, the bolded aesthetic score is not the best one."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Paper addresses the problem of prompt neglect in text-guided image inpainting. Existing solutions (smartbrush, imagen editor) are argued to have reduced generation quality. The proposed method is training-free. The paper proposes to techniques, Prompt-aware Introverted Attention layer, and Reweighting Attention Score guidance for accurate prompt following and high image quality."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- the hypothesis that the problems of existing methods is based because of the self-attention, and that this problem can be addressed there, is interesting and convincing (even though it is not well explained).\n- the qualitative results clearly show the superiority of the proposed method. Also the quantitative results are ok, but they do not seem to indicate the large improvement seen in the qualitative results."
            },
            "weaknesses": {
                "value": "---\n- the self-attention map analysis (in Appendix B) is important for the motivation of the paper and should be moved to main paper. This would help to provide a motivation in section 3.3. before just stating it in math, helping the reader understand the proposed method. \n\n- the explanation of the main idea behind section 3.3 is not well presented. The main idea is the introduction of c_j in the self attention, but what this represent is not well explained in words. \n\n- does the 'introverted' nature make it hard to use information from outside of the impainted region. For example if you ask for an object with a whole behind which the background should continue ? Or a bike in front of a fence, etc. \n\n- section 3.4 should also start out by stating the problem it addresses and how it is planning to address this. I found the presentation not good of this section, and very hard to understand. \n\n- in the user study the results of DreamShaper are better than SmartBrush, but in figure 5 the results of DreamShaper are very bad not following the prompt at all, and SmartBrush is much better. Any explanation, this makes me doubt the correctness/usefullness of the user study. \n\nMINOR points: \n\nThink would be better to directly put equation of c_j also in (5), and then explain. Try to first explain the main idea, then the details (SOT, EOT, clipping etc). Now the main idea is hard to distill. \n\nMore usage of \\citep might make reading easier (e.g. line 96).\n\nToo many forward references in introduction (to future tables and figures and appendices).\n\nReferences for relevant information to appendix are out of place in the introduction. The main information should be in the main paper; the introduction introduces the most relevant information of the paper.\n\nline 248. Maybe better to keep professional factual style (instead of diary-style 'we did this' 'than that'), so better 'a thorough analysis of Stable Inpainting led to the conclusion...'"
            },
            "questions": {
                "value": "Overall I found the visual results appealing. The quantitative improvement less so (maybe a new metric should be developed to better show the superiority of the method ?). I found the presentation of the crucial section 3.3-3.4 of bad quality and they need to be improved much. \n\n- see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates the diffusion based image inpainting task. The proposed method is training-free, which modifies the self attention block and use post training alignment/guidance. The experimental results prove the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The method is intuitively clear and reasonable. The self attention may contain irrelevant information regarding area to be inpainted. The modification to self attention block makes sense. The visualization of attention map proves the efficiency of the method.\n- Although a bit heuristic, the training-free nature makes the method easily extensible to other pretrained inpainting diffusion models.\n- The proposed RASG is cleverly simple yet effective, which transforms the post training alignment to the form of non-deterministic DDIM. It seems to efficiently avoid noisy latent deviating too far from the original trajectory."
            },
            "weaknesses": {
                "value": "- Lack of experiments on running time. What is the additional time cost associated with the proposed method?\n- Lack of ablation study on hyperparameters. How sensitive is the model to different hyperparameters? Espscially $\\eta$."
            },
            "questions": {
                "value": "Please check the weakness.\n\n(Minor comment) In line 183, it should be VQGAN or VAE within SD, where usually VAE is chosen."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}