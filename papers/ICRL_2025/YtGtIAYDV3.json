{
    "id": "YtGtIAYDV3",
    "title": "Node-based Multiple Graph Learning with Theoretical Guarantees",
    "abstract": "In many applications, inferring graph topology, i.e., learning the graph structure from a given set of nodal observations, is a significant task. Existing approaches are mostly limited to learning a single graph assuming that the observed data are homogeneous. In many applications, data sets are heterogeneous and involve multiple related graphs, i.e., multiview graphs.  Recent work on learning multiview graphs ensures the similarity of learned view graphs through edge-based similarity between the graphs. In this paper, we take a node-based approach instead of assuming that similarities and differences between networks are driven by individual edges, providing a more intuitive interpretation of network differences. Moreover, unlike existing methods that employ Gaussian Graphical Models (GGM), which learn precision matrices rather than the actual graph structures, we characterize the graph using a Laplacian matrix. Thus, the approach is expected to work broadly beyond Gaussian graphical learning. We develop an optimization framework to learn the individual graphical structures, assuming that the differences are due to individual nodes that are perturbed across views. The proposed optimization framework is presented for the special case of two views. Furthermore, we derive the upper bound on the estimation error of the proposed graph estimator and characterize the impact of the sample size, number of nodes, and the spectrum of the graph Laplacians on estimation errors. The approach is evaluated on synthetic graph data for robustness against noise, graph density, and sample size. Finally, the proposed framework is applied to two-view real-world graph data for graph learning and clustering.",
    "keywords": [
        "graph learning",
        "multiple graphs",
        "perturbation",
        "graph signal smoothness"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "This paper introduces a node-based framework for learning two closely related graphs from observed node data.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=YtGtIAYDV3",
    "pdf_link": "https://openreview.net/pdf?id=YtGtIAYDV3",
    "comments": [
        {
            "summary": {
                "value": "This paper furthers the area of  learning graph structure from multiple views, a problem important in Graph Signal Processing(GSP). It proposes a formulation(4.1) for learning the Laplacian for each view. The formulation is solved through ADMM and guarantees are given."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Following are some of the strengths of the paper\n- It is rigorous in the formulation and the algorithmic guarantees it provide\n- It provides Node based learning, in contrast to edge based learning which is the state of the art\n- it is easy to follow."
            },
            "weaknesses": {
                "value": "The paper appears to have several shortcomings.\n- Contributions seem to be incremental. \n-Assumptions look a little restrictive.\n- It is restricted only to two views and not multiple views\n- Insights from Analysis is missing\nFew suggestions are as follows\n-A more comprehensive discussion contrasting the formulations(4.1 and 4.3) with existing formulations and highlighting the relative advantages will be useful.\n-Is it possible to quote a result from literature which shows how standard the assumptions are and what are the corresponding guarantees.\n-If the formulation is specialised to two-views then the paper should be renamed as such. \n-To understand the importance of the derived results it would be good to add key insights. For example if \nd more than n^2 then the learning is good."
            },
            "questions": {
                "value": "The following clarifications will help in addressing some of the weakness\n\n- Formulation 4.1\n\n   - What is the need for adding sparsity? It requires $\\gamma_2$. If it is not added how will the formulation and analysis look like.\n\n   - is it the case that the formulation is restricted to two views because you need $L_1-L_2 = V + V^\\top$\n    \n- Assumptions \n  - Can they be contrasted more with state of the art. For example if the signals are gaussian, what is the result already known and how does it compare with the subgaussian result discussed here . This will help explain the power of the results obtained. Good to quote results from Gaussian Graphical models to show the importance.\n\n- Analysis\n  - Provide some insights from the analysis. Can we say that if $ d \\in \\Omega(n^2)$ only then the learning will be good(the first term suggests so). If this is indeed the case does the experimental result suggest that\n  - Are there any regimes for the hyper parameters for which the learning is good. Just saying they are positive may not yield comprehensive understanding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper is concerned with the problem of inferring multiple related\ngraphs from nodal observations. In a graph signal processing (GSP)\nframework, these are considered as graph signals, which are\nincorporated in their optimization program via a smoothness penalty\nwhen inferring the graph structure.\n\nThe novelty of this work comes primarily from the use of a mixed-norm\npenalty in the two-view graph learning problem, which draws\ninspiration from the RCON penalty of Mohan et al. (2014). The\ndifference in this work, however, is the general smoothness assumption\non the graph signals, as opposed to the particular case of a gaussian\ngraphical model. Some guarantees are given (Theorem 5.1), as well as\nempirical evidence for the utility of the proposed routine."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper provides a straightforward approach to a well-defined\nproblem, with reasonable algorithms and empirical evidence to back up\nthe utility of the proposed approach. I can imagine the proposed\nmethod being useful to practitioners, particularly if they find the\nresults of optimization programs based on a gaussian graphical model\nassumption to have subpar outputs."
            },
            "weaknesses": {
                "value": "The novelty of this work is limited -- it seems to be a combination of\npenalties used in the GSP and GGM literature, brought together for a\nnew special case. Although I see the utility of using the RCON penalty\noutside of the context of GGMs, the intellectual contribution of this\npaper is incremental, not offering much more insight than another tool\nin the toolbox. I do not want to downplay the utility of such\ncontributions, but it is nonetheless a weakness of this work.\n\nI also have a major concern about the theoretical result in Theorem\n5.1, and its relation to Assumptions A1-A3. It is not clear how the\nsignals relate to the graphs at all under these assumptions. Implicit\nin the penalty is an assumption that the signals are smooth, but I\ndon't see this clearly stated anywhere in the assumptions. Based on my\nreading of the assumptions, I could very well apply the result to\nsignals support in a \"high-pass\" band of the spectrum of the graphs\nand not expect much difference in the recovery performance. I am not\nsure if I am right here, but the results of Theorem 5.1 don't seem to\ndescribe much in this regard, given that they do not (seem to!) rely\non the relationship between the graphs and the signals. This is\nconcerning, as the whole point of the paper is to infer graph\nstructure from node signals that are assumed to have some relationship\nto the graph structure.\n\nOf course, under the reasonable assumption that the signals are smooth\nwith respect to the graphs that they are supported on, it makes sense\nthat the proposed algorithm works -- which is the setting of all of\nthe generating graph filters described in Section 6.1.1."
            },
            "questions": {
                "value": "Please clarify how the results of Theorem 5.1 depend on the graph signal structure, i.e., on being low-pass or something to that effect -- otherwise, please clarify the utility of this theorem if the result does not depend on that property."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a graph signal processing method to learn the graph Laplacians of two graphs under the assumption that graph signals are smooth and that variations between the two views arise from perturbations in node connectivity. The authors explicitly design a model employing an ADMM solver and provide a theoretical analysis of the error bound. A notable result is that increasing the sample size alone does not guarantee convergence to the true values. Instead, convergence also depends critically on the topology of the underlying true graph structure."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Compared to existing techniques that focus on edge-similarity based graph learning, this paper investigates node perturbation.\n\n2. This paper characterizes the graph via Laplacian matrix, rather than following previous Gaussian Graphical Models.\n\n3. This paper utilizes ADMM optimization, which is interpretable and provides the upper bound on the estimation error."
            },
            "weaknesses": {
                "value": "1. While the paper provides interesting insights, it appears to overclaim its contributions. The focus is on node perturbation in multi-view graph learning; however, the method merely offers solution for two-view cases. Such a claim lacks rigor in the context of research.\n\n2. The components of the proposed model in Equation 4.1 seem to be a combination of existing techniques. For instance, the first term quantifies the variation of graph signals (backbone), while the second term can be simplified to $\\|\\|-\\mathbf{W}^{(k)}\\|\\|_{F}^{2}$ (notably, $\\mathbf{L} = \\mathbf{D} - \\mathbf{W}$), which regulates the sparsity of the affinity graph through the parameter $\\gamma_1$. The third term introduces a penalty to avoid zero values in the elements of the degree matrix using $\\log(\\cdot)$ (Kalofolias et al., 2017), and the fourth term resembles techniques from RCON (Mohan et al., 2014). Consequently, the formulation appears to be less novel and more a synthesis of existing methods.\n\n3. The paper does not provide any analysis of computational complexity. In fact, the method is with high computational and space complexities, which limits its applicability in large-scale scenarios."
            },
            "questions": {
                "value": "In addition to my concerns in Weaknesses, I have the following comments:\n\n1. The authors should provide a careful summary of their contributions, particularly since the model appears to be a combination of existing techniques. Furthermore, the ADMM solver seems straightforward and lacks novel insights. While the derived error bound appears solid, I question how this bound will guide future research. Is there any difference between this error bound and previous research?\n\n2. Could you generalize your model into multi-view version? It seems that the current paper lacks the completeness necessary to substantiate claims of addressing node perturbation in multi-view graph learning.\n\n3. The baseline method, MVGL, supports multiple views, whereas the proposed model is limited to handling only two. What advantages does your approach offer over MVGL?\n\n4. Do you have any recommendations for initializing the ADMM solver? Have you identified any initialization methods that perform better than others in your experiments? Additionally, I suggest including the convergence curves."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}