{
    "id": "PTjKXwrVCT",
    "title": "Forecasting Needles in a Time Series Haystack",
    "abstract": "Shocks and sudden spikes are common characteristics of real-world time series data. For example, demand surges or electricity outages often occur in time series data, manifesting as spikes (\u201cNeedles\u201d) added to the regular time series (\u201cHaystack\u201d). Despite their importance, it is surprising to find their absence in the benchmarking protocol at the frontier of time series research\u2014Time Series Foundation Models (TSFMs). To address this gap, we present the Needle-in-a-Time-Series-Haystack (NiTH) Benchmark, which includes both synthetic and real-world spiky time series data from diverse domains like traffic, energy, and biomedical systems. For synthetic data, we develop a flexible framework using Poisson-based modeling to generate spiky time series, allowing us to evaluate forecast models under various conditions. To accurately assess model performance, we introduce a new metric based on Dynamic Time Warping, specifically designed for spiky data. We evaluate the zero-shot forecasting capabilities of 6 popular TSFMs over 64 million observations, identifying their limitations related to architecture, tokenization, and loss functions. Furthermore, we demonstrate that the incorporation of the proposed NiTH dataset, due to its diversity compared to the common pre-training corpus of TSFMs, results in improved performance.",
    "keywords": [
        "Time Series Forecasting",
        "Zero-Shot Forecasting",
        "Time Series Benchmark"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We propose a benchmark for time series foundation model zero-shot forecasting performance on spiky time series.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=PTjKXwrVCT",
    "pdf_link": "https://openreview.net/pdf?id=PTjKXwrVCT",
    "comments": [
        {
            "summary": {
                "value": "The authors study the problem of forecasting spiky time series. They proposed a dataset called Needles in the Time Series Haystack (NITH), which they used to evaluate multiple times series foundation models in zero-shot settings. They also propose a scalable way to generate spiky time series, and a novel metric to evaluate models on spiky time series. They found that most time series foundation models cannot model spiky time series. The authors also evaluate how design choices of foundation models affect their performance on spiky data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper studies an important problem. Forecasting or predicting spiky data is indeed understudied. \n2. The authors conduct a **lot** of experiments-- with multiple times series foundation models, and their design choices\n3. The author propose a scalable way to generate spiky time series, and evaluate models on forecasting spiky data"
            },
            "weaknesses": {
                "value": "1. **Clarity**: The paper introduces a lot of \"novel\" things, and mathematical instruments to model and evaluate time series models on spiky data. There's a lot of notation which is hard to follow ($\\alpha, \\beta, \\theta, \\omega$, to name a few) as sometimes symbols apply prior to their usage. For example, it is particularly hard to follow 219 -- 225, where it's unclear what $\\alpha$ and $\\beta$ are until it is introduced in Algorithm 1.  \n2. **Too many \"novel\" things, but their :** The authors propose too many new things, that it is hard to keep track. Moreover, it seems that the authors miss a lot of basic things and instead over-complicate their proposed solutions. For example, [1] proposes a simple way to inject spikes in time series which was not considered. For the \"haystack\", the authors use Gaussian Processes, when simpler time series can be generated using polynomial or linear trends, periodicity (sine waves), and some noise (Gaussian or Random walks). Similarly, there has been a lot of work in time series anomaly detection on better metrics to evaluate time series of similar natures, for example [2]. Moreover, simpler metrics such as $\\alpha^{T}(Y - \\hat{Y})$, where $\\alpha$ is a vector which gives more weight to needles rather than the haystack. \n3. **Baselines and related work:** The authors only consider time series foundation models. They do not consider any statistical models such as ARIMA, or models which are designed to predict spiky time series. Moreover, the authors do not consider any prior work on modeling or evaluating spiky time series, e.g. [3, 4, 5, 6]. Also, the authors continually pre-train pretty large models on NITH, perhaps smaller models can be fine-tuned more effectively on spiky time series.\n4. **Normalization:** All time series models including foundation models use some form of normalization (e.g. Chronos uses mean scaling while TimesFM uses RevIN), and I believe that this influences the forecasting performance. This is perhaps the most component of the design space of these models.  \n5. **Lags and lack of connection to a real-world application:** The authors mention (random) lag multiple times in the paper, but it is unclear to me why is this an important consideration for spiky time series? I feel this is also where connection to a real-world problem (e.g. energy load prediction) would be helpful. For example, in these cases it may be more important for to predict whether there will be spikes, or the number of spikes and the amplitude of the spike, each of which is a different problem (classification or regression), and can be evaluated with different metrics.   \n6. **Loss function:** Most TSFMs are trained to predict some notion of conditional mean (MSE) or median (MAE) of a time series. These loss functions are not appropriate for training models that can be expected to do well on spiky data, thus the fact that TSFMs don't perform too well is not surprising. On the other hand, I feel that the authors could have spent more time on this important aspect.  \n7. **Memorization:** The authors claim that the benchmark tests a model's ability to memorize spikes. Could the authors provide more information and evidence on why this is the case? \n\n## Minor\n1. In line 380, I am not sure what \"struffle\" means\n2. I think the results in Figure 7 may not be comparable because the encoder-only, decoder-only and encoder-decoders models have different number of parameters, so it is unclear if the differences are due to the difference in # of parameters or due to the inductive biases of these models. \n3. It is not immediately clear to me what is a good value of the scaled DTW.\n4. In line 316, I suspect that the authors have mistakenly annotated the wrong predicted time series (they have interchanged Pred 2 and 3).\n\n\n### References\n1. Goswami, M., Challu, C. I., Callot, L., Minorics, L., & Kan, A. Unsupervised Model Selection for Time Series Anomaly Detection. In The Eleventh International Conference on Learning Representations.\n2. Paparrizos, J., Boniol, P., Palpanas, T., Tsay, R. S., Elmore, A., & Franklin, M. J. (2022). Volume under the surface: a new accuracy evaluation measure for time-series anomaly detection. Proceedings of the VLDB Endowment, 15(11), 2774-2787.\n3. Fadlallah, B., Chen, B., Keil, A., & Pr\u00edncipe, J. (2013). Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information. Physical Review E\u2014Statistical, Nonlinear, and Soft Matter Physics, 87(2), 022911.\n4. L\u00f3pez, Manuel Zamudio, Hamidreza Zareipour, and Mike Quashie. \"Forecasting the Occurrence of Electricity Price Spikes: A Statistical-Economic Investigation Study.\" Forecasting 6.1 (2024): 115.\n5. Fatone, L., Mariani, F., Recchioni, M. C., & Zirilli, F. (2012). The analysis of real data using a stochastic dynamical system able to model spiky prices.\n6. Zhang, X., Jin, X., Gopalswamy, K., Gupta, G., Park, Y., Shi, X., ... & Wang, Y. (2022). First de-trend then attend: Rethinking attention for time-series forecasting. arXiv preprint arXiv:2212.08151."
            },
            "questions": {
                "value": "1. Is DTW or it's scaled version as you have proposed differentiable? If not, how are models trained using these as a loss function?  \n2. What loss function is used to continually pre-train Chronos and TimesFM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The manuscript investigates the performance of learners capable of predicting spikes in time series. \nAlthough shocks and sudden spikes are common characteristics in real-world time series (according to the authors), their main motivation is the absence of a benchmarking protocol.\nTo address this issue, the authors collected previously published real-world time series, created synthetic time series with those characteristics, tested some time series foundation models, and presented an extension version of DTW as a new measure."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The most important contribution is the organization of a vast amount of time series published in the literature. \nThis type of manuscript has great potential to be cited and is important for guiding new researchers to find more appropriate time series. \nAnother relevant aspect presented by the authors is the execution of foundational time series models, a hot topic nowadays, as benchmarks for their datasets. \nI believe these experimental results are relevant for positioning the current research status as a foundation for future contributions. In summary, this suggests that new studies should, in some way, improve upon the presented results. \nFinally, another notable strength is the volume of experiments conducted by the authors."
            },
            "weaknesses": {
                "value": "The major weakness, in my opinion, is the difficulty in clearly understanding the manuscript's goals by reading the title and abstract. \nWhen the authors say that shocks and sudden spikes are common characteristics in real-world time series, I suppose there is substantial data available to address this problem. So, from the real-world perspective, is the authors' main contribution just the compilation of those data? \nOn the other hand, the authors also proposed a function to create \"spiky\" time series. It is an interesting proposal, but their focus is essentially on modifying the stochastic component. It is not clear whether it is also possible to modify the deterministic part of the time series, thereby varying the general behavior of the time series. \nMoreover, I understand that foundational time series models are currently prominent, making it natural to use them to attract readers' attention. However, it is unclear why classical approaches were not considered in their investigation. \nAnother important point is the authors' choice to select MAE, MSE, and DTW as evaluation \"metrics.\" Why were these selected? I understand they are important when comparing predicted observations, but the application presented by the authors involves not only predicting observations, but also predicting spikes. There are more appropriate \"metrics\" for this task that are not influenced by non-spike predictions."
            },
            "questions": {
                "value": "1 - I recommend revising all definitions and terms. For example, DTW cannot be considered a metric since it does not satisfy the triangle inequality. How does this affect the analysis? And how does it impact the proposed \"metric\"?\n\n2 -The authors opted to use MAE, MSE, and DTW to evaluate their experiments. However, these may not be the most appropriate metrics. Given the focus on spikes, it\u2019s also crucial to assess the timing of spike predictions. When comparing predictors, it is important to evaluate their ability to predict spikes accurately and the time deviation from the expected occurrence. Metrics commonly used for concept drift or change point detection might be more suitable. Why not consider more consistent evaluators?\n\n3 -In Section 3.1.1, the authors mention that \"haystack\" is the base time series without spikes. Does this \"base\" refer to a stationary process, or could it represent any kind of time series behavior?\n\n4 - When the authors state that \"No TSFMs could accurately forecast needles across all settings,\" is this a result of their research? Is there a citation supporting this claim?\n\n5 - The authors mention, \"We apply our filtering algorithm.\" Was this algorithm proposed in this work? Where is it described, and how was it evaluated?\n\n6 - Was \"D_T\" defined on Page 4?\n\n7 - Several equations lack clarity, such as \"\\tau,\" \"\\hat{\\tau},\" and U(3,5).\n\n8 - Figure 2 includes \"pred1,\" which is not discussed in the text.\n\n9 - Does Figure 3 show results for all time series?\n\n10 - Figure 3 is not visually easy to read, especially with text closely surrounding it.\n\n11 - The impact of adding noise to the synthetic time series should be discussed more thoroughly, particularly regarding the signal-to-noise ratio.\n\n12 - The discussion on results often includes phrases like \"We believe...,\" which implies some uncertainty. This phrasing gives the impression that the authors may not be entirely confident in their conclusions. Based on the results, I share the authors\u2019 sense of uncertainty, as the conclusions do not seem fully substantiated. Consequently, the claim in the introduction about context length remains unconvincing.\n\n13 - When creating synthetic time series, is it possible to alter the function used for generating the deterministic part of the series, independent of spike generation?\n\n14 - In the appendices, the authors included a section on time series statistics. While it contains several plots, the statistical information does not appear substantial."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors tackle the challenge of predicting rare spikes in time series data. To help improve forecasting for these \u201cneedle\u201d events, the authors introduce a benchmark called NITH with both synthetic and real-world datasets. They test several prominent time series foundation models and find that they generally struggle with these sharp, rare events. To improve evaluation, they propose a new metric called Scaled Dynamic Time Warping (SDTW) tailored to these spikes. Additionally, they show that pre-training models on synthetic data created to mimic real-world spiky patterns can boost model performance in this setting."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1. The introduction of SDTW is a helpful contribution which addresses issues with existing metrics for measuring performance on the specific tasks of spiky time series forecasting. \n\nS2. The new benchmarks (NITH-Synth, NITH-Real) capture an important subset of time-series forecasting problems, and provide a new way of evaluating and probing existing foundation models. Construction of useful benchmarks is an important way to continue to make progress on these models."
            },
            "weaknesses": {
                "value": "W1. Very strong constraints are applied to the time series setting (>8 spikes, >512 segments, etc) in order to establish the synthetic spike benchmarks. It is not clear from this work how helpful these constraints are when applying the benchmark to other real-world settings and under what settings they could be relaxed. Better establishing the importance of the \u2018spike\u2019 setting could improve this weakness.\n\nW2. The NITH-Real dataset seems likely to have some overlap with the foundation model training sets, particularly through its use of the UCR Anomaly and NAB benchmarks. Separating out the results by benchmark and analyzing the overlap where possible could help reduce this risk. \n\nW3. The continuous pre-training approach can lead to loss of generalization. Could the authors evaluate how the models trained on the NITH-Synth benchmark fare on other standard forecasting benchmarks? There are many existing methods for improving continual pre-training which can be applied in this setting as well to mitigate this performance difference if it exists."
            },
            "questions": {
                "value": "Q1. A significant problem with previous metrics seems to be the equal weighting of timesteps when compared to the practitioners interest in only those anomalous or spiky time-steps which follow a different frequency and magnitude. SDTW requires the ground truth needle locations as a result. Is there some way to remove this requirement, perhaps by applying a standard filter to the time-series so that MSE or DTW can be used instead?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The Time Series Foundation Model (TSFM) has been a debated topic in recent years. Proper evaluation and selection of the most suitable model are also desired. The paper presents a benchmark that includes both synthetic and real data from various domains, which fills the gap in the absence of benchmarking. The paper introduces a Possion-based modeling method for generating spiky time series for synthetic data, which enables the evaluation to proceed in various conditions. The paper uses a Dynamic Time Warping based metric to define the model performance, and extensive experiments are conducted to examine the zero-shot forecasting performance of six well-established TSFMs on spiky time series."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper introduces a theoretical generation scheme for the spiky time series generation. The spiky time series generation follows the models of Markov chain transitioning between haystack signals that are sampled from the Gaussian process, while the needle signals are generated from a separate Gaussian process, along with outlier and rarity constraints. The paper is well-organized and has good clarity except for some minor issues in the figures.  The dataset in the benchmark is rich and full of diversity, so I believe it is useful for the evaluation of TSFMs."
            },
            "weaknesses": {
                "value": "Though the benchmark is only designed for time series forecasting, and the model performance is only evaluated in terms of the zero-shot forecasting capabilities, evaluation of other tasks might be required, e.g., classification.  \n\nAlso, it would be interesting to discuss other stochastic processes for the time series generation and include an additional experiment for the ablation study. \n\nIt is hard to read in some figures in the paper, e.g., Figure 4, Figure 6-8 and Figure 14-35; I believe the fontsize should be increased."
            },
            "questions": {
                "value": "1. why the haystack distribution is set as the scaled Poisson distribution, and are there any advantages to using the scaled variant?\n2. why the Gaussian process was chosen rather than other stochastic process, needs to clarify\n3. what's the advantages of the method over other related generation methods [1][2], it seems like the needles signals can be instead \ngenerated with other schemes together with the method [1].   \n\n\nReferences:\n[1] https://arxiv.org/abs/2311.01388\n[2 ] https://arxiv.org/pdf/2403.03698"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}