{
    "id": "5wxCQDtbMo",
    "title": "Rethinking Efficient 3D Equivariant Graph Neural Networks",
    "abstract": "Understanding complex three-dimensional (3D) structures of graphs is essential for accurately modeling various properties, yet many existing approaches struggle to fully capture the intricate spatial relationships and symmetries inherent in such systems, especially in large-scale, dynamic molecular datasets. These methods often face trade-offs between expressiveness and computational efficiency, limiting their scalability. To address this gap, we propose a novel Geometric Tensor Network (GotenNet) that precisely models the geometric intricacies of 3D graphs while ensuring strict equivariance under the Euclidean group E(3). Our approach directly tackles the expressiveness-efficiency trade-off by leveraging effective geometric tensor representations without relying on irreducible representations or Clebsch-Gordan transforms, thereby reducing computational overhead. We introduce a unified structural embedding, incorporating geometry-aware tensor attention and hierarchical tensor refinement, allowing for flexible and efficient representations for various tasks. The proposed model consistently outperforms state-of-the-art methods in both scalar and higher-order tensor predictions, demonstrating exceptional robustness across diverse datasets, and establishes GotenNet as a versatile and scalable framework for 3D equivariant Graph Neural Networks.",
    "keywords": [
        "graph neural networks",
        "computational physics",
        "3D graphs"
    ],
    "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=5wxCQDtbMo",
    "pdf_link": "https://openreview.net/pdf?id=5wxCQDtbMo",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces GotenNet, a Geometric Tensor Network, that advances molecular representation learning by addressing computational challenges in 3D graph neural networks. GotenNet's novel tensor embedding strategy eliminates the need for complex irreducible representations and Clebsch-Gordan transformations, enhancing computational efficiency while preserving model expressiveness. Its Geometry-Aware Tensor Attention mechanism enables refined edge representations, capturing complex geometric relationships for better molecular property predictions. Additionally, the Hierarchical Tensor Refinement approach allows the model to adapt across scales, accommodating both broad patterns and detailed molecular features. Evaluated on benchmark datasets such as QM9 and Molecule3D, GotenNet consistently outperforms existing methods, demonstrating robustness and scalability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper presents a novel framework, GotenNet, which innovatively combines geometric tensor representations with advanced attention mechanisms. This approach addresses the expressiveness-efficiency trade-off in 3D graph modeling, a challenge that has been inadequately tackled in prior works.\n2. The paper is well-structured and clearly articulates the problem being addressed, the proposed solutions, and the significance of the findings."
            },
            "weaknesses": {
                "value": "1. **Comparison of computational complexity with previous methods**: Could you provide details on the model size, as well as training and inference times, in comparison with existing methods? This information would help highlight GotenNet\u2019s computational efficiency relative to other models in the field."
            },
            "questions": {
                "value": "Please see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed a new network archtiecture that is SE(3) equivariant and builds on Transformers and previous works on equivariant Transformers. The results on various datasets show good performance and speedup."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The experimental results are great and intensive."
            },
            "weaknesses": {
                "value": "1. The writing of the paper should be greatly improved. Please see \"Questions\" below.\n2. The paper does not discuss why the proposed architecture differs from other works and why this is better in terms of accuracy and efficiency even though the results are seemingly good. Also a reference implementation would be great to make sure the better results are reproducible.\n3. The proposed architecture should be simplified (partially because of the way it is presented).\n4. The comparison is somewhat unfair. For example, on QM9, this work trains the proposed model for 1,000 epochs while some previous works only train for 300 epochs. Besides, the batch size is 32 on QM9 (some previous works are 128), and smaller batch sizes on QM9 can sometimes lead to better results."
            },
            "questions": {
                "value": "> Writing\n\n1. I think the title is too general. We can always rethink something and make them more efficient. Be very specific to your proposed method.\n2. Line 16 -- 18: Mention what is the difference from previous works clearly.\n3. Line 22 -- 23: Mention the datasets you tested so that readers can know the scale of experiments in this work.\n4. Figure 1: I think the x and y axes are similar. Also mention which direction is better.\n5. Line 62: What is \"this\" in \"Some recent works have sought to address this by...\"? Make sure the sentence is clear.\n6. Line 113: Equivariant networks model translational \"invariance\".\n7. Line 159: \"Effective Representations\" -> too general. Be specific to what makes it effective.\n8. Figure 2: Please double checkt the errors in the caption.\n9. Line 195 -- 215: Better to link to existing literature since they are similar to (or basically the same as) vector spaces of irreps.\n10. Line 220 -- 221: The notation of h, r, t has no meaning and should be replaced by other variables that directly reflect what they are presenting.\n11. Line 263: $z$ denotes the \"one-hot encoding\" of the atomic number.\n12. Section 3.3: This is unclear. It would be better to first mention the high-level concept and then go to the details. Also state the differences from previous works.\n13. Line 323 or Equation 8: This is essentially an SO(3) linear layer. It would be great to link to previous works.\n14. Line 373: Geoformer Wang et al. -> make sure the format of citation is correct.\n15. Figure 3: Mention the dataset you tested.\n16. Line 423: \"The\" proposed method\n17. Line 448: I don't get the definition of Scalability in the paper. Should be better to use \"Evaluation on Efficiency\"?\n\n> Question \n\n1. Line 66 -- 67: How do you define scalability? I think that means you invest more compute, you always get better results. Efficiency should be the correct term to use if you are saying some models are slow or take much memory.\n2. Table 1: Somewhat hestitated to trust the results here. From the method section, it is unclear to me what are the differences that can improve the results. Besides, it would be great to discuss some potential oversmoothing, which prevents deeper networks from performing better.\n3. It would be good to report training time for completeness.\n4. Have you conducted any experiments on materials/catalyst datasets? I think OC20 IS2RE dataset would be good to test given the compute requirement is not that high and the train/val/test splits are well-defined."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The importance of equivariance in neural networks has become ubiquitous\nin machine learning research. Neural networks operating on input features\nwith certain symmetries, such as rotations, translation and permutation\npredict with higher certainty and better quality if the architecture respect\nthese symmetries. In Graph Neural Networks equivariance with respect to\npermutation of the nodes has been the foundation upon which a plethora\nof ML approaches have been developed. The authors present a deepening\nof this paradigm, by combining the success of transformers, permutation\ninvariance and E(3) equivariance and distilling these three components into\nan architecture for molecular learning. The main contribution of the paper\nis the development of Geometry Aware Tensor Attention layer that defines\nan E(3) equivariant transformer layer for both node and edge embeddings. A\nstrong highlighted advantage is computational speed, compared to architectures\nusing Chlebsch-Gordan decompositions or architectures based on irreducible\nrepresentation.\n\nThrough a thorough set of experiments, the authors show that their\narchitecture, GotenNet, is able to outperform a wide variety of benchmarks on\nmolecule property predictions. Their overall performance and scaling in terms\nof computations is also shown."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper presents a rather strong experimental section showing that the\nauthors have taken the time to set up a good set of experiments. In particular\nthe fact that all comparison partners have been retrained and compared to\nthree versions of the model shows dedication to the and has been a pleasure\nto read. The experiments addressing the inference times is an important\ncomplementary addition, since often there is a rather steep tradeoff between\npracticality and usability, where better performance on metrics is achieved\nthrough exponential increases in parameter count or long inference times. To\nsee both good results and good inference time / scalability is reassuring to\nthe reviewer. The introduction of geometric tensors as a replacement of CG\ncoefficient and the computation irreducible representation is a novel and\noriginal viewpoint which combined with the favorable tradeoff between good\nperformance and accuracy is an impactful contribution. Writing is clear,\nconcise and easy to follow."
            },
            "weaknesses": {
                "value": "Since the reviewer is not too familiar with the literature, the current\nwriting makes it somewhat hard to compare to the current available literature.\nFor instance, what would be the comparable difference between GotenNet and an\nequivariant transformer (such as equiformer) or the Graph Attention Networks.\nIs the current architecture a mix of the two? In that sense some more context\nmight be good in the introduction or overview of related work.\n\nThe paper as written in its current form is of great quality, and could be\npotentially be further strengthened through the incorporation of the points\nabove and by addressing the questions below in a section discussing the current\nlimitations of the method and potential future avenues of research. For\ninstance it seems that the current method is _specifically_ tailored to the\nprediction of molecule properties and how would that generalize? Another\npoint to potentially discuss is how this method would extend beyond $E(3)$\nand $SE(3)$ equivariance (maybe scaling for instance) and/or if the method\nwould generalize to point clouds (where this line of work is also highly\nrelevant). Since the paper is already of great quality addressing these\ntopics in the rebuttal would be sufficient, however addressing them in the\nrevision would be beneficial for the reader."
            },
            "questions": {
                "value": "Some questions: \n\n-  How  does  the  current architecture  generalize  to  different tasks? As\npresented, it  would work only on  molecule type graphs, is it also possible to\ngeneralize the architecture to for instance social  networks or  other types\nof graphs? In  other words,  how general  is the  method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose a novel Geometric Tensor Network (GotenNet) to address  trade-offs between expressiveness and computational efficiency. The core of this model is to use inner-product in Eq. (9) to avoid  tensor products or CG coefficients.\n***\n\nIn order to better explain my comments and facilitate the understanding of the author, other reviewers, and AC, I will sort out the timetable of related models that are not mentioned in the article, mainly involving the following references:\n1. SE(3)-Transformer [a] (in NeurIPS 2020): In Eq. (11) of [a], the calculation of attention $\\alpha$used $\\bf{q}^\\top\\bf{k}$, which sum all inner-products of high-degree steerable features together.\n2. SE(3)-Transformer implemented by e3nn [b]: See code below\n```python\ndot = o3.FullyConnectedTensorProduct(irreps_query, irreps_key, \"0e\")\n...\nexp = edge_weight_cutoff[:, None] * dot(q[edge_dst], k).exp()\n```\nAnd the main difference is the module $\\texttt{o3.FullyConnectedTensorProduct}$, which introduces different learnable weights to each inner-product. However, now the model still uses tensor products to generate the key and value pair and is limited by efficiency.\n\n3. ViSNet [c] (in Nature Communications: 05 January 2024, preprint at arXiv:2210.16518): In Eq. (6-7) of [c], authors introduced the connection between inner-products and Legendre polynomials (though a mathematical common sense). But in methodology, they only use the 1st-degree steerable features (Cartesian vectors) to build their models.\n\n4. SO3KRATES [d] (in Nature Communications: 06 August 2024, preprint at arXiv:2309.15126): The SO3KRATES technique in Eq. (14), CG coefficient used in such form, but is equivalent to the inner-products of high-degree steerable features. This paper proposed an Euclidean transformer and achieved nice performance in property prediction tasks (e.g. rMD17 and MD22).\n\n5. HEGNN [e] (in NeurIPS 2024, preprint at arXiv:2410.11443): During studying symmetric geometric graphs, the authors proposed a equivariant GNN model that uses the scalarization trick (i.e., inner product) to introduce high-degree steerable features in Eq. (6) of [e], and pointed out the connection between this approach and SO3KRATES. They used a method similar to Deepsets and Legendre polynomials to prove that this method can recover the information of all angles between each pair of edges, achieved good performance in dynamics tasks (e.g. N-body and MD17), and demonstrated that the introduction of high-degree steerable features can make the model more robust.\n\n[a] SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks\n\n[b] https://docs.e3nn.org/en/stable/guide/transformer.html \n\n[c] Enhancing geometric representations for molecules with equivariant vector-scalar interactive message passing\n\n[d] A Euclidean transformer for fast and stable machine learned force fields\n\n[e] Are High-Degree Representations Really Unnecessary in Equivariant Graph Neural Networks?\n\nIn subsequent comments, I will use the names of the above models without citation.\n\n---\n> **[First Comment, Rating 3, Confidence 5, with 4/1/4 of Soundness/Presentation/Contribution]**\n\nThe model and experiment in this paper are excellent and deserve a clear acceptance or even an award (8-10), but the terrible presentation of this version is totally unworthy of such an achievement. If the author can improve the presentation, I will be very happy to increase the score in the rebuttal. I am very confident that this is a paper that will have a profound impact on the entire field, so I hope it will be accepted in a very perfect manner."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "For the increasingly important discipline of AI for Science, dealing with equivariance is undoubtedly very important. According to the classification of [f], there are two mainstream models: scalarization-based models and high-degree steerable models. The timeline I introduced in the summary (and this article) is actually doing something very valuable, that is, bridge the gap between these two types of models.\n\nIn fact, the only papers that truly achieve this are SO3KRATES, HEGNN, and GotenNet (this paper). Although the core formulas of these three works are equivalent, their publication dates are close together\u2014especially since HEGNN's preprint appeared after the ICLR submission deadline. I believe this is merely a case of normal concurrent discovery, and therefore I recognize the originality of this paper. Moreover, equivariance is a very strong constraint, and I would even guess that this form is the only feasible form, which makes the contribution seem small in terms of formula form, but in fact it is a very important contribution.\n\nMoreover, these three works exhibit significant differences. Both SO3KRATES and GotenNet employ equivariant attention, but the inner product formulation used in GotenNet is more concise and yields far better results than SO3KRATES. While both HEGNN and GotenNet point out the inner product formulation, HEGNN\u2019s contributions lie more in theoretical research and model framework, with experiments focusing on dynamical problems, creating a complementary relationship with GotenNet. From my perspective, although GotenNet is on par with these two other works and shares an equivalent formulation, its superior performance makes it an excellent contribution.\n\n[f] A Survey of Geometric Graph Neural Networks: Data Structures, Models and Applications"
            },
            "weaknesses": {
                "value": "I think the whole article is amazingly good except for the presentation, and there is no major weakness. Here I mainly give the shortcomings of the presentation that I think need to be improved. For other parts (such as models and experiments), there are only some suggestions for the author to choose whether to adopt (see questions).\n\n> **W1. Lack of a good review of relevant literature.**\n\nThis article should introduce several works in the timeline I sorted out in the summary section, especially SO3KRATES and HEGNN, which should be introduced in detail. The results of SO3KRATES should be added to the experimental results for easy comparison (although GotenNet outperform SO3KRATES).\n\nFor models such as eSCN and EquiformerV2 that use spherical activation functions, a simple comparison should be made. For example, the use of Fibonacci grid requires a large number of sampling points, which brings a large constant in complexity, resulting in a theoretical complexity of $\\mathcal{O}(L^3)$, which is actually quite time-consuming (and this can only achieve qusi-equivariance).\n\n> **W2. Weird math definitions and confusing math symbols.**\n\nI don't know what the meaning of Def. 1, Theo. 1-2 in the main text is. I think it is just to make it look like there is some theory. Theo. 3 and Prop. 1, which have some meaning, are too trivial. They can be explained in one sentence: \"The product of an invariant scalar and other steerable features does not change its equivariant form.\" However, the key initialization by spherical harmonics is not only placed in the appendix, but also a clear formula is not given (while Eq. (11) in SO3KRATES and Eq. (5) in HEGNN are clearly written). I suggest replacing this with a discussion of the model architecture. In fact, it is a very valuable point to explain why GotenNet performs so much better than SO3KRATES with a similar architecture. Theoretical analysis may be very difficult, so it is not required, but it would be great if the authors could come up with some engineering tricks.\n\nMoreover, this article will surely have many followers in the future, and a good symbolic system will help future generations understand. The number of superscripts is sometimes one, sometimes two (though it has been explained), and the letters that appear are similar (the $l$ for the number of layers and the $L$ for high-degree steerable features are used at the same time), which is confusing. In addition, the current symbolic representation cannot well show which are invariant scalars and which are high-degree steerable features. It is recommended that the author adopt the HEGNN symbol system (e.g. using $\\Delta\\boldsymbol{h}$ to represent inter-layer updates, and using fonts such as $\\tilde{\\boldsymbol{v}}^{(l)}$ to represent $l$-degree steerable features).\n\n> **W3. Others.**\n\n- It is recommended to unify the terminology of the article, such as high-degree steerable features, and not to mix order and degree, otherwise it always gives readers the feeling that the authors are studying multi-body interactions. \n\n- There are too many crossing lines in Fig. 2 (b-d), which makes it very difficult to understand. Can it be simplified?\n\n- The rotation matrix $R$ in Line 879 (in fact, it should be emphasized that it is an orthogonal matrix) should be the Wigner-D matrix $\\boldsymbol{D}^{(l)}(r)$. In addition, the inversion factor should be multiplied according to the parity of the vector itself. For details, refer to [g].\n\n[g] https://docs.e3nn.org/en/stable/api/o3/o3_irreps.html"
            },
            "questions": {
                "value": "Before anything else, is the author willing to provide an anonymous link to the project as soon as possible (this will not affect my subsequent rating increase) so that the results can be reproduced? \n\n> **Q1. Why use the same weight for steerable features of different degrees (in Eqs. (7) and (11))? Can they be different?**\n\nNote that there are several formula items: $o\\_{ij}^d\\circ r\\_{ij}^{[1,L\\_{max}]}$, $o\\_{ij}^t\\circ h\\_{j}^{l,[1,L\\_{max}]}$, $m\\_2\\circ h^{l,[1,\\hat{L}\\_{max}]}\\bf{W}\\_{vu}$. They seem to be multiplied by the same coefficient. Steerable features of different degrees represent different orthogonal bases, so their weights should not be the same (as HEGNN does). Can the authors change it to have different weights for each degree and test the performance of the corresponding model?\n\n> **Q2. Why does Eq. (9) use a permutation-invariant operator?**\n\nThe results of different degrees should be ordered, so there seems no need for permutation invariance. What are the benefits of using permutation-invariant operations?\n\n> **Q3. Is it possible to explore the expressive power of GotenNet using high-degree steerable features?**\n\nOne of the great benefits of this model is that the complexity grows slowly with the feature degree. Can authors show the experimental results of using higher-degree features (e.g. $L\\_{\\max}\\in\\\\{6,8\\\\}$)? In fact, similar HEGNN and EquiformerV2 have demonstrated the benefits of using such higher-degree features.\n\n> **Q4. Is it possible to explore the expressive power of GotenNet on large-scale geometric graphs and dynamics tasks?**\n\nIs it possible to add speed attributes like EGNN so that GotenNet can be applied to dynamics tasks? In addition, it is easy to associate the fast multipole method with the possibility that high-degree representations may have good effects on large-scale geometric graphs. Can the authors try the effect of GotenNet on large-scale geometric graphs (e.g. 100-body in HEGNN)?\n\n> **Q5. Is it possible to test the effect of $L\\_{\\max}=1$?**\n\nAlthough the benefits of high-degree steerable features are obvious, in many cases a low-degree model is good enough. Can authors give a case where only Cartesian vectors ($L\\_{\\max}=1$) are used?\n\n> **Q6. Is it possible to provide an e3nn version of GotenNet implementation later?**\n\nFrom the full text, I guess the author may not know the e3nn library, which can easily implement the interaction of multi-channel high-degree steerable features. I believe it will be easier for everyone to follow the work of this article. However, this project may be very large, and I hope the author can provide this version after the article is accepted."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}