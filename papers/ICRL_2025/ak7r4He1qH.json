{
    "id": "ak7r4He1qH",
    "title": "AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments",
    "abstract": "Evaluating large language models~(LLM) in clinical scenarios is crucial to assessing their potential clinical utility. Existing benchmarks rely heavily on static question-answering, which does not accurately depict the complex, sequential nature of clinical decision-making. Here, we introduce AgentClinic, a multimodal agent benchmark for evaluating LLMs in simulated clinical environments that include patient interactions, multimodal data collection under incomplete information, and the usage of various tools, resulting in an in-depth evaluation across nine medical specialties and seven languages.\nWe find that solving MedQA problems in the sequential decision-making format of AgentClinic is considerably more challenging, resulting in diagnostic accuracies that can drop to below a tenth of the original accuracy. Overall, we observe that agents sourced from Claude-3.5 outperform other LLM backbones in most settings. Nevertheless, we see stark differences in the LLMs\u2019 ability to make use of tools, such as experiential learning, adaptive retrieval, and reflection cycles. Strikingly, Llama-3 shows up to 92\\% relative improvements with the notebook tool that allows for writing and editing notes that persist across cases. To further scrutinize our clinical simulations, we leverage real-world electronic health records, perform a clinical reader study, perturb agents with biases, and explore novel patient-centric metrics that this interactive environment firstly enables.",
    "keywords": [
        "Language Agents",
        "Medical Benchmark",
        "Multimodal Benchmark",
        "Multimodal Language Models"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "AgentClinic turns static medical QA problems into agents in a clinical environment in order to present a more clinically relevant challenge for multimodal language models",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ak7r4He1qH",
    "pdf_link": "https://openreview.net/pdf?id=ak7r4He1qH",
    "comments": [
        {
            "summary": {
                "value": "Recognizing that existing static QA benchmarks often fail to reflect the complexities of clinical decision-making tasks, the authors propose AgentClinic, an open-source multimodal agent benchmark for simulating clinical environments. The framework introduces patient agents (informed by real clinical cases), doctor agents, a measurement agent, and a moderator, with agents exhibiting 24 different biases. The patient cases represent nine medical specialities across seven multilingual environments. The study reveals significant performance differences across models, highlighting the impact of biases and tool integration, with models like Claude-3.5 and Llama-3 demonstrating notable improvements with tools like adaptive retrieval and note-taking."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Dataset: This is a novel and comprehensive benchmark that closely mimics physician-patient interactions. Its inclusion of diverse medical scenarios ensures a comprehensive assessment of model capabilities, providing a challenging benchmark for evaluating model accuracy and generalization.\n\nApproach: The authors use a robust benchmarking approach that includes multiple state-of-the-art models (e.g., Claude 3.5, GPT-4, Mixtral-8x7B, Llama 3, etc.) evaluated on the same task, and quantify uncertainty in performance. This methodology ensures a fair and statistically rigorous comparison of the models, enhancing the reliability and transparency of the results.\n\nExperiment: The human evaluation ratings provided by physicians adds a layer of real-world applicability of this benchmark."
            },
            "weaknesses": {
                "value": "Stigmatizing language in medical records can influence not only how physicians perceive patients, but also how treatment decisions are made. Physicians have been found to prescribe pain medication less often when patient notes contain stigmatizing versus neutral language (P Goddu et al., 2018; Kelly et al., 2010). This dynamic can influence treatment outcomes and the overall patient-provider relationship. The clinical cases in the paper could benefit from more nuanced language that reflects these concerns, particularly in how bias is conveyed through medical documentation. Additionally, the paper\u2019s \"Bias and Patient Agent Perception\" section could be expanded to explore patient trust in the healthcare system."
            },
            "questions": {
                "value": "1. What is the rationale behind an N of 20 for the interaction time when assessing the diagnostic accuracy of AgentClinic-MedQA? I\u2019m surprised at the meaningful drop in diagnostic accuracy from 52% to 25%. \n\n2. Although Claude 3.5 is reported to have the highest accuracy on both AgentClinic-MIMIC-IV and AgentClinic-MedQA, its performance varies by as much as 13 percentage points. What factors contribute to this moderate variability? Does this suggest issues with model robustness or potential limitations in the dataset or experimental design?\n\n3. Have the authors conducted a detailed error analysis to identify the types of questions/ cases the models struggle with the most? Are there specific failure modes common to all the models, or do different models exhibit distinct weaknesses?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces AgentClinic, a multimodal benchmark that evaluates language models' medical diagnostic abilities through **interactive dialogue** and examinations rather than static questions. Using four agents and incorporating biases, tools, and real clinical cases across specialties and languages, it demonstrates Claude-3.5's superior performance while revealing varied capabilities among models in *tool usage* and *bias handling*."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Overall, the paper is well-written with clear logical flow and is easy to follow. The motivation is clearly presented and makes sense: \"Existing diagnostic challenges are not static QAs, but are interactive, dialogue-driven, sequential decision-making environments that require data collection, ordering appropriate medical exams, and understanding medical images across patients with unique family histories, lifestyle habits, age categories, and diseases.\" This naturally addresses key limitations in previous medical LLM agents' static QA-oriented tasks. The human evaluations from experts make the presented experiments solid. The discussion is also impressive and comprehensive."
            },
            "weaknesses": {
                "value": "Weaknesses discussed in the Discussion section:\n- The simulated clinical environments are currently simple; more methods could be benchmarked:\n  1. Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate (https://arxiv.org/abs/2305.19118)\n  2. MedAgents (https://aclanthology.org/2024.findings-acl.33/)\n  3. ReConcile (https://arxiv.org/abs/2309.13007)\n- Uncertainty issues\n\nSuggestions for improvement:\n- Include a statistics table for utilized/built datasets, covering sample size, dataset modalities included, task types/descriptions\n- Address ethics issues with closed-source LLMs by replacing them with open-source LLMs running offline to prevent patient-sensitive data leakage\n- Fix typography: use ``xxx'' in LaTeX for quotes\n- Add more multi-agent collaboration/MDT baselines\n- Evaluate sensitivity to various prompting strategies"
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Note: Closed-source models such as GPT-4 are prohibited for MIMIC-III and MIMIC-IV according to their data use policy (https://physionet.org/about/licenses/physionet-credentialed-health-data-license-150)"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The article establishes a dynamic evaluation benchmark by simulating doctor-patient dialogues to assess model capabilities. Detailed experiments were conducted in scenarios involving bias, multilingualism, multiple departments, and multimodality, providing the community with a comprehensive evaluation framework."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Built an evaluation framework for multiple agents to assess the performance of LLM models from various dimensions, including diagnostic performance and patient experience.\n- Constructed a comprehensive test set to measure LLM's diagnostic capabilities in a simulated environment from multiple settings, including performance of different models in biased environments, multilingual, multi-department, and multi-modal scenarios.\n- Tested several representative closed-source and open-source models and provided an objective evaluation."
            },
            "weaknesses": {
                "value": "- The article's simulation of interactions between multiple agents is relatively simple, with the main interactions in the experiment limited to between the doctor agent and the patient agent.\n- The article does not conduct an in-depth analysis of the reasons for the model's performance differences on AgentClinic-MedQA and MedQA.\n- Although the article introduces human scoring of the model's performance, it does not involve humans in the simulation experiments, making it difficult to measure the differences between the model's performance and human performance."
            },
            "questions": {
                "value": "- Why is a measurement agent needed, and how does its role differ from that of a tool? I hope the author can provide a deeper explanation.\n- It seems that the doctor agent in AgentClinic-MedQA has the opportunity to gather more information through multiple rounds of interaction compared to the doctor agent in MedQA. However, in actual tests, the performance of the doctor agent in AgentClinic-MedQA may be inferior to that of the doctor agent in MedQA. There could be many reasons for this phenomenon. Can the author provide an intermediate metric, such as the amount of useful information obtained by the doctor agents in both AgentClinic-MedQA and MedQA, to further determine the causes of this phenomenon?\n- Is it possible to have human doctors interact with simulated patients under the most basic settings and then compare the performance differences between human doctors and LLM doctors? This might provide a more intuitive comparison."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this work, the authors introduce AgentClinic, a multimodal benchmark for evaluating LLMs within simulated clinical environments. The pipeline\u2019s performance is assessed using the MedQA, MIMIC-IV, and NEJM datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Some conclusions may hold clinical relevance"
            },
            "weaknesses": {
                "value": "My primary comments are as follows:\n\n1. The paper is challenging to follow, as many crucial details are buried in the lengthy appendix rather than presented in the main text. For instance, it is unclear what specific biases are explored in this work and how they are defined. Additionally, how agent tools are integrated within the pipeline, what specific data are extracted from the MIMIC-IV dataset, the nature of the NEJM case challenge dataset, and whether it includes QA pairs all need clarification. Furthermore, the difference between specialist case use reports and general QA tasks should be explained. I recommend providing brief explanations for any non-standard terms in the main text for improved clarity.\n\n2. Although the paper claims to simulate a real-world clinical environment, several settings and use cases seem impractical. For example, what is the intended purpose of the measurement agent? Why can\u2019t the complete records be provided directly, or why doesn\u2019t the doctor agent extract values directly from the database? Additionally, evaluating a patient agent seems unrealistic within actual clinical workflows.\n\n3. Since the study focuses on evaluating LLMs within clinical contexts, the benchmark appears to lack some medical LLMs."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}