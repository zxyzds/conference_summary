{
    "id": "ywHOnGOLb1",
    "title": "A Competitive-Cooperative Actor-critic Framework for Reinforcement Learning",
    "abstract": "In the field of Deep reinforcement learning (DRL), enhancing exploration capabilities and improving the accuracy of Q-value estimation remain two major challenges.\nRecently, double-actor DRL methods have emerged as a promising class of DRL approaches, achieving substantial advancements in both exploration and Q-value estimation. However, existing double-actor DRL methods feature actors that operate independently in exploring the environment, lacking mutual learning and collaboration, which leads to suboptimal policies. To address this challenge, this work proposes a generic solution that can be seamlessly integrated into existing double-actor DRL methods by promoting mutual learning among the actors to develop improved policies. Specifically, we calculate the difference in actions output by the actors and minimize this difference as a loss during training to facilitate mutual imitation among the actors. Simultaneously, we also minimize the differences in Q-values output by the various critics as part of the loss, thereby avoiding significant discrepancies in value estimation for the imitated actions. We present two specific implementations of our method and extend these implementations beyond double-actor DRL methods to other DRL approaches to encourage broader adoption. Experimental results demonstrate that our method effectively enhances four state-of-the-art (SOTA) double-actor DRL methods and five other types of SOTA DRL methods across four MuJoCo tasks, as measured by return.",
    "keywords": [
        "Deep reinforcement learning; Double-actor framework; Competition and Cooperation"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "This work aims to develop a generic framework that can be seamlessly integrated with existing double-actor DRL methods to promote cooperation among actor-critic pairs",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ywHOnGOLb1",
    "pdf_link": "https://openreview.net/pdf?id=ywHOnGOLb1",
    "comments": [
        {
            "summary": {
                "value": "This paper addresses two significant challenges in Deep Reinforcement Learning (DRL): enhancing exploration capabilities and improving the accuracy of Q-value estimation. The authors observe that existing double-actor DRL methods, while promising, suffer from a lack of cooperation between the two actors, leading to suboptimal policies. To mitigate this, they propose a competitive-cooperative actor-critic framework that encourages mutual learning among actors by minimizing the differences in their output actions and the discrepancies in the Q-values estimated by their respective critics. They present two specific implementations of their method and extend it to multi-critic DRL methods. The effectiveness of their approach is demonstrated through extensive experiments on four MuJoCo tasks, where it enhances the performance of nine state-of-the-art DRL methods in terms of return."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tGenerality and Flexibility: The proposed framework is designed to be generic and can be seamlessly integrated into existing double-actor DRL methods as well as extended to multi-critic DRL methods. This broad applicability enhances its potential impact on the DRL community.\n2.\tImproved Policy Performance: By promoting mutual imitation among actors, the framework addresses the issue of independent exploration in existing methods, leading to the development of better policies and improved performance.\n3.\tConcrete Implementations: The paper provides two specific implementations of the framework, demonstrating its flexibility and practical applicability.\n4.\tComprehensive Experiments: The authors conduct extensive experiments on four MuJoCo tasks and evaluate their method against nine state-of-the-art DRL algorithms. The results show consistent performance improvements, supporting the effectiveness of their approach."
            },
            "weaknesses": {
                "value": "1.\tExperimental Scope: The experiments are restricted to MuJoCo tasks. Expanding validation to environments with more complex and dynamic variables, or to real-world tasks, would provide stronger evidence for the framework\u2019s generalizability and practical relevance.\n2.\tLimited Analysis of Implementation Trade-offs: While both implementations of collaborative loss show performance improvements, the paper could benefit from a more nuanced discussion on the trade-offs between complexity and performance across various use cases."
            },
            "questions": {
                "value": "1.\tExtension to Complex Tasks: Could the authors elaborate on the applicability of the competitive-cooperative framework in more complex, non-simulation tasks?\n2.\tTrade-offs in Collaborative Loss Implementations: What specific use cases would benefit more from each implementation of the collaborative loss? Are there trade-offs between performance and computational complexity that should be considered?\n3.\tExploration and Exploitation Balance: How does the framework affect the exploration-exploitation balance compared to other multi-actor DRL methods, and how would this influence application to environments with sparse rewards?\n4.\tImpact of Q-value Discrepancy Minimization: Given the role of Q-value minimization in aligning actor policies, are there specific scenarios or tasks where this could inadvertently limit exploration or policy diversity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a training-method to improve double actor reinforcement learning. They propose to use an additional loss that minimizes the discrepancy between actors and ciritic predictions during training. The authors present two variants of this \"collaborative loss\". The authors show how this loss can be incorporated into the implementation of a wide range of existing double-actor and multi-critic RL algorithms. They highlight benchmark results for four Mujoco environments, and show the developed approach matches or outperforms existing RL algorithms without the collaborative loss."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Strengths:\n- The overall writing is fine; I could follow the paper well\n- The method is simple and, indeed, seems widely applicable to many existing algorithms\n- The method is described in reasonable detail, which allows to understand the methodology well\n- Many baseline algorithms are used for comparison"
            },
            "weaknesses": {
                "value": "Weaknesses:\n- There seems to exist quite a bit of work that uses some sort of interaction/regularization between multiple actors (e.g., Li2023 et al.: Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control), indeed going beyond two actor-critic pairs, so I do not feel the statement \u201cExisting double-actor DRL methods involve each actor-critic pair exploring independently\u201d is warranted\n- Given the close results compared to baseline algorithms, the limited choice of environments (just 4 Mujoco environments) is a bit lacking; showcasing environments with weak performance of other methods, and highlighting the performance the proposed method would have been more convincing\n- Some of the reported baseline benchmark results seem off, e.g., CAL reports > 5000 reward for Ant in their implementation at 1M steps (Figure 2, Li et al., Simultaneous Double Q-learning\u2026), which deviates very significantly from the reported reward. In Table 1, for SD3, 1750 is reported for Hopper; however, for Hopper in the SD3 paper, it seems to be much closer to 3500 than 2882; similarly >4000 for Ant is reported in the SD3 paper compared to the score of 1176 given in this paper. While I accept that there can be differences based on random seeds and unreported hyperparameter settings, these deviations seem pretty large. Can the authors please check these numbers or explain the discrepancies?\n- Overall, I do not find the empirical results to be compelling; I do not see that the proposed approach significantly outperforms the existing methods\n\nMinor:\n- For intrinsic exploration, there should be more foundational related work, which should be cited\n- Sometimes, the terminology in the related work is weak, e.g., \u201cCurrent mainstream DRL algorithms typically utilize an architecture based on one actor and two critics.\u201d -> That is the case for some algorithms like SAC or Double-DQN, but not necessarily for others; would can be more specific here\n- Section 4.4. adds relatively little new information; I think it\u2019s pretty straightforward, and a statement like \u201cspace complexity increases due to the parameters of the value network, which includes a five-layer Multilayer Perceptron\u201d does seem too specific to the given implementation\n- The term \u201cmutual information\u201d is an established term, and it is not obvious that the difference between actors/critics (4.2. (2), (4)) reduces mutual information\n- Section 4.5 is difficult to follow in the main paper; maybe this can be re-structured not entirely to rely on the appendix"
            },
            "questions": {
                "value": "Clarification:\n- The \u201ccollaborative loss\u201d tries to minimise the difference between policies and q-function estimates (compared to existing work which generally advocates for improved diversity). I feel that, in effect, the algorithm should behave more similar to a normal \u201csingle-model\u201d algorithm, like TD3. Doesn\u2019t this go against the spirit of double actor-learning?\n- 4.2 (2) also indicates that the critic are optimised to predict the same target value Q-hat, which means that the predictions are even more similar. Can you provide some plot or analysis that showcases how different policy predictions and critic predictions actually are? I would really like to see a comparison to non double-actor methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a framework that promotes mutual learning and imitation between actors. It achieves this by minimizing differences in actions between actors and Q-value discrepancies between critics, thereby improving the performance of reinforcement learning algorithms. The framework is implemented through two specific approaches: Direct Imitation and Selective Imitation. Direct Imitation minimizes differences in actions produced by actors and Q-value discrepancies between corresponding critics, fostering mutual learning between actors. Selective Imitation utilizes the value function to assess actions and imitates only those with higher assessments, thereby avoiding the replication of lower-quality strategies. Additionally, the framework extends to multi-critic architectures. Experimental results demonstrate that this method significantly improves the performance of nine state-of-the-art reinforcement learning algorithms across four MuJoCo tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper proposes an innovative competitive-cooperative actor-critic framework, which addresses the limitation of independent exploration in existing double-actor methods.\n2. This paper rigorously analyzes the proposed framework and validates its effectiveness through extensive experiments.  \n3. This paper proposes a general and scalable framework for improving the performance of reinforcement learning algorithms. Both approaches mentioned in this paper achieve good performance on four widely adopted MuJoCo environments.\n4. This paper demonstrates through an ablation experiment that the best results are achieved when both the actors and critics simultaneously engage in mutual imitation."
            },
            "weaknesses": {
                "value": "1. This paper does not provide the implementation code, which may limit the reproducibility of the experimental results. To improve transparency and reproducibility, it is recommended that the authors make the code publicly available, for instance by sharing a link to a GitHub repository, so that reviewers and readers can verify and reproduce the experiment.\n2. This paper does not consider the weight between the Q function and the collaborative loss when computing the actor's gradient. It is recommended that the authors consider the weight, and conduct an ablation study on this parameter to explore the impact of different values on the performance in the test environments, or please give a justification for this fixed weight."
            },
            "questions": {
                "value": "1. Could the authors provide a more detailed explanation of how the mutual learning mechanism between actors directly impacts the exploration process and policy convergence? Specifically, how does minimizing the action differences between actors quantitatively lead to improved exploration outcomes?\n2. In the selective imitation method, why was the value function chosen to determine when imitation should occur?\n3. Have you considered adding a weight parameter between the Q function and the collaborative loss when computing the actor's gradient? Additionally, how does this hyperparameter influence the experimental performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}