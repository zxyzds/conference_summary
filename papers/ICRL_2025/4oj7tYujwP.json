{
    "id": "4oj7tYujwP",
    "title": "ERiC-UP$^3$ Benchmark: E-Commerce Risk Intelligence Classifier for Detecting Infringements Based on Utility Patent and Product Pairs",
    "abstract": "Innovation is a key driver of economic and social progress, with Intellectual Property (IP) protection through patents playing a crucial role in safeguarding new creations. For businesses actively producing goods, detecting potential patent infringement is vital to avoid costly litigation and operational disruptions. However, the significant domain gap between products and patents\u2014coupled with the vast scale of existing patent databases\u2014makes infringement detection a complex and challenging task. Besides, the machine learning (ML) community has not widely addressed this problem, partly due to the lack of comprehensive datasets tailored for this task. In this paper, we firstly formulate a new task: detecting potentially infringing patents for a given product represented by multi-modal data, including images and textual descriptions. This task requires a deep understanding of both technical and legal contexts, extending beyond simple text or image matching to assess functional similarities that may not be immediately apparent. To promote research in this challenging area, we further introduce the ERiC-UP$^3$ ($\\textbf{E}$-commerce $\\textbf{R}$isk $\\textbf{i}$ntelligence $\\textbf{C}$lassifier on $\\textbf{U}$tility $\\textbf{P}$atent $\\textbf{P}$roduct $\\textbf{P}$air) benchmark, a large-scale, well-structured dataset comprising over 13-million patent samples and 1 million product samples. It includes 11,000 meticulously annotated infringement pairs for training and 2,000 for testing, all rigorously reviewed by patent experts to ensure high-quality annotations. The dataset reflects real-world scenarios with its multi-modal nature and the necessity for deep functional understanding, offering unique characteristics that set it apart from existing resources. As a case study, we provide results from a series of baseline methods and propose a simple yet effective infringement detection pipeline. We also explore additional approaches that may enhance detection performance, such as text style rewriting, cross-modal matching effectiveness, and image domain alignment. Overall, the ERiC-UP$^3$ benchmark is the first strictly annotated product-patent infringement detection dataset and stands as the largest multi-modal patent dataset, as well as one of the largest multi-modal product datasets available. We aim to advance research extending language and multi-modal models to diverse and dynamic real-world data distributions, fostering innovation and practical solutions in IP infringement detection.",
    "keywords": [
        "Benchmark; Product-Patent Infringement Detection; Large-scale Multi-Modality Dataset; Contrastive Learning; Retrieval; Domain Gap"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4oj7tYujwP",
    "pdf_link": "https://openreview.net/pdf?id=4oj7tYujwP",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a benchmark dataset for E-commerce intelligence for machine learning field. This research narrows the gap between the E-commerce area with current artificial intelligence research. In addition, this draft further provides analysis for the proposed benchmark and gives several baseline methods for reference to following research works."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This work narrows the gap between e-commerce and machine learning research, which is a valuable try and has potential to further enlarge the impact of machine learing.\n2. In addition to the proposed benchmark dataset, it also provides detailed statistical analysis with several backbone experiments for reference.\n3. Overall, the writing is easy to follow."
            },
            "weaknesses": {
                "value": "1. Some parts of the draft are not well-prepared, such as tab.7 and 8. The overall format needs a careful polish.\n2. Even if the proposed benchmark is for multi-modal learning, especially for vision-language interaction, I still think this topic fits better for data mining or multi-media conferences, especially considering it is a dataset-oriented paper.\n3. Captions of figures and tables are necessary to be enriched. At least, they need to indicate the conclusion of the tables and figures. Overall, they need to be more informative."
            },
            "questions": {
                "value": "Please check the weaknesses section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The article proposes a new task of detecting potential infringing patents for a given product, and introduces a large-scale MultiModal Machine Learning dataset called ERiC-UP3, aimed at promoting research in this field. The dataset contains over 13 million patent samples and 1 million product samples, providing real-world scenarios needed for deep functional understanding to promote innovative and practical solutions for intellectual property infringement detection. It also provides some evaluation baselines and testing methods. In essence, it has the following setting:\n\nSearch task set: Retrieve the patent q that product p is most likely to infringe and give the probability ranking of infringing patents in the patent list\n\nTask objective: Ensure that patents with the most similar functions and potential infringement are ranked highest in the list sorting"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Marking products and infringing data is a very heavy workload, this work has done some valuable efforts to annotate the data.\n\nThe text data of the product and the patent have been rewritten, which can avoid the potential meaning difference."
            },
            "weaknesses": {
                "value": "The writing is rather confused.\n\nThis article mainly discloses the patent product infringement data of MultiModal Machine Learning, but the main text mainly discusses the single modality of text, and there is little use and verification of image modality.\nFor datasets, some graphic and table information is invalid. There is a lot of redundant information in the paired graphic and text dataset. This article rarely mentions and verifies how to ensure that this data is effective for training, and rarely uses this data for experiments and verification of graphic and text information for infringement conflicts.\nThe expert evaluation mentioned in the article mainly evaluates whether there is infringement between the product and the patent, rather than evaluating the validity of the data\n\nHowever, from the perspective of CS, it is not clear whether these MultiModal Machine Learning data are effective and what the purpose of using these data is. \n\n\nI have several questions regarding this work:\n\n1. The overall framework of the paper is quite chaotic, and the research framework is not clear.\n\n2. The experiment is comprehensive, but many tables have unclear meanings and are chaotic, a bit like an experimental report\n  - The Table 7 compares which method, I can't tell, and it's not specifically written in the article, just said the score is high.\n  - In Table 8, Using LLM to rewrite the text data of the product and the patent can effectively avoid the difference in emphasis between the two. However, it is hasty and inaccurate to determine that 0.5b qwen is the best for only three categories. Llama3-8b also has multiple high scores. Why not consider llama3-8b?\n\n\n3. In Figure 6, the significance of calculating the recall rate of the top 500 is not great, and the average value of each CPC category is not given, and it cannot be seen that this mAR@500 is a good evaluation index, and the number of samples of each CPC classification is very different, the variance is very large, why not use top 10% or top 1% as the evaluation index, as shown in the figure below is the order of magnitude of 10 ^ 6. \n\n4. The experimental framework of MultiModal Machine Learning fusion retrieval is not clear\n  For example, how to evaluate after image retrieval mAR@500 scores are not given\n  Why is it first evaluated through text matching to the relevant patent pool, and then evaluated through image retrieval, rather than directly conducting image retrieval (missing this experiment)?\n\n  This article mainly conducts experiments on text modality, with little emphasis on the role of MultiModal Machine Learning data, and does not reflect the significance of MultiModal Machine Learning data for infringement retrieval.\n\nRegarding the experiment of MultiModal Machine Learning in this article, the following questions are raised:\n\n- The MultiModal Machine Learning experiment in the main text of this article is just a simple stitching, text classification + image retrieval. Where is the specific integration of MultiModal Machine Learning reflected?\n- The experiments are all here and there, and the overall performance of MultiModal Machine Learning cannot be seen\n- Table 9 shows the image retrieval results after text classification. Which method is used for the first step of text classification? Or is it directly given classification for image retrieval to eliminate errors caused by text classification? If not, how to eliminate errors? Why not directly retrieve images? The explanation is not comprehensive enough.\n\n\n5. The experimental data is incorrect. The experimental table of MultiModal Machine Learning in the above text is different from the data given in the last supplementary material.\n    - The experimental data of the plain text of table5 and table16 are inconsistent, and there is no other data of 71.43.\n  \n6. Many of the experiments in the supplementary materials are not mentioned in the main text, and there is no clear definition of how the methods are done, how to conduct mixed experiments, or how to conduct mixed voting, and how to evaluate them\n   - According to Table 16, the article only mentions simple concatenation, but does not explain how the following two fusion are done, and the description is quite confusing. Is the voting experiment at the end just a simple union of the results of the baselines of the original two modes? Or are there other voting operations?"
            },
            "questions": {
                "value": "as above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents ERiC-UP$^3$, a dataset with annotations to detect infringement behaviors between a given Amazon product and existing patents. This dataset includes 1 million product samples, 13 million patent samples, and 11,000 meticulously annotated infringement pairs for training and 2,000 for testing. This work benchmarks existing baselines and proposes a two-stage pipeline for effectively conducting infringement detection. This paper also provides some best practices to improve detection."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. A new dataset for detecting infringements in the patent domain (the unique aspect lies in the annotation).\n2. A proposed pipeline to surpass existing methods.\n3. Some useful takeaways to improve detection."
            },
            "weaknesses": {
                "value": "1. The dataset offers some novelty but is largely a domain adaptation from existing datasets like [1] and [2]. Its main advantage lies in expert annotations on infringement cases. However, the dataset\u2019s scale is limited, with relatively few annotations, and the patent and product samples were scraped from the internet. Additionally, the distinction between \"base\" and \"large\" versions is minimal.\n\n2. The writing lacks clarity, making it hard to grasp key points at first glance: (1) Is infringement treated as a ranking problem? (2) What constitutes the \"domain gap\"? Is it simply a stylistic shift? (3) Why were these particular classes selected?\n\n3. The technical pipeline appears ad hoc. Why use a two-stage approach instead of a streamlined, end-to-end model? Why can't existing models address this problem effectively? Why wasn\u2019t the current infringement detection pipeline integrated into the study?\n\n4. Key baselines are missing from this study: (1) multimodal baselines, such as LLaVA, and (2) baselines from prior infringement detection research.\n\n5. Important ablations on the pipeline components are absent. For instance, how does removing expert labels affect training? What are the results if detections are run without training labels?\n\n6. The analysis part is shallow, with findings that are largely known within the field.\n\n7. The literature review lacks the recent works.\n\n8. Some obvious typos in number and upper/lower case.\n\n[1] A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models\n[2] TMID: A Comprehensive Real-world Dataset for Trademark Infringement Detection in E-Commerce"
            },
            "questions": {
                "value": "see wearkness. I also has a question about the significant of this work: 1) Can Google Patents (https://patents.google.com/) be used for detect infringement? 2) Is Amazon conduct infringement screening before releasing the product? If they do so, i think that only very limited samples in the ERiC-UP$^3$ involves infringement, and training model with ERiC-UP$^3$ cannot significant detect real-world product."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}