{
    "id": "x4ZmQaumRg",
    "title": "Active Learning for Neural PDE Solvers",
    "abstract": "Solving partial differential equations (PDEs) is a fundamental problem in engineering and science.  While neural PDE solvers can be more efficient than established numerical solvers, they often require large amounts of training data that is costly to obtain. Active Learning (AL) could help surrogate models reach the same accuracy with smaller training sets by querying classical solvers with more informative initial conditions and PDE parameters. While AL is more common in other domains, it has yet to be studied extensively for neural PDE solvers. To bridge this gap, we introduce AL4PDE, a modular and extensible active learning benchmark. It provides multiple parametric PDEs and state-of-the-art surrogate models for the solver-in-the-loop setting, enabling the evaluation of existing and the development of new AL methods for PDE solving. We use the benchmark to evaluate batch active learning algorithms such as uncertainty- and feature-based methods. We show that AL reduces the average error by up to 71\\% compared to random sampling and significantly reduces worst-case errors. Moreover, AL generates similar datasets across repeated runs, with consistent distributions over the PDE parameters and initial conditions. The acquired datasets are reusable, providing benefits for surrogate models not involved in the data generation.",
    "keywords": [
        "Active Learning",
        "Neural PDE Solvers",
        "Scientific Machine Learning",
        "Benchmark",
        "Framework",
        "Neural Operators"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "A extensible benchmark to evaluate pool-based active learning for neural PDE solvers.",
    "creation_date": "2024-09-17",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=x4ZmQaumRg",
    "pdf_link": "https://openreview.net/pdf?id=x4ZmQaumRg",
    "comments": [
        {
            "summary": {
                "value": "This paper provided a bechmark called AL4PDE, which unifies active learning (AL) with neural PDE solvers. Specifically, it studies how several state-of-the-art neural surrogate model may be applied to solve parametric PDEs under a solver-in-the-loop (AL) setting. A complete set of numerical experiments on various tasks is included to justify the effectiveness of AL based methods compared to methods based on random sampling."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Extensive numerical experiments on multiple PDEs are provided to validate the effectiveness of the proposed methodology. Also, details about the numerical experiments, such as the neural network models and training procedures, are included for the sake of completeness."
            },
            "weaknesses": {
                "value": "1. Though the authors have conducted a literature review on how AL has been used for solving other problems from scientific ML, such as PINN and direct prediction, it seems to the reviewer that the authors have missed a few important references like [1,2]. It might be meaningful for the authors to include these work and briefly discuss them in the introduction. \n\n2. Given that this work aims for a complete benchmark on various tasks, the authors might consider including some more experiments on high-dimensional PDEs, just like the setting of [3]."
            },
            "questions": {
                "value": "The parameter $c$ mentioned in line 96-97 (referred to as the field variables or channels) seems a bit ambiguous here as the following PDE doesn't contain anything about $c$. Would it be possible for the authors to provide more detailed explanation for the field variable/channel $c$ here? (This also highly relates to the parameter $N_c$ appearing in equations (3) and (5).)\n\nReferences:\n\n[1] Bruna, Joan, Benjamin Peherstorfer, and Eric Vanden-Eijnden. \"Neural Galerkin schemes with active learning for high-dimensional evolution equations.\" Journal of Computational Physics 496 (2024): 112588.\n\n[2] Gajjar, Aarshvi, Chinmay Hegde, and Christopher P. Musco. \"Provable active learning of neural networks for parametric PDEs.\" In The Symbiosis of Deep Learning and Differential Equations II. 2022.\n\n[3] Gao, Wenhan, and Chunmei Wang. \"Active learning based sampling for high-dimensional nonlinear partial differential equations.\" Journal of Computational Physics 475 (2023): 111848."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a benchmark framework for neural PDE solvers under active learning (AL) settings (AL4PDE). It provides a modular benchmark with various parametric PDEs and AL methods. The experimental results show that AL significantly reduces average and worst case errors compared to random sampling and yields reusable datasets across experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-presented and easy to follow.\n\nThe proposed framework is novel in extending neural PDE methods with active learning methods.\n\nThe benchmark includes various batch selection strategies and neural PDE solvers, covering recent and classical works."
            },
            "weaknesses": {
                "value": "One key benefit of AL is data efficiency, which is also stressed in the paper. It is important to show how much data reduction can be achieved with reasonable model performance.\nCurrent experiment section only shows performance comparison of different active learning methods and lacks the \"offline\" performance, which is training the model with full dataset and evaluate its performance. \n\nAt line $88$, the author claims \"We demonstrate that using AL can result in more accurate surrogate models trained in less time.\" As mentioned above, this claim is not supported with empirical evidence as the experimental section only compares active learning performance, which cannot demonstrate improvement in accuracies regarding offline performance.\n\nThe novelty of this framework seems limited, as it is a combination of existing AL and neural PDE methods."
            },
            "questions": {
                "value": "Please refer to the strengths and weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Thie article introduces an active learning benchmark for neural PDE solver. It compare exploration-exploitation tradeoffs based uncertainty (epsitemic uncertainty of an ensemble of models with top-K and SBAL) or features (using dimensionality reduction using Gaussian sketching with Core-Set and LCMD). The authors then show a benchmark of these method on 1D and 2D parametric PDEs adding the baseline of sampling uniformly at random to represent the lack of active learning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Contributing a benchmark in active learning for PDE solvers fills a needed gap in computational infrastructure for PDE solvers that is key to the central challenge of data efficiency.\nThe article is pedagogical and presents clearly the capability of the benchmark."
            },
            "weaknesses": {
                "value": "The authors should help compare methods of Bayesian active learning and those of the field of design of experiments (DoE), which is missing in the literature review. For example, instead of using a baseline of uniform sample, Latin Hypercube sampling should be provided, as well as more sophisticated DoE methods. This benchmark effort is an opportunity to bridge these areas of research and communities that try to solve the same problem with a slightly different point of view and a different approach.\n\nThe benchmark should broaden the UQ methods by connecting to existing efforts (for example, the open-source UQ 360). Any UQ method that provide a confidence interval should suffice for active learning as the spread of the confidence interval can be a proxy of the uncertainty.\n\nIn the implementation details, the tradeoffs of the choice of taking the spatial average over the features to make make feature-based AL translation invariant are not discussed. It seems that the averaging creates a significant dimensionality reduction that may outweigh the benefits of a translational invariance in terms of data efficiency.\n\nAll the implementations use periodic boundary conditions, which significantly limits the scope of applications."
            },
            "questions": {
                "value": "Could you please add baselines form DoE?\n\nCould you link your benchmark code to existing open source code for uncertainty quantification (e.g. UQ 360)?\n\nCould you explain the tradeoffs of reducing the dimensionality of features vs. implementing translational invariance in terms of data efficiency of the feature-based AL?\n\nCould you add examples that do not use periodic boundary conditions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a modular active learning (AL) framework for training surrogate models of partial differential equation (PDE) solvers. It introduces a numerical solver for generating PDE samples, several surrogate models, batch selection strategies, and acquisition functions designed for active learning within this framework."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The well-designed modular framework provides a solid foundation for further research on active learning in the context of PDEs."
            },
            "weaknesses": {
                "value": "1. Although the framework is tailored for PDE problems, the implemented acquisition functions are orthogonal to PDE problems i.e. they are AL methods that are used in general domains. As a framework of AL for PDE, at least some PDE-specific AL methods such as adaptive sampling [1], also mentioned in Related Work, should be also implemented.\n1. The paper\u2019s scope in terms of the surrogate models, acquisition functions, and types of PDEs studied is quite limited, impacting its practical applicability.\n\n[1] W. Gao and C. Wang, Active Learning Based Sampling For High-dimensional Nonlinear Partial Differential Equations, Journal of Computational Physics, Vol. 475, 2023."
            },
            "questions": {
                "value": "No question"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}