{
    "id": "1W6oINj8ne",
    "title": "BRSSD10k :  A SEGMENTATION DATASET \\\\OF BANGLADESHI ROAD SCENARIO",
    "abstract": "In this paper, we present a novel Bangladeshi Road Scenario Segmentation Dataset designed to advance autonomous driving technologies under the challenging and diverse road conditions of Bangladesh. This comprehensive instance segmentation dataset comprised 10,082 high-resolution images captured across nine major cities, including Dhaka, Sylhet, Chittagong, and Rajshahi, addressing the critical need for region-specific computer vision data in developing countries. Unlike existing autonomous driving datasets that primarily focus on western road conditions, BRSSD10k encompasses a wide range of environments unique to Bangladesh, including unstructured urban areas, hilly terrains, village roads, and densely populated city centers. The dataset features instance segmentation annotations with classes specifically tailored to reflect the distinctive elements of Bangladeshi roads, such as rickshaws, CNGs (auto-rickshaws), informal roadside stalls, and various nonstandard vehicles. To demonstrate its utility as a benchmarking tool for autonomous driving systems, we present comparative results from several state-of-the-art instance segmentation models tested on this dataset, achieving an mAP of 0.441. This evaluation not only showcases the dataset's effectiveness in assessing model performance but also underscores the need for adaptive algorithms capable of handling diverse and unpredictable urban environments in the context of autonomous navigation.",
    "keywords": [
        "Instance Segmentation",
        "Computer Vision",
        "Dataset",
        "Autonomous Driving",
        "Bangadeshi Road"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "BRSSD10k is Bangladesh's first instance segmentation dataset for autonomous driving, with 10,082 high-res images from 9 cities. It offers detailed road element annotations, providing a key benchmark for AI models in diverse South Asian road scenarios",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1W6oINj8ne",
    "pdf_link": "https://openreview.net/pdf?id=1W6oINj8ne",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces BRSSD10k, a segmentation dataset specifically tailored to the unique and diverse road scenarios in Bangladesh. This dataset consists of 10,082 high-resolution images from nine cities across the country, with detailed annotations covering 34 classes that reflect the region's distinct transportation environment. Classes include locally prevalent elements such as rickshaws, CNGs (auto-rickshaws), and informal roadside stalls, which are critical for developing robust autonomous driving systems for Bangladeshi roads."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The authors have compiled a comprehensive dataset with over 10,000 high-resolution images and detailed instance segmentation annotations, covering a diverse range of geographic regions within Bangladesh.\n\n- A rigorous two-stage validation process for annotations ensures high-quality data, which is essential for developing robust and accurate computer vision models.\n\n- Comparative evaluation with multiple state-of-the-art models (e.g., YOLOv5, YOLOv8, YOLOv9) showcases the benchmark's effectiveness and sets a baseline for future research on BRSSD10k.\n\n- The inclusion of region-specific object classes (e.g., rickshaws, CNGs, informal stalls) provides a unique contribution, enabling autonomous systems to better understand and navigate environments outside of structured Western road layouts."
            },
            "weaknesses": {
                "value": "- The dataset only covers limited regions in one country, which is not enough to evaluate the generalization ability of segmentation.\n\n- The quality of the segmentation masks is not satisfactory.\n\n- Certain critical classes, such as traffic lights, construction vehicles, and road blockers, are underrepresented in the dataset.\n\n- The dataset currently lacks nighttime and adverse weather imagery (e.g., rain or fog), which are essential for real-world segmentation.\n\n- The paper only evaluates three versions of the YOLO model, which may limit insights into how BRSSD10k performs across different model architectures. \n\n- There is no analysis on how models trained on BRSSD10k generalize to other datasets or vice versa."
            },
            "questions": {
                "value": "- The authors need to consider including more baselines for evaluation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Privacy, security and safety"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Human faces appear on the road. They are not removed and blurred."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The manuscripts presents a novel road-driving dataset for instance segmentation. The dataset includes more than 10000 high resolution images acquired along 9 cities in Bangladesh. The dataset taxonomy includes 34 classes that reflect typical needs of autonomous driving and regional characteristics. The taxonomy is mostly well-balanced (Figure 2). There are around 6000 training, 2000 validation and 2000 test images. The presented experiments involve object detection with stock models and report mAP50 performance on validation and test datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The dataset will likely prove as a valuable contribution to the field.\n\n- Many stuff classes are annotated (sky, road, wall, fence).\n\n- Little effort is required to extend the dataset for panoptic segmentation."
            },
            "weaknesses": {
                "value": "- it is hard to recommend n+1-th road-driving dataset for publication at a major conference\n\n- dataset focuses on typical images, for which our models are known to work well \n\n- the baseline models address only object detection (some universal segmentation model such as MaskFormer would be a better choice)"
            },
            "questions": {
                "value": "It would make sense to extend the dataset with full panoptic labels.\n\nIt would make sense to cite and discuss related road driving datasets: ACDC, WildDash, FishyScapes, SegmentMeIfYouCan."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a new dataset focused on object detection and segmentation,\ntailored to the specific driving conditions in Bangladesh in terms of its\nappearance and taxonomy.\n\nThe dataset encompasses ~10k camera images collected in Bangladesh using a cell\nphone. The images are sourced from video chunks originating from diverse\nregions, and contiguous footage is sampled to 1 Hz. The frames are annotated\nwith object bounding boxes and segmentation masks.\n\nThe paper motivates the dataset as helpful in developing computer vision\nalgorithms specific to Bangladeshi driving scenes and performs a brief\ncomparative analysis of different YOLO-based models trained on this dataset. The\npaper helpfuly provides metadata like class and geographic distribution\nhistograms as well as many qualitative examples in order to help the reader get\na sense of the dataset.\n\nWhile it is definitely important to promote datasets which cover a diverse range\nof environments, I think the quantitative argument made in this paper to\nmotivate the dataset could be strengthened. For example, the argument could be\nimproved by showing experimental results which demonstrate the limitations of\nother dataset on data collected in Bangladesh."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- [S1] Diverse object classes from multiple cities in Bangladesh, reflecting a\n  unique label distribution that is materially different from other established\n  datasets such as Waymo Open and nuScenes.\n- [S2] The authors also present the results of a few detection baselines based\n  on YOLO, trained and evaluated on this dataset's corresponding splits."
            },
            "weaknesses": {
                "value": "- [W1] The related section could be made a bit more comprehensive. For example,\n  it would be interesting to also discuss other datasets focusing on non-Western\n  streets, such as the dataset introduced in [@traphic]. Even though it's\n  mentioned later in the paper, the BadODD dataset should also be covered in the\n  related work section and in the relevant tables.\n- [W2] While it is helpful to benchmark a few existing models on the proposed\n  dataset, it would be beneficial to also compare these numbers with those from\n  models trained on a mainstream dataset such as CityScapes or Mapillary Vistas.\n  If models trained on a dataset like Cityscapes or Mapillary Vistas fail to\n  perform well on this dataset, that would make for a good quantitative argument\n  for why this dataset will help the community.\n  - As a side-note, even if the taxonomy another dataset won't match the one in\n    BRSSD10k perfectly, this gap could be alleviated by the use of an\n    off-the-shelf VLM, which have been shown to be very good at tasks like open\n    set object detection---see, for example, Grounding DINO [@liu2024grounding].\n- Minor Suggestions\n  - Sections 7.3, 7.4, and 7.5 can be shortened and replaced with more\n    comparisons, or additional details about the dataset or its software\n    development kit. Readers can refer to the corresponding references if they\n    are curious about the specific loss functions used to train these models.\n  - The citation markers seem to be missing parentheses around them. For\n    example, a sentence like \"... complex environments He et al. (2017)\" should\n    be formatted like \"... complex environments (He et al., 2017).\"\n- References:\n  - [@traphic]: Chandra, Rohan, et al. \"Traphic: Trajectory prediction in dense\n    and heterogeneous traffic using weighted interactions.\" CVPR. 2019.\n  - [@liu2024grounding]: Liu, Shilong, et al. \"Grounding dino: Marrying dino\n    with grounded pre-training for open-set object detection.\" arXiv preprint\n    arXiv:2303.05499 (2023)."
            },
            "questions": {
                "value": "- [Q1] How is the dataset split into train/val/test? Do you perform geographic\n  splitting, or is the splitting purely at random?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a road  segmentation dataset for autonomous driving purpose.  It focuses on the scenarios in Bangladeshi and make specific adaptions in class definition and labeling. Validation experiments are conducted."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "It clearly analyzes the practical scenario characteristics in Bangladeshi. The class definition and labeling process fully fits the scenarios. This dataset acts as a valuable resource for developing autonomous driving models in this country. It may also contribute to general vision perception tasks."
            },
            "weaknesses": {
                "value": "1.The authors just use one paragraph to summarize the related datasets without any detailed comparison. I do not think the authors really understand the development of this field as there are only eight references. \n2. The scenarios in the dataset are more likely to be corner cases comparing with the mainstream segmentation datasets. Its universality cannot be verified.\n3.The structure of the manuscript is poorly organized. The logic between sections 3-6 are chaotic.\n4.It is really confusing that the authors validate the segmentation dataset with YOLO.\n5. It is really funny that the GT maps in Figure 3 are wrong."
            },
            "questions": {
                "value": "The authors strongly emphasize that the main motivation of this work is that there lack segmentation datasets in Bangladeshi. It should be clarified that the contribution of a dataset does not lay in its location, but the data quality, diversity, and scale."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}