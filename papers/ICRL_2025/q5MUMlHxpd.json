{
    "id": "q5MUMlHxpd",
    "title": "Voila: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning",
    "abstract": "Multimodal Large Language Models (MLLMs) have become a powerful tool for\nintegrating visual and textual information. Despite their exceptional performance\non visual understanding benchmarks, measuring their ability to reason abstractly\nacross multiple images remains a significant challenge. To address this, we introduce VOILA , a large-scale, open-ended, dynamic benchmark designed to evaluate MLLMs\u2019 perceptual understanding and abstract relational reasoning. VOILA\nemploys an analogical mapping approach in the visual domain, requiring models\nto generate an image that completes an analogy between two given image pairs,\nreference and application, without relying on predefined choices. Our experiments demonstrate that VOILA presents MLLMs with demanding relational reasoning tasks. Through multi-step analysis, we reveal that current MLLMs struggle\nto comprehend inter-image relationships and exhibit limited capabilities in highlevel relational reasoning. Notably, we observe that performance improves when\nusing least-to-most prompting strategies. Comprehensive evaluations on opensource models and GPT-4o show that while the MolmoE-8B model achieves a\nstate-of-the-art performance of 34% and 19% at finding the text-based answer to\nthe questions on easy and hard scenarios, human performance consistently remains significantly higher at 70% on both difficulty scenarios.",
    "keywords": [
        "abstract reasoning",
        "relational reasoning",
        "perceptual understanding",
        "MLLMs",
        "visual analogy"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "To assess MLLMs' perceptual understanding and abstract relational reasoning capacity, we present Voila which applies an analogical mapping strategy to the visual domain.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=q5MUMlHxpd",
    "pdf_link": "https://openreview.net/pdf?id=q5MUMlHxpd",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces VOILA, a benchmark specifically designed to evaluate Multimodal Large Language Models (MLLMs) in perceptual understanding and abstract relational reasoning across images. By using analogical mapping, VOILA requires MLLMs to generate an image that completes an analogy between two image pairs, testing the models' relational reasoning without predefined answer choices. The benchmark includes challenging tasks, with models struggling to match human performance, especially in higher-level reasoning steps. Performance improves with least-to-most prompting strategies, but there remains a substantial gap between the best-performing model and human results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "\u2022 VOILA stands out by focusing on abstract visual analogies, making it a valuable addition to existing MLLM evaluation benchmarks, particularly in assessing perceptual and relational reasoning.\n\n\u2022 The multi-step reasoning approach and comprehensive ablation studies offer a detailed examination of the current limitations in MLLMs."
            },
            "weaknesses": {
                "value": "\u2022 The paper emphasizes that this is a dynamic benchmark. For a static benchmark, once a configuration is provided, it can be fully generated and then annotated for correctness by human evaluators. However, as a dynamic benchmark, once it is generated, how to ensure the correctness of the benchmark and how to make it scalable, which remain challenging and difficult to guarantee. This issue is unsolved in this paper.\n\n\u2022 VOILA\u2019s reliance on GPT-4o for model evaluation makes the evaluation process both resource-intensive and costly for practitioners."
            },
            "questions": {
                "value": "Please refer to Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a new benchmark called VOILA which evaluates the perceptual understanding and analogical reasoning capabilities of multimodal language models, particularly its reasoning capabilities across multiple images. VOILA requires a MLLM to understand relations between images and generate a new image that follows the pattern. This task is open-ended and results are evaluated using a strong LLM. Results indicate that even the best MLLM performs much worse than an average human (30% vs 70%)"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This is generally a well organized and well written paper.\nThe proposed benchmark is interesting and highlights a critical shortcoming in existing MLLMs\nThe benchmark is well curated, there is also manual filtering involved to ensure quality"
            },
            "weaknesses": {
                "value": "This task is not exactly novel, [1] propose a new task that evaluates visual cognition of MLLMs. The new novelty comes from the requirement that MLLM generate the output image. \nThe evaluation is not grounded in existing psychological assessments for instance the reasoning used in [1] is widely used in neurodevelopmental and neuropsychological research.\nGPT4o seems to be poor at identifying relationships (from image) in VOILA, does this affect the evaluation which in turn also uses GPT4o. \n\nReferences\n[1] https://arxiv.org/abs/2406.10424"
            },
            "questions": {
                "value": "Can the authors conduct a small experiment to validate that their evaluation strategy is accurate?\nThe prompt to identify relation asks the model to identify differences in subject, types and actions all in a single prompt. What if this problem was broken down to three sub-prompts? It seems intuitive that performance should improve"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed a new benchmark named VOILA, aiming to evaluate the performance of visual language models in understanding abstract relation understanding by designing visual analogy questions.\nExperiments show that many models can describe the images and idenitify the relationship between images, but cannot apply the relationship very reliably."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The visual analogy perspective of evaluating vision language models is interesting.\n2. The paper gives a very detailed the description of how the benchmark is collected."
            },
            "weaknesses": {
                "value": "1. The analogies considered in this paper seems to be restricted to only numbers, subjects, and actions."
            },
            "questions": {
                "value": "1. One line of related works I think this paper need to mention is on the benchmarking of multiple image understanding, such as works of Muirbench and MIRB, especially MIRB, which has already included visual analogy as one sub task for testing current state-of-the-art visual language models.\n2. The data generation pipeline uses diffusion models to geerate images from given text prompts. I'm wondering how reliable is the diffusion model? how many images are discarded. Perhaps include a performance of human on the final dataset can answer this (I noticed that in table 3 there are some human performance, but why is human performance only tested on the 'Applying relationship' subset?)\n3. In my understanding, this paper only considers analogy on properties like number, subject, and action. Is these enough to cover all possible visual combinations that can form analogies?\n\n\n\nMuirbench: https://arxiv.org/abs/2406.09411\nMIRB: https://arxiv.org/abs/2406.12742"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new benchmark for evaluating multi-modal large language models\u2019 perceptual understanding and abstract relational reasoning through analogy completion. It contributes a dataset creation pipeline, a large evaluation dataset consisting of 10K+ questions, and extensive evaluations of both open-source and proprietary models on this benchmark as well as detailed analyses of their performance on different subtasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The idea of using visual analogy completion for evaluating multi-modal large language models is well motivated and somewhat unique \n- The visual analogy completion problem is well defined and formulated, with clear definitions of the three properties and four rules.  \n- The dataset has been manually cleaned to ensure that the generated images match the texts."
            },
            "weaknesses": {
                "value": "- The experiments can be more complete. Currently, most evaluations focus on the first three steps of the analogy completion task with only a few data points on the image generation stage. \n- The paper presentation can be improved. For example, the writing can be improved to highlight the most insightful and interesting takeaways. \n- While the visual analogy task is interesting, the reviewer is uncertain about the practical value of this benchmark especially given the abundance of multi-modal benchmarks \u2013 for example, why should the community use this benchmark to evaluate models\u2019 relation understanding instead of existing VQA benchmarks that can also test for relation understanding?"
            },
            "questions": {
                "value": "- Did the authors include examples of each subtask in the prompts? \n- The authors mention they evaluated Emu-2 on the image generation subtask in 5.1, but Table 3 is missing its numbers? \n- In Figure 6, it\u2019s interesting that LlaMa 3.2 is the only model with higher performance on Voila-WD than Voila-ND, while all other models exhibit large drops from Voila-ND to Voila-WD regardless of their absolute accuracies. Can the authors provide insights into why this occurs for LlaMa 3.2?\n- Nit: table 4, it\u2019s VOILA-ND in both columns \u2013 should one of them be VOILA-WD instead?\n\nSuggestions: \n- While the authors motivate the visual analogy task by saying that it tests for higher-order reasoning, they break down the task into four subtasks, each of which is an easier task (despite being still difficult to the model). While the reviewer agrees with the authors that this task decomposition enables fine-grained analyses of models\u2019 weaknesses, it\u2019d also be interesting to see an evaluation of the end-to-end analogy completion task. \n- Table 2 can highlight the performance drops from the previous to the next stage\n- The authors mentioned that models struggle with the visual analogy task when images are combined in a collage format due to resolution constraints. It\u2019d be interesting to see if this finding holds with high resolution image collages for multi-modal models that support the AnyRes strategy such as Llava-OneVision."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces VOILA, a benchmark designed to assess multimodal large language models (MLLMs) on tasks requiring visual perception and abstract relational reasoning. VOILA primarily focuses on visual analogical reasoning, where models must generate an image that completes an analogy given two pairs of images. The benchmark includes two subsets, VOILA-WD (with distractions) and VOILA-ND (without distractions), testing MLLMs on varying levels of complexity. Contributions:\n1. A dataset creation pipeline allowing large-scale generation of visual analogy questions, and a large-scale, open-ended benchmark to evaluate MLLMs\u2019 high-level visual reasoning capabilities. \n2. Comprehensive evaluation of state-of-the-art MLLMs, highlighting performance gaps in relational reasoning compared to human baselines.\n3. Ablation studies reveal model limitations in handling complex visual relationships, suggesting areas for improvement in future MLLM development."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The proposed problem is a novel multimodal reasoning problem, which is well-defined and advances abstract relational reasoning in MLLMs\u200b.\n2. The benchmark design is comprehensive, encompassing various rule configurations and levels of task difficulty.\n3. The results reveal the weaknesses of current multimodal LLMs."
            },
            "weaknesses": {
                "value": "1. The paper does not address why the proposed problem is an essential and meaningful problem to solve. In what applications/scenarios is the capability essential?\n2. The comparison between the model and human performance may not be fair: (1) Humans are given two examples. (2) It seems the task for humans is to choose properties from available options, while MLLMs need to predict those properties.\n3. Table 3 takes up a lot of space and has a lot of numbers, but it is barely mentioned or explained in the text."
            },
            "questions": {
                "value": "1. The prompt4 for generating images seems unnecessary for evaluating the \"reasoning\" capabilities of MLLMs. Also, the reviewer is curious how the authors use GPT4o to generate images since it is not clear whether they call another image generation model to generate images in the web interface. And it seems gpt4o API does not support image output. \n2. Why Table 4 and Table 5 only use the selected models for the ablation study?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}