{
    "id": "yRd4loGAhJ",
    "title": "SEAL: Scaling to Emphasize Attention for Long-Context Retrieval",
    "abstract": "In this work, we introduce a novel approach called Scaling to Emphasize Attention for Long-context retrieval (SEAL), which enhances the retrieval performance of large language models (LLMs) over extended contexts. Previous studies have shown that each attention head in LLMs has unique functionality and collectively contributes to the overall behavior of the model. Similarly, we observe that specific heads are particularly related to long-context retrieval and are positively or negatively correlated with retrieval scores. Building on this insight, we propose a cost-efficient, learning-based mechanism to emphasize these heads, improving the model's performance in long-context retrieval tasks. By applying SEAL, we achieved significant improvements in in-domain retrieval performance across various tasks and considerable improvement in the cross-domain document QA task of LongBench. Additionally, when combined with existing training-free context extension techniques, SEAL extends the context limits of LLMs while maintaining highly reliable outputs, opening new avenues for research in this field.",
    "keywords": [
        "large language models",
        "long context",
        "retrieval",
        "attention",
        "supervised fine-tuning"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=yRd4loGAhJ",
    "pdf_link": "https://openreview.net/pdf?id=yRd4loGAhJ",
    "comments": [
        {
            "summary": {
                "value": "This work focuses on scaling to emphasize attention to long-context retrieval, designed to enhance the retrieval performance of LLMs in handling extended contexts. A cost-effective, learning-based mechanism is proposed to improve the model's performance in long-context retrieval tasks, which emphasizes specific attention heads tailored to retrieval tasks. Experimental results demonstrate superior performance over the compared baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper is well-organized and easy to read. \n2. The proposed method presents a reasonable approach for long-context retrieval by identifying the key components of Transformer architecture to boost retrieval performance. \n3. The approach is practical and has the potential for broad application in various RAG settings."
            },
            "weaknesses": {
                "value": "1. The term \"cost-efficient\" is not clearly defined, resulting in ambiguity when assessing the cost-effectiveness of the approach. The strategy of identifying key components initially and subsequently fine-tuning these components may prove to be computationally intensive. It would be beneficial to provide details regarding the computational time involved in this process.\n2. A more thorough evaluation would benefit from comparisons with a broader range of advanced baseline models. Currently, the proposed method is compared against only one simple. Including more sophisticated long-context modeling methods and state-of-the-art techniques would better validate the effectiveness of the proposed method.\n3. To confirm the versatility of the proposed method, it would be beneficial to conduct experiments on different LLMs of varying sizes."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces SEAL (Scaling to Emphasize Attention for Long-context retrieval), a novel attention scaling approach that improves retrieval performance for long-context tasks in Large Language Models (LLMs). It addresses the challenge of performance degradation over extended contexts, particularly in retrieval tasks. SEAL fine-tunes specific attention heads or channels using a minimal amount of training data, leading to significant improvements in long-context retrieval across various benchmarks. The paper focuses on cost-efficient enhancement of long-context capabilities without altering the model\u2019s learned behavior."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. SEAL presents an innovative approach by leveraging attention head/channel scaling to enhance long-context retrieval.\n2. The method uses very few trainable parameters and requires minimal training data, making it highly efficient."
            },
            "weaknesses": {
                "value": "1. The term \u201clong-context retrieval\u201d is ambiguous. It would be clearer to refer to \u201cretrieval tasks that have long contexts,\u201d which directly emphasizes tasks like passage retrieval or number retrieval.\n2. The paper lacks explicit detail about which context extension techniques are used. For example, Figure 6 mentions the use of Self-Extend, but no experiments isolating its performance are provided.\n3. Logical Flow in Writing: Certain parts of the paper are difficult to follow due to writing issues such as ambiguous expressions, inconsistent time tense, and occasional typographical errors (e.g., \u201cbiases\u201d instead of \u201cbias\u201d).\n4. The distinction between \u201cin-domain\u201d and \u201cout-of-domain\u201d in the experiments is confusing. Specifically, if \u201cin-domain\u201d refers to training on retrieval tasks, why are the same datasets used for both \u201cin-domain\u201d and \u201cout-of-domain\u201d experiments?"
            },
            "questions": {
                "value": "1. What specifically constitutes \u201clong-context retrieval\u201d? Could the authors clarify this definition and provide more precise terminology?\n2. Why are different LLMs used in Figures 5 and 6? Is there a specific reason for the model changes, and how do these variations impact the comparability of the results?\n3. Can the authors provide experiments isolating the effect of Self-Extend in Figure 5 to verify its individual impact on performance?\n4. What is the rationale behind using the same datasets for \u201cin-domain\u201d and \u201cout-of-domain\u201d experiments in Table 3? How is \u201cout-of-domain\u201d defined in this context, and what criteria differentiate the two?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an approach called Scaling to Emphasize Attention for Long-context retrieval (SEAL), which emphasizes specific heads or channels (attention outputs) particularly related to long-context retrieval by efficiently adjusting the strength of each attention component. The authors claimed that SEAL achieves significant improvements in in-domain retrieval performance and cross-domain document QA tasks, also extends the context limits of LLMs while maintaining highly reliable outputs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper proposes SEAL to efficiently adjusting the strength of each attention component, and achieves superior performance to various LLM baselines in long-context retrieval.\n2. The content, figures, and tables of the paper provide a detailed explanation and analysis of the motivation, methods, and experiments, facilitating the readers' understanding."
            },
            "weaknesses": {
                "value": "1. The experimental results in Table 1 show that SEAL-H and SEAL-C require fewer parameters than Baseline and SEAL-L. However, their performance does not consistently surpass SEAL-L in long-context scenarios, failing to demonstrate the authors' claims.\n2. The experiments only select SEAL-L as the baseline, it should include other PEFT methods for comparison."
            },
            "questions": {
                "value": "1. Except fewer parameters, what other advantages does SEAL have over LoRA or other PEFT methods? Since the parameters of SEAL-L is also small compared to LLMs, what are the unique application scenarios for SEAL?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel and practical method, SEAL, to improve the long-context retrieval ability of LLMs. \n\nFirst, through perturbation experiments, it finds a certain attention head or a certain channel in it can cause a positive or negative effect on long-context retrieval accuracy.\n\nSecond, it demonstrates directly scaling the hidden states of these heads or channels can indeed improve the retrieval accuracy of LLMs. \n\nThird, it adds trainable scale factors into the model and use a small amount of samples of retrieval tasks to fine-tune the model. The results show SEAL can remarkably improve the long-context retrieval ability of LLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper discovers that a certain attention head can cast a remarkable positive or negative effect on long-context retrieval accuracy, even as well as a certain channel. This is interesting and helpful for us to further understand the role of the internal modules of LLMs.\n\n2. The proposed method, SEAL, is very cost-effective, which only needs very few training samples and tuned parameters.\n\n3. There are enough evaluation results of various models to demonstrate the method\u2019s effect."
            },
            "weaknesses": {
                "value": "1. Narrow scope\n\nThe method seems to only be applicable for classic retrieval tasks such as NIAH, and the training data is also the same types of tasks. It will not be surprising that this leads to an improvement, since this task has been too simple, fixed and formulaic, which may represent a narrow application scope for this method. It would be better to train and test on more tasks such as Knowledge-QA.\n\n2. No unique advantages\n\nThe author should empirically test whether the time or space required by SEAL is significantly less than that of LoRA. Otherwise it cannot show significant superiority of SEAL compared to LoRA. Because the parameters tuned by LoRA are already very few. Though SEAL can theoretically tune much less parameters, it may not significantly save much time. \n\n3. There is little detailed description about the procedures of the method in the abstract or introduction. This will make it hard for readers hard to grasp the method quickly. There usually should be a paragraph included in the introduction to describe the specific operation of the method.\n\n4. The curve of data points in Figure 4 (a) may be too small, making it hard to clearly see the changes."
            },
            "questions": {
                "value": "1. Can you demonstrate the unique advantages of your method compared to LoRA through more experiments?\n2. Can you train and test on more various task types to demonstrate the generalization of your method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}