{
    "id": "DSsSPr0RZJ",
    "title": "DSBench: How Far Are Data Science Agents from Becoming Data Science Experts?",
    "abstract": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.",
    "keywords": [
        "data science",
        "agent",
        "benchmark",
        "llm"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=DSsSPr0RZJ",
    "pdf_link": "https://openreview.net/pdf?id=DSsSPr0RZJ",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces DSBench, a comprehensive data science benchmark designed to assess the performance of data science agents on real-world tasks. DSBench integrates the challenges from ModelOff and Kaggle competitions, creating a realistic environment that covers 466 data analysis and 74 data modeling tasks. To enable fair comparison, the paper proposes the Relative Performance Gap metric, which normalizes various evaluation metrics for data modeling tasks. Evaluation of state-of-the-art models, including GPT-4o, Claude, and Gemini, reveals that DSBench remains challenging, with significant performance gaps between these models and human capabilities."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Novelty** \n- This paper offers a fresh contribution to data science by introducing DSBench, a new benchmark that evaluates data science agents under realistic task conditions derived from ModelOff and Kaggle competitions. It pushes the boundaries of traditional benchmarking.\n\n**Quality** \n- The design of DSBench, with its comprehensive task types and Relative Performance Gap (RPG) metric, demonstrates rigor in addressing evaluation inconsistencies across various modeling tasks. \n\n**Clarity** \n- Task designs, methods, and performance comparisons are clear and well-organized. They contain many details but are not hard to follow.\n\n**Significance**\n- DSBench sets a new standard in evaluating data science agents, driving advancements in realistic, end-to-end task performance. Its contributions are useful to future advancements of intelligent, autonomous data science agents."
            },
            "weaknesses": {
                "value": "- I'm bit unsure of the robustness and persuasiveness of the RPG metric is valid. It could be beneficial to further assess how well the RPG reflects actual performance across varying data types and task complexities.\n- Although Kaggle tasks are highly relevant, they often focus on a narrow subset of domains (e.g., retail, finance)."
            },
            "questions": {
                "value": "1. Can authors please explain if the metric and dataset are sufficient to cover the real-world diversity that the benchmark aims to address?\n2. Explain why RPG is able to reflect actual performance across varying data types and task complexities."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a new benchmark for automated data science, DSBench.  This\nencompasses both excel exercises from ModelOff and kaggle competitions.  DSBench\nalso includes metrics for evaluating success in both kinds of tasks.  The paper\nevaluates several open and closed source LLMs/VLMs.  The key take away is that\nSOTA architectures are still far away from achieving human-level performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Originality**: This is chiefly an engineering paper, it introduces no novel techniques or ideas.  DSBench collates existing resources.\n\n**Quality**: The key contribution is DSBench, which I am confident took quite some effort to set up.  All in all, this contribution seems to be of good quality: it encompasses a large number of relevant tasks which go beyond what the literature currently offers.  It also comes with some rather natural metrics.  The interesting bit is the evaluation, which on the one hand shows that DSBench can indeed be used as intended, and on the other points out a performance gap for existing architectures.  The findings are otherwise quite intuitive: newer models perform better, more complex tasks are harder to solve.  The real contribution is the research that DSBench will enable.\n\nUnfortunately, as a non-expert (my work on automating data science predates LLMs), I cannot assess the overlap between DSBench and recent works in this area.\n\n**Clarity**: The text is generally readable, with a few linguistic quirks here and there.  The figures are easy to understand.\n\n**Significance**:  Automated data science is a central topic nowadays.  It is not impossible that DSBench will provide a significant boost to auto-DS.  But again, this depends on overlap with existing work, which I am not overly familiar with.  As a result, I have decided to grade conservatively the contribution aspect of the paper."
            },
            "weaknesses": {
                "value": "**Quality**: The only relatively minor issue I'd like to point out concerns Section 3.3, which promises discussing the errors made by LLMs, but does not provide any details, really.  I think this should be amended."
            },
            "questions": {
                "value": "Plase see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "DSBench introduces a benchmark to test the performances of LLM or VLM agents. It contains data analysis and modeling tasks sourced from Kaggle and ModelOff competitions. Compared to other similar benchmarks, the benchmark is more realistic in that it contains data files, tables, and images and provides executable evaluation functions to validate the answers. The paper tests various models and agent systems and found them to pale in comparison to human data scientists."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper collects a set of data science challenges from Kaggle and ModelOff. By releasing the code and data, the community can use it to measure the performances of their agent tools. The paper demonstrates through experiments that there exists huge gaps between data science agents and humans, demonstrating that the benchmark is not saturated."
            },
            "weaknesses": {
                "value": "In general, the paper is well written. The only concerns relate to the sustainability of the benchmark; by releasing the dataset, there is a high chance that it will be, either intentionally or unintentionally, incorporated into the training data for LLMs or VLMs. It seems like this possibility has not been considered by the authors and their explanation for the correlation between accuracy and year of release (Table 5, L416-L421) is weak. Some critical information is missing and it affects my judgement. I would be happy to change my score if the authors can clear my potential misunderstandings."
            },
            "questions": {
                "value": "1. In Table 3, what does \"context length\" refer to? Is it the number of English words, characters, or tokens?\n2. The paper assumes that the LLMs should be able to generate the answers from the data files. Is it possible for the LLMs to know the answers to the questions from pretraining? For example, the answers to the challenge may be discussed in a Reddit forum that has been scraped in pretraining.\n3. In Figure 2(b), what are A-F, A-I, and A-D?\n4. L265: what is N in \\hat{F} = \\mathcal{G}(E,N,S,M)?\n5. Figure 3: Can you define m0, m1, ..., m17, perhaps as a separate table in the Appendix?\n6. Please define the versions of the model used in the main paper instead of relegating it in the Appendix.\n7. What are the settings for the models (e.g., temperature)? Are the experiments conducted multiple times? If so, how are they aggregated?\n8. For the analyses (e.g., L404 -- L421), it would be nice to state if they have been corroborated by prior research. For example, Qian et al. has also observed that ML agents perform better on older Kaggle challenges and hypothesize that older challenges have more data leakage into pretraining data.\n9. Can you provide a unified script to download the datasets, perhaps using an API, process the downloaded data and run the experiments? It would help for readers to verify the results.\n10. L406-L409: Figure 4 shows only GPT-4o, AutoGen and Gemini, but your text references Llama3-8B and GPT 3.5.\n11. How do I interpret the RPG of human? Doesn't 'human' represent the best human generated answer and RPG compares against the best human?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces DSBench, a comprehensive data science benchmark containing 466 data analysis tasks and 74 data modeling tasks sourced from ModelOff and Kaggle competitions. Compared to existing benchmarks, DSBench provides a more realistic evaluation environment, encompassing long-context understanding, multimodal task backgrounds, large data file processing, and multi-table structure reasoning. Through evaluation of state-of-the-art LLMs, LVLMs, and agents, the study finds that they still struggle with most tasks, with the best agent achieving only 34.12% accuracy in data analysis tasks and a 34.74% Relative Performance Gap (RPG) in data modeling tasks."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "### 1. Originality & Vision\n- Pioneering creation of a comprehensive real-world data science benchmark, analogous to SWE-bench in software engineering, marking a significant step toward AGI evaluation\n- First benchmark to integrate both data analysis and modeling tasks in realistic settings with complex data structures and multimodal contexts\n- Novel introduction of RPG (Relative Performance Gap) metric that effectively normalizes performance across diverse modeling tasks\n- Innovative approach to testing both pure language understanding and tool utilization capabilities\n- Make consideration of not only the performance but also the cost\n\n### 2. Technical Robustness\n- Rigorous task collection methodology, carefully curated from established platforms (ModelOff and Kaggle) ensuring real-world applicability\n- Comprehensive coverage of data science tasks: 466 analysis tasks + 74 modeling tasks, spanning different complexity levels and domains\n- Sophisticated evaluation framework that considers:\n  * Multi-table reasoning capabilities\n  * Long-context understanding\n  * End-to-end solution generation\n  * Tool integration and utilization\n  * Multiple modalities processing\n\n### 3. Practical Significance\n- Direct application to real-world data science scenarios, bridging the gap between academic benchmarks and practical challenges\n- Clear identification of current AI systems' limitations in data science tasks:\n  * Understanding complex data relationships\n  * Handling multi-step reasoning\n  * Managing tool interactions\n- Provides valuable insights for developing more capable data science agents by revealing specific areas where current models fall short\n- Sets a new standard for evaluating AI systems' practical data science capabilities, essential for progressing toward AGI\n\n### 4. Research Impact\n- Creates a foundation for systematic evaluation of data science capabilities in AI systems\n- Enables quantitative comparison of different AI approaches in real-world data science scenarios\n- Provides a roadmap for developing more capable AI systems that can handle complex, real-world data science tasks\n- Serves as a crucial benchmark for measuring progress toward AGI in the domain of data science"
            },
            "weaknesses": {
                "value": "### 1. Statistical Rigor in Dataset Scale\n- While 540 samples is reasonable given the scarcity of high-quality ModelOff & Kaggle competitions, the paper could benefit from:\n  * Reporting confidence intervals through multiple experimental runs\n  * Conducting bootstrap analysis to estimate the robustness of performance metrics\n  * Providing power analysis to justify the sample size\n- The paper could discuss how the current sample size was determined and what would be an ideal size for future extensions\n\n### 2. Evaluation Methodology\n- Human baseline performance (22 competitions) could be strengthened by:\n  * Including more expert evaluators per task\n  * Reporting inter-rater agreement scores\n  * Documenting the selection criteria for human experts\n- The evaluation process could benefit from:\n  * Explicit discussion of potential biases in task selection\n  * Analysis of task difficulty distribution\n  * More detailed failure case studies with expert annotations"
            },
            "questions": {
                "value": "In Table 1's comparison section, I suggest reversing the criteria for \"Exec. Eval.\", \"Code only\", and \"Env. Fix.\" to negative statements. This way, checkmarks would indicate DSBench's unique advantages where other benchmarks fall short, making it easier for readers to quickly grasp DSBench's contributions. This would better highlight how DSBench addresses limitations in existing benchmarks and make the comparison more intuitive."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper constructs DSBench, a benchmark focused on evaluating LM-based data science agents through realistic tasks sourced from ModelOff and Kaggle competitions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. I believe the dataset provided by the authors can, to some extent, reflect a model's ability to tackle data science tasks, and even today, it remains quite challenging, serving as a non-trivial evaluation benchmark.\n2. The authors conducted some interesting analyses across various dimensions for different models and different data science tasks (e.g., examining task completion rates relative to task release time, the correlation between task difficulty and context length, etc.), offering potential insights into the development of LM as agents."
            },
            "weaknesses": {
                "value": "1. **Data Source:**\n- All the data analysis tasks mentioned by the authors at line 106 are related to finance. I would appreciate it if the authors could clarify this point in the paper and explain why other types of analysis tasks are not as suitable. This concern arises because ModelOff is actually a global financial modeling competition.\n- Similarly, for data modeling, it seems this is also influenced by the fact that there are numerous modeling task competitions on Kaggle.\n\nI wonder if the authors have explored more platforms or data sources and could explain why they were not suitable for evaluating data science agents?\nAdditionally, in my opinion, competitions are not always the closest representation of the real world. For example, as far as I know, Spider2-V[1] incorporates a lot of tools and software from industrial data pipelines. Could this be a more realistic measure of real-world scenarios?\n\n2. **Evaluation Metrics:**\n- I fully understand that collecting and building complex evaluation environments is a considerable engineering task. However, if the evaluation is based solely on competition platforms and existing output-only metrics, it seems that it may not fully capture the comprehensive capabilities of data science agents. This is similar to what the authors mentioned in lines 153 and 154, such as extracting insights, proper data handling, etc.\n\n3. **Need better presentation, especially for some tables and figures:**\n- I noticed in Figure 4, some of the models do not have the reported accuracy (does this mean zero accuracy?); while the width of the bars are not set as the same. However, I do not find clear explanation to these.\n- I believe transforming Table 5 into a line chart with corresponding accuracy values would clearly illustrate the trend in accuracy over time.\n\n[1] Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?, NeurIPS 2024."
            },
            "questions": {
                "value": "1. As highlighted in Table 1, What is the purpose of distinguishing tables from data files? What is the difference between tables and data files? I would like the authors to clarify how they treat these two types of data samples during evaluation.\n\n2. I also find the taxonomy somewhat difficult to understand. For example, \"tables\" and \"excels\" are categorized separately, and I hope the author can clarify the distinctions between these categories more clearly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}