{
    "id": "KYipmCMmSO",
    "title": "Characterizing the Training Dynamics of Private Fine-tuning with Langevin Diffusion",
    "abstract": "We show that differentially private full fine-tuning (DP-FFT) can distort pre-trained backbone features based on both theoretical and empirical results. We identify the cause of the distortion as the misalignment between the pre-trained backbone and the randomly initialized linear head. We prove that a sequential fine-tuning strategy can mitigate the feature distortion: first-linear-probing-then-fine-tuning (DP-LP-FFT). A new approximation scheme allows us to derive approximate upper and lower bounds on the training loss of DP-LP and DP-FFT, in a simple but canonical setting of 2-layer neural networks with ReLU activation. Experiments on real-world datasets and architectures are consistent with our theoretical insights.   We also derive new upper bounds for 2-layer linear networks without the approximation. Moreover, our theory suggests a trade-off of privacy budget allocation in multi-phase fine-tuning methods like DP-LP-FFT.",
    "keywords": [
        "differential privacy",
        "convergence",
        "fine-tuning theory",
        "transfer learning theory",
        "langevin diffusion",
        "gradient flow"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We provide a quantitative analysis of training and neuron dynamics in private fine-tuning.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=KYipmCMmSO",
    "pdf_link": "https://openreview.net/pdf?id=KYipmCMmSO",
    "comments": [
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "Thanks for your positive feedback!\n\n**Weakness**\n> \u201cIn practice, trainings are usually done in a discrete manner (say t=1,2,...), but Lagenvin diffusion is a continuous approximation for $t>0$. This gap may hamper the generality of this work's findings. For example, if $\\Delta t$ in Theorem 3.3 belongs to [0,1], then feature distortion might not be an issue, because we start from t=1. Therefore, it would be helpful if authors could provide a short discussion of the value of $\\Delta t$, or name some driving factors that may significantly affect the value of $\\Delta t$\u201d.\n\nResponse:\n\nThank you for highlighting the distinction between continuous Langevin diffusion and the discrete time steps used in practical training. This is an important point, as it helps clarify how theoretical findings based on continuous dynamics translate to real-world implementations. We will add a short discussion in our paper on this issue.\nTo approximate continuous Langevin diffusion in discrete time steps, one can use the Euler\u2013Maruyama method, a standard numerical approach for solving stochastic differential equations. This method discretizes the continuous process by mapping the time interval $\\Delta t$ into discrete steps of size $\\eta$, allowing us to approximate the dynamics as follows:\n$$\\theta_{t+1}=\\theta_t-\\eta\\nabla\\mathcal{L}(\\theta_t)+\\sqrt{2\\eta\\sigma^2}\\xi_t$$,\nwhere $\\eta$ is the discrete step size, $\\sigma$ is the noise scale, and $\\xi_t$ represents Gaussian noise at each step. By choosing an appropriate $\\eta$, we approximate the continuous process well over discrete steps, allowing theoretical insights into the continuous process to inform practical implementations.\nAs for the value of $\\Delta t$ in Theorem 3.3, it represents the initial time interval during which feature distortion occurs. In practical terms, the rate and degree of feature distortion can vary based on factors such as:\n- The initialization scale of model parameters, particularly the linear head,\n- The learning rate in DP-SGD, which affects the stability and alignment of features in early training stages,\n- The noise scale $\\sigma$, as higher noise may delay alignment or lead to greater initial feature distortion.\nWe also provide a detailed description in remark F.2, Appendix F in our updated paper.\n\n**Question**\n> \u201cTypos in Equation (10), Line 1148\u201d.\n\nResponse:\n\nThank you for catching that error. Yes, the time range in Equation (10) is indeed a typo. We appreciate your careful reading, and we will correct Equation (10) and Line 1148 in the revised version of our paper."
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "Thanks for your positive feedback!\n\n**Weakness**:\n\nReview:\n> \u201cThere is a lack of comparison of feature distortion with DP-LP.\u201d\n\nResponse:\n\nIn the DP-LP (Differentially Private Linear Probing) method, only the final linear layer, or \u201chead,\u201d is updated, while the pre-trained backbone features remain frozen. Since the backbone features are not fine-tuned, feature distortion does not occur under DP-LP. Thus, a comparison of feature distortion with DP-LP is unnecessary, as there is no risk of altering the backbone representations in this method.\n\nFor additional context on why freezing features prevents distortion, see:\n\n- Kaiming He, Haoqi Fan, YuxinWu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Computer Vision and Pattern Recognition (CVPR), 2020.\n- Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, Percy Liang. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. ICLR 2022.\n\n**Question**:\n> \u201cIn line 199 , why are subspaces separated by the two cones\n\nResponse:\n\nThe separation of subspaces follows directly from Assumption 3.1. This assumption leads to the convexity of cones $S_{+}$ and $S_{-}$, each containing all positive and negative data points, respectively. Hancheng Min et al. provide a rigorous proof of this convex cone structure in Appendix C of their paper:\n- Hancheng Min, Enrique Mallada, and Ren\u00e9 Vidal. Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization. ICLR 2024."
            }
        },
        {
            "title": {
                "value": "Response to Q3"
            },
            "comment": {
                "value": "Question:\n> \u201cPublic pre-training and private fine-tuning approaches have been seriously questioned recently. I would appreciate the author's thoughts on the recent position paper by Tram\u00e8r et al. (2024).\u201d\n\nResponse:\n\nWe generally agree with the points raised by Tram\u00e8r et al as guiding principles. That being said:\n1. On the utility critique, there are use cases where there exists public data (that can be responsibly curated) that is representative of the private domain. For example, natural language on public web forums has similarities to speech patterns on phone messaging apps (while not being identical). While their observations are generally true (e.g., pretraining on ImageNet won\u2019t help a Sarcoma classifier), there are still important use cases where pre-training on public data can help.\n2. On the privacy critique of public pretraining, we feel the points brought up by Tram\u00e8r et al are not just a weakness of pretraining on public data, but of differential privacy as a whole, which is not nuanced enough to capture contextual integrity. Indeed, there has been work (Cummings et al, CCS 2021) showing that users do not understand or appreciate the privacy affordances of differential privacy. This does not mean that DP as a whole should not be studied, but it broadens the scope of interesting privacy questions to include alternative metrics and techniques that are more aligned with user expectations.\n3. More generally, we do not think that topics should be considered \u201cnot worth studying\u201d simply because they are out of fashion or controversial. There are responsible and irresponsible ways of deploying DP finetuning, as with any technology. We are trying to understand the foundations of a problem, which is separate from the (very important) problem of understanding how to deploy technologies responsibly.\n4. There are some potential solutions to the problems Tramer et al. mentioned:\n    - Use synthetic data (e.g. the random priors used in Tang et al. 2023).\n\n        \u201cLearning to See by Looking at Noise\u201d by Baradad et al. (NeurIPS 2021).\n    - Do differentially private pre-training.\n\n        \u201cViP: A Differentially Private Foundation Model for Computer Vision\u201d by Yu et al. (ICML 2024)."
            }
        },
        {
            "title": {
                "value": "Response to Q2"
            },
            "comment": {
                "value": "Question:\n> \u201cHow realistic are Assumptions 3.1 and 3.2? Were they experimentally validated?\u201d\n\nResponse:\n\nWe discussed these assumptions in our response to Weakness-3. Additionally, empirical results from Hancheng Min et al. (2024) align with Assumption 3.1, providing practical evidence that supports its validity.\n\nYour question highlights a fundamental challenge in deep learning theory\u2014the gap between theoretical assumptions and empirical realities. Developing strong theories often requires certain idealized assumptions on the data that may not align with real datasets like ImageNet. In deep learning theory, there are typically two main approaches to making assumptions on data:\n\n1. Statistical Assumptions: These assume distributional properties of data, often unrelated to real datasets but motivated by theoretical goals. For instance, Lee et al. (NeurIPS 2022) in \u201cConvergence for Score-Based Generative Modeling with Polynomial Complexity\u201d assume the data distribution satisfies a log-Sobolev inequality to leverage stochastic differential equation theory.\n2. Geometric Assumptions: These assume certain geometric patterns in data points, which is common in studying training dynamics of multi-layer networks. For example:\n    - Kumar et al. (ICLR 2022) assume that out-of-distribution (OOD) data points are perpendicular to training data. (https://openreview.net/forum?id=UYneFzXSJWh)\n    - Phuong and Lampert (ICLR 2021) assume orthogonal separability.\n    - Wang and Pilanci (ICLR 2022) and Min et al. (ICLR 2024) also use geometric assumptions relevant to neural network convergence and alignment.\n3. Implicit assumptions: assume that a family of abstract loss functions that have certain properties used in standard optimization theory, such as convexity, PL condition, and smoothness. These assumptions restrict the properties of loss landscapes and thus implicitly restrict properties of the training data.\n    - Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, Michael I. Jordan. \u201cHow to Escape Saddle Points Efficiently\u201d. ICML 2017."
            }
        },
        {
            "title": {
                "value": "Response to Q1"
            },
            "comment": {
                "value": "Question:\n> \u201cIs there a reason why Langevin diffusion is defined differently from Ganesh et al. (2023b)?\u201d\n\nResponse:\n\nOur definition of Langevin diffusion is equivalent to that of Ganesh et al. (2023b), with a difference in notation. In our work, we use the noise scale $\\sigma$, which directly corresponds to the noise multiplier in DP-SGD, while Ganesh et al. use $\\frac{1}{\\beta}$ for the same term. We chose $\\sigma$ for consistency with DP-SGD terminology. When we apply the Euler\u2013Maruyama method on our Langevin diffusion, we obtain a discrete update that corresponds to DP-SGD. For our analysis with clipping, please refer to \u201cAppendix F: Theory with Clipping\u201d.\n\nIt\u2019s also worth noting that there are two equivalent definitions of Langevin diffusion commonly used in the literature:\n- \u201cOn the universality of Langevin diffusion\u201d (https://arxiv.org/abs/2204.01585v3) by Ganesh et al.\n- \u201cInitialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks\u201d (https://proceedings.neurips.cc/paper_files/paper/2023/hash/1165af8b913fb836c6280b42d6e0084f-Abstract-Conference.html) by Ye et al.\n\nBoth definitions provide similar privacy guarantees, as shown in Lemma 2.1 of the first paper and Theorem 3.1 of the second."
            }
        },
        {
            "title": {
                "value": "Response to W5 (Part 2)"
            },
            "comment": {
                "value": "8. The assumptions used for Section 5.2 are based on the following papers:\n    - Etienne Boursier, Loucas Pullaud-Vivien, and Nicolas Flammarion. Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs. In Advances in Neural Information Processing Systems, volume 35, pages 20105\u201320118, 2022. In this paper, the neuron alignment is carefully analyzed for the case all data points are orthogonal to each other.\n    - Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, Percy Liang. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. ICLR 2022. They assume that the pre-trained encoder has been orthogonalized to have orthonormal rows.\n    - Nilesh Tripuraneni, Michael I. Jordan, and Chi Jin. On the theory of transfer learning: The importance of task diversity. NeurIPS 2020.\n9. About notation $D$: The notation $D$ denotes the imbalance matrix (Definition E.8 in Appendix E, Line 1905-1909). Prior work on gradient flows has found that the imbalance matrix remains invariant over the evolution of gradient flows modeling gradient descent. For example,\n    - Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. ICML 2018.\n    - Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. NeurIPS 2018.\n10. Corollary 5.3 is derived by comparing the expected loss upper bounds provided in Proposition 5.2 (for DP-LP-FFT) with those in Theorem E.24 (for DP-FFT). These comparisons allow us to predict the relative performance of DP-LP-FFT under certain conditions.\n11. Explanation of the proof of Theorem 3.3:\n    - Equation (28): this equation results from expanding the derivative of the cosine similarity between $w_j$ and $\\bar{x}_{c(j)}$. It follows directly from differentiating the cosine similarity expression.\n    - High probability guarantee ($1-2^{-h}$): The probability $1-2^{-h}$ reflects the likelihood that initialization causes feature distortion, which depends solely on the sign of each entry in the linear head $v$. With zero-mean Gaussian initialization (commonly used in deep learning frameworks), each entry $v_j$ has a probability of 1/2 of being positive or negative. Given $h$ independent entries in the linear head, the overall probability of alignment (sign consistency) is $1-2^{-h}$."
            }
        },
        {
            "title": {
                "value": "Response to W5 (Part 1)"
            },
            "comment": {
                "value": "1. Tilde notation: The notation (x,y) \u223c (x\u2019,y\u2019) means that y=y\u2019. We will remove this notation and just write $y=y\u2019$ in our paper.\n2. Gaussian initialization: We make this assumption based on the following papers:\n    - Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, Percy Liang. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. ICLR 2022. In their experiments, they initialize the linear head with zero-mean gaussian distribution.\n    - Xinyu Tang, Ashwinee Panda, Vikash Sehwag, Prateek Mittal. Differentially Private Image Classification by Learning Priors from Random Processes. NeurIPS 2023. In their experiments, they initialize the linear head of the WideResNets with zero-mean gaussian distribution.\n3. \u201cOptimality\u201d: Thanks for pointing out the ambiguity of \u201coptimality\u201d. It simply means that $w_j$ perfectly aligns with the mean data direction of a certain label.\n4. Ganesh\u2019s paper: The final version of Ganesh\u2019s paper, published in Conference on Learning Theory, is based on the third version of their arxiv paper. Follow this link: https://arxiv.org/abs/2204.01585v3. Our theorem 4.1 is based on their Lemma 2.1. Similar results can be found in Theorem 3.1 of \u201dInitialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks\u201d by Ye et al. (NeurIPS 2023).\n5. Explanation of Theorem 4.2 and 4.3:\n    - Explanation of Theorem 4.2:\n        - Interpretation of constants: $A_1$ and $A_2$ respectively measures the maximum/minimum alignment between the pre-trained features $w_j$ and the data points $x_i$. $B_1$ and $B_2$ are the noise scale multiplied by the norms of the pre-trained features.\n        - Limit behavior: If we take limit $t\\rightarrow\\infty$, then we get a lower bound limit $\\frac{B_1}{A_1}$ and an upper limit $\\frac{B_2}{A_2}$. When there is only one feature vector and one data point, the upper limit and lower limit are equal and we obtain an exact dynamics for the loss.\n        - Effect of noise: If we only increase the noise scale $\\sigma$ and fix other initial settings, $B_1,B_2$ are larger. As a result, the expectation of the loss would decrease faster but cannot get close to zero because of the large noise. The faster convergence induced by noise is caused by the curvature of the loss landscapes. The second term $\\sigma^2\\mathrm{tr}(H_{\\theta}\\mathcal{L})$ of Ito\u2019s Lemma (Equation 2) introduced the curvature property of the loss function $\\mathcal{L}$ into our analysis.\n        - Effect of pre-training: If we use a bad pre-trained encoder (i.e. decrease the alignment between the pre-trained features and the data points), the loss lower bound is further away from zero. This agrees with the intuition that given a bad backbone, linear probing cannot achieve good performance.\n        - Order of convergence: the linear probing setup gives linear and convex properties. So we obtain exponential convergence, which agrees with the standard results in strongly convex settings.\n    - Explanation of Theorem 4.3:\n        - Limit behavior: If we take limit $t\\rightarrow\\infty$, then we get a lower bound limit $\\frac{C_l}{B_l}=\\frac{\\sigma^2(1+\\mu^2)}{4}$ and an upper limit $\\frac{C_u}{B_u}=\\frac{\\sigma^2}{2\\mu^2}$.\n        - Effect of noise: If we only increase the noise scale $\\sigma$ and fix other initial settings, the lower limit and upper limit both increase. This indicates that the loss cannot get close to zero because of the large noise.\n        - Effect of data separability: If we only increase the metric $\\mu\\in[0,1]$ of data separability, i.e. let the training data to be more separable, the upper limit $\\frac{\\sigma^2}{2\\mu^2}$ decreases. Meanwhile, the upper limit and lower limit get closer. This indicates that the loss is expected to converge to a smaller value in the end as the training task becomes easier.\n        - Effect of pre-training: the limit of the upper and lower bounds does not explicitly depend on the pre-training conditions. The intuition is that for our simplified setting, with different pre-trained backbones, full fine-tuning the whole model would finally produce similar performance conditioned on the training data and the noise.\n6. According to the proof of Theorem 5.1, when we fix the total training time $T$, the necessary time $t_{lp}$ for DP-LP increases as we increase the noise scale $\\sigma$. So the time proportion $\\frac{t_{lp}}{T}$ of DP-LP increases throughout the whole fine-tuning process. According to Theorem 4.1, the privacy budget proportion of DP-LP depends on the time proportion of DP-LP. Therefore, a greater proportion of the privacy budget should be allocated to DP-LP when the total privacy budget is smaller.\n7. Comments on Theorem 5.1: Sorry for the confusion. In this theorem, we use r as a high-probability upper bound for the privacy budget necessary to do linear probing. $\\rho$ denotes the probability commonly used in inequalities derived by concentration bounds."
            }
        },
        {
            "title": {
                "value": "Response to W4"
            },
            "comment": {
                "value": "Review:\n> \u201cThe experimental setup lacks details on the clipping threshold, a key parameter influencing DP-SGD performance and necessary for reproducibility.\u201d\n\nResponse:\n\nThank you for highlighting the need for clarity on the clipping threshold. In our experiments, we used clipping thresholds \ud835\udc36 = 0.1 C=0.1 and \ud835\udc36 = 1 C=1. For the vision benchmarks in our paper, these values are based on established empirical studies that explore optimal clipping thresholds for DP-SGD. In particular, Appendix B.1 of Unlocking High-Accuracy Differentially Private Image Classification through Scale by Soham De et al. (2022, DeepMind) provides an in-depth analysis of clipping norms, concluding with the choice of \ud835\udc36 = 1 C=1 for their primary experiments.\n\nOur experimental settings also draw from the methodologies outlined in:\n\n- Unlocking High-Accuracy Differentially Private Image Classification through Scale by Soham De et al., 2022.\n- Differentially Private Image Classification by Learning Priors from Random Processes by Tang et al. (NeurIPS 2023).\n\nWe hope this additional context clarifies our choice of parameters and provides the reproducibility details necessary."
            }
        },
        {
            "title": {
                "value": "Response to W3"
            },
            "comment": {
                "value": "Review:\n> \"Assumptions 3.1 and 3.2 are unconventional and strongly restrictive. Only one previous work referenced these assumptions, making it difficult to accept them as standard or practical in the context of simple binary classification.\"\n\nResponse:\n\nWe appreciate the reviewer\u2019s concerns about the unconventionality of Assumptions 3.1 and 3.2. However, these assumptions are rooted in a well-established line of research within neural alignment, developed over multiple studies focused on multi-layer network dynamics. Neural alignment has been studied in prior work for orthogonally separable data (Assumption 3.1) and for orthogonal data.\n\n1. Assumption 3.1, for example, draws from foundational work, including Phuong and Lampert (2021), Wang and Pilanci (2022), and Min et al. (2024). Min et al. provided a detailed review of papers using Assumption 3.1 in Section 3.2 of the paper \u201cEarly Neuron Alignment in Two-layer ReLU Networks with Small Initialization\u201d (ICLR 2024).\n    - Mary Phuong and Christoph H Lampert. \u201cThe inductive bias of ReLU networks on orthogonally separable data.\u201d ICLR 2021.\n    - Yifei Wang and Mert Pilanci. \u201cThe convex geometry of backpropagation: Neural network gradient flows converge to extreme points of the dual convex program.\u201d ICLR 2022.\n    - Hancheng Min, Enrique Mallada, and Ren\u00e9 Vidal. \u201cEarly Neuron Alignment in Two-layer ReLU Networks with Small Initialization.\u201d ICLR 2024.\n\n2. Regarding Assumption 3.2, we assume that a \u201cclustering\u201d behavior emerges in the pre-trained features, which allows the features to work well in transfer learning (Galanti et al., 2022). This phenomenon is well-documented empirically in the neural collapse literature (Kothapalli, 2023), suggesting that pre-trained features $w_j$ tend to converge around the mean direction for data in class $c(j)$. Assumption 3.2 says that  data with positive label (resp. negative) only activates the $j$-th neuron if $j\\in F_{+}$ (resp. $j\\in F_-$). As a result, any positive data pair $(x,y)\\sim(x,y')$ activate the same set of neurons. From a contrastive learning viewpoint, it makes the representations of them semantically similar (Saunshi et al., 2019). Namely, when the features $w_j$ and data inputs $x_i$ are normalized unit vectors, the difference between representations of a positive data pair is bounded by: $ \\| g(x) - g( x' ) \\|_{ \\infty } \\le \\max \\cos(w_j,x_i)$ with $y_i=c(j)=y$, which represents the maximum cosine similarity between the features $w_j$ and the data points.\n\n    In conclusion, the form of Assumption 3.2 is consistent with empirical observations from the neural collapse literature and contrastive learning theory. By aligning with these well-documented phenomena, the assumption provides a reasonable and interpretable foundation for our theoretical analysis.\n\nWe will clarify these points in our manuscript."
            }
        },
        {
            "title": {
                "value": "Response to W2 (part 2)"
            },
            "comment": {
                "value": "3. On the Zeroth-Order Approximation:\n\n    **Review**:\n    > \u201cIn addition, the analysis is done for a non-standard simplified Langevin diffusion instead of DP-SGD. The benefits of such an approach seem questionable. The authors \u2018Apply a zeroth-order asymptotic expansion\u2019 which sounds very complicated but looks like a simple removal of Brownian motion representing Gaussian noise. This noise is the core component of DP training, and its removal makes method (1) equivalent to simple gradient flow.\u201d\n\n    **Response**:\n    - **Please note that our approximation does not remove the effect of noise, nor is the resulting model equivalent to gradient flow. The noise multiplier \u03c3 remains explicitly in our convergence bounds.** We retain the key noise effects for the loss dynamics by keeping the second-order term from Ito\u2019s lemma in Equation (2) and preserving the second-order terms associated with Brownian motion. This approach allows us to capture the essential stochastic characteristics of DP-SGD without modeling the full noise term directly on the parameters. \n        In essence, this approximation enables us to analyze the expected behavior of parameter updates while preserving the noise-sensitive behavior of the loss itself. By isolating these core elements, we provide insights into the overall training dynamics under differential privacy without losing the major noise effects that influence convergence properties and feature alignment. To support our claim that this approximation does not introduce too much error, we have proved an error approximation guarantee (Theorem F.4 in the updated manuscript), which shows that our approximated model does not differ too much from the original Langevin diffusion model. \n    - To complement our results under our approximation, we provided a rigorous theoretical analysis **without approximation** in Section 4.1.1. This non-approximated model relies on a 2-layer linear network, allowing us to maintain theoretical rigor without the complexities introduced by the approximation. This section presents a practical balance between analytical feasibility and capturing essential dynamics. The approximation allows us to obtain lower bounds for the DP-LP and DP-FFT loss, which are very challenging without the approximation in stochastic analysis.\n    - Furthermore, we believe that our work represents a significant step in the theoretical analysis of non-linear activations and multi-layer architectures in DP-SGD\u2014an advancement in the field. Future developments in this area, potentially building on our zeroth-order framework, can further enhance the community\u2019s understanding of DP-SGD dynamics in more intricate and realistic settings.\n\n4. Addressing Approximation Criticisms with Additional Analysis\n\n    **Review**:\n    > \u201cIn addition, it ignores the crucial per-sample clipping operation, which makes DP deep learning feasible. Thus, the proposed approximation loses the main features of differentially private training. The authors say on line 164 \u2018our modeling preserves the noisy behavior characteristic of DP-SGD\u2019 which needs to be justified.\u201d\n\n    **Response**:\n\n    - In the updated version of our paper (see attached), we provide analysis of Langevin diffusion with clipping. Note that this is the first analysis of clipped Langevin diffusion, and the zeroth order approximation is crucial to the analysis. We also provide the approximation error of the zeroth order approximation under clipping. See \u201cAppendix F: Theory with Clipping\u201d.\n\n    - Theory with clipping and approximation\n        - We have updated our paper in openreview and we provide analysis of Langevin diffusion with clipping. The additional theoretical results are: (1) The existence of a unique strong solution of the Langevin diffusion with clipping. (2) The zeroth order approximation error of Langevin diffusion with clipping. (3) A clipping version of Theorem 3.3 (feature distortion).\n        \n        We put all these additional results in \u201cAppendix F: Theory with Clipping\u201d."
            }
        },
        {
            "title": {
                "value": "Response to W2 (part 1)"
            },
            "comment": {
                "value": "We appreciate the reviewer\u2019s points regarding the simplicity of our theoretical model and the approximation we use. Below, we clarify why these modeling choices are fundamental to our analysis and not limitations.\n1. Exponential Convergence and Model Choice\n\n    **Review**:\n    > \u201cIt is unclear how this model aligns with realistic applications, particularly since results in Section 4.1.1 suggest exponential convergence, implying that the underlying problem is no harder than strongly convex optimization (or that PL conditions hold for the loss).\u201d\n\n    **Response**: While it is true that our 2-layer neural network exhibits exponential convergence, the key goal here is not solely to establish convergence rates, but rather to preserve the model\u2019s architecture in a way that captures layer-specific dynamics of DP fine-tuning. Abstracting the convergence as PL conditions on a general function would indeed obscure critical structural details of the two-layer setup, and would not allow us to capture the dynamics of adapting a pre-trained encoder with a linear head. Thus, our focus here is not on the order of convergence guarantees, but on preserving architectural features that are intrinsic to the phenomena we study. \n\n    Additionally, note that exponential convergence is a common characteristic in simplified settings, as seen in Kumar et al.\u2019s analysis of non-private transfer learning and Min et al.\u2019s work on linear networks, yet these models continue to provide insights applicable to complex architectures.\n\n2. Intuition from Simplified Models\n\n    **Review**:\n    > \u201cThe theoretical model appears oversimplified. Using a 2-layer neural network with ReLU activations while enabling a decoupling of feature learning and classification seems far removed from the practical settings under consideration.\u201d\n\n    **Response**:\n\n    Two-layer neural networks are widely accepted in the theoretical community as valuable tools for deriving intuition (examples below). Similar to the approach taken by Tian et al. in \u201cUnderstanding Self-Supervised Learning Dynamics without Contrastive Pairs,\u201d who used a 2-layer model to explore foundational questions in self-supervised learning, our goal here is to identify core principles of representation alignment and feature distortion in DP fine-tuning. Such simplified models allow for clearer insights and serve as a basis for studying more intricate behaviors in deep networks. The theoretical insights derived from these models are supported by empirical results on complex architectures (e.g., Figure 1, Figure 3), demonstrating their relevance to real-world applications. For instance, see below some recent distinguished ML theory papers that use 2-layer NNs for analysis, while studying phenomena that apply empirically to more complex architectures:\n\n    1. (ICML 2021 **Outstanding Paper Award Honorable Mention**) Yuandong Tian, Xinlei Chen, Surya Ganguli. Understanding Self-supervised Learning Dynamics without Contrastive Pairs.\n\n    2. (ICLR 2022 **Oral**) Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, Percy Liang. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution.\n\n    3. (NeurIPS 2022 **Oral**) Yuandong Tian. Understanding Deep Contrastive Learning via Coordinate-wise Optimization."
            }
        },
        {
            "title": {
                "value": "Response to W1"
            },
            "comment": {
                "value": "W1:\n>\u201cThe idea of combining FFT and LP is not entirely novel, as similar approaches were introduced and experimentally tested by Tang et al. (2023). A more detailed discussion of how this paper builds on or extends prior work is needed.\u201d\n\nResponse:\nWe acknowledge the reviewer\u2019s point that the concept of combining LP and FFT methods has been explored in prior works, including Tang et al. (2023). Our paper makes distinct theoretical and empirical advancements over existing approaches:\n1. **Theoretical Foundation**: To the best of our knowledge, our work is the first to provide a rigorous theoretical analysis of DP-LP-FFT under a continuous model. Previous work, including Tang et al., primarily focused on empirical evaluations without theoretical insights. By analyzing the dynamics of DP-SGD using a novel approximation technique based on Langevin diffusion, we bridge a significant gap, providing theoretical backing for the observed phenomena in DP fine-tuning across various privacy settings.\n2. **Broader Scope of Representation Alignment**: Kumar et al. show that the out-of-distribution performance of pre-trained features can be distorted in non-private fine-tuning while they leave in-distribution feature distortion as an open question for both non-private and private settings. Unlike prior work, our analysis demonstrates that representation alignment in LP-FFT occurs universally across both private and non-private settings, thereby addressing unresolved theoretical questions posed by Kumar et al. (2021). By highlighting and formalizing this alignment trend as a universal phenomenon, we introduce the concept of \"representation alignment\" that holds even as data distribution shifts across domains.\n\n\tReference: Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, Percy Liang. Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution. ICLR 2022 (oral).\n\n3. **Empirical Generalization and Mechanistic Insight**: Tang et al. investigated LP-FFT in a specific scenario involving synthetic pre-training data. In contrast, our experiments systematically confirm that the trade-offs observed by Tang et al. are generalizable across diverse datasets and architectures. We provide a deeper understanding of this phenomenon by identifying the mechanisms of feature distortion and alignment through visualizations and empirical validations. This approach provides a clearer understanding of how and why LP-FFT performs well in DP contexts, paving the way for a theoretical understanding of privacy-utility trade-offs in differentially private models. We also provided results of LP-LoRA in comparison to LP-FFT in our appendix.\n\nFor reference, the ICLR2022 oral paper \u201cFine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution\u201d also analyzed LP-FFT. However, the novelty and importance of the theoretical insights provided by Kumar et al. were still recognized despite this observation. We hope that, similarly, our theoretical contributions here will be acknowledged for extending foundational insights into novel, underexplored domains."
            }
        },
        {
            "title": {
                "value": "General response"
            },
            "comment": {
                "value": "We would like to extend our sincere gratitude to all the reviewers for their time and effort in evaluating our work. We are especially grateful to reviewer avt2 for the detailed and insightful feedback, which has been invaluable in refining our contributions. We also appreciate the positive feedback from reviewers HKiw and kixW, whose encouraging comments have reinforced our confidence in the value of this research."
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenge of differentially private fine-tuning of deep learning models. The authors highlight that na\u00efve full-parameter fine-tuning leads to misalignment between the pre-trained features and the last layer. They propose a hybrid strategy that combines linear probing with fine-tuning, demonstrating theoretically and empirically that this approach mitigates feature distortion. The theoretical framework is based on a simplified Langevin diffusion and a two-layer ReLU neural network. The paper\u2019s theoretical insights are further supported by experimental evaluations on various vision tasks and models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**S1.** The importance and relevance of the problem tackled \u2014 differentially private deep learning \u2014 is highlighted well, given the challenge posed by the high dimensionality of typical models.\n\n**S2.** The concept of splitting the privacy budget between full parameter tuning (or Full Fine-Tuning, FFT) and Linear Probing (LP: tuning only the last layer) appears intriguing and potentially valuable for practical applications.\n\n**S3.** The attempt to provide rigorous theoretical analysis addresses a gap in the current literature.\n\n**S4.** The paper includes multiple experiments across diverse datasets and models, supporting the benefits of the proposed hybrid approach."
            },
            "weaknesses": {
                "value": "**W1.** The idea of combining FFT and LP is not entirely novel, as similar approaches were introduced and experimentally tested by Tang et al. (2023). A more detailed discussion of how this paper builds on or extends prior work is needed.\n\n**W2.** The theoretical model appears oversimplified. Using a 2-layer neural network with ReLU activations while enabling a decoupling of feature learning and classification seems far removed from the practical settings under consideration. It is unclear how this model aligns with realistic applications, particularly since results in Section 4.1.1 suggest exponential convergence, implying that the underlying problem is no harder than strongly convex optimization (or that PL conditions hold for the loss).\n\nIn addition, the analysis is done for a non-standard simplified Langevin diffusion instead of DP-SGD. The benefits of such an approach seem questionable. The authors *\"Apply a zeroth-order asymptotic expansion\"* which sounds very complicated but looks like a simple removal of Brownian motion representing Gaussian noise. This noise is the core component of DP training, and its removal makes method (1) equivalent to simple gradient flow. In addition, it ignores the crucial per-sample clipping operation, which makes DP deep learning feasible. Thus, the proposed approximation loses the main features of differentially private training. The authors say on line 164\n> our modeling preserves the noisy behavior characteristic of DP-SGD\n\nwhich need to be justified.\n\n**W3.** Assumptions 3.1 and 3.2 are unconventional and strongly restrictive. Only one previous work referenced these assumptions, making it difficult to accept them as standard or practical in the context of simple binary classification.\n\n**W4.** The experimental setup lacks details on the clipping threshold, a key parameter influencing DP-SGD performance and necessary for reproducibility.\n\n**W5.** The paper\u2019s theoretical results and mathematical proofs in the Appendix are difficult to follow, partly due to unclear notation and insufficient explanation. Specific issues and questions include:\n\n- The notation $\\sim$ in line 214 is ambiguous.\n\n- The choice of zero-mean Gaussian initialization for the linear head is restrictive.\n\n- The meaning of *\u201coptimality\"* of $w_j$ in line 232 is unclear.\n\n- After a quick look at the work of Ganesh et al. (2023b), I did not find the exact statement of Theorem 4.1. Could the authors please point me to the exact place in the original paper?\n\n- Theorems 4.2 and 4.3 are hard to comprehend and presented without almost any commentary or explanation.\n\n- It is unclear to me why the authors say that\n> According to Theorem 5.1, a greater proportion of the privacy budget should be allocated to DP-LP\nwhen the total privacy budget is smaller.\n\n- Moreover, I would like to ask what is parameter $\\rho$. It seems like it can make $r$ arbitrarily large.\n\n- Section 5.2 relies on assumptions from the Appendix, which are also very strong (like E.7). Such an approach makes it hard to access the theoretical contributions of the paper adequately.\n\n- What does variable $D$ denote on line 442? How big can $t_{lp}$ be in practice for realistic parameter values? It is unclear how Corollary 5.3 is obtained for the main paper text.\n\n- The steps of Theorem 3.3 proof are unclear. For instance, how is formula (28) obtained? Why the probability on line 958 equals $1-2^{-h}$?"
            },
            "questions": {
                "value": "Most of my concerns are mentioned in the Weaknesses part.\n\n**Q1.** Is there a reason why Langevin diffusion is defined differently from Ganesh et al. (2023b)?\n\n**Q2.** How realistic are Assumptions 3.1 and 3.2? Were they experimentally validated?\n\n**Q3.** Public pre-training and private fine-tuning approach has been seriously questioned recently. I would appreciate the author's thoughts on the recent position paper by Tram\u00e8r et al. (2024).\n\n___\n\nTram\u00e8r, F., Kamath, G., & Carlini, N. (2024). Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining. Proceedings of the 41st International Conference on Machine Learning."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper examines differentially private (DP) fine-tuning (FT) methods, showing through theoretical and empirical analysis that DP full FT can distort pretrained backbone features due to misalignment between the pretrained backbone and the randomly initialized linear head. To address this, the authors propose DP-LP_FFT, which first performs linear probing and then fine-tunes. Additionally, the paper provides convergence rates for the proposed methods using two-layer neural networks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper investigates an interesting phenomenon in fine-tuning methods, where non-private fine-tuning distorts pretrained features and leads to degraded OOD performance. A similar empirical effect is observed in private settings, as shown in Figure 3.\n\n- The proposed DP-LP running before DP-FFT can theoretically reduce feature distortion. It would be beneficial to provide experimental results to compare with Figure 3.\n\n- The proposed LP mechanism in private fine-tuning is effective, as demonstrated in Tables 1 and 2."
            },
            "weaknesses": {
                "value": "There is a lack of comparison of feature distortion with DP-LP."
            },
            "questions": {
                "value": "In line 199 , why are subspaces separated by $\\mathbb{I}\\_{\\boldsymbol{x}\\_i^{\\top} z>0}=\\mathbb{I}\\_{y_i=-1}$ or $\\mathbb{I}\\_{\\boldsymbol{x}\\_i^{\\top} z>0}=\\mathbb{I}\\_{y_i=1}$ ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work studies the training dynamics of DP fine-tuning using a Langevin diffusion approximation of DP-SGD, and provides theoretical understandings of how fune-tuning techniques (such as DP-LP, DP-FFT, or a hybrid) affect the out-of-distribution performance of a 2-layer ReLU network.\nUsing a zeroth-order approximation, the authors provide a theoretical explanation of an empirical phenomenon that randomly initialized linear heads distort pre-trained backbone features in the early stages of DP-FFT.\nTo mitigate or even avoid the feature distortion, they propose a hybrid method that combines DP-LP and DP-FFT, and further examine the privacy budget allocation across DP-LP and DP-FFT. \nExtensive numerical experiments support theoretical findings and demonstrate the effectiveness of the proposed hybrid method.\nOverall, this work deepens our understanding of DP fine-tuning by providing a solid theoretical explanation. \nI found this work interesting and thus recommend an Accept."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Use Langevin diffusion as an approximation of DP-SGD to study the dynamics of DP tuning. While obtained from an approximation model, findings are very interesting and align with empirical observatons. \n2. The proposed hybrid tuning method is supported by theoretical guaranteens and evidence from extensive experiments. \n3. Insights into privacy budget allocation are of practical interest."
            },
            "weaknesses": {
                "value": "In practice, trainings are usually done in a discrete manner (say t=1,2,...), but Lagenvin diffusion is a continuous approximation for t>0. This gap may hamper the generality of this work's findings. For example, if $\\Delta t$ in Theorem 3.3 belongs to [0,1], then feature distortion might not be an issue, because we start from $t=1$. Therefore, it would be helpful if authors could provide a short discussion of the value of $\\Delta t$, or name some driving factors that may significantly affect the value of $\\Delta t$."
            },
            "questions": {
                "value": "1. theorem 3.4 says after $\\Delta t$, DP-FFT does not distort the pre-trained features. But the Eq (10) is stated for $\\forall t\\in(0, \\Delta t)$. Is there a typo in the range of $t$? I guess it should be $\u2200t\\in(\\Delta t, \\infty)$?\n2. typos around line 1148"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}