{
    "id": "pf9J3GNxSe",
    "title": "Critical Phase Transition in Large Language Models",
    "abstract": "Large Language Models (LLMs) have demonstrated impressive performance. To understand their behaviors, we need to consider the fact that LLMs sometimes show qualitative changes. The natural world also presents such changes called phase transitions, which are defined by singular, divergent statistical quantities. Therefore, an intriguing question is whether qualitative changes in LLMs are phase transitions. In this work, we have conducted extensive analysis on texts generated by LLMs and suggested that a phase transition occurs in LLMs when varying the temperature parameter. Specifically, statistical quantities have divergent properties just at the point between the low-temperature regime, where LLMs generate sentences with clear repetitive structures, and the high-temperature regime, where generated sentences are often incomprehensible. In addition, critical behaviors near the phase transition point, such as a power-law decay of correlation and slow convergence toward the stationary state, are similar to those in natural languages. Our results suggest a meaningful analogy between LLMs and natural phenomena.",
    "keywords": [
        "natural language processing",
        "large language models",
        "statistical physics",
        "phase transitions",
        "critical phenomena",
        "large-scale numerical experiments"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We have discovered a temperature-induced phase transition in LLMs, with its critical properties similar to those of natural language.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=pf9J3GNxSe",
    "pdf_link": "https://openreview.net/pdf?id=pf9J3GNxSe",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes that qualitative changes in language model outputs can be analyzed as phase transitions\u2014an analogy drawn from statistical physics. The authors empirically explore the shifts in characteristics of model-generated natural language that appear with sampling temperature scaling, which they posit may resemble phase transitions observed in physical systems. They observe a shift from what they characterize as an ordered to a disordered phases (i.e., the temporal structures in the different sets of sequences are statistically distinctive) as a result of changes in temperature. They conclude that LMs exhibit phase transition-like behavior around critical parameter values. They compare several quantitive attributes of the sequences generated at different temperatures to those of human generated text and find notable differences."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The motivation behind the paper is nice: the application of a well-studied concept from physics can perhaps allow us to use knowledge about/properties of that concept to better understand natural language and language models\n* The work provides a comparison of quantitative aspects of human- and machine-generated language, an approach that is more objective than the qualitative comparisons that are often done and thus perhaps a better ground from which to draw conclusions \n* The finding that the statistical properties of language at different sampling temperatures is interesting (anecdotally, at least)"
            },
            "weaknesses": {
                "value": "* The paper is generally difficult to follow:\n    * There is confusing terminology that isnt defined/contextualized before it is used, e.g., \u201clong-range correlation\u201d in the introduction). This will likely confuse most readers (myself included)\n    * The implications of the observed critical properties are incredibly unclear. See subsequent questions for the parts that felt particularly unclear to me, although this is not comprehensive. As such, it is difficult to draw meaningful conclusions from the paper\n* The work only looks at changes in model behavior as a result of changing the temperature parameter. This is a very narrow exploration and also somewhat contrived since it's an external change to the model that doesn\u2019t have any implications about what happens when internal model attributes vary\n* Only rather small models are used (on the scale of 100M parameters) and so results are not relevant for modern LLMs"
            },
            "questions": {
                "value": "* In the introduction, the motivation that \u201cthe decay of correlation in natural languages follows a power law\u201d is used. This is an incredibly vague statement. What are some concrete correlations in natural language that this applies to?\n* Terminological clarifications: \n    * What are \u201cchanges with a singularity\u201d, both in a physics and POS sequence context?\n    * What is a concrete example of a \u2018critical phenomenon\u2019 in a physical system and what might be an example of one in natural language? \n    * What does it mean that \u201cthe high-temperature phase is not simply disordered\u201d? What does this look like qualitatively in language? Does it tell us something about human-generated natural language? And what does it mean about a language model?\n* What are the implications of causality being broken for the conclusions that can be drawn? It seems as though this might invalidate results. I don\u2019t see why the effect \u201cshouldn\u2019t matter\u201d. I also don\u2019t think it\u2019s accurate to use the example of next characters as a situation where there are not dependency relationships. Anecdotally: language modeling can also be successful at the character-level. Perhaps I am not understanding something."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper argues that LM generations undergo a phase transition at temperature $T = 1$. Specifically, they generate texts from GPT-2 using different temperatures (conditioned on only the BOS token), and map each text into a sequence of POS tags (though the focus only on PROPN), and study three features of the POS tags.\n\n1. **Correlation between two tags:** The authors study whether PROPN tags are correlated at particular intervals in the text. They find the integrated correlation $\\\\tau$ between PROPN and PROPN saturates to a finite value for $T>1.0$, which means correlation decays faster than critical decay, whereas for $T<1.0$, $\\\\tau$ diverges, which means the correlation converges to a finite value. In other words, sequences with $T<1$ have long-range correlation between POS tags, and sequences above $T>1$ converge to zero correlation.\n2. **Power spectra of POS sequences**: The authors study whether there is periodic appearance of the PROPN tag via the power spectrum. Again, there appears to be long-range order at $T<1.0$, but at $T>1.0$ the spectrum is featureless.\n3. **Time evolution of POS sequences:** The authors study the probability of the PROPN tag at time $t$. They find that this probability converges quickly to a limiting value for $T<1.0$ and $T>1.0$, but converges much more slowly for values close to $T\\approx 1.0$."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper investigates whether LLMs go through phase transition with the temperature parameter. This (I believe) contrasts with much of the literature, which is about the possibility of a phase transition with the size of the model. The statistics that the authors introduce may have value for future work that aims to study structural features of LLM generations.\n\nThe writing and organization are clear, and the figures are clearly explained."
            },
            "weaknesses": {
                "value": "- There are obvious empirical concerns: namely, in the main paper, the authors only study GPT-2 small, and all of the analysis is specifically about the structure of where the proper noun PROPN tag occurs in generated text. This must be quite narrow, and it would be more convincing if the authors could summarize the results for other models and other POS tags in the main paper (they are referenced as being in the Appendix, which I did not read).\n- This is very far from my area of expertise, but I am not convinced that there is truly *singularity* at $T=1.0$. In their analysis in \u00a74.2 (power spectra) and \u00a74.3 (time evolution), the model seems to demonstrate continuous behavior around $T=1.0$. In particular, I am looking at Figure 3 (A), where the peak in integrated correlation actually occurs at $T=1.1$, and Figure 5, where the values neighboring $T=1.0$ (from $0.9$ to $1.2$) all behave qualitatively similarly to $T=1.0$. This makes me worried that the paper is just showing that generations have structurally different qualities for small $T$ and large $T$, which is somewhat obviously true because $T$ directly controls the peakiness of the LM prediction, such that low $T$ will be highly redundant and high $T$ will be quite degenerate (esp. for GPT-2 era models).\n- It is not clear to me how important the authors' findings are for the community. They mention their results suggest \"a meaningful analogy between LLMs and natural phenomena,\" but the analogy was not clear to me from reading the paper. Are there practical insights about what LMs are learning or how we should build future LMs?\n- There seems to be [related work](https://community.wolfram.com/groups/-/m/t/2958851) from 2023 that also studies phase transition at $T=1.0$, which the authors mention, and the distinction in their contributions is not clear to me. This may lessen the novelty of their work."
            },
            "questions": {
                "value": "- Can you help me interpret Equation (1)? My understanding is that $a$ and $b$ are POS tags, and $y_t$ is the particular POS tag at position $t$. Therefore, this is measuring whether there is a predictable relationship that when $y_t=a$, then $y_{t+\\\\Delta t}=b$. What is the purpose of the second term? Does it serve as some kind of normalization, to account for the base frequency of $y_t=a$ and $y_{t+\\\\Delta t}=b$? Why are there $18 \\\\times 17/2$ possible pairs $a$ and $b$? I would have assumed $18^2$?\n- L. 240: \"The plateau value increases with the position $t$ of the former tag.\" Is this true? It looks like the green line ($t=15$) is ordered between purple ($t=0$) and blue ($t=3$).\n- L. 244: What is \"the prefactor\" of the decay? I am again failing to see a reliable trend for $t$, since it seems like $t=0$, $t=63$, and $t=225$ all have extreme error bars, while $t=3$ and $t=15$ do not.\n- L. 301: What does it mean for a spectrum to be featureless?\n- For the power spectra analysis, I can see how the plots at different temperatures look different, but why is there *singular* behavior at $T=1.0$?\n- In Figure 5, doesn't $v(t)$ immediately reach its limiting value for $T=1.0$, contrary to what's written in the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates whether phase transitions occur in Large Language Models (LLMs). The authors conduct a statistical analysis of sequences generated by GPT-2 at various temperatures and found the phase transition occurs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The project fosters the understanding of LLMs from an interesting angle of phase transition.\n2. The experiments are well designed and cleared documented."
            },
            "weaknesses": {
                "value": "1. While LLMs develop fast over the past few years, this work tests on GPT-2, instead of newer generations of models."
            },
            "questions": {
                "value": "1. In what ways will the understanding of phase transition impact future LLM works?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper identifies a phase transition around temperature 1.0 when increasing the temperature parameter for LLM text generation. They focus on POS tags of generated sequences, finding a phase change in (1) the correlation of POS tags at two time points (for some time change $\\Delta t$) and (2) periodic behavior. Natural text seems to correspond to roughly temperature 1.0."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Drawing a connection between statistical physics and LLM text generation is very interesting.\n2. The paper identifies a phase transition for multiple metrics (correlation between two tags, power spectra)."
            },
            "weaknesses": {
                "value": "1. The results for periodic behavior are nicely quantified, but slightly unsurprising (degeneration into repeated sequences for low temperature, but not high temperature; e.g. https://arxiv.org/pdf/1904.09751). Looking at the specific types of repetitive loops that the model falls into could be more informative.\n2. The result that GPT-2 is closest to natural language text at T=1.0 is a bit unsurprising given that this leaves the probabilities unchanged from how they were trained. For example, we would likely expect lowest perplexity over a large corpus for T=1.0, because the model is trained to minimize loss (log-perplexity) for T=1.0."
            },
            "questions": {
                "value": "1. What is the motivation behind the POS tag correlation on p. 5? It's not necessarily intuitive why a proper noun appearing at time t should be highly correlated with a proper noun appearing at time $t+\\Delta t$ for large $\\Delta t$ (e.g. 100 tokens later; Figure 2). What do these correlations tell us about the actual text that gets generated?\n2. Consider including the word temperature in the title; phase changes often refer to transitions over the course of pretraining (https://arxiv.org/abs/2209.11895) rather than varying the temperature parameter, so it would be helpful to clarify that this paper focuses on varying temperature."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}