{
    "id": "WL4BmXG7Pl",
    "title": "Crafting Heavy-Tails in Weight Matrix Spectrum without Gradient Noise",
    "abstract": "Training strategies for modern deep neural networks (NNs) tend to induce a heavy-\ntailed (HT) empirical spectral density (ESD) in the layer weights. While previous\nefforts have shown that the HT phenomenon correlates with good generalization\nin large NNs, a theoretical explanation of its occurrence is still lacking. Especially,\nunderstanding the conditions which lead to this phenomenon can shed light on the\ninterplay between generalization and weight spectra. Our work aims to bridge this\ngap by presenting a simple, rich setting to model the emergence of HT ESD. In\nparticular, we present a theory-informed analysis for 'crafting' heavy tails in the\nESD of two-layer NNs without any gradient noise. This is the first work to analyze a noise-free setting and incorporate optimizer (GD/Adam) dependent (large)\nlearning rates into the HT ESD analysis. Our results highlight the role of learning\nrates on the Bulk+Spike and HT shape of the ESDs in the early phase of training,\nwhich can facilitate generalization in the two-layer NN. These observations shed\nlight on the behavior of large-scale NNs, albeit in a much simpler setting. Last\nbut not least, we present a novel perspective on the ESD evolution dynamics by\nanalyzing the singular vectors of weight matrices and optimizer updates",
    "keywords": [
        "Heavy Tails",
        "Spectral Analysis",
        "Generalization"
    ],
    "primary_area": "learning theory",
    "TLDR": "We analyze how heavy tails emerge in the weight spectrum of shallow neural networks and their relationship with generalization.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=WL4BmXG7Pl",
    "pdf_link": "https://openreview.net/pdf?id=WL4BmXG7Pl",
    "comments": [
        {
            "summary": {
                "value": "The paper studies the heavy tail phenomenon of the spectrum of weight matrices and its impact on generalization.  The paper presents a theorem for single step of Adam which shows an emergence of \"Bulk + spike\" spectrum. It futher empirical evaluates how this spectrum evolves into a heavy tail."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "a) The paper extends the single step analysis of GD Ba et. al. for Adam.   \nb) The observation that bulk + spike evolves into a heavy tail is interesting."
            },
            "weaknesses": {
                "value": "a) The paper lacks sufficient motivation for the problem setting and the relevance of the chosen algorithm, leaving it unclear why this is an important quantity to study. Notably, both the heavy-tail mechanism and strong generalization stem from learning the single-index direction, making it uncertain whether the heavy tail is simply a byproduct of good generalization or its cause. Consequently, it is challenging to assess whether this problem setting and analysis truly captures the influence of the heavy-tail spectrum on generalization.\n\nb) The empirical analysis is restricted to single-index teachers and two-layer networks, which limits the study's scope and makes it insufficient to determine whether any findings are broadly applicable.\n\nc) The theoritical analysis does not capture how \"BULK + spike\" transition into a heavy tail phenomenon. Hence the paper falls short of theoritically understanding the emergence of HT-ESD - the theoritical analysis is only captures the BULK + spike shape of spectrum after a single step of Adam.\n\nMinor :\n- Make a consistent notation of $\\beta^{*}$ through out the paper.\n- The inline math can be formatted better to ensure better readbility."
            },
            "questions": {
                "value": "discussed above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper aims to investigate the relationship between heavy tails in the weight spectrum distribution of neural networks and the ability of the network to generalize over unseen samples. The authors set up a teacher-student setting where a two-layer neural network learns a single-index model. First, the paper shows that for a sufficiently large step size, the full-batch Adam can align with the teacher's direction in just one step, corresponding to a spike in the ESD emerging from the initial mass. The analysis continues by empirically showing that the dynamics of the ESD evolves from the bulk to a heavy tail distribution. Finally, the paper shows the connection between the ESD heavy tails and the orientation of the singular vector of weight updates."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- To my knowledge, this paper is the first to study the HT-generalization correspondence in the single-index teacher-student setting. Altough not bringing any revolutionizing idea, the paper establishes an important first step towards understating this phenomenon within the community.\n- The analysis is comprehensive and precise, also taking into account techniques used in practice such as weight normalization and learning rate schedulers."
            },
            "weaknesses": {
                "value": "- The paper is showing mostly empirical result. In this theoretical setting one might expect to have more theoretical support of the claims."
            },
            "questions": {
                "value": "- In line 241, shouldn't we divide the similarity by the norm of $u_1$? Otherwise it can grow even if $u_1$ and $\\beta$ are not getting more aligned\n- In line 308 you talk about a sweet for $\\eta$, but I can't see it in the figure, can you clarify?\n\nMinor:\n- Line 297: $\\eta=0.1$, but the figures says $\\eta=1.$"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper is interested in understanding the evolution of the empirical spectral distribution for the first-layer weight matrix in a two-layer neural network during training. The paper considers a student teacher (single index model) with Gaussian. It shows that early-stage feature learning occurs for a much smaller step size for Adam compared to GD. \n\nThe paper also empirically studies the spectrum's evolution during further training and claims the existence of the initial spike is needed to transition to the heavy-tailed phase. Finally, the paper looks at various kernel alignment metrics, their evolution, and correlation with generalization.\n\n**Justification for Score**\n\nI think overall the paper presents intersting insights for a different optimizer. Hence I think it is worth accepting. However, the paper doesn't always position itselves in the most informative way compared to prior work and I have concerns about the significance of the spike that has been seen. Hence I do not give it a higher score."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Novelty:** As far as I know, the theoretical works on the early-stage emergence of spikes have been primarily limited to gradient descent. No theoretical work I know of analyzes Adam. Hence I think this is a novel contribution of the paper. Additionally, the insight that Adam requires a smaller step size than gradient descent to see the spike is important. \n\n\n**Clarity:** The paper is mostly well-written. My one concern is that when people talk about the spiked structure (Ba et al, Moniri et al). they mean for the features $F = \\sigma(WX)$ and not for the weight matrix $W$. This was confusing at first."
            },
            "weaknesses": {
                "value": "**Originality:** In terms of originality, the paper should do a better job of positioning itself with respect to Martin and Mahoney,  Ba et al., and Moniri et al. I think this is important. \n\n\n**Significance:** I think the result on smaller step size results in spikes is significant. If this were a result as presented in Moniri et al. However, as a result is currently presented, I am not sure the theory result says enough. Specifically, \n\n1. The paragraph after the corollary is clear to me. The step size scales are different for the spectral norm and the Frobenius norm. How do we rationalize this to show that there is a spike?\n\n2. Spikes in $W$ do not necessarily correspond to spikes in $F$. Especially if you have Gaussian Data. See [1] (flipping the role of $W$ and $X$). \n\n[1] Wang, Zhichao, Denny Wu, and Zhou Fan. \"Nonlinear spiked covariance matrices and signal propagation in deep neural networks.\" arXiv preprint arXiv:2402.10127 COLT (2024)."
            },
            "questions": {
                "value": "1. Generalization with Heavy-Tailed models. Do the authors know of any works that theoretically characterize the generalization error of models with heavy tails? Even in the regression setting I know of [2]. Also, do the authors know about papers that consider generalization error for models with spiked covariance (besides the Ba et al. 2023 paper), again including regression. \n\n[2] Wang, Yutong, Rishi Sonthalia, and Wei Hu. \"Near-interpolators: Rapid norm growth and the trade-off between interpolation and generalization.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2024.\n\n2. Emergence of heavy tails. For $\\eta=0.1$, the experiment needed $t \\sim 10^4$. However, the authors claim $t \\sim 10^4$ was not enough for $\\eta = 0.01$. I imagine we need more steps, maybe even something like $t \\sim 10^7$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The submission studies an interesting problem. When does HT-ESD happen? Why does it lead to good generalization?\n\nThe key results are below:\n\n1. Using a toy model,  ADAM after one step can provably induce a spike in spetrum. (thm 4.1)\n\n2. With experiments, the authors show that bulk decay happens (Fig 4), and that spike + decay leads to heavy tail.\n\n3. With experiments, the authors show that successful feature learning happens when the spectrum has the right tail exponent. (fig 5)\n\n##Strength:##\n\nI like the experiment part of this work.  In particular, comparing ADAM vs GD in Fig2 Fig3, and demonstrating the decay in Fig 4. \n\nI think the perspective is novel. The spectrum distribution not only results from the minibatch noise but also from full batch updates.\n\n##Weakness:##\n\nAlthough I enjoy reading the work, I am not confident that the authors successfully answer the questions asked.\n\n1. The spike after one large update is trivial. Given the backprop structure of neural net, one step update to the weight is rank one. When the step is large, one gets a rank-one spike.\n\n2. The paper demonstrates that decay happens without explaining why. More specifically, does the decay happen just correctly such the spectrum is heavy tailed?\n\n3. The paper demonstrates that the tail index should be in the correct range for the features to be learned. However, no explanation is given. Further, the experiments for this result are synthetic and toy.\n\nI am happy to update my review if the authors could address the above concern.\n\n\n##Minors:##\n\nWhat is u in line 241?"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "See summary"
            },
            "weaknesses": {
                "value": "See summary"
            },
            "questions": {
                "value": "See summary"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}