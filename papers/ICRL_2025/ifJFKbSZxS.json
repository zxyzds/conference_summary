{
    "id": "ifJFKbSZxS",
    "title": "A Markov decision process for variable selection in Branch and bound",
    "abstract": "Mixed-Integer Linear Programming (MILP) is a powerful framework used to address a wide range of NP-hard combinatorial optimization problems, often solved by Branch and bound (B\\&B). A key factor influencing the performance of B\\&B solvers is the variable selection heuristic governing branching decisions. Recent contributions have sought to adapt reinforcement learning (RL) algorithms to the B\\&B setting to learn optimal branching policies, through Markov Decision Processes (MDP) inspired formulations, and ad hoc convergence theorems and algorithms. In this work, we introduce B\\&B MDPs, a principled vanilla MDP formulation for variable selection in B\\&B, allowing to leverage a broad range of RL algorithms for the purpose of learning optimal B\\&B heuristics. Computational experiments validate our model empirically, as our branching agent outperforms prior state-of-the-art RL agents on four standard MILP benchmarks.",
    "keywords": [
        "Mixed-integer linear programming; Branch and bound; Reinforcement learning; Markov decision process"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We unlock the potential for reinforcement learning applications in mixed-integer linear programming by formulating variable selection in Branch and bound as a Markov decision process.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ifJFKbSZxS",
    "pdf_link": "https://openreview.net/pdf?id=ifJFKbSZxS",
    "comments": [
        {
            "summary": {
                "value": "This paper studies learning efficient branching strategies by reinforcement, and introduces B&B MDPs, a principled vanilla MDP formulation for variable selection, allowing to leverage a broad range of RL algorithms for the purpose of learning optimal B&B heuristics. The proposed method defines a Bellman optimality operator to unlock the full potential of approximate dynamic programming algorithms. On easy instances, DQNBBMDP consistently obtains best performance among RL agents"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper overperforms the previous RL agents while narrowing the gap with the IL approach."
            },
            "weaknesses": {
                "value": "1. The primary innovations of BBMDP are unclear.\n2. The experiments are conducted on a relatively small scale. The medium instances require only 1 minute to solve.\n3. BBMDP's performance is not significantly superior to other RL methods. It is even inferior on the medium dataset."
            },
            "questions": {
                "value": "1. Could you clearly differentiate between BBMDP and TreeMDP?\n2. Regarding the statement: \u201cYet, if the performance of IL heuristics are capped by that of the suboptimal branching experts they learn from, the performance of RL branching strategies are, in theory, only bounded by the maximum score achievable.\u201d What does \"suboptimal branching experts\" refer to? Is it the strong branching rule?\n3. On page 2, line 91, there's a mention of \u201cSince \u03c1 necessarily defines a total order on nodes.\u201d Given that node selection and variable selection influence each other, what does it mean to have this fixed order?\n4. Could you conduct tests on larger datasets, similar to those used by Gasse et al.?\n5. Would it be possible to provide additional metrics, such as wins and P-D convergence plots?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates an RL method for variable selection in branch-and-bound. It applies the general definition of MDP to B&B search so that general RL algorithms and theories can be applied to solve the problem. The MDP is solved with Q-learning. In experiments, the method is evaluated on four standard MILP benchmarks and compared against other RL methods. It achieve the best results compared to other RL methods, but still worse than IL methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper developed BBMDP tailored for B&B search that allows the application of a boarder range of developed RL methods.\n2. The empirical results look promising in comparison to other RL methods.\n3. The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "I don\u2019t see major weaknesses, but have a couple of questions that could be of potential improvement to the paper:\n1. Can you show the sample complexity of the methods compared to previous ones?\n2. Is your methods also effective on larger instances? For instances that cannot be solved within the runtime limit, one can still evaluate the primal-dual gap/primal-dual integral to see if the method is effective or not."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel Markov Decision Process (MDP) framework, BBMDP, for optimizing variable selection in Branch and Bound (B&B) algorithms in Mixed Integer Programming (MIP). By restricting the node selection strategy to depth-first search (DFS), the authors derive a more canonical Bellman equation for BBMDP, enabling broader use of reinforcement learning frameworks. This approach offers greater robustness than existing TreeMDP models by preserving optimality and convergence properties. The authors apply Deep Q-learning to BBMDP, demonstrating improved performance over previous RL approaches on TreeMDP across multiple MILP benchmarks, with significant reductions in both computation time and the number of B&B nodes required. Results further suggest that BBMDP effectively narrows the gap between reinforcement learning and imitation learning methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow\n2. The introduction of BBMDP provides a principled, canonical MDP formulation for variable selection in B&B, addressing limitations of previous TreeMDP approaches by enabling a broader application of reinforcement learning techniques with theoretical support.\n3. The paper includes experiments on many standard MILP benchmarks"
            },
            "weaknesses": {
                "value": "The adoption of a DFS node selection strategy allows for a more canonical Bellman operator and potentially broader applicability of current RL frameworks. However, the impact of this restriction on performance is not fully explored, and the paper lacks a discussion on potential trade-offs related to this design choice.\n\nThe authors highlight BBMDP\u2019s potential to support a wider range of RL algorithms, yet only DQN, which is also compatible with TreeMDP, is tested. Consequently, the empirical results do not demonstrate the benefits of BBMDP\u2019s broader RL applicability.\n\nFor medium instance testing, only 20 instances are evaluated, which may be insufficient to reliably represent each problem class. Increasing the test set to at least 100 instances would provide a more robust assessment of performance across diverse scenarios.\n\nWhen comparing methods on node count and solution time, there are inconsistencies between the two metrics: a shorter runtime does not always correspond to fewer nodes processed. For instance, DQN-BBMDP sometimes achieves a lower node count yet requires more time, and vice versa when compared with IL approaches.\n\nWhile the proposed methods show improved performance and reduce the gap with IL approaches, a considerable performance disparity remains. \n\nAdditionally, all tested problem classes are synthetic mathematical benchmarks, leaving the performance on realistic datasets unexamined."
            },
            "questions": {
                "value": "Besides the concerns raised in the weakness part, I have the following additional questions:\n\n1. Why define R(s, a) = \u22122 for all transitions until episode termination? Is intuition behind the number -2? Is that the results of tuning?\n2. Any intuition of why choosing the HL-Gauss cross-entropy loss?\n3. Why not including IL-DFS to the medium problem classes? Will the drop of the performance of using DFS will become more significant for the larger instances?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work addresses the issue that these alternative MDP formulations introduce approximations that undermine the asymptotic performance of RL branching agents in the general case. This paper introduces a B&B MDP formulation for variable selection in B&B, which preserves optimality without sacrificing the convergence properties brought by previous contributions. Computational experiments validate our model empirically, as our branching agent outperforms prior state-of-the-art RL agents on four standard MILP benchmarks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1)\tOriginal: This paper introduces a vanilla MDP formulation for variable selection in B&B, allowing the leveraging of a broad range of RL algorithms to learn optimal B&B heuristics.\n2)\tQuality:\nThe overall presentation is good\nThe paper discusses the related works about RL-based methods in a fairly clear manner.\n3)\tClarity: \nOverall, the paper is clear and easy to follow, however, some writing is confusing, especially since some symbols are not defined.\n4)\tSignificance: \nThe paper studies introduce a vanilla MDP applied to the variable selection problem to learn optimal B&B branching strategies, which seems to be an important research direction."
            },
            "weaknesses": {
                "value": "1.\tThe related work about imitation learning for variable selection is insufficient,\n2.\tThere is no pipeline to explain how the proposed method works.\n3.\tThe proposed method seems to only apply to the binary integer programming problem, it is a little unclear to me how much technical novelty is in the B&B MDP and whether the contributions in this paper are significant enough.\n4.\tThe experiment seems insufficient, only testing in easy and medium difficulty levels, lack of comparison in hard difficulty levels.\n5.\tThe ideas in the paper took me some time to properly digest. I believe all of the information needed for the reader to digest is there, but think that the paper could make this process easier for the reader."
            },
            "questions": {
                "value": "1.\tThe meaning of symbols is confusing. For example, what\u2019s the meaning of ($v, \\varepsilon$) in Line 84?\n2.\tWhy is the reward defined as -2 in the MDP definition, as I know, some papers define the reward as -1[1], can you explain it?\n3.\tThis may be important because the convergence guarantees of RL algorithms are often made in the discounted setting with litter than 1, Why is this paper setting the discounted with 1. Is there any experimental or theoretical support for this point?\n4.\tAccording to Gasse et al. (2019), why was this paper not compared to the facilities dataset? Why was this paper not compared to the hard dataset? Why does this paper not report the average per-instance standard deviation in Table 1?\n5.\tWhy did this paper only test 20 instances on the medium transfer instances instead of testing 100 instances like the easy difficulty level? What would happen if this method also tested the 100 instances on the medium difficulty level?\n6.\tFor the variable selection problem, reinforcement learning seems to have no advantage over imitation learning both in training speed and testing effectiveness. So, what is the motivation behind our research in this paper?\n7.\tSome articles have already defined branching as MDP [1]. Can you summarize the differences between you and these articles?\n8.\tThe contribution summary of the article is unclear. Can you further summarize it?\n[1] Improving Learning to Branch via Reinforcement Learning. NeurIPS Workshop, 2020. https://openreview.net/forum?id=z4D7-PTxTb"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}