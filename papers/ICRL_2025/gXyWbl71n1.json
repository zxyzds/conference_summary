{
    "id": "gXyWbl71n1",
    "title": "MAI: A Multi-turn Aggregation-Iteration Model for Composed Image Retrieval",
    "abstract": "Multi-Turn Composed Image Retrieval (MTCIR) addresses a real-world scenario where users iteratively refine retrieval results by providing additional information until a target meeting all their requirements is found. Existing methods primarily achieve MTCIR through a \"multiple single-turn\" paradigm, wherein methods incorrectly converge on shortcuts that only utilize the most recent turn's image, ignoring attributes from historical turns. Consequently, retrieval failures occur when modification requests involve historical information. We argue that explicitly incorporating historical information into the modified text is crucial to addressing this issue. To this end, we build a new retrospective-based MTCIR dataset, **FashionMT**, wherein modification demands are highly associated with historical turns. We also propose a Multi-turn Aggregation-Iteration (**MAI**) model, emphasizing efficient aggregation of multimodal semantics and optimization of information propagation in multi-turn retrieval. Specifically, we propose a new Two-stage Semantic Aggregation (TSA) paradigm coupled with a Cyclic Combination Loss (CCL), achieving improved semantic consistency and modality alignment by progressively interacting the reference image with its caption and the modified text. In addition, we design a Multi-turn Iterative Optimization (MIO) mechanism that dynamically selects representative tokens and reduces redundancy during multi-turn iterations. Extensive experiments demonstrate that the proposed MAI model achieves substantial improvements over state-of-the-art methods.",
    "keywords": [
        "Multi-modal",
        "Multi-turn retrieval",
        "Retrospective-based",
        "Composed image retrieval"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=gXyWbl71n1",
    "pdf_link": "https://openreview.net/pdf?id=gXyWbl71n1",
    "comments": [
        {
            "summary": {
                "value": "Existing multi-turn composed image retrieval (MTCIR) methods adopt the \u201cmultiple single-turn\u201d paradigm and neglect the historical correlation information in multi-turn interactions. To address this problem, this paper first builds a new retrospective-based MTCIR dataset, wherein modification demands are highly associated with historical turns. Then, the authors develop a new Multi-turn Aggregation-Iteration (MAI) model, which contains a two-stage semantic aggregation paradigm coupled with a cyclic combination loss. Besides, a multi-turn iterative optimization mechanism is designed to select representative tokens and reduce redundancy dynamically. Experimental results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1)\tThis paper focuses on an interesting multi-turn composed image retrieval task, and points out the critical \u201cmultiple single-turn\u201d problem in the existing research field. This research is very valuable and provides significant insights for further research.\n2)\tThe authors have constructed a new dataset for the multi-turn composed image retrieval task, which is more similar to real-world scenarios, and is more massive and diverse. This is an important contribution if the dataset is made publicly available.\n3)\tA multi-turn key information-aware approach, named the Multi-turn Aggregation model, is proposed to achieve multimodal semantics aggregation and multi-turn information optimization. The proposed method is reasonable and the parameter-free multi-turn iterative optimization (MIO) mechanism is interesting.\n4)\tThe effectiveness of the proposed method has been demonstrated by extensive experimental results.\n5)\tThe writing of this paper is good and easy to follow."
            },
            "weaknesses": {
                "value": "1)\tWill the collected dataset be made publicly available? Some key characteristics of the dataset are still not clear, such as the average and variance of turn number for each query, the average length for modification text, etc. The dataset will be more comprehensible as more detailed statistics are provided.\n2)\tIt seems MIO is executed for each input, and it contains a DPC-kNN for clustering and density computation. The complexity of this module should be analyzed.\n3)\tIn Table 6, what does the w/o CCL mean? It seems w/o CCL has outperformed most of the compared methods. Are all the methods evaluated on the same backbone? If they are not, the comparison results may not be fair.\n4)\tExcept for the proposed dataset, the proposed method should be also evaluated on existing multi-turn composed image retrieval datasets to further demonstrate the effectiveness."
            },
            "questions": {
                "value": "please try to address the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new dataset, FashionMT, specifically designed for multi-turn composed image retrieval (MTCIR) tasks. FashionMT is characterized by its retrospective-based nature, where the modified text in each new turn may involve information from historical reference images, and its massive diversity, containing 14 times more fashion images and 30 times more categories than previous MTCIR datasets. The authors also introduce a new Multi-turn Aggregation-Iteration (MAI) model that focuses on efficient aggregation and iterative optimization of multimodal semantics in MTCIR. The MAI model includes a Two-stage Semantic Aggregation (TSA) paradigm and a Cyclic Combination Loss (CCL) to enhance semantic consistency and modality alignment, as well as a Multi-turn Iterative Optimization (MIO) mechanism to dynamically select representative tokens and reduce redundancy during multi-turn iterations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "a.\tThe paper proposes a new dataset, FashionMT, for multi-turn composed image retrieval (MTCIR) tasks\nb.\tFashionMT is characterized by its retrospective-based nature and massive diversity, containing 14 times more fashion images and 30 times more categories than previous MTCIR datasets\nc.\tThe authors introduce a new Multi-turn Aggregation-Iteration (MAI) model that focuses on efficient aggregation and iterative optimization of multimodal semantics in MTCIR\nd.\tThe MAI model includes a Two-stage Semantic Aggregation (TSA) paradigm and a Cyclic Combination Loss (CCL) to enhance semantic consistency and modality alignment\ne.\tThe MAI model also includes a Multi-turn Iterative Optimization (MIO) mechanism to dynamically select representative tokens and reduce redundancy during multi-turn iterations"
            },
            "weaknesses": {
                "value": "a.\tHow to avoid false negative samples during FashionMT dataset construction? The existing single-turn dataset already has the presence of false negative samples, will it be more obvious with multiple rounds?.\nb.\tFor the ZS-CIR method, how was it trained and tested on FashionMT? They are designed for zero-shot, is the adaptation process also zero-shot paradigm?\nc.\tHow does MAI perform for the single-turn dataset? The authors should add the results of MAI for datasets such as FashionIQ, CIRR, etc. to confirm the superiority of MAI even with turn=1.\nd.\tThe clustering used in MIO is affected by the number of cluster centers, denoted as k, and the authors do not mention in the implementation details the value of k, and the number of iterations used when performing k-means, which affects the accuracy of the clustering. And does clustering increase the time overhead of the whole model? Authors need to increase the comparison of model training and inference time.\ne.\tThe Q-former used in MAI is initialized by BLIP-2 with the Flan-t5-xxl language model, while the version of BLIP-2 used in SPRC is blip2-pretrain, right? If MAI uses the same version of BLIP-2 as SPRC to initialize the Q-former parameters, how does the result behave? The authors need to add this experiment to further demonstrate the superiority of MAI?\nf.Typos and minors. There are non-standard punctuation marks, such as Line 431, P8, \"thanks to TSA\", etc. It is recommended to check the whole paper."
            },
            "questions": {
                "value": "refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces MAI (Multi-turn Aggregation-Iteration), a model for multi-turn composed image retrieval (MTCIR), along with FashionMT, a new large-scale dataset specifically designed for MTCIR. The key innovation is a two-stage semantic aggregation approach that uses image captions as a bridge between visual and textual modalities, plus a memory-efficient mechanism for retaining historical information across multiple turns."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Creation of a large-scale, diverse dataset (FashionMT) that better reflects real-world scenarios\n2. Memory-efficient design through the Multi-turn Iterative Optimization mechanism"
            },
            "weaknesses": {
                "value": "1. Limited evaluation of existing datasets (mostly focused on their new dataset), how is the model's performance on existing benchmark like CIRR, FashionIQ and CIRCO. \n2. The fixed number of turns (4) in the dataset may not reflect varying real-world scenarios, does the author have plan to extend the dataset with a different number of turns? \n3. More visualization of the dataset is expected: I'd like to know (1)sample image sequences across multiple turns: namely the quality of the dataset, the relation of images in different turns, and whether the modified text can describe the relationship between images. (2) visualizations of the distribution of different types of modifications: such as Rollback operations and combination operations."
            },
            "questions": {
                "value": "1. Statistics for Modification text types: is possible to supply a category of the modified text type, so that the performance can be compared according to different categories? such as Rollback operations and combination operations.\n2. Will the dataset be open-source? \n3. How does the model perform with varying numbers of turns beyond the fixed 4-turn setup?\n4. Could the approach be extended to other domains beyond fashion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no need"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces FashionMT and the MAI model. While the method shows strong results, its complexity and potential scalability challenges could limit its immediate applicability in more generalized settings. Further testing on diverse datasets and longer interaction sequences, along with analysis of the model's efficiency, would enhance the practical relevance of this work."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- FashionMT is a significant contribution. The dataset provides richer and more realistic interactions with over 1 million images and 95 categories. This dataset fills a critical gap in the field where existing datasets fail to incorporate historical context across multiple turns.\nThe model\u2019s approach to maintaining and optimizing key tokens during multiple retrieval iterations is highly relevant to real-world e-commerce scenarios, where users iteratively refine their search queries. The MAI method effectively addresses the shortcomings of the \u201cmultiple single-turn\u201d paradigm that fails to leverage historical turn information.\n- Extensive experiments show that MAI achieves significant improvements over existing methods in both the combination and rollback settings."
            },
            "weaknesses": {
                "value": "- While the model is innovative, it introduces considerable complexity with multiple components (TSA, CCL,). The addition of multiple layers, clustering mechanisms, and token filtering might make the model difficult to implement and optimize in real-world settings where computational efficiency is key.\n- While the results on FashionMT are strong, the paper does not provide comparisons on non-fashion datasets. The model's application to a more diverse range of image retrieval tasks, such as general object retrieval or scene retrieval, would provide a stronger claim to its versatility."
            },
            "questions": {
                "value": "- Can the MAI model be adapted to other MTCIR applications outside fashion, such as general e-commerce, furniture, or other products?\n\n- It would be beneficial to know how this impacts performance when dealing with significantly larger datasets. Is the trade-off between memory efficiency and retrieval performance consistent across different dataset sizes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}