{
    "id": "hQ2TUZmse1",
    "title": "Refining Counterfactual Explanations With Joint-Distribution-Informed Shapley Towards Actionable Minimality",
    "abstract": "Counterfactual explanations (CE) identify data points that closely resemble the observed data but produce different machine learning (ML) model outputs, offering critical insights into model decisions. Despite the diverse scenarios, goals and tasks to which they are tailored, existing CE methods often lack actionable efficiency because of unnecessary feature changes included within the explanations that are presented to users and stakeholders. We address this problem by proposing a method that minimizes the required feature changes while maintaining the validity of CE, without imposing restrictions on models or CE algorithms, whether instance- or group-based. The key innovation lies in computing a joint distribution between observed and counterfactual data and leveraging it to inform Shapley values for feature attributions (FA). We demonstrate that optimal transport (OT) effectively derives this distribution, especially when the alignment between observed and counterfactual data is unclear in used CE methods. Additionally, a counterintuitive finding is uncovered: it may be misleading to rely on an exact alignment defined by the CE generation mechanism in conducting FA. Our proposed method is validated on extensive experiments across multiple datasets, showcasing its effectiveness in refining CE towards greater actionable efficiency.",
    "keywords": [
        "Explainable artificial Intelligence",
        "Feature attributions",
        "Counterfactual explanations"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=hQ2TUZmse1",
    "pdf_link": "https://openreview.net/pdf?id=hQ2TUZmse1",
    "comments": [
        {
            "comment": {
                "value": "**Q8:** \u201cPerhaps writing a sentence at the beginning of Section 3, and/or at the end of Section 2, so the reader knows what to expect in the transition from Section 2 to Section 3, and how they fit in the big picture of the paper.\u201d\n\n**A8:** Good advice! We plan to add the following sentences at the end of Section 2. \n> To solve (1), we resort to FA to identify the most influential features to obtain the modification indicator variable $\\mathbf{c}$. The next section introduces commonly used Shapley value methods for FA, which, together with our later proposed one, are integrated into our algorithmic framework COLA, to obtain the refined counterfactual $\\mathbf{z}$.\n\n**Q9:** \u201cRow 472: Result III could benefit from having a sentence entry point to situate the reader as to why this is important and how it fits in the big picture of the paper. Furthermore, another sentence would be useful to provide insights on why CF-$p_{\\text{OT}}$ outperforms CF-$p_{\\text{Ect}}$.\u201d\n\n**A9:** Agree! To give an entry point to the reader to understand why Result III is important, we plan to add the following sentence at the beginning of Result III. \n\n> This result demonstrates the effectiveness of $p$-SHAP in eliminating the influence of the CE generation process by replacing the CE algorithm-dependent knowledge of $\\mathcal{D}$ with the optimal transport (OT) joint distribution between the factual and counterfactual data, shown in Figure 4.\n\nThis sentence reinforces the added content in A4 above.\n\nThen, we will add the following sentence in line 485.\n\n> CF-$p_{\\text{OT}}$ outperforms CF-$p_{\\text{Ect}}$ because it utilizes a more theoretically grounded approach to identify the key features that require modification, whereas CF-$p_{\\text{Ect}}$ relies on CE algorithm-dependent knowledge, which lacks solid justification on its effectiveness for FA.\n\n### Responses to \u201cQuestions\u201d\n**Q10:** \u201cIn order to make this methodology widespread and for reproducibility, I don\u2019t know if the authors considered converting their code (in the supplemental material) to a Python package or creating a Python class? I am not sure if the editor should require this for publication?\u201d\n\n**A10:** Thank you for bringing this up! Interestingly, we share the same thought as you. We have already developed an implementation of COLA as a Python software library (separate from the code used for the experiments in this paper, which is included in the supplemental material). However, since we are currently in the double-blind review process, we have decided to hold off on publishing this library for now. Once the review process concludes, we will promptly make it available on GitHub.\n\n**Q11:** \u201cTypo on row 196? \u201cFirst p-SHAP degrades to RB-SHAP\u201d, shouldn\u2019t it be B-SHAP?\u201d\n\n**A11:** Sorry for this! It should be B-SHAP. We will fix it in the revised version.\n\n**Q12:** \u201cTypo on row 474? Word alignment misspelled?\u201d\n\n**A12:** Sorry again. It should be spelled as \u201calignment\u201d rather than \u201calginment\u201d. We will fix it in the revised version.\n\n**Q13:** \u201cPerhaps clarify on row 231 that it is the tightest Lipschitz upper bound, not necessarily the tightest upper bound.\u201d\n\n**A13:** Agree! To make the statement mathematically rigorous, we will clarify in row 231 that \u201c\u2026which in turn provides the tightest Lipschitz upper bound on \u2026\u201d.\n\nOnce again, we sincerely thank the reviewer for dedicating so much time and effort to reviewing our paper! Your suggestions and insights have been **incredibly valuable**, and they **directly contribute to improving the quality** of our manuscript. We deeply appreciate your thoughtful feedback and remain open to further discussions if you may have additional points of interest or questions."
            }
        },
        {
            "comment": {
                "value": "**Q4:** \u201cIt would be great to have a clear explanation/insight in the paper about why $A_{\\text{Prob}}$ does not necessarily require knowledge of how $r\\sim \\mathcal{D}$ is generated in order to achieve good CE performance.\u201d\n\n**A4:** Thank you for raising this very interesting point! To summarize, we would argue that not only does $A_{\\text{Prob}}$ not require explicit knowledge of how $r\\sim \\mathcal{D}$ is generated to achieve good CE performance, but in fact, all components in COLA, except for $A_{\\text{CE}}$, should exercise caution in utilizing knowledge of $r\\sim \\mathcal{D}$, where $\\mathcal{D}$ is the distribution of counterfactual data.\n\nLet's revisit our core objective: The factual data $\\mathbf{x}$, the model $f$, and the desired counterfactual effect $\\mathbf{y}^*$ should all be determined in advance and remain fixed for our application, before choosing any counterfactual explanation (CE) algorithm to generate the counterfactual data $\\mathbf{r}$ such that $\\mathbf{y}^* = f(\\mathbf{r})$. We want $A_{\\text{Prob}}$ and $A_{\\text{Shap}}$ to perform feature attribution (FA) for $\\mathbf{x}$ and $f$ in order to achieve $\\mathbf{y}^*$, and the FA results should be consistent and unique as long as $\\mathbf{x}$, $f$, and $\\mathbf{y}^*$ are fixed.\n\nHowever, if $A_{\\text{Prob}}$ or $A_{\\text{Shap}}$ depends on the distribution $\\mathcal{D}$, then the results would no longer be unique. This is because, unlike $\\mathcal{D}_{\\text{Train}}$, which is observed from the real world, $\\mathcal{D}$ is generated by a chosen CE algorithm. \n\nSince there are many CE algorithms available, each could produce a different $\\mathcal{D}$ distribution, resulting in different FA outcomes for the same input data, the same trained model, and the same counterfactual effect. This variability would deviate from the fundamental objective of FA, which is to provide a unique and reliable explanation. The same reasoning applies to $A_{\\text{Value}}$.\n\nWe will add the following sentences in the revised version, after line 215.\n\n> The key reason that $p$-SHAP does not require explicit knowledge of how $r \\sim \\mathcal{D}$ is generated is because its goal is to work directly with the factual data $\\mathbf{x}$, the model $f$, and the desired outcome $\\mathbf{y}^*$, independent of the specific CE algorithm used to produce $\\mathbf{r}$. By focusing solely on these fixed components, $p$-SHAP ensures consistency in FA without being influenced by the variability of different CE generation processes, which is a major difference to CF-SHAP.\n\n**Q5:** \u201cIn the \u201cTheoretical Aspects of p-SHAP\u201d, it would be helpful to know more precisely why using OT for obtaining the joint distribution $\\text{Prob}(\\mathbf{r},\\mathbf{x})$ helps later with minimizing $D(f(\\mathbf{z}), \\mathbf{y}^*)$. This connection doesn\u2019t seem to be precise as it is.\u201d\n\n**A5:** Thank you! And we believe that the to-be-added sentence in A3 above should address this concern.\n\n**Q6:** \u201cTheorem 4.1 is labeled as \u201c$p$-SHAP towards CE\u201d, but the theorem is much more a result of OT  + Lipschitz continuity of $f$, and its direct connection to $p$-SHAP seems to be missing.\u201d\n\n**A6:** In the revised version, we will add the following sentence before \u201cComplexity of COLA\u201d in line 349.\n\n> By Theorem 4.1, COLA aims to minimize the dissimilarity between $f(\\mathbf{z})$ and $\\mathbf{y}^*$ by modifying $\\mathbf{z}$ based on FA results, which identify the most important features to adjust to achieve the desired counterfactual effect.\n\n### Responses to Addressing \u201cOTHERS\u201d\n**Q7:** \u201cThe paper provides so many entry points for the reader, and it would be nice to also have the definition of \u201cExact Alignment\u201d and \u201cFeature Alignment (FA) performance\u201d at least once in a footnote.\u201d\n\n**A7:** Thank you for your good suggestions! In the revised version, we will modify line 25 to be \n\n> \u2026 it may be misleading to rely on a counterfactual distribution defined by the CE generation mechanism \u2026.\nThis is because the exact alignment literally refers to a known distribution of a counterfactual, which is usually defined by a given CE method [1].\n\nThen, we will modify line 197 to be \n\n> \u2026. when $A_{\\text{Prob}}$ defines a joint distribution between $\\mathbf{x}$ and $\\mathbf{r}$, indicating an $i\\leftrightarrow j$ alignment of any $\\mathbf{x}_{i}$ and $\\mathbf{r}_j$.\n\nThis modification reinforces the concept that the distribution determines the alignment, as stated in line 308. And we will see if they need to be move to footnote based on the length of the paper after the draft revision."
            }
        },
        {
            "comment": {
                "value": "We sincerely appreciate the time and effort that the reviewer, iyvf, has invested in thoroughly reviewing our paper. We especially want to thank you for accurately summarizing our paper\u2019s contributions and for providing such a detailed description of the key steps in COLA, as outlined.\n\nWe are truely grateful for your recognition of the importance of the problem we are addressing and acknowledgment of our contributions on both the theoretical and empirical fronts. We are deeply thankful for your supportive comments and constructive insights.\n\nBelow, we made every effort to address the points you raised. Please feel free to let us know if any aspects require further clarification. We remain open to any additional discussions you may have. (**Q** and **A** are questions raised by the reviewer and answers provided by the authors, respectively)\n\n### Responses to Addressing \u201cPRESSING ISSUES\u201d\n\n**Q1:** \u201cCOLA has a well-defined procedure to compute $z$. It seems clear from Theorem 5.1 that $z$ would satisfy the constraints from Equation 1, but it is not clear how the chosen $z$ from COLA is actually optimal, i.e., why is it minimizing, or closer to minimizing, the problem in Equation 1, given that the last algorithm in COLA to find $z$ does not minimize any function. Even if it is not optimal, as suggested by Figure 4, the paper would benefit from having a clearer narrative of why COLA is closer than other methods to optimal, considering the minimization problem in Equation 1.\u201d\n\n**A1:** Thank you for your insightful question. We acknowledge that the $\\mathbf{z}$ obtained through COLA is not theoretically guaranteed to be a global optimum. However, $\\mathbf{z}$ is aimed at minimizing the dissimilarity between $f(\\mathbf{z})$ and the desired outcome $\\mathbf{y}^*$.\n\n(**What function is COLA trying to minimize?** ) COLA refines $\\mathbf{z}$ based on the feature attribution (FA) results in Lines 3-4 (as detailed in Algorithm 1 and Figure 2). Specifically, FA is used to determine the most important features of each data point in $\\mathbf{x}$ that need to be modified in order to achieve the desired counterfactual effect $\\mathbf{y}^* = f(\\mathbf{r})$, while keeping the less important features unchanged. Theorem 4.1 shows that when optimal transport (OT) is employed to compute the joint distribution in $p$-SHAP, it minimizes an upper bound of the $1$-Wasserstein distance between $f(\\mathbf{x})$ and $\\mathbf{y}^*$. This implies that $p$-SHAP, during the FA process, identifies feature importance in a way that effectively moves $f(\\mathbf{x})$ closer to $\\mathbf{y}^*$ by modifying the relevant features of $\\mathbf{x}$.\n\nIn the revised version, we will add the following sentence before \u201cComplexity of COLA\u201d in line 349.\n> By Theorem 4.1, COLA aims to minimize the dissimilarity between $f(\\mathbf{z})$ and $\\mathbf{y}^*$ by modifying $\\mathbf{z}$ based on feature attribution results, which identify the most important features to adjust to achieve the desired counterfactual effect.\n\n**Q2:** \u201cThe connection between $p$-SHAP and $p_{\\text{OT}}$ in Section 4 can be improved. This is the part that seems to be able to benefit the most from clarity and extra sentences connecting these two different topics and situating them in the bigger goals of the paper. A few suggestions \u2026 \u201d\n\n**A2:** We completely agree with the reviewer on this point. Indeed, **our paper establishes a novel connection between the Shapley method and optimal transport**, which has not been previously reported. We appreciate your suggestions and will incorporate additional clarifications to strengthen this section. Below, we address your specific suggestions item by item.\n\n**Q3:** \u201cWhen introducing the OT problem in this section, the reader might benefit from having a sentence or two mentioning what the goal of OT is, in addition to its purpose in p-SHAP. This is somewhat done in row 209, but I believe it would be clearer if it comes earlier in the previous paragraph.\u201d\n\n**A3:** Agree, and thank you for the good suggestion! We will add the following sentences in the revised version when OT is introduced in Section 4, in the beginning of \u201cTheoretical Aspects of $p$-SHAP\u201d.\n>  Intuitively, OT determines the most cost-effective way to move $\\mathbf{x}$ closer to $\\mathbf{r}$. We later prove that the transportation plan $p_{\\text{OT}}$, obtained by solving the OT problem, effectively guides $p$-SHAP in identifying the key features of $\\mathbf{x}$ that need to be modified to bring $f(\\mathbf{x})$ closer to $\\mathbf{y}^*$."
            }
        },
        {
            "comment": {
                "value": "We sincerely appreciate the reviewer for recognizing the importance of the problem the paper addresses, the effectiveness of our method, and the robustness of our empirical results. \n\nResponses to the reviewer are as follows. (**Q** and **A** are questions raised by the reviewer and answers provided by the authors, respectively). \n\n**Q1:** \u201cThe approach is computationally demanding, mainly due to the optimal transport (OT) calculation, which may limit its scalability in large datasets. (which the authors acknowledge.)\u201d\n\n**A1:** In fact, OT is computed really fast: To compute OT without the entropic term (i.e. $\\varepsilon=0$ in (8)), one just solves a linear programming problem fairly fast. To compute OT with the entropic term (i.e. $\\varepsilon>0$ in (8)), the convergence is significantly faster, using  Sinkhorn\u2013Knopp algorithm [1].\n\nThe main computational cost, in practice, stems from the Shapley value calculation. Our proposed CF-$p_{\\text{OT}}$ is as efficient as all other baselines, including RB-$p_{\\text{Uni}}$ [2], RB-$p_{\\text{OT}}$ (a variant of [2]), CF-$p_{\\text{Uni}}$, CF-$p_{\\text{Rnd}}$ (variants of [3]), and CF-$p_{\\text{Ect}}$ [2,3]. The other operations in COLA (i.e., lines 4-16) are polynomial in both the number of data points and data dimensions, requiring significantly less time than computing the Shapley values.\n\n**Q2:** \u201cWhile OT provides a probabilistically informed alignment, it may only sometimes yield the optimal feature attributions for all tasks, as it lacks task-specific tuning and causal considerations. However, the authors denote this as future work.\u201d\n\n**A2:** The proposed COLA is a versatile algorithmic framework that can refine explanations generated by other task-specific CE algorithms. As the reviewer suggested, exploring how to tune COLA towards further adapting to a specific task and considering causal graphs (assumed available) is worth exploring. Thank you for pointing out.\n\n**Q3:** \u201cThe approach is primarily evaluated on binary classification and tabular data, leaving its effectiveness on multiclass or regression and other data types (e.g., images, text) scenarios unexplored. How well would this method generalize to different multiclass and regression settings?\u201d\n\n**A3:** Though our experiments are done for binary classification tasks, we remark that this paper's proposed **COLA and $p$-SHAP apply directly to multiclass classification, regression, and image classifications**. We refer to the illustration in Figure 2 to explain this, as follows.\n- For multiclass classification, lines 1-2 are unaffected. Lines 3-4 involve the computation of the Shapley value, which functions for both binary and multiclass classification tasks (similar to the SHAP Python library), so these steps also remain unchanged. Line 5 and lines 6-16 involve matrix operations on the data, which likewise stay the same.\n- For regression, the only difference compared to binary classification is that the desired counterfactual effect (or target model output), $\\mathbf{y}^*$, is a real value. After determining $\\mathbf{y}^*$ in line 1 of COLA, the rest of the process is independent of $\\mathbf{y}^*$, meaning that all subsequent steps remain unchanged.\n- For (binary or multiclass) image classification, the pixels of an image can be treated as a vector. Specifically, in lines 1-2 of Figure 2, each row in $\\mathbf{x}$ and $\\mathbf{r}$ would represent an image. Thus, all steps remain the same.\n\nOnce again, we thank the reviewer for their insightful comments, and we are open to any further discussions or questions.\n\n### References\n[1] Knight, Philip A. \"The Sinkhorn\u2013Knopp algorithm: convergence and applications.\" SIAM Journal on Matrix Analysis and Applications 30.1 (2008): 261-275.\n\n[2] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in neural information processing systems, 30, 2017.\n\n[3] Albini, Emanuele, et al. \"Counterfactual shapley additive explanations.\" Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency"
            }
        },
        {
            "comment": {
                "value": "### Responses to Other Points\n\nWe then reply to the other points raised, as Q2-Q4 and A2-A4 below.\n\n**Q2:** \u201cThe writing could be clearer. The paper introduces and repeatedly uses terms without defining them: \"exact alignment\" (line 25, 197, etc; finally explained only on line 308), \"well-performed action plans\" (line 98), etc. It would be useful to define the terms at the time of their first use.\u201d\n\n**A2:** Thank you for your good suggestions! In the revised version, we will modify line 25 to be \n> \u2026 it may be misleading to rely on a counterfactual distribution defined by the CE generation mechanism \u2026.\nThis is because the exact alignment literally refers to a known distribution of a counterfactual, which is usually defined by a given CE method [1].\nThen, we will modify line 197 to be \n> \u2026. when $A_{\\text{Prob}}$ defines a joint distribution between $\\mathbf{x}$ and $\\mathbf{r}$, indicating an $i\\leftrightarrow j$ alignment of any $\\mathbf{x}_{i}$ and $\\mathbf{r}_j$.\n\nThis modification reinforces the concept that the distribution determines the alignment, as stated in line 308.\n Besides, we will clarify the term \u201caction plan\u201d in Figure 1 and its caption, in the revised version. \n\n**Q3:** \u201cI do not understand the \"decoupling\" phenomenon mentioned in line 85-86, and the reference to the later demonstration was lost (or I may have missed it). Perhaps more exposition and an explicit reference to where in the paper to find this \"later\" would be helpful.\u201d\n\n**A3:** The original sentence in lines 84-86 reads:\n> \u2026 it is not effective to decouple FA with CE in order to select the most important features to change. We will demonstrate later that this decoupling can result in counterproductive feature modifications \u2026\nThe word \u201cdecouple\u201d here, means that FA is performed **independently** of CE. For example, B-SHAP in (4) and RB-SHAP in (5) are defined independently of CE (i.e. it does not use any information from the CE mechanism or the generated CE data points). Such FA lead to counterproductive feature modifications, as shown by Figure 3 with the curves RB-$p_{\\text{Uni}}$ and RB-$p_{\\text{OT}}$.\n\nIn the revised version, we will modify line 85 as\n\n> \u2026.. it is not effective to perform FA independently of CE to select \u2026.. We will demonstrate later that this decoupling can result in counterproductive feature modifications in the empirical results in Result II of Section 6.\n\n**Q4:** \u201cThe \"counter-intuitive finding\" described in lines 101-105 remained difficult to parse until I had reached the end of the paper, because \"associating\" and \"alignment\" had not been defined yet. These terms can be defined and explained earlier in the paper, so that the findings become easier to understand in a first read. The abstract could also be amended similarly to ensure that it is easier to understand.\u201d\n\n**A4:** We agree with the reviewer that clarifying these terms earlier would be more reader-friendly. The modifications made in A1 above will do the work. Additionally, in line 204, we will further clarify\n> Contrary to common expectations, we demonstrate that OT can be more effective than relying on a counterfactual distribution defined by the CE generation mechanism done by [1], in Result II of Section 6 later.\n\nOnce again, we thank the reviewer for their time and effort in helping us improve the manuscript. We remain open to any further discussions or questions.\n\n### References\n[1] Albini, Emanuele, et al. \"Counterfactual shapley additive explanations.\" Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022.\n\n[2] Kommiya Mothilal, Ramaravind, et al. \"Towards unifying feature attribution and counterfactual explanations: Different means to the same end.\" Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 2021."
            }
        },
        {
            "comment": {
                "value": "(**Intuition of why $p$-SHAP performs better**) Let us revisit our core objective: First, we want to minimize the dissimilarity of the factual $\\mathbf{x}$ and a to-be-found counterfactual $\\mathbf{z}$ to achieve the counterfactual effect $\\mathbf{y}^*$. Second, we want $\\mathbf{z}$ is more similar than $\\mathbf{x}$ compared to the original counterfactual $\\mathbf{r}$. We want an FA method that satisfies both the two needs above. The proposed p-SHAP does so with support from theoretical aspects: The first need is supported by Theorem 4.1, namely, optimal transport (OT) based $p$-SHAP does attribution towards minimizing a upper bound of the dissimilarity between the counterfactual classification $f(\\mathbf{z})$ and the desired counterfactual effect $\\mathbf{y}^*$. The second need is supported by Theorem 5.1, namely, OT-based $p$-SHAP, used in our proposed COLA, achieves better proximity than the original counterfactual under the Frobenius norm. Importantly, the joint distribution leverages CE information to guide $p$-SHAP for effective feature attribution, while $p$-SHAP itself remains independent of the specific CE algorithm used.\n\nIn the revised version, we will slightly reorder the content and provide a brief summary of the intuition outlined above, to help readers better understand why p-SHAP outperforms other methods, as demonstrated by our empirical results."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for recognizing the novelty and effectiveness of our proposed method, and we greatly appreciate the acknowledgment of our exhaustive experiment results. \n\nResponses to the reviewer are as follows. (**Q** and **A** are questions raised by the reviewer and answers provided by the authors, respectively). \n\n### Intuition Behind $p$-SHAP and Why It Performs Well\n\nWe first address the reviewer's concern mentioned in the final point of weaknesses and reiterated in the questions: specifically, the intuition behind why p-SHAP performs better than other variants, as discussed in Q1 and A1 below.\n\n**Q1:** \u201cIt is unclear to me why the proposed SHAP variant performs better than other SHAP variants, or indeed why would SHAP feature attributions be a good way to perform post-hoc optimisations on generated counterfactual explanations at all. The paper presents the proposed method adequately and displays the improvements it confers, but I'm unsure why this is the case. Any intuition or ideas that the authors have about this would be good to include. While additional experiments aren't strictly necessary, I am curious about how non SHAP based methods would perform. I have not looked at the code directly, but if the results are easily replicable, this might lead to rapid future work in this area, providing more understanding of the phenomena reported in the paper. Reordering the writing would help readers make sense of the contributions in a single pass. The inclusion of SHAP seems arbitrary - and although demonstrated to work well - it is unclear why other methods are not included.\u201d\n\n**A1:** Thank you for acknowledging the contributions and impact of our proposed method. We would like to try our best to explain why SHAP feature attribution, if used properly, is a good way to perform post-hoc optimization for CE towards fewer modifications, detailed as follows.\n\n(**Intuition behind using FA**) To create a unified approach for refining generated explanations with minimal changes, a simple idea is to determine which features are most important to modify for each data point to achieve the desired counterfactual outcome. Feature attribution (FA) is for this purpose exactly. \n\n(**Intuition behind using Shapley**) Previous research has shown that performing feature attribution (FA) separately from counterfactual explanations (CE) does not work well for determining the importance of features in achieving a counterfactual outcome [1,2]. Therefore, it makes sense to integrate CE information into the FA process, which should help the attribution results better capture the true importance of each feature in relation to the counterfactual effect. And [1] proposed CF-SHAP, which uses counterfactual data distribution $\\mathcal{D}$. It is shown in [1] that CF-SHAP performs better than the other Shapley methods. This result also aligns with ours in Section 6 and Appendix H. \n\n(**An identified issue for using Shapley with CE**) Yet, CF-SHAP assumes a known counterfactual data distribution (determined by a selected CF algorithm; see [1]). We remark that this sounds tricky: The factual data are collected from real-world and hence their distribution is determined by the physical world, staying **objective**. However, the counterfactual data distribution $\\mathcal{D}$ need to be generated from a CE algorithm. Remark that there are hundreds of CE algorithms, and selecting which to generate $\\mathcal{D}$ is **subjective**. It is important to note that FA is to identify the importance of features of a model\u2019s input. However, the FA method CF-SHAP yields different importance scores to the same input data of a given model: Consider one counterfactual data point $r_0$ generated by two different CE methods \u201cCE-a\u201d and \u201cCE-b\u201d, for an observed factual $x_0$. Since the model, the factual, and the counterfactual are all fixed, the FA results should be determined and unique. But the FA method CF-SHAP would yield different FA results based on which of \u201cCE-a\u201d and \u201cCE-b\u201d is used to generate $\\mathcal{D}$. This thought experiment motivates us to re-think the real need of an FA method for CE, and propose something new to utilize the CE information better rather than associating it with $\\mathcal{D}$ in Shapley value computation."
            }
        },
        {
            "comment": {
                "value": "### Response to Other \"Minor Points 1,3,4,5,6\"\n\n**Q7** \u201c1. Line 51: \"some CE algorithms assume differentiable models, whereas others are designed specifically for tree-based or ensemble models\" -- this is misleading, as many CE algorithms work for black-box ML models, you can find them in the Verma et al. survey paper.\u201d\n\n**A7:**  We would like to **take the reviewer\u2019s suggestion** and revise the sentence to be \u201cTo our best knowledge, there has not been an unified way to refine counterfactuals generated by arbitrary CE algorithms without assumptions on models.\u201d\n\n**Q8:** \u201c3. Line 124 $M_{c_{ik}}$ is not defined which makes understanding Eq 1(d) and 1(e) hard. Also how does having $c_{ik} = 0$ allows no change in its value? I don't see that being implied from the equation\u201d\n\n**A8:**  **There is no $M_{c_{ik}}$ defined in our paper**. In the following response, we assume the reviewer was referring to the notation $M$ that is multiplied to the variable $c_{ik}$. This is a technique in writing mathematical formulation, and is stated in line 134: \u201c\u2026.if $c_{ik}=1$, remark that $M$ is a sufficiently large constant such that $z_{ik}$ has good freedom to change.\u201d To further explain it, if $c_{ik}=1$, (1d) and (1e) becomes $-M\\leq z_{ik}\\leq M$. Since $M$ is large enough, $z_{ik}$ has sufficient freedom. On the contrary, if $c_{ik}=0$, (1d) and (1e) become $z_{ik}\\leq x_{ik}$ and $z_{ik}\\geq x_{ik}$ respectively, and they have to be satisfied simultaneously. Hence $z_{ik}=x_{ik}$, which means that for any $i$ and $k$, $z_{ik}$ is not allowed to change to any value other than $x_{ik}$.\n\n**Q9:** \u201c4. B-SHAP and RB-SHAP have different background distributions and it is clear to the community about the advantages and disadvantages of each. You define CF-SHAP, but never state what trade-off does one expect when using background distribution from CF distribution instead of the training distribution. Please explain what should one expect from such a FA method.\u201d\n\n**A9:**  **This is simply not true**. It is not us who defined CF-SHAP. The researchers propose the CF-SHAP method in [3] and we just cited their work as shown in lines 186-187. As indicated by the authors in [3], CF-SHAP outperforms the Shapley methods that do not use CF distribution when one wants to do contrastive feature attribution. This conclusion aligns exactly with our results in the paper. However, remark that the distribution of counterfactuals is not always available, imposing limitations on this method. Our results further demonstrate that even if such a counterfactual distribution is known, it may not be the best option, as the proposed p-SHAP can outperform it.\n\n**Q10:** \u201c5. Line 197, correct the typo from RB-SHAP to B-SHAP.\u201d\n\n**A10:**  **Thank you for this**. We will correct the typo in our revised version.\n\n**Q11:** \u201c6. The first paragraph of section 4 is obvious, it is just a generalization of the notations from Equations 4-6, I don't think there is a need to give so much attention and explanation for it.\u201d\n\n**A11:**  **Disagree.** Ours in (7a) and (7b) differ fundamentally with (4)-(6), explained as follows. The CF-SHAP proposed in [3] replies on a concrete counterfactual algorithm to define the counterfactual distribution $\\mathcal{D}$ in (6). Hence, CF-SHAP is dependent on a CE algorithm. Our proposed p-SHAP, however, is completely independent of CE algorithms. As for (4) and (5), they do not use counterfactual distribution and differ from ours. This is why this paragraph is mandatory. And we would like to extend this part further in the revised version.\n\n### References\n[1] Guidotti, Riccardo. \"Counterfactual explanations and how to find them: literature review and benchmarking.\" Data Mining and Knowledge Discovery 38.5 (2024): 2770-2824.\n\n[2] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. Advances in neural information processing systems, 30, 2017.\n\n[3] Albini, Emanuele, et al. \"Counterfactual shapley additive explanations.\" Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022.\n\n[4] Ramaravind K. Mothilal, Amit Sharma, and Chenhao Tan (2020). Explaining machine learning classifiers through diverse counterfactual explanations. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency.\n\n[5] Kaivalya Rawal and Himabindu Lakkaraju. Beyond individualized recourse: Interpretable and interactive summaries of actionable recourses. Advances in Neural Information Processing Systems, 33:12187\u201312198, 2020.\n\n[6] Dan Ley, Saumitra Mishra, and Daniele Magazzeni. Globe-ce: a translation based approach for global counterfactual explanations. In International Conference on Machine Learning, pp. 19315\u201319342. PMLR, 2023.\n\n[7] Verma, Sahil, Keegan Hines, and John P. Dickerson. \"Amortized generation of sequential algorithmic recourses for black-box models.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 8. 2022."
            }
        },
        {
            "comment": {
                "value": "**Q5:** \u201cd.) An underlying and unstated key assumption in the formulation of the paper is that sparsity is the most important metric to consider when generating CFEs. This is atleast not agreed upon by the community, see the list of metrics proposed in Verma et al. survey paper.\u201d\n\n**A5**:  **We never had such assumption at all**. \n\nOur goal of seeking more sparse explanations is **NOT** equivalent to \u201cwe assuming that sparsity is the most important metric to consider when generating CE explanations\u201d. It is out of the scope of this paper to discuss which metric is the most important to be considered when generating CE explanations.  \n\nIn our opinion, which metric is the most important would-be task-specific. We refer to [Fact 2], that one should select a proper CE method, and, if sparsity is desired, then apply our proposed method to refine the explanations.\n\n**Q6:** \u201cThe paper needs to really strengthen its experimental results section. I would expect something like a table, where the columns are the various metrics along with a CFE is evaluated, like validity, proximity, sparsity, distance of training data manifold, adherence to causal constraints, and the time to generate the CFEs (including dependence on any other technique) and the rows to be a large set of baselines.Only when the numerical results demonstrate the superiority of COLA, can the supporting theory, theorems, and the claims of near-optimal performance be useful.\u201d\n\n**A6**:  **We have already reported validity, proximity, and sparsity** in Table 3. Namely, \n- Validity is referred to as \u201ccounterfactual effect\u201d and is shown in columns 3 and 4 in Table 3, and we are showing how the refined explanations remain the desired counterfactual with significantly better sparsity. \n- Proximity is $\\|\\mathbf{z}-\\mathbf{x}\\|$, and we already reported in the subcolumns of columns 3 and 4 in Table 3 (normalized by $\\|\\mathbf{r}-\\mathbf{x}\\|$). \n- Sparsity is reported as the \u201c# of Modified Features\u201d in columns 3 and 4. Our proposed Shapley method is benchmarked towards other four Shapley methods proposed in [2] and [3] combined with different joint probability distributions setups.\n\nOur experiments are designed to demonstrate the versatility of the proposed COLA and the effectiveness of the proposed p-SHAP. (See A3 above). \n\nAt last, as explicitly mentioned by the other **Reviewers iyvf, hbcQ, WJ7H**, our experiments are **exhaustive and robust**. Thus, we argue that **this judgment of \u201cextremely weak experiments\u201d is not fair**.\n\n### Response to \u201c2. [Major]\u201d\n\nThe problem being addressed in this paper is challenging because:\n\n- (For the first challenge mentioned) To our best knowledge, existing sparse CE algorithms are task-specific, which means none of them can be used solely for various tasks. It\u2019s unrealistic that expecting one single existing (sparse) CE algorithm would meet all the needs universally for various CE tasks. \nAlso, except for this paper, there exist no refinement mechanism that works for arbitrary models or CE tasks (see [Fact 2]).  \nNote that the proposed COLA is a sparsification mechanism used to refine the explanations generated from existing CE algorithms. So even if there were already some \u201cuniversally applicable sparse CE method\u201d, the proposed COLA would still be applicable to it in order to sparsify its generated explanations further (see [Fact 1]). \nHence, **whether a general-purpose sparsification mechanism is possible remains open and challenging**. \n- (For the second challenge mentioned) First, existing *sparse* CE algorithms usually have some assumptions about the model (e.g., model differentiability, model architecture, available causal graph, specific tasks, focused CE metrics, etc.). If the reviewer could point out a concrete work, we\u2019d happily refer to the work in our paper (see [Fact 2]). Second, the question of **whether a versatile mechanism exists without assumptions about the model remained open and challenging** before our work in this paper. \n- (For the third challenge mentioned). Some researchers have tried to use FA in finding CEs [3]. However, it is *obviously* challenging to integrate FA (like Shapley) to improve CE, because FA mechanisms are not specifically or essentially designed for CE.  Using FA like feature importance to find CEs may be misleading because, as the reviewer stated, \u201cA feature that is marked as important by some FA method does not necessarily mean that it is an effective way to change the prediction\u201d. \nSo **finding an effective FA that also works for CE tasks is challenging**, which is addressed in this paper. It\u2019s also worth noting that our proposed method p-SHAP is demonstrated to outperform existing work [2,3] in finding proper FAs for CEs and obtaining sparser CEs."
            }
        },
        {
            "comment": {
                "value": "**Q2**: \u201c2. The experimental results are very weak for the following reasons: a.) Result 1 in Section 6 (lines 415 - 421) are not useful when a datapoint does not achieve the desired classification. In other words, if a datapoint does not achieve y* classification (which makes it a valid CFE), there is no point in mentioning the sparsity of that, because it is an invalid and not useful CFE. Therefore the third column in Table 3 is not useful at all. \u201d\n\n**A2**:  The reviewer **misunderstood** the mechanism of COLA. \n\nAs COLA is to refine/sparsify the explanations generated by other CE methods (see [Fact 1]), that is to say, the to-be-refined explanations already achieve the desired classification (referred to as \u201cthe target outcome $\\mathbf{y}^*$\u201d in line 269, which is formally defined as $\\mathbf{y}^*=f(\\mathbf{r})$ in line 260). Otherwise, one would seek an alternative CE method rather than the selected one to obtain the desired classification first. Therefore, The third and fourth columns in Table 3 suggest exactly how good sparsity our method COLA could achieve compared to not using it. \n\nBesides, to our best knowledge, none of the hundreds of CE methods surveyed in [1] could guarantee an arbitrary desired counterfactual effect. We\u2019d appreciate it if the reviewer would clarify the question further in case it bothers you. \n\n**Q3**: \u201cb.) Several baselines are mentioned in lines 368-372, but none of them are used in the experimental results section? Why? Aren't these the baselines that you need to compare against to show that your technique is better than them atleast in the one metric you are targeting (which is sparsity)? Instead what you do in Table 3 is compare against 5 variations of your own proposed method. Sure one of your own method is better than the other 4 you mention, that does not tell me anything about how does COLA compare to previous techniques. Please let me know if you disagree. Therefore Result 2 is not useful. \u201d\n\n**A3**:  **These stay entirely away from the truth**. \n\nWe clarify in [Fact 3] that: 1) lines 368-372 indicate the CE algorithms (not baselines) that we used in the proposed COLA, to evaluate its versatility, and the empirical results are provided in Figures 3 and 4 and Appendix H; 2) the baselines to evaluate the proposed p-SHAP are set in Table 2 and empirical results are provided in Table 3, Figures 3 and 4, and appendix H; 3) other rows in Table 3 except CF_$p_\\text{OT}$ are not 5 variations of our own proposed methods. \n\nIn fact, RB-$p_{\\text{Uni}}$ is exactly the random Shapley proposed in [2]. RB-$p_{\\text{Uni}}$ follows the random Shapley in [2] but changes the joint distribution to the same as CF-$p_{\\text{ot}}$ for the ablation study. CF-$p_{\\text{Uni}}$ and CF-$p_{\\text{Rnd}}$ are the Counterfactual Shapley proposed in [3]. Since it assumes a known counterfactual distribution conditional on each factual data point, whereas it is unknown in our experiment, we used $p_{\\text{Uni}}$ and $ p_{\\text{Rnd}}$ instead. CF-$p_{\\text{Ect}}$ is exactly the Counterfactual Shapley proposed in [3]. When there is a one-to-one alignment between factual and counterfactual data points, Counterfactual Shapley degrades to the Baseline Shapley proposed in [2]. \n\nHence, **it is NOT true that we compare our proposed one with its variations**.\n\n **Q4:** \u201cc.) The baselines considered in the paper (the results for which are not reported atleast in the main paper), are outdated and several new techniques have been proposed that are better at all metrics than DiCE and KNN, for e.g. see Amortized Generation of Sequential Algorithmic Recourses for Black-box Models.\u201d\n\n**A4**:  **This is simply not true**.\n\nAs we mention before, CE algorithms are not baselines, and all their results have been reported in Table 3, Figures 3 and 4, as well as Appendix H. Besides, **these CE methods that we use are not outdated, either**. In fact, DiCE [4] was proposed in 2020 and is still the most commonly used CE method in the industry and academia nowadays; KNN (as a CE method) [3] was proposed in 2022; AReS [5] was proposed in 2020; GLOBE [6] was proposed in 2023. By contrast, the specific literature [7] mentioned by the reviewer emerged in 2021 on arXiv and published in early 2022. Moreover, **we do not see why the performance of CE methods matters here**, because the proposed COLA can refine explanations generated by any CE methods (including [7]) regardless of which performs better than the others."
            }
        },
        {
            "comment": {
                "value": "### Clarification of Misunderstandings\n\nWe would like to point out some misunderstandings of Reviewer hZrx and clarify several important facts before replying to their specific comments:\n\n**[Fact 1]**: The paper does NOT propose a new CE method. Instead, there are two proposed things: One is the proposed OT-based p-SHAP, used for feature attributions for refining counterfactual explanations. The other is a versatile mechanism to refine/sparsify explanations generated by other CE methods. We have clearly stated this in the main contributions of Section 1.\n\n**[Fact 2]**: To our knowledge, this is the first work that provides a unified way for refining/sparsifying explanations generated by existing tabular data CE methods, without assumptions on the machine learning models and the CE methods. We\u2019d happily refer to others\u2019 work for this if there is any.\n\n**[Fact 3]**: Due to [Fact 1], the baselines in this paper are not CE methods. \n1) To demonstrate the validity and effectiveness of COLA, we have provided its empirical results (in Figures 3 and 4, and Appendix H) under different scenarios (i.e. using COLA to refine the generated counterfactuals from different CE methods for different ML algorithms). \n\n2) To explore the potential and importance of using joint distribution informed by Shapley to refine counterfactuals and demonstrate the proposed p-SHAP, we have provided its empirical results in Table 3.  Our baselines are those different Shapley methods and different joint probability distribution setups. In Table 2, CF-$p_{\\text{OT}}$ stands for our proposed OT-based p-SHAP. All the other rows are baselines. We benchmarked our proposed p-SHAP to all the baselines in all the mentioned CE methods stated in lines 368-372.\n\n### Response to \"7.[Really Major]\"\n\nWe disagree with the reviewer about the weakness of the experiment. We have benchmarked our proposed p-SHAP and some other existing Shapley methods (baselines) on 4 datasets, under different scenarios (changing in 5 different CE algorithms and 13 different machine learning algorithms). We believe the experimental results strongly indicated the effectiveness of our proposed COLA. Besides, **the strength of our experimental results has been explicitly acknowledged by Reviewers iyvf, hbCQ, and WJ7H** regarding its exhaustiveness and robustness. \n\nMore detailed responses to the reviewer are as follows. (**Q** and **A** are question raised by the reviewer and answer provided by the authors respectively).\n\n**Q1**: \u201c1. First and the most important weakness is that you need another CFE generation technique to generate a CFEs for a lot of datapoints that you use to compute the joint distribution and then generate a CFE using your technique, is that right? If that is the case, in the paragraph of computation complexity, why do you not include that? Also it is not the case that your technique just takes a CFE from some other technique and refines it, it needs a whole lot of datapoints and CFEs to even start functioning. Please correct me if I am wrong.\u201d\n\n**A1**: **What you said is not correct** for three reasons. \n\nFirst, the proposed COLA is not a CE (referred to by the reviewer as \u201cCFE\u201d) algorithm. Instead, it is a unified mechanism to refine (or sparsify) explanations generated by any CE algorithm (See [Fact 1]). It is a sparsification mechanism for other CE methods, yet does NOT rely on one specific CE method.\n\nSecond, we have already considered the computation complexity of other CE methods when analyzing the computational complexity. See Lines 348-353 \u201cComplexity of COLA\u201d. Note that $A_{\\text{CE}}$ is the CE algorithm in step 1, and  $O(M_{\\text{CE}})$ indicates its complexity. \n\nThird, our proposed COLA does NOT need many data points and CEs to work. In fact, it also work with one single factual data point and one corresponding CE data point. This is exactly the case of CF-$p_{\\text{Ect}}$ stated in the last row of Table 2, where each factual data point is known to be aligned with another exact counterfactual data point. As we can see from the corresponding results in Figure 4, COLA functions well in this case. The concrete number of the used data points depends on the used CE method."
            }
        },
        {
            "summary": {
                "value": "The paper titled \"Refining Counterfactual Explanations with Joint-Distribution-Informed Shapley Towards Actionable Minimality\" proposes a framework for improving counterfactual explanations (CE) in machine learning models. Current CE methods often include unnecessary feature changes, making them difficult to apply in practical scenarios. The approach proposed in this paper seeks to minimize these changes while maintaining the validity of the counterfactual exercise, making CE more actionable for both users and stakeholders, particularly in terms of simplified and actionable decision-making.\n\n*Major contributions of the paper include:*\n1. The setup of a well-defined problem for finding CE with minimal feature changes (Equation 1).\n2. The development of the algorithm \"COunterfactuals with Limited Actions (COLA)\" to find the solution to the previous point.\n\n*Sub-contributions that are incorporated into the definition of COLA:*\n- The definition of a generic Shapley framework (p-SHAP), which nests several other commonly used Shapley frameworks for computing feature importance and allows for incorporating the distributional connection between factual data $x$ and counterfactual data $r$, i.e., their joint distribution $Prob(x,r)$, in its computation.\n- They show that p-SHAP correctly measures the causal behavior of shifting $x$ towards $r$ (Theorem 4.2).\nThey propose Optimal Transport (OT) techniques to recover $Prob(x,r)$, given $x$ and $r$, which can be incorporated into p-SHAP for CE. The OT guarantees tighter bounds on the distance between factual and counterfactual, $D(f(x), y^*)$, under the assumption that the model $f$ is Lipschitz continuous and $y^* = f(r)$ (Theorem 4.1).\n- They derive the computational complexity of COLA.\n\nThe authors combine a collection of algorithms in order to construct COLA. Each of the algorithms is listed below:\n\na. FIND $r$: For a counterfactual $y^*$ and a factual $x$, find $r$ within an $\\epsilon$ radius of $x$ to minimize the distance $D(y^*, f(r))$.\n\nb. COMPUTE $Prob(r,x)$: Once $r$ is found, use it to estimate the joint density $Prob(x,r)$, (mostly) with OT.\n\nc. COMPUTE p-SHAP values: Once $Prob(x,r)$ is obtained, use it to estimate p-SHAP values $\\phi$ between $x$ and $r$.\n\nd. OBTAIN THE CE: Using $\\phi$, together with $Prob(x,r)$ and $r$, edit $x$ minimally in order to obtain the CE of $y^*$ from $x$, called $z$.\n\n*Numerical Results:*\n- The authors test their algorithm using four different datasets, where the task is classification. They show that the algorithm is feasible and achieves $z$ with empirically minimal changes compared to five other CE algorithms, with minimal or zero misalignment $D(f(z), y^*)$, across 12 different classifier models.\n- In simpler setups, the authors compare COLA\u2019s performance in terms of finding the $z$, measuring its performance $D(f(z), y^*)$, while comparing it to the optimal theoretical performance. Although not achieving optimality for some scenarios, COLA sometimes still outperforms CE methods that use exact alignment."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper attempts to contribute to important theoretical explainability literature, in addition to connecting to real-world demands to make model explanations more actionable. This is a growing demand in several areas of society, especially with current and incoming AI regulation.\n- The authors thankfully write for a broader audience without being too verbose. Although most of the paper\u2019s contribution is presented starting in Section 4, the content before this section provides several points of entry and the exact amount of revision a reader needs to understand their framework. I personally learned from their exposition.\n- The method is mostly sound and tries to be as generic as possible (e.g., as p-SHAP is a generic formulation), and the authors provide intuition for most of their steps. Overall, the paper seems well-polished and organized, lacking only a few important adjustments. \n- A clear algorithm is defined, consisting of a collection of widely used algorithms, and its computational complexity is derived, which is informative for scalability. \n- Their methodology is tested in a wide variety of settings, i.e., using four different divergence functions, four different datasets, comparing COLA to five other CE algorithms, and utilizing 12 different classifiers. This is useful to demonstrate the robustness of their method compared to others."
            },
            "weaknesses": {
                "value": "*PRESSING ISSUES*\n- COLA has a well-defined procedure to compute $z$. It seems clear from Theorem 5.1 that $z$ would satisfy the constraints from Equation 1, but it is not clear how the chosen $z$ from COLA is actually optimal, i.e., why is it minimizing, or closer to minimizing, the problem in Equation 1, given that the last algorithm in COLA to find $z$ does not minimize any function. Even if it is not optimal, as suggested by Figure 4, the paper would benefit from having a clearer narrative of why COLA is closer than other methods to optimal, considering the minimization problem in Equation 1.\n- The connection between $p-SHAP$ and $p_{OT}$ in Section 4 can be improved. This is the part that seems to be able to benefit the most from clarity and extra sentences connecting these two different topics and situating them in the bigger goals of the paper. A few suggestions:\n    - When introducing the OT problem in this section, the reader might benefit from having a sentence or two mentioning what the goal of OT is, in addition to its purpose in p-SHAP. This is somewhat done in row 209, but I believe it would be clearer if it comes earlier in the previous paragraph.\n    - It would be great to have a clear explanation/insight in the paper about why $A_{prob}$ does not necessarily require knowledge of how $r \\sim \\mathcal{D}$ is generated in order to achieve good CE performance.\n    - In the \u201cTheoretical Aspects of p-SHAP\u201d, it would be helpful to know more precisely why using OT for obtaining the joint distribution $Prob(r,x)$ helps later with minimizing $D(f(z),y^*)$. This connection doesn\u2019t seem to be precise as it is.\n    - Theorem 4.1 is labeled as \u201cp-SHAP towards CE\u201d, but the theorem is much more a result of OT + Lipschitz continuity of $f$, and its direct connection to p-SHAP seems to be missing.\n\n*OTHER*\n- The paper provides so many entry points for the reader, and it would be nice to also have the definition of \u201cExact Alignment\u201d and \u201cFeature Alignment (FA) performance\u201d at least once in a footnote.\n- Perhaps writing a sentence at the beginning of Section 3, and/or at the end of Section 2, so the reader knows what to expect in the transition from Section 2 to Section 3, and how they fit in the big picture of the paper.\n- Row 472: Result III could benefit from having a sentence entry point to situate the reader as to why this is important and how it fits in the big picture of the paper. Furthermore, another sentence would be useful to provide insights on why $CF-P_{OT}$ outperforms $CF-P_{Ect}$.\n- Please add titles to the x-axes of Figures 3 and 4, either in the image or in their descriptions."
            },
            "questions": {
                "value": "Specific suggestions (please feel free to ignore these if I didn\u2019t understand things well):\n- In order to make this methodology widespread and for reproducibility, I don\u2019t know if the authors considered converting their code (in the supplemental material) to a Python package or creating a Python class? I am not sure if the editor should require this for publication?\n- Typo on row 196? \u201cFirst p-SHAP degrades to RB-SHAP\u201d, shouldn\u2019t it be B-SHAP?\n- Typo on row 474? Word alignment misspelled?\n- Perhaps clarify on row 231 that it is the tightest Lipschitz upper bound, not necessarily the tightest upper bound."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a post-hoc modification of a set of counterfactual explanations. It does so with the aim of reducing the modification-costs of the counterfactual explanations found. To do so, it employs a new variant of the common SHAP feature attribution technique, which explicitly includes both in-distribution and counterfactual-distribution data in the SHAP baseline prediction computation. The feature attributions so obtained are used to modify the existing set of counterfactual explanations, and it turns out that this modification reduces the costs associated with the counterfactuals."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper presents a new problem perviously unreported in the literature, and solves it effectively. The empirical findings seem to be robust and diverse. The paper presents the proposed method adequately and displays the improvements it provides through exhaustive experiments."
            },
            "weaknesses": {
                "value": "The writing could be clearer. The paper introduces and repeatedly uses terms without defining them: \"exact alignment\" (line 25, 197, etc; finally explained only on line 308), \"well-performed action plans\" (line 98), etc. It would be useful to define the terms at the time of their first use.\n\nI do not understand the \"decoupling\" phenomenon mentioned in line 85-86, and the reference to the later demonstration was lost (or I may have missed it). Perhaps more exposition and an explicit reference to where in the paper to find this \"later\" would be helpful.\n\nThe \"counter-intuitive finding\" described in lines 101-105 remained difficult to parse until I had reached the end of the paper, because \"associating\" and \"alignment\" had not been defined yet. These terms can be defined and explained earlier in the paper, so that the findings become easier to understand in a first read. The abstract could also be amended similarly to ensure that it is easier to understand.\n\nIt is unclear to me why the proposed SHAP variant performs better than other SHAP variants, or indeed why would SHAP feature attributions be a good way to perform post-hoc optimisations on generated counterfactual explanations at all. The paper presents the proposed method adequately and displays the improvements it confers, but I'm unsure why this is the case. Any intuition or ideas that the authors have about this would be good to include. While additional experiments aren't strictly necessary, I am curious about how non SHAP based methods would perform. I have not looked at the code directly, but if the results are easily replicable, this might lead to rapid future work in this area, providing more understanding of the phenomena reported in the paper."
            },
            "questions": {
                "value": "Reordering the writing would help readers make sense of the contributions in a single pass. The inclusion of SHAP seems arbitrary - and although demonstrated to work well - it is unclear why other methods are not included."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes an algorithm to compute CFEs for datapoints such that a minimal number of features are changed, thereby generating sparse CFEs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper establishes an excellent mathematical framework to generate CFEs such that they change the smallest number of features from the original datapoint. This framework connects CFEs with Feature attribution technique SHAP."
            },
            "weaknesses": {
                "value": "There are several weakness in the paper, I list them as follows. The authors only need to address the ones marked as major for the rebuttal (if they like to):\n1. Line 51: \"some CE algorithms assume differentiable models, whereas others are designed specifically for tree-based or ensemble models\" -- this is misleading, as many CE algorithms work for black-box ML models, you can find them in the Verma et al. survey paper. \n\n2. [Major] Line 79: \"Three major challenges remain in addressing this problem\". The next three lines do not sounds like challenges at all. The first one says that one CE algorithm will not suffice (which is not true as past work have proposed algorithms that work for generating sparse CEs), the second one about not assuming anything about the model (this is not novel several works have done this), and the third is about how feature attribution techniques can be misleading (which is obvious because they are not optimized to be used as CE, a feature that is marked as important by some FA method does not necessarily mean that it is an effective way to change the prediction). Please state the challenges (if there are any).\n\n3. Line 124 $M_{c_{ik}}$ is not defined which makes understanding Eq 1(d) and 1(e) hard. Also how does having $c_{ik}$ = 0 allows no change in its value? I don't see that being implied from the equation\n\n4. B-SHAP and RB-SHAP have different background distributions and it is clear to the community about the advantages and disadvantages of each. You define CF-SHAP, but never state what trade-off does one expect when using background distribution from CF distribution instead of the training distribution. Please explain what should one expect from such a FA method. \n\n5. Line 197, correct the typo from RB-SHAP to B-SHAP.\n\n6. The first paragraph of section 4 is obvious, it is just a generalization of the notations from Equations 4-6, I don't think there is a need to give so much attention and explanation for it. \n\n7. [Really Major]. You mention a lot of details about the optimal transport thing to compute the most sparse CE, and do a really good job at explaining Algorithm 1 in Section 5. At this point, I am excited and was anticipating some great experimental results because you spent such a large part of the paper establishing strong mathematical grounds for generating CFEs and even proved some theorems, but the experimental section is **extremely weak** and does not convince me at all about the relevance of the propose technique. \n       1. First and the most important weakness is that you need another CFE generation technique to generate a CFEs for a lot of datapoints that you use to compute the joint distribution and then generate a CFE using your technique, is that right? If that is the case, in the paragraph of computation complexity, why do you not include that? Also it is not the case that your technique just takes a CFE from some other technique and refines it, it needs a whole lot of datapoints and CFEs to even start functioning. Please correct me if I am wrong. \n\n       2. The experimental results are very weak for the following reasons:\n              **a.)** Result 1 in Section 6 (lines 415 - 421) are not useful when a datapoint does not achieve the desired classification. In other words, if a datapoint does not achieve y* classification (which makes it a valid CFE), there is no point in mentioning the sparsity of that, because it is an invalid and not useful CFE. Therefore the third column in Table 3 is not useful at all. \n\n             **b.)** Several baselines are mentioned in lines 368-372, but none of them are used in the experimental results section? Why? Aren't these the baselines that you need to compare against to show that your technique is better than them atleast in the one metric you are targeting (which is sparsity)? Instead what you do in Table 3 is compare against 5 variations of your own proposed method. Sure one of your own method is better than the other 4 you mention, that does not tell me anything about how does COLA compare to previous techniques. Please let me know if you disagree. Therefore Result 2 is not useful. \n\n             **c.)** The baselines considered in the paper (the results for which are not reported atleast in the main paper), are outdated and several new techniques have been proposed that are better at all metrics than DiCE and KNN, for e.g. see Amortized Generation of Sequential Algorithmic Recourses for Black-box Models. \n\n            **d.)** An underlying and unstated key assumption in the formulation of the paper is that sparsity is the most important metric to consider when generating CFEs. This is atleast not agreed upon by the community, see the list of metrics proposed in Verma et al. survey paper."
            },
            "questions": {
                "value": "I have stated all the questions in the weakness section of the review. The paper needs to really strengthen its experimental results section. I would expect something like a table, where the columns are the various metrics along with a CFE is evaluated, like validity, proximity, sparsity, distance of training data manifold, adherence to causal constraints, and the time to generate the CFEs (including dependence on any other technique) and the rows to be a large set of baselines. Only when the numerical results demonstrate the superiority of COLA, can the supporting theory, theorems, and the claims of near-optimal performance be useful."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenge of creating valid and actionable counterfactual explanations by minimizing unnecessary feature changes. The authors introduce the COLA framework, which computes a joint distribution between observed and counterfactual data to inform Shapley values for feature attributions. This joint-distribution-informed approach allows them to refine the counterfactual explanations without imposing restrictions on model types or algorithms to compute the counterfactuals."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper addresses the important task of making counterfactual explanations actionable by minimizing unnecessary feature changes.\n\nThe authors offer a detailed theoretical basis for their methodology with clear explanations and motivations. \n\nBy sharing their code and giving computational details, the author makes the approach more accessible and transparent.\n\nThe results seem robust and the counterintuitive finding that relying solely on exact alignment between factual and counterfactual data can be suboptimal is interesting."
            },
            "weaknesses": {
                "value": "The approach is computationally demanding, mainly due to the optimal transport (OT) calculation, which may limit its scalability in large datasets. (which the authors acknowledge.)\n\nWhile OT provides a probabilistically informed alignment, it may only sometimes yield the optimal feature attributions for all tasks, as it lacks task-specific tuning and causal considerations. However, the authors denote this as future work.\n\nThe approach is primarily evaluated on binary classification and tabular data, leaving its effectiveness on multiclass or regression and other data types (e.g., images, text) scenarios unexplored."
            },
            "questions": {
                "value": "How well would this method generalize to different multiclass and regression settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}