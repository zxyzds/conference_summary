{
    "id": "dp1BH2bK4Y",
    "title": "Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives",
    "abstract": "The Chain-of-Thought (CoT) paradigm has become a pivotal method for solving complex problems. However, its application to intricate, domain-specific tasks remains challenging, as large language models (LLMs) often struggle to accurately decompose these tasks and, even when decomposition is correct, fail to execute the subtasks effectively. This paper introduces the Re-TASK framework, a novel theoretical model that revisits LLM tasks from the perspectives of capability, skill, and knowledge, drawing on the principles of Bloom's Taxonomy and Knowledge Space Theory. While CoT offers a workflow perspective on tasks, the Re-TASK framework introduces a Chain-of-Learning view, illustrating how tasks and their corresponding subtasks depend on various capability items. Each capability item is further dissected into its constituent aspects of knowledge and skills. Our framework reveals that many CoT failures in domain-specific tasks stem from insufficient knowledge or inadequate skill adaptation. In response, we combine CoT with the Re-TASK framework and implement a carefully designed Re-TASK prompting strategy to improve task performance. Specifically, we identify core capability items linked to tasks and subtasks, then strengthen these capabilities through targeted knowledge injection and skill adaptation. We validate the Re-TASK framework on three datasets across the law, finance, and mathematics domains, achieving significant improvements over the baseline models. Notably, our approach yields a remarkable 44.42\\% improvement with the Yi-1.5-9B model and a 33.08\\% improvement with the Llama3-Chinese-8b on the legal dataset. These experimental results confirm the effectiveness of the Re-TASK framework, demonstrating substantial enhancements in both the performance and applicability of LLMs.",
    "keywords": [
        "LLM",
        "Task",
        "Capability",
        "Knowledge",
        "Skill",
        "Chain-of-Thought"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We propose the Re-TASK framework, combining Chain-of-Thought with targeted knowledge injection and skill adaptation, to improve LLM performance on complex, domain-specific tasks.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=dp1BH2bK4Y",
    "pdf_link": "https://openreview.net/pdf?id=dp1BH2bK4Y",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces the Re-TASK framework, which aims to enhance the performance of LLMs in domain-specific tasks by integrating domain-specific knowledge acquisition and skill adaptation. By combining the CoT approach with Re-TASK, the authors propose a novel prompting strategy that targets both capability and knowledge aspects within tasks. Their approach demonstrates performance improvements across three domains (law, finance, and mathematics) using open-source LLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The framework\u2019s design is motivated by a learning theory, which adds a theoretical foundation to the methodology.\n- The distinction between knowledge and skill acquisition is an interesting concept, reflected in the architecture and prompting strategy.\n- The results are promising for domain-specific applications"
            },
            "weaknesses": {
                "value": "- A primary limitation is the choice to limit experiments to open-source models. It\u2019s not clear why the authors couldn\u2019t include a proprietary model like GPT-4, as this would help validate how the framework performs across a wider range of LLMs.\nThe process of constructing capabilities seems relatively manual, which may restrict the framework's scalability to broader, less-defined tasks outside highly specialized domains.\n- Constructing capabilities still seem fairly manual at this point, which might make it challenging to apply this framework to more general tasks. With specialized domains, it\u2019s a bit easier to pre-define relevant knowledge, but for broader tasks, the approach could lose some of its practicality.\n- Retrieving and injecting relevant knowledge has been explored in previous work in RAG. It would be helpful to clarify how this approach is distinct from those existing methods.\n- Section 4.3 on automatically constructing capability items could use more detail, especially since this part feels critical for broader applications.\n- While the framework shows significant gains in the law domain but not as much in two other domains. This might stem from manually obtained procedural knowledge. This suggests the framework might need predefined knowledge assumptions, which could limit its flexibility."
            },
            "questions": {
                "value": "- The term \u201cskill adaptation\u201d felt somewhat ambiguous, though Section 3.2 provided more clarity with its explanation of conceptual and procedural knowledge. Authors could consider adding a more concrete example to illustrate this concept clearly in the main text\n- Providing a real-world example in the main paper would greatly help readers to understand the core components of the framework"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an LLM prompting framework called Re-TASK, which utilizes ideas from education and cognitive science to improve prompting efficacy. Specifically, the framework leverages Bloom's Taxonomy and defines concepts like tasks, capability items, knowledge, and skills. It employs a more structured approach to complete a given task through a series of instructions/subtasks. Meanwhile, the order of these subtasks is guided by principles borrowed from the Knowledge Space Theory. The paper shows that the proposed framework is effective by testing it with some Chinese LLMs on law/finance-related tasks and testing it with llama and mistral on a math dataset."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Integrating ideas from education and cognitive science into improving LLM prompting is quite novel."
            },
            "weaknesses": {
                "value": "Main conerns:\n1. The prompting framework requires significant manual effort and domain expertise---it can only be done by experts who can successfully identify the capacity items, decompose the task, and apply all structured prompting techniques. The bar for using it seems too high.\n2. Most evaluation uses custom datasets, which makes it hard to compare the proposed framework on more general tasks with a wider range of related methods. Also, the fact that two of the three evaluation domains are limited to datasets and LLMs in Chinese makes the results less strong.\n3. The paper only considers CoT as baselines. Are there other methods applicable to the datasets?\n4. There's a line of related research that prompts LLMs to generate relevant knowledge and then incorporate the knowledge into the question to improve the answer quality (see for example [1]). How's the proposed method related to and different from these works?\n5. An important axis to evaluating prompting methods is whether the method can also help improve training. For instance, CoT can be used to reformulate training data, and by training with CoT, we can improve the LLM's ability in reasoning. However, the proposed Re-TASK framework seems too complicated and difficult to adapt for training.\n\n\nOther minor issues:\n1. To make the presentation clearer, there could be more introductions on the cognitive science related concepts.\n2. Figure 2 can be moved to earlier (e.g., to the introduction section) to give readers a better sense of the prompting framework. It would also help improve the paper's clarity by accompanying Figur 2 with a concrete example. Currently the illustration is too high-level, abstract, and difficult for readers to understand. There could also be some notation definition in the caption, e.g., what does C_{ij} represent? Adding it could make it easier to understand as a standalone figure.\n3. Section 4.4 seems misplaced. Moving it to the beginning of Section 4 might help improve presentation clarity.\n\nReference\n[1] Generated Knowledge Prompting for Commonsense Reasoning. Liu, Jiacheng and Liu, Alisa and Lu, Ximing and Welleck, Sean and West, Peter and Bras, Ronan Le and Choi, Yejin and Hajishirzi, Hannaneh."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a prompting strategy inspired by education theories on knowledge. Built on top of CoT (chain of thoughts), the ReTASK strategy includes relevant \"capabilities items\" in the prompts to help LLMs break down the task to subtasks, inject relevant knowledge, and demonstrate how the knowledge can be applied.\n\nThe capability items for a given task can be manually curated or generated by strong LLMs.\n\nThe experiments show prompts containing relevant \"capability items\" substantial quality gains over CoT on benchmarks including Chinese legal sentencing, FinanceIQ, and MMLU high school math."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "[Originality] It's a novel idea to apply Bloom\u2019s Taxonomy and Knowledge Space Theory to LLM prompting.\n[Significance] LM prompting strategies have large impacts on the performance and are an important topic to study."
            },
            "weaknesses": {
                "value": "[Clarity] The connections to the educational theories seem strenuous. The term \"capability item\" also seems unnecessarily abstract. A clearer message would be \"hints generated by a large LM improve problem solving of smaller LMs\". The paper can also benefit from providing a detailed description about how high quality hints (\"capability items\") are generated with large LMs, e.g., what's the prompting strategy used?\n\n[Significance] It is not surprising that hints from high quality LLMs of ~70B params can improve the performance of LLMs of size ~7B. The gains on ~30B models seem to be much more modest.\n\n[Significance] If we have a general strategy to generate high quality hints, the experiments should include many more benchmarks, e.g., all of MMLU (instead of limiting to high school math), GPQA, MATH, HumanEval-Python, Arena-Hard, to name a few. A rating from the LMSys Chatbot Leaderboard would also be more convincing."
            },
            "questions": {
                "value": "In the prompt templates show in the appendix (Figure 19 and 20), there are placeholders such as \"[Please Put the Knowledge Recall 1 of Subtasks Here.]\" and \"[Please Put the Knowledge Application of the overall Task Here.]\" \n\nAre these placeholders replaced by paragraphs containing the relevant knowledge in the LM input prompts? If so, what's the process used to retrieve or generate the relevant paragraphs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}