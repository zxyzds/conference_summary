{
    "id": "1pXzC30ry5",
    "title": "RMP-SAM: Towards Real-Time Multi-Purpose Segment Anything",
    "abstract": "Recent segmentation methods, which adopt large-scale data training and transformer architecture, aim to create one foundation model that can perform multiple tasks.\n    However, most of these methods rely on heavy encoder and decoder frameworks, hindering their performance in real-time scenarios.\n    To explore real-time segmentation, recent advancements primarily focus on semantic segmentation within specific environments, such as autonomous driving. However, they often overlook the generalization ability of these models across diverse scenarios.\n    Therefore, to fill this gap, this work explores a novel real-time segmentation setting called real-time multi-purpose segmentation.\n    It contains three fundamental sub-tasks: interactive segmentation, panoptic segmentation, and video instance segmentation. \n    Unlike previous methods, which use a specific design for each task, we aim to use only a single end-to-end model to accomplish all these tasks in real-time.\n    To meet real-time requirements and balance multi-task learning, we present a novel dynamic convolution-based method, Real-Time Multi-Purpose SAM (RMP-SAM). \n    It contains an efficient encoder and an efficient decoupled adapter to perform prompt-driven decoding. \n    Moreover, we further explore different training strategies and one new adapter design to boost co-training performance further. \n    We benchmark several strong baselines by extending existing works to support our multi-purpose segmentation.\n    Extensive experiments demonstrate that RMP-SAM is effective and generalizes well on proposed benchmarks and other specific semantic tasks. \n    Our implementation of RMP-SAM achieves the optimal balance between accuracy and speed for these tasks.\n    Code and model will be available.\n    %",
    "keywords": [
        "segment anything; real-time segmentation; multi-purpose model;"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1pXzC30ry5",
    "pdf_link": "https://openreview.net/pdf?id=1pXzC30ry5",
    "comments": [
        {
            "summary": {
                "value": "The paper presents a real-time, versatile segmentation model capable of interactive segmentation, panoptic segmentation, and video instance segmentation. \nWhile retaining the SAM encoder-decoder structure, the model incorporates an efficient encoder and adapter to enhance performance.\nIn the decoder, RAP-SAM introduces a three-stage pipeline that leverages novel pooling-based dynamic convolutions to refine mask tokens. Following the decoder, two additional prompt adapters are implemented to improve interaction between visual prompts and segmentation tokens.\nRAP-SAM demonstrates efficiency and generalizability across various segmentation benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The model achieves multi-purpose segmentation through an efficient structure and unified training approach.\n2. The paper is well-written and easy to follow.\n3. The experiments on panoptic segmentation, interactive segmentation, and video segmentation are solid, comprehensive, and persuasive, effectively demonstrating the model's contribution."
            },
            "weaknesses": {
                "value": "1. The paper lacks a detailed comparison with other SAM-like methods. A single COCO instance segmentation comparison in Table 4 is insufficient to substantiate claims of superiority over SAM. The results presented in Table 4 are not particularly outstanding. Additional experiments, such as on the SegAny task, with detailed metrics (AP for small, medium, large objects) on COCO instance segmentation, and evaluations with different object detectors, would strengthen the case.\n\n2. Efficiency benchmarks are insufficiently detailed. For a model promoting efficiency, there should be a more comprehensive evaluation across different GPU platforms, such as the 3090 and V100, testing throughput and latency. Additionally, plotting latency versus performance compared to other SAM-like methods would provide a clearer visualization of the model's efficiency."
            },
            "questions": {
                "value": "1. Do you have the results for TopFormer in Table 3? Additionally, please bold the results in all comparison tables for clarity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work addresses the need for real-time multi-purpose segmentation by introducing a novel setting that encompasses interactive, panoptic, and video instance segmentation, striving for a single end-to-end model capable of handling all tasks in real-time. The proposed Real-Time Multi-Purpose SAM (RMP-SAM) utilizes an efficient encoder and a decoupled adapter for prompt-driven decoding, along with innovative training strategies and adapter designs, demonstrating effectiveness and strong generalization across benchmarks and specific semantic tasks while achieving an optimal balance between accuracy and speed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.Demonstrates impressive performance and inference speed.\n\n2.Filling the gap in real-time multi-purpose segmentation.\n\n3.The whole method is very simple and easy to understand.\n\n4.Code is provided for easy reproduction by the reader."
            },
            "weaknesses": {
                "value": "1.Based on existing technology development, the entire pipeline is not novel.\n\n2.Differences with SAMv2 should be further clarified, especially in terms of claimed semantic labels?"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a real-time multi-purpose segmentation model called RMP-SAM. RMP-SAM handles various tasks such as interactive segmentation, panoptic segmentation, and video instance segmentation using a single model. To balance the accuracy and speed, RMP-SAM utilizes a lightweight encoder and a dynamic convolution-based decoder. RMP-SAM achieves fast inference while maintaining satisfactory performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- RMP-SAM unifies interactive segmentation, panoptic segmentation, and video instance segmentation within a single model.\n- RMP-SAM offers a good trade-off between speed and accuracy.\n- Extensive experiments demonstrate the model's effectiveness."
            },
            "weaknesses": {
                "value": "- The authors do not provide detailed information for joint training. Joint training for multiple tasks can be complex. How do the authors train RMP-SAM for some potential problems, such as avoiding the model being dominated by a single task and performance degradation by conflicts between different tasks?\n\n- This paper ignores some related methods, making it difficult to assess the model's performance relative to existing SOTA approaches. For example, some universal methods[1,2,3] obtain better results than RMP-SAM using ResNet50. The authors should make a comprehensive comparison with other methods. \n\n[1] Tube-Link: A flexible cross tube framework for universal video segmentation. CVPR 2023.\n\n[2] Dvis: Decoupled video instance segmentation framework. CVPR 2023.\n\n[3] Univs: Unified and universal video segmentation with prompts as queries. CVPR 2024."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors explore a novel real-time segmentation setting called real-time multi-purpose segmentation. It contains three fundamental sub-tasks: interactive segmentation, panoptic segmentation, and video instance segmentation. In contrast to previous methods that use a separate design for each task, the authors use only a single end-to-end model to handle all these tasks in real time. To fulfill the real-time requirements and balance multitask learning, a new dynamic convolution-based method, Real-Time Multi-Purpose SAM (RMP-SAM), is introduced. They benchmark several strong baselines by extending existing work to support multi-purpose segmentation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Large models can perform many tasks, but are not real-time capable because of the large encoders, while real-time models are often specialized in only one task. The method presented here aims to combine the two things, i.e., \"the first real-time multi-purpose segmentation model\".\n- Precise implementation details are given and the comparisons with the other methods appear to be fair.\n- The method achieves good results in the trade-off between performance and speed across the various tasks and datasets.\n- The ablation studies are useful and show interesting insights."
            },
            "weaknesses": {
                "value": "- Many architectural elements were adopted from other works, it is not clear to me if there are already similar architectures as proposed here, or where exactly is the innovation (except the jointly training).\n- In the related work section, many works are cited and also compared at the task level, but I also miss a comparison at the architectural level.\n- The tables, especially table 3, are difficult to read because nothing is in bold print and you have to search for the trade-off here. A plot like Fig. 1b would be more useful.\n- The references to the appendix could be a little more precise and there is no reference to Table 2."
            },
            "questions": {
                "value": "- What does the dot size in Fig. 1b indicate?\n- The abstract says \"generalization ability of these models across diverse scenarios\", a learnable classifier with CLIP text embeddings is also used and \u201csegment anything\u201d is in the title. Is there a connection to open-vocabulary?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}