{
    "id": "OYTDePFRLC",
    "title": "Forward Learning with Differential Privacy",
    "abstract": "Differential privacy (DP) in deep learning is a critical concern as it ensures the confidentiality of training data while maintaining model utility.  Existing DP training algorithms provide privacy guarantees by clipping each individual backpropagated gradient and then injecting noise. Different from backpropagation, forward-learning algorithms based on perturbation inherently utilize randomness to estimate the gradient of each sample in parallel. These algorithms offer high parallelizability, suitability for non-differentiable modules, and applicability in black-box settings. Moreover, the introduction of noise during the forward pass indirectly provides randomness protection to the model parameters and their gradients, suggesting its potential for naturally providing differential privacy. In this paper, we propose a forward-learning algorithm, Differential Private Unified Likelihood Ratio method (DP-ULR), and demonstrate its differential privacy guarantees.  DP-ULR features a novel batch sampling operation with rejection, which we theoretically analyze in conjunction with classic differential privacy mechanisms. DP-ULR is also underpinned by a theoretically guided privacy controller that dynamically adjusts noise levels to manage privacy costs effectively in each training step. Our experiments indicate that DP-ULR achieves competitive performance compared to traditional differential privacy training algorithms based on backpropagation, maintaining the same privacy loss limits.",
    "keywords": [
        "Differential Privacy",
        "Forward Learning",
        "Likelihood Ratio Method"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=OYTDePFRLC",
    "pdf_link": "https://openreview.net/pdf?id=OYTDePFRLC",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces the Differential Private Unified Likelihood Ratio (DP-ULR) method,to achieve differential privacy in deep learning through forward learning. Unlike traditional backpropagation-based methods, DP-ULR leverages the inherent randomness of forward learning to estimate gradients."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This work studies an important problem with some new insights. \n\n2. Extensive theoretical analysis is provided.\n\n3. The paper is well written."
            },
            "weaknesses": {
                "value": "1. The assumption of this work may not always hold in practice. First, is the number of repeat time K (which is similar to the number of training batches in DP-SGD) related to privacy cost? If so, we cannot assume it to be too large. In other words, the assumption stated in lines 237-238 is not hold. Second, the privacy cost also relies on the assumption of a full-rank covariance matrix of gradient proxies. If these assumptions do not hold, the privacy guarantees may be compromised.\n\n2. DP-ULR is highly sensitive to the choice of parameters such as the noise standard deviation, batch size, and the rejection threshold. However, it is unclear how to select these parameters appropriately. This sensitivity can make it challenging to find the optimal settings for different datasets and models, potentially requiring extensive hyperparameter tuning. \n\n3. As mentioned in the first paragraph of page 2, the original ULR mothed (Jiang et al., 2023) incorporates noise addition into ULR algorithm. Therefore, it appears that the contribution of this work is primarily focuses on the analysis of DP properties. The authors should clearly outline their contributes, highlighting the differences from the original ULR algorithm."
            },
            "questions": {
                "value": "1. In line 198, should be description be \u201cAdd noise to the l-th module\u2019s input\u201d (i.e., \u201coutput\u201d-> \u201cinput\u201d) ?\n\n2. Please also refer to Weaknesses section for more questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposed DP-URL algorithm, which is an improvement of DP-SGD by dynamically adjusting the injected noise into the model."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The proposed algorithm improved the accuracy of DP-SGD without sacrificing the privacy, especially in the large batch-size applications."
            },
            "weaknesses": {
                "value": "The runtime and complexity of the proposed algorithm are not well discussed. Even though DP-SGD does not require gradient clipping, it requires the computation of the Jacobian matrices, whose complexity seems to be cubic if I understand correctly."
            },
            "questions": {
                "value": "What is the complexity of one step in DP-URL? \n\nWhat is the runtime of DP-URL in comparison with DP-SGD?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Forward-learning algorithm is a kind of non-DP algorithm that updates the model parameters without back-propagation, which has advantage on parallelizability and black-box settings.  The paper introduces DP-ULR which attempts to ensure differential privacy (DP) in forward-learning by utilizing randomness in the forward pass. The method leverages a unique batch sampling approach with rejection and integrates a privacy controller to dynamically adjust noise for privacy-cost management. Their experimental results indicate the proposed method shares a comparable performance as the DPSGD algortihm."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The research problem is clearly motivated by the limitation of DP-SGD, such as its infeasibility of black-box modules and differentiable requirement.\n- The paper is well-written, with clear demonstration and algorithm details.\n- The results are comparable to DPSGD, but without the need to perform back-propagation."
            },
            "weaknesses": {
                "value": "- **Insufficient differential privacy protection**: This work aims to provide a final $(\\epsilon, \\delta)$-DP, which by definition should bound the influence of a sample in the dataset, including both feature \\(x\\) and label \\(y\\). However, in Line 199 of Algorithm 1, for each sample $(x_i, y_i) \\in B_t$ in a batch, only the output derived from the input $x_i$ is perturbed while $y_i$ is used with its ground-truth value. Thus, it might be unconvincing that the proposed algorithm guarantees DP.\n\n- **Data-dependent noise magnitude**: As equation (8) shows, the noise magnitude is determined by calculating the minimum spectrum of a matrix, which is non-privatized. Therefore, calculating the sigma itself violates privacy, and the overall differential privacy guarantee might be problematic.\n\n- **The efficiency advantage is unclear compared to ZeroDP**: Starting from Line 278, the algorithm requires one more forward pass to compute the Jacobian matrix and loss without any perturbation. Thus, in total, there are two forwards for every sample. There are several works that privatize loss values obtained in two forward passes in zeroth-order optimization for achieving DP guarantee on protecting every sample. If this work also requires two forward passes, what is the advantage compared to the line of ZeroDP works? In Section 3.4, the authors discuss the efficiency advantage compared to DP-SGD. However, there lacks discussion to compare the advantage to other forward-based DP algorithms, such as the following references:\n  - Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning\n  - Faster Differentially Private Convex Optimization via Second-Order Method\n  - Private Fine-tuning of Large Language Models with Zeroth-Order Optimization"
            },
            "questions": {
                "value": "- How does this compare with the line of work on ZeroDP?\n- Why does using the ground truth $y_i$ still yield a sample-level DP guarantee?\n- Does using the non-privatized minimum spectrum of a matrix to estimate $\\sigma$ violate privacy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors theoretically analyze the feasibility of DP with noise introduced by forward learning and propose a ULR variant, DP-ULR. The paper presents the novel batch sampling operation with rejection, which is used in DP-ULR. The paper also analyzes the performance and privacy guarantee of DP-ULR in both theoretical and experimental terms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The authors discovered that forward learning's inherent randomness can be utilized to achieve DP, which is an enlightening combination of noise introduced by both DP and forward learning. \n\n- The paper introduces a novel sampling-with-rejection technique for DP-ULR, which provides a quadratic privacy amplification. Realizing such a technique may introduce impairment on the privacy cost, the authors propose an assumption for DP-ULR and design experiments to demonstrate that such an impairment is minimal. \n\n- The paper is well written, clearly presenting the motivation for combining DP and forward learning. Besides, the theoretical analysis part is presented in conjunction with traditional methods, which makes it easy for readers to follow."
            },
            "weaknesses": {
                "value": "- The authors mention that DP-ULR inherits certain advantages over backpropagation-based DP-SGD, such as suitability for non-differentiable or black-box settings, high parallelizability, and efficient pipeline design. However, the paper lacks experiments on non-differentiable or black-box settings to support these ideas. Even though current hardware or software implementation may limit the efficiency of DP-ULR, the authors do not provide an asymptotic complexity analysis of DP-ULR either."
            },
            "questions": {
                "value": "- How does DP-ULR perform on non-differentiable modules or in black-box settings as mentioned in the abstract?\n- Could the authors provide an asymptotic complexity analysis of DP-ULR?\n- The authors mention that non-isotropy may introduce redundant noise to DP-ULR, which may impair the accuracy of the gradient estimation. The reviewer wonders about the proportional relationship between this impairment and the number of model parameters. For example, experiments comparing DP-ULR and DP-SGD on different model sizes could be added to the paper to illustrate this relationship. It would help to clarify whether DP-ULR's performance degrades relative to DP-SGD as models become larger."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a new algorithm, the Differential Private Unified Likelihood Ratio method (DP-ULR), which enhances privacy protection in deep learning by utilizing forward-learning techniques instead of traditional backpropagation. DP-ULR integrates noise during the forward pass. Theoretical analysis and experiments claim to show that DP-ULR achieves better performance than DP-SGD."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper proposes a novel approach to DP training other than the popular DP-SGD\n2. The paper conducts reasonable theoretical analysis and experiments."
            },
            "weaknesses": {
                "value": "I think the proposed approach is interesting but I have some questions about it.\n\n1. Misleading DP-SGD baseline: It is common sense that the larger the batch size is, the better performance we can achieve with DP-SGD. See Dormann et al. (https://arxiv.org/pdf/2110.06255), Figure 2 of De et al. (https://arxiv.org/pdf/2204.13650) and Figure 4(b) of Yu et al. (https://arxiv.org/pdf/2306.08842). Why do the results in Table 1 of this paper give the opposite conclusion? Let's put your proposed method aside, and just look at the results of DP-SGD, could authors explain why the performance drops for large batch size when the noise level is the same? Could it be the learning rate set improperly? \n\n2. The results presented in Table 1 reveal that the performance gap between DP-ULR and DP-SGD varies and is relatively small in some instances. To draw more reliable conclusions, it would be beneficial to report the average accuracy across multiple runs, along with the standard deviation. Additionally, the performance of MLP on MNIST appears notably low, even at reduced noise levels.\n\n3. For the entire experiment, it makes more sense to utilize more popular models such as ResNet-18 or Wide-ResNet-16-4 or 40-4 which is widely used in literature such as De et al. Also, it is very important to show whether the proposed method can be applied to the complex model which is more complex than ResNet-5 used in the paper.  BTW, it is not very clear from Figure 4 what is the final test accuracy for DP-SGD and DP-ULR. I can see they are close to each other. Does it mean that the proposed method can not achieve better utility than DP-SGD when dealing with a large model?"
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}