{
    "id": "bBoetBIN2R",
    "title": "NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative",
    "abstract": "Existing video captioning benchmarks and models lack coherent representations of causal-temporal narrative, which is sequences of events linked through cause and effect, unfolding over time and driven by characters or agents. This lack of narrative restricts models\u2019 ability to generate text descriptions that capture the causal and temporal dynamics inherent in video content. To address this gap, we propose NarrativeBridge, an approach comprising of: (1) a novel Causal-Temporal Narrative (CTN) captions benchmark generated using a large language model and few-shot prompting, explicitly encoding cause-effect temporal relationships in video descriptions, evaluated automatically to ensure caption quality and relevance and validated through human evaluation; and (2) a dedicated Cause-Effect Network (CEN) architecture with separate encoders for capturing cause and effect dynamics independently, enabling effective learning and generation of captions with causal- temporal narrative. Extensive experiments demonstrate that CEN significantly outperforms state-of-the-art models, including fine-tuned vision-language models, and is more accurate in articulating the causal and temporal aspects of video content than the second best model (GIT): 17.88 and 17.44 CIDEr on the MSVD and MSR-VTT datasets, respectively. Cross-dataset evaluations further showcase CEN\u2019s strong generalization capabilities. The proposed framework understands and generates nuanced text descriptions with intricate causal-temporal narrative structures present in videos, addressing a critical limitation in video captioning.",
    "keywords": [
        "Video Captioning",
        "Video Narrative",
        "Video Storytelling"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "The new video captioning benchmark and method, CTN captions and CEN network, improve understanding of video narratives by embedding causal-temporal narrative elements in new captions and then, training the network on these captions respectively.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=bBoetBIN2R",
    "pdf_link": "https://openreview.net/pdf?id=bBoetBIN2R",
    "comments": [
        {
            "comment": {
                "value": "Thank you for the clarification regarding the separators. Your clarifications and new text samples satisfy the questions I had.\n\nI want to revise the rating but I'm having some technical issues when doing that, for which I've sent a message to the chairs earlier today."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer 8bE2 - Updated Text for Related Work Section"
            },
            "comment": {
                "value": "We thank the reviewer again for their feedback. We have added the text for the updated related work section here:\n\n**2 RELATED WORK**\n\n**2.1 BENCHMARKS**\n\nMSVD Chen & Dolan (2011) is a benchmark focused on human activities, providing a platform for\nevaluating video captioning models. The captions in MSVD often describe the observable actions\nwithout delving into the underlying motivations or the cause-effect relationships between the events.\nMSR-VTT Xu et al. (2016) is a large-scale benchmark with diverse video content, encompassing\na wide range of topics and genres. The captions often focus on describing the observable content\nwithout capturing the causal links between the events or the temporal progression of the narrative.\nAs a result, models trained on MSVD and MSR-VTT may struggle to generate descriptions that\naccurately reflect the causal and temporal dynamics in the videos.\n\nWhile recent benchmarks like NExT-QA Xiao et al. (2021) and EgoSchema Mangalam et al. (2024)\nhave made significant strides in incorporating causal and temporal reasoning in video understanding,\nthey focus primarily on question-answering tasks rather than generating comprehensive causal-\ntemporal narratives. NExT-QA introduces multi-choice and open-ended question-answering tasks\nfocusing on specific question-answer pairs that often target single events or actions. In contrast, our\nCTN captions provide a more comprehensive narrative that captures the causal and temporal relation-\nships across the entire video sequence (see Appendix A.7 for a detailed comparison). EgoSchema\nMangalam et al. (2024), on the other hand, emphasizes long-form video understanding and temporal\nreasoning but does not explicitly focus on causal-temporal narrative for video captioning.\n\nSimilarly, efforts like VCR Zellers et al. (2019), V2C Fang et al. (2020), and Motivation Vondrick\net al. (2016) integrate causality into their analysis of visual description or question-answering, relying\nheavily on commonsense reasoning for generating predictions. VCR Zellers et al. (2019) focuses on\nvisual commonsense reasoning, V2C Fang et al. (2020) aims to generate commonsense descriptions\nfor video captioning, and Motivation Vondrick et al. (2016) explores the prediction of motivations\nbehind actions in videos. However, these works primarily rely on commonsense reasoning and do not\ndelve into the causal and temporal structures underpinning video narratives, unlike our CTN benchmark dataset.\n\n**2.2 VIDEO CAPTIONING**\n\nVideo captioning techniques have evolved from LSTM-based Gao et al. (2017); Song et al. (2017);\nNadeem et al. (2023) frameworks to the latest designs using SOTA GNNs Hendria et al. (2023); Zhang\net al. (2020); Pan et al. (2020) and Transformers Wang et al. (2022); Lin et al. (2022); Yang et al.\n(2023), with a focus on enhancing the complexity of captions through the injection of multimodal data.\nDespite these advancements, current architectures often struggle to capture the intricate temporal\nsequences and causal relationships in video storytelling. To bridge this gap, video captioning can\nbenefit from cross-fertilization with ideas and strategies developed in related fields, such as action\nrecognition Sun et al. (2022); Wang et al. (2016); Kazakos et al. (2019); Xiao et al. (2020); Chen\n& Ho (2022); Gao et al. (2020); Panda et al. (2021); Sardari et al. (2023); Alfasly et al. (2022);\nPlanamente et al. (2021); Zhang et al. (2022), event localization Tian et al. (2018); Lin et al. (2019);\nDuan et al. (2021); Lin et al. (2021), and question-answering Alamri et al. (2019); Hori et al. (2019);\nSchwartz et al. (2019); Geng et al. (2021); Yun et al. (2021); Li et al. (2022a); Shah et al. (2022);\nNadeem et al. (2024). The integration of causal reasoning Liu et al. (2022); Xue et al. (2023) has\nshown promise in enhancing the ability of neural networks to discern causal relationships, leading to\nimproved performance in image captioning Liu et al. (2022) and visual question answering Xue et al.\n(2023). However, current SOTA models still struggle to effectively handle the narrative complexity in\nvideos, particularly in terms of causal and temporal progression.\n\nRecent advancements in vision-language models (VLMs) such as VideoLLaVA Lin et al. (2023) and\nShareGPT4Video Chen et al. (2024) have shown promising results in various video understanding\ntasks. However, as our experiments show (see Section 4.4), even these advanced models struggle with\ngenerating accurate causal-temporal narratives. This underscores the need for specialized architectures\nlike our proposed CEN that are explicitly designed to capture and generate causal-temporal narratives\nin videos.\n\nIn light of these challenges, our work explicitly addresses the limitations of the current approaches and provides a platform for causal-temporal narrative learning by introducing NarrativeBridge, a comprehensive framework that encompasses the CTN\ncaptions benchmark and the CEN architecture."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer 8bE2"
            },
            "comment": {
                "value": "Thank you for your follow-up questions. Let us address them:\n\n1. SEPARATOR CLARIFICATION:\n\nWe apologize for any confusion in our previous response. We need to make an important point: The original ground truth captions (MSRVTT and MSVD) actually contain no separators - the captions are simple descriptive sentences (see Figure 1). Our choice to use space separation is primarily functional and for fair comparison with baselines:\n\na) The original captions in both MSR-VTT and MSVD datasets have no comma or period separators\n\nb) The separators are not included in the tokenization\n\nc) We agree that grammatically, commas or other punctuation may be more appropriate. This needs to be explored separately from the provided baselines.\n\n2. PROPOSED TEXT ADDITIONS/MODIFICATIONS:\n\nHere are the specific additions we propose for the camera-ready version:\n\n- Section 3.2, after line 340:\n\n\"The preprocessing step maintains a simple space separation between cause and effect statements. While grammatical separators like commas or semicolons could potentially be used, we opt for space separation to maintain consistency with the raw input format of the training datasets (MSR-VTT and MSVD), which contain no separators and also, for a fair comparison with the baselines.\"\n\n- Section 3.1 (CTN CAPTIONS BENCHMARK), after line 257:\n\n\"Distribution of Caption Quality:\n1. Pre-filtering statistics: Mean EMScore \n2. Post-filtering statistics: Mean EMScore \n3. X% of initial captions retained after filtering\"\n\nIn Appendix A.4: [Insert Figure X: Histogram showing EMScore distribution with \u03b8=0.2 threshold line]\n\n- Appendix A.2 (PROMPT DESIGN PROCESS):\n\nPrompt Requirements Analysis:\n1. Temporal coherence requirement\n\n2. Causal relationship requirement\n\n3. Event specificity requirement\n\n4. Context preservation requirement\n\n5. Single coherent narrative requirement\n\n\n\n- Appendix A.5 (Human Evaluation):\nExpert Selection and Evaluation Process:\n1. Experts: 3 video understanding researchers (mean experience: 7.2 years)\n           2 video generation specialists (mean experience: 6.5 years)\n2. Sampling: Stratified random sampling of 100 videos per dataset\n3. Evaluation interface details: we provide 100 sampled videos in a directory. The CTN captions are provided in an excel sheet. These are mapped using video ids. Domain experts watch the video, read the CTN and then, give rating against each criterion in the sheet.\n\n- In the Ethics Statement, we will add:\n\n1. Privacy implications in surveillance applications are concerning but CTN captions and the CEN model don't recognize individuals.\n\n2. Guidelines for the responsible deployment of our model include adding safety features for regulated services e.g. children/minor-related content should filter graphic descriptions using automatic content classifiers (ACCs). \n\n- We will remove the redundant text in the introduction and the related work.\n\nPlease let us know of your feedback or any other questions."
            }
        },
        {
            "comment": {
                "value": "Thank you for your detailed comments. Overall it sounds good. Two things to discuss:\n\n- I'm not convinced using a space as separator is really as good. Regarding how that matches the training data, is it because the training data itself has separators like commas, periods, etc. dropped from the text during tokenization? Otherwise I find it very unusual, as separating sub-sentences with a comma will more often than not, result in a grammatically incorrect phrase, as for example: \"the dog went outside the dog saw the car\". I would be very surprised if grammatically incorrect phrases performed better than correct ones. Would you please explain in this space, how this may happen?\n\n- more generally, it would be really good if you could provide samples of the text to add and modify in the camera-ready versions of the paper, so I can evaluate the actual text (except for minor changes like typos or small word replacements). Otherwise I have to review the paper based on a text I won't be able to see until the discussion period is over.\n\nThank you"
            }
        },
        {
            "title": {
                "value": "Response to Reviewer qdoh"
            },
            "comment": {
                "value": "We sincerely thank the reviewer for their constructive feedback. We address each point:\n\n1. DATASET STATISTICS\n\nWe will add comprehensive CTN statistics to the camera-ready version:\n\nMSRVTT-CTN:\n- Videos: 10,000 (1 CTN caption per video)\n- Train/Val/Test: 6,513/497/2,990\n- Average caption length: ~19 words\n- Average cause length: ~10 words\n- Average effect length: ~9 words\n\nMSVD-CTN:\n- Videos: 1,970 (1 CTN caption per video)\n- Train/Val/Test: 1,200/100/670\n- Average caption length: ~17 words\n- Average cause length: ~9 words\n- Average effect length: ~8 words\n\n2. CROSS-DATASET PERFORMANCE\n\nThe superior performance of Fine-tune v.v. on MSVD can be explained by:\n- MSR-VTT's larger size (10,000 vs 1,970 videos) enables better initial feature learning\n- Fine-tuning helps adapt these rich features to MSVD's domain\n- Common phenomenon in transfer learning where pre-training on larger datasets improves performance on smaller ones\n\nTerminology\n\nWe will:\n- Replace \"v.v.\" with \"cross-dataset\" throughout\n- Clarify all abbreviations in the methodology section\n- Add a glossary in supplementary material if needed\n\n\n3. CAUSE-EFFECT MODELING\n\nTo directly measure cause-effect capabilities, we conduct:\n\nA) Human Evaluation (100 samples):\n- Causal accuracy: 4.8/5.0\n- Temporal coherence: 4.7/5.0 \n- Inter-rater reliability (ICC): 0.87\nThis validates our CTN's quality beyond standard metrics.\n\nB) Additional Ablation Analysis:\n\nSingle frozen CLIP-ViT baseline without cause-effect training (Stage-1):\n\nMSVD: R-L: 27.81, C: 46.23, S: 15.82\n\nMSRVTT: R-L: 25.34, C: 32.31, S: 14.27\n\nTwo frozen CLIP-ViTs baseline without cause-effect training (Stage-1):\n\nMSVD: R-L: 28.40, C: 53.84, S: 16.58\n\nMSRVTT: R-L: 26.10, C: 40.92, S: 14.55\n\nNote: Stage 2 training was performed in both cases. These results demonstrate that:\n\n- Training CLIP-ViTs with separate cause and effect is crucial for performance\n- Two specialized encoders outperform a single encoder\n- Full CEN significantly outperforms these baselines, demonstrating the importance of specialized cause-effect encoding.\n\nC) Cross-Dataset Generalization:\n\nOur strong cross-dataset results show the model learns generalizable causal relationships rather than dataset-specific patterns.\n\nD) Qualitative Analysis:\n\nAs shown in Figure 5 and Appendix A.9:\n- Complex multi-step causal chains\n- Diverse cause-effect scenarios\n- Temporal sequence understanding\n\nE) We appreciate the suggestion for additional cause-effect evaluation. Our additional experiments where we use an LLM (Llama-3.2-3B-Instruct) to measure:\n\ni) Temporal Order Accuracy for both MSRVTT-CTN and MSVD-CTN:\n- CEN: 81.2% correct temporal ordering\n- Best baseline (GIT): 52.1%\n- Measured by comparing predicted vs ground truth temporal sequences, giving a score of 0 or 1\n\nii) Causal Chain Extraction for both MSRVTT-CTN and MSVD-CTN:\n- CEN: 84.5% accurate cause-effect pairs\n- Best baseline: 48.3%\n- Measured by comparing predicted vs ground truth causal sequences, giving a score of 0 or 1\n\nTogether, these analyses demonstrate CTN and CEN's strong capabilities specifically in cause-effect modeling, beyond standard captioning metrics.\n\n4. FIGURE 1 EXAMPLE\n\nOur example demonstrates:\n- Clear cause (reckless driving) leading to effect (car damage)\n- Temporal sequence of events (play beer pong)\n- Real-world complexity in video content\nHowever, we will add another example to illustrate the cause-effect clearly.\n\n5. BENCHMARK VS DATASET\nWe use \"benchmark\" as CTN provides:\n- Standardized evaluation protocols e.g. zero-shot, for larger models especially recent vision-language models (VLMs)\n- Performance baselines\n\nBut we will clarify this terminology where needed.\n\nWe will add these changes to the camera-ready version. Given our responses, we request the reviewer to reconsider their rating. \n\nIf the reviewer has any other questions, please let us know."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer YiKX"
            },
            "comment": {
                "value": "Thank you for these valuable suggestions and the time taken to review our paper. We address each point:\n\n1. BASELINE PERFORMANCE\n\nThe reviewer raises an interesting point about ablated baselines. To clarify:\n- EJointCE, Only_Ecause, and Only_Eeffect all use our full training pipeline\n- They benefit from the same CLIP features and training strategy\n- Their strong performance validates our overall approach\n- However, the full CEN still provides significant gains over these baselines\n\nWe conduct additional experiments using frozen CLIP-ViT features:\n\n1. Single CLIP-ViT:\n- MSVD: R-L: 27.81, C: 46.23, S: 15.82\n- MSRVTT: R-L: 25.34, C: 32.31, S: 14.27\n\n2. Two CLIP-ViTs:\n\n- MSVD: R-L: 28.40, C: 53.84, S: 16.58\n- MSRVTT: R-L: 26.10, C: 40.92, S: 14.55\n\nNote: Stage 2 training was performed in both cases. These results demonstrate that:\n- Training CLIP-ViTs is crucial for performance\n- Two specialized encoders outperform a single encoder\n- Full CEN still provides significant gains over frozen features\n\n2. PRESENTATION CLARITY\n\nWe will improve Table 1 and 2 by:\n- Adding \"CTN\" to dataset names (MSVD-CTN, MSRVTT-CTN)\n- Clarifying method names in ablation study\n- Adding detailed descriptions in table captions\n\n3. DATA GENERATION APPROACH\n\nOur approach intentionally leverages multiple human annotations to create CTN captions:\n\n- MSR-VTT: 20 human annotations per video\n- MSVD: 50 human annotations per video\n\nMultiple perspectives help capture comprehensive cause-effect relationships.\n\nAlso, this methodology:\n- Can be applied to unlabeled videos - as demonstrated in Appendix A.8\n- Eliminates need for expert (familiar to causal-temporal narrative) annotations\n- Provides consistent cause-effect structuring\n- Enables scalable dataset creation\n\n4. METRIC SELECTION\n\nWe use multiple complementary metrics:\n- ROUGE-L: Measures fluency and grammatical structure\n- CIDEr: Captures semantic similarity\n- SPICE: Evaluates detailed semantic content\n- Methods with lower metric scores (SPICE, CIDEr, ROUGE-L) consistently produce lower quality outputs, validating our metrics' reliability (Figure 5)\n\nThis combination provides comprehensive evaluation across different aspects of caption quality.\n\n\nWe will add the required changes to the camera-ready version. Given our responses, we request the reviewer to reconsider their rating. If the reviewer has any other questions, please let us know."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer 8bE2"
            },
            "comment": {
                "value": "We are grateful to the reviewer for their thorough and constructive feedback. We address each point:\n\n1. DISTRIBUTION OF CAPTION QUALITIES\n\nWe will add:\n- Histogram of EMScore distributions\n- Vertical line showing \u03b8=0.2 threshold\n- Analysis showing how filtering improves caption quality (in addition to appendix A.4)\n- Statistics before/after filtering\n\n2. PROMPT ABLATION\n\nStandard prompts are employed in these LLM based tasks, but we provide 4 ablations in A.2 for more clarity. We will add the following to address this concern:\n- Ablation for each prompt requirement\n- Comparison of outputs with/without each requirement\n- Examples illustrating improvement from each requirement\n\n3. FRAME LEVEL INFORMATION IN CAPTION GENERATION\n\nThank you for this suggestion. We need to clarify an important point:\n\n- We do NOT generate captions from frames. Our input is:\n1. MSR-VTT: 20 human annotations per video\n2. MSVD: 50 human annotations per video\n- As shown in Figure 1, these human annotations lack causal-temporal narrative structure. Our method transforms these diverse human perspectives into structured cause-effect narratives.\n- For unlabeled videos (Appendix A.8), we agree that incorporating visual features probably in a VLM may improve performance. This is an exciting direction for future work.\n\n\n4. CAUSE-EFFECT SEPARATION\n\nOur choice of space separation was based on:\n- Clean tokenization for model processing\n- Preserves explicit cause-effect structure - based on Explicit Marking Theory (Knott, A., & Sanders, T. (1998)) and Rhetorical Structure Theory (Mann, W. C., & Thompson, S. A. (1988))\n- Maintains temporal ordering - as it is the causal-temporal narrative\n- Enables direct mapping for downstream tasks\n- Consistency with training data format\n\n5. HUMAN EVALUATION DETAILS\n\nWe will add comprehensive details to the camera-ready version:\n- Expert selection criteria: 3 video understanding and 2 video generation independent domain experts with at least >5 years\u2019 experience\n- Sampling: stratified random sampling across datasets\n- Evaluation interface details: we provide 100 sampled videos in a directory. The CTN captions are provided in an excel sheet. These are mapped using video ids. Domain experts watch the video, read the CTN and then, give rating against each criterion in the sheet.\n\n6. WRITING CLARITY\n\nWe appreciate the feedback on verbosity. We will remove redundant explanations and maintain technical precision while improving conciseness. We will remove the line about paradigm shift although our intention is solely to highlight the importance of CTN captions in comparison to existing simple captions.\n\n7. ETHICAL CONSIDERATIONS\n\nWe will expand the ethical discussion in the camera-ready:\n- Privacy implications in surveillance applications - CTN captions and CEN model don't recognize individuals\n- Guidelines for responsible deployment - adding safety features for regulated services\n\nRegarding specific QUESTIONS:\n\n- We will replace the fatality video example.\n- CTN/CEN similarity indicates improved learning (visualization in Figure 4 is helpful), not overfitting (supported by cross-dataset results in ablations)\n- For videos with additional events, we focus on generating the temporal sequence of events (handled in the prompt) as shown in Figure 1 \u2013 guys playing beer pong (an additional event). Events without clear causal structure are dealt with similarly and the outputs remain coherent to the causal-temporal narrative. Here are the outputs for the examples you have shared: 1. a person is singing - CTN: {\"Cause\": \"a person began to sing\", \"Effect\": \"produced musical song with their voice\"}  2. a person is sitting in a chair - CTN: {\"Cause\": \"a person remained seated without any apparent activity or movement\", \"Effect\": \"maintained a stationary position in the chair\"}. We can explore the confidence score using an LLM. \n- Figure 6 captions: prompt is unable to generate one coherent cause-effect caption instead there are many ranging from 20-50. This fails in the preprocessing part. Even the EMScore based evaluation for each caption gives a score less than 0.2 (threshold).\n- Effect encoder receiving cause input: we have run this experiment by concatenating the cause tokens before the effect tokens and then using a projection layer (initialised randomly) - decreased performance by 3.21% and 5.43% on CIDEr for MSRVTT and MSVD respectively. At this point, we can only say that it is because of the added complexity in the form of the projection layer. We will add this to the camera-ready version.\n\nWe will add these changes to the camera-ready version as promised. Given our detailed responses, we ask the reviewer to reconsider their rating, please. If the reviewer has any other concerns or questions, please let us know."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer 8P6x"
            },
            "comment": {
                "value": "We sincerely thank the reviewer for their detailed feedback. We address each point below:\n\n1. HUMAN EVALUATION SAMPLE SIZE AND METHODOLOGY\n\nThe reviewer raises a point about sample size. However, our sampling methodology follows rigorous statistical practices:\n\na) Sample Selection:\n- MSRVTT (10,000 videos): 50 samples selected\n- MSVD (1,970 videos): 50 samples selected\n- Proportional representation from train/val/test splits maintained\n- With 100 total samples from 11,970 videos, we achieve a 95% confidence level (standard Z-score = 1.96) with \u00b19.6% margin of error\n\nb) Statistical Validity:\n- High inter-rater reliability (ICC: 0.87, 95% CI: 0.83-0.91)\n- Stratified sampling ensuring coverage across video types\n- Sample size aligns with standard practices in human evaluation studies [1,2]\n\n2. CAPTION CONCATENATION APPROACH\n\nWe respectfully disagree that our concatenation approach diminishes caption quality:\n\na) Intentional Design Choice:\n- Preserves explicit cause-effect structure - based on Explicit Marking Theory [3] and  Rhetorical Structure Theory [4]\n- Maintains temporal ordering - as it is the causal-temporal narrative\n- Enables direct mapping for downstream tasks \n\nb) Quality Validation:\n- In addition to SPICE and CIDEr, we use ROUGE-L scores (measuring fluency): 31.46 (MSVD) and 27.90 (MSR-VTT)\n- Qualitative examples (Figure 5, Appendix A.9) demonstrate coherent outputs\n- Methods with lower metric scores (SPICE, CIDEr, ROUGE-L) consistently produce lower quality outputs, validating our metrics' reliability\n\n3. ENCODER STRUCTURE CLARIFICATION\n\nWe apologize if this wasn't clear enough in the paper:\n\na) Feature Processing:\n- Features are mean-pooled as explicitly mentioned in Section 3.2.1 (line 312)\n- h_cause and h_effect are single representations\n- Dimension: 512 (matching CLIP's feature dimension)\n\nb) Implementation Details:\n- Mean pooling applied after transformer encoding\n- Concatenation along feature dimension\n- Final dimension: 1024 (512+512)\n\nGiven our detailed responses and proposed improvements, we respectfully ask the reviewer to reconsider their rating. \n\nWould the reviewer find it helpful if we included any additional analyses or clarifications in the camera-ready version?\n\n[1] Chen, David, and William B. Dolan. \"Collecting highly parallel data for paraphrase evaluation.\" Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies. 2011.\n\n[2] Xu, Jun, et al. \"Msr-vtt: A large video description dataset for bridging video and language.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n\n[3] Knott, Alistair, and Ted Sanders. \"The classification of coherence relations and their linguistic markers: An exploration of two languages.\" Journal of pragmatics 30.2 (1998): 135-175.\n\n[4] Mann, William C., and Sandra A. Thompson. \"Rhetorical structure theory: Toward a functional theory of text organization.\" Text-interdisciplinary Journal for the Study of Discourse 8.3 (1988): 243-281."
            }
        },
        {
            "summary": {
                "value": "In this paper, a video captioning dataset named Causal-Temporal Narrative (CTN) is proposed. Compared to previous video captioning datasets which lack in causal-temporal  information which connects a cause and an effect, CTN, built with LLMs, includes caption that directly connects cause and effect events. Also, a new architecture named Cause-Effect Network (CEN) is proposed, which consists of two independent encoders which capture cause and effect events, respectively."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The motivation of the work is clear, while the problem of improving the causal-and-effect understanding capability of a vision-language model is an important research direction.\n2. Experimental results show that both the proposed methods, CTN and CEN contribute to better performance on MSVD and MSRVTT benchmarks."
            },
            "weaknesses": {
                "value": "1. Including detailed statistics about the constructed CTN dataset (e.g., # of samples, average caption length, # of captions per sample, or more..) may enable readers to easily understand the dataset.\n2. Questions about Table 2.\n    1. Results show that Fine-tune v.v. shows better performance than CEN on the MSVD benchmark under the cross-dataset evaluation setting, compared to CEN trained with both datasets. The result of cross-dataset evaluation performing better than whole dataset training is somewhat counter-intuitive. Can you provide an explanation for these results?\n    2. What does the abbreviation of v.v. stands for? It would be better to mention the full name.\n3. Is there additional experimental results or analyses that could show CTN and CEN contributes to better cause-and-effect modeling besides results on MSVD and MSRVTT? Since the quantitative results of Tables 1 and 2 are not directly aimed at measuring cause-and-effect capability, some additional experiments may be required.\n4. These are some **minor** issues about the presentation of the work. These are **not** critical weaknesses, and somewhat personal opinions.\n    1. Personally, I think the example in Figure 1 is not very intuitive to explain the importance of causal-temporal narrative. Cause: \u201ca car drove recklessly\u201d and Effect: \u201cthe car was severely damaged\u201d seems to be a valid cause-and-effect event, but the following event of \u201ca group of guys started playing beer pong\u201d seems like it\u2019s not a direct effect of \u2018cause'. This is a personal opinion, but you could consider finding a better example to demonstrate the idea.\n    2. I think \u2018dataset\u2019 instead of \u2018benchmark\u2019 might be a better word for CTN, as it is only used for training but not for evaluation."
            },
            "questions": {
                "value": "Please refer to weaknesses part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper highlights that current video captioning benchmarks lack causal-temporal narratives and proposes a new benchmark to address this gap. This benchmark is generated by prompting a large language model (LLM) with a series of captions extracted from a video. To ensure relevance, EMScore is adapted to estimate the similarity between the generated captions and the video, discarding samples with low scores. The paper further introduces a two-stage framework to generate causal-temporal narrative (CTN) captions. In the first stage, two contrastive losses are applied to train Causal and Effect Video Encoders with their respective captions. In the second stage, the encoded causal and effect features are concatenated to decode a cohesive caption."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper proposes a benchmark focused on causal-temporal narratives, adapting an evaluation score for automated assessment.\n\nIt introduces a novel captioning model that encodes causal and effect information separately, then fuses them to decode the final caption.\n\nExperiments demonstrate that the proposed CEN framework outperforms comparative methods in performance."
            },
            "weaknesses": {
                "value": "The paper includes a human evaluation of the generated CTN captions; however, the assessment is less convincing due to the limited sample size. Despite the large dataset size\u2014approximately 300,000 samples in MSRVTT and MSVD\u2014only 100 samples were selected for human evaluation, reducing the reliability of the results.\n\nFor supervision during caption decoding, the paper simply concatenates the causal and effect text, which disrupts the original sentence grammar and lowers the quality of the synthetic supervision. The rationale behind this approach is unclear. An alternative method could involve inputting the causal and effect text into an LLM to fuse them into a more natural and coherent sentence. With ground truth captions generated by simple concatenation, evaluation metrics such as SPICE and CIDEr may not accurately reflect the quality of the generated captions. This approach could lead to discrepancies between metric scores and the true readability and coherence of the captions.\n\nThe structure of the feature encoder in stage 2 is not clearly depicted. Typically, encoded features from transformer encoders consist of a series of representations. It is unclear whether h_cause and h_effect are lists of representations or single representations. If h_cause and h_effect are single representations, it is not specified how they are generated\u2014whether by selecting a specific token, or by applying mean pooling or max pooling. Furthermore, the specific dimension along which these features are concatenated is not detailed."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tackles the problem of captioning videos taking into account causal-temporal narratives. It does so by presenting two main contributions: a dataset and benchmark (CTN) with causes and effects per video, and a network architecture (CEN) for extracting such narratives directly from videos.\n\nWhen applying the CTN benchmark to CEN, the network gets better metrics when compared to similar methods, potentially establishing it as SOTA in the domain."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Significance:\n\n* The problem discussed is important and relevant to much of the field of video captioning and video generation. It's true that typical captioning methods don't typically expose causal information, which makes generated captions much less useful than they could be.\n\nOriginality:\n\n* The work appears original though the novel aspects of it are a bit of straightforward applications of existing methods.\n\nQuality:\n\n* The work produces good results. While the paper discusses both results on existing benchmarks, and in CTN, the former being more interesting because they've been established independently.\n* I appreciate that both automatic benchmarks and human evaluations were used to evaluate this work.\n* The proposed CEN network architecture is interesting and original.\n\nClarity:\n\n* The work is mostly clear, except for some aspects that are discussed in the weaknesses section."
            },
            "weaknesses": {
                "value": "* The automatic evaluation process is found to filter out most bad cause and effect generations. More complete analysis on the distribution of the qualities of the generated captions is missing. I can imagine a histogram that displays the distribution of qualities with a vertical line showing the threshold that's been picked. Also the paper would benefit from analysis on whether the automatic evaluation filtering improves on the method, or is a patch for more fundamental issues in the prior step.\n\n* The contribution of CTN depends strongly on the prompt used to zero shot cause and effect generation. Given that this prompt is so central to the paper, the paper would benefit from more complete ablations regarding different aspects of the prompt. Especially important, the list of requirements that are presented in the prompt are justified in the paper but no corresponding ablation per requirement is done to validate the explanation. (the ablation donefor 4 prompts of varying degrees of quality appears insufficient for this purpose)\n\n* When building CTN, the captions are 1) generated from frames, and then 2) a model combines multiple captions into cause and effect captions without looking at frame information again until 3) the automatic evaluation step. Has the team considered using a model that can read text + image in step 2 as well? \n\n  Concluding cause and effect just from the individual frame captions is a lot more error prone than it could be if image information was also used. I think this possibility could be mentioned / analyzed as a potential alternative.\n\n* Why are causes and effects only separated by a space in CTN? Have other choices been ablated? In particular, I can imagine using a \"comma\" or an \"and\" between both to work slightly better.\n\n    * Has the team considered using an LLM to combine both captions together, instead of using concatenation?\n\n* For A5.1: more discussion on the human evaluation process would be very beneficial. How were the experts chosen? How is information presented to them? How were the 100 videos chosen?\n\n  For example it would be ideal to have an independent expert set; and it would be ideal if they were presented with the captions without labels that identify the models, and in such a way that each the models are independently ordered for each video (so annotators cannot be biased to always vote for the first model, for example)\n\n* Clarity: The paper, especially in the first few pages, is a bit too repetitive or verbose. It may benefit from some proofreading for succinctness. For example, the following two paragraphs are very close to each other, and both express a similar idea:\n\n```\nIt means that our CTN benchmark differs significantly by focusing on generating comprehensive causal-temporal narratives, capturing broader temporal relationships within a single caption, and providing a more holistic view of video content\n```\n\n  and\n\n```\nWe introduce a new benchmark CTN specifically designed to capture and evaluate causal-temporal narrative in video captioning. Our approach goes beyond existing benchmarks by explicitly modeling causal-temporal narrative in a single, coherent caption, enabling a more comprehensive understanding of video content.\n```\n\n  In other cases ideas are repeated in the same paragraph:\n\n```\nHowever, as our experiments show (see Section 4.4), even these advanced models struggle with generating accurate causal-temporal narratives. To address this limitation, there is a need to develop new video captioning models that can overcome the shortcomings of current frameworks. This also underscores the need for specialized architectures like our proposed CEN that are explicitly designed to capture and generate causal-temporal narratives in videos\n```\n\n  one could summarize this text as:\n\n```\nHowever, as our experiments show (see Section 4.4), even these advanced models struggle with generating accurate causal-temporal narratives. This also underscores the need for specialized architectures like our proposed CEN that are explicitly designed to capture and generate such narratives in videos\n```\n\n  Such verbosity makes the paper hard to read, despite the main ideas being explained very clearly.\n\n* The paper would benefit from a more complete discussion on possible downsides of this approach. For example for CTN how it compares to using frame data in step 2 as discussed above, or what downsides could arise from the concrete proposed architecture.\n\n* The paper mentions surveillance; maybe analyze this in the context of ethic concerns?\n\n* `We acknowledge potential ethical concerns (e.g., privacy in surveillance, risk of misleading content) but are committed to responsible development`, could this be made more concrete?\n\n* The paper mentions `lays the foundation for a paradigm shift`, which appears potentially grandiose in my opinion. Better to let the community arrive to this conclusion, than write it in the text.\n\nI'll be glad to revise my score depending on the response to this review."
            },
            "questions": {
                "value": "* The fatality video may be too graphic for some readers. I suggest switching it to a video on another topic.\n\n* In figure 5, CTN captions appear too similar to CEN. Is this a symptom of overfitting?\n\n* What does the system do when a video with no apparent cause and effect is used as input? For example, a person is singing but there's no audience reaction. Or a person is just sitting in a chair. The paper mentions \"our method assumes a certain level of causal and temporal coherence within the video content\"; maybe it would benefit from providing a probability value on the confidence for the existence of cause and effect in the input video?\n\n* `As expected, the resulting captions (Figure 6) are of low quality`, how is this measured?\n\n* Has the team considered the encoder of effect to also receive the encoded cause as input? How would that compare with the current approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on incorporating causal-temporal narrative into video captioning.  The authors create a new dataset called Causal-Temporal Narrative (CTN) captions, using a LLM to generate captions that explicitly describe cause-effect relationships and maintain temporal consistency within videos based on multiple annotated ground-truths for the same video. The quality of these CTN captions is assessed both automatically (using EMScore) and through human evaluation.\n\nFurthermore, the authors propose a novel model architecture called the Cause-Effect Network (CEN), specifically designed to learn from the CTN captions. CEN decouples the cause and effect captions by finetuning one visual encoder for each of them. The representations of both encoders are then used during the LLM finetuning stage.\n\nExperiments on MSVD and MSR-VTT datasets demonstrate that CEN significantly outperforms state-of-the-art video captioning methods for generating CTN captions. Ablation studies further validate the design choices of CEN."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- While most of the causal video understanding focuses on video question answering, this paper proposes to bring causal understanding into video captioning. \n- The data generation pipeline looks sound.\n- The proposed approach achieves strong results on the proposed benchmark."
            },
            "weaknesses": {
                "value": "- The weakest ablated baseline already performs well above state-of-the-art approaches. It would be great to understand what the differences are. For instance, how does a baseline with the CLIP ViT features used directly perform?\n- The presentation can be improved, for instance it is not clear that the evaluation is on the CTN captions in Table 1, and the ablation study includes method names which are hard to interpret when having a first look at the table.\n- The data generation approach largely relies on the multiple GT captions present for each video in MSR-VTT and MSVD. \n- Captioning metrics which do not move much like Rouge-L are hard to interpret."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}