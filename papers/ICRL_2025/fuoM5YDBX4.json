{
    "id": "fuoM5YDBX4",
    "title": "Transformer Learns Optimal Variable Selection in Group-Sparse Classification",
    "abstract": "Transformers have demonstrated remarkable success across various applications. However, the success of transformers have not been understood in theory. In this work, we give a case study of how transformers can be trained to learn a classic statistical model with \"group sparsity\", where the input variables form multiple groups, and the label only depends on the variables from one of the groups. We theoretically demonstrate that, a one-layer transformer trained by gradient descent can correctly leverage the attention mechanism to select variables, disregarding irrelevant ones and focusing on those beneficial for classification. We also demonstrate that a well-pretrained one-layer transformer can be adapted to new downstream tasks to achieve good prediction accuracy with a limited number of samples. Our study sheds light on how transformers effectively learn structured data.",
    "keywords": [
        "transformer",
        "self-attention",
        "variable selection",
        "group-sparse classification"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=fuoM5YDBX4",
    "pdf_link": "https://openreview.net/pdf?id=fuoM5YDBX4",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates one-layer transformers trained on specific dataset, where the input variables are generated from multiple groups, while the true label of this input is determined by variables from a single group. Based on these simplifications and assumptions, it theoretically demonstrates that the one-layer transformers can almost attend to the variables from the label-relevant group. Moreover, it provides a tight lower and upper bound for the population cross-entropy loss of a one-layer transformer trained by gradient descent. It also shows that the well pre-trained one-layer transformers can be efficiently transferred to a downstream task sharing a similar \u201cgroup-sparse\u201d structure and further provides an improved generalization error bound for one-layer transformers fine-tuned by SGD, which surpasses that of linear logistic regression applied to vectorized features. The numerical experiment observations support the theoretical findings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well written and easy to follow. \n\nThe theoretical analysis is thorough and the results align with the group sparsity assumption, although I did not check all the math and proof in detail.\n\nIn a sense, it provides new insights into sparsity analysis on the workings of attention based models."
            },
            "weaknesses": {
                "value": "(1) too much data assumption and model simplification. According to Definition 3.1, each patch x_j is i.i.d from Gaussian, and its label is determined from a given v (which can be learned from samples later). These data assumptions make group attention trivial.\n\n(2) And the model is one-layer transformers, which may cannot capture attention-based models to a sufficiently satisfactory extent. With these simplifications, the contribution of this paper is limited, especially considering the pre-work from Jelassi et al. (2022)."
            },
            "questions": {
                "value": "the input x_j is concatenated with position encoding, does Theorem 3.2 still hold if using addition instead of concatenation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper analyzes the variable selection of the Transformer theoretically."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The author presented a novel theoretical analysis on how the transformer can learn to select variables for the group sparse classification, which is interesting and novel."
            },
            "weaknesses": {
                "value": "1. Given your previous definition of the notations, pls define $\\Theta$ in advance \uff08otherwise people would get confused about this common notation.What is the 'variable' in the paper? Are they some features or some attributes of the data that define the label? What is the $v_1$ in the Thereom 3.2-2\uff1f The weight corresponding to the label 1 or the value vector (output from value matrix in Transformer)\uff1f\\\\\n2. The proof stretch should be placed in the Supplementary instead of the main paper. \\\\\n3. The experiments are conducted on synthetic data, however, in the higher dimensional images (e.g. 3*224*224), things would change. I have concerns on these too simple experiments. \\\\\n4. The Lemma 5.1, I do not see why the $W^{(T^*)}_{1,2}$ holds and $W$ is diagonal. In NLP, it seems that the feature is correlated to its position and would correspond to the output, is that a too strong assumption?\n5. Can you show a more detailed demonstration on why the inequality of $\\alpha^(T^*)$ holds (or is just a definition?) and why Lemma5.2 can be incorporated into Lemma 5.3? Also, I do not think Lemma 5.2. holds as the underlying assumption is the Transformer is naturally with low error. Lemma 5.2 is more like a strong assumption and definition to me. \\\\\n6. How you define the group sparse? What is this different from the standard classification?\n\n\nI would like to raise my scores if more details are provided."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the theoretical understanding of transformers, which have shown success in various applications but lack a clear theoretical foundation. The study focuses on how transformers can learn a statistical model with group sparsity, where only a subset of input variables (groups) influences the label. The research shows that a one-layer transformer can use attention mechanisms to select relevant variables and ignore irrelevant ones for classification. Additionally, it demonstrates that a well-pretrained one-layer transformer can adapt to new tasks with limited samples and achieve good accuracy. This study provides insights into how transformers learn structured data effectively."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper investigates the theoretical understanding of transformers, an important and interesting problem that may greatly benefit the deep learning community.\n2. This paper is well-written and has a good motivation.\n3. The conclusion that a one-layer transformer can use attention mechanisms to select relevant variables and ignore irrelevant ones for classification makes sense to me."
            },
            "weaknesses": {
                "value": "1. Though the problem is interesting, the conclusion, a one-layer transformer can use attention mechanisms to select relevant variables and ignore irrelevant ones for classification, seems to be well-known in the deep learning community and has already been verified by the original paper (Attention Is All You Need).\n2. The theoretical understanding of transformers given by this paper mainly focuses on the well-pretrained one-layer transformer. However, it will be much more interesting to study the multi-layer transformer since the scaling law shows that the loss of transformer scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. \n3. The study focuses on how transformers can learn a statistical model with group sparsity, where only a subset of input variables (groups) influences the label. However, the concept of group sparse is not properly defined, making me confused to fully understand it. Further, what is a similar \u201cgroup-sparse\u201d structure as mentioned in the Introduction section? What kind of \u201cgroup-sparse\u201d inputs are similar?"
            },
            "questions": {
                "value": "Please refer to the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}