{
    "id": "F1OdjlfCLS",
    "title": "Overfitting: An Unexpected Asset in AI\u2010Generated Image Detection",
    "abstract": "AI-generated images have become highly realistic, raising concerns about potential misuse for malicious purposes. In this work, we propose a novel approach, DetGO, to detect generated images by overfitting the distribution of natural images. Our critical insight is that a model overfitting to one distribution (natural images) will fail to generalize to another (AI\u2010generated images). Inspired by the sharpness\u2010aware minimization, where the objective function is designed in a $\\min$-$\\max$ scheme to find flattening minima for better generalization, DetGO instead seeks to overfit the natural image distribution in a $\\max$-$\\min$ manner. This requires finding a solution with a minimal loss near the current solution and then maximizing the loss at this solution, leading to sharp minima. To address the divergence issue caused by the outer maximization, we introduce an anchor model that fits the natural image distribution. In particular, we learn an overfitting model that produces the same outputs as the anchor model while exhibiting abrupt loss behavior for small perturbations. Consequently, we can effectively determine whether an input image is AI-generated by calculating the output differences between these two models. Extensive experiments across multiple benchmarks demonstrate the effectiveness of our proposed method.",
    "keywords": [
        "Overfitting",
        "AI-generated image detection",
        "Generative models"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=F1OdjlfCLS",
    "pdf_link": "https://openreview.net/pdf?id=F1OdjlfCLS",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes DetGO, a novel method for detecting AI-generated images. Instead of trying to identify artifacts in generated images, DetGO overfits a model to the distribution of real images. The core idea is that a model excessively tuned to real images will generalize poorly to AI-generated ones. This approach is inspired by Sharpness-Aware Minimization (SAM), but inverts the logic. While SAM seeks flat minima for better generalization, DetGO seeks sharp minima to prevent generalization. It uses a dual-model framework: an anchor model (a pre-trained DINOv2 model) encodes real images, and an overfitting model is trained to match the anchor model's output on real images while exhibiting high sensitivity to small perturbations. The difference in output between these two models then serves as the basis for distinguishing real from generated images."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. **Novel Approach:** The paper takes an innovative perspective by treating overfitting as an advantage rather than a problem to solve.\nTraining Efficiency: Only requires natural images for training, eliminating the need for AI-generated images in the training process.\n2. **Training Efficiency:** Only requires natural images for training, eliminating the need for AI-generated images in the training process."
            },
            "weaknesses": {
                "value": "See Questions."
            },
            "questions": {
                "value": "1. **Self-supervised Backbone:** what is the intuition for using a DinoV2 as a backbone? Why not use some other self-supervised model?\n2. **Implementation Details:** How is $\\epsilon$ sampled from the Gaussian distribution? What are the parameters of this distribution? How is $\\rho$ chosen?\nThe justification for using two convolutional layers in $g_\u03b8(x)$ is weak. More explanation is needed. \n3. **Novelty of design:** The design change is essentially learning the representation similarity between the original and perturbed images on the DinoV2 representation space. However, the Backbone (DinoV2) and the noise distribution (Gaussian) used are too similar to the existing work RIGID **[R]**. However, the method does not use it as a baseline.\n4. The performance of Ojha in Table 1 and Table 3 is lower than its reported performance, please give the implementation details of the baseline methods.\n\n**[R]** rigid: a training-free and model-agnostic framework for robust ai-generated image detection"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper formulates the problem of AI-generated image detection as an OOD detection process, and they propose to fit a model solely on real images, without assumptions on and access to any AI-generated data. Specifically, they propose to use a dual-model framework, where the detection is based on the output differences between the overfitted model and a normally-trained, anchor model."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The idea of relying on overfitting is new in the context of AI-generated image detection. \n- The experiments test a diverse spectrum of generated images, including GAN and diffusion images as well as Sora video frames.\n- The ablation studies cover almost all important hyperparameters."
            },
            "weaknesses": {
                "value": "- The largest weakness is that conventional OOD detection methods are not discussed and compared. As the reviewer understands, the authors have formulated the problem of AI-generated image detection as a typical OOD detection process, where the outlier is also normally not exposed to the detector. Therefore, the authors should acknowledge this similarity and test conventional OOD detection methods before proposing a new method. For example, a well-trained model itself can be used to detect OOD samples, here the AI-generated images, based on their output confidence. This also corresponds to the second limitation described in Section 5.\n\n- The underlying assumption that the proposed method can work is that the $\\epsilon$ can represent the nature of AI-generated images. However, the authors just simply adopt the Gaussian noise, without any discussion of possible alternatives and impact. More generally, the $\\epsilon$ can be treated as a special type of universal fake image, and therefore, it is important to discuss its properties on the final detection performance. In particular, there may exist one perfect type of $\\epsilon$ that can generalize to most kinds of unseen fake images.\n\n\n- More generally, this paper does not discuss the key question: how to trade off the generalization power to unseen real images and unseen AI-generated images. In particular, the authors have already stated that \u201cOur experimental results demonstrate that variations in the intensity of this noise significantly influence the detection performance of the model, as shown in Table 9. When the perturbation is minimal, the model tends to overfit the training set rather than learning meaningful representations of the real images. Conversely, when the perturbation is excessively large, the model only learns to distinguish between real images and pure noise, which also leads to a deterioration in detection performance. \u201d \n\n- The authors have not mentioned how to select the threshold, as depicted in Figure 1.\n\n- In Section 4.1, the authors claimed to evaluate the accuracy (ACC). However, the reviewer cannot find any ACC results. AUC and AP are both threshold-independent evaluation metrics, according to previous works [1, 2]. A high value of AUC or AP does not necessarily imply a high value of accuracy. Additionally, considering the previous issue, the threshold of loss is likely to have a significant impact on the accuracy.\n[1] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors that generalize across generative models. In CVPR, pp. 24480\u201324489. IEEE, 2023.\n[2] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A. Efros. Cnn-generated images are surprisingly easy to spot... for now. In CVPR, pp. 8692\u20138701. Computer Vision Foundation / IEEE, 2020.\n\n- The authors do not describe how they use the generative models to generate the test data and how much data are used.\n\n- As mentioned in Section 5, access to high-quality training data may have a large impact on performance. The authors should at least explore such impact by showing the results as a function of the number of training images."
            },
            "questions": {
                "value": "See the above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose a SAM-based approach to detect generated images by overfitting the distribution of natural images. Additionally, an anchor model are utilized to solve the divergence issue. The authors conduct extensive experiments across multiple AI-image benchmarks and the results demonstrate the effectiveness of the approach."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to follow. The structure is clear.\n2. The motivation of this paper is similar to Out Of Distribute (OOD) detection, where the model is only trained and fitted on real images. However, detecting generated images by overfitting is enlightening.\n3. The methodology is logical, and the motivation is intuitive and profound. The design of sharpening the first derivative of the loss is intriguing.\n4. Extensive and effective experiments proving the effectiveness of the proposed method. Ablation experiments validate the hypotheses."
            },
            "weaknesses": {
                "value": "1. Some technical or motivational clarification is needed. \n2. Some ablation studies are recommended.\nPlease refer to Questions."
            },
            "questions": {
                "value": "1.  In line 196, the authors believe that \u03b5 follows a Gaussian distribution, I wonder whether there is a reasonable explanation. \n2. Meanwhile, I believe that when \u03b5 is designed to be sampled from a Gaussian distribution, the proposed approach will suffer from performance degradation when Gaussian blur is applied to images, as the added noise \u03b5\u2019 and the designed \u03b5 belong to the same distribution, resulting in an increase in L(x+\u03b5\u2019). I hope the authors can clarify some of the points.\n3. The AUROC and AP metrics are insensitive to classification thresholds. Based on this, they may not fully reflect the generalization ability of the model. I hope the authors can provide more experimental results, such as the AUROC and AP on real samples of LSUN-Bedroom (Table 3) and generated samples from ImageNet settings (Table 1), or the Accuracy with fixed thresholds on different datasets.\n4. Why is g(x) designed as multiple trainable convolutional layers? Would other architectures, such as U-Net, yield better or worse results?\n5. Will the selection of the training dataset affect the detection performance?\n6. Can you evaluate the complexity of the proposed DetGO?\n\nBased on the above Strengths and Weaknesses, I will give a Borderline Score. I will revise my score if the author's rebuttal can provide reasonable explanations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}