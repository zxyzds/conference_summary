{
    "id": "aCPFCDL9QY",
    "title": "Self-Updatable Large Language Models with Parameter Integration",
    "abstract": "Despite significant advancements in large language models (LLMs), the rapid and frequent integration of small-scale experiences, such as interactions with sur- rounding objects, remains a substantial challenge. Two critical factors in assimilating these experiences are (1) **Efficacy**: the ability to accurately remember recent events; (2) **Retention**: the capacity to recall long-past experiences. Current methods either embed experiences within model parameters using continual learning, model editing, or knowledge distillation techniques, which often struggle with rapid updates and complex interactions, or rely on external storage to achieve long-term retention, thereby increasing storage requirements. In this paper, we propose **SELF-PARAM** (Self-Updatable Large Language Models with Parameter Integration). SELF-PARAM requires no extra parameters while ensuring near-optimal efficacy and long-term retention. Our method employs a training objective that minimizes the Kullback-Leibler (KL) divergence between the predictions of an original model (with access to contextual information) and a target model (without such access). By generating diverse question-answer pairs related to the knowledge and minimizing the KL divergence across this dataset, we update the target model to internalize the knowledge seamlessly within its parameters. Evaluations on question-answering and conversational recommendation tasks demonstrate that SELF-PARAM significantly outperforms existing methods, even when accounting for non-zero storage requirements. This advancement paves the way for more efficient and scalable integration of experiences in large language models by embedding knowledge directly into model parameters.",
    "keywords": [
        "Large Language Models",
        "Knowledge Injection",
        "Self-Updatable LLMs"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "This paper proposes a simple method to absorb contexts of any form into model parameters.",
    "creation_date": "2024-09-14",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=aCPFCDL9QY",
    "pdf_link": "https://openreview.net/pdf?id=aCPFCDL9QY",
    "comments": [
        {
            "title": {
                "value": "Rebuttal to Reviewer s4VY"
            },
            "comment": {
                "value": "We thank the reviewer for the comments, where we address as below: \n\n**[W1] Novelty of the Approach**:  We would like to emphasize that SELF-PARAM is not merely a case of \"collecting training data from highly capable models\" or \"using KL divergence,\" as this characterization overlooks the unique structure of our approach. \n- **Distinct from Data Collection and Fine-Tuning:** Although `gpt-4o-mini` is more advanced, the question-answer pairs could also be constructed using models such as `mistral-7B-instruct`. We have conducted a quick experiment using `mistral-7B` as the base model (i.e., Column Mistral-7B in Table 2) and generate 50 question-answering pairs using `mistral-7B-instruct-v0.3` and train `Mistral-7B` using these question-answering pairs with our proposed method SELF-PARAM. The experimental results are shown below: \n\n    |  | Injecting 100 Contexts into `mistral-7B` | \n    | ---- | ---- | \n    | `gpt-4o-mini` | 0.4521 |\n    | `mistral-7B-instruct-v0.3` | 0.4502 |\n\n    From the table we can see that changing the model from `gpt-4o-mini` to `mistral-7B-instruct` does not affect the performance, showing that this process does not rely on the inherent superiority of the external model but rather that the Target Sentence Set needs to be diverse. We will add the experiments on 500 contexts and `Llama3-8B-Instruct` for `Llama3-8B` in our paper as the ablation study. Moreover, The role of the question-answer pairs is more akin to preprocessing the context as all the question-answering pairs are from the context and they essentially do not have anything new other than the context itself, making it different from collecting an entirely new dataset. \n\n\n- **Baselines Demonstrating the Need for Our Approach:** Simply using another model to construct question-answering pairs from the context and fine-tuning the model does not yield good performance, as demonstrated by the baselines `FT (S), Q` in Tables 1 and 2, `FT (Q)` in Table 3. \n\n    In these baselines:\n    - **`FT (S), Q` and `FT (Q)`**: Here, the model is fine-tuned on question-answer pairs generated by a more advanced model. However, without our KL-divergence-based objective, this approach struggles to match the efficacy of SELF-PARAM. The significant performance gap shows that the mere inclusion of data from a larger model does not inherently improve long-term retention or context integration.\n\n    Together, these baselines demonstrate that collecting data from a larger model alone, even with fine-tuning, is insufficient for the nuanced task of context injection. SELF-PARAM leverages KL divergence in a way that aligns model outputs across contexts, directly embedding context within model parameters.\n    \n- **Innovative Use of KL Divergence:** Although KL divergence has been introduced long ago, prior works such as parameter-based injection of prompts or factual statements [1, 2] show that this technique can still underpin novel approaches to knowledge embedding. SELF-PARAM extends this idea to address the challenge of context injection where the context can be of more complicated forms, leveraging KL divergence in a way that facilitates both long-term retention and efficient context integration without additional parameters. \n\n[1] Prompt Injection: Parameterization of Fixed Inputs  \n[2] Propagating Knowledge Updates to LMs Through Distillation  \n\n**[W2] Clarification of Equation and KL Divergence**: To address concerns about the formulation in Eq. (2), we plan to revise the equation as follows: \n\n$$\\hat{\\theta} = \\arg \\min_{\\hat{\\theta}} KL\\left[p_\\theta (\\cdot | x) \\big|\\big| p_{\\hat{\\theta}}(\\cdot)\\right]$$\n\nIn this form, the KL divergence is computed over the output distribution conditioned on input $x$. This requires summing over sentences in the distribution $p_\\theta(\\cdot|x)$, which necessitates constructing a Target Sentence Set. We will update the paper with this equation and add corresponding clarifications. \n\nAs for [W3], regarding the ablation of SlimPajama, we are running a sequential injection baseline without using SlimPajama to regularize the model. As the sequential injection takes around two days to run, we will post the results a little later. \n\nAs for the question [Q1], we provide the following response: \n[Q1] Comparison of Fine-Tuning in Sequential Injection Setting**: Given that fine-tuning did not achieve high efficacy in single-context and batch-context injection settings, we deemed it unnecessary to test its retention ability in the sequential injection setting, as it does not meet the required efficacy baseline. However, to provide clarity, we conducted additional fine-tuning experiments to explicitly show the difference between this baseline and our method. As the sequential injection takes some time to run, we will make a new comment below later about these results."
            }
        },
        {
            "title": {
                "value": "Rebuttal to Reviewer 3X9B"
            },
            "comment": {
                "value": "We sincerely thank the reviewer for the insightful comments! We address the concerns as below: \n\n**[W1] Baseline Analysis and Use of Regularization**: We appreciate the reviewer\u2019s feedback and will clarify in the paper that we incorporate regularization to mitigate catastrophic forgetting by fine-tuning on Slimpajama data while training with contexts or questions, shown as `FT (C), Q` and `FT (S), Q` in Tables 1 and 2. This regularization approach, commonly used in continual learning (e.g., retention sets or replay buffers), helps maintain prior knowledge, as seen in related work [1, 2]. Regarding distillation-based methods, while they typically target prompt or factual statement integration, they are not directly applicable for our goal of embedding more complex contexts, such as paragraphs or conversations, directly into the model. We will emphasize the use of Slimpajama as a retention mechanism in the paper to clarify this aspect of our approach.\n\n**[Q1] Limitations of Scaling SELF-PARAM to Larger LLMs or Complex Contexts**: We acknowledge the challenge of scaling SELF-PARAM to larger LLMs, where full fine-tuning may become impractical. Low-Rank Adaptation (LoRA) can address this limitation by enabling more efficient parameter updates; however, LoRA may slow convergence, which could impact training efficiency. For highly complex contexts, another potential limitation arises when `gpt-4o-mini` may be unable to generate question-answer pairs that fully capture the context\u2019s nuances, potentially leading to underutilized information during KL-divergence training. To mitigate this, we could explore generating a more diverse question-answer set and filtering redundant pairs to better preserve the depth of complex contexts during training.\n\n**[Q2] Sensitivity to Question-Answer Diversity in Context Injection**: Our training objective, Eq. (2), uses 50 question-answer pairs per context over one epoch. During our exploration of this project, we found that training on a smaller set of 10 question-answer pairs for five epochs led to lower performance. We report the results we obtained below: \n  \n| Model & # of Contexts| 50 QA pairs (1 Epoch) | 10 QA pairs (5 Epochs) |\n  | -- | -- | -- |\n  | OpenLlama-3B-v2 - 100 | 0.5082 | 0.4203 | \n  | OpenLlama-3B-v2 - 500 | 0.5048 | 0.4203 | \n  | Mistral-7B - 100 | 0.4521 | 0.3891 |\n  | Mistral-7B - 500 | 0.4384 | 0.3813 |\n\nThis analysis suggests that question-answer diversity is important to ensure comprehensive attention to context information. Limited diversity may lead to slightly reduced performance. We will add this analysis and these results into our paper. \n\n**[Q3] Comparison with Parameter-Free Methods**: In our related work section, we referenced three categories of methods without additional parameters: (1) Continual Learning, (2) Model Editing, and (3) Knowledge Distillation. As noted in [W1], the baselines `FT (C), Q` and `FT (S), Q` in Tables 1 and 2 apply regularization-based continual learning (category (1)). However, methods in categories (2) and (3) are typically restricted to embedding factual statements or prompts, making them not directly applicable in our setting where we focus on integrating broader contexts like paragraphs or conversations. We would like to emphasize that methods without additional parameters are indeed compared in our experiments, fulfilling the scope of parameter-free comparisons. \n\n[1] Improving Information Retention in Large Scale Online Continual Learning   \n[2] Active Continual Learning: on Balancing Knowledge Retention and Learnability"
            }
        },
        {
            "title": {
                "value": "Rebuttal to Reviewer CZiX (Part 2/2)"
            },
            "comment": {
                "value": "**[W3-1] Storage Complexity Across Categories**: Our goal with Storage Complexity was to represent the characteristic complexity within each category of methods, inspired by [1]. While we chose a set of representative baselines, we intended this analysis to convey that other methods within each category exhibit similar Storage Complexity. By highlighting category-level complexity rather than specific bytes, we aim to offer a more generalized perspective. We will refine our wording to clarify our intentions. \n\n**[W3-2] Applicability to Rapid Integration and Session-Based Scenarios**: We thank the reviewer for this insightful suggestion. Based on further research, we found that LOCOMO [2] could be adapted to our setting, possibly by dividing interactions into session-like segments based on date, even if they lack a formal session structure. We find this a promising direction and intend to include it in our future work. \n\n**[W4] Hallucination Mitigation** We appreciate the reviewer\u2019s example of \u201cAssume the current year is 1985\u201d as it effectively highlights the potential risk of hallucination. To mitigate such issues, we filtered out prompts that require direct access to prior context in the original PwC dataset. For example, prompts such as \u201csummarize the previous context\u201d and \u201cwrite a title for the above text\u201d were removed, leaving only factual questions. This refined dataset contains context and questions that are based on constant knowledge, as illustrated below: \n  \n    - Question: What language was spoken in the tape recording of Jamal Khashoggi's murder? Answer: Arabic\n    - Question: Identify the start date for United Airlines' service from Paine Field in Everett, Washington. Answer: March 31\n    \nThese factual statements are less likely to induce hallucination, which helps support the method\u2019s robustness.\n\nRegarding the Target Sentence Set, we are not directly fine-tuning the model to predict each word within this set. Instead, we employ KL divergence to transfer the behavior from the original model to the updated one, which may reduce the risk of hallucination. We appreciate the reviewer\u2019s valuable feedback and will further review the Target Sentence Set to ensure that any problematic cases are addressed. Removing examples that may induce hallucination is unlikely to reduce our method\u2019s efficacy, as such examples would typically increase the KL divergence, making optimization more challenging. \n\n\n[1] Towards LifeSpan Cognitive Systems.    \n[2] Evaluating Very Long-Term Conversational Memory of LLM Agents"
            }
        },
        {
            "title": {
                "value": "Rebuttal to Reviewer CZiX (Part 1/2)"
            },
            "comment": {
                "value": "We sincerely thank the reviewer for all these constructive points! We provide the following detailed responses to each point: \n\n**[W1-1] Cost implications of using an external teacher LLM**: We appreciate the reviewer\u2019s emphasis on this point. We anticipated the potential impact of the information augmented by an advanced model, so we included an experiment that fine-tunes on the Target Sentence Set (QA pairs generated by `gpt-4o-mini`), denoted as `FT (S), Q` in Tables 1-2 and `FT (Q)` in Table 3. As mentioned in Lines 358\u2013360, \u201cMeanwhile, FT (S) performs better than FT (C) but still falls short of SELF-PARAM, highlighting that our method does more than merely injecting generated QA pairs\u2014it facilitates the model\u2019s ability to internalize and dynamically apply the context.\u201d Thus, while the Target Sentence Set may potentially introduce knowledge from the external model, the primary gain stems from the design of Eq. (2) and the KL-divergence training. We will further clarify this point in our experiments to demonstrate the effectiveness of our framework. \n\n**[W1-2] Role of KL Divergence in Observed Performance Gains**: As indicated in [W1-1], we argue that our method\u2019s performance advantage primarily arises from the training structure we introduce, rather than the Target Sentence Set or external model knowledge alone. To make this distinction clearer, we will add further clarifications on the role of `FT (S), Q` as a baseline in our experiments. \n\n**[W1-3] Clarification on the Title**: We understand the reviewer\u2019s concern and will adjust our language accordingly. \n  - By \u201cself-updatable,\u201d we refer to the model\u2019s capacity to integrate new data autonomously. While we used gpt-4o-mini to generate the Target Sentence Set in our experiments, this task could be handled by the model itself, given sufficient capacity (e.g., mistral-7b-instruct). We conduct the experiments on `mistral-7B` with batch context injection (the same setting as the column `Mistral-7B` in Table 2), while instead of using `gpt-4o-mini` to generate the question-answering pairs, we use `Mistral-7B-Instruct-v0.3` to run the generation, obtaining 50 question-answering pairs for each context. Then we run the experiments of batch context injection with 100 contexts and show the results below: \n    |  | 100 Contexts | \n    | ---- | ---- | \n    | `gpt-4o-mini` | 0.4521 |\n    | `mistral-7B-instruct-v0.3` | 0.4502 |\n\n    From this table we can see that our method is still effective even without using a more advanced model to construct the question-answering pairs. Moreover, In practical applications, preprocessing via APIs (such as gpt-4o-mini) can be used to handle incoming data before self-updating, which serves as preprocessing rather than altering the model\u2019s inherent updating mechanism. We will clarify this terminology to avoid any potential ambiguity in our description.\n  - By \"Updatable LLMs with Parameter Integration\", we aim to demonstrate that we can update the parameters to integrate the context into the model parameters. To avoid exaggeration, we propose to change our title to \"Self-Updatable Large Language Models with Integrating Context into Model Parameters\". \n\n**[W2] Novelty and Terminology Clarification for Context Injection**: We appreciate the reviewer\u2019s insights on distinguishing our approach from related work. While earlier methods have proposed integrating prompts or factual statements, our concept of \u201ccontext injection\u201d specifically addresses integrating various context types (e.g., paragraphs or conversations) into the model. We will adjust our phrasing to \u201cWe focus on the context injection task, defined as ...\u201d instead of \u201cWe introduce\u201d to ensure our claims are presented with appropriate nuance."
            }
        },
        {
            "title": {
                "value": "Rebuttal to Reviewer wRpc"
            },
            "comment": {
                "value": "We sincerely thank the reviewer for the insightful suggestions! We address the concerns as below: \n\n**[W1] Computation Cost**: We appreciate the reviewer\u2019s concern regarding computation cost and acknowledge that our method involves an additional forward pass. However, we would like to clarify that the backward pass is comparable to standard fine-tuning with a next-word-prediction loss. This yields a time complexity ratio of 2:3 between our method and standard fine-tuning, assuming equal forward and backward pass times (though the backward pass can be slower). While our method indeed incurs additional computation, we believe this overhead is justified by the performance gains it achieves, which standard fine-tuning would not match regardless of training duration.\n\n**[W2] Baselines**: We acknowledge that BM25 and DPR are less recent retrieval methods, and appreciate the reviewer\u2019s suggestions. We would like to mention that we have also included RAPTOR as a baseline, which is a current approach (published in January 2024) and actually newer than the recommended methods (published in 2021 and 2022). Nonetheless, we will incorporate these suggested references into the related work section. \n\n**[Q1] Clarification of Self-Updating**: By \u201cself-updating,\u201d we refer to the model\u2019s capability to integrate new data autonomously. In our experiments, we utilized `gpt-4o-mini` to generate the Target Sentence Set, but this could be replaced by the model itself if the model is fairly powerful, such as `mistral-7b-instruct`. Once we obtain the Target Sentence Set, the training is conducted solely on the model itself. In practical applications, APIs such as `gpt-4o-mini` may be used to preprocess incoming data before self-updating. Here, the role of APIs serves primarily as preprocessing rather than altering the model\u2019s inherent updating capability. We will further clarify this terminology to prevent ambiguity.\n\n**[Q2] Evaluation of Retention**: Thank you for highlighting the need for clarity regarding retention evaluation. In Section 4.3, we describe how sequential injection experiments are used to assess retention, specifically under \u201cResults and Discussions.\u201d To address this, we will add an explicit statement at the beginning of the Experiment section to ensure the retention evaluation setup is clear.\n\n**[Q3] Ablation Study**: We appreciate the reviewer\u2019s suggestion to further clarify our ablation analysis. Our study incorporates two main techniques: (1) KL-Divergence minimization (Eq. 2) and (2) Target Sentence Set construction. In our experiments, we conducted the following analyses:\n\t- We fine-tuned the model solely on the Target Sentence Set, labeled as `FT (S), Q` in Table 1-2 and `FT (Q)` in Table 3. This setting serves as an ablation for KL-Divergence, isolating the effect of our objective function by comparing the results of `FT (S), Q` with those of our full method. This comparison highlights the unique contribution of KL-Divergence to our approach.\n\t- Further, comparing `FT (S), Q` with `FT (C), Q` (fine-tuning on context without the Target Sentence Set) allows us to assess the impact of the Target Sentence Set. The performance gains of `FT (S), Q` over `FT (C), Q` indicate that the Target Sentence Set offers marginal improvements, but the primary performance boost is attributed to our design in Eq. (2)."
            }
        },
        {
            "summary": {
                "value": "This paper introduces SELF-PARAM, a novel method for updating LLMs with new knowledge without requiring additional storage parameters. The key insight is using KL divergence minimization between models with and without context access to embed knowledge directly into model parameters. Through comprehensive experiments across various injection scenarios and a practical demonstration on conversational recommendation tasks, the authors show SELF-PARAM outperforms existing methods while maintaining zero storage overhead - effectively solving the challenge of efficient knowledge integration in LLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The writing of the paper is clear and easy to follow and understand.\n2. The paper proposes a novel and elegant solution to knowledge injection in LLMs by using KL divergence minimization - it's technically sound while being simple. The zero additional storage requirement is a plus compared to existing methods that need external memory or retrieval modules.\n3. The empirical evaluation is overall comprehensive and convincing. The authors tested their method across multiple dimensions (single/batch/sequential injection) and backed up their claims with solid results."
            },
            "weaknesses": {
                "value": "1. The computing cost is significantly larger for the proposed method compared to next-token-prediction fine-tuning. For one query, it has to run the base model twice to get the two corresponding probability distributions. This paper only discusses the space complexity. The comparison of time complexity is also worth discussing.\n2. In the experiment setting, this method utilizes two popular dense retrieval (DPR) and spare retrieval (BM25) methods as baselines. However, these two methods are a little bit too old to be considered as the baselines. Using a stronger retrieval model, such as [1][2], will better demonstrate the superiority of the proposed method.\n\n[1] Izacard, Gautier, et al. \"Unsupervised dense information retrieval with contrastive learning.\" arXiv preprint arXiv:2112.09118 (2021).\n\n[2] Wang, Liang, et al. \"Text embeddings by weakly-supervised contrastive pre-training.\" arXiv preprint arXiv:2212.03533 (2022)."
            },
            "questions": {
                "value": "1. I found the term self-updating very confusing here. This up-updating method requires (1) an extra training process to update the parameters and (2) an extra instruct model (GPT-4o-mini) to construct the training data. I think the authors could provide more insight into how to understand self-updating.\n2. Retention is discussed in the introduction but not mentioned in the experiments. I wonder how the authors evaluate the aspects of retention performance. \n3. An ablation study is missing from this paper; there are multiple techniques (KL-div, diverse instructions construction...) introduced in this method, and how each one contributes to the final result should be investigated and compared. At least, some discussion should be included."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a method to inject context information into LLMs on-the-fly, with two criteria: being able to effectively use the injected the context (efficacy), and retain context information for a long time, as more additional information is added (retention).\nThe method involves two key steps:\n\n1. An external LLM is used to augment the given context information by generating relevant question-answer pairs.\n2. The base LLM is fine-tuned on the question-answer pairs, using KL-divergence.\nNotably, the base LLM is fine-tuned such that $P(A_{i} | Q, A_{0, ..., i-1})$ follows $P(A_{i} | C, Q, A_{0, ..., i-1})$. I.e., the LLM is trained to answer the plain question (where the context had *not been* given), as if the context *had been* given.\n\nThe authors evaluate the method under three settings:\n\n1. Single context injection: inject a single piece of context information and evaluate on a held-out question.\n2. Batch context injection: inject a batch (100 to 500 pieces) of context information and evaluate on held-out questions.\n3. Sequential injection: inject contexts 0 to i and evaluate on question for context i (for i in 0 ... 20).\n\nThe authors compare with the following baselines:\n\n1. prompting with the relevant context (upper-bound)\n2. fine-tuning on the context itself\n3. LLMs with external memory\n4. infinite-context methods (InfLLM)\n5. RAG methods\n\nThe proposed method outperforms all previous methods (2\u20135), often by a wide margin."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The method substantially outperforms previous methods.\n1. The method is empirically validated on three evaluation settings, across three base LLMs, comparing with many baseline methods.\n1. The writing is easy to follow and the experiment results are presented clearly."
            },
            "weaknesses": {
                "value": "1. **The requirement of an external teacher LLM is heavily underemphasized**: Throughout the paper, the authors emphasize that the main component of the proposed method is the *KL divergence objective* applied to the base model. However, the augmented context information (QA pairs) generated by the *external model* appear to play a significant role and come with non-trivial costs. This highlights several points of potential improvement:\n\n    1. **Cost implications of using an external teacher LLM**: The external LLM used to augment context information (gpt-4o-mini) is considerably more advanced than the base models considered in this study (OpenLLaMA 3B, Mistral 7B, Llama 3 8B), effectively positioning it as a \"teacher\" model. This substantial cost factor is neither discussed nor mentioned in the paper and could benefit from additional clarity.\n\n    1. **Unclear role of KL divergence in observed performance gains**: Given the role of augmented context information (QA pairs) generated by the external LLM, it remains uncertain how much of the observed performance gains stem directly from the KL divergence objective. A useful ablation study might involve comparing the proposed method with a fine-tuning baseline that employs next-word prediction (NWP) on the generated QA samples, rather than KL divergence with the context-prompted base LLM. This could clarify the unique contribution of the KL divergence. Note that the fine-tuning baseline experiment in the paper only uses the context information itself, rather than the augmented QA pairs.\n\n    1. **Potentially misleading title**: Given the dependency on an external LLM, it may be misleading to describe the method as enabling \"self-updatable LLMs\". Additionally, describing fine-tuning (using KL divergence) as \u201cupdatable LLMs with parameter integration\u201d may over-exaggerate the scope of the work.\n\n1. **The novelty of the proposed context injection method may be overstated**: As described in Lines 124-128, previous works have also introduced the approach of distilling a given prompt into the model by replicating the output distribution when the prompt has been prepended (e.g., Choi et al. 2022), extending to distilling factual knowledge (context information) as well (Padmanabhan et al. 2024). Claims such as \u201cThis paper introduces the concept of Context Injection in LMs\u201d (Line 154) might be reconsidered for a more balanced presentation.\n\n1. **Lack of discussion on the costs associated with fine-tuning**: The computational cost of fine-tuning LLMs is substantially higher than that of inference, typically requiring roughly triple the computation due to backward passes, and significantly more memory to store intermediate activations. This additional cost is unique to fine-tuning approaches and does not apply to non-fine-tuning alternatives (e.g., MemoryLLM, InfLLM, RAG methods). However, the paper only contrasts storage requirements, which might downplay the runtime costs associated with fine-tuning in the proposed method.\n    1. **Insufficient clarification of storage requirements across methods**: Providing more details on the storage requirements of the baseline methods\u2014specifically by showing the constant factor (e.g., number of bytes) relative to the full model size\u2014could be beneficial to readers, alongside asymptotic complexity.\n    1. **Limited applicability of the method for \"rapid integration\"**: While the paper discusses the need for \u201crapid and frequent integration of small-scale experiences\u201d (Lines 11-12), it is unclear how the fine-tuning-based approach proposed here supports rapid integration. A clarification might help strengthen this point.\n        - To achieve rapid integration in a ChatGPT-like setting, one could use the context window to retain current session information and employ the proposed method (or baseline methods) to store and retrieve context from prior sessions. Fine-tuning could occur between sessions to support this integration. This may represent a more realistic evaluation setting. This may also achieve higher overall efficacy, as evidenced by the results in the single-context injection scenario, where the base model with the context provided directly in the prompt outperforms all other methods.\n\n1. **Potential risk of encouraging hallucination**: Standard LLM pretraining typically encourages the model to respond accurately to the given context. Under the proposed method, however, the model is fine-tuned to respond to a question \\( Q \\) as though a specific context \\( C \\) were provided, even when it is not. This could introduce a risk of hallucination (and other undesirable behavior) by training the model to produce responses that might not be contextually relevant. For example, if the model is trained with QA pairs based on the context \u201cAssume the current year is 1985,\u201d it might answer a question like \u201cWho is the current president?\u201d with \u201cRonald Reagan,\u201d disregarding the actual context. *(This is an illustrative example to convey the potential issue.)*\n\n*This section was written by the reviewer, re-worded using ChatGPT, and manually checked by the reviewer*."
            },
            "questions": {
                "value": "Please refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents SELF-PARAM, a framework for enabling self-updatable large language models that integrate new knowledge directly into model parameters without additional storage modules. By minimizing the Kullback-Leibler (KL) divergence between an original model (with context access) and a target model (without context), SELF-PARAM effectively embeds context within the model\u2019s parameters. The framework addresses the challenges of rapid experience integration and long-term retention by incorporating diverse question-answer pairs related to the injected knowledge. Experimental results show SELF-PARAM\u2019s effectiveness across single and batch context injections, sequential knowledge injections, and conversational recommendations, outperforming baseline methods in knowledge retention and response accuracy without extra parameters or external storage."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality: The approach of embedding knowledge directly in model parameters through KL divergence minimization of the source & target model while excluding the context information from the target model is a creative solution to self-updateability without extra storage.\nQuality: Experimental results validate the approach across multiple tasks, showing improvements over existing methods in terms of efficacy and retention.\nClarity: The methodology and experiments are largely well-explained, making the paper accessible to researchers familiar with LLMs.\nSignificance: Addressing rapid knowledge integration and long-term retention in LLMs is a critical step forward, particularly for applications requiring frequent updates in dynamic environments."
            },
            "weaknesses": {
                "value": "Limited Baseline Analysis: While the paper compares extensively against memory-based methods, it does not do any comparison with other methods that are similar to the proposed methods, such as regularization approaches or other distillation based approaches that are commonly used in the continual learning literature."
            },
            "questions": {
                "value": "Questions\n- Could the authors elaborate on potential limitations when scaling SELF-PARAM to even larger LLMs or highly complex contexts?\n- How sensitive is the model\u2019s performance to variations in the diversity of the question-answer pairs generated for context injection?\nSuggestions\n- I would suggest that the authors add other baselines that do not add additional modules or parameters; they are mentioned in the introduction section, but no explicit comparisons were made in the experimental section. Since this paper highlights the component that no additional parameters are needed, such comparisons should be made with those methods as well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a context injection method SELF-PARAM.\nGiven a context, SELM-PARAM (i) employs an instruct model to generate a set of contextually relevant question-answering pairs (ii) trains the model by minimizing the KL divergence with the distribution where the context is provided.\nEmpirical evaluation is conducted to inject the contexts in PwC dataset into OpenLlama-3B-v2, Mistral-7B, and Llama-3-8B and INSPIRED and REDIAL into Mistral-7B, where the proposed method achieves consistently superior performance than the baselines in various settings."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The proposed method requires no additional parameter and storage, which is computationally and memory efficient and compatible with the standard serving engine.\n* In the empirical evaluation, the proposed method achieves superior performance in various settings, which seems inspiring."
            },
            "weaknesses": {
                "value": "* My primary concern lies in the novelty.\nPrompting highly capable language models have been a common method to collect training data and KL divergence is a common loss in knowledge distillation.\n* Eq. (2) is ill-posed.\nThe summation is over any sentence $s$, making the term infinite and cannot be minimized.\nIt is also unclear to me what is the KL divergence between two scalar rather than distributions.\n* Unrelated sentences are randomly sampled from SlimPajama to maintain the linguistic capabilities, but relevant ablation and evaluation are missing."
            },
            "questions": {
                "value": "* Why the fine-tuning is not compared in the sequential injection setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}