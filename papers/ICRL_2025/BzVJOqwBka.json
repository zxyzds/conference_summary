{
    "id": "BzVJOqwBka",
    "title": "Prompt-Guided Distillation from Multimodal Large Language Models to Task-specific Models for Multimodal Sentiment Analysis",
    "abstract": "Multimodal Sentiment Analysis (MSA) has made some progress with the advent of Multimodal Large Language Models (MLLMs). However, the scalability and the closed-source nature of some MLLMs imposes challenges for efficient application in the real-word. In this study, we explore an innovative pathway to infuse the capabilities of general MLLMs into task-specific small models for MSA. We introduce the Prompt-Guided Multimodal Framework (PGMF), a refined teacher-student framework designed to transfer knowledge from powerful, general MLLMs to smaller, efficient models. The PGMF-Teacher utilizes MLLM-generated prompts and a tailored conditional alignment module to achieve better MSA, while the PGMF-Student distills this expertise to predict independently of MLLMs' guidance. Extensive evaluations on two popular MSA datasets including SIMS and MOSI demonstrate that compared to previous task-specific small models, PGMF-Teacher achieves state-of-the-art performance with the help of MLLMs' prompts, while PGMF-Student achieve competitive results with fewer parameters and without relying on MLLMs' prompts. The proposed framework offers a novel way to equip task-specific small models with the capability of MLLMs.",
    "keywords": [
        "Multimodal Sentimen Analysis",
        "Representation Learning",
        "Multimodal Large Language Model",
        "Knowledge Distillation"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=BzVJOqwBka",
    "pdf_link": "https://openreview.net/pdf?id=BzVJOqwBka",
    "comments": [
        {
            "title": {
                "value": "Response to Reviewer PNKv"
            },
            "comment": {
                "value": "## **Response to W1**\n\n**Thank you for suggesting the inclusion of the CMU-MOSEI dataset for evaluation.** Due to the **financial cost** of using chatGPT API to help in training on such a large dataset, we faced some **economic constraints** during the initial experiments, which made it challenging to conduct evaluations on MOSEI. In response to your feedback, we are currently preparing to conduct experiments on the CMU-MOSEI dataset. This process will require some additional time because we need to run **five times** to obtain reliable results on this larger dataset. **We will update our results on OpenReview** **once available****.**\n\n## **Response to W2**\n\nThanks for your suggestion! We agree that optimized seed results may not provide meaningful insights. We will remove these rows to make room for additional content, such as related work. \n\n## **Response to W3**\n\nIn the revised version, we will bring the \"Related work\" section back into the main body of the paper. Thanks for your suggestion.\n\n## **Response to W4**\n\nThank you for your feedback! We have realized that the method section may appear somewhat redundant. Our intention was to provide thorough details to prevent potential misunderstandings. However, we understand that this has led to issues with verbosity. We will check these sections to improve clarity and conciseness. Additionally, we will relocate the \u201cRelated Work\u201d section back to the main body, as recommended. \n\n## **Response to W5**\n\n1. Figure 3 is an **attention difference map (not attention map)**, obtained by subtracting the attention map without MLLM guidance from that of the PGMF-Student. The values represent the difference in attention scores, with positive differences indicating areas where the PGMF-Student pay more attention to.\n2. In the original attention map, each row\u2019s attention scores sum to 1 after the softmax operation. However, due to the relatively long sequence length (e.g., 55 frames for video sequences in the SIMS dataset), the attention scores become more distributed across frames, resulting in a sparsity and low attention scores. Despite the low individual values, we can see from the difference map that these subtle changes in scores effectively shift the model\u2019s focus across large regions, showing the prompt\u2019s impact on attention distribution.\n3. In long-sequence video data, we think changes between adjacent frames are continuous and gradual, making large attention score changes less possible. However, small changes in attention scores across many regions are sufficient to shift the model\u2019s focus and significantly impact the multimodal alignment.\n4. Our pipeline is intentionally designed to be simple, without any complex mechanisms in the alignment and fusion modules. This choice ensures that the improvements in performance are due to the framework itself, rather than relying on intricate structures or tricks. We believe this simplicity also demonstrates the effectiveness of our proposed framework."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer wMDu"
            },
            "comment": {
                "value": "## **Response to Weakness**\n\nThank you very much for your feedback for our work! We do appreciate your recognition of the originality of our work. Indeed, we are the first to introduce this specific framework that leverages MLLMs to help the training of the task-specific small-scale models for better MSA.\n\n**About the weakness, our choice of a simple and straightforward model structure was intentional.** By keeping the design clear and direct, we aimed to demonstrate the core effectiveness of our idea and framework itself. This can ensure that the improvements we observe are due to the framework **rather than any particular trick or complex structural design**. We believe that a simple yet effective method can also offer valuable contributions to the community, as has been shown by many solid works [3,4,5]. Our goal is to do a solid work which we hope can inspire further research and practical applications in MSA."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer 9bUK"
            },
            "comment": {
                "value": "## Response to Q6/W6\n\nThank you for highlighting the importance of comparing parameter counts with other baselines. **In Table 3 of the paper**, we included a comparison with ALMT (the best-performing prior method). However, we also have realized that it is important to compare with more methods. Therefore, **as shown in Table blow**, we have added parameter count comparisons with other relevant methods. **In addition, it is worth noting that for the latest methods like** **CuDA** **[6] and FISFN [7] mentioned above, we were unable to include parameter counts due to the lack of open-source code.** \n\nAs we can see from the table below, although PGMF-Student is with the second smallest parameter size only to Self-MM (a simple, direct and effective method), it achieves better performance. This demonstrates the effectiveness of our proposed framework and shows that PGMF achieves a balance of performance and parameters. In addition, we have also achieved significant improvements compared to the Transformer-based methods with few parameters.\n\n| SIMS         |           |                                |                           |                           |                |                |\n| ------------ | --------- | ------------------------------ | ------------------------- | ------------------------- | -------------- | -------------- |\n| Method       | Parameter | Transformer-based Architecture | Acc-2                     | F1                        | MAE            | Corr           |\n| TFN          | 35.63M    |                                | 78.12\u00b11.56                | 77.83\u00b11.62                | 0.434\u00b11.12     | 0.579\u00b11.50     |\n| MISA         | 21.66M    | \u2714\ufe0f                              | 77.72\u00b11.10                | 76.54\u00b11.67                | 0.451\u00b11.83     | 0.570\u00b11.95     |\n| Self-MM      | 0.38M     |                                | 77.94\u00b11.11                | 77.72\u00b10.68                | 0.418\u00b11.05     | 0.589\u00b11.54     |\n| TETFN        | 1.53M     | \u2714\ufe0f                              | 80.18\u00b10.49                | 79.34\u00b10.52                | 0.422\u00b11.30     | 0.588\u00b11.71     |\n| ALMT         | 2.60M     | \u2714\ufe0f                              | 79.91\u00b10.29                | 80.17\u00b10.60                | 0.421\u00b10.69     | 0.583\u00b10.70     |\n| PGMF-Teacher | 2.54M     | \u2714\ufe0f                              | **83.06\u00b10.95**            | **84.06\u00b10.43**            | **0.370\u00b10.50** | **0.690\u00b10.80** |\n| PGMF-Student | 0.82M     | \u2714\ufe0f                              | 81.40\u00b11.58                | 81.85\u00b11.41                | 0.382\u00b11.39     | 0.662\u00b11.26     |\n| **MOSI**         |           |                                |                           |                           |                |                |\n| Method       | Parameter | Transformer-based Architecture | Acc-2                     | F1                        | MAE            | Corr           |\n| TFN          | 9.50M     |                                | 77.38\u00b11.37/78.11\u00b10.60     | 77.35\u00b11.33/78.02\u00b10.57     | 0.949\u00b13.13     | 0.662\u00b11.95     |\n| MISA         | 1.14M     | \u2714\ufe0f                              | 80.93\u00b10.99/81.05\u00b10.83     | 80.90\u00b11.03/81.01\u00b10.87     | 0.773\u00b11.81     | 0.775\u00b10.63     |\n| Self-MM      | 0.16M     |                                | 82.94\u00b10.63/83.18\u00b10.35     | 82.95\u00b10.63/83.09\u00b10.36     | 0.717\u00b11.53     | 0.792\u00b10.55     |\n| TETFN        | 1.25M     | \u2714\ufe0f                              | 80.87\u00b10.52/80.82\u00b10.53     | 80.87\u00b10.52/80.82\u00b10.53     | 0.726\u00b11.68     | 0.791\u00b10.86     |\n| ALMT         | 2.50M     | \u2714\ufe0f                              | 83.00\u00b10.22/85.12\u00b10.20     | 83.00\u00b10.22/85.19\u00b10.27     | **0.713\u00b10.75** | 0.795\u00b10.54     |\n| PGMF-Teacher | 1.45M     | \u2714\ufe0f                              | **85.05\u00b10.66/86.61\u00b10.69** | **85.15\u00b10.66/86.69\u00b10.69** | 0.734\u00b11.46     | **0.797\u00b10.60** |\n| PGMF-Student | 0.53M     | \u2714\ufe0f                              | 83.62\u00b10.91/85.37\u00b11.00     | 83.68\u00b10.96/85.50\u00b10.96     | 0.746\u00b11.63     | 0.775\u00b11.10     |"
            }
        },
        {
            "title": {
                "value": "Response to Reviewer 9bUK"
            },
            "comment": {
                "value": "## Response to Q4/W4\n\n**In our initial submission, the most recent baseline with best results was ALMT.** It was published in December 2023 at EMNLP, which was **one of the best results with open-source code available** at the time. However, with recent releases from conferences like EMNLP 2024, we now have additional methods for comparison. Specifically, we have included the latest baselines for comparison, including KuDA [6] and FISFN [7]. **As shown in the table below**, we can see that PGMF-Teacher/Student achieves SOTA performance in all metrics on the SIMS dataset, demonstrating the effectiveness of our idea and framework. On MOSI datasets, the PGMF-Teacher/Student also can achieve good performance, especially in Acc-2 and F1. **In addition, it should be noticed that these recent methods are not open-sourced. So we are unable to conduct multiple runs to report the mean and standard deviation for a comprehensive comparison.** \n\n| SIMS         |                           |                           |                |                |\n| ------------ | ------------------------- | ------------------------- | -------------- | -------------- |\n| Method       | Acc-2                     | F1                        | MAE            | Corr           |\n| KuDA         | 80.74                     | 80.71                     | 0.408          | 0.613          |\n| FISFN        | 80.50                     | 80.7                      | 0.397          | 0.619          |\n| PGMF-Teacher | **83.06\u00b10.95**            | **84.06\u00b10.43**            | **0.370\u00b10.50** | **0.690\u00b10.80** |\n| PGMF-Student | 81.40\u00b11.58                | 81.85\u00b11.41                | 0.382\u00b11.39     | 0.662\u00b11.26     |\n| **MOSI**         |                           |                           |                |                |\n| KuDA         | 84.40/86.43               | 84.48/86.46               | **0.705**      | 0.795          |\n| FISFN        | 85.0/86.0                 | 85.0/86.0                 | 0.707          | **0.801**      |\n| PGMF-Teacher | **85.05\u00b10.66/86.61\u00b10.69** | **85.15\u00b10.66/86.69\u00b10.69** | 0.734\u00b11.46     | 0.797\u00b10.60     |\n| PGMF-Student | 83.62\u00b10.91/85.37\u00b11.00     | 83.68\u00b10.96/85.50\u00b10.96     | 0.746\u00b11.63     | 0.775\u00b11.10     |\n\n## Response to Q5/W5\n\n**Thank you very much for your feedback on the \"EFFECT OF EACH COMPONENT\" section!** We would like to clarify that this paragraph was not generated by AI. In our submission, we condensed this section to meet length requirements, which may have inadvertently impacted the depth of the analysis. To address your concern, we have added detailed discussion in this section to provide a more critical analysis. In addition, we will reduce less essential content, such as removing single-run results and retaining only the multi-run averages and standard deviations for comparison, as suggested by Reviewer PNKv. The revised paragraph is as follows:\n\n> To evaluate the impact of each component within the framework, we conducted experiments by removing specific components. First, when we removed the MLLMs' prompt from the PGMF-Teacher, we observed a significant drop in performance across both datasets. Specifically, on the SIMS dataset, the F1 score decreased from 84.06% to 80.84%, and MAE increased from 0.370 to 0.436. A similar trend was observed on the MOSI dataset, where the F1 score dropped from 85.15% to 79.60%, and MAE increased from 0.734 to 0.914. These phenomenoa show that the MLLMs plays a crucial role in helping the model capture relevant multimodal information more effectively. Scond, we removed the guidance of the PGMF-Teacher during the training of the PGMF-Student. This led to a noticeable decrease in the student model's performance, with the F1 score on SIMS dropping from 81.85% to 78.72%, and on MOSI from 83.68% to 83.00%. The increase in MAE values on both datasets also reflects the PGMF-Student model's reduced ability to align multimodal information without teacher guidance. This result shows that the importance of knowledge distillation, as the PGMF-Teacher's guidance can help the PGMF-Student learn the relationship between each modality effectively. \n>\n> In addition, we also observed that the guidance from the PGMF-Teacher had a greater impact on the student model\u2019s performance on the SIMS dataset compared to the MOSI dataset. We believe that this difference may be because of the diversity of data in the SIMS dataset. Specifically, the data of SIMS dataset contains complex environments and disturbances such as lighting, head pose and audio background noise. This makes the data difficult for the PGMF-Student to achieve better performance without relying on the guidance of the PGMF-Teacher."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer 9bUK"
            },
            "comment": {
                "value": "## Response to Q1/W1\n\n**As shown in Figure 2 of the paper**, we first use the MLLMs' prompt as a query to extract essential information from other modalities, obtaining a shifted attention map \u25b3. This shifted attention map \u25b3 is then applied to the original attention map by the **dot product (it can be seen as an attention map fusion/transfer)**, effectively adjusting and optimizing the alignment process with the help of MLLMs. To verify the effectiveness of our idea, we choose the straightforward (dot product) to design the conditional alignment module. By employing a simple pipline, we focused on demonstrating that the MLLMs, even with basic structures/operations, could provide guidance that helps improve the task-specific small-scale model\u2019s representation learning and overall performance. Specifically, the responses to contributions are as follows:\n\n1. **As mentioned in General Response, the dot product within the conditional alignment is not the innovation point of the PGMF. Our focus is to validate the effectiveness of the framework through the straightforward design.** Instead, the core contribution lies in providing a novel way for MLLMs to directly participate in regulating the alignment process, helping the task-specific small-scale model focus on more relevant cross-modal relationships. This innovation combines MLLMs' prompts with cross-modal alignment, enabling more efficient completion of alignment tasks and improving performance in multimodal sentiment analysis. Although some existing methods [1,2] use MLLMs/LLMs to assist task-specific small-scale model training, they typically rely on MLLMs/LLMs to generate high-quality data. In contrast, our approach differs fundamentally in principle.\n2. **As shown in Figure 3 of the paper**, we demonstrates the effect of the conditional alignment module by showing the difference between vision-language attention maps with and without the MLLMs' help. The difference map clearly indicates that the MLLMs can help the model focus more precisely on key regions in the language and visual modalities, demonstrating the effectiveness of this core idea. Additionally, experiments on the SIMS and MOSI datasets (**Table 1, Table 2 and Table 3 of the paper**) show that the prompt-guided alignment module enables both the PGMF-Teacher and PGMF-Student to achieve state-of-the-art performance across most metrics. These results further confirm the empirical contribution of our framework.\n\n## Response to Q2/W2 \n\n**As mentioned in General Response**, our primary focus is not on the fusion method itself, but rather on how MLLMs are leveraged to help the learning of task-specific small-scale models. Therefore, **our innovation lies in the whole pipeline of the proposed framework PGMF.** Therefore, we opted for a simple way in the fusion module, using concatenation and a Transformer encoder, to emphasize the core impact of MLLMs in improving the task-specific small-scale model\u2019s performance. **By achieving performance improvements** **without complex module design****, we believe the model's improvements in** **MSA** **are** **mainly derived from the framework**. We hope this clarification helps to convey the main contribution to our work.\n\n## Response to Q3/W3\n\n**As mentioned in General Response**, **our core idea centers on the whole pipline** that using MLLMs to help the learning of task-specific small-scale models. **We intentionally chose the simplest method to validate our idea, which we believe can better demonstrate the effectiveness of our method.** By keeping the implementation straightforward, we can clearly show that the performance gains are due to the prompt-guided alignment **rather than any additional complex module designs**. \n\nIn addition, Reviewer wMDu confirmed that our method has not been seen in other papers, it is original. Reviewer PNKv recognized that our method is with solid contribution and could be utilized for more tasks by the research community. These comments also demonstrate the novelty of our PGMF. **In a word, we believe that simple and effective methods are also valuable, simplicity is not an indication of lack of innovation. There are many works [3,4,5] that has simple architecture but make a great contribution to the community.**"
            }
        },
        {
            "title": {
                "value": "General Response"
            },
            "comment": {
                "value": "# General Response\n\nWe appreciate the valuable time and effort from all reviewers, as well as the constructive comments and suggestions that contributed significantly to the improvement of our paper. **We are eager to engage in further discussions with you to address your concerns.**\n\n**First, we would like to restate our motivation:**\n\nWe found that there are two limitations to apply MLLMs to MSA. 1) Although MLLMs have shown some improvement in MSA, their performance gains are often marginal and come at a high computational cost. 2) **Unlike the common practice in other fields** [1,2], using MLLMs to generate high-quality data for training small models is challenging for MSA due to the complexity of generating video, audio, and text data together. **These limitations** **motivate** **us to explore a different** **and more efficient** **direction: leveraging the guidance** **of** **MLLMs to help in training task-specific small-scale models for better MSA.** By involving MLLMs' prompt as guidance during the alignment and distillation process, we designed the PGMF that make the task-specific small-scale model to benefit from the MLLMs' knowledge. This presents a novel efficient framework.\n\n**Second, we would like to restate the novelty is the framework rather than the model's architecture:**\n\n1. At the framework level, our pipeline **for the first time** utilizes prompt outputs from MLLMs as conditional guidance within a teacher-student framework, effectively improving the alignment and learning process of the student model. The experimental results validate the effectiveness of this framework, demonstrating that it enables the student model to achieve state-of-the-art performance with reduced complexity. \n2. **We intentionally chose the simple** **and straightforward** **architecture modules** **to validate our framework**,which we believe can better demonstrate the effectiveness of our framework. By keeping the implementation straightforward, we can clearly show that the performance gains are due to the prompt-guided alignment **rather than any additional complex module designs**. \n3. **We believe that simplicity and effectiveness are valuable for the community.** There are many works [3,4,5] that have simple architecture but make a great contribution to the community. For example, in the MSA field, Self-MM [5] achieves significant performance despite using only MLPs in its network architecture. Similarly, our framework PGMF, although intentionally designed to be simple, offers a new perspective on using MLLMs to guide smaller models in multimodal tasks, which we believe can inspire further exploration and refinement in this area.\n\nThank you for your patience and suggestions. **We look forward to discussing our work with you.**\n\nSincerely,\n\nThe Authors\n\n\n# Reference Used throughout the Rebuttal\n\n[1] Chen, L. et al., 2023. Sharegpt4v: Improving Large Multi-modal Models with Better Captions. *arXiv* *preprint**arXiv:2311.12793*.\n\n[2] Chen, L. et al., 2024. Sharegpt4video: Improving Video Understanding and Generation with Better Captions. *arXiv* *preprint* *arXiv:2406.04325*.\n\n[3] Zadeh A. et al., 2017. Tensor Fusion Network for Multimodal Sentiment Analysis. In EMNLP 2017.\n\n[4] Yao-Hung Hubert Tsaiet al., 2019. Multimodal Transformer for Unaligned Multimodal Language Sequences. In ACL 2019.\n\n[5] Wenmeng Yu et al., 2021. Learning Modality-specific Representations with Self-supervised Multi-task Learning for Multimodal Sentiment Analysis. In AAAI 2021.\n\n[6] Feng, X. et al., 2024. Knowledge-Guided Dynamic Modality Attention Fusion Framework for Multimodal Sentiment Analysis. In EMNLP 2024.\n\n[7] Jin, Y., 2024. GSIFN: A Graph-Structured and Interlaced-Masked Multimodal Transformer Based Fusion Network for Multimodal Sentiment Analysis. *arXiv* *preprint* *arXiv:2408.14809*."
            }
        },
        {
            "summary": {
                "value": "This study proposes a Prompt-Guided Multimodal Framework (PGMF) to transfer the capabilities of large Multimodal Large Language Models (MLLMs) to smaller, task-specific models for Multimodal Sentiment Analysis (MSA). PGMF consists of a teacher model (PGMF-Teacher) and a student model (PGMF-Student). The teacher uses MLLM-generated prompts to achieve better alignment and sentiment analysis, while the student learns to predict independently. Experiments show that PGMF-Teacher achieves state-of-the-art performance, while PGMF-Student achieves competitive results with fewer parameters, providing an efficient way to enhance small models with MLLM capabilities."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Leveraging Multimodal Large Language Models (MLLMs) to address the current challenges in the field of multimodal sentiment analysis represents a promising and worthwhile direction for exploration.\n2. Embedding the teacher-student model paradigm within this domain is also a well-considered and potentially impactful approach."
            },
            "weaknesses": {
                "value": "1. It appears that your CONDITIONAL ALIGNMENT primarily facilitates attention-based interaction between the GPT-generated prompts and the corresponding content. However, I am unclear about the specific significance of taking the dot product of these two attention maps. From my perspective, your alignment module seems to merely apply attention mechanisms followed by a dot product, which does not appear to introduce any substantive algorithmic novelty. Could you elaborate further on the theoretical or empirical contributions this approach provides beyond the existing methods?\n\n2. Your multimodal fusion module appears to simply concatenate features from different modalities and feed them into a transformer encoder. This approach is quite common and widely adopted in existing literature.\n\n3. In a word, it seems that the paper primarily applies the teacher-student model paradigm to the domain of multimodal sentiment analysis (MSA), incorporating GPT-generated content as prompts. While the motivation is sound, the implementation appears somewhat simplistic, lacking sufficient innovation to substantiate a significant contribution.\n\n4. The selection of baselines in your comparison is quite limited, and notably, none of the baselines are from 2024. Given that this field remains highly active and rapidly evolving, I strongly recommend including more recent baselines from 2024 to provide a more comprehensive and current evaluation of your proposed approach.\n\n5. The analysis presented in the \"EFFECT OF EACH COMPONENT\" section appears rather superficial and lacks depth, raising the concern that it may have been generated by AI without sufficient refinement or critical examination.\n\n6. Since the goal is to train a student model with reduced complexity, it would be highly informative to include a comparison of parameter counts with other baselines. Such a comparison would help substantiate claims regarding the efficiency and compactness of the student model relative to existing approaches."
            },
            "questions": {
                "value": "1. It appears that your CONDITIONAL ALIGNMENT primarily facilitates attention-based interaction between the GPT-generated prompts and the corresponding content. However, I am unclear about the specific significance of taking the dot product of these two attention maps. From my perspective, your alignment module seems to merely apply attention mechanisms followed by a dot product, which does not appear to introduce any substantive algorithmic novelty. Could you elaborate further on the theoretical or empirical contributions this approach provides beyond the existing methods?\n\n2. Your multimodal fusion module appears to simply concatenate features from different modalities and feed them into a transformer encoder. This approach is quite common and widely adopted in existing literature.\n\n3. In a word, it seems that the paper primarily applies the teacher-student model paradigm to the domain of multimodal sentiment analysis (MSA), incorporating GPT-generated content as prompts. While the motivation is sound, the implementation appears somewhat simplistic, lacking sufficient innovation to substantiate a significant contribution.\n\n4. The selection of baselines in your comparison is quite limited, and notably, none of the baselines are from 2024. Given that this field remains highly active and rapidly evolving, I strongly recommend including more recent baselines from 2024 to provide a more comprehensive and current evaluation of your proposed approach.\n\n5. The analysis presented in the \"EFFECT OF EACH COMPONENT\" section appears rather superficial and lacks depth, raising the concern that it may have been generated by AI without sufficient refinement or critical examination.\n\n6. Since the goal is to train a student model with reduced complexity, it would be highly informative to include a comparison of parameter counts with other baselines. Such a comparison would help substantiate claims regarding the efficiency and compactness of the student model relative to existing approaches."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a distillation framework for Multimodal Sentiment Analysis, where knowledge is extracted from Multimodal LLMs and used to bias the attention maps of a smaller task-specific model. This model is then used as a teacher for a task-specific student model, which does not require the LLM-generated prompts during inference. The method achieves competitive performance on CMU-MOSI and state-of-the-art performance on CH-SIMS."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "*  The presented method of prompt-guided attention transfer from MLLMs is a solid contribution and could be utilized for more tasks by the research community.\n* Ablations regarding the effect of prompt-guidance, the effect of loss weight hyperparameters, and extension of the method to different architectures are included (though some are delegated to the appendix).\n* Multiple-run averages, along with standard deviations are included."
            },
            "weaknesses": {
                "value": "* The evaluation could include the CMU-MOSEI dataset, which is larger and more recent than CMU-MOSI.\n* The \"Searched best seed\" related rows in Tables 1 and 2 should be removed. Optimized seed results are rather uninformative, since the seed is not a tunable hyperparameter.\n* I do not support the choice of delegating the \"Related work\" section to the Appendix, since relation to prior work should be a key component of a research paper. Reducing the size of figures 1, 2, removing  the optimized seed results, and performing a general revision of  the paper should create space in the main part of the paper.\n* In general  Sections 2.4, 2.5 are a bit verbose and the equations are rather uninformative. Equation 11, especially so. What exactly is the fusion operation? I think these sections could benefit from a revision both in terms of concreteness and clarity and in terms of length.\n* The  attention scores in Figure 3 range from -0.0004 to  0.0006 (very close to 0 and a difference from high to low score in the 4th decimal point). I think this is very concerning for the soundness of the method. What does the model actually attend to? Could this be due to the choice of the Hadamard product operation for fusing the attention matrices, which makes the scores extremely sparse / close to zero?"
            },
            "questions": {
                "value": "My questions are included in the  \"Weaknesses\" section of the review"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper  propose a novel framework that integrates the generalized knowledge of MLLMs to guide smaller, task-specific models for better MSA.\n\ufeff\nSpecifically, visual and audio features are aligned with language features via two alignment modules: Visual-to-Language Alignment and Audio-to-Language  Alignment. These conditional alignment layers establish correspondences between modalities with the help of the prompt, facilitating effective multimodal fusion with the help of MLLMs. \n\ufeff\nBoth PGMF-Teacher and PGMF-Student can achieve good performance on two popular datasets (i.e., SIMS and MOSI), especially for PGMF-Student which can achieve improved performance without relying on prompt from MLLMs while maintaining fewer parameters. \n\ufeff\nThis approach also offers a novel way to empower task-specific small models with the capabilities of MLLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "originality: The specific method of this paper has not been seen in other papers, so I believe it is original;"
            },
            "weaknesses": {
                "value": "It is currently a common practice to distill knowledge from large models to small models and achieve improvement. This paper uses a multimodal large model to identify the clues that play a decisive role in predicting emotional labels in each modality, and then integrates them into the small model training framework for improvement. The idea is relatively straightforward, and the innovation is not particularly prominent."
            },
            "questions": {
                "value": "none"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no concern"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}