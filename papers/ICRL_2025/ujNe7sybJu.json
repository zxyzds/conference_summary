{
    "id": "ujNe7sybJu",
    "title": "Realizing Video Summarization from the Path of Language-based Semantic Understanding",
    "abstract": "The recent development of Video-based Large Language Models (VideoLLMs), has significantly advanced video summarization by aligning video features\u2014and, in some cases, audio features\u2014with Large Language Models (LLMs). Each of these VideoLLMs possesses unique strengths and weaknesses. Many recent methods have required extensive fine-tuning to overcome the limitations of these models, which can be resource-intensive. In this work, we observe that the strengths of one VideoLLM can complement the weaknesses of another. Leveraging this insight, we propose a novel video summarization framework inspired by the Mixture of Experts (MoE) paradigm, which operates as an inference-time algorithm without requiring any form of fine-tuning. Our approach integrates multiple VideoLLMs to generate comprehensive and coherent textual summaries. It effectively combines visual and audio content, provides detailed background descriptions, and excels at identifying keyframes, which enables more semantically meaningful retrieval compared to traditional computer vision approaches that rely solely on visual information, all without the need for additional fine-tuning. Moreover, the resulting summaries enhance performance in downstream tasks such as summary video generation, either through keyframe selection or in combination with text-to-image models. Our language-driven approach offers a semantically rich alternative to conventional methods and provides flexibility to incorporate newer VideoLLMs, enhancing adaptability and performance in video summarization tasks.",
    "keywords": [
        "Visual Language Model",
        "Large Language Model",
        "Video Summarization",
        "Video Understanding",
        "VideoLLM"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ujNe7sybJu",
    "pdf_link": "https://openreview.net/pdf?id=ujNe7sybJu",
    "comments": [
        {
            "summary": {
                "value": "* This work introduces a new framework inspired by the Mixture of Experts (MoE) approach, allowing VideoLLMs to complement each other\u2019s strengths without fine-tuning.\n* It provides detailed background descriptions and identifies keyframes, improving semantically meaningful video retrieval over traditional methods that rely solely on visual features.\n* The summaries enhance performance in downstream tasks such as summary video generation, both through keyframe selection and by using text-to-image models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Unlike other models that require resource-intensive fine-tuning, this framework works at inference time, saving computational resources and time and showing descent performance in various downstream tasks.\n\n* The framework is flexible and can incorporate new VideoLLMs as they are developed, allowing it to stay current with advancements in the field."
            },
            "weaknesses": {
                "value": "1. The main weakness of this work lies in the novelty part. In addition, this work is more suitable for submission to application-related conferences.\n\n   (a). The filtering strategy is more like an ensemble method that tries to get the consensus agreement across models via similarity matching, which is a common approach. \n\n   (b). For the keyframe retrieval part, utilizing CLIP via similarity score across image and text is already well explored in [1,2]. \n\n   (c). The high-level idea of summarizing video using VLM and LLM was explored in [3], which should also be cited. \n\n   (d). The idea of visualizing the visual steps for summarization is interesting. However, it was also explored in [4], where it generate the instruction step by diffusion models.\n\n2. In the experiment, the proposed model in Table 3 is significantly higher than the baseline with 94.17, which is nearly identical to ground truth. Is there any reason behind this? The result seemed seemed abnormal.\n\n3. The paper requires proofreading polished L346 ?? presents. Table 3 is out of bound.\n\n4. The framework relies on the availability and compatibility of multiple VideoLLMs, which may not always be feasible or efficient for every application, particularly in low-resource environments.\n\n[1] Luo, Huaishao, et al. \"Clip4clip: An empirical study of clip for end-to-end video clip retrieval and captioning.\" Neurocomputing 508 (2022): 293-304.\n\n[2] Portillo-Quintero, Jes\u00fas Andr\u00e9s, Jos\u00e9 Carlos Ortiz-Bayliss, and Hugo Terashima-Mar\u00edn. \"A straightforward framework for video retrieval using clip.\" Mexican Conference on Pattern Recognition. Cham: Springer International Publishing, 2021.\n\n[3] Argaw, Dawit Mureja, et al. \"Scaling Up Video Summarization Pretraining with Large Language Models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\n[4] Sou\u010dek, Tom\u00e1\u0161, et al. \"Genhowto: Learning to generate actions and state transformations from instructional videos.\" 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2024."
            },
            "questions": {
                "value": "1. Please address the weakness 1,2,4.\n2. How were the generated image frames evaluated?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a method for textual video summarization using multiple multimodal LLMs. The multiple multimodal LLMs individually takes video frames and generates textual summaries, which are then reduced to a single summary for the input video. The method is evaluated on several datasets, showing performance gains. Ablation studies are also provided. Some applications are also shown,"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The method improves the performance. Some application-oriented textual video summarization may benefit from the paper. Also, the applications of the method are interesting."
            },
            "weaknesses": {
                "value": "The idea of the paper, which is to use multiple LLMs to generate multiple video summaries and then integrate them into a single video summary, looks na\u00efve, and people can expect that they perhaps improve the performance. The technical contribution of the paper looks weak, and no interesting insights are provided. I would like to see more specific descriptions about why these results are worth sharing with the community.\n\nThe first point of the summary of the contributions, saying the method an generate single coherent, and unbiased summary, does not make much sense for me. What does it mean by a coherent and unbiased summary? As far as I understand, the experimental results do not say anything related to coherency and unbiasedness of generated summaries."
            },
            "questions": {
                "value": "I would like to see the responses to both of two points raised as the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a text summary generation method for videos by combining multiple fundamental models as a mixture of experts. The easy-to-understand flow is presented in Figure 2. A summary pool is generated by using multiple VideoLLMs. Then, the summaries are polished (outliers are filtered out) to generate a better summary. Some applications such as key frame extraction and abstracted key story generation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "+ The quantitative evaluation for text summary generation and keyframe prediction shows superior performance as compared to the subset of the proposed method.\n+ Some possible applications are demonstrated, which might be useful."
            },
            "weaknesses": {
                "value": "- Technical novelty is very weak.\n  - As can be seen in Section 3, this work is just a combination of existing fundamental models. The flow and how to combine them might be the main scope of this paper, but I could not see any novel technical contributions in this paper. The authors might want to clarify their technical novelty.\n  - No technical details are given in the manuscript. Some key components are presented such as \u201cfilter outliers\u201d and \u201ccooperate,\u201d but their technical details are all missing. The authors might want to show the equations or the pseudo code of them so that possible readers can grab the idea. It is almost impossible to reproduce the proposed method.\n\n- Experimental comparison is not good enough\n  - The proposed work is compared only with the subset of their proposed model: each LLM without MoE. It is apparent that the mixture model would outperform a single model. The author might want to design the experimental comparison better.\n  - In Section 4, only the numerical comparison is discussed. Since the task is the video summarization, the authors should show the generated summaries and discuss how they are qualitatively different (I see a set of examples in Figure 3, but this is just an example, no detailed discussion is given). \n  - In addition, the authors might want to discuss the failure cases and limitations.\n\nHere are some comments to improve the paper:\n- reference error in line 346"
            },
            "questions": {
                "value": "Please answer my comments in the Weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This submission focuses on the task of video summarization. Specifically, it integrates multiple VideoLLMs to generate comprehensive and cohrent textual summaries without additional fine-tuning. The proposed model also enhance the quality of downstream tasks that required rich semantic extraction and retrieval capabilities."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ This paper propose a Mixture of Experts inspired paradigm that jointly utilising multiple off-the-shelf VideoLLMs via LLM cooperation. The resulting system can be easily upgraded by incorporating more VideoLLMs.\n+ This paper proposes an inference-time algorithm to generate textual summaries, key frames identification, and visual summary generation."
            },
            "weaknesses": {
                "value": "- The first strategy in filter outliers is based on the assumption that most of the LLM will output correct and similar. The second strategy compare visual and textual embedding for correctness of the summary. While these are reasonable for simpler video, it may face problems if the input video is rich and highly diverse, and exists multiple events. The definition of \"good summary\" is highly depend on the quality of the selected VideoLLMs."
            },
            "questions": {
                "value": "The paper is well written and easy to follow. I believe it is a great engineering project that shows how off-the-shelf VideoLLMs can be leveraged with a inference-time algorithm to generate a single textual summaries. My main concern is the technical novelty of this submission. It appear to me that the paper only provides a way to combined existing models to produce a better summary. In other words, the results are benefitted mainly from the capability of pre-trained LLM and VideoLLMs. I would really like the authors to outline what is the novel technical contributions and/or insights generated from this approach that is beneficial to researcher in this field."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}