{
    "id": "QiUitwJDKI",
    "title": "InnateCoder: Learning Programmatic Options with Foundation Models",
    "abstract": "Outside of transfer learning settings, reinforcement learning agents start their learning process from a clean slate. As a result, such agents have to go through a slow process to learn even the most obvious skills required to solve a problem. In this paper, we present InnateCoder, a system that leverages human knowledge encoded in foundation models to provide programmatic policies that encode \"innate skills\" in the form of temporally extended actions, or options. In contrast to existing approaches to learning options, InnateCoder learns them from the general human knowledge encoded in foundation models in a zero-shot setting, and not from the knowledge the agent gains by interacting with the environment. Then, InnateCoder searches for a programmatic policy by combining the programs encoding these options into a larger and more complex program. We hypothesized that InnateCoder's scheme of learning and using options could improve the sampling efficiency of current methods for synthesizing programmatic policies. We evaluated our hypothesis in MicroRTS and Karel the Robot, two challenging domains. Empirical results support our hypothesis, since they show that InnateCoder is more sample efficient than versions of the system that do not use options or learn the options from experience. The policies InnateCoder learns are competitive and often outperform current state-of-the-art agents in both domains.",
    "keywords": [
        "programmatic policies",
        "reinforcement learning",
        "options"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We present a system that leverages human knowledge encoded in foundation models to provide programmatic policies that encode \"innate skills\" to agents in the form of temporally extended actions, or options.",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=QiUitwJDKI",
    "pdf_link": "https://openreview.net/pdf?id=QiUitwJDKI",
    "comments": [
        {
            "title": {
                "value": "Thank you"
            },
            "comment": {
                "value": "**Can you provide examples of the programs?**\n\n**Answer:** Thank you for suggesting this. Please see Appendix E (Figure 10) of the revised version for a representative example of a final policy and an example of a program a foundation model generates.\n\n**A discussion on epsilon is lacking.**\n\n**Answer:** We added a plot in Appendix G (Figure 12) of the revised version where we show how the performance of InnateCoder changes as we vary the value of epsilon. What we observe is that InnateCoder is robust to the value of epsilon, as it presents similar performance for a wide range of values. We also observe that values that are \u201ctoo small\u201d or \u201ctoo large\u201d perform worse than those that find a good balance between the syntax (discovering new programs) and semantics (using existing programs). \n\n**This is minor. The Sec. 3.1 introduces the definition of options. I think it might be better to introduce this concept informally before the formal definition.**\n\n**Answer:** We have added a few intuitive sentences in the beginning of Section 3.1.\n\n**Why are the results of Llama 3.1 + GPT-4o in Mayari worse than the results of Llama 3.1 or GPT-4o alone?**\n\n**Answer:** The combination of programs written by Llama 3.1 and GPT-4o does not lead to \u201cmonotonic improvements\u201d, as evidenced by the drop in performance against Mayari. This happens because none of the competition winners is constrained by the DSL we use in our experiments. As a result, the optimization done in self-play might not be specific for the opponents evaluated in Table 1, but to policies written in the DSL and encountered during the self-play process.\n\nWe added the above paragraph in Section 4.2 of the revised version. \n\n**What is the percentage of options adopted?**\n\n**Answer:** Great question! Following your suggestion, we added an example in Appendix F of the revised version where we show that options can cover a large number of lines in the policy that InnateCoder returns. Importantly, we also show that on average 63% of the options are used in a best response during self play. Please see Appendix F for details. \n\n**SHC looks quite nice, can you summarize it?**\n\n**Answer:** Yes, SHC is a strong baseline. The initial program is obtained by sampling the production rules of the grammar that defines the DSL. The structure of the underlying space is given by a neighborhood function, which essentially mutates the tree that represents a program. Please see Definition 1 and the two paragraphs that follow the definition for a more detailed explanation. \n\n**How about multi-modal information as input?**\n\n**Answer:** We only use text, but we believe the system could be improved by using multimodal inputs, as suggested. This is an exciting direction for future research."
            }
        },
        {
            "title": {
                "value": "Thank you"
            },
            "comment": {
                "value": "**Fair comparison in MicroRTS. Does InnateCoder have access to the policies it is evaluated against during training?**\n\n**Answer:** No, InnateCoder does not have access to the policies it is evaluated against during training. It can only see policies that are generated in the self play process. Once the self-play process is finished, we evaluate the generated policies against held-out policies. \nNote that the results in Figure 3 are not computed with respect to the winners of the competition. They are computed with respect to the last policy each of the following evaluated methods generate: FM, IC-GPT, IC-Llama, LISS-o, LISS-r, SHC. Each of these methods is trained in self-play and, importantly, they do not \u201cinteract\u201d with policies of other methods during training.  \nWe clarified the empirical design in the revised version by splitting the experiments paragraph into three parts: Efficiency Experiment (Figure 3), Competition Experiment (Table 1), and Size and Information Experiments (Figures 5 and 8). Please let us know if this split still does not clarify the empirical setup. \n\n**Could you add an explanation of 2L?**\n\n**Answer:** Thank you for the suggestion. Please see a brief description of 2L in Appendix M. \n\n**Figure 4 is pixelated.**\n\n**Answer:** We tried to improve the quality of the plots; please let us know what you think of them (Figure 4 of the revised version). \n\n**How well would Deep RL perform on Karel?**\n\n**Answer:** We evaluated the state-of-the-art DRL method for Karel, HPRL-PPO [1], in Appendix B.2 (Table 2). InnateCoder is never worse than HPRL-PPO and often substantially superior. \n\n[1] Guan-Ting Liu, En-Pei Hu, Pu-Jen Cheng, Hung-Yi Lee, and Shao-Hua Sun. Hierarchical programmatic reinforcement learning via learning to compose programs. In Proceedings of the International Conference on Machine Learning, 2023.\n\n**Can you elaborate on the exact training procedure? What is the experimental design?**\n\n**Answer:** InnateCoder uses a foundation model to generate a set of programs that are broken down into options and used to induce the language\u2019s underlying semantic space. IC then uses a self-play algorithm to produce one program for playing on that map. We repeat this process 30 times to generate 30 agents. \n\nIn Figure 3, for each of the 30 seeds, we evaluate the program a method generates after X games played against the last program the methods evaluated generate. That way we compute the values for different values of X (x-axis). Note this is the evaluation of the methods, not their training. During training they only have access to the programs generated through self-play. What you called reference policies are the policies the evaluated methods generate after they have exhausted their computational budget.\n\nAs for the evaluation against competition bots (Table 1), we randomly select 9 out of the 30 programs from the previous experiment and evaluate them against the 3 winners of the last competitions on the 4 maps we use in our experiments. Table 1 presents the average results of 2 matches for each map, for a total of 2 * 4 * 9 = 72 matches. It is remarkable that the policies InnateCoder generates, which were not tuned to defeat the competition agents, can have such a high average winning rate: larger than 50% in all three tests. \nNote that our initial description of how the numbers in Table 1 was computed was incorrect; this is now fixed in the revised version. Thank you for asking this question. \n\n**I'm looking forward to seeing this cleared up during the discussion, and if by the end of it I'm fully convinced that this method actually outperforms all other known alternatives on these benchmarks (mainly MicroRTS, but also Karel - although in the latter I suppose SOTA isn't as difficult as in a competitive game), I'd be happy to increase my rating.**\n\n**Answer:** We hope our answers above clarify your doubts, and that now you understand that InnateCoder presents SOTA results in both Karel and MicroRTS. We also hope the new organization of the empirical section will make it easier for other readers to understand our experimental design. Please let us know if you have any other questions and/or suggestions."
            }
        },
        {
            "title": {
                "value": "Thank you"
            },
            "comment": {
                "value": "**Creating a DSL is costly, why not learn a policy that outputs programs?**\n\n**Answer:** We recognize that writing a DSL from scratch is costly, but it is a cost that occurs only once. After a DSL is created, it can be used to solve as many problems as needed, which amortizes the cost of creating it. Take for example the DSL used in Microsoft Excel's FlashFill. It was designed once and it is used daily by millions of people around the world. By contrast, the use of an LLM to generate the programs and effectively guide the search must be paid for every problem one wants to solve. Having said that, the idea of training a model to output programs based on the game history is also interesting and we hope the community will investigate it in future works. \n\n**Why is ML needed for this setting?**\n\n**Answer:** Recently, a software engineer from a giant Computer Games company personally reported to us that they can spend months writing programs encoding agent behavior in video games. Our methods could be used to automate or semi-automate the process of writing these programs, potentially saving hundreds of hours of professional programmers. \nMoreover, previous work compared the programs professional programmers wrote with those one of the baselines we used in our experiments wrote (SHC) [1]. They showed that a version of SHC can write stronger programs than the human programmers enlisted for their experiment. \n\n[1] Programmatic Strategies for Real-Time Strategy Games. Julian Mari\u00f1o, Rubens Moraes, Tassiana Oliveira, Claudio Toledo, and Levi Lelis. In the Proceedings of the Conference on Artificial Intelligence (AAAI), 2021.\n\n**I am willing to increase my score if I can get a better sense of the value add of using machine learning here vs. just hand-crafting policies.**\n\n**Answer:** Thank you for giving us the chance to explain the importance of machine learning in generating programmatic policies. Please do not hesitate to ask follow-up questions. \n\n**How much prompt engineering was needed?**\n\n**Answer:** All of our prompt engineering work was to ensure that the LLM could produce programs in a language that it was not trained on, and we did not spend more than a couple of days writing the prompts. Note that there are other alternatives to prompt engineering that could produce similar or even better results such as fine-tuning and token masking (e.g., https://blog.dottxt.co/coding-for-structured-generation.html). Our work shows what is possible in terms of generating programmatic options, and there are many ways of improving our system. These are interesting directions for future research.  \n\n**Can you provide examples of programs?**\n\n**Answer:** Thank you for suggesting this. Please see Appendix E (Figure 10) of the revised version for a representative example of a final policy and an example of a program a foundation model generates. We also explain parts of the example in Appendix E, to demonstrate that the programs generated can be fairly complex.  \n\n**Which type of search is more important between syntax and semantic?**\n\n**Answer:** The two searches are equally important and must be used together. The search in the syntax space allows you to find novel programs, while the search in the semantic space allows you to combine existing programs. We added a plot conveying this intuition in Appendix G (Figure 12) of the revised version."
            }
        },
        {
            "title": {
                "value": "Thank you"
            },
            "comment": {
                "value": "**Issues with the presentation**\n\n**Answer:** Thank you for raising concerns about the presentation. We have removed the discussion around DSLs in the problem definition section, as it was redundant with the discussion we had in the introduction. We also replaced the abstract example with a simplified version of the DSL we use with Karel. Please see the chances in blue in the revised version (Figure 1 and Section 2) and let us know if you would like to see further changes.\n\n**Differences between InnateCoder and LISS**\n\n**Answer:** LISS can only be used in a transfer learning setting, where the agent learns by solving multiple environments. InnateCoder allows us to induce semantic spaces in a zero-shot setting. In addition to the source of options, we connect semantic spaces with the option framework in RL. This is interesting because it shows a novel way of using options that, for the first time, can benefit from thousands of options.\n\n**Concerns with the dependency on DSLs**\n\n**Answer:** The dependency on DSLs is a weakness of program synthesis approaches at large. As these methods improve, they will use general-purpose languages, thus bypassing the need of a DSL. If synthesizers were good enough to use general-purpose languages, InnateCoder could be used as is. In fact, due to current LLMs\u2019 ability to write programs in general-purpose languages, InnateCoder's performance might improve once we consider general-purpose languages. \n\n**Show examples of programs**\n\n**Answer:** Thank you for suggesting this. Please see Appendix E (Figure 10) of the revised version for a representative example of a final policy and an example of a program a foundation model generates. \n\n**Are InnateCoder and baselines deterministic?**\n\n**Answer:** Yes, all programs written in the DSLs used in our experiments are deterministic. This is true for all baseline, including the Deep RL baselines (RAISocketAI in Table 1, DRL in Figures 6 and 7, and  HPRL-PPO in Table 2). \n\n**How are the numbers in Table 1 produced?** \n**Answer:** Thank you for asking this question. It made us notice that our description of how the numbers are computed for Table 1 was incorrect. This is fixed in the revised version of the paper (see the paragraph \u201cCompetition Experiment\u201d in Section 4). \n\nHere is how the computation is done. We randomly select 9 out of the 30 programs generated in the \u201cEfficiency Experiment\u201d and each of these programs plays 2 matches against the 3 winners of the last competitions. Since we use 4 maps, we have a total of 4 * 2 * 9 = 72 matches. We present the average winning rate against each opponent. Recall that the winning rate for each match can be 0, 0.5 or 1.0, for loss, draw, and victory. This experiment measures the average winning rate of the programs InnateCoder generates against the best-known agents for MicroRTS. \n\n**Why is there a gap in performance between <=1400 and >=5000?**\n\n**Answer:** The gap in performance is due to the increased chance of sampling helpful options with larger sets. We added in Appendix F of the revised version an example that illustrates this. In the example, we show a program that uses several options that are present in the set of 5000 of a run of InnateCoder, but not in the set of 1400, of another run of the algorithm.  \n\n**What is the difference between E and V^n?**\n\n**Answer:** E is defined as V^n(s0). The reason why we wrote it this way was to differentiate the search from the evaluation of our application domain. We would be happy to use V^n(s0) instead of E, if the reviewer believes that it would improve the readability of the paper. \n\n**Figure 1 is after Figure 2.**\n**Answer:** This was fixed in the revised version, as we modified Figure 1 following your suggestion. \n\n**3.2.1 is the only subsection under 3.2.**\n**Answer:** We removed that heading in the revised paper. \n\n**L208 \"starting at the non-terminal __ the node represents\".**\n**Answer:** Added the word \u201csymbol\u201d. \n\n**L230 \"that is is\".**\n**Answer:** Fixed."
            }
        },
        {
            "title": {
                "value": "Summary of Revisions"
            },
            "comment": {
                "value": "We would like to thank all reviewers for taking the time and providing invaluable feedback on our manuscript. We have edited the paper to deal with all suggestions and comments to the best of our abilities. We believe that the paper has improved substantially after the changes. We have uploaded a revised version of our manuscript where the edits are highlighted in blue. Please do not hesitate to ask any further questions, we would be happy to promptly answer them. \n\nHere is a summary of the changes we made. \n\n1. Changed Figure 1 to be a concrete example based on Karel the Robot (**Reviewer LYYo**).\n2. Added an explanation in the second paragraph of Appendix F for the divide between versions of InnateCoder that use 1400 or fewer options and versions that use 5000 or more (**Reviewer LYYo**).\n3. Provided in Appendix E (Figure 10) examples of the programs that InnateCoder generates (**Reviewers LYYo, gPM1, qHn7**).\n4. Included in Appendix G (Figure 12) a study of how InnateCoder behaves for different values of epsilon (**Reviewers gPM1 and qHn7**).\n5. Fixed the explanation of how the numbers in Table 1 are computed (**Reviewer nF6R**).\n6. Explained in Section 4.2 why InnateCoder with a library generated with Llama 3.1 + GPT-4o performs worse against Mayari (**Reviewer qHn7**).\n7. Added in Appendix F an example showing which parts of the final policy are composed of options from the initial library, and how often policies are used in training (**Reviewer qHn7**).\n8. Answered all questions the reviewers asked (please see individual comments below). \n\nWe have moved parts of the Related Work section to Appendix A, so we could fit all the changes requested in the main paper. Please let us know if you feel strongly about this change."
            }
        },
        {
            "summary": {
                "value": "InnateCoder uses foundations models to generate domain-specific language candidates (options), and then use stochastic hill-climbing in the induced syntax / semantics space to find a best-performing agent."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Writing overall is fine, although I have some small complaints which I'll mention in the weakness section.\n- I believe this method is novel to my knowledge.\n- InnateCoder was evaluated in two domains used in previous methods, showing nontrivial improvement over baselines in a number of tasks while matching performances in the rest. \n- I appreciate the authors' effort to eliminate data leakage as a factor during evaluation."
            },
            "weaknesses": {
                "value": "- I find the presentation of this paper to be a little confusing for someone not familiar with the prior work. For example, in section 2, the authors start to talk about the pros and cons of DSL before giving an introduction or even a definition of DSL. I also think using the actual language (or a simplified version) in one of the tested domains, instead of the generic \"if b then c\" or \"c1\" \"c2\" as the example could be more helpful.\n- Could the authors specify what are the exact differences between InnateCoder and LISS? Is the option source the only difference?\n- I have concerns that application of this method could be limited. The DSL-dependency demands all discrete actions with a hand-crafted grammar.\n- I would suggest showing some examples of the foundation model / final policy (can put those in the appendix).\n\nMinor issues:\n\n- Figure 1 is after Figure 2.\n- 3.2.1 is the only subsection under 3.2.\n- L208 \"starting at the non-terminal __ the node represents\".\n- L230 \"that is is\"."
            },
            "questions": {
                "value": "- The policies produced by InnateCoder seem to be deterministic. Is this the case for other baselines? How are the numbers in Table 1 produced? Are they calculated (by enumerating through all initial positions) or simulated (for a certain number of games and taking the average)? If the latter, how many games did you run to get the numbers?\n- Why is there a clear difference between agents with $\\leq 1400$ options and with $\\geq 5000$ options? Agents with $\\geq 5000$ options have sharp performance increases in the early stage of learning while agents with $\\leq 1400$ options don't and can even have performance drops. Despite having such a distinction between the two groups, the curves within the two groups look similar. What factors contribute to this cut-off?\n- L210 \"$\\mathcal{E}$ is an approximation of ..., ${\\hat{V}^n(s_0)}$\". What is the difference between $\\mathcal{E}$ and ${\\hat{V}^n}$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Authors introduce a new way to learn programmatic options.  The set of options is built by first sampling a set of programs from a foundation model that is given a description of the MDP and the domain-specific language (DSL).  Then the set of programs is improved by searching through sets of modified programs to find ones with high reward.  Results show that this approach outperforms (i) other programmatic search algorithms that do not use foundation models and (ii) Deep RL approaches in the games MicroRTS and Karel the Robot."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Authors do a nice job of explaining the algorithm.\n- Authors also did a nice job of providing the DSL for each environment and the exact prompts used."
            },
            "weaknesses": {
                "value": "The main weakness here is that the contribution seems to be small.  It seems likely that the better use of a LLM is to learn a policy that outputs short programs conditioned on the history of the game that has occurred.  This way the foundation model can adjust its output as it gets more information on how the game works.  The reasoning provided for not testing this is that this would be expensive.  But if you were to apply this algorithm from scratch to a new environment, it would already be costly to come up with a sufficiently expressive DSL.\n\nIt is also unclear to me why machine learning is needing for settings like these.  The hand-engineered action space of (non-terminals, terminals, etc.) is already small, so regular software engineering should often be able to learn the sequence of these actions.  \n\nIn addition, although it was helpful to see the exact prompts, the authors could be more transparent of how much prompt engineering was needed.  The exact \"Program writing guidelines\" and the \"list of tasks\" provided make it seem like there may have been quite a bit of prompt tuning was needed."
            },
            "questions": {
                "value": "1.  Can you discuss the types of prompt engineering that was required?\n2.  Can you provide some examples of what the final programs look like in these domains?  It is difficult to know how complex these domains are.\n3.  Is there any results on what type of search is more important between syntax and semantic search?\n\nI am willing to increase my score if I can get a better sense of the value add of using machine learning here vs. just hand-crafting policies."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work introduces a new approach for learning policies for reinforcement learning tasks, by using the knowledge from pretrained LLMs. Instead of learning a NN policy, as is common in regular DRL, InnateCoder generates a number of options, each represented as a program in a custom DSL. These are then combined into a policy, with the exact combination being obtained via stochastic hill climbing during the training process."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "This paper works on a very interesting direction of using pretrained LLMs to synthesize agent policies in RL tasks. This is important, as we still lack any meaningful foundation models for agents (here understood as action-taking systems), and this seems to be one good way of using the \"general knowledge machines\" for some agentic behavior. \n\nThe results obtained with this approach seem very good, and claim SOTA - I'm not sufficiently familiar with this specific domain to evaluate this claim, but it seems reasonable as it's compared to winners from public competitions."
            },
            "weaknesses": {
                "value": "My only main concern is about the fairness of comparison in the MicroRTS evaluation, possibly due to not understanding this part of the paper. In Figure 3, you plot the winrate. The winrate is defined on line 341 (somewhat confusingly under \"Other specifications\") as \"The winning rate of a policy is computed for a set of opponent policies [...]\" - what policies? Is it equally sampled between COAC, Mayari and RAISocketAI?\n\nMore importantly, over the course of the training in Figure 3 (the one that goes up to 1e5 games), is each method exclusively trained on self-play, and then checkpoints are \"separately\" evaluated against reference opponent policies (COAC, Mayari, RAISocketAI presumably)? Or do they interact with those policies throughout the training process?\n\nGenerally speaking, the writing is at times difficult to follow, possibly because I'm not that familiar with this specific subfield. For example, in line 283, \"2L\" is introduced as the algorithm used to train the policies, without much further elaboration or expanding the acronym. There is, of course, a reference to the paper that introduced it, but seeing as it's not a widely known algorithm, it might be useful to include some outline of how it works.\n\nAs a half-nitpick: the pixelated aesthetic of Figure 4 makes it rather difficult to tell what's going on. Consider for example FourCorners, where I can see something happening early on in the training, but mostly it's just straight converged lines. Or OneStroke, where it's just the straight converged lines. I understand it's difficult to present so much information in a finite amount of space, but perhaps there's some other way to make it work?\n\nFurthermore, I'd be curious to see how well regular DRL algorithms would perform on Karel as a baseline - it does not seem to be an overwhelmingly difficult task, though I have not used it personally."
            },
            "questions": {
                "value": "Can you elaborate on the exact training procedure? Specifically - when training e.g. IC-Llama, what exactly is the workflow of the policy being improved, the round-robin tournament between 30 seeds, what data is used to improve the policy, and how/when it's evaluated? \n\nMy only real doubt is regarding the fairness of training IC against fixed versions of opponent policies. Also, fairness of the evaluations - how do the winrates differ against different reference policies? (for example, it could be the case that IC does really well against weaker opponents, but still mostly loses to the strongest policy, but all of this gets averaged to a high winrate on the graph - I don't claim that this is happening, but I'm not confident that it's not happening given the data in the paper)\n\nI'm looking forward to seeing this cleared up during the discussion, and if by the end of it I'm fully convinced that this method actually outperforms all other known alternatives on these benchmarks (mainly MicroRTS, but also Karel - although in the latter I suppose SOTA isn't as difficult as in a competitive game), I'd be happy to increase my rating."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel framework called InnateCoder which leverages foundation models (FM) and stochastic hill-climbing (SHC) to search programmatic policies to solve various RL problems. InnateCoder encode general human knowledge from foundation models in a zero-shot setting to formulate programmactic policies, while these policies interact with RL environment using options as \"innate skills\". The framework works as follows: given the environment description and its corresponding domain specific language (DSL), a prompt can be built to query the foundation models to generate programmatic policies. Although the generated programs are unlikely to solve the problem directly, they provide important corpus of options. By tweaking the syntax and semantics of these options, new options and programs are obtained for the framework to evaluate and search via SHC. The search stops when run out of budgets or the optimal solution is found."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Presents a novel method called InnateCoder which leverages foundation models to provide corpus of knowledge instead of trying to solve the problem directly. Authors claim that the generated programmatic policies can be used as the starting point of option and program searching, and InnateCoder should extend the searching by extending to and visiting neighborhood programs. This method leverages the foundation models' great ability on higher-level abstraction and left the lower-level operations to options. The overall method is simple and very intuitive.\n- Performes extensive evaluations on different benchmarks. The baselines are properly selected. The proposed framework is highly competitive in nearly all evaluated benchmarks.\n- Related disccusion is extensive and informative. The discussion is helful and meaningful for researchers in related community."
            },
            "weaknesses": {
                "value": "- The main text does not provide a single complete example programmatic policies or an option for a single environment, it could be helpful to add at least one for better readability. \n- The discussion on $\\epsilon$ in SHC is lacking. In Sec. 4.1 line 371, it says the results shows the semantic space can be less conducive to search than the syntax space, depinding on the quality of the options used to induce it. If so, modifying the $\\epsilon$ may improve the results.\n- This is minor. The Sec. 3.1 introduces the definition of options. I think it might be better to introduce this concept informally before the formal definition."
            },
            "questions": {
                "value": "- In Sec. 4.2, why the results of Llama 3.1 + GPT-40 in Mayari is worse than the results of Llama 3.1 or GPT-4o alone?\n- I understand how does the filtering options work in  Sec. 3.2.1 in general. However, could you demonstrate some specific numbers if possible?  For example, people would like to see the percentage of adopted options for each environment.\n- The SHC looks very nice in many domains. Could you briefly summarize how does it work, e.g., how does SHC accquire the inital programmatic component? By enumeration of the DSL?\n- I did not read the prompt line by line, so sorry if I miss anything. When you use prompts to query the programmatic policies, do you provide any multi-modal information other than plain text, such as images or animations? If not, do you think it is possible to improve the quality of the generated programs and options?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}