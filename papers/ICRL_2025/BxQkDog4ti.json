{
    "id": "BxQkDog4ti",
    "title": "Don't Cut Corners: Exact Conditions for Modularity in Biologically Inspired Representations",
    "abstract": "Why do biological and artificial neurons sometimes modularise, each encoding a single meaningful variable, and sometimes entangle their representation of many variables? \nIn this work, we develop a theory of when biologically inspired networks---those that are nonnegative and energy efficient---modularise their representation of source variables (sources). \nWe derive necessary and sufficient conditions on a sample of sources that determine whether the neurons in an optimal biologically-inspired linear autoencoder modularise. Our theory applies to any dataset extending far beyond the case of statistical independence studied in previous work. Rather we show that sources modularise if their support is \"sufficiently spread''.\nFrom this theory, we extract and validate predictions in a variety of empirical studies on how data distribution affects modularisation in nonlinear feedforward and recurrent neural networks trained on supervised and unsupervised tasks. Furthermore, we apply these ideas to neuroscience data. First, we explain why two studies that recorded prefrontal activity in working memory tasks conflict on whether memories are encoded in orthogonal subspaces: the support of the sources differed due to a critical discrepancy in experimental protocol. And second, we study spatial and reward information mixing in entorhinal recordings, and show our theory matches data better than previous work. In sum, our theory prescribes precise conditions on when neural activities modularise, providing tools for inducing and elucidating modular representations in brains and machines.",
    "keywords": [
        "neuroscience",
        "representation learning",
        "disentanglement",
        "modularisation",
        "neural networks",
        "hippocampus",
        "cortex"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "We derive necessary and sufficient conditions on task structure under which nonnegativity and energy efficiency lead to modularised representations, and use this to understand the behaviour of artificial neural networks and biological neurons.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=BxQkDog4ti",
    "pdf_link": "https://openreview.net/pdf?id=BxQkDog4ti",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates why neural representations in biologically inspired networks sometimes form modular structures, where each neuron encodes a single variable, and other times create mixed-selective representations. The authors develop a theory that predicts when modularization will occur in networks optimized for energy efficiency with nonnegative firing rates. They derive necessary and sufficient conditions for modularity based on the spread of source variables. The theory is validated in both linear and nonlinear networks. The theory provides a cohesive explanation for the conflicting findings in the prefrontal cortex and entorhinal cortex data from neuroscience studies."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper introduces a novel theory that precisely predicts necessary and sufficient conditions for modular representations in biologically inspired networks, extending previous work beyond statistical dependencies.\n\n2. The mathematical formulation is rigorously derived, and validated across various neural network architectures and experiments.\n\n3. The theory provides explanations for conflicting neuroscience findings and has close links to biologically plausible architectures and brain representations.\n\n4. The paper provides a cohesive theory for understanding modularity in neural representations, with implications for both interpreting biological neural data and guiding the design of artificial neural networks for better interpretability and efficiency.\n\n5. The paper is well-written, presenting complex theoretical concepts with clarity and intuition."
            },
            "weaknesses": {
                "value": "1. The experiments use nonnegative activities in neural networks, which aligns with biological plausibility, but it would be valuable to discuss inhibitory neurons in the brain and how inhibition might relate to the theory and findings.\n\n2. While the L2 norm of firing rates and weights is a reasonable approximation for biological energy, other biological constraints (e.g., sparse connectivity, synaptic range, anatomical structure, and decoding flexibility) may also play a role.\n\n3. It\u2019s unclear how the theory would extend to more complex datasets. For example, what would the different conditions/variations in source variables mean in naturalistic data (e.g., natural images, audio, text)? How might we approximate \"spread\", and quantify modularity conditions in such stimuli?\n\n4. The discrepancies in prefrontal working memory modeling and the brain data could have further explanations, particularly why some neurons tune to both colors despite orthogonal encoding and why exact subspace angles were not obtained."
            },
            "questions": {
                "value": "1. Beyond prefrontal working memory and the entorhinal cortex, does the theory generalize to other modular representations in the brain?\n\n2. If decoding flexibility were considered as a biological constraint, how might it impact the theory and its predictions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the conditions under which modularity should arise in optimal representations. The authors developed a mathematical theory of when a modular representation should be favored in a linear autoencoder. They found that modularity should appear when the support of the sources is \u201csufficiently spread\u201d. The paper also presented simulation results to show that some of theoretical results may be generalizable to non-linear problems. The later sections of the paper applied these theoretical ideas to explain some experimental observations from neurophysiological experiments in cognitive tasks. \nThe paper makes several interesting points about when modularity should be favored, and applies the theory to several examples."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality: The theoretical part of the work builds up prior work by Whittington et al, 2023 and other studies. Previous work by Whittington et al, 2023 assumed mutual independence of the sources. In the current work, the authors show that, with several additional assumptions, \u201csufficient spread\u201d of the factors of variation can also lead to modular representation. The theory has some new elements, although it is a bit incremental. The application to the several neuroscience problems seems to be new.\n\nQuality: The paper considered both linear and nonlinear cases. This is a strength. For the former, analytical results were provided. For the latter, some preliminary numerical results were given. \nThe paper also considered several neuroscience applications. This may also be seen as a strength.\n\n\nClarity: The overall structure of the paper is clear. Some intuitions behind the theory were provided.\n\n\nSignificance: The question of when modularity arises in optimal representation is an interesting one and we still lack a clear understanding. This work made a few interesting points on this problem."
            },
            "weaknesses": {
                "value": "The writing needs improvements throughout the paper. In particular, the description of the theory can be substantially improved. For example, Theorem 2.1 should be made more accessible. \n\n\nWhile several applications are attempted, each application appears to be preliminary. If the model predictions and experimental tests can be made more rigorous, that would strengthen the paper.\n\nIn Section 5, there are some qualitative differences between the model predictions and the data. As the paper pointed out, Panichello and Buschman (2021) showed that a substantial fraction of the neurons were tuned to both colors, contradicting a key prediction of the model. This seems to be a more important feature of the data compared to the issue of orthogonality v.s. non-orthogonality.\n\nLooking at the math, the theory appears to only work for scalar variables. Can it be applied to circular variables? If the answer is no, the applications to real data would be questionable. In section 5, color is sampled from a color-wheel. \n\nBased on the way the theory is written, the results seem to rely the assumption that the energy (or cost) is a quadratic function of neural activity. If the cost scales linearly with neural activity, would the theoretical results change fundamentally? Assuming a linear scaling could make sense biologically, as the metabolic cost may scale linearly with the number of spikes. \n\nRelevant earlier theoretical literature on grid cell modularity was not cited/discussed (e.g., by Fiete/Burak et al, and Wei/Prentice/Balasubramanian).\n\n\nIt is difficult to understand what is really going on in Fig. 1. Fig.1 may come out too early and the results need to be better unpacked. \n\n\nThe theory seems to ignore biologically-relevant noise."
            },
            "questions": {
                "value": "What are the assumptions about the noise in the system being studied? How does noise affect the theoretical results?\n\nIn section 5, the authors seems to be equating orthogonality with modularity. Am I understanding this correctly? If this is the case, can the authors unpack the idea?\n\nCan they authors unpack the results in Fig. 5c? Was that an actual simulation or just a schematic?\n\nThe definition of the matrix in Eq. 10 is unclear. Please clarify. \n\n\nCan the authors explain what the first part of the title mean?\n\nIn the abstract, it is stated that \u201cFrom this theory, we extract and validate predictions\u2026\u201d What does \u201cextract\u201d predictions mean? [minor point]"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper seeks to explain why and when a population of biological or artificial neurons sometimes modularise and sometimes entangle the representation of source variables. This is a fundamental question that is highly relevant to both neuroscience and AI. The authors propose and prove a new theory emphasising the importance of the shape of empirical data distribution in extreme regions in dictating whether neurons are mixed selective or modular. Specifically if the sources to be represented are supported in all extreme regions, the neurons modularise. The application of this theory outside of linear autoencoders is tested in feed forward and recurrent neural networks, including experiments that provide explanations for discrepancies in previous neuroscience literature."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This work is original, of high quality and undoubtedly contributes to the community\u2019s understanding of neural modularisation. The nonlinear verification of theory and additional application to neuroscience results are significant for the field and a strength of the paper. The submission is well written and clear throughout, although its clarity suffers somewhat due to the amount this submission seeks to cover."
            },
            "weaknesses": {
                "value": "- In my opinion this submission contains too much, and would benefit from more focus and time spent on fewer experiments. The appendix is already large but some experiments could be moved there.  \n- Figure text and panels are too small throughout.\n- It is not clear how relevant encoding of the extreme points of source distributions are for computation / cognition. I.e. neurons do not just autoencode.\n- The bio description of energy minimisation assumes l2 penalty is an appropriate penalisation function for modelling biology. This is a fair starting assumption, but no argument is presented about how this maps to the costs biological neurons will be seeking to minimise."
            },
            "questions": {
                "value": "- Biologically inspired linear rnns are repeatedly mentioned. Linear rnns are less like biological circuits (which are nonlinear).  Is the biologically inspired term referring to the energy costs?\n150: satisfied for all w is a little unclear. The proof could  \n- Autoencoding seems limited as an objective for theory, in that brains perform computations over inputs and states and produce behaviour. Is it not the relevance of representing the extreme points for behaviour that is important? \n- Section 2.2 assumes positive weights?\n- 120 \"be better\" is imprecise \n- Fig 1d, neuron\u2019s angle isn\u2019t described? \n- I don\u2019t understand the what where regression justification. \n- How relevant are these results for more typical ANN experiments?  E.g classic image benchmarks or language modelling tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}