{
    "id": "4mni4W1ZXy",
    "title": "Regularity explains emergence",
    "abstract": "We investigate the mechanisms behind emergence in large language models from the viewpoint of the regularity of the optimal response function $f^*$ on the space of prompt tokens. Based on theoretical justification, we provide an interpretation that the derivatives of $f^*$ are in general unbounded and the model gives up reasoning in regions where the derivatives are large. In such regions, instead of predicting $f^*$, the model predicts a smoothified version obtained via an averaging operator. The threshold on the norm of derivatives for regions that are given up increases together with the number of parameters $N$, causing emergence. The relation between regularity and emergence is supported by experiments on arithmetic tasks such as multiplication and summation and other tasks. Our interpretation also shed light on why fine-tuning and Chain-of-Thought can significantly improves LLM performance.",
    "keywords": [
        "large language model",
        "emergence ability",
        "approximation",
        "scaling law",
        "regularity"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4mni4W1ZXy",
    "pdf_link": "https://openreview.net/pdf?id=4mni4W1ZXy",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates the concept of \"emergent abilities\" in large language models (LLMs) by developing a theoretical framework based on the regularity (or smoothness) of the optimal response function. The authors suggest that LLMs approximate this response function by smoothing out regions with high derivative values, leading to approximation errors that gradually decrease as the model size, N, grows. The theory proposes that as N increases, the model can capture more complex aspects of the response function without the need for smoothing, which results in sudden improvements or \"emergence\" of new abilities. The authors present a key theorem that quantifies the relationship between model size and approximation quality. They also provide experimental evidence to support the theory, including function approximation with ResNets and arithmetic tasks to demonstrate the model\u2019s behavior in regions with high derivatives."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- I appreciated the theoretical framework based on Siegel & Xu (2020), which links the regularity of optimal response functions with the concept of emergence. This framework offers a fresh perspective on the phenomenon of \"emergent abilities\" in large language models (LLMs).\n\n- The main theorem effectively illustrates how model size relates to approximation quality, especially in regions where the optimal response function shows complex behavior. Although primarily qualitative, this theoretical foundation provides valuable insights into why larger models may perform better with irregular functions.\n\n- I found some of the empirical results intriguing, particularly the scaling experiments with Qwen models that revealed various trends in arithmetic calculation outcomes."
            },
            "weaknesses": {
                "value": "- Although the paper provides a unique and intuitive perspective on the mechanisms underlying emergence, it doesn\u2019t specify a precise threshold or clear scaling rule to predict when this emergence occurs. I would appreciate it if the authors could better highlight exactly what constitutes the list of \"quantitative/concrete\" predictions proposed by the theory.\n\n- The toy model experiments using ResNet don\u2019t closely match the large language models (LLMs) setup. This setup is qualitatively different from the autoregressive transformers typically used in LLMs. While the authors argue that the theory applies to any model type, this actually highlights a limitation of the theory rather than supporting the use of ResNets to examine phenomena observed in LLMs.\n\n- The choice of arithmetic tasks doesn\u2019t clearly connect to the theory\u2019s focus on changes in derivatives, as the observed U-shaped trend can be explained solely by the task structure. Choosing tasks more closely aligned with the theory would make the paper\u2019s ideas clearer and more applicable.\n\n- Overall, while I find the results in some parts of the paper interesting, they often appear disconnected, lacking a clear and logical progression.\n\n- Presentation should be improved. In particular, it would greatly help if captions contained the necessary information to understand the content beyond what is provided in the existing title headers."
            },
            "questions": {
                "value": "- Can the theory predict specific model sizes or conditions where emergent behavior happens? Is there a certain size, N, where the model shifts from smoothing f\u2217 to accurately capturing it in areas with sharp changes?\n\n- Could you design an experiment with an autoregressive transformer model that would produce results more relevant to the theory?\n\n- Can the theory\u2019s error bounds predict error rates for specific tasks at different model sizes?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Paper introduces the idea that LLMs and other machine learning display a smoothing behavior in regions of input space where derivative of model output with respect to input is large. The behavior is said to emerge in specific regions of parameter space where training data has a \u201clarge derivative\u201d in such regions of input space the result is that the network learns a \u201csmoothed version\u201d of the input output map rather than the map itself. The \nclaim is that the averaging behavior scales with parameters number and can yield to \u201cemergence\u201d-- where performance of model jumps on specific tasks as a function of parameter number. The authors introduce and prove a theorem which states that when a model, neural network map, cannot meet a performance standard within epsilon, then the model will learn an averaged version of the training data. The paper then provides numerical experiments with ResNet for fitting a trigonometric function and then uses the Qwen chat model for some analysis of algebraic operations."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "I found the central claim interesting but preliminary for several reasons. Theoretical insight into how computations in language models can achieve zero shot task behavioral changes\u2013 for example\u2013 sorting a last in ascending vs descending order based on small changes in prompt are interesting. The idea that behavior on such tasks is influenced by the magnitude of local derivative of output on training data leading to learning of an averaged function are interesting - -although it isnt clear how the smoothed function can perform computations insight clear."
            },
            "weaknesses": {
                "value": "Technically, I find the notion of derivative in token space to be problematic. I have worked on similar problems in the case of CNNs where the notion of the derivative is well defined because inputs can be taken to be over the real numbers. \n\nThe problem with prompts is that tokenization causes the input domain for networks to be discrete valued (say integer valued), and the nature of the derivative on such input spaces is more more subtle. How is the derivative to be defined on such spaces? The problem is that the local behavior of a derivative taken on Z embedded into R is not representative of the notion that the authors seek\u2013 which is a function that measured changes on input instances. \n\nTherefore, I would like to see a much more rigorous development of the main theorem with specific definition and analysis of the derivative for token valued functions which are the main object of study for LLMs. \n\n\nSecond, the numerical experiments in the paper are very limited\u2013 the title of the paper is about language models, but the first experiment is on ResNet. \n\nThe language model experiment is limited and I do not see a global investigation of this notion of the network derivative in different regions of parameter space and the input-output function f or the \u201csmoothed version S*f. \n\nCan the authors systematically evaluate the derivative and inferred the smoothed input-output function on a more general class of language models? \n\nTo solidify their central claim, can the authors analyze models of increasing size showing convergence to their central claim with model size?"
            },
            "questions": {
                "value": "How do the authors define the derivative over token valued neural networks? \n\nCan the authors systematically evaluate the derivative and inferred the smoothed input-output function on a more general class of language models? \n\nTo solidify their central claim, can the authors analyze models of increasing size showing convergence to their central claim with model size?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an explanation for the mechanism behind emergent capabilities in large language models through the regularity of the optimal response function. The authors claim that models do not model the optimal response in regions where derivatives are large, instead opting to predict a smoother function obtained through averaging values. They justify this theoretically and have accompanying experimental results on a synthetic function and certain arithmetic tasks (multiplication, sequence of single-digit addition, and addition word problems), where some intuitions from their theory are reflected in the accuracy trends of Qwen models as the number of parameters scale."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The theory is presented clearly, and the perspective of parameter size controlling the threshold on the extent to which the model predicts an irregular optimal response function is an interesting idea. The experimental setups are clear, and the synthetic setup is particularly compelling."
            },
            "weaknesses": {
                "value": "- While the theory seems sound and the synthetic experiment is compelling, I still reserve some skepticism for the connection to the LLM experiments on arithmetic tasks. Particularly I believe the title of this paper \u201cRegularity Explains Emergence\u201d is very strong and has a high burden of proof especially given the numerous natural language tasks where emergence has been observed [1] and the extensive discussions around the empirical validity of emergence in the existing literature (eg. [2]). \n- To expand on this point, I currently can\u2019t disentangle whether the theory provided by the authors truly gives an explanation for emergent capabilities in LLMs as they claim, or it provides one instance where emergence can occur and one can frame a theoretical narrative around. For the arithmetic tasks, while I can see that there can be conclusions drawn from the approximations of the gradient vector that are reflected in the accuracy trends across model scales and quantities like digit position and number of summands, I\u2019m not convinced this is a result that we wouldn\u2019t already expect intuitively and is necessarily explained from the theoretical results. The causal connection is not strong, likely due to the limitations of the theory and how it cannot explain more nuanced trends in eg. model scale (please see Questions below for expansion on this point).\n- In conclusion, I believe that the authors need to be more clear about the scope of their theory and the tasks considered in this work, or provide stronger connections between the observed emergence and the regularity of the optimal function. Are there examples of natural language tasks where the theory may predict a regular optimal response function and we do see linear improvements in the task across scale?\n\n- As a minor comment, there are areas in the paper where the writing has some typos and grammatical errors; I\u2019ve listed several below but I\u2019d like to ask the authors to go over their exposition and address some of the writing.\n\nLine 19: improves -> improve\n\nLine 44: \\citet instead of \\citep\n\nLine 47: task -> tasks\n\nLine 53: (Theorem 2.5 ->  (Theorem 2.5)\n\nLine 56: avilable -> available\n\nLine 58: LLM model -> LLMs\n\nLine 61: method -> methods\n\nLine 282: and R-value function -> an R-value function\n\nLine 391: Figures 6-6 -> Figures 7-8\n\nLine 391: despite of -> despite\n\nLine 482-483: \u201cOn the other hand\u2026\u201d sentence needs rephrasing\n\n[1] Srivastava, Aarohi, et al. \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.\" arXiv preprint arXiv:2206.04615 (2022).\n[2] Schaeffer, Rylan, Brando Miranda, and Sanmi Koyejo. \"Are emergent abilities of large language models a mirage?.\" Advances in Neural Information Processing Systems 36 (2024)."
            },
            "questions": {
                "value": "1. Do you have any insights about how the relation between the number of parameters N and the optimal \\epsilon(N) is reflected in the accuracy plots for the arithmetic tasks? For instance, it seems that the threshold \u2018saturates\u2019, in the sense that for 32B-110B the accuracy is similar even for the digits where accuracy is not at 100%. As a visualization, could you show what the accuracy looks like for a fixed digit position and x-axis being model scale (from Figures 3-4)?\n2. You present average error results in Appendix C for the arithmetic tasks, and while general trends are the same as accuracy, it seems much noisier and the trend is not as consistent across model scale (eg. similar error between models with a difference of 2 orders of magnitude). Do you have any explanations for this, and could you also report the standard error across examples for the average error results?\n3. What was the reasoning behind choosing summation of single digit integers as opposed to performing regular addition on d digits, analogous to the multiplication setting? How would the results change for potentially \u2018harder\u2019 or \u2018simpler\u2019 subsets of examples on these arithmetic tasks (for example, addition where there\u2019s no carry for the first digit, or multiplication where there\u2019s no carry across the digits?)\n4. From the Big-Bench paper it was shown that the tasks exhibiting the most \u2018linear\u2019 trend in performance were perhaps more knowledge-based or required easier text manipulations, and the tasks with more \u2018breakthrough\u2019 performance trends had logical reasoning/sequential steps. How would this relate under your framework? I\u2019m not sure these differences in the tasks are necessarily reflected in the regularity of the optimal function."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies emergence of capabilities as a function of model size. It tries to argue that \"emergence\" happens in cases where the \nderivative of the ground truth is very large, and where larger models manage to approximate better. Experiments are provided 1) running a series of  ResNets on a small domain sin/cos function and 2) querying Qwen models on multiplication, addition, and language-formulated multi-step addition."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea to try to relate expressiveness of the model in terms of its ability to approximate steeper functions could be interesting, \nif developed well."
            },
            "weaknesses": {
                "value": "To begin with, the paper is very badly written, to the point of being unreadable. I had to apply a lot of guessing goodwill to try to understand what the main claims are. To illustrate with one example: The statement of Theorem 2.5 says \"Under assumptions 2.4\" (this is an assumption on the boundedness of the difference between loss of the minimizer and loss of the parametric minimizer with N parameters, called (5) in the paper) \"... Instead of the upper bound (5), which yields an infinite value...\". How can you assume a finite bound and then say it's infinite?\nSadly, the paper is so full of defectuous English that even with the best of interpretations it is not possible to follow beyond the vague main ideas.\n\nIt is not really clear what the contributions are on top of the work cited: Siegel and Xu,20, E et al 22, Wu 2023. Beyond a combination of results, what is new?\nSurely, it is known that large variability of the ground truth around a point gives more trouble to a model, and larger models interpolate better. \n\nAnother weakness pertains to the experiments. It is difficult to see how they illustrate the theoretical claim. First, a lot of assumptions are being made on the derivative, which is the key object of study. Like line 273: \"we will assume that [the embedding] keeps the metric space structure of the set {0, 1, \u00b7 \u00b7 \u00b7 , 9}\" - without any justification I don't see why this is true. It is not clear to me how Figs. 1 and 2 demonstrate *any* emergence (and error bars are completely missing everywhere). \nFor the Qwen based LLM experiments, I am surprised how small the dataset is (128?). \nThere might be a potentially interesting observation in Lemma 3.2 saying that derivatives of middle digits are larger and thus harder to learn for small models, but the way this is written it is unclear whether this is true, and there might be confounding issues here (for instance, it's easy to guess whether the last digit is even or odd, given the two numbers to multiply; it could be that allowing 0 for the first digit increases the probability that guessing 0 there is correct, etc etc).\nFigs. 3 and 4 are not very conclusive without error bars, especially for such small training sets.\n\nThis paper needs to be carefully rewritten (and it wouldn't hurt to use a language model for grammar control). Apart from grammatical errors, there is general sloppiness (for instance,  line 424 goes from \"Grace\" to \"Tina\", to name just one of many examples. Or the missing definition of \"i\" in the sum in line 219. What are d and k in line 219... Etc. )\nThe color scheme on all figures should be unified to go from lighter to darker (or something like that) for larger models - it doesn't help to have a color mix."
            },
            "questions": {
                "value": "1) I am unclear on what exactly your contribution is and what was already implicit in prior work - can you make that precise?\n2) Why is the assumption that embeddings of digits preserve the metric space structure true?\n3) Why are your datasets so small? What are the error bars? What am I supposed to see in the Figures ?\n4) What are the bars above the variables starting line 282?\n5) Sec 4.2. CoT: can you say anything more specific beyond speculation? Why are the derivatives multiplying and reducing?\n6) Why do you need a 2-component (2dim) function in your first example line 219 (why is one component not enough here?)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}