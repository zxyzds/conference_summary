{
    "id": "vsU2veUpiR",
    "title": "Mechanistic Unlearning: Robust Knowledge Unlearning and Editing via Mechanistic Localization",
    "abstract": "Methods for knowledge editing and unlearning in large language models seek to edit or remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates how mechanistic interpretability---which, in part, aims to identify model components (circuits) associated to specific interpretable mechanisms that make up a model capability---can improve the precision and effectiveness of editing and unlearning. \nWe find a stark difference in unlearning and edit robustness when training components localized by different methods. We highlight an important distinction between methods that localize components based primarily on preserving outputs, and those finding high level mechanisms with predictable intermediate states.\nIn particular, localizing edits/unlearning to components associated with the \\textit{lookup-table mechanism} for factual recall 1) leads to more robust edits/unlearning across different input/output formats, and 2) resists attempts to relearn the unwanted information, while also reducing unintended side effects compared to baselines, on both a sports facts dataset and the CounterFact dataset across multiple models.\nWe also find that certain localized edits disrupt the latent knowledge in the model more than any other baselines, making unlearning more robust to various attacks.",
    "keywords": [
        "Model Editing",
        "Unlearning",
        "Mechanistic Interpretability",
        "Localization",
        "Adversarial Robustness"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We achieve stronger robustness of LLM factual editing/unlearning when localizing to components identified by some kinds of mechanistic analysis",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vsU2veUpiR",
    "pdf_link": "https://openreview.net/pdf?id=vsU2veUpiR",
    "comments": [
        {
            "summary": {
                "value": "This work explores methods for knowledge editing and unlearning in large language models, focusing on how mechanistic interpretability can enhance the precision and effectiveness of these processes. The study reveals that localizing edits to components associated with lookup-table mechanisms for factual recall leads to more robust unlearning, resisting unwanted information relearning and minimizing side effects. Additionally, certain localized edits disrupt latent knowledge more effectively than other methods, resulting in increased resilience against various attacks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Studying unlearning methods from the perspective of knowledge storage and mechanistic interpretability is indeed a very important and promising direction.\n\n* This paper further confirms that causal tracing-based localization methods are not suitable for editing and unlearning tasks.\n\n* The paper is well presented and the literature review is thorough.\n\n* The experimental design is generally comprehensive."
            },
            "weaknesses": {
                "value": "1. The test dataset appears to be limited to this triplet format; is it constrained by the knowledge format, and could it be applied to more broadly and flexibly expressed knowledge sentences, such as continuous text, etc.?\n\n2. Manually analyzing and then selecting layers for operations seems to lack convenience and flexibility in the context of large-scale data editing/unlearning.\n\n3. The proposed new unlearning method lacks sufficient originality. There are already some works that attempt unlearning directly from the perspective of mechanistic interpretability, including [2].\n\n4. Knowledge is not necessarily stored entirely in the MLP; there are certain cases where it exists in the attention mechanism [1], yet the method described in the paper only considers knowledge stored in the MLP.\n\n5. There is a lack of discussion on unlearning methods in Representation Engineering [3, 4].\n\n6. Can the proposed method achieve performance advantages on other representative series of transformers, such as LLaMA?\n\n\n---\n**References:**\n\n[1] Dissecting Recall of Factual Associations in Auto-Regressive Language Models\n\n[2] Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces\n\n[3] The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning\n\n[4] Improving Alignment and Robustness with Circuit Breakers"
            },
            "questions": {
                "value": "Please see the Weaknesses section above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the effectiveness of mechanistic interpretability techniques in improving the precision and robustness of knowledge editing and unlearning in LLMs. The authors mainly discuss two types of localization methods, i.e., OT techniques that focus on preserving outputs, and mechanistic localization, which identifies high-level mechanisms with predictable intermediate states. They claim in the paper that localizing edits to components associated with the FLU mechanism leads to more robust unlearning across different input/output formats and resists attempts to relearn unwanted information. They conduct experiments on the Sports Facts and CounterFact dataset using Gemma-7B and Gemma-2-9B models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper contains the following several strengths:\n+ The paper addresses an important topic by attempting to improve the robustness of knowledge unlearning in LLMs through  localizing edits to components associated with the FLU.\n+ The authors provide a in-depth analysis and comparison between mechanistic unlearning and previous OT methods."
            },
            "weaknesses": {
                "value": "+ The paper would benefit from a more in-depth theoretical analysis to explain why FLU could inherently lead to more robust unlearning. While the authors claim that targeting the fact lookup components is more effective, they do not provide analysis or proof to support this.\n+ The experiments are limited, especially limiting itself to Gemma-7B and Gemma-2-9B models and two datasets. The authors could provide a larger variety of models and unlearning tasks in order to better demonstrate the consistency of their findings.\n+ Can the author provide more ablation study, for example on the loss weights parameter being used in 2.3, so that we could better understand the contribution of each loss in the finetuning process."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This article investigates the performance of various localization methods in unlearning/editing tasks, particularly focusing on their limitations in adapting to shifts in prompting/output distributions and adversarial relearning scenarios. It compares three main approaches: output tracing, attribution patching, and FLU. Through experiments conducted on two models and two datasets, the findings reveal that the component set identified by FLU localization is more closely tied to the factual query process, demonstrating greater robustness and generalization when fine-tuned. Additionally, the authors achieve more efficient parameter editing by controlling model modifications through weight masking."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The motivation and approach of this article are interesting, as it breaks down factual recall into more granular steps to enhance the accuracy and generalization of editing methods. \n\n2. The extensive experiments demonstrate the effectiveness of the proposed approach. The findings indicate that fine-tuning the FLU-related components identified through manual localization effectively eliminates specific knowledge from the model and makes it less susceptible to re-learning.\n\n3. Using multiple-choice questions (MCQs) can help eliminate the influence of input patterns while allowing for a more effective exploration of knowledge deletion. This approach can provide clearer insights into how specific knowledge is affected by unlearning processes."
            },
            "weaknesses": {
                "value": "1. Regarding the editing task, only the decline in the original answers is reported, which aligns with the unlearning aspect. However, results seem to lack a demonstration of improvement in the correctness of the new answers. This is significantly related to the performance of the editing task. \n\n2. Additionally, for the analysis of intermediate representations using probing, this concept is derived from existing work and does not represent a novel contribution to this research.\n\n3. [Critical] In the unlearning task, there is no theoretical proof or guarantee that the knowledge is fully forgotten. Since approximate unlearning can be easily exploited, this method is vulnerable. I believe that a theoretical guarantee is crucial for the unlearning task because the security issue is fundamentally a \"to be or not to be\" problem.\n\n4. [Critical] No adaptive attack experiments were conducted. The authors performed only standard unlearning/editing experiments, without testing for membership inference or adaptive attacks, despite the fact that approximate unlearning methods are particularly susceptible to adaptive attacks.\n\n5. [Critical] There is a lack of experiments on adaptive unlearning, which would involve sequentially unlearning specific types of knowledge\u2014for instance, first basketball, then football, and finally table tennis. Would adaptive unlearning impact the efficiency of the unlearning methods?"
            },
            "questions": {
                "value": "1. In the section defining the method, it mentions, \"In practice, we fix \u03c4 such that C\u03c4 contains the same number of parameters in OT, FLU, and random localizations.\" How should this statement be understood? For example, in the counterfact dataset, what are the MLP results using OT and EAP, given that our analysis highlights layers 3-5, 7-10, and 14-17 as the critical MLPs?\n\n2. Could you explain in more detail the process of direct path patching on the counterfact dataset? For example, after obtaining the set of components related to the fact extraction mechanism, how do we replace all edges from each MLP to all components in the set?\n\n3. In the manual localization process, why is only the MLP considered as the localization component, while other methods like EAP and OT do not also set the form to only consider MLP? Instead, they assess both attention heads and MLP components simultaneously?\n\n4. [Critical] Can the authors provide theoretical proof or guarantee to show that the knowledge is forgotten?\n\n5. [Critical] Can the authors provide the experiments of adaptive attack (attackers that easily conquer approximate unlearning )?\n\n6. [Critical] Can the authors provide the experiments of sequential unlearning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors study whether insights from mechanistic interpretability can improve our ability to perform unlearning or make targeted model edits. More specifically, using a collection of factual recall tasks, the authors study different ways of selecting a subset of the model's parameters to finetune for unlearning or editing certain facts. Building on prior work identifying a subset of model parameters involved in factual recall, the authors show that finetuning only these parameters results in more effective unlearning/model editing than finetuning parameters selected via other techniques. These claims are supported by a variety of analyses, such as robustness to different ways of eliciting the model's factual knowledge and robustness to retraining the model to relearn the facts."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "Overall, the results in this paper are very strong, and the suite of evaluations is impressively thorough.\n1. The authors do a good job of establishing that finetuning on the \"manual interp\" subset of parameters results in qualitatively different unlearning dynamics and quantitatively stronger results. The field of interpretability has struggled recently to demonstrate that their insights are useful for downstream tasks in general\u2014and for unlearning/model editing in particular as demonstrated by Hase et al. (2023)\u2014so I expect these results will be a breath of fresh air for the interpretability community.\n2. The authors perform a very thorough suite of evaluations showing that mechanistic unlearning more effectively changes knowledge stored in the model weights (see (3) and (4) for more detail). This is another place where the authors set themselves apart from the field: the unlearning literature has often struggled with thoroughly evaluating the efficacy of their methods.\n3. I was especially impressed by the relearning evaluations, showing that\u2014when training the model to relearn the unlearned facts\u2014the mechanistically unlearned model relearns the facts much more slowly (figure 2).\n4. Also impressive were the results that sweep over the number of masked parameters, revealing qualitative differences in the various unlearning techniques. Figure 5, right, which shows that mechanistic unlearning generalizes to MCQ rephrasing substantially better than any other unlearning technique, was especially striking."
            },
            "weaknesses": {
                "value": "Overall, this work's presentation was very poor.\n1. Many important details are missing from the main body of the text. (a) The \"fact loookup localization\" (which is also called \"manual interpetability\"\u2014why not use one term?) method is entirely explained in an appendix. While it's reasonable to put most of the FLU details in the appendix (since this is a replication of prior work) understanding at a high level what FLU is and how it assigns importance scores to various model components is essential for understanding the rest of the work. (b) The definition of the unlearning loss is described only as \"the log(1 - p) measure from Mazeika et al.\"\u2014this is an important part of the method and should be explained.\n2. It is very difficult to follow discussions of the tasks. There are two tasks related to sports facts, one related to unlearning facts about basketball athletes (how many athletes? I think this is mentioned later, but it should be included in section 2.1) and one related to editing 16 (random?) athletes to change their sport to \"golf.\" The authors later refer to these tasks with vague phrases like \"For editing the subset of athletes...\" The authors should instead give distinct names to these two tasks (e.g. \"Sports-unlearning\" and \"Sports-editing\") which they use to refer to the tasks throughout the rest of the text.\n3. Although the results are strong, the authors do not make it easy to tell this from reading the work. For example, the tables of numbers in the first results section are not a reasonable way to present these results. Tables like these are good for when we want to inspect small numerical differences; in contrast here we don't care about small differences (e.g. between forget scores of 0.002 and 0.000) but about large differences (e.g. between MCQ scores of 0.110 and scores >0.5). The authors should choose a different way of presenting these results, perhaps as a bar chart. \n4. Similarly, the authors present results for all three tasks for each of their evaluations, resulting in a large number of figures which are left to the reader to synthesize. This work would be much stronger if the authors found ways of presenting their work that summarized and emphasized the key takeaways.\n5. The main takeaway from the counterfact retraining experiment should have been that this experiment isn't informative, since relearning on some facts doesn't generalize to other facts for *any* of the unlearning techniques. This experiment should therefore be moved into an appendix.\n\nI also have some object-level concerns about various choices made by the authors. I mostly expect these to be easy to address with follow-up experiments (and I'll be happy to raise my score once I see these follow-ups).\n1. The task definitions involve various arbitrary-seeming choices: only basketball athletes are targeted for unlearning, only golf is uses as the target relation for editing, only a particular set of 16 facts is used for CounterFact, etc. This makes it hard to tell if these results are due to cherry-picking. The authors should sweep over options for these choices (e.g. also targeting athletes for different sports, or rerandomizing for different unlearned facts) and present averages over the conditions.\n2. While it is good that the authors study multiple models, they pair specific models with specific tasks, again making the reader wonder about cherry-picking. Unless there is a good reason not to do so, the authors should test all three tasks for *both* Gemma-7B and Gemma-2-9B.\n3. In addition to the all MLPs baseline, the authors should also include a baseline for the same number of MLPs as in the \"manual interp\" condition, but randomly selected."
            },
            "questions": {
                "value": "(All of my questions were asked in the \"weaknesses\" section.)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}