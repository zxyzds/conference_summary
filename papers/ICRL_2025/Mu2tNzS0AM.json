{
    "id": "Mu2tNzS0AM",
    "title": "LLM-Informed Semi-Supervised Learning for Text Classification",
    "abstract": "Large Language Models (LLMs) have shown impressive zero-shot and few-shot capabilities in many NLP tasks including text classification. While these models outperform others in terms of raw performance when few examples are available, they are expensive to use in practice and may lag behind traditional approaches when data (labeled or unlabeled) is plentiful. Semi-supervised learning (SSL) on the other hand can utilize large amounts of unlabeled data in combination with labeled data to improve a model's performance. In this paper, we propose to unify LLM and SSL under a common framework which effectively leverages the few-shot capabilities of LLMs in combination with SSL's ability to extract valuable information from unlabeled data to improve the model capabilities in text classification. Our approach, entitled LLM-SSL, utilizes LLMs to generate predictions on unlabeled examples and uses these predictions to guide the SSL training and improve the quality of pseudo-labels during training. We show that LLM-SSL outperforms both prior SSL approaches as well as few-shot LLMs on six text classification benchmarks.",
    "keywords": [
        "semi-supervised learning",
        "LLM"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "Fuse consistency learning and large language models to improve model capabilities in semi-supervised learning setups with scarce labels.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Mu2tNzS0AM",
    "pdf_link": "https://openreview.net/pdf?id=Mu2tNzS0AM",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces LLM-SSL, a method that combines FixMatch with LLMs for semi-supervised text classification. The authors claim that incorporating LLMs improves semi-supervised text classification performance compared to FixMatch and other SSL baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper is clearly written and easy to follow."
            },
            "weaknesses": {
                "value": "1. Missing Important Baselines:  \nThe paper omits several key baselines for semi-supervised text classification, such as PET [1], which also uses large amounts of unlabeled data for training. Additionally, it would be useful to evaluate the performance of using LLMs like Mistral-7B-Instruct directly for text classification without any fine-tuning. With few-shot examples, in-context learning could serve as an alternative for semi-supervised text classification. Recent work [2] also explores similar applications of LLMs in text classification and could provide additional comparative insights.\n\n2. Contribution:  \nThe use of LLMs for labeling is presented as a primary contribution of this paper. However, this approach is already well-established in NLP and may not suffice as a novel contribution.\n\n3. Unfair Comparison:  \nThe comparisons may not be entirely fair, as none of the baselines incorporate LLMs. Adding LLMs for labeling naturally boosts performance, making it difficult to isolate the effectiveness of the proposed SSL method. To present a fair comparison, it would be better to exclude LLMs. Without them, the performance of the proposed method appears to be close to that of other SSL baselines.\n\nReferences:  \n[1] Schick et al., Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference  \n[2] Zhang et al., Generation-driven Contrastive Self-Training for Zero-Shot Text Classification with Instruction-Tuned GPT"
            },
            "questions": {
                "value": "Why not use LLMs to estimate the quality of model-labeled examples?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the semi-supervised text classification problem with the help of LLMs. The method is built upon FlexMatch (Zhang et al., 2021) and further introduces hard pseudo-labels from LLMs for the unlabeled texts. The LLM's pseudo-labels and traditional pseudo-labels from FlexMatch are blended together during the iterations and then used as the supervision for the model training. The experiment results on 6 datasets show that the proposed method outperforms both FlexMatch and LLM, showing that the integration of both methods is effective."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper works on a well-defined problem by introducing LLM as help.\n- The proposed method is intuitive and easy-to-follow.\n- Six datasets are comprehensive."
            },
            "weaknesses": {
                "value": "- Integrating LLM into the semi-supervised text classification problem is a natural idea, however, the way proposed in this paper is a little straightforward. Based on the line 1 in Algorithm 1, we will have to send all the unlabeled texts to the LLM, which can be very expensive. We can of course expect the performance improvement as the classifier is trained with additional signals from LLM, however, perhaps at a huge cost. An analysis between cost and improvement will be very useful. I hope to see a cost-effective method to improve the classification.\n\n- Many weakly supervised text classification works have been ignored in this paper. Their problem setting often involves very limited supervised data, and sometimes, even zero annotated documents. The final classification performance can be very strong too (e.g., 90% F1 or higher). For example, some recent works [1, 2] in this direction also discussed how to leverage language models to better generate pseudo-labels and put them into a bootstrap framework for classifier training. More recently, label selection and debiasing have been studied too [3]. I believe some of these methods can be easily adapted as baselines here.\n\n[1] Mekala, Dheeraj, and Jingbo Shang. \"Contextualized weak supervision for text classification.\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020.\n\n[2] Wang, Zihan, Dheeraj Mekala, and Jingbo Shang. \"X-Class: Text Classification with Extremely Weak Supervision.\" Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.\n\n[3] Dong, Chengyu, Zihan Wang, and Jingbo Shang. \"Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification.\" Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023."
            },
            "questions": {
                "value": "- This is a minor format issue. I think the authors somehow didn't know how to use the citation properly in this ICLR template. Please explore \\citet and \\citep and check their differences. In many cases, I believe you may want to use (NAME et al., YEAR) instead of NAME et al., (YEAR)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents LLM-SSL, a novel approach that combines semi-supervised learning (SSL) with few-shot prompting from large language models (LLMs) for text classification. In this method, the LLM generates pseudo-labels for unlabeled data, while SSL techniques, relying on training dynamics, leverage these pseudo-labels to enhance model training. Specifically, the authors propose using a class-specific exponential moving average of pseudo-margins over training iterations to estimate the model's certainty in each pseudo-label and refine these labels by combining predictions from both the LLM and the model.\n\nExperiments were conducted on six datasets across 20- to 200-shot cases, comparing several baselines. These experiments showed consistent improvements over the baselines. An additional ablation study further explores the contributions of each component within the method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Overall, the paper is well-written and clear, though minor edits are needed.\n2. The motivation is well-established.\n3. The proposed method is novel, building on established SSL literature, including works like FixMatch, FlexMatch, SoftMatch, and MarginMatch.\n4. Results demonstrate consistent improvements over the baselines.\n5. The method shows faster convergence.\n6. The ablation study reveals that the different components are needed for achieving the overall performance."
            },
            "weaknesses": {
                "value": "1. The paper does not compare itself to several key methods in the field of few-shot text classification, which would provide important context. These include:\nA. Basic few-shot text classification methods, such as SetFit and FastFit.\nB. Self- and semi-supervised methods specifically designed for classification. For instance, the paper \"Zero-Shot Text Classification with Self-Training\" achieves strong results on datasets like IMDB and GoEmo in a zero-shot setup, although by using unlabeled training data without augmentation. Several other methods in this category could also be relevant for comparison.\nC. LLM-based approaches for enhancing classification. For example, \"Selective In-Context Data Augmentation for Intent Detection using Pointwise V-Information\" leverages LLM-generated examples and employs smaller classifiers training dynamic to to determine whether to include these examples in training. \"In-Context Learning for Text Classification with Many Labels\" uses retrieval models to select the most relevant examples for LLM few-shot contexts.\n\nIncluding comparisons or at least discussing key distinctions between LLM-SSL and these methods is essential for demonstrating the relevance and advantages of the proposed approach in few-shot text classification.\n\n2. Although much of the method presentation focuses on the SSL aspect, ablation results show that LLM-SSL without the APM component still performs well, closely matching LLM-SSL\u2019s overall performance and outperforming most baselines. This raises questions about the specific impact of the SSL component, as much of the improvement may stem from the integration of the LLM with FlexMatch, rather than the SSL itself.\n\n3. Regarding the experimental setup, only Mistral is used as the LLM throughout the experiments. Given that an essential part of the method involves the LLM, it would be valuable to assess its impact by experimenting with different LLMs. This would clarify the LLM\u2019s role within the method, which is currently underexplored.\n\n4. Although less critical, the use of only one encoder model, such as BERT (which is relatively dated), is a limitation. Including newer models like RoBERTa, DeBERTa, or other recent alternatives could demonstrate the robustness of the method across various base architectures."
            },
            "questions": {
                "value": "1. Why LLM ZS and FS are the same across all shots (in all cases the shots exceed the context!?) and don't have STD results?\n2. The SSL extension of the method can be very general (not necessarily for languages, and not for classification) and can target many different tasks. How will it perform on other tasks?\n3. In the pseudo-code you describe the algorithm as changing the data, is it not easier to describe it by changing the data?\n\nComments:\n\n1. No explanation on the way the STD was calculated, no STD for ablation, no significant testing.\n\n2. Small points:\n\na. row 167: grammar, confidence at an iteration\n\nb. row 182: \\neq can be a better annotation in eq. 3\n\nc. row 219:1 in the pseudo-code:  pseudo-labels \"for the every unlabeled example\" -> for every\n\nd. pseudo-labels line 8, avreage PM, why not APM, why eq. 4 and not 5?\n\ne. pseudo-labels line 12, the loss function H is not defined.\n\nf. row 253: superior to what? which specific model version?\n\ng. you mention that the pseudo-label can be a two-hot vector but didn't explain how it affects the loss function.\n\nh. row 331: why n Edunov is not in brackets, same 333, verify for all the paper.\n\nProbably will be good to proofread the whole paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper porposed LLM-SSL, a novel approach that combines Large Language Models (LLMs) with Semi-Supervised Learning (SSL) for text classification tasks. The primary goal is to improve the quality of pseudo-labels generated during SSL by leveraging the vast knowledge of LLMs, particularly in the early stages of training when the model's predictions are less reliable. Additionally, the approach introduces Average Pseudo-Margin (APM) metric to monitor the training dynamics of unlabeled examples, ensuring high-quality pseudo-labels throughout training. Experiments on six text classification benchmarks show significant improvements over strong baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed method (including APM and LLM) is well-motivated and novel.\n\n1. The experimental results seems very competitive."
            },
            "weaknesses": {
                "value": "1. Some details in experiments in unclear to me, e.g., what's weak augmentation and strong augmentation?\n1. I did not see discussion about APM threshold $\\gamma$. In my understanding, it should be a important hyperparam to control our confidence of pseudo labels. I want to see more evidence to show its robustness."
            },
            "questions": {
                "value": "Please refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Summary:\n\nThis work proposes a method to make an LLM to generate pseudo labels in a semi-supervised way to train another models on text-classification tasks.\n\nContribution:\n\nThe experimental results of this method on text-classification tasks are good compared to some settings such as the few-shot in-context learning of LLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The experimental results of this method on text-classification tasks are good compared to some settings, such as the few-shot in-context learning of LLMs."
            },
            "weaknesses": {
                "value": "1. The cost of this work can be quite large compared to the non-training method of utilizing unlabeled texts by language models.\n\n2. The backbone is not good enough to capture complex semantics of different downstream tasks which is likely to fail in the text-generation tasks.\n\n3. The tasks are quite simple which involves largely emotion classification.\n\n4. There are many advanced few-shot methods for in-context learning. However, the author has not mentioned them.\n\n5. The captions of tables are not up to the standard of the ICLR submission format."
            },
            "questions": {
                "value": "1. I wonder about the results of advocating more advanced few-shot methods, such as better retrieval or chain-of-thought methods. I am unsure whether we need to train an encoder-only model to solve the emotion classification task."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}