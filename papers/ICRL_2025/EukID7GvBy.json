{
    "id": "EukID7GvBy",
    "title": "Gradual Learning: Optimizing Fine-Tuning with Partially Mastered Knowledge in Large Language Models",
    "abstract": "During the pretraining phase, large language models (LLMs) acquire vast amounts of knowledge from extensive text corpora. Nevertheless, in later stages such as fine-tuning and inference, the model may encounter knowledge not covered in the initial training, which can lead to hallucinations and degraded performance. This issue has a profound impact on the model's capabilities, as it will inevitably face out-of-scope knowledge after pretraining. Furthermore, fine-tuning is often required to adapt LLMs to domain-specific tasks, necessitating the acquisition of new knowledge. However, this phenomenon limits the model\u2019s ability to learn and integrate new information during fine-tuning. The effectiveness of fine-tuning largely depends on the type of knowledge involved. Existing research suggests that fine-tuning the model on partially mastered knowledge\u2014for instance, question-answer pairs where the model has a chance of providing correct responses under non-greedy decoding\u2014can enable the model to acquire new knowledge while mitigating the forgetting of previously learned information. Notably, this approach can still lead to the forgetting of fully mastered knowledge, constraining the fine-tuning dataset to a narrower range and limiting the model's overall potential for improvement. Given the model\u2019s intrinsic reasoning abilities and the interconnectedness of different knowledge areas, it is likely that as the model\u2019s capacity to utilize existing knowledge improves during fine-tuning, previously unmastered knowledge may become more understandable. To explore this hypothesis, we conducted experiments and, based on the results, proposed a two-stage fine-tuning strategy. This approach not only improves the model's overall test accuracy and knowledge retention but also preserves its accuracy on previously mastered content. When fine-tuning on the WikiQA dataset, our method increases the amount of knowledge acquired by the model in this stage by 24%.",
    "keywords": [
        "Large Language Model; DCAI ; Fine-tuning"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=EukID7GvBy",
    "pdf_link": "https://openreview.net/pdf?id=EukID7GvBy",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a two stage finetuning strategy which supposedly improves the models knowledge retention capacity. Using the taxonomies introduced by [1], the paper conducts multi-stage finetuning ablations on the different sequence of subsets.\n\n[1] https://arxiv.org/pdf/2405.05904"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Conducts several ablations on sequential multi-stage finetuning on easy to hard dataset subsets."
            },
            "weaknesses": {
                "value": "- The contributions of the paper is minimal, given they performed ablations on data taxonomies introduced by [1]. At best, the work is an extension of the ablations performed by [1] themselves (see Section 5 of the paper).\n- Only one downstream dataset (WikiQA) is considered, which limits the broader applicability of the approach.\n- The paper is very poorly written, with large swaths of the text borderline unintelligible. Specially sections 3.1, 3.2, 4.1.2 need a lot of work to make the flow understadable.\n- Numerous evidence on poor quality of writing / organization: Table on page 5 is redundant, statistical analysis on changes from one subset to the next is not highlighted well in Tables 2,3,4,12. It is highly unclear what I'm looking at - the change should be presented _relatively_, not as absolute numbers which is meaningless.   \n\n\n\n[1] https://arxiv.org/pdf/2405.05904"
            },
            "questions": {
                "value": "- In writing, please add a space between all citations - it becomes hard to read the full sentences!\n- What is the overall recommendation in terms of finetuning for multiple stages? It is not clear from the text."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper attempts to address the knowledge forgetting problem in the scenario of continual fine-tuning, where we might lost some information/performance if we do continual fine-tuning. \n\nThey perform 2-stage fine-tuning:\n1. Categorize the data into certain categories to understand how the model knows about the previous knowledge.\n2. The second stage with mix up with some replay data to continue fine-tuning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Efforts in designing strategies for how we fine-tuning with the replay data and the training data. We can see that, some unknown data change to MaybeKnown, justifying the  hypothesis.\n2. Retain the model original performance, and also comparable performance with SFT over new data (1-stage fine-tuning)."
            },
            "weaknesses": {
                "value": "1. Paper is not well written, especially the citation position. \n2. It\u2019s not practical to assume we have the original replay data. For example, if we use a pre-trained model (Qwen), they will not release the SFT data for the community. if we want to improve the model performance on QA using our new data, we probably don\u2019t have original SFT data for replay. The assumption seems not practical."
            },
            "questions": {
                "value": "Maybe the author can help answer the second weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tackles the challenges that LLMs primarily acquire knowledge during pretraining and often struggle to integrate new information effectively in later stages such as fine-tuning and inference times. This can lead to issues like hallucinations and decreased performance (catastrophic forgetting).\nThe core argument of this paper is that fine-tuning LLMs on partially mastered knowledge can help them leverage their existing knowledge base and reasoning capabilities to comprehend new concepts. The authors propose a two-stage fine-tuning to test this hypothesis.\nStage 1: The model is fine-tuned on data representing knowledge it partially understands, meaning it has some chance of answering related questions correctly without fine-tuning.\nStage 2: The training data is augmented with data that demonstrates improved mastery after the first stage, including knowledge initially classified as less well-understood. This augmented dataset is then used for a second round of fine-tuning.\nThe results on the WikiQA dataset show that the two-stage fine-tuning method leads to improved test accuracy, enhanced knowledge mastery and mitigation of forgetting."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The motivation of this paper is well articulated, and it is important indeed such as how to avoid hallucination and catastrophic forgetting when LLMs are fine-tuned after pre-training."
            },
            "weaknesses": {
                "value": "1. Limited generalizability due to focus on a single closed-book question answering: The experiments rely heavily on the WikiQA dataset, which is specifically designed for closed-book question answering. This focus raises concerns about the generalizability of the results to other knowledge domains and tasks. \n\n2. Tiny Incremental Improvements: While the two-stage fine-tuning method shows improvements in test accuracy and knowledge mastery compared to one-stage fine-tuning, these improvements are relatively small. For example, Table 7 in the source shows that for the Qwen2 model, the accuracy improvement from one-stage to two-stage is smaller than 1%. This raises the question of whether the added complexity and computational cost of the second stage are justified by such marginal gains.\n\n3. Practicality of the method: Given the need for pre-classification, the practicality of the two-stage method for tasks that are not already determined is debatable."
            },
            "questions": {
                "value": "- The compiled \\cite looks broken throughout the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates the challenges of finetuning in language models, noting that introducing new knowledge can lead to hallucinations and degraded performance. The authors hypothesize that reinforcing partially mastered knowledge could improve the model\u2019s performance with new knowledge. They use a two-phase approach: (1) Knowledge detection (types) and (2) Two-stage finetuning with 'MaybeKnown' and 'HighlyKnown' Knowledge Replay. Primarily using Qwen-2, they analyze the contributions of each finetuning stage.\n\nWriting and Clarity:\n\n  - The paper suffers from unclear writing, especially in the abstract and motivation sections.\n  - Poor organization and coherence in presenting experiment motivation, methods, and results, impacting the overall flow.\n  - Findings are not clear and are not connected to the observations enough.\n\n\nReliance on Previous Contribution:\n\n    - Misinterpret \u2018MaybeKnown\u2019 recommendation (line 374) for improving performance on less known knowledge -  Gekhman et al 2024 claimed that the \u2018MaybeKnown\u2019 helps to less hallucinate data the model already knows.\n\nExplanation and Motivation:\n \n Lacks clear motivation for choosing specific strategies:\n\n    - No justification for preferring a replay-based approach for catastrophic forgetting (line 154).\n    - Insufficient explanation for focusing on the \u2018MaybeKnown\u2019 category over \u2018HighlyKnown\u2019 for finetuning.\n    - Weak or missing rationale for strategies 1-5 (lines 403-416).\n    - Justification is absent for selecting the first answer in multi-answer QA (lines 245-246).\n   - Foundational intuition (Section 3.1) for acquiring new knowledge from partial knowledge is weak and lacks references.\n\n\nWeak Results:\n  - The proposed method shows modest improvements (~4-7%) in accuracy (Table 7) which is not strongly associated with a specific factor.\n  - Results lack consistency: replay strategies sometimes decrease performance (e.g., Table 9, Strategies 4 and 5 for WeaklyKnown).\n  - Small performance changes in Table 8 might result from various, unexamined factors.\n  - No analysis of performance differences between models (Qwen and LLaMA).\n- Not robust - used only one dataset (closed).\n\n\nResults and Clarity:\n  - Results are presented in raw counts (Tables 3, 4, 5, 10), without proportions, making the transition and interpretation difficult.\n  - Graph construction (line 338) is unclear and does not clearly support the hypothesis; the contribution of node reclassification is ambiguous."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Proposes Interesting intuition: integrating partially-known knowledge can boost performance on new known knowledge\n        - Presents a clear, structured methodology in Fig. 1"
            },
            "weaknesses": {
                "value": "The paper's contribution appears limited, as the concept of using \u2018MaybeKnown\u2019 knowledge to enhance test-set performance has already been established by Gekhman (2024). The newly proposed two-stage finetuning with a replay method results in only minor improvements (~1-2%), which, coupled with weak, non-robust analysis and disorganized writing, supports a recommendation for rejection."
            },
            "questions": {
                "value": "Formatting:\n\nMissing spaces and inconsistent citation format throughout the paper (e.g., Intro lines 40, 45, 51).\n\nTerminology:\n\nDefine terms before using them:\n\u201cKG\u201d is used without explanation (line 67).\nKnowledge types (line 75).\nPrompt template (line 247).\nThe second stage of finetuning in the Introduction is unclear and would benefit from clarification.\n\nMethodology:\n\n- Method Section 3.2: clarify data categories used in the finetuning steps (lines 203-206).\n- Confirm if knowledge re-detection after each finetuning stage is performed on the training set\u2014currently unclear.\n- Definitions and Supplementary Information:\n   Table 1 is identical to the one in Gekhman\u2019s work (Figure 2(a)), and presented in the main paper which might be mistakenly interpreted as \n   some contribution.\n- Define \u201cOrigin\u201d in Table 7. Does it represent baseline performance without finetuning on \u2018MaybeKnown\u2019?\n- Consider adding a supplementary section, e.g., for explaining prompt template creation (line 247).\n\nResults Interpretation:\n\n- Table 8: Strategy 1 and Strategy 4 have identical scores\u2014does this imply an effect specific to \u2018HighlyKnown\u2019 knowledge?\nDivide the main results from analyses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper first designed an experiment to validate their hypothesis that as the model\u2019s capacity to utilize existing knowledge improves during fine-tuning, previously unmastered knowledge may become more understandable. Then they propose a two-stage fine-tuning method to address this."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "* knowledge probing/ augmentation is an important research problem for large language models."
            },
            "weaknesses": {
                "value": "* The presentation in this paper is very poor. Actually, it's very hard for me to follow them due to the poor logic in the abstract and introduction. Also, figures and tables are not well-designed.\n\n* Many format problems. There is no space between text and citation; cited and citep are not properly used in this paper; Some citations are even raw text (line 242)\n\n* The proposed method and findings are trivial. And the experiment settings that tune the model in the test set are also problematic to me."
            },
            "questions": {
                "value": "I suggest the authors submit this paper maybe tot a workshop."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}