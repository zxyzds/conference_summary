{
    "id": "VD4PFpecG2",
    "title": "Regret Bounds for Episodic Risk-Sensitive Linear Quadratic Regulator",
    "abstract": "Risk-sensitive linear quadratic regulator is one of the most fundamental problems in risk-sensitive optimal control. \nIn this paper, we study online adaptive control of risk-sensitive linear quadratic regulator in the finite horizon episodic setting. \nWe propose a simple least-squares greedy algorithm and show that it achieves $\\widetilde{\\mathcal{O}}(\\log N)$ regret under a specific identifiability assumption, where $N$ is the total number of episodes. If the identifiability assumption is not satisfied, we propose incorporating exploration noise into the least-squares-based algorithm, resulting in an algorithm with $\\widetilde{\\mathcal{O}}(\\sqrt{N})$ regret. \nTo our best knowledge, this is the first set of regret bounds for episodic risk-sensitive linear quadratic regulator. \nOur proof relies on perturbation analysis of less-standard Riccati equations for risk-sensitive linear quadratic control, and a delicate analysis of the loss in the risk-sensitive performance criterion due to applying the suboptimal controller in the online learning process.",
    "keywords": [
        "reinforcement Learning",
        "regret",
        "LEQR",
        "finite horizon",
        "episodic setting"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=VD4PFpecG2",
    "pdf_link": "https://openreview.net/pdf?id=VD4PFpecG2",
    "comments": [
        {
            "summary": {
                "value": "This paper considers the risk-sensitive LQR problem with unknown system matrices. The authors consider the standard LEQR formulation to capture the risk-aware scenario in the control problem and study the episodic problem setting where the system needs to be reset to the initial condition at the end of each episode. The authors propose online learning algorithms to solve the problem and show that the regret of the algorithm is bounded as \\sqrt{N}, where N is the number of episodes in the problem. Under some additional assumption, the authors show that the regret bound becomes \\logN."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThis paper initiate the consideration of risk-sensitive formulation of LQR in an online episodic setting.\n2.\tThe paper rigorously characterize the regret bounds of the proposed algorithms."
            },
            "weaknesses": {
                "value": "1. Although the authors mentioned a previous work Basei et al. 2022 that considers an episodic control problem, it would be good to further explain why it is important/of practical interest to consider the episodic setting in the control problem. In addition, it would be good to discuss what the major challenge is if we move to the non-episodic setting which is the most standard setting in control problems (i.e., the system involves continuously and does not reset).\n2. It would be good to explain more how the LEQR formulation captures the risk-aware scenario in Section 2.1. The authors also mention that their analysis extends to \\gamma<0. What does \\gamma<0 stand for in the risk-aware scenario? In general, it would be good to provide an intuitive explanation of how different values of \u03b3 (positive, negative, or zero) correspond to different risk attitudes. This would help readers better understand the practical implications of the LEQR formulation.\n3. The authors should justify their choice to use only data from epoch l-1 for estimation in Algorithm 1, and discuss whether using data from multiple previous epochs could improve accuracy. Additionally, they should clarify whether the i.i.d. assumption on initial states is necessary for their results to hold.\n4. The optimal controller based on the initial estimate \\theta^1 needs to satisfy Assumption~1 as well in Theorem 1. Could you justify this assumption on \\theta^1, i.e., how do you choose \\theta^1 so that the assumption is satisfied? \n5. Regarding the regret bounds provided in Theorems~1 and 2, they depend exponentially on the horizon length T for each episode. However, in episodic MDP problems, the regret bound depends on T only polynomially (e.g., https://proceedings.mlr.press/v70/azar17a/azar17a.pdf). Could the authors comment on this issue:  whether this is fundamental to the risk-sensitive setting or if it's an artifact of the regret analysis? Moreover, would it be possible to improve this dependence to polynomial in T?\n6. Could you explain why the least squares approach in Algoirthm~2 needs a regularization term?\n7. Please explicitly specify how the initial estimate \\theta^1 is chosen in Algorithm~2."
            },
            "questions": {
                "value": "Please refer to Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper\u00a0studies the online adaptive control of risk-sensitive Linear Quadratic Regulator (LQR) problem in a finite-horizon episodic setting, which is referred to as the Linear Exponential-of-Quadratic Regulator (LEQR) problem. The goal is to design the online algorithm that selects control that minimizes the total regret, i.e., the difference between the sum of minimum exponential risk-sensitive cost (defined in Eq. 2) and the algorithm's cost in each episode.\n\nThe authors introduce two algorithms: a least-squares greedy algorithm (Algorithm 1) that achieves\u00a0$\\tilde{O}(\\log N)$\u00a0regret under identifiability condition (Assumption 1) and a second algorithm (Algorithm 2: Least-Squares-Based Algorithm with Exploration Noise) that incorporates exploration noise to achieve\u00a0$\\tilde{O}(\\sqrt{N\u200b})$\u00a0regret without the identifiability assumption. Finally, the authors have shown the different performance aspects of the proposed algorithms on two synthetic LEQR systems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "**The following are the strengths of the paper:**\n1. This paper is the first to provide regret bounds for the episodic finite-horizon LEQR problem, which has applications in risk-sensitive control problems in areas like finance and healthcare.\n\n2. The authors proposed two algorithms with sub-linear regret bounds guarantees. The regret bounds are derived using perturbation analysis of modified Riccati equations, which incorporate exponential risk-sensitive cost (defined in Eq. 2).\n\n3. Finally, the authors have demonstrated the different performance aspects (sub-linear regret and effect of risk parameter and horizon on regret) of the proposed algorithms on two synthetic LEQR systems."
            },
            "weaknesses": {
                "value": "**The following are the weaknesses of the paper:**\n1. Since verifying the identifiability assumption (Assumption 1) for a given problem may not be possible, the first algorithm may not be useful in practice. \n\n2. Both proposed algorithms are restricted to fixed finite-horizon settings and linear dynamics, which limits their real-world application, where horizon length can vary across episodes and problems with non-linear dynamics.\n\n3. The following parts of the paper are not clear enough:\n    - Why are the $N$ episodes in Algorithm 1 divided into epochs of increasing lengths to estimate the system matrices?\n    - Why does the paper only consider the exponential risk-sensitive cost? Are there any real-world motivating examples for this choice?"
            },
            "questions": {
                "value": "Please address the weaknesses of the paper. I have a few more questions/comments:\n1. Instead of using exponential noise, can a UCB (upper confidence bound)-based exploration strategy in Algorithm 2?\n\n2. How to extend these results for other risk measures, e.g., coherent risk measures (VaR, CVaR, EVaR, etc.) as studies in the following paper:\n\t1. Lam et al., [Risk-Aware Reinforcement Learning with Coherent Risk Measures and Non-linear Function Approximation](https://openreview.net/pdf?id=-RwZOVybbj)\n\n3. What is the overall dependence of regret bounds given in Theorem 1 and 2 on horizon length ($T$) and risk-sensitivity parameter ($\\gamma$)?\n\n\n**Minor comment:**\n\nAdding the layman's explanation and consequences of Assumption 1 will help readers identify when this assumption can hold in practice. Also, details (in the appendix and refer to the main paper) can added on how the standard Riccati equations are modified to incorporate exponential risk-sensitive costs.\n\nI am open to changing my score based on the authors' responses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Since this work is a theoretical paper, I do not find any ethical concerns."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies an online linear quadratic regulator (LQR) problem with risk-sensitive cost functions. In particular, when the environment can provide enough exploration noise, a least-square model fitting algorithm is provided and it achieves log N regret. When the environment is not \"noisy\" enough, the authors present a gaussian exploration method that achieves square root N regret. Numerical experiments are provided to validate the proposed algorithms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow. All the assumptions are well-motivated and standard in the literature. The proposed algorithms are simple and can be easily implemented by practitioners. \n\n----"
            },
            "weaknesses": {
                "value": "*1.* I think the authors should provide an explicit dependence of gamma in the two theorems, i.e., the big O should contain gamma orders, as gamma is the key in the studied risk-sensitive LQR problems. With gamma dependence in the results, the authors could compare the regret order with risk-neural MDP/LQR problems. Discussions/guidance on gamma selections could be included.\n\n*2.* Literature/discussions on risk-averse RL/LQR problems are limited, in particular, the choice of risk-averse metrics and the reason for studying the LEQR problem.\n\n*3.* The authors could provide a more interesting real-world control problem in the simulation. \n\n----"
            },
            "questions": {
                "value": "*1.* Do the two theorems hold when gamma goes to 0, i.e., the limit is right-continuous in gamma? \n\n*2.* Typo: line 218-219, \"We\" play."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a least-squares greedy algorithm for the online adaptive control of the risk-sensitive linear quadratic regulator (LQR) in a finite-horizon, episodic setting, which achieves a regret bound of $O(\\log n)$ under identifiability assumption. It also proposes adding exploration noise to the least-squares algorithm when the identifiability assumption does not hold, resulting in a modified approach with a regret bound of $O(\\sqrt{n})$."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The authors introduce two algorithms based on least-squares methods tailored for LEQR. The first least-squares greedy algorithm without exploration noise achieves a logarithmic regret bound $O(\\log n)$ under an identifiability condition, and the second algorithm, designed for scenarios where the identifiability assumption does not hold, achieves a $O(\\sqrt{n})$ regret bound.\n- This work provides the first regret bounds for finite-horizon episodic LEQR, showing the existence of logarithmic regret under identifiability assumption.\n- The theoretical analysis relies on perturbation analysis of special Riccati equations for the risk-sensitive LEQR setting, where additional matrices ($\\widetilde P$) are present in the Riccati equations due to the risk sensitivity. The authors show nontrivial analysis techniques addressing the challenges in the analysis.\n- The authors also introduce novel analysis on risk-sensitive loss."
            },
            "weaknesses": {
                "value": "- It would be beneficial to establish matching lower bounds for both algorithms to provide a more comprehensive understanding of their optimality.\n- Typo/Inconsistency of the plot descriptions: Regret performance of Algorithm 1 (System 1) vs Effect of the risk parameter (Algorithm 1 System 1)"
            },
            "questions": {
                "value": "- As the authors noted, studying regret bounds for LEQR in the infinite-horizon average-reward (non-episodic) setting would be interesting. What challenges arise in extending the current proof methods to this setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}