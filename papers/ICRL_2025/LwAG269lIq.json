{
    "id": "LwAG269lIq",
    "title": "Data-Driven Discovery of PDEs via the Adjoint Method",
    "abstract": "In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form and formulate a PDE-constrained optimization problem aimed at minimizing the error of the PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations) allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, we consider a family of parameterized PDEs encompassing linear, nonlinear, and spatial derivative candidate terms, and elegantly derive the corresponding adjoint equations. We show the efficacy of the proposed approach in identifying the form of the PDE up to machine accuracy, enabling the accurate discovery of PDEs from data. We also compare its performance with the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND  [Rudy, Samuel H., et al. Science advances 3.4 (2017): e1602614.], on both smooth and noisy data sets. Even though the proposed adjoint method relies on forward/backward solvers, it outperforms PDE-FIND for large data sets thanks to the analytic expressions for gradients of the cost function with respect to each PDE parameter.",
    "keywords": [
        "PDE discovery",
        "Symbolic Regression",
        "Adjoint method"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "Finding governing equations given data set using Adjoint method",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=LwAG269lIq",
    "pdf_link": "https://openreview.net/pdf?id=LwAG269lIq",
    "comments": [
        {
            "title": {
                "value": "Answer to 2rUM"
            },
            "comment": {
                "value": "> The manuscript has not been well-written, so that the reader cannot find their motivation clearly. The theoretical part has been shown rigorously. The experiment part: description not clear\n\n> Hopefully, the authors can make your contribution clearly and make your experiment and theories to test your results.\n\nWe thank the referee for their comment. However, we disagree with them. We believe the test cases are actually detailed, please see Appendix B. Can the referee be more specific? What details are missing?"
            }
        },
        {
            "title": {
                "value": "Answer to qLFP"
            },
            "comment": {
                "value": "> 1. The proposed framework is motivated...\n\n**Answer:** Actually, we believe majority of physical processes are described with PDEs of 1st, 2nd, and rarely 3rd order derivatives. So, we do not agree with the referee. However, the adjoint method can be used for large class of PDEs, subject to the stability of the deployed numerical scheme. Please note that in appendix B.2, we have considered Kuramoto-Sivashinsky equation, which has derivatives up to 4th order, and leave more complicated test cases for future.\n\n> 2. The proposed framework closely resembles the PINN-SR algorithm....\n\n**Answer:** In the proposed method we are not learning any surrogate model. So we do not agree on the resemblance of proposed adjoint method with PINN-SR. However, we leave out such interesting comparisons to future work given the time constraint.\n\n> 3. The motivation for the equation discovery architecture is a little convoluted....\n\n**Answer:** We agree with the referee. We are trying to say that PDE discovery and data-driven surrogate models are answering two different questions. We added this to the introduction \"Once the governing equation is found, one can either use the standard numerical methods for prediction, or train a PINN-like surrogate model for fast evaluation.\"\n\n> 4. The Bayesian class of equation discovery algorithms also does a good job...\n\n**Answer:** We thank the referee for pointing us to Bayesian class of equation discovery for PDE discovery. We added the citations in the introduction as related works. However, we leave further comparisons to future study given the time constraint.\n\n> line 87. PINN-SR uses the Adam optimizer, which can process data in batches. Thus, why do authors feel that PINN-SR will not scale well with the size of the data set?\n\n**Answer:** Although Adam optimizer eases the optimization problem by considering batches of data, the underlying cost function of the regression problem remains the same as PDE-FIND. Here, we make the claim and show that the explicit gradients computed from the adjoint method is more accurate that gradients computing from the point-wise L2 error using backpropagation. \n\n> line 135. The vector $f^p$  may be missing a comma.\n\n**Answer:** Thanks for spotting this typo. We corrected the manuscript.\n\n> line 189. Does the framework work only for zero boundary conditions?\n\n**Answer:** The compact support of $\\lambda$ is motivated by the fact that we consider boundary conditions as known. In case boundary conditions are also parameterized, we need to add another constraint with its Lagrange multiplier to find its parameters. We added this justification to the text.\n\n> line 218. How $\\sigma$ is selected. Consider an example of the Navier-Stokes equation, where the viscosity can be in the order of $10^{-4}$ to $10^{-5}$. In such cases, the proposed algorithm will discard the relevant terms.\n\n**Answer:** As mentioned in the text, $\\sigma$ is a user-defined threshold, to drop out terms that have close to zero coefficients in return. In several cases, we showed that adjoint method finds coefficients of unimportant terms to be less than $10^{-8}$, see results for Burgers' and heat equation. This would allow finding PDEs with such small coefficieint.\n\n> line 282. Is this method applicable to time-independent systems like Darcy and Poisson's equations? If applicable, how averaging of gradients in algorithm 2 will be done in the absence of the time component.\n\n**Answer:** We thank the referee for their question. While we restricted ourself to temporal-spatial PDEs, one may consider time-independent problems by introducing artificial time, such that at the steady state, the time-independent systems is recovered. This is definitely an interesting problem, and we may investigate further in the future work.\n\n> Fig. 1(c). It is intriguing to see that the error increases with an increase in data in both methods.\n\n**Answer:**  The increase of error versus data size for PDE-FIND method is clearly visible. However. for the adjoint method, the error stays on the same order of magnitude.\n\n> Fig. 1(d). Since the proposed method uses a forward solver in the loop, is it not the computational time supposed to increase with increasing training samples?\n\n**Answer:** Yes, the computational time increases for both PDE-FIND and adjoint method in Fig1.d.\n\n> Eq. (13-14). The authors show that even in ill-posed problems, the proposed method can provide an alternate sparse equation. However, estimating the error and visually observing the solution fields before concluding on the accuracy would be best.\n\n**Answer:** In the case of the considered ill-posed problem, since we know the solution f, the error at any point can be obtained by plugging in $f=sin(x-t)$ into found PDE. For example, in case of 13 and 14, $|\\cos(x-t)(-1+0.996)| \\leq |\\cos(x-t)(-1+0.989)|$ at all x and t."
            }
        },
        {
            "comment": {
                "value": "> 1. The algorithms proposed in the ...\n\n**Answer:** In line 227-234, we discuss the numerical complications in the proposed adjoint method for PDE discovery task. We note that although each iteration of adjoint method is more expensive compared to PDE-FIND, in several test cases we showed that the method converges relatively fast (in less than 100 iterations/epochs).\n\nAs mentioned in contributions, the adjoint method has not been used in the literature to discover PDEs from data, and that is the novelty of this paper. If the referee disagrees, we kindly ask them to point us to a paper that uses adjoint method for PDE discovery task. \n\n> The paper is ...\n\n**Answer:** Thanks for spotting the typos. We have corrected them in the revised version. \nIn **Problem setup.** we explained the deployed notation. That should clarify referees confusion.\n\nEq.(2) denotes iterated differential operator. We added this to the text.\n\nPlease note that eq.(3) is semi-discrete, i.e. the first term includes $L^2$ error between solution of PDE and data points on discrete points (with superscript k for spatial and j for temporal indices), and the second term (constraint on PDE) is continuous. We think the equation illustrate that clearly. \n\nWe thank the referee for their suggestion about arrangement of sections. We changed section 4,5,6 into subsections of section 3. \n\n> 3. Many results ...\n\n**Answer:** Eq. (4) is obtained by taking derivative of Eq. (3) with respect to parameters $\\alpha$, and shifting derivatives from $f$ to $\\lambda$ using integration by parts and enforcing $\\lambda\\rightarrow 0$ for $x\\in \\partial \\Omega$. Clearly, we are assuming that all considered derivatives of $\\lambda$ exist, and $\\lambda\\rightarrow 0$ on $ \\partial \\Omega$. That defines the functional space of $\\lambda$. Similar steps, integration by parts and letting  $\\lambda\\rightarrow 0$ for $x\\in \\partial \\Omega$, are considered to obtain eq. 6, besides functional derivative of $f$. We believe these derivations are trivial and leave it to the reader.\n\nThe compact support of $\\lambda$ is motivated by the fact that we consider boundary conditions as known. In case boundary conditions are also parameterized, we need to add another constraint with its Lagrange multiplier to find its parameters. We added this justification to the text.\n\n> 4. The manuscript only considers very simple PDEs...\n\n**Answer:** We agree with the referee that more tests on more complex dynamics can improve the paper. We kindly remind the referee that in Appendix B.4, we actually have shown an example of 2-dimensional coupled system of PDEs, i.e. reaction diffusion system of equations. Please see other challenging problems in Appendix B. We leave more complicated test cases for future work given the time constraint.\n\n> 5. Point (4) is particularly critical...\n\n**Answer:** To the best of our knowledge, PDE-FIND is the state-of-the-art method for PDE reconstruction, and that is why we only compared our method to that as a point of reference.\n\n> There is only very limited discussion on the use of l2 regularization...\n\nWe thank the referee for their comment. We note that if the data can be represented with a unique PDE, then the adjoint method does not need the regularization factor at all. In fact, in Appendix D, Fig 18, we showed that by letting $l^2$ regularization factor to zero, adjoint method is capable of finding the underlying PDE as a solution to the sparse search. We only introduced $l^2$ regularization to distinguish between PDEs in the pathological cases, where the solution is not unique. Similarly, the thresholding is only used to make the sparse search faster, dropping out irrelevant terms. Without it, the adjoint method still works, but may converge in more interations. We thank the referee for pointing us to IHT method. We have added the reference to the manuscript.\n\n>When considering sensitivity to noise (line 411), the noise variance is specified in percentage. With respect to what? Is it an SNR?\n\nWe introduce noise $\\epsilon\\sim \\mathcal{N}(0,\\sigma^2)$ to each data point $f^*$ by $f^*(1+\\epsilon)$. Therefore, the added term $\\epsilon f^*$ to the signal $f^*$ has the noise/signal ratio of $\\epsilon$.\n\n> Where does the normalization of the integral in (3) come from? This doesn\u2019t seem to make sense given that the manuscript considers a regular grid.\n\n**Answer:** We agree with the referee. We removed the normalization factor in Eq. 3. Given we consider normalized settings, our results do not change.\n\n> When discussing the learning rate...\n\n**Answer:**  This was an observation we made in this work. We clarified that in the revised version.\n\n> When defining $f^*$ on line 121,...\n\n**Answer:** Actually, $f^*$ are discrete points defined on the grid. The coarser time grids would work because $f^*$ only appears as the final condition for the adjoint equation, and is not present in neither the forward nor backward equations."
            }
        },
        {
            "title": {
                "value": "Answer to x4HQ"
            },
            "comment": {
                "value": "> (1) The approach presented here is relatively incremental, as the reconstruction of parameterized PDEs has been extensively studied within the inverse problem community for several decades. Many of the concepts explored, particularly adjoint-based parameter estimation, are already well-established. The paper would benefit from a more explicit discussion of how this work advances or differs from existing methods in the literature on PDE-constrained optimization and inverse problems. \n\n**Answer:** As mentioned in **contributions**, the adjoint method has not been used in the literature to discover PDEs from data, and that is the novelty of this paper. If the referee disagrees, we kindly ask them to point us to a paper that uses adjoint method for PDE discovery task.\n\n> (2) The demonstration example is limited to simple lower dimensional problems, which may not be sufficient to convincingly illustrate the method\u2019s robustness or scalability to higher-dimensional, real-world PDEs. Given the computational efficiency implied by the adjoint method, testing on a more challenging, higher-dimensional example or a PDE with more complex dynamics would strengthen the paper. This would also allow for a more thorough evaluation of the method's effectiveness in a broader range of realistic scenarios.\n\n**Answer:** We agree with the referee that more tests on more complex dynamics can improve the paper. We kindly remind the referee that in section B.4, we actually have shown an example of 2-dimensional coupled system of PDEs, i.e. reaction diffusion system of equations. We leave more complicated test cases for future work given the time constraint.\n\n> (3) The paper would benefit from a comparative analysis with other state-of-the-art methods for PDE reconstruction. This comparison could help clarify the unique contributions and limitations of the adjoint-based approach relative to current methods in data-driven PDE reconstruction.\n\n**Answer:** To the best of our knowledge, PDE-FIND is the state-of-the-art method for PDE reconstruction. We would like to ask the referee to be more specific about their request. Which other method should we consider for comparison besides PDE-FIND?\n\n\n> (1) Can you provide more complex examples in high dimensional space?\n\n**Answer:** In section B.4, we actually have shown an example of 2-dimensional coupled system of PDEs, i.e. reaction diffusion system of equations. \n\n> (2) Can you provide some a posteriori analysis analysis regarding the learning performance with respect to the given data or type of PDEs? \n\n**Answer:** We are not sure what the referee means. The performance of adjoint method is affected by the choice of the numerical methods, which depends on the type of PDE. However, we showed that a simple finite difference method is good enough in recovering underlying PDEs, as long as we pick small enough $\\Delta t$ to ensure stability.\n\n> (3) Can you derive some well posed-ness of the parametrization of PDE? i.e, if I provide two similar parameterized PDEs, how about the learning performances regarding interpolation and prediction?\n\n**Answer:** We would like to ask the referee to clarify what they mean. In the current manuscript, we have provided an analysis on regularizing ill-posed PDE discovery task in section 6, and incomplete guessed PDE space in Appendix C. Furthermore, in section 4, we addressed the case when we are given partial observations in time. That may answer their question about performance versus interpolation."
            }
        },
        {
            "summary": {
                "value": "The authors claim that they can discover some PDE via an adjoint-based method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The topic looks very interesting."
            },
            "weaknesses": {
                "value": "The manuscript has not been well-written, so that the reader cannot find their motivation clearly.\nThe theoretical part has been shown rigorously.\nThe experiment part: description not clear"
            },
            "questions": {
                "value": "Hopefully, the authors can make your contribution clearly and make your experiment and theories to test your results."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a method for reconstructing a parameterized partial differential equation (PDE) from observed data using the adjoint method. The authors claim that their approach offers a new perspective on PDE reconstruction by leveraging adjoint-based optimization, demonstrating the method on simple examples and show better performance compared to the well known PDE-FIND."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is generally well-written, with clear explanations of the methodology and a solid foundation in the adjoint approach. The derivations are technically sound and should be accessible to readers familiar with inverse problems and PDEs."
            },
            "weaknesses": {
                "value": "(1) The approach presented here is relatively incremental, as the reconstruction of parameterized PDEs has been extensively studied within the inverse problem community for several decades. Many of the concepts explored, particularly adjoint-based parameter estimation, are already well-established. The paper would benefit from a more explicit discussion of how this work advances or differs from existing methods in the literature on PDE-constrained optimization and inverse problems.\n(2) The demonstration example is limited to simple lower dimensional problems, which may not be sufficient to convincingly illustrate the method\u2019s robustness or scalability to higher-dimensional, real-world PDEs. Given the computational efficiency implied by the adjoint method, testing on a more challenging, higher-dimensional example or a PDE with more complex dynamics would strengthen the paper. This would also allow for a more thorough evaluation of the method's effectiveness in a broader range of realistic scenarios.\n(3) The paper would benefit from a comparative analysis with other state-of-the-art methods for PDE reconstruction. This comparison could help clarify the unique contributions and limitations of the adjoint-based approach relative to current methods in data-driven PDE reconstruction."
            },
            "questions": {
                "value": "(1) Can you provide more complex examples in high dimensional space?\n(2) Can you provide some a posteriori analysis analysis regarding the learning performance with respect to the given data or type of PDEs?\n(3) Can you derive some well posed-ness of the parametrization of PDE? i.e, if I provide two similar parameterized PDEs, how about the learning performances regarding interpolation and prediction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper considers the problem of identifying the parameters of a system of PDEs based on measurement data. The proposed method fits the solution of the PDE to the data while adapting its parameters. It uses the adjoint method that combines a data goodness-of-fit objective with a PDE satisfaction constraint using a Lagrangian formulation. Using variational techniques, it is then possible to obtain evolution equations for those weights (multipliers), which then allow the gradient objective to be computed with respect to the PDE parameters. The proposed method is compared with PDE-FIND for a number of PDEs in the standard setting as well as when the temporal data is coarse and when the data is noisy."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed method archives good results on the considered PDEs considered compared to PDE-FIND\n- The experiments consider relevant settings, including coarse temporal resolution, noisy data, and ill-posed tasks\n- Using the adjoint method, it is possible find straightforward algorithms based on gradient descent"
            },
            "weaknesses": {
                "value": "1. The algorithms proposed in the work implicitly (i) solve the forward PDE (1), which can be challenging and time consuming (line 242); (ii) solve the adjoint PDE model (7), which again can be challenging and time consuming (line 243); and (iii) compute gradients of the adjoint variables $\\lambda$, which are only know implicitly through (7) (line 244). The manuscript does not thoroughly discuss how this affects the computational complexity of the proposed method and whether the choice of solution algorithm could affect the performance of the proposed method. This is particularly important since other methods, such as PDE-FIND, do not appear to require such explicit forward evolutions.\nIt is not clear from the manuscript how the proposed method differs from classical adjoint methods, proposed originally in the context of design or shape optimization (see, e.g., Jameson 2003). The text only mentions that \u201cunlike the usual use of PDE-constrained adjoint optimization where the governing equation is known, in this paper we are interested in finding the form along with the coefficients of the PDE given data.\u201d Nevertheless, the parameters of the PDE can be mapped directly to design parameters used in traditional adjoint methods. As such, there doesn\u2019t seem to be any challenge requiring substantial contributions in the setting of this paper.\n\n1. The paper is not clearly written. There are several typos (\u201cEq. equation\u201d in lines 201, 215, 376, 482, 1172) and odd uses of language (\u201cHere, $x(k)$ is coordinates\u201d; ) that make it hard to follow. Many elements are either undefined or defined implicitly/informally. For instance, in (2) are those gradient products or an iterated differentiation (as in multi-index differentials from Sobolev spaces)? What is meant by \u201csemi-discrete total variation of C\u201d (line 176)? The definition is given without a reference or explanation. Additionally, the structure of the experiments is not clear as Section 3 named \u201cResults\u201d is followed by Section 4,5,6 named \u201cPartial observations in time\u201d... all containing experimental results as well. This lack of care in the writing renders the paper confusing for the reader.\n\n1. Many results are provided without derivations or references. Since their definitions are incomplete, it is impossible to judge their validity. For instance, the derivations of (4) and (6) are only informally explained. And since nowhere is it defined in which functional space in which the solution $f$ of (1) must be in, it is impossible to judge the correctness of (4), even if the reader had to assume the smoothness/differentiability of $\\lambda$. Additionally, since conditions such as $\\lambda(x,t) \\to 0$ for $x \\in \\partial\\Omega$ are not justified and without knowing the functional (metric) space in which $\\lambda$ is optimized, it is impossible to guarantee its differentiability in (4) and (6). Similar comments apply to the derivation of (7)-(8) from (6).\n\n1. The manuscript only considers very simple PDEs. In particular, the main text only tackles the 1D heat equation with D=1, the 1D Burgers equation, and a simple 1D wave equation. These are low-dimensional, simple first-order equations with very few parameters to fit. In fact, the proposed method does not appear to be able to handle PDEs beyond linear, first order in time (particularly since it must solve (1) explicitly). In particular, it does not consider very overparametrized models either (large $p$ or $d$). Hence, the performance of the method in PDEs whose solutions are challenging or where there is substantial uncertainty on the model (large $p$ or $d$) is not tested.\n\n1. Point (4) is particularly critical given that the proposed method is only compared against one baseline (PDE-FIND) which is already somewhat dated (2017). Many other baselines are available, in particular, those detailed in the introduction. It is unclear why none were used in the experiments.\n\n1. There is only very limited discussion on the use of l2 regularization. Particularly since it is well-known from polynomial fitting that the l2-norm is not well-adapted to find sparse solutions, i.e., where many values are 0. That is why, e.g., Brunton et al. 2016 use the l1 norm. While the thresholding heuristic employed by the paper is reminiscent of iterative hard thresholding (IHT) methods from compressed sensing (see, e.g., Blumensath and Davis, Applied Computational Harmonic Analysis, 2009) it does not inherit the same guarantees or theoretical advantages. In fact, this relation is not even mentioned in the manuscript."
            },
            "questions": {
                "value": "See Weakness above. We include here some additional minor issues:\n\n1. When considering sensitivity to noise (line 411), the noise variance is specified in percentage. With respect to what? Is it an SNR?\n1. Where does the normalization of the integral in (3) come from? This doesn\u2019t seem to make sense given that the manuscript considers a regular grid.\n1. When discussing the learning rate, it is mentioned that \u201cthe gradient of the cost function is most sensitive to the highest order terms of the PDE.\u201d Why is that the case?\n1. When defining $f^*$ on line 121, shouldn\u2019t it be $f^*: \\Omega \\times [0,T] \\to \\mathbb{R}^N$? As it is, the desired solution is only on $\\mathcal{G}$. How would that be consistent with, e.g., coarser time grids?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "An adjoint-based equation discovery framework is proposed in this paper to discover the underlying governing partial differential equations (PDEs) of physical systems. The proposed method initializes a general PDE by considering derivatives and polynomials of the derivatives up to a finite degree. A loss function is formed by incorporating the error between the prediction from the general PDE and the training labels and the corresponding adjoint equations. After that, a PDE-constrained optimization problem is solved to estimate the parameters of the initial PDE model, such that the solution of the general PDE will accurately approximate the training labels. The successful optimization yields coefficients of only the relevant derivatives to take significant values, thereby uncovering the underlying governing equation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The idea of using adjoint equations to uncover underlying equations is interesting, and it seems to be new in the literature. \n2. The theoretical derivations and analysis do a good job of explaining the key concepts of the framework. \n3. Even in partial observations, when data at fine mesh is not available, the adjoint equations can discretize the general PDE on a finer mesh and, therefore, outperform other methods in the low data limit."
            },
            "weaknesses": {
                "value": "Please see below the comments that need to be further clarified. \n1. The proposed framework is motivated by assuming a general PDE, which contains derivatives and their polynomials up to a certain degree. This is equivalent to basis functions in regression-based equation discovery frameworks, which is one of the major limitations of the basis-dependent discovery methods. Since, in an unknown scenario, the knowledge about the underlying physics will be minimal, one may need to consider a large number of basses/derivatives. In most of the examples, the authors consider only up to 3rd-order derivatives and their polynomials. Authors should consider a higher order of derivatives and polynomials to check the performance of their framework, both accuracy and computational efficiency.\n2. The proposed framework closely resembles the PINN-SR algorithm. Therefore, a comparison with PINN-SR algorithms is necessary to gauge the effect of the adjoint equations.\n3. The motivation for the equation discovery architecture is a little convoluted. The authors state that since data-driven simulators fail to learn the exact physics of dynamical systems, they fail to extrapolate beyond the training regime. Thus, we need an equation discovery framework, which post-discovery can be coupled with any standard numerical methods to obtain accurate predictions. While the second statement is true, it can be argued that after the discovery of a governing equation, one can still train a data-driven or physics-informed machine learning emulator for accelerating computational simulations. Therefore, the motivation needs to be reworked.\n4. The Bayesian class of equation discovery algorithms also does a good job in distilling governing equation equations from data, particularly in noisy and low-data limits [1-3]. This paper should have discussed these frameworks. A comparison with a few such frameworks would further benefit the content of this paper.\n\n[1] Nayek, Rajdip, et al. \"On spike-and-slab priors for Bayesian equation discovery of nonlinear dynamical systems via sparse linear regression.\" Mechanical Systems and Signal Processing 161 (2021): 107986.\\\n[2] Zhang, Zhiming, and Yongming Liu. \"Parsimony-enhanced sparse Bayesian learning for robust discovery of partial differential equations.\" Mechanical Systems and Signal Processing 171 (2022): 108833.\\\n[3] Hirsh, Seth M., David A. Barajas-Solano, and J. Nathan Kutz. \"Sparsifying priors for Bayesian uncertainty quantification in model discovery.\" Royal Society Open Science 9.2 (2022): 211823."
            },
            "questions": {
                "value": "Please see below questions on the paper content:\n1. line 87. PINN-SR uses the Adam optimizer, which can process data in batches. Thus, why do authors feel that PINN-SR will not scale well with the size of the data set?  \n2. line 135. The vector $\\boldsymbol{f}^p$ may be missing a comma.\n3. line 189. Does the framework work only for zero boundary conditions? \n4. line 218. How $\\sigma$ is selected. Consider an example of the Navier-Stokes equation, where the viscosity can be in the order of $10^{-4}$ to $10^{-5}$. In such cases, the proposed algorithm will discard the relevant terms.\n5. line 282. Is this method applicable to time-independent systems like Darcy and Poisson's equations? If applicable, how averaging of gradients in algorithm 2 will be done in the absence of the time component.\n6. Fig. 1(c). It is intriguing to see that the error increases with an increase in data in both methods.\n7. Fig. 1(d). Since the proposed method uses a forward solver in the loop, is it not the computational time supposed to increase with increasing training samples?\n8. Eq. (13-14). The authors show that even in ill-posed problems, the proposed method can provide an alternate sparse equation. However, estimating the error and visually observing the solution fields before concluding on the accuracy would be best. \n\n**Limitations:**\nThe proposed framework does not show discovery from irregular observations (both in time and space). This should be included in the limitations."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}