{
    "id": "qyU5s4fzLg",
    "title": "Improving Unsupervised Constituency Parsing via Maximizing Semantic Information",
    "abstract": "Unsupervised constituency parsers organize phrases within a sentence into a tree-shaped syntactic constituent structure that reflects the organization of sentence semantics. \nHowever, the traditional objective of maximizing sentence log-likelihood (LL) does not explicitly account for the close relationship between the constituent structure and the semantics, resulting in a weak correlation between LL values and parsing accuracy.\nIn this paper, we introduce a novel objective for training unsupervised parsers: maximizing the information between constituent structures and sentence semantics (SemInfo). \nWe introduce a bag-of-substrings model to represent the semantics and apply the probability-weighted information metric to estimate the SemInfo.\nAdditionally, we develop a Tree Conditional Random Field (TreeCRF)-based model to apply the SemInfo maximization objective to Probabilistic Context-Free Grammar (PCFG) induction, the state-of-the-art method for unsupervised constituency parsing. \nExperiments demonstrate that SemInfo correlates more strongly with parsing accuracy than LL.\nOur algorithm significantly enhances parsing accuracy by an average of 7.85 points across five PCFG variants and in four languages, achieving new state-of-the-art results in three of the four languages.",
    "keywords": [
        "unsupervised constituency parsing",
        "information theory",
        "semantic information"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=qyU5s4fzLg",
    "pdf_link": "https://openreview.net/pdf?id=qyU5s4fzLg",
    "comments": [
        {
            "summary": {
                "value": "Unsupervised parsing methods typically involve maximizing sentence log likelihood, which does not correlate well with parsing accuracy. In contrast, this paper uses a paraphrase model to define SemInfo, an alternate objective that involves maximizing a mutual information metric between tree spans and sentence paraphrases. This alternate objective can then be used as a replacement for log likelihood when doing PCFG induction. Experiments show that this objective produces substantially better F1 scores than log likelihood maximization across a wide range of PCFG variants and languages."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "(1) Overall, the paper is strong, and the proposed method is a fundamental improvement in unsupervised parsing. In addition to providing strong results, it provides insight into what is missing in log likelihood maximization and how one might augment log likelihood with other objectives.\n\n(2) The evaluation is very thorough, with 5 PCFG variants and 4 languages, and the method produces very large gains.\n\n(3) The paper is well-written: it is well-organized, does a good job of motivating and explaining the method, and provides useful analysis."
            },
            "weaknesses": {
                "value": "No major weaknesses; some minor weaknesses are discussed below. I support the acceptance of this paper regardless of whether the additional experiments mentioned below are run or not.\n\n(1) From what I can tell, the paper builds upon Chen et al. (2024), which is a simpler instantiation of the idea that spans are likely to be constituents if they tend to be regenerated by paraphrase models. While this is acknowledged in the related works section, perhaps it is also worth acknowledging the influence of their paper in Sections 2 and/or 3 (unless this paper was concurrent).\n\n(2) I would be curious about ablations for each of the additional components of the method (naive substrings -> maximal substrings, the average baseline and entropy regularization in REINFORCE, the addition of the LL term in SemInfo), for both SemInfo as a training objective and when used for MaxTreeDecoding.\n\n(3) While the paper is motivated as an alternate objective for unsupervised parsing, it instead sort of feels like a method for distilling the grammatical knowledge of a large model (in this case, gpt-4o-mini) into a PCFG. From that perspective, the experiments are not that surprising given that we know that large models have mastery of grammar. The promise of the motivation would be better met if the method used a weaker paraphrase method (e.g., something hand-coded) that does not require already \"knowing\" the grammar of the language."
            },
            "questions": {
                "value": "Potential typo: Equation 13: t \\in P(t | x) -> t ~ P(t | x)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a novel method for unsupervised constituency parsing. The method is based on semantic information from text statistics (e.g. tf-idf, and bag-of-words). The main contributions are: i) novel metric to measure the information between syntactic structures and semantic, and ii) grammar induction model based on the semantic metric as a learning objective. The method shows a high correlation between the proposed metric and parsing performance, and the proposed model shows competitive performance compared to the state-of-the-art."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Clear description of background knowledge and related work needed to understand the proposed method.  \n- The authors perform a  comprehensive comparison of the proposed metric with different models and languages."
            },
            "weaknesses": {
                "value": "- Dependence of the model on synthetic data (e.g. gpt) for paraphrasing."
            },
            "questions": {
                "value": "Please address the following questions during the rebuttal:\n\n- Does the proposed model produce different parse trees over multiple runs?  Could the uncertainty impact the performance of the model?\n- Could the method use similar sentences (e.g. nearest neighbours) instead of synthetic data? or other sources of data (e.g. translations, multi-lingual). Could this change have a strong effect on performance? \n- Please speculate on the combination of the proposed semantic metric with contextual embeddings for taking into account paraphrasing. For example: https://aclanthology.org/P15-1030.pdf"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I have no concerns."
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a new approach to learning an unsupervised constituency parser.  The approach works by a novel distillation procedure that leverages an LLM-based paraphrasing model.  The connection between paraphrasing and syntax is nicely motivated by linguistic theory.  Their approach combines a nice mix of old (keyword extraction, conditional random fields) and new (LLMs).  They provide strong empirical results in five languages over state-of-the-art methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper presents a new approach to learning an unsupervised constituency parser.  The approach works by a novel distillation procedure that leverages an LLM-based paraphrasing model.  It is a very clever idea!  The connection between paraphrasing and syntax is nicely motivated by linguistic theory.  Their approach combines a nice mix of old (keyword extraction, conditional random fields) and new (LLMs).  They provide strong empirical results in five languages over state-of-the-art methods."
            },
            "weaknesses": {
                "value": "I found the mathematical exposition imprecise and hard to follow. This paper would benefit significantly from iteration with colleagues to identify and address these issues.  This is my main hesitation about this paper and why I gave it an overall score of 6 instead of 8."
            },
            "questions": {
                "value": "What is the tree probability (line 152) proportional to the sum of potentials? Normally, it is proportional to the product (or exponentiated sum) of potentials.\n\nCan you say a bit more about how the Tree CRF is parameterized? What are the features and grammar?\n\nHow stable are the results of changes in the paraphrasing model? \n\nAt the top of page 6, you suggest that adding log Z term to the objective stabilizes during *early* training; you could better support that claim if you dropped the log Z term after pre-training. Have you considered trying this?\n\nPlease explain in section 3 that the paraphrasing model is taken as a given.  I was confused about that until I got to the experiments section (page 6)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work investigates enhancing the performance of Probabilistic Context-Free Grammar (PCFG) parsers in the Unsupervised Constituency Parsing task by incorporating semantic information into the training objective. To achieve this, the authors use the `gpt-4o-mini-2024-07-18` model as an external paraphrasing tool to generate paraphrases for input sentences, assigning semantic scores to individual phrases based on these paraphrases. The parser is then trained with the Reinforce algorithm to maximize the expected total semantic scores of the induced constituents.\n\nThis work demonstrates non-ensemble non-distilled state-of-the-art (SOTA) unsupervised parsing performance for English, Chinese, and French, along with substantial performance on German. The authors also conduct additional experiments to show strong correlations between the proposed objective and parsing performance, motivating further investigation of this objective function for non-PCFG models in future work."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "* Demonstrates the effectiveness of utilizing semantic information in parsing.\n* Proposes a novel approach for measuring semantic information through paraphrasing.\n* Exhibits significant performance improvements across various setups (different PCFG variants as base models and multiple languages).\n* Includes experiments in four languages: English, French, Chinese, and German.\n* Focuses on model optimization, making it complementary to other efforts in the field.\n* The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "**I would be happy and am strongly willing to increase my rating if most of the following issues, especially the last one, are fixed.**\n\n* The authors frequently use \u201csentence-F1,\u201d \u201ccorpus-level sentence-F1,\u201d and \u201cmean sentence-F1\u201d interchangeably, which may lead to confusion. If I understand correctly, each of these terms refers to the \u201ccorpus-level sentence-F1\u201d defined on line 299: \u201cThe corpus-level score is computed by averaging the sentence-F1 score across the entire corpus.\u201d In prior research, this metric is commonly referred to as the sentence-level F1 score.\n\n    The commonly accepted distinction, as stated in Footnote 8 of [Kim et al. (2019)](https://aclanthology.org/P19-1228/), is as follows: Corpus-level F1 calculates precision and recall at the corpus level to derive the F1 score, whereas sentence-level F1 calculates F1 for each sentence and then averages across the corpus.\n\n    **Suggestions:** Although this paper defines its terminology, it is recommended to (1) avoid using multiple terms for the same concept, and (2) adopt standard terminology to prevent confusion. If the intent is to differentiate between sentence-level F1 scores used in correlation analysis and the average score used in main experiments, a preferable approach might be to use \u201csentence-level F1\u201d and \u201cmean sentence-level F1.\u201d\n\n* The paper claims to be \u201cunsupervised\u201d and compares its results with other \u201cunsupervised\u201d models. However, it relies on an external paraphrasing model that may involve either supervised or unsupervised methods of paraphrasing supervision. If this paraphrasing model is supervised, then the entire approach would more accurately be considered an \u201cindirectly\u201d or \u201cdistantly supervised\u201d method, which is not directly comparable to fully unsupervised baselines. Specifically, the paraphrasing model used here, `gpt-4o-mini-2024-07-18`, incorporates reinforcement learning from human feedback (RLHF; [OpenAI source](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)), which makes it a supervised model due to its reliance on human feedback/labels.\n\n    Additionally, one key motivation for claiming an \u201cunsupervised\u201d approach is applicability to low-resource languages or other structure-prediction tasks where GPT models may not be available. This limitation should be more clearly addressed in the paper.\n\n    **Suggestions:** To support the \u201cunsupervised\u201d claim, I suggest conducting experiments with an unsupervised paraphrasing model. A comparable approach is found in [He et al. (2018)](https://arxiv.org/pdf/1808.09111), where their dependency parser model, which requires POS labels, reports two sets of results: one using gold POS tags and the other using induced unsupervised POS tags. This dual-setup approach acknowledges the compounded error introduced by unsupervised POS tagging, providing a clearer comparison.\n\n* Regarding the paraphrasing model, several important questions and concerns should be addressed:\n    1. Quality Requirements: What level of performance is required from the paraphrasing model to effectively support the proposed method? Is there a minimum paraphrasing accuracy or quality that significantly impacts the results, and how is this threshold determined?\n    2. Robustness to Model Choice: How robust is the proposed model to variations in the paraphrasing model used? For example, would different paraphrasing models (with varying architectures, training datasets, or language capabilities) yield comparable outcomes, or are the results highly sensitive to the choice of the paraphrasing model? This could be demonstrated through experiments with multiple paraphrasing models, both high- and low-performing, to understand the potential variability in results.\n    3. Efficiency in Training: How efficient is the training process, given that it requires multiple calls to the paraphrasing model? Discussing any optimizations or alternatives could strengthen the method\u2019s practicality.\n\n    Each of the above points warrants at least some discussion in the paper.\n\n* There is no need for probability notation in Equation (7); the `P` notation can be safely replaced with an indicator function. It is recommended to avoid probability notation here, as `s \u2208 MS(x^p, x)` is not a random variable, given that `x^p` serves as the expectation argument.\n* In Table 1, the statistical test is conducted using only three samples, which affects its reliability. Since training multiple model instances is resource-intensive, I suggest performing the test at the sentence level. Specifically, consider each sentence\u2019s F1 score as a separate sample, following [Shayegh et al. (2024)](https://aclanthology.org/2024.acl-long.808.pdf).\n* The final argument in Section 5.2, which discusses the performance on the German dataset, could benefit from further elaboration. Currently, the claim lacks sufficient support.\n* **[Minor Weakness]** In Appendix A.2, the correlation is reported at 10k, 20k, and 30k steps. However, Figure 5 shows that LL-SF1 diminishes significantly after 10k steps, while achieving better correlation at earlier stages. Although this does not affect the conclusion that SemInfo-SF1 maintains a higher correlation throughout the entire training process, it would be beneficial to include results for earlier steps in Appendix A.2 for completeness.\n* **[Minor Weakness]** Figure 4 appears to depict a worst-case scenario for LL-SF1 correlation, showing a large negative value. However, Table 2 shows that the correlation is nearly zero for all models and slightly positive on average, which may make Figure 4\u2019s presentation somewhat misleading.\n* The paper introduces PCFG models as the state-of-the-art (SOTA) in the task five times, yet only twice (and neither in the Approach nor Introduction sections) specifies that this refers to non-ensemble SOTA. Additionally, the authors claim to achieve new SOTA results four times without clarifying that this pertains only to non-ensemble models.\n    1. Transparency of Setup: This approach somewhat obscures the actual setup. It should be clearly stated as non-ensemble, and more importantly, **non-distilled** SOTA, especially as works like [Kim et al. (2019)](https://aclanthology.org/P19-1228/), [Cao et al. (2020)](https://aclanthology.org/2020.emnlp-main.389.pdf), and [Shayegh et al. (2024b)](https://arxiv.org/pdf/2310.01717) demonstrate significant performance improvements by distilling induced knowledge into a URNNG model ([Kim et al.; 2019b](https://arxiv.org/abs/1904.03746)), all of which outperform the best results in this paper. The first two works achieve this by distilling the best model (which requires a validation set), and the latter by distilling ensemble knowledge (non-best, thus without a validation set).\n    2. Comparison Justification: It is unclear why the authors chose to compare only with non-ensemble and non-distilled models. This choice is not discussed in the paper. Since distilled models have demonstrated efficiency during inference (Table 5 in [Shayegh et al.; 2024b](https://arxiv.org/pdf/2310.01717)), inference efficiency cannot be the reason. If training efficiency is the concern, then an efficiency analysis on the paraphrasing models is warranted.\n    3. Discussion of Related Work: Even if the authors choose not to compare directly with the mentioned works, these studies are significant enough to merit discussion.\n\n    In general, there is an overemphasis on claiming SOTA. This is unnecessary, as the paper\u2019s novelty and valuable contributions stand on their own and are beneficial to the field. Notably, [Shayegh et al. (2024b)](https://arxiv.org/pdf/2310.01717) show that each individual model with specialized expertise can contribute substantially to final parsing performance when part of an ensemble. Specifically, they take advantage from [Li & Lu (2024)](https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2306.00645&hl=en&sa=T&oi=gsr-r&ct=res&cd=0&d=15034760626359817941&ei=bG4mZ7CxJuG86rQPgN2emQk&scisig=AFWwaeZgwSDS3qGcVtN0rBG9whiK) despite their lower standalone performance, because of its unique approach to Unsupervised Constituency Parsing.\n\n    **Suggestions:**\n    * Instead of heavily emphasizing standalone SOTA status, consider including a correlation analysis with other baselines to demonstrate the unique insights offered by the new model, particularly due to the proposed new objective.\n    * Incorporating the model into the ensemble approach from [Shayegh et al. (2024b)](https://arxiv.org/pdf/2310.01717) and measuring any resulting performance boost could also provide strong support for the model\u2019s contribution."
            },
            "questions": {
                "value": "* All the questions in the 3rd point in weaknesses.\n* Why is the IDF term included in the SemInfo definition? Is there a specific reason for incorporating this term?\n* In Section 5.3.1, why was 30k steps chosen as the evaluation point? Is there a particular rationale behind this choice?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}