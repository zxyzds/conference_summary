{
    "id": "J3H8Az3YlB",
    "title": "RECAST: Reparameterized, Compact weight Adaptation for Sequential Tasks",
    "abstract": "Incremental learning aims to adapt to new sets of categories over time with minimal computational overhead. Prior work often addresses this task by training efficient task-specific adaptors that modify frozen layer weights or features to capture relevant information without affecting predictions on any previously learned categories. While these adaptors are generally more efficient than finetuning the entire network, they still can require tens to hundreds of thousands task-specific trainable parameters even for relatively small networks, making it challenging to operate on resource-constrained environments with high communication costs like edge devices or mobile phones. Thus, we propose Reparameterized, Compact weight Adaptation for Sequential Tasks (RECAST), a novel method that dramatically reduces the number of task-specific trainable parameters to fewer than 50 \u2013 several orders of magnitude less than competing methods like LoRA. RECAST accomplishes this efficiency by learning to decompose layer weights into a soft parameter-sharing framework consisting of a set of shared weight templates and very few module-specific scaling factors or coefficients. This soft parameter-sharing framework allows for effective task-wise reparameterization by tuning only these coefficients while keeping templates frozen. A key innovation of RECAST is the novel weight reconstruction pipeline called Neural Mimicry, which eliminates the need for pretraining from scratch. This allows for high-fidelity emulation of existing pretrained weights within our framework and provides quick adaptability to any model scale and architecture. Extensive experiments across six diverse datasets demonstrate RECAST outperforms the state-of-the-art by up to 3% across various scales, architectures, and parameter spaces. Moreover, we show that RECAST\u2019s architecture-agnostic nature allows for seamless integration with existing methods, further boosting performance.",
    "keywords": [
        "reparameterization",
        "weight decomposition",
        "weight reconstruction",
        "task-incremental learning",
        "transfer learning",
        "lifelong learning",
        "incremental learning"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "A novel weight-decomposition and reconstruction framework to facilitate incremental learning in extremely low parameter settings.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=J3H8Az3YlB",
    "pdf_link": "https://openreview.net/pdf?id=J3H8Az3YlB",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a Neural Mimicry method to study a very small set of parameters for continual learning purposes. It studies both ViT and ResNet applications. It showed performance benefits over several benchmarks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper studies an important task.\n- Motivating towards neural mimic and learnings from proxy domains are helpful in solving the CL tasks."
            },
            "weaknesses": {
                "value": "- Experiments are very small scale. Two target models are only ~21M in size (Ln 305), and does not demonstrate scalability to large models.\n- No forgetting values are presented that are critical aspects of CL learning setups.\n- Fig. 1 is misleading and hard to get the 2*10^-6 parameters - instead this is a ratio instead of parameters. Moreover, it's not thoroughly shown this is a parameter that scales across networks. Would it be the same for LLM? Otherwise stating the method yields <<<1% can be fairly misleading, and in fact wrong if the units are parameters."
            },
            "questions": {
                "value": "As above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the problem of continual learning with the goal of creating a learning pipeline that reduces the number of learnable parameters, making it feasible for resource-limited devices. To achieve this, the authors propose a Neural Mimicry pipeline, which decomposes model weights into a bank of templates. Using these frozen templates, the model only learns coefficients for subsequent downstream tasks. Experimental results highlight the potential of the proposed method. However, I have several questions, and my detailed comments are as follows."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The proposed idea of incorporating Neural Mimicry into continual learning is novel and interesting. By decomposing model weights into frozen template banks and focusing solely on learning coefficients, the approach significantly reduces the number of learnable parameters. This paper introduces a new pipeline and valuable insights for the continual learning community.\n\nExperiments on both CNN and ViT-small models demonstrate the effectiveness of the proposed method and highlight its architecture-agnostic nature."
            },
            "weaknesses": {
                "value": "The writing in this paper could be improved. For the pseudo-code, it would enhance readability to introduce the notations alongside the code for easier understanding. Additionally, more details on how to obtain the pre-trained model weights should be clearly provided.\n\nThe effectiveness of the method on larger models and datasets has not yet been adequately addressed."
            },
            "questions": {
                "value": "How are the pre-trained weights obtained for further Neural Mimicry decomposition? This seems quite important for the algorithm\u2019s performance. If the model is pre-trained on data that differs significantly from the sequential tasks that follow, learning only coefficients may be insufficient, in my view.\n\nIn Table 1, why the total storage for RECAST + Piggyback/CLR increase linearly? For CNN-based models, why can\u2019t this be further reduced, given that the learnable task parameters are already minimized from  $P$  to  $4 \\times 10^{-3} P$ ?\n\nHow does the algorithm perform with larger models and datasets, such as ViT-base/Large with the ImageNet dataset? Would the Neural Mimicry method still be effective? Further discussion on this would be beneficial.\n\nIn Table 2, why does the total storage remain unchanged? As more tasks are added, shouldn\u2019t total storage also increase with  $M + T \\times 6/8 \\times 10^{-4} Pr$ ?\n\nEfficiency in training/learning on edge devices is also important. It would be highly valuable to include a comparison of wall-clock training time and GPU memory in Tables 1 and 2. Given the significantly reduced number of learnable parameters, the proposed method should enhance training efficiency, which could be seen as an advantage of this approach, but empirical evidence is needed to confirm this.\n\nIt would also be helpful to report the training complexity of the Neural Mimicry process.\n\nFinally, in Table 1, why do the accuracies for EWC, LWF, and GDUMB perform so much worse than ResNet-34?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this work, the authors propose a parameter efficient task incremental learning method. Specifically, the proposed method reparameterizes the original model by using pre-trained template banks and lightweight learnable coefficients for each tasks during incremental learning. In the experimental results, the proposed method shows better accuracy with less trainable parameters when combining with previous incremental learning methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed method can reduce learnable parameter for incremental learning.\n\n2. In the experiments, the proposed methods shows better accuracy with less trainable parameters when combining with previous incremental learning methods."
            },
            "weaknesses": {
                "value": "1. The paper is not well written and not well organized. For example:\n    \n    (1) The proposed method and related works are mixed together in Section 2.\n    \n    (2) Subsection 2.1 introduces too many concepts from later sections, making it difficult to follow.\n   \n    (3) The opening paragraphs of Subsections 2.1 and 2.2 are largely redundant.\n\n     (4) The overview of the proposed RECAST method (Fig. 2) should be introduced at the beginning of Section 2, but it isn\u2019t presented until Subsection 2.2.\n\n2. The proposed RECAST method itself does not seem directly related to task incremental learning.\n\n3. The paper lacks sufficient background information, especially on relevant works like Template Mixing methods, making it difficult to assess the novelty of the approach.\n\n4. It is unclear how RECAST integrates with previous incremental learning methods, as shown in Tables 1 and 2."
            },
            "questions": {
                "value": "Overall, this work appears to introduce a new parameter-efficient task incremental learning method, which significantly reduces the number of learnable parameters while improving accuracy when combined with previous incremental learning techniques. However, the current version has presentation quality issues, and the contribution of the technical aspects remains unclear. My specific questions are as follows:\n\n1. What are the technical differences between the proposed RECAST and previous Template Mixing methods? For example:\n   - How does the design of template banks and coefficients differ in RECAST?\n   - How does RECAST support incremental learning with substantially fewer learnable parameters, whereas Template Mixing methods do not achieve this?\n\n   It's essential to clarify the specific differences to highlight the novelty of the proposed method.\n\n2. How does RECAST balance model plasticity and stability in incremental learning? From my understanding, RECAST reparameterizes the model using lightweight coefficients and frozen template banks for each downstream task, making it a parameter-efficient fine-tuning method rather than one specifically designed for incremental learning. In the experiments, RECAST is combined with existing incremental learning methods to achieve better performance. If this understanding is correct, it would be useful to compare RECAST with other parameter-efficient fine-tuning methods, such as prompt-tuning, LoRA, and DoRA.\n\n3. How does RECAST integrate into existing incremental learning methods, as shown in Tables 1 and 2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns found."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "RECAST is a Task Incremental Learning (TIL) method that efficiently reparameterizes model components to emulate a pretrained target network by decomposing weights into shared templates and module-specific coefficients. The shared templates, stored in a template bank, match the shape of each layer and remain fixed across tasks, while the module-specific coefficients are fine-tuned for each new task. \n\nThrough a process called Neural Mimicry, RECAST selects a target model and combines the templates and coefficients to approximate it. The approach is architecture-agnostic and requires minimal parameter updates across tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Clear presentation of results, focusing on both parameter efficiency and storage alongside accuracy. \n- The method enhances existing adapter-based methods, making it compatible and valuable as a supplementary tool. \n- It is versatile across architectures, with applicability to both CNNs and Transformers. \n- The setting is flexible, allowing adjustment of groups *G* and templates *n* to achieve the desired level of model compression."
            },
            "weaknesses": {
                "value": "I recommend adding another baseline based on rehearsal, such as ER[1], ER-ACE[2], or DER++[3]. While the proposed method is more aligned with adapter-based approaches, claiming strong performance in settings utilizing a memory buffer without including one of these established baselines for comparison seems somewhat unfair. \n\nI understand that the accuracy reported is averaged across the six datasets used at the end of training. However, I'm curious if accuracy varies between tasks encountered earlier in training and those seen later. Including a table that shows accuracy results for individual tasks would strengthen the paper\u2019s validity. This would also relate to Question 1 regarding the influence of task order. \n\n\n    [1] Jeffrey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software (TOMS), 11(1):37\u201357, 1985 \n    [2] Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuytelaars, Joelle Pineau, and Eugene Belilovsky. New Insights on Reducing Abrupt Representation Change in Online Continual Learning. In International Conference on Learning Representations Workshop, 2022. \n    [3] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark Experience for General Continual Learning: a Strong, Simple Baseline. In Advances in Neural Information Processing Systems, 2020."
            },
            "questions": {
                "value": "1.  Does the task order adhere to the sequence in which the datasets are presented in the text (lines 294-296), or does averaging across three different runs involve altering the task order?  \nI believe exploring the latter could be an intriguing study on how task order impacts weight adaptation and whether the learned coefficients are affected by such variations.  \n\n**About LoRA and Figure 3**\n\n2. It\u2019s not entirely clear what is being evaluated in Figure 3. \nWhile a comparison with LoRA is certainly necessary in this work, could you clarify what \"CF\" stands for in the figure? \n\n3. Additionally, rather than comparing to LoRA-only, it might be more informative to use the same baseline using LoRA in one case and RECAST in the other, e.g., fine-tuning the same backbone with the two methods.\nIt\u2019s possible that Figure 3 already presents this setup, as it seems ViT-Small is used as the backbone and lower bound. If this is indeed the case, the figure caption and related section would benefit from clarification, with additional emphasis on these details to enhance interpretability.\n\n4.  Furthermore, the comparison does not appear to fully explore the parameter range for both methods; although the x-axis in the plot represents the number of parameters, the methods are not evaluated across the entire range, and a comparison with the same number of parameters is only partially explored. Could you clarify the reasoning behind this?\n\n**CL Setting**\n5. Is there a specific reason why the authors have chosen to focus exclusively on the Task Incremental Setting, rather than exploring the more challenging Class Incremental Setting?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}