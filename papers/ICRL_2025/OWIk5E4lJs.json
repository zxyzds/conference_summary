{
    "id": "OWIk5E4lJs",
    "title": "Interactive-Action Image Generation via Synthetic Physical Priors",
    "abstract": "While diffusion-based text-to-image generation has made notable advancements, generating accurate images containing interactive actions remains a challenge due to the lack of inherent physical and spatial priors. To address this problem, we propose a novel pipeline that synthesizes a dataset enriched with physical priors using a graphics engine, combined with a captioning technique. Building on the dataset, we introduce a distillation-structured fine-tuning method, where a teacher network assists in inverting the semantics of interactive actions, leveraging the synthesized priors effectively. This fine-tuning method disentangles the synthetic data features while mitigating random misalignment during the fine-tuning process. Extensive experiments demonstrate that our method not only achieves state-of-the-art results but also highlights the synthetic data's potential to be applied more broadly in enhancing the generation of interactive action images.",
    "keywords": [
        "synthetic data",
        "interactive action",
        "generative model"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=OWIk5E4lJs",
    "pdf_link": "https://openreview.net/pdf?id=OWIk5E4lJs",
    "comments": [
        {
            "summary": {
                "value": "This work focuses on the interactive actions image generation based on pretrained text-to-image diffusion models, where a customized synthetic dataset with interactive actions is provided to finetune the pretrained model to improve the spatial and interactive action aware diffusion models.The finetune stage is akin to distillation procedure to address the problem of disentanglement and text-visiual misalignment. The experimental results demonstrate the effectiveness, especially the interactive actions between objects, of the proposed model."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is easy to follow. This work clearly identifies the weakness of generating interactive actions in current t2i models and proposes a effective solution to address the limitation.\n- The method design is well-motivated, including the usage of curated dataset and the specific distillation training framework to take advantage of both pretrained prior and the new learned knowledge."
            },
            "weaknesses": {
                "value": "- While this work demonstrates the effectiveness on several interactive actions, it is unclear how well the method generalizes to a wider range of more complex interactions. Moreover, is the model capable of dealing with actions related to more than two objects.\n- The motivation of IAScore is clear, however, its reliance on detection introduces potential errors. A more detailed analysis of the reliability and limitations of this metric would make it more credible."
            },
            "questions": {
                "value": "Please refer to the weakness part for more details."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper first points out that existing diffusion models tend to struggle with verbs that describe relationships between objects. To address this, the authors propose a method for fine-tuning these diffusion models to better capture action-related information. They generate synthetic data representing actions using robot simulation model, SAPIEN, and train a special token following a textual inversion framework. To account for the domain gap between synthetic and real images, they employ a distillation learning approach for fine-tuning. The proposed method shows improved results compared to previous studies on the evaluation benchmark designed by the authors."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The use of robot simulation to generate datasets for certain actions is an interesting and innovative approach.\n- The paper clearly explains the challenges encountered during the development and the method is well described without ambiguity.\n- The authors introduce new metric for evaluating the proposed task, and clearly present comparison results with previous studies."
            },
            "weaknesses": {
                "value": "- The proposed method is limited to simple actions that can be performed in robot simulations, such as picking up or pulling something. More complex actions like tickling or shaving cannot be represented in this way, making it difficult to train on broader action concepts.\n- The use of distillation learning increases the training load compared to previous studies."
            },
            "questions": {
                "value": "Regarding the weakness mentioned above, how can a dataset be generated for more complex actions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method to improve the capacity of text-to-image diffusion models to generate interactions by fine-tuning the model on synthetic data. To address the domain gap between the synthetic data and real images, the author developed a distillation approach to preserve the performance of the mode in generating the realistic style while still learning the interactions from the synthetic data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper studies the interesting and important problem that visual generative models lack the proper knowledge about the physical world and are not able to properly generate physical-plausible actions. The authors propose reasonable fine-tuning-based methods and collect synthetic data from a simulator to address this problem. The authors show that the existing methods do not work well due to the synthetic-real domain gap. Ablation studies are performed to help understand the function of each component."
            },
            "weaknesses": {
                "value": "My major concern about this paper is its motivation to improve the ability of text-to-image models to generate human/animal-object interactions. With the improved performance of the base generation, I have been seeing this as less and less of a problem. In many of the examples shown in the papers where SD1.5 fails to generate correct interactions, the latest models, like SD3 and Flux, can do pretty well. That being said, I do believe that there are many actions that cannot be accurately described by the language. However, I don't see current methods as a solution to that, as there are cases where the learned motions are not clearly matched with the training images. For example, in the third row of the teaser image, the robot arm picks up the objects with two hands, while the generated images mostly contain a subject holding an object with one hand. Also, I also have the following concerns:\n\n1. There are some conflicts between the different losses, i.e., the distillation loss would conflict with learning the interaction through the synthetic data, which is also verified by the ablation study in Figure 8 (c). The optimal trade-off between two losses may not be the same for different interactions.\n\n2. The issue with overfitting the synthetic data has been studied, and the function of the proposed distillation loss is quite similar to the class-specific prior preservation loss. Although the authors claim that real data is not common, robot manipulation datasets like BridgeDatav2 and DROID datasets do exist. \n\n3. There exists another gap that the interaction between the robot arm and objects is quite different from how humans interact with objects - the robot arm typically does not have a similar structure compared with human hands. Instead, one could just collect more human-object interaction data. And there do exist such datasets, like Epic Kitchen, Hands23, and Ego4D."
            },
            "questions": {
                "value": "I do not have additional questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a method to enhance diffusion-based text-to-image generation for interactive actions by synthesizing a dataset with physical priors using a graphics engine. It introduces a distillation-based fine-tuning approach that improves understanding of interactive actions and reduces misalignment. Experiments show that this method achieves state-of-the-art results and expands the use of synthetic data in generating interactive images."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper propose a novel method that synthesizes a dataset enriched with physical priors using a graphics engine.\n2. This paper introduce a distillation-structured fine-tuning method using SD.\n3. Qualitative results are good compared to the baselines."
            },
            "weaknesses": {
                "value": "1. It is challenging to evaluate this as a new approach. However, the authors propose a new metric, the IAS score, and evidence is needed to demonstrate that this metric in the evaluation.\n2. There is a lack of quantitative analysis for the ablation studies. (such as, preposition word ..)\n3. The authors released the code in the supplementary materials. Is it possible to view more samples of the dataset or generated results?\n4. I am curious about if SD-XL or other models were used instead of SD1.5."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the issue of text-to-image generation models struggling to accurately depict interactive actions (e.g., \"a person picks up a cup\"). To address this, the authors generate a synthetic dataset enriched with physical constraints using a graphics engine. They train the model through a teacher-student network with distillation learning, enhancing the model's ability to capture spatial and interactive relationships. The approach reduces alignment issues between subjects and objects, improving realism in generated images."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The distillation-based method is a straightforward yet reasonable approach.\n* The authors address a challenging task effectively.\n* They propose a new metric for evaluation."
            },
            "weaknesses": {
                "value": "* Limited Interaction Complexity: The paper focuses on single-object manipulation, which seems somewhat limited in scope.\n* The qualitative results seem somewhat lacking. It seems this issue should be addressed through the proposed distillation method."
            },
            "questions": {
                "value": "* In the paper, they compare performance with ReVersion fine-tuned on their dataset, which is fair; however, the original ReVersion paper also demonstrates strong performance. For a more comprehensive comparison, the authors should include additional experimental results using the original (non-fine-tuned) ReVersion model.\n\n* The ablation study shows that 'and' improves semantic understanding, while 'or' enhances visual representation. What is the reason for this conflict?\n\n* Can the model perform in more complex interactions, such as multi-object manipulation or tool use (e.g., hammering a nail, tightening a screw with a screwdriver)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}