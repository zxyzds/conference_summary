{
    "id": "FbZSZEIkEU",
    "title": "Adaptive Circuit Behavior and Generalization in Mechanistic Interpretability",
    "abstract": "Mechanistic interpretability aims to understand the inner workings of large neural networks by identifying \\textit{circuits}, or minimal subgraphs within the model that implement algorithms that are responsible for performing specific tasks. The *indirect object identification (IOI)* task serves as a key benchmark for circuit discovery, largely due to the existence of a manually derived circuit in GPT-2 that appears to implement a clear and interpretable algorithm. This circuit was identified using a particular format for the input prompt, though the circuit itself appears to implement a far more general algorithm that should apply to a much larger set of prompts. In this paper, we investigate how the IOI circuit generalizes to prompt variants specifically designed to evaluate the validity of the algorithm implied by the circuit. Surprisingly, we find that the IOI circuit maintains high performance on variants of the IOI prompt where the implied algorithm should fail. This raises a critical question: *Does the IOI circuit truly implement such a simple algorithm, or do underlying mechanisms modulate its behavior in order to generalize to different prompts?* Our analysis reveals that the functionalities of individual attention heads in the IOI circuit remain largely intact even on prompt variants for which the proposed IOI algorithm would fail. We focus on one such variant named DoubleIO and discover a circuit that reuses all heads and paths in the IOI circuit while adding only 9 additional edges. Our findings indicate that circuits within LLMs may be more flexible and general than previously recognized, underscoring the importance of testing specific aspects of their behavior to unveil the full extent of their capabilities.",
    "keywords": [
        "Mechanistic Interpretability",
        "Circuit Generalization",
        "Science of ML"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We test the IOI circuit in GPT-2 on prompt variants that violate its assumptions, leading to notable deviations from both the proposed mechanism and the full model. However, it is also flexible and regains faithfulness to the model by adding 9 edges.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=FbZSZEIkEU",
    "pdf_link": "https://openreview.net/pdf?id=FbZSZEIkEU",
    "comments": [
        {
            "summary": {
                "value": "The authors study the IOI circuit introduced by Wang et al (2023). [[1]](https://arxiv.org/pdf/2211.00593#page=4.37) They notice that the hypothesized algorithm implemented by the circuit would not work in cases where the IO token appears twice in the prompt (DoubleIO prompt). However, they note that the model and circuit can in fact predict the correct token on the DoubleIO prompt.\n\nThe authors hypothesize the circuit is able to perform well on the DoubleIO prompt by \"S2-hacking\". That is, having the Induction, Previous Token and Duplicate Token heads pay extra attention to the second instance of the S2 token, thereby causing the S-inhibition heads to suppress the prediction of this token and thus predict the IO token more strongly.\n\nThey further identify a circuit for the DoubleIO prompt using activation patching and find that it has significant overlap with the original IOI circuit and reuses many of the same components."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Evaluating on prompt variants intended to test specific aspects of the operation of the circuits is a good idea.\n\nIt is a useful finding that the IOI circuit components are often reused in the DoubleIO prompt formats.\n\nThe authors present some interesting experimental results demonstrating the similarity of the functions of the attention heads in the IOI circuit between the IOI and DoubleIO prompts."
            },
            "weaknesses": {
                "value": "- The experimental reports are lacking in many details about the experimental methodology, making it difficult to be confident that the claims are robust.\n- The explanations throughout the paper should be clearer to fully communicate the ideas and experiments of the authors.\n- The S2 hacking hypothesis is quite vague and the author do not present any deep understanding that would explain the mechanisms by which certain attention heads pay extra attention to the S2 token.\n- In the experiments on the DoubleIO and other prompt variations, it is unclear at which token positions paths are being ablated, as this is unspecified by the original circuit.\n- The authors write: \u201cGiven the algorithm inferred from the IOI circuit, it is clear that the full model should completely fail on this task\u201d. However this is a misunderstanding of the original work. The IOI circuit was discovered using mean ablations that keep most of the prompt intact. Therefore Wang et al. don\u2019t expect it generalize to different prompt formats.\n- The authors write \u201cIn the base IOI circuit, the Induction, Duplicate Token, and Previous Token heads primarily attend to the S2 token\u201d this is incorrect according to Section 3 of Wang et al., 2023. These heads are _active_ at the S2 token, but do not primarily attend to it.\n- The authors write: \"The proposed IOI circuit is shown to perform very well while still being faithful to the full model\" In fact, the IOI circuit is known to have severe limitations, as shown in concurrent work by Miller et al. (2024) [[2]](https://arxiv.org/abs/2407.08734).\n\nNitpicks:\n- In Figure 2, it is not clear what Head 1, 2, 3 and 4 refer to.\n- The paper should include Figure 2 from Wang et al. 2023 [[1]](https://arxiv.org/pdf/2211.00593#page=4.37) to make it easier to follow discussions about the circuit."
            },
            "questions": {
                "value": "The authors write: \u201csince the S2 token is the only input with a path to the END token through the Induction, Duplicate Token, and Previous Token heads, these heads do not need to attend to any of the other input tokens\u201d I don't understand what exactly this means. Why do the other heads not need to attend to any other input tokens?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores how IOI circuits respond to new modified prompts specifically crafted to disrupt the original pre-defined circuits' functionality. These variations introduce two new sets of IOI prompts: Double IO and Triple IO, named according to the frequency of IO occurrences within the prompts. The study reveals that, while the circuit structures remain largely consistent, their functionalities exhibit slight variations. As the functionality of nodes in the IOI circuits are mainly discussed under the settings of the original IOI prompt, it is not surprising to see the variations in functionality when prompts are different."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The discussions in this paper are comprehensive and closely follow the structure presented in \"Interpretability in the Wild.\" Emphasizing the limitations in estimating the functionality of IOI circuits is crucial for advancing the field of interpretability. Such an emphasis underscores the challenges in understanding how these circuits function across varied prompt structures, particularly when circuit consistency does not guarantee consistent functional behavior."
            },
            "weaknesses": {
                "value": "- The evaluations on circuits discovered on IOI double and IOI triple is extremely trivial. The logit difference is not even close to the full model. It is hard to be convinced that these circuits are even faithful and accurate to reflect how the model actually perform such tasks. These also diminish the credibility and validity of the findings from later parts. The logit difference of circuit should at least recover 80% of that from the full model to be considered as faithful and accurate. \n- There is no empirical evidence to show that the originally designed functionality (algorithm) of IOI would fail on the two new prompts. Though I like the idea, it is hard to convince with no evidence that the designed prompts are able to satisfy the purpose. I suggest to run some simulations. \n- Since the circuits are found by hand with path patching, and the logit difference varied significantly from the full model, it is hard to justify if the circuits found are served mainly as to make some specific conclusions in the paper such as 'the circuits are largely reused in different prompt settings'.  Showing KL and logit difference together while being close to the full model would be preferred."
            },
            "questions": {
                "value": "- What is the KL divergence of the three circuits for IOI, double and triple IO. \n- Is the functionality of nodes in this paper mainly focus on the change in logit difference similar to IOI paper. If so, why? How about based on change in overall logit distribution? \n- Please refer to weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper evaluates the indirect object identification (IOI) circuit in GPT-2 using variant prompts (DoubelIO and TripleIO). These variants introduce additional objects (IO), thus expected to disrupt the identified circuit\u2019s performance. The authors found that the circuit maintains high performance due to a \u2018S2 hacking\u2019 mechanism, where the head defaults to the correct token. The base IOI circuit is largely reused for the variant prompts."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Originally: Evaluated the IOI circuit using variant prompts, providing new insights into its robustness.\n2. Quality: The approach with the DoubleIO variance is motivated and effective. \n3. Clarity: The writing is clear and easy to understand. \n4. Significance: demonstrated that the IOI circuits are more adaptable than previously understood."
            },
            "weaknesses": {
                "value": "1. The paper could benefit from a clearer discussion of its motivation and broader context. The authors highlight \"circuit analysis as a promising angle.\u201d Is it expected that a \"simple, human-interpretable algorithm\" exists for any task? So far, the study feels narrowly confined to a specific problem. The authors can discuss how the approach in the paper might generalize to more complex tasks. One potential way to address this is to relate the findings to more general problems, such as in NLP, recognizing the relation between subjects in a sentence.\n2. The paper relies heavily on differences in attention scores to evaluate circuit functionality, but it lacks a thorough justification for this choice. Is it possible for the same functionality to be represented by different attention patterns? Alternatively, could similar attention scores result in different interpretations of functionality? A clearer explanation of the link between attention and underlying mechanisms is needed. \n3. Some statements lack contextual clarity, such as the \"first demonstration of generalization via circuit reuse.\" Providing concrete context or evidence for each claim would enhance rigor and clarity."
            },
            "questions": {
                "value": "1. How might the proposed circuit-based approach manage tasks that involve multiple subjects or ambiguous variables? For example, are there instances where tokens other than the IO or S have the highest probability? If so, would the ratio score introduced in the paper sufficiently capture the model\u2019s mechanism in these cases, or would normalizing by the highest predicted token likelihood be necessary?\n2. The current approach relies on identifying the functionality of attention heads by using predefined target tokens. Could you elaborate on its adaptability to tasks where reasoning components are less clearly defined, or multiple relevant variables may need to be considered?\n3. Minor Point: Please consider reporting variances for all values presented, such as in Figures 7 and 8, to give a more comprehensive view of the data."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates circuit reuse in language models.\nIt does so in the context of the IOI task and its circuit. \nIn particular, it proposes 2 variants to the standard task which follow a similar \nstructure to the original, and then proceeds to the analyze the circuits/behavior of the\nmodel for these variants. \nIn these investigations it finds two different things.\n1. The standard IOI circuit has really good performance on these tasks despite it being unfaithful.\nThe authors explain this behavior by a phenomenon they call S2-Hacking. In this phenomenon \nthe S2 token, via S-inhibition heads + incorrect the mean ablation, ends up artificially pushing \nthe name mover heads to return the IO token.\n2. The circuit for the DoubleIO tasks is very similar to the circuit in the standard IOI task.\nThe authors discover this by applying standard path-patching techniques to discover the new circuit and comparing it to the original. \nThis observatin is important as it shows that circuits are \"robust\" \nand are shared in similar enough tasks."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- I think the paper was presented in a very clear manner, and the authors did an excellent job of explaining the methodology and their ideas.\n- Their methodology seems adequate, as it follows standard practices for circuit discovery.\n- I appreciated that the authors investigated the S2-Hacking issue in detail, as I believe this sort of problem can lead to \"interpretability illusions\" where we might be deceived into thinking a circuit is appropriate when, in reality, it is simply a quirk of the methodology. (I don\u2019t think this fits precisely into that category, as the circuit was quite unfaithful, but it remains an interesting finding.)\n- As the authors mention, to the best of my knowledge, this is one of the first careful looks at circuit reuse in LLMs."
            },
            "weaknesses": {
                "value": "- In my view, the main weakness of this paper is its scope. Although I think the authors did an excellent job presenting their results, I believe that it is necessary to validate the conclusions with more circuits. I understand that several papers only focus on one specific circuit (the IOI paper being a prime example). However, as the techniques presented in the paper are not new and not necessarily its main strength, I believe that more empirical validation should be provided to strengthen the paper's thesis. For example, I would like to see similar analysis (which could be automated) for other circuits, such as Greater-Than [1] or Docstring [2]. I think this would greatly strengthen the paper. Otherwise I think the contributions, despite the excellent presentation and great use of the methodology, might not be enough. \n\n\n\n[1] M. Hanna, O. Liu, and A. Variengien. *How Does GPT-2 Compute Greater-Than*. Interpreting Mathematical Abilities in a Pre-Trained Language Model, 2:11, 2023.\n\n[2] S. Heimersheim and J. Janiak. *A Circuit for Python Docstrings in a 4-layer Attention-Only Transformer*. Alignment Forum, 2023. URL: [https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only](https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only)"
            },
            "questions": {
                "value": "Nitpicks:\n-  In paragraph 4 of section 4, should we have S2 rather than S? \"These heads then feed into the S-Inhibition heads, which will always suppress attention on the S tokens as a result and push the Name Mover heads toward returning an IO token.\"\n- Have the authors attempted to analyze other circuits in a similar way? If so what have been their findings? \n- Are the authors aware of other instances of other \"illusions\" similar to S2 hacking?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}