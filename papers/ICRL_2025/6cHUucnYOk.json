{
    "id": "6cHUucnYOk",
    "title": "Escaping the Big Data Paradigm in Self-Supervised Representation Learning",
    "abstract": "The reliance on large-scale datasets and extensive computational resources has become a significant barrier to advancing representation learning from images, particularly in domains where data is scarce or expensive to obtain. In this paper, we address the critical question: Can we escape the big data paradigm in self-supervised representation learning from images? We introduce SCOTT (Sparse Convolutional Tokenizer for Transformers), a simple tokenization architecture that injects convolutional inductive biases into Vision Transformers (ViTs), enhancing their efficacy in small-scale data regimens while remaining compatible with Masked Image Modeling (MIM) tasks. Alongside, we propose MIM-JEPA, a Joint-Embedding Predictive Architecture within a MIM framework, operating in latent representation space to capture more semantic features. Our approach enables ViTs to be trained from scratch on datasets orders of magnitude smaller than traditionally required --without relying on massive external datasets for pretraining. We validate our method on three small-size, high-resoultion, fine-grained datasets: Oxford Flowers-102, Oxford IIIT Pets-37, and ImageNet-100. Despite the challenges of limited data and high intra-class similarity, our frozen SCOTT models pretrained with MIM-JEPA significantly outperform fully supervised methods and achieve competitive results with state-of-the-art approaches that rely on large-scale pretraining, complex image augmentations and bigger model sizes. By demonstrating that robust off-the-shelf representations can be learned with limited data, compute, and model sizes, our work paves the way for computer applications in resource constrained environments such as medical imaging or robotics. Our findings challenge the prevailing notion that vast amounts of data are indispensable for effective representation learning, offering a new pathway toward more accessible and inclusive advancements in the field.",
    "keywords": [
        "Representation Learning",
        "self-supervised learning",
        "data efficiency",
        "computer vision",
        "SCOTT",
        "MIM-JEPA",
        "Joint-Embedding Predictive Architecture",
        "Masked Image Modeling"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "We introduce SCOTT and MIM-JEPA, enabling Vision Transformers to be trained from scratch on small datasets\u2014significantly outperforming supervised methods and challenging the big data paradigm in self-supervised learning.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=6cHUucnYOk",
    "pdf_link": "https://openreview.net/pdf?id=6cHUucnYOk",
    "comments": [
        {
            "title": {
                "value": "Response to Reviewer 6LsZ"
            },
            "comment": {
                "value": "We would like to thank Reviewer 6LsZ for their valuable review. We would like to address the weaknesses statements in order to reinforce the understanding on the importance and novelty of our work:\n\n__W1 Distinction between SCOTT and SparK.__ As noted in the paper, SparK introduced the use of sparse convolutions to enable CNN architectures for masked image modeling (MIM) tasks. However, SparK and our work differ significantly in both architectural design and learning framework:\n\n- __Architecture:__ SparK proposes a fully convolutional encoder-decoder architecture reminiscent of UNet, which is far from SCOTT, a shallow CNN tokenizer to replace the ViT ineffiecient patch-and-embed. We leverage on SparK contributions to design sparse layers of the SCOTT tokenizer. While introducing SCOTT might seem obvious to improve efficiency of ViTs this should not be taken for granted, to the best of our knowledge there is no prior work to propose a CNN-like tokenizer for ViTs that is compatible with MIM tasks, nor to demonstrate its effectiveness. This is indeed our contribution; we are the __first__ to propose to replace the patch and embed strategy of ViTs by a shallow convolutional tokenizer that is compatible with MIM tasks to enable efficient training and prove it.\n\n- __Learning Framework:__ Spark trains the UNet in a BERT style generative framework, where the task is to predict the masked input signal. In contrast, we propose to train a ViT enabled SCOTT model in a MIM-JEPA framework, which is not generative and targets are generated by a momentum-based target network in abstract representation space, where the noise present in the input signal is potentially eliminated.\n\nIn summary, while SparK contributed foundational ideas around sparse convolutions, SCOTT represents a distinct approach designed to integrate sparse convolutions within ViTs in an SSL context, making it uniquely suited for efficient training on small, fine-grained datasets. \n\n__W2 Concerns on comparability of experimental setting.__ The primary goal of our work is to propose a method that enables __effective representation learning on small-scale, fine-grained datasets without requiring extensive data or computational resources.__ With this objective, our experimental design focuses on assessing our proposed contributions -architecture (SCOTT) and learning framework (MIM-JEPA)- to determine their capability to perform competitively with limited data and compute, compared to state-of-the-art methods that depend on large-scale resources for pretraining.\n\nGiven this aim, we selected baselines that represent state-of-the-art (SOTA) performance across different learning paradigms, as we believe this context provides a meaningful basis for evaluating our results. Specifically, we included: \n- __Fine-Tuned Vision Transformers (ViTs) from Supervised Pretraining:__ These models represent the top performance achieved in fully supervised setting (i.e., ViT, and SparseSwin).\n- __Self-Supervised Learning (SSL) Pretrained ViTs:__ We also include SSL-pretrained ViTs (e.g., DinoV2, I-JEPA) to establish a baseline performance of SOTA self-supervised models pre-trained on large datasets.\n\nWe would like to emphasize that our method operates at a clear disadvantage relative to these baselines, as SCOTT variants (i.e., SCOTT-7) use smaller model sizes and pre-train on much smaller datasets. Despite these constraints, SCOTT achieves performance comparable to larger-scale methods, which we believe highlights the strength of our approach and reinforces the contributions of our work, rather than detracting from the comparability of the experiments.\n\n__Reviewer 6LsZ : \u201cFor example, experiments can be added to utilize Dino/I-JEPA or other pre-training paradigms to train on the small dataset and compare it with the proposed method.\u201d__\n\nWe note that directly pre-training Dino or I-JEPA on small datasets could offer valuable insights. However, such experiments are computationally intensive and may not reflect optimal training conditions for the standard ViT models which are data thirsty. For instance, Dino contrastive learning objective relies on the forward pass of many different local and global views, resulting in substantial GPU memory consumption (which is beyond our current available resources :-( ).\n\nWe thank Reviewer 6LsZ for their valuable feedback and the opportunity to clarify our work\u2019s distinctions and objectives. We believe our method\u2019s ability to achieve competitive performance on small-scale datasets without large-scale resources is a meaningful advancement in accessible self-supervised learning. We hope that our responses address the reviewer\u2019s concerns and reinforce the novelty and impact of our contributions, and we respectfully encourage the reviewer to consider reevaluating our work in light of these clarifications. We are looking forward to the reviewer\u2019s response and appreciate their time and consideration. We look forward to the Reviewer's response."
            }
        },
        {
            "title": {
                "value": "Response Continuation to Reviewer BkxW"
            },
            "comment": {
                "value": "__Q1. Specific advantages of CNN biases over Augmentation techniques.__ Introducing convolutional biases is a more fundamental approach to improving data efficiency compared to augmentation techniques. Augmentation techniques expand the dataset by applying transformations, but they do not enhance the model\u2019s inherent capacity to generalize from the data structure itself. In contrast, -extracted from the original manuscript- \u201cCNNs, inspired by the hierarchical processing of the mammalian visual cortex (Hubel & Wiesel, 1959; Fukushima, 1988), provide important priors for learning spatial relationships in visual data.\u201d\n\nMoreover, designing effective augmentation strategies is often domain-specific and requires expert knowledge, especially outside of the natural images domain. Convolutional biases, however, are inherently compatible with the spatial structure of image data, making them broadly applicable and effective across various domains.\n\nWhile convolutional biases provide this general advantage, they remain fully compatible with augmentation techniques, and the two approaches can be complementary in cases where further data transformations may enhance performance.\n\nWe appreciate Reviewer BkxW's thoughtful feedback and valuable recognition of our contributions. We hope that our responses clarify key aspects of our approach, particularly the unique advantages of SCOTT and MIM-JEPA in advancing data-efficient learning beyond reliance on extensive pre-training datasets. Given these points, we respectfully encourage the reviewer to consider updating their initial evaluation, as we believe these clarifications highlight the relevance and potential impact of our work. Thank you once again for your time and constructive insights. We look forward to the Reviewer BkxW's response."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer BkxW"
            },
            "comment": {
                "value": "We thank Reviewer BkxW for the helpful comments and valuable review, especially for recognizing that our key contributions --SCOTT and MIM-JEPA-- can shift away the reliance on extensive pre-training datasets, which is the main goal of our work. We appreciate the reviewer\u2019s acknowledgment that our method outperforms fully supervised approaches and achieves results competitive with state-of-the-art models pre-trained on much larger datasets.\n\nHowever, given these positive insights, we are somewhat unclear about the basis for the initial score of 3 (\u201creject, not good enough\u201d). We believe this score may be influenced by the noted weaknesses and questions, which we feel may stem from a misunderstanding of our core contributions. We proceed to address these points in detail below:\n\n__W1. On dataset resolution terminology.__ We agree that the term \u201chigh-resolution\u201d can be somewhat subjective and may lead to different interpretations depending on the audience's background. In  Image classification literature there is often crisp distinction between \u201chigh\u201d and \u201clow\u201d resolution datasets, where \u201chigh-resolution\u201d is often used to distinguish datasets like Flowers-102 and Pets-37 from low-resolution datasets such as CIFAR or MNIST. Our intent was simply to clarify that our datasets contain images of higher resolution compared to these low-resolution benchmarks, which are not within the scope of our work. \n\nWe recognize that in other fields, such as image generation, \u201chigh-resolution\u201d often refers to even larger image sizes. We are open to adjusting the terminology if Reviewer BkxW or others have suggestions for a more precise description that would prevent potential misunderstandings.\n\n__W2 and Q2. Extending to dense prediction tasks.__ As stated in the paper, \u201cWe focus on classification because many industrial and medical applications rely on classification (e.g., disease or defect detection).\u201d This aligns with our primary objective of addressing needs in domains where classification is central, and data is often limited.\nRegarding dense prediction tasks, such as segmentation, a key advantage of introducing a convolutional tokenizer like SCOTT is the flexibility to build a UNet-like decoder. With SCOTT, progressive downsampling feature maps can be used as skip connections to guide the upsampling process in the decoder, making SCOTT well-suited for dense prediction. This flexibility is not possible with the patch-and-embed tokenization strategy of standard ViTs, which lacks these spatial hierarchies.\nAlthough we briefly mentioned dense prediction as a future direction, we refrained from expanding on this potential benefit in the current paper, as there were no experiments conducted to fully support these foreseeable applications. We plan to explore this in future work to validate SCOTT\u2019s applicability to dense prediction tasks. \n\n__W3. Comparing to traditional fine-tuning methods:__ In terms of comparing our approach to traditional finetuning methods, we direct Reviewer BkxW to Table 1 in our original manuscript, where we report results of ViT-12/16 pretrained on ImageNet (1k and 21k) and finetuned on the target datasets. We would like to take the opportunity to highlight that our method SCOTT + MIM-JEPA achieves comparable results while using way less resources across different axes, this is the main contribution of our work.\n\nWhile fine-tuning pretrained models may remain the preferred option in the natural images domain, our objective is to demonstrate that SCOTT+MIM-JEPA can enable competitive training on domain-specific datasets with limited resources. This capability is particularly relevant for a wide range of computer vision tasks in resource-constrained fields (e.g., medical imaging, robotics) where pretraining on massive datasets is not feasible.\n\nAs part of our future work, we plan to extend our research beyond natural images, focusing on domains where larger, pretrained models currently dominate. We believe that our approach has the potential to serve as a valuable alternative in these areas, and we do not claim otherwise."
            }
        },
        {
            "title": {
                "value": "Continuation to Previous Response To Reviewer 7fBC"
            },
            "comment": {
                "value": "__W2.__ We will proceed by citing our own manuscript to answer the different questions. In essence, our design choices are to address the fundamental difference between computer vision and natural language processing, which we demonstrate with our contributions that have been overlooked in prior works in favor of \u201cusing more data\u201d.\n\n__Reviewer 7fBC: What kind of problems are there for similar design? Why is the proposed method better? Why choosing such design (e.g., MIM-JEPA)?__\n\n- __Convolutional tokenizer in ViT for MIM:__ \u201cintroducing a CNN tokenizer conflicts with the patch-wise masking strategy because one cannot eliminate pixel information from masked patches -to avoid trivial solutions- as ViTs do byremoving or replacing them with a mask token.\u201d\n\n- __Generative MIM architectures:__ \u201cSince the introduction of MIM, various methods have explored different reconstruction targets, such as raw pixels (He et al., 2022; Xie et al., 2020; 2022), or patch-level tokens via a learned tokenizer (Bao et al., 2021; Peng et al., 2022). While these approaches have been effective in scaling self-supervised learning to larger datasets, they often lead to feature representations at a low-level of semantic abstraction\u201d \u2026 \u201cJEPAs are conceptually close to Generative Architectures, however, the loss function is applied in embedding space, not input space\u201d, that is, the model does not reconstruct the noise present in the input signal space and can focus on semantic meaning.\n\n\u201cBuilding on these ideas, we integrate our Sparse Convolutional Tokenizer for Transformers (SCOTT) within the ViT architecture of a JEPA framework based on MIM and dubbed MIM-JEPA. This combination enables effective self-supervised learning on small-scale datasets, where traditional ViT approaches typically struggle.\u201d\n\n__W3 Experiments and ablations.__ We would appreciate further clarification from Reviewer 7fBC regarding concerns about the experiments and ablations. As noted in the paper, __\u201cWe focus on classification because many industrial and medical applications rely on classification (e.g., disease or defect detection),\u201d__ and the main goal of our method is to provide a viable solution for domains where resources and labeled data are limited. With this in mind, we selected Flowers-102, Pets-37 and ImageNet-100, which are particularly challenging due to their high intra-class similarity, making it harder to distinguish between categories (e.g., different types of flowers) compared to more generic categories (e.g., persons, cars, planes).\n\nIn Table 1, we report results across the most relevant available learning paradigms:\n- __Fully supervised learning:__ Training ViTs and SCOTT from scratch.\n- __Transfer Learning from Supervised Pretraining:__ Fine-tuned ViTs pretrained on large datasets.\n- __Probing State-of-the-Art (SOTA) Self-Supervised Models:__ Evaluating the performance of DinoV2 and I-JEPA pretrained on large datasets.\n- __Our Method (SCOTT+MIM-JEPA):__ To assess its effectiveness in small-scale, data-limited settings.\n\nWe believe these comparisons provide a comprehensive view of SCOTT+MIM-JEPA\u2019s performance against key learning paradigms. However, if Reviewer 7fBC has specific suggestions for additional experiments, we would appreciate the feedback on what additional comparisons might further demonstrate our method\u2019s effectiveness in these settings.\n\n__Regarding incomplete ablations (see Table 2)__, we presented several analyses to examine the impact of key components in our architecture and learning framework. Specifically:\n- No MIM-JEPA and No SCOTT, i.e., a ViT trained in supervised learning.\n- No MIM-JEPA pretraining, i.e., a ViT enabled with SCOTT tokenizer in supervised learning.\n- No SCOTT, that is a ViT with patch-and-embed but pretrained using MIM-JEPA.\n- No color augmentations.\n- Random masking instead of Blockwise masking.\n\nIf there are additional ablations that Reviewer 7fBC feels would be valuable to include, we would appreciate any specific recommendations and will include them in our final version.\n\nWe hope this detailed clarification offers Reviewer 7fBC an updated perspective on our work and its contributions toward advancing representation learning beyond the big data paradigm, thereby making computer vision more accessible to resource-limited domains. Given these points, we respectfully ask the reviewer to reconsider the initial evaluation of our work. We appreciate the reviewer\u2019s time and look forward to the Reviewer's 7fBC response."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer 7fBC"
            },
            "comment": {
                "value": "We appreciate Reviewer 7fBC for recognizing the importance of this work, describing it as \u201cpromising since training Transformer-based vision model is very data thirsty\u201d. However, we noted some potentially contradictory points in the review: the summary mentions that \"the experiments on small-scale datasets show promising results,\" yet the weaknesses section states that \u201cthe comparison experiments in the paper are weak\u201d and \u201cthe experiments are not sufficient to demonstrate the effectiveness of the method.\u201d\nWe appreciate the opportunity to clarify and expand on our experimental section to address these points. In this response, we will outline our approach and address specific questions from Reviewer 7fBC. \n\nTo recall, the primary goal of our work is to propose a method that enables __effective representation learning on small-scale, fine-grained datasets without requiring extensive data or computational resources.__ With this goal, our experimental design focuses on assessing our proposed contributions (architecture: SCOTT and learning framework: MIM-JEPA) capability to perform competitively with limited data and compute, compared to state-of-the-art methods that depend on large-scale pretraining.\n\n__W1. Comparison experiments to Conv+ViT baselines.__ We would like to clarify that our primary goal is not to propose a new conv+ViT architecture achieving state-of-the-art performance in supervised learning (SL). Instead, our focus is to: __\u201cenable effective representation learning on small-scale, fine-grained datasets without requiring extensive data or computational resources\u201d.__\n\nTo achieve this, we propose SCOTT\u2014a novel approach that integrates a convolutional tokenizer with Vision Transformers (ViTs) in a way that is compatible with masked image modeling (MIM) within a self-supervised learning (SSL) framework. To the best of our knowledge, this is the first work to incorporate a convolutional tokenizer with ViTs specifically for MIM-based SSL, making SCOTT a unique contribution within the SSL context.\n\nGiven this novelty and focus, we selected baselines that represent state-of-the-art (SOTA) performance across different learning paradigms, as we believe this provides a more meaningful comparison for our results, regardless of whether these baselines employ conv+ViT architectures.\nSpecifically, we included: \n-\t__Fine-Tuned Vision Transformers (ViTs) from Supervised Pretraining:__ These models represent the top performance achieved in fully supervised setting (i.e., ViT, and SparseSwin).\n-\t__Self-Supervised Learning (SSL) Pretrained ViTs:__ We also include SSL-pretrained ViTs (e.g., DinoV2, I-JEPA) to show the baseline performance of SOTA self-supervised models trained on large datasets.\n\nWe did not include conv+ViT supervised models in Table 1, as these underperform compared to the SOTA baselines selected. Instead, fully supervised SCOTT enabled ViT performance is reported as a representative example of a conv+ViT model trained from scratch in fully supervised conditions for the same number of epochs (300), where it outperforms standard ViTs under the same settings but underperforms any of the pretrained models.\n\nRegarding the Related Works section of the manuscript, given the page-limit constraint, we included references to conv+vit works that either achieved top results or are directly relevant to our proposed method. For instance, we noted, \u201cRecognizing this limitation, numerous studies have previously explored incorporating convolutional priors into ViT architectures (Wu et al., 2021; Chen et al., 2021; Yuan et al., 2021; Graham et al., 2021).\u201d.  If there are specific works that Reviewer 7fBC finds missing or particularly relevant, we would appreciate the recommendations and will revisit the Related Works section to include these in the final version."
            }
        },
        {
            "title": {
                "value": "Continuation to Previous Response To Reviewer M9yc"
            },
            "comment": {
                "value": "__Minor points:__ \n\n__M1. Contrastive objectives:__ Our method, similar to other ViT approaches, can easily incorporate a CLS token to support contrastive objectives if desired. While contrastive learning methods, such as Dino or BYOL, have achieved impressive results, they generally rely on view-invariant transformations to optimize different crops of the same image to converge on a single \u201cclass\u201d representation. This assumption is effective for datasets like ImageNet, where the main object is typically isolated and centered in the image.\n\nHowever, generalizing this assumption to more diverse, \"in-the-wild\" datasets is challenging, as these datasets often contain complex backgrounds or multiple objects without a single dominant subject. For further context, we refer to the insights on these limitations in Balestriero R. et al.  \u201cA Cookbook of Self-Supervised Learning\u201d. Therefore, while adding a CLS token for contrastive objectives is technically straightforward, our focus remains on methods better suited to the diversity of real-world data and less structured datasets.\n\n__M2 and M3.__ Improving reading experience. We will kindly hear suggestions on how we could present the information to enhance a better understanding of our unique contributions. As per the Reviewer M9yc\u2019s minor point consideration of introducing I-JEPA in the background, we will do our best to add it while it is a challenge given the manuscript page limit policy.\n\nWe hope this extensive clarification will help Reviewer M9yc provide an updated evaluation and informed judgement of our work, with a focus on our key contributions to escape the big data paradigm in computer vision and making it more accessible to a wider range of applications. We are looking forward to the reviewer\u2019s response and appreciate their time and consideration."
            }
        },
        {
            "title": {
                "value": "Continuation to Previous Response To Reviewer M9yc"
            },
            "comment": {
                "value": "__Q3.2. Unsupervised Pre-Training on Target Datasets:__ we respectfully disagree with the notion that pre-training on the full, unlabeled target dataset creates an unfair advantage, as no label-based learning signal involved in this process. Our approach aligns with standard practices in the field of representation learning, where our work is situated, and differs fundamentally from supervised learning, where we suspect the concern about data overlap (\u201cthis overlap can lead to an advantage, as the model learns features directly from the target dataset\u201d) may originate.\n\nIn representation learning, it is standard practice to use the full, unlabeled dataset for pre-training as the model learns structural patterns within the data independently of labels. This practice contributes to efficient learning in data-limited settings without requiring large external datasets, which is precisely the focus of our work: to demonstrate that one can train a ViT from scratch with less than 10,000 images and achieve results comparable to those obtained by methods pretrained on massive datasets, such as the 142 million images in LVD-142M used to train DinoV2. To reinforce this point, we refer to Table 15 in DinoV2 original manuscript (\u201cComposition of our LVD-142 Dataset\u201d), which shows that Flowers102, Pets37, and ImageNet are all included in the process of achieving the LVD-142 Dataset.\n\n__Cross-dataset generalization evaluation__: while cross-dataset generalization is an interesting research direction, it is not the intended goal of our work and is unfair to judge it from that perspective. As Reviewer M9yc suggests \u201cFor instance, if Flowers-102 is used for evaluation, then Pets-37 or ImageNet-100 could serve as a pre-training dataset\u201d is a relevant question for \u201cfoundational models\u201d trained with extensive resources, but we do not claim such generalization capabilities from our method, which is focused on effective performance given limited data.\n\nAdditionally, given the small sample sizes and fine-grained nature of the datasets used in our study, to the best of our knowledge, no existing methods have demonstrated strong cross-dataset generalization under these conditions. In particular, to the best of our knowledge it is still an unresolved problem to pre-train on a small, unrelated, fine-grained dataset and then perform well in an evaluation setting where the features differ significantly across domains (e.g., flowers vs. pets) -especially when probing, where simple classifiers are trained on top of frozen features. \n\n__Q4. Accuracy gap between method and SOTA.__ We hope that the extensive clarification on the fairness of our experiments will solve your concerns regarding the \u201cpromising\u201d potential of SCOTT+MIM-JEPA, given our primary objective: to enable effective model training on small-scale, fine-grained datasets -a direction that \u201cis critical for advancing self-supervised learning in data-limited settings\u201d citing Reviewer  M9yc\u2019s words in the strengths section. \n\nWhile pretraining on large-scale datasets or fine-tuning from open-source, pre-trained models may be the optimal approach in the natural image domain, our work addresses the need for effective SSL methods on small datasets, which is especially relevant for a broad range of applications (e.g., medical imaging, robotics) where large-scale data is unavailable or costly to obtain. We believe that training competitive models with constrained resources offers great value to a \"long tail\" of computer vision tasks that may not benefit from traditional large-scale pretraining.\n\nRegarding the noted \u201caccuracy gap,\u201d it is important to emphasize that our results were achieved using probing, where a simple classifier is trained on top of frozen features. Full fine-tuning would likely yield higher accuracy but is outside the scope of our current study which is not to achieve state-of-the-art and is left for future work."
            }
        },
        {
            "title": {
                "value": "Continuation to Previous Response To Reviewer M9yc"
            },
            "comment": {
                "value": "__Q1. SCOTT.__ As noted, incorporating CNN priors to improve ViT tokenization is no new in supervised learning, but it presents distinct challenges in SSL MIM tasks. Focusing on Spark, which pioneered the idea of introducing sparse convolutions in MIM, their work proposes a fully convolutional encoder-decoder architecture reminiscent of UNet and train in a generative BERT style framework, this is far from our work in both architectural design (Fully CNN vs. ViT) and learning framework perspective (Generative Architecture vs. JEPA).\n\nWhile introducing SCOTT might now seem obvious after reading the paper to improve efficiency of ViTs this should not be taken for granted, to the best of our knowledge there is no prior work to propose a CNN-like tokenizer for ViTs that is compatible with MIM tasks, nor to prove its effectiveness. This is indeed our contribution, we are the __first__ to propose to replace the patch and embed strategy of ViTs by a shallow convolutional tokenizer that is compatible with MIM tasks to enable efficient training. This compatibility with any MIM framework (e.g., iBOT, I-JEPA, DinoV2) demonstrates SCOTT novelty and potentiality to improve the data-efficiency of standard ViT models trained in them. \n\n__Q2. MIM-JEPA.__ We appreciate the request for clarification on MIM-JEPA\u2019s design. Although MIM-JEPA and I-JEPA both instantiate a Joint-Embedding Predictive Architecture (JEPA), their specific implementation differs, the most two salient:\n- __Context processing:__ In I-JEPA, only the visible patches are processed by the context-encoder. MIM-JEPA, in contrast, processes both visible and masked patches within the transformer part of the context-encoder, resulting in more extensive computation for representation learning for the masked areas.\n- __Predictor:__ In I-JEPA, the predictor receives the \u201cencoded visible patches\u201d and conditioned on positional tokens, predicts the representations of a target block at a specific location. In contrast, in MIM-JEPA, the predictor receives all patches, both visible and masked, and predicts the representations of their corresponding target patches.\n\nWhile both methods are JEPAs, the specific implementation details mark a difference in making it effective for small datasets. \n\n__Q3. Evaluation concerns:__\n\n__Q3.1. Supervised Baselines:__ as stated in Appendix \u201cE.2. Evaluation Details\u201d of the original manuscript: \u201cSupervised ViTs and SCOTT variants are trained for 300 epochs\u201d. We appreciate the opportunity to clarify this aspect and confirm that supervised baselines were trained from scratch with the entire model optimized over the same number of epochs as the pre-trained models. We hope this response clarifies Reviewer M9yc\u2019s  confusion about the fairness of our experiments and we would like to take this opportunity to highlight probing on frozen features produced by our pre-training method outperforms by far all this fairly trained supervised baselines (>26% top-1 accuracy in Flowers102 and >38% top-1 in Pets37, extracted from Table 1 of the original manuscript)."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer  M9yc"
            },
            "comment": {
                "value": "We thank Reviewer M9yc for the time and thoughtful feedback. However, some of the reviewers\u2019 statements (e.g., \u201cthe accuracy gap between our method and large-scale models make it difficult to consider this approach promising\u201d, \u201cfor instance, if Flowers-102 is used for evaluation, then Pets-37 or ImageNet-100 could serve for pre-training\u201d) suggest a possible misunderstanding of the core objectives and contributions of our work. We believe these interpretations may have led to an underestimation of the novelty and impact of our approach.\n\nTo clarify and reinforce the main contributions, we would like to restate the key objectives of our work as outlined in the Abstract and other sections of the manuscript:\n\n- __The primary goal of this paper is to propose a method to: \u201cenable effective representation learning on small-scale, fine-grained datasets without requiring extensive data or computational resources\u201d.__ Although we compare our results directly to state-of-the-art (SOTA) supervised finetuned models and SSL methods such as DinoV2 and I-JEPA (both of which are trained on large-scale data), our objective is not to outperform these large-data trained models. Rather, we aim to demonstrate that similar results can be achieved with significantly fewer resources. We hope this clarification shows that judging our method from the perspective of large-data SOTA approaches overlooks the resource constraints our method was specifically designed to address.\n- __Our key contribution is in addressing the challenge above__, which is relevant to a long tail of computer vision applications where large-scale data is either unavailable or costly to obtain. Achieving this requires non-trivial efforts across both architecture and learning framework, to effectively address the fundamental differences between NLP and CV is SSL, which we believe have been overlooked in prior work in favor of \u201cusing more data\u201d.\n- __Learning framework:__ previous MIM frameworks, such as BEIT, iBOT which DinoV2 builds upon, or I-JEPA, to mention some, proved that masked modeling can effectively work on vision tasks by leveraging the ease with which ViTs can mask patches. However, these frameworks rely heavily on large-scale data to achieve competitive results. In contrast, our work is the first to achieve comparable results without requiring large-scale datasets, making SSL more accessible to low-data regimes.\n- __ViT Architecture:__ through previous literature in Supervised Learning (not SSL), we observed that the patch and embedding tokenization strategy in ViTs is a major contributor to its data inefficiency. While previous literature, such as CCT, have shown that a convolutional tokenizer improves data inefficiency, conventional convolutions are not compatible with MIM. To overcome this, \u201cfollowing pioneering work of Spark to enable BERT pre-training on CNN architectures\u201d, we propose to introduce a shallow sparse convolutional tokenizer as a drop in replacement for the patch-and-embed in ViTs. To our knowledge, this is the first such approach for MIM-compatible convolutional tokenization in ViTs. If Reviewer M9yc is aware of prior works that introduce this idea, we would appreciate any references that could enhance our related work section.\n\nWe hope this clarifies the purpose and originality of our work. We proceed to answer the specific questions of Reviewer M9yc:"
            }
        },
        {
            "summary": {
                "value": "This paper introduces SCOTT, a Sparse Convolutional Tokenizer designed to enhance Vision Transformers (ViTs) by incorporating convolutional inductive biases, enabling effective self-supervised learning on small datasets. SCOTT integrates with MIM-JEPA, a Joint-Embedding Predictive Architecture within a Masked Image Modeling (MIM) framework, to capture higher-level semantic features. The approach is validated on fine-grained datasets, such as Oxford Flowers-102 and Oxford IIIT Pets-37, achieving competitive results with significantly fewer data and computational resources."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper addresses an important problem: enabling model training on small-scale, unlabeled datasets, which is critical for advancing self-supervised learning in data-limited settings.\n\n2. The authors conduct extensive experiments using multiple datasets."
            },
            "weaknesses": {
                "value": "1. The contributions of the proposed methods appear incremental compared to previous work.\n\n2. The evaluation and comparisons with baseline and prior methods seem unfair due to differences in training setups.\n\n3. The writing quality could be improved for clarity and readability.\n\nPlease see my comments below for further details."
            },
            "questions": {
                "value": "This paper proposes SCOTT and MIM-JEPA, two components that collaboratively enable effective model training on small-scale, unlabeled datasets. Together, they achieve promising results and open avenues for future research in resource-constrained settings. However, I have the following questions and concerns.\n\n1. The novelty of SCOTT appears limited. Many prior works have explored injecting convolutional layers into vision transformers, as mentioned in the paper. The key challenge in combining convolutional layers with masked image modeling (MIM) is that the masked areas can diminish due to the convolutional nature. However, sparse convolution techniques, including submanifold sparse convolution, have been well-established for managing masked areas, and it seems that SCOTT directly adopts these existing techniques. Could the authors elaborate on the unique contributions of SCOTT over these previous approaches?\n\n2. I am unclear about the novelty of MIM-JEPA compared to I-JEPA. The training pipelines for the two methods seem very similar. Could the authors provide further details to clarify the specific contributions of MIM-JEPA beyond what is already achieved by I-JEPA?\n\n3. The evaluation methodology raises concerns about fairness:\n\n   3.1 For the baseline of training a model from scratch with fully supervised learning, is only the final (or several) layers trained, or is the entire model fine-tuned? From the text, it appears to be the former, which would weaken this baseline and result in significantly lower accuracy compared to pre-trained methods. To properly evaluate the effectiveness of supervised learning, which is generally a strong baseline, the entire model should be trained for the same number of epochs as in pre-training (300 or 1200 epochs).\n\n   3.2 When comparing with SSL pre-training baselines and other SSL works, the datasets used for pre-training differ, raising concerns about fairness. While prior methods are pre-trained on larger datasets like ImageNet or LVD-142M, SCOTT+MIM-JEPA is pre-trained on the target dataset itself, which is then also used for evaluation (e.g., attention or linear probing). This overlap can lead to an advantage, as the model learns features directly from the target dataset. For a fair comparison, SCOTT+MIM-JEPA should pre-train on a small, unrelated dataset. For instance, if Flowers-102 is used for evaluation, then Pets-37 or ImageNet-100 could serve as a pre-training dataset.\n\n4. The accuracy of SCOTT+MIM-JEPA is still notably lower than models trained on large-scale data. While this is expected, the significant accuracy gap makes it difficult to consider the approach \"promising,\" especially given the aforementioned evaluation concerns.\n\n\nminor point(s):\n\n1. The method is tailored specifically for MIM-related self-supervised learning, which limits its application scope, as it is not compatible with contrastive learning. However, given the popularity of MIM, this specialization is understandable and not a major issue.\n\n2. The paper's writing could be improved. The paper references many prior works that inspired it, but the current organization makes it challenging to distinguish the unique contributions of this work.\n\n3. Introducing I-JEPA in the background section would be helpful, as the current work builds directly upon it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Sparse Convolutional Tokenizer for Transformers (SCOTT) which is a tokenization architecture that injects convolutional inductive biases into Vision Transformers. The purpose it enable small-scale data training while compatible with MIM tasks. The author of the paper also proposes Joint-Embedding Predictive Architecture within a MIM framework. The experiments on small-scale dataset shows promising results."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. This paper is easy to follow and has clean organization. \n\n2. This topic is promising since training Transformer-based vision model is very data thirsty."
            },
            "weaknesses": {
                "value": "1. The comparison experiments in the paper is weak since there are tons of conv+ViT baselines. This paper, however, only compare to a few, also the related works missed many related references. Therefore, the paper\u2019s experiments is not quite convincing.\n\n2. The motivation is clear but this paper lacks the analysis of related works. What kind of problems are there for similar design? Why the proposed method is better? Why choosing such design (e.g., MIM-JEPA)? The overall elaboration is not quite self-sufficient.\n\n3. The experiments are not sufficient to demonstrate the effectiveness of the method. The settings and comparison is too simple, and very limited ablations are conducted."
            },
            "questions": {
                "value": "Please refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces two advancements in self-supervised learning from images with limited data SCOTT (Sparse Convolutional Tokenizer for Transformers) and MIM-JEPA (Masked Image Modeling with Joint-Embedding Predictive Architecture). SCOTT infuses convolutional biases into ViTs, enhancing their effectiveness in data-constrained environments, while MIM-JEPA optimizes the representation learning in a latent space. This dual approach reduces the dependency on large-scale datasets, enabling effective training on datasets like Oxford Flowers-102, Oxford IIIT Pets-37, and ImageNet-100."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The integration of convolutional biases through SCOTT and the focus on semantic feature extraction via MIM-JEPA can shift away from the reliance on extensive pre-training datasets.\n\n2. The proposed methods outperform fully supervised methods and achieve results competitive with state-of-the-art models pre-trained on much larger datasets."
            },
            "weaknesses": {
                "value": "1. The authors claim that the datasets used are high-resolution; however, I believe these datasets should not be considered high resolution.  (Of course, compared to low-resolution CIFAR and MNIST, there are). I suggest that the authors also include results from higher, domain-specific resolution datasets, as well as from low-resolution datasets, to provide a more comprehensive analysis of performance variations across different resolutions.\n\n2. The methodology appears to be primarily limited to classification tasks. Although the authors mention that future work will extend to segmentation, it would be beneficial if they could discuss the potential applicability of their methods to segmentation tasks more explicitly. \n\n3. Fine-tuning on pre-trained general models might still be the best way to train domain-specific images, offering less training time and potentially better performance. The authors should consider comparing their approach directly to traditional fine-tuning methods to substantiate their claims and highlight any genuine advantages or limitations."
            },
            "questions": {
                "value": "1. What specific advantages do convolutional biases offer over other techniques designed to improve data efficiency in vision models, such as attention augmentation or advanced data augmentation techniques?\n\n\n2. Can the authors provide preliminary insights on how their approach might be adapted for segmentation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work demonstrates that robust off-the-shelf representations can be learned with limited data, compute, and model sizes by integrating a Sparse Convolutional Tokenizer into Transformer architectures. The authors introduce CNN-like inductive biases while maintaining compatibility with masked image modeling objectives, enabling the self-supervised pretraining for Masked Image Modeling. To show the advantages of the paper, the authors provide extensive comparisons with other baseline methods on several downstream tasks. The authors also conducted an ablation study to show the effectiveness empirically."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+ The paper is well-organized and easy to read.\n+ The paper proposes a Joint-Embedding Predictive Architecture for the Masked Image Modeling task, enabling self-supervised pre-training on a much smaller dataset.\n+ This paper provides strong performance across all the tasks and architecture in a self-supervised learning setting."
            },
            "weaknesses": {
                "value": "- The difference between the proposed Sparse Convolutional Tokenizer for Transformers (SCOTT) and SparK is not obvious, it looks more like a simple leverage of previous work. The authors need to claim more of the difference with previous work.\n- Experimental results with different settings are not very comparable. As for the model size, pre-training datasets, pre-training method setting are all different from the method proposed in the paper. Although the author claims that achieving absolute performance is not the main goal, the results are supposed to be comparable. For example, experiments can be added to utilize Dino/I-JEPA or other pre-training paradigms to train on the small dataset and compare it with the proposed method."
            },
            "questions": {
                "value": "Please refer to the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}