{
    "id": "c8QlNuhy2G",
    "title": "MathGLM-Vision: Solving Mathematical Problems with Multi-Modal Large Language Model",
    "abstract": "Large language models (LLMs) have demonstrated significant capabilities in mathematical reasoning, particularly with text-based mathematical problems. However, current multi-modal large language models (MLLMs), especially those specialized in mathematics, tend to focus predominantly on solving geometric problems but ignore the diversity of visual information available in other areas of mathematics. Moreover, the geometric information for these specialized mathematical MLLMs is derived from several public datasets, which are typically limited in diversity and complexity. To address these limitations, we aim to construct a fine-tuning dataset named MathVL, and develop a series of specialized mathematical MLLMs termed MathGLM-Vision by conducting Supervised Fine-Tuning (SFT) on MathVL with various parameter-scale backbones. To extensively evaluate the effectiveness of MathGLM-Vision, we conduct experiments on several public benchmarks and our curated MathVL-test benchmark consisting of 2,000 problems. Experimental results demonstrate that MathGLM-Vision achieves significant improvements compared with some existing models, including backbone models and open-source mathematical MLLMs. These findings indicate the importance of diversity dataset in enhancing the mathematical reasoning abilities of MLLMs.",
    "keywords": [
        "Mathematical Reasoning",
        "Multi-modal large language model"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=c8QlNuhy2G",
    "pdf_link": "https://openreview.net/pdf?id=c8QlNuhy2G",
    "comments": [
        {
            "summary": {
                "value": "The paper presents MathGLM-Vision, a specialized multi-modal large language model designed to solve mathematical problems. Specifically, the authors introduce MathVL, a fine-tuning dataset with a diverse range of mathematical problems involving visual elements. MathGLM-Vision models are fine-tuned on MathVL and are evaluated across public benchmarks. Results demonstrate significant performance improvements in mathematical problem-solving."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tMathVL is a contribution as it diversifies the visual components beyond traditional geometry, covering topics like arithmetic, algebra, and statistics. This expansion enhances the MLLM\u2019s utility across a broader spectrum of math problems.\n2.\tThe experimental results show that MathGLM-Vision, with visual input, consistently outperforms its text-only counterparts, validating the value of multi-modal inputs.\n3.\tThe paper uses a comprehensive set of benchmarks, including MathVL-test and public datasets, to evaluate performance. This approach provides a well-rounded assessment of MathGLM-Vision's mathematical reasoning capabilities and general vision-language understanding.\n4.\tThe detailed analysis of error types\u2014reasoning, vision recognition, knowledge, calculation, and question misunderstanding\u2014provides useful insights into the model\u2019s weaknesses and areas for improvement."
            },
            "weaknesses": {
                "value": "1.\tWhile the dataset is innovative, the model\u2019s architecture and training process primarily involve data engineering and fine-tuning rather than a new modeling approach. This diminishes the paper's novelty as it focuses on enhancing existing models with improved data rather than proposing a novel methodological framework.\n2.\tThe paper does not provide in-depth information about adjustments made to the base model architectures (GLM-4V, CogVLM) to optimize them for mathematical reasoning tasks. Adding technical details on modifications could clarify how MathGLM-Vision addresses the unique challenges of mathematical problem-solving.\n3.\tAlthough the paper mentions experiments on general vision-language benchmarks, the analysis is brief, and performance drops in non-mathematical tasks. This may suggest a trade-off between specialization in mathematical tasks and general multi-modal understanding, a limitation for applications beyond mathematics."
            },
            "questions": {
                "value": "1.\tIn Section 2, the authors mention that MathVL includes \u201cstep-by-step solutions.\u201d Were these solutions uniformly generated for all problem types, or was there variation based on the problem\u2019s complexity? Could examples of different solution formats be provided?\n2.\tWhen curating MathVL, which specific properties were prioritized in selecting public datasets? Was the primary focus on dataset size, diversity of problem types, or specific visual elements?\n3.\tThere is limited explanation on how the model handles equations or complex symbolic mathematics within visual inputs. For instance, were any specialized tokenization or embedding techniques employed for this type of input?\n4.\tWere any strategies, like curriculum learning, employed to ensure the model progressed from simpler to more complex mathematical problems? Adding this detail could be useful for readers interested in the training dynamics.\n5.\tMathGLM-Vision is designed to handle multiple images per problem, yet most benchmarks (e.g., MathVista) are single-image-based. How did the model perform on tasks involving single versus multiple images? Could the authors provide specific examples from MathVL-test where multiple images significantly contributed to?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Current MLLMs specialized in math focus too much on geometry and miss other kinds of visual math information. To improve this, this paper created a new dataset called MathVL and developed MathGLM-Vision, which performed better on math problems than other models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This article provides the MLLM community with high-quality Math VL data. The introduced dataset, MathVL, is very useful and significantly enhances the mathematical abilities of some MLLMs.\n2. The experiments in this article are very detailed, and the writing is excellent."
            },
            "weaknesses": {
                "value": "The paper claims three contributions: dataset, model, and benchmark. However, the dataset was only validated on weaker models, and the differences from existing datasets are not clearly described (details below). The developed model is essentially a fine-tuning version of the proposed dataset without any architectural innovation (while SFT is a common practice, it does not constitute a contribution since the model is merely a product of the dataset, especially as the final model is not state-of-the-art). Therefore, the claimed contribution are not justified well.\n\nThe difference between this article and Math-LLaVA is mentioned in lines 81-88, where it states that Math-LLaVA focuses on solving geometric problems. However, after reading Math-LLaVA, I found that it also includes various tasks, such as Math Word Problems and Textbook Question Answering. \n\n\n1. For Table 4: 1) I am particularly interested in the effectiveness of MathVL on other stronger models, such as the 7B InternLM-XC2, since the base model used in the experiment seems to have low mathematical ability according to the results in the table (about 40% accuracy on MathVista). 2) Why is the accuracy for GLM-4V-9B at 46.7 on MathVista, with its fine-tuned MathGLM-Vision-9B version reaching 52.2, while CogVLM2, which has an accuracy of only 40.85, achieves 61.1 after fine-tuning as MathGLM-Vision-19B? Why does a weaker model show better performance after fine-tuning?\n2. Table 5, results for GLM-4V-9B, GLM-4V-9B w/o VQA datasets, and MathGLM-Vision-9B are missing.\n3. Table 6, it seems to lack the exact composition method of MathVL-test in the paper. I am curious whether it is sampled from Open-source Data or Open-source Data + Chinese K12 Data. If it is the latter, then the performance drop with only SFT on Open-source Data alone would not necessarily indicate the importance of Chinese K12 Data, given that a significant performance drop is only observed on MathVL-test without Chinese K12 Data."
            },
            "questions": {
                "value": "1. The VQA dataset's size (i.e., the number of data points it contains) would be helpful to clarify; \n2. For the error analysis, could you confirm which tool or method was used to identify error types? Was it done through a specific automated tool, manual annotation, or perhaps a LLM?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces MathVL,  a diverse and comprehensive multi-modal mathematical dataset for SFT. MLLMs of different scales trained on MathVL have demonstrated excellent performance, proving the dataset's effectiveness for solving mathematical problems. Beside, the paper also establishes a benchmark dataset called MathVL-test to evaluate the mathematical reasoning abilities of MLLMs using a multi-image format."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed model, MathGLM-Vision, achieves good results on several benchmarks. The performance on MathVL-test is promising.\n2. This paper presents multiple cases to better understand the collected dataset."
            },
            "weaknesses": {
                "value": "1. The writing of the paper requires improvement. Several citations are not formatted correctly (e.g., Lines 75-78).\n2. The paper appears to be more engineering-focused, with the primary contribution being the introduction of the MathVL dataset. While this is a valuable resource, the abstract does not mention any plans to make the dataset open-source or publicly available. Without access to the dataset, the practical impact of the work is significantly diminished, as other researchers cannot replicate or build upon the proposed model. Therefore, making the dataset publicly available would greatly strengthen the paper\u2019s contribution and its value to the broader research community.\n3. Maybe more figure examples can help to understand better the challenges presented in Line82-85. For example, the paper claims that current MLLMs tend to overlook the diversity of visual information in mathematics. Some visualizations or analyses would better support this conclusion."
            },
            "questions": {
                "value": "My primary concern revolves around the paper's contribution to the community. While I acknowledge that the paper introduces a model for addressing mathematical problems and offers a useful benchmark for the field, I believe that the most impactful contribution to the advancement of Math MLLMs would be the provision of the dataset for others to use in training their own models. Making the data publicly available would significantly enhance the reproducibility and accessibility of the research, fostering broader progress in the development of Math MLLMs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}