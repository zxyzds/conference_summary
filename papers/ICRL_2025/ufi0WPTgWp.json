{
    "id": "ufi0WPTgWp",
    "title": "Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization",
    "abstract": "Videos contain a wealth of information, and generating detailed and accurate descriptions in natural language is a key aspect of video understanding. In this paper, we present video-SALMONN 2, an advanced audio-visual large language model (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with paired audio) captioning through directed preference optimization (DPO). We propose new metrics to evaluate the completeness and accuracy of video descriptions, which are optimized using DPO. To further improve training, we introduce a novel multi-round DPO (mrDPO) approach, which involves periodically updating the DPO reference model, merging and re-initializing the LoRA module as a proxy for parameter updates after each training round (1,000 steps), and incorporating guidance from ground-truth video captions to stabilize the process. To address potential catastrophic forgetting of non-captioning abilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO LLM by using the captions generated by the mrDPO-trained model as supervised labels. Experiments show that mrDPO significantly enhances video-SALMONN 2's captioning accuracy, reducing global and local error rates by 40\\% and 20\\%, respectively, while decreasing the repetition rate by 35\\%. The final video-SALMONN 2 model, with just 7 billion parameters, surpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining competitive performance to the state-of-the-art on widely used video question-answering benchmark among models of similar size. Upon acceptance, we will release the code, model checkpoints, and training and test data. Demos are available at https://video-salmonn-2.github.io.",
    "keywords": [
        "Multi-modal large language models",
        "video captioning",
        "multi-round DPO",
        "rebirth tuning"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ufi0WPTgWp",
    "pdf_link": "https://openreview.net/pdf?id=ufi0WPTgWp",
    "comments": [
        {
            "summary": {
                "value": "- The paper introduces video-SALMONN 2, an advanced audio-visual large language model (LLM) for generating detailed and accurate video captions, surpassing models like GPT-4o and Gemini-1.5-Pro on this task.\n  \n- A new evaluation pipeline is presented, with metrics specifically designed to quantify missing information, hallucination, and repetition rates in audio-visual captions, enhancing the precision of video content descriptions.\n\n- Multi-round Directed Preference Optimization (mrDPO) is developed to improve caption accuracy through reinforcement learning, periodically updating a reference model and using low-rank adaptation (LoRA) for smoother and more effective model training.\n\n- Rebirth tuning is introduced to prevent performance degradation in non-captioning tasks post-mrDPO, using supervised fine-tuning on self-labeled data to maintain both captioning quality and broader LLM capabilities."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The introduction of video-SALMONN 2 with its emphasis on synchronized audio-visual inputs for captioning represents a significant advancement. By integrating both visual and audio data, the model moves beyond purely visual or audio-focused LLMs, capturing richer and more contextually complete video descriptions. The novel use of multi-round Directed Preference Optimization (mrDPO) as a reinforcement learning technique to improve captioning quality is a noteworthy approach.\n\n- The paper demonstrates high-quality experimental validation, showcasing the effectiveness of mrDPO and rebirth tuning. Through detailed metrics like missing information, hallucination, and repetition rates, the authors provide quantitative evidence of video-SALMONN 2\u2019s superiority over leading commercial models like GPT-4o and Gemini-1.5-Pro in video captioning tasks. The thorough benchmarking against these models on multiple metrics adds robustness to the study\u2019s findings.\n\n- The paper is clear and methodically structured, making complex techniques like multi-round preference optimization accessible. Each step in the methodology is well-illustrated, from model architecture through training strategies, with detailed explanations of mrDPO and rebirth tuning. The clarity of the evaluation pipeline and the metrics for assessing captioning quality further aids in understanding the model's improvements."
            },
            "weaknesses": {
                "value": "- The paper introduces rebirth tuning to help the model retain non-captioning abilities, but the evaluation of this feature feels limited. Expanding the assessment to include the model\u2019s performance on non-captioning tasks, like video question-answering, at each training stage could give a more comprehensive picture of its capabilities. This approach would better demonstrate how effectively rebirth tuning preserves broader language skills.\n\n- The evaluation pipeline depends heavily on other large language models, such as GPT-3.5 and GPT-4, to assess metrics like missing information and hallucination rates. This reliance raises concerns about objectivity and reproducibility, as these models come with their own biases. Developing an alternative, self-contained evaluation method or comparing LLM-based assessments with human judgments could strengthen the evaluation\u2019s reliability and transparency.\n\n- The model\u2019s performance evaluation relies mostly on internal datasets, which limits the generalizability and reproducibility of the results. Including open datasets or releasing the test set would allow the research community to more easily validate the findings and enhance the overall credibility of the work.\n\n- The mrDPO technique is key to the model\u2019s effectiveness, but the paper lacks a detailed analysis of its individual components, like LoRA updates, regularization terms, and the multi-round structure. Investigating how each component contributes to the model\u2019s overall performance would provide insights into which aspects are most beneficial, potentially guiding future improvements and applications of mrDPO.\n\n- The training strategy, particularly mrDPO and the use of several LoRA proxies, adds computational overhead without a clear analysis of the associated trade-offs. Quantifying the specific GPU memory, runtime, and computational load required for each training round would clarify the model\u2019s resource demands. Additionally, identifying the point at which diminishing returns begin in mrDPO could shed light on the efficiency of each round, highlighting which may be resource-intensive with limited added benefit. Exploring alternative configurations for resource-limited environments and comparing high-resource to optimized setups would offer insights into how performance scales in different compute settings. This analysis would clarify the balance between computational cost and performance improvements, making the model more adaptable and accessible for varied research needs.\n\n- Although the model is designed to process both audio and visual inputs, the paper provides minimal qualitative insight into how well it aligns these features in its captions. Including examples of successful synchronizations as well as cases where alignment fails would offer a clearer view of the model\u2019s multi-modal understanding abilities."
            },
            "questions": {
                "value": "- Could the authors include more diverse evaluation tasks to assess the impact of rebirth tuning on non-captioning skills? For instance, evaluating on tasks like video question-answering at each stage of training could provide more insights into how rebirth tuning preserves or enhances the model\u2019s broader language abilities.\n\n- Considering the reliance on GPT-3.5 and GPT-4 for assessing metrics like hallucination rates, how have the authors addressed potential biases from these models in their evaluation pipeline?\n\n- Could the authors clarify if there are plans to evaluate video-SALMONN 2 on open, widely-used datasets for multimodal captioning? Including results on open datasets or releasing parts of the internal test set would enhance the model's credibility. It would also allow for broader validation by the research community, helping others replicate and build upon the findings.\n\n- Can the authors provide a detailed analysis of the impact of individual components of mrDPO, like LoRA updates, regularization terms, and the multi-round structure, on the overall performance? How does each element contribute uniquely to the training effectiveness?\n\n- Given that mrDPO involves multiple LoRA proxies and additional training rounds, can the authors quantify the GPU memory, runtime, and computational load associated with each round? Is there an observed point where additional rounds yield diminishing returns? Common metrics are FLOPS, MACs, Latency and so on. \n\n- While the model is designed for audio-visual input, the paper lacks qualitative insight into its success rate in synchronizing audio and visual elements. Could the authors provide examples or failure cases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose a VideoLLM named video-SALMONN2, which is capable of understanding both the video and audio modality. To train the model, a training strategy with multi-round DPO (mrDPO) and rebirth tuning is applied. mrDPO iteratively performs DPO training for multiple steps, while rebirth tuning improves the original model with captions generated from the model updated with mrDPO. The performance is evaluated on the custom dataset, in terms of repetition and event missing rate."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. video-SALMONN shows competitive performance on multiple metrics on the custom dataset for evaluation.\n2. The goal this paper is aiming on is a promising direction."
            },
            "weaknesses": {
                "value": "1. Lots of details about the method seem to be missing or not clearly stated, making it hard to understand the method.\n    1. Training losses are not detailed. It can be inferred that for the audio alignment stage, speech recognition and audio training losses are applied, for the SFT stage (L231-238), video captioning loss is applied, and for the DPO stages, a DPO loss is applied. Still, it would be explicitly mentioned which loss is applied for each stage, as it was very confusing, especially in the case of the audio alignment stage, where the information about training loss is simply mentioned in L376-377. Also, since there exist multiple variants of DPO, it would be better to include the exact formulation of DPO in the equation.\n    2. How are the captions for DPO sampled? It is only stated as \u201cFirst, distinct video captions are sampled from the model\u2019s output distribution, given the\n    input video\u201d in L268.\n    3. What are the definitions of \u2018basic atomic events\u2019 (L289), \u2018information missing and hallucination rates\u2019 (L292), \u2018global captions\u2019 (L269), and \u2018local captions\u2019 (L293)?\n    4. How is a problematic pattern detected when building training data for rebirth training, which is mentioned in L351?\n    5. It seems that the final model (video-SALMONN) is a model trained with rebirth training with data generated with mrDPO, and is further trained with a single round of gDPO (L407), which is not mentioned in the method section. It would be better to state the final model in the methods sections for better understanding.\n2. The advantage of mrDPO is confusing. \n    1. If training with mrDPO results in \u201cthe model gradually begins to produce repetitive and unnatural text patterns in its responses\u201d (L337), why is it required? \n    2. Such claim in 2-(a) is also contradictory with the statement in L349, which says the final mrDPO model \u201cexcels at generating complete and accurate video descriptions\u201d. \n    3. Also, are there experimental results that back up the claim that the mrDPO model \u201cdemonstrates significant improvements in captioning\u201d (L335)?\n    4. Results in Fig.4-(a) seem direct-DPO with single-round DPO shows the lowest total error rate, which makes the advantage of mrDPO more doubtful. Authors claim that the dashed line indicates a \u2018high frequency of unnatural text generation\u2019, but how are they determined as so?\n3. Missing experiments and analysis\n    1. Results on more conventional datasets are required. Current evaluations are only done on custom datasets and a Video-MME short set, where the method is compared only with a few baselines, extensive comparison on more conventional benchmarks with competitive baselines is required.\n    2. The motivation for using GPT-based \u2018missing\u2019 and \u2018hallucinatory\u2019 events as metrics is not clear. Ablation results on metrics, comparing proposed metrics and conventional captioning metrics comparing generation results with ground-truth captions can make the design choice more convincing.\n    3. Some qualitative examples showcasing outputs of an initial model, a model trained with mrDPO, and the final model would help understand the actual behavior of each model and the effect of each component.\n4. Incorporating audio modality into VideoLLMs has already been proposed [1, 2, 3], and the proposed model seems to have no significant difference compared to them in terms of audio inclusion.\n\n**References**\n\n[1] Yang et al., Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning, CVPR 2023\n\n[2] Wang et al., InternVideo2: Scaling Foundation Models for Multimodal Video Understanding, ECCV 2024\n\n[3] Zhang, Li, and Bing, Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding, EMNLP 2023 (demo track)"
            },
            "questions": {
                "value": "Please refer to the weaknesses part.\n\nOverall, as mentioned in the weaknesses part I think the paper needs large revision at this point. First, the methods part can be written in a better way to make readers understand the technical details and contributions of this paper more easily. Also, more experiments and analysis would make the proposed methods more convincing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces video-SALMONN 2, a powerful audio-visual large language model (LLM) designed for detailed and accurate video captioning. The model is built upon a pre-trained visual LLM and is further enhanced with auditory capabilities by training on audio-only data and videos with synchronized audio tracks. To accurately assess the model's performance, new metrics are proposed to evaluate the completeness and accuracy of video descriptions, which are then optimized using a novel multi-round direct preference optimization (mrDPO) approach. The mrDPO process involves periodically updating the DPO reference model, merging and re-initializing the low-rank adaptation (LoRA) module, and incorporating guidance from ground-truth video captions to stabilize the training. To address potential catastrophic forgetting of non-captioning abilities, a rebirth tuning process is introduced, which fine-tunes the pre-DPO LLM using the captions generated by the mrDPO-trained model as supervised labels."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.the proposed novel pipeline including mrDPO, LoRa proxy and rebirth, is detailedly introduced, and experiment results on each iteration are well presented.  \n2.good ablation experiments are provided on each mrDPO iteration, LoRa proxy, and rebirth. And the experiments provide good observation on the DPO' effect on the model performance.\n3.the whole pipeline is interesting and inspirational."
            },
            "weaknesses": {
                "value": "1.the model performance improvement is mainly evaluated on the internal benchmarks, and the quality of the internal benchmark is unknown.\n2. video-mllms could not only do recgnition and captioning of a video, but also exploite the knowledge of LLM to do further recognition reseasoning. However, in paper, only one comparison with public benchmark \"Video-MME short video\" is provided. So it is not sure the proposed method could be beneficial for comprehensive improvements of a video-mllm."
            },
            "questions": {
                "value": "1. for video QA evaluation\uff0ccould authors provide result on the Video-MME Medium and Long Video track? and also ablation results on other video understanding benchmark, such as mvbench, videovista, MLVU, to make the comparison more transparent and persuasive?\n2. for video captioning evaluation, could authors provide more comparison results on  public benchmark, such as MSR-VTT and Vatex\uff1f\n3. will author plan to publicly release the internal benchmark?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a large multimodal model (audio-visual) for video captioning task. multi-round DPO and LoRA is used to train the model. Experiments look good, showing that the training recipe enhances video-SALMONN 2\u2019s captioning accuracy. Besides, this paper introduce an evaluation pipeline that computes the missing and hallucination rates of audiovisual events in video captions using text-based LLMs, breaking down the process into sub-tasks suited for current LLMs and a new benchmark for video captioning with a human-annotated test set."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Audio is very important in understanding video, and incorporating it is a great idea.\n2. The approach of using DPO (Direct Preference Optimization) and LoRA (Low-Rank Adaptation) to enhance specific capabilities with limited data is intuitive and effective.\n3. Detailed video captioning is an excellent task, although it is not novel. It's great to see that people are starting to pay attention to and evaluate this task.\n4. The event missing rate (Miss), hallucination rate (Hall), and text repetition rate (Rep) are introduced as the new metric for evaluation the detailed video captioning tasks.\n5. Dividing the tasks into global and local parts is very beneficial. A recent work, MovieChat [1] also made a similar categorization (for video QA). Perhaps the authors could discuss and compare this in future versions.\n\n[1] Song, Enxin, et al. \"Moviechat: From dense token to sparse memory for long video understanding.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."
            },
            "weaknesses": {
                "value": "1. The writing and illustrations in the paper need improvement, especially the figures, which severely hinder the proper understanding of the article.\n2. Since DPO was used for training, based on my experience, DPO does not offer as stable training conditions as SFT. I hope the authors can present the specific parameters and training curves for DPO to further enhance reproducibility.\n3. Although the authors present LoRA as one of the main contributions, there is no ablation study to demonstrate how LoRA prevents catastrophic forgetting or performance drop.\n4. I believe that although the automated metrics are good (and perhaps better than metrics like CIDEr), the new metrics still need to align with human perception. I suggest that the authors refer to a concurrent work, AuroraCap [1], where they first use human Elo to score each model and then calculate the correlation between human Elo and the proposed metrics in the paper. This would make the benchmark more convincing.\n5. There are too few details about the proposed benchmark. I only found statistics on video types and lengths in the supplementary material, but there is no clear information regarding caption length (word count, noun count) or the number of video sources.\n\n[1] Chai, Wenhao, et al. \"AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark.\" arXiv preprint arXiv:2410.03051 (2024)."
            },
            "questions": {
                "value": "Please revise the Strengths and Weaknesses sections point by point. This is a paper with great potential. If the authors can provide additional responses to certain issues, discuss related work more thoroughly, and include more experiments and observations, I would be very happy to raise my score.\n\nAdditionally, although this is a concurrent work (and the authors can choose not to compare, as it's not a mandatory requirement), I would be very interested to see the model's performance on the VDC benchmark [1]. This would help me make a final decision. Again, this is completely optional.\n\n[1] https://huggingface.co/datasets/wchai/Video-Detailed-Caption"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}