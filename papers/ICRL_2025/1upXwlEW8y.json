{
    "id": "1upXwlEW8y",
    "title": "Prompt Optimization with Logged Bandit Data",
    "abstract": "We study how to use naturally available user feedback, such as clicks, to optimize large language model (LLM) pipelines for generating personalized sentences using prompts. Naive approaches, which estimate the policy gradient in the prompt space, suffer either from variance caused by the large action space of prompts or bias caused by inaccurate reward predictions. To circumvent these challenges, we propose *Direct Sentence Off-policy gradient* (DSO), which estimates the policy gradient by leveraging similarity among generated sentences, substantially reducing variance while suppressing the bias. Empirical results on our newly established suite of benchmarks, called *OfflinePrompts*, demonstrate the effectiveness of the proposed approach in generating personalized descriptions for movie recommendations, particularly when the number of candidate prompts is large.",
    "keywords": [
        "off-policy evaluation",
        "prompt tuning",
        "large language models",
        "contextual bandits"
    ],
    "primary_area": "causal reasoning",
    "TLDR": "This paper proposes a new OPL method for prompt-guided language generation, which leverages the similarity among sentences.",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1upXwlEW8y",
    "pdf_link": "https://openreview.net/pdf?id=1upXwlEW8y",
    "comments": [
        {
            "summary": {
                "value": "This paper proposed a new policy gradient-based prompt optimization. The goal is to learn a policy that is able to generate prompt with good response (as in good reward). This paper proposed a new DSO that is better than traditional policy gradient and IS based method. Some experimental results provided by this paper show that the new method is able to outperform others."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The idea of learning a policy to generate good prompts is new to me.\n2. The proposed method clearly addressed the weakness of IS."
            },
            "weaknesses": {
                "value": "1. The experimental session is the major weakness of this paper. This paper only contain a synthetic experiment and a single model experiment on a single dataset with simulated reward function. Experimental results on more datasets and models will make the paper more convincing.\n\n2. The following work should be discussed in the related work since they study prompt optimization with human feedback by learning a reward function and hence related.\n\nhttps://arxiv.org/abs/2402.00396\nhttps://arxiv.org/abs/2405.17346\n\nAn similar line of work on prompt optimization should also be discussed:\n\nhttps://arxiv.org/abs/2306.03082\nhttps://arxiv.org/abs/2310.02905\nhttps://arxiv.org/pdf/2402.09723"
            },
            "questions": {
                "value": "1. Can the author describe what's the main insight for the thoerms in this paper? and how they are reflected in the performance of the new approach? There seems to have some disconnection between the theoretical section and empirical verification.\n\n2. How does your method performs in normal prompt optimization setting? like [1]?\n\n\n[1] https://arxiv.org/abs/2306.03082"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses prompting policy optimization for large language model (LLM) pipelines by leveraging logged user feedback, such as clicks, to generate personalized sentences. The authors propose a novel method called Direct Sentence Off-policy gradient (DSO), which uses similarity between generated sentences to estimate the policy gradient. While this approach relies on importance sampling, it can reduce the variance of importance weights by treating them in a sentence space rather than the prompt space. Experiments on a synthetic task and an LLM-based task for personalized movie descriptions are shown to claim the effectiveness of the proposed DSO method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Using similarity in the generated sentence space to control the bias-variance tradeoff through importance weights is an interesting approach.\n* The paper evaluates the proposed method on two types of tasks, synthetic and LLM-based tasks, demonstrating applicability in varied settings.\n* Theoretical analysis provides insights into the characteristics of DSO, although some detailed proofs could not be fully verified by the reviewer."
            },
            "weaknesses": {
                "value": "**Lack of clarity in algorithmic steps**:\nThe specific steps for implementing the algorithm are unclear. It seems that gradient estimation would require sampling from both the prompt policy and the LLM. If this understanding is correct, how many samples would need to be generated per data point? Should this match the $m$\u00a0 samples used to estimate $\\pi_0(\\phi(s_i)|x_i)$?\n\n**Notation abuse and lack of clarity in definitions**:\nThis paper has some notation abuse, which leads to ambiguity. For example, the authors introduce $\\pi_\\theta(a|x, s)$ or $\\pi_\\theta(a|x, \\phi(s))$ as a conditional distribution over prompts given the generated sentence $s$ in  Section 4 and Appendix D.1. However, this is problematic because $\\pi_\\theta$ is originally defined as a prompt selection policy and should not depend on $s$, which the LLM generates after selecting $a$. Additionally, while the expressions are somewhat interpretable, there is a lack of consistency in function arguments throughout the paper. For instance, $\\pi_\\theta(s|x)$\u00a0is used without explanation as\u00a0$\\sum_a\u00a0\\pi_\\theta(a|x) p_{LLM}(s|x,\u00a0a)$.\u00a0To improve clarity, the authors should avoid redefining $\\pi_\\theta$ with different inputs and instead provide explicit auxiliary definitions where needed, along with a rationale for introducing these conditional probabilities.\n\n**Unpractical setting  in Full-LLM Experiment with MovieLens**:\nThe LLM-based experiment in Section 7 lacks realistic user personalization. As shown in Figures 10 and 12, the prompt policy reduces user information to a single word (from a set of only 1000 words) before feeding it to the LLM. This simplistic representation raises concerns about whether the Full-LLM experiment setup can effectively capture real-world personalization. Without a richer prompt (e.g., short sentences) to convey nuanced user information, it is unclear if this approach offers any advantage over simply passing user attributes directly to the LLM. Consequently, this setup might be better categorized as a toy task rather than a realistic evaluation of the proposed method's applicability in real-world tasks.\n\n**Concerns regarding the formulation of baseline approaches**:\nThe problem formulation in this work is novel; however, applying existing methods, particularly the regression approach, seems overly naive for this setup. Since the LLM that generates $s$ is available in this setup, it would be more appropriate for the reward predictor to take $(x, s)$ as input instead of $(x, a)$. Otherwise, the reward predictor would have to learn the LLM's inherent randomness (noise), which seems inefficient. Using $(x, s)$ would allow the reward predictor to avoid this redundancy and better capture the generated sentence features. A Nadaraya-Watson kernel regression (using the same kernel as in DSO) or a neural model like DistilBERT could be employed as the reward predictor to improve adaptability. In connection with the above, in the numerical experiments, using $(x, a)$ as the reward predictor's input in the regression approach may be unfair as a baseline comparison against DSO. DSO leverages (multiple) generated sentence(s) $s\u2019$ for each context $x$ sampled from $\\pi_\\theta$ and the LLM. Thus, any observed performance gap between DSO and the regression approach may simply be due to this difference in formulation rather than\u00a0any inherent\u00a0advantage\u00a0of\u00a0DSO.\n\n**Organization of the paper**:\nThe structure of the paper could be improved. For instance, details of the synthetic experiment setting and Section 4.2 (not cited in the main text) could be moved to the appendix, as these sections may be of lower priority for understanding the main contributions. Shifting these sections would allow more space for core elements like detailed algorithmic steps, problem setup, and full LLM experiment details in the main text."
            },
            "questions": {
                "value": "**Figure 6 Interpretation**:\nIt seems that each bar in Figure 6 represents the results across 5 random seeds. Given the variation across seeds, can we still conclude that the proposed method (DSO) consistently outperforms the regression baseline? The performance between DSO and regression appears similar when accounting for this variability.\n\n**Minor comments**\n* Line 391: $\\sigma_o$ should be $\\sigma_s$?\n* Line 989: MSE loss should be $\\sum_{i=1}^{n} (r_i - \\hat{q}(x_i, a_i))^2$ instead of $\\sum_{i=1}^{n} (r_i - \\hat{q}(x_i, a_i))$.\n* Line 1075: $\\nabla_{\\theta} \\pi_{\\theta}$ should be $\\nabla_{\\theta} \\log \\pi_{\\theta}$?\n* In Section 3.1, the classification of \"conventional approaches\" into \"regression-based methods\" and \"importance sampling (IS)\" feels somewhat unclear. It may be more intuitive to categorize these as \"reward predictor-based approaches\" and \"reward predictor-free approaches.\" This distinction clarifies that IS methods directly use observed rewards, whereas regression-based methods estimate rewards across all actions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents Direct Sentence Off-policy gradient (DSO) for optimizing large language model (LLM) pipelines using logged user feedback such as clicks. DSO addresses the challenges of high variance and bias in policy gradient estimation by leveraging the similarity among generated sentences. The paper provides theoretical analysis on the source of bias and variance reduction of DSO. Experiments on both synthetic environment and a proposed benchmark (OfflinePrompts based on MovieLens-10M) demonstrate the effectiveness of this method. OfflinePrompts is a new benchmark suite,to demonstrate DSO's effectiveness in generating personalized movie descriptions. This is an additional contribution of the paper by providing a practical solution for leveraging naturally logged feedback for prompt policy optimization in language generation tasks."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- The algorithm DSO motivated by utilizing the information behind the sentence embedding is generally sound.\n- The theoretically anslysis highlights the benefit of such algorithimic designs by indicating the source of bias and variance of such algorithms.\n- The introduction of the OfflinePrompts benchmark suite is a valuable resource for the research community, facilitating further development and testing of off-policy learning methods for language generation"
            },
            "weaknesses": {
                "value": "The experiments for real-world validation is insufficient. (Indeed, we lack good benchmarks for this task.) How well does the real-world performance align with the score/reward in the simulated environment (OfflinePrompts)? I found Figure 11 in the appendix indicates the positive correlation between the simulated rewards and the click feedback from users. Is there other statistics (such as the accuracy)? I am curious on the click rate improvement using the policy trained by DSO in real-world settings."
            },
            "questions": {
                "value": "How well the sythetic environments represent the real case? I note that there are some gaps between the sythetic environments and the target task. For example, reward is real-valued in synthetic case but it is binary in the real case (click or not); the policy is parameterized by an estimated reward function in the sythetic case."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a new method for offline prompt policy learning for LLMs. The main challenge in this setting is the distribution shift between the logged data and the target data. Importance sampling can correct the distribution shift but only at the cost of potentially very high variance. The key idea behind the new method is to exploit similarity relations between sentences to reduce the variance. The bias-variance trade-off of the new method is analyzed theoretically and the method is tested on synthetic data and a LLM movie description task."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The method is well-motivated and the theoretical analysis supports the desired variance reduction. Intuition for the analysis is provided. \n* Ablations w.r.t to differences in the setting (dataset size, number of actions, reward noise) and w.r.t to the hyperparameters (kernel type, kernel bandwidth) of the method are carried out.\n* Plan to open-source a benchmark for offline prompt policy learning"
            },
            "weaknesses": {
                "value": "* Figure 6: there are 5 bars for each method. I was/am a bit confused about what the difference between these bars is. For now, I assume these are the results from the 5 random seeds, ordered by performance. But I think it would be good to have a label for this or mention it in the Figure caption. \n* Literature on contextual bandits/kernelized bandits is left out.\n* The performance gain (in particular compared to regression) seems much stronger in the synthetic setting than in the full-LLM experiment."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "I am not an expert on this, but I suspect that increasing personalization can also have potentially harmful social consequences (e.g. by reinforcing bubbles). On the other hand, I don't see an immediately greater risk than for other personalization methods that already exist and are widely accepted. So, I guess, it's fine."
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}