{
    "id": "2hKDQ20zDa",
    "title": "Language Reconstruction with Brain Predictive Coding from fMRI Data",
    "abstract": "Many recent studies have shown that the perception of speech can be decoded from brain signals and subsequently reconstructed as continuous language. However, there is a lack of neurological basis for how the semantic information embedded within brain signals can be used more effectively to guide language reconstruction. Predictive coding theory suggests the human brain naturally engages in continuously predicting future words that span multiple timescales. This implies that the decoding of brain signals could potentially be associated with a predictable future. To explore the predictive coding theory within the context of language reconstruction, this paper proposes PredFT (FMRI-to-Text decoding with Predictive coding). PredFT consists of a main decoding network and a side network. The side network obtains brain predictive coding representation from related brain regions of interest (ROIs) with a self-attention module. This representation is then fused into the main decoding network for continuous language decoding. Experiments are conducted on two popular naturalistic language comprehension fMRI datasets. Results show that PredFT achieves current state-of-the-art decoding performance on several evaluation metrics. Additional observations on the selection of ROIs, along with the length and distance parameters in predictive coding further guide the adoption of predictive coding theory for language reconstruction.",
    "keywords": [
        "fMRI-to-text decoding",
        "predictive coding theory"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=2hKDQ20zDa",
    "pdf_link": "https://openreview.net/pdf?id=2hKDQ20zDa",
    "comments": [
        {
            "title": {
                "value": "Rebuttal (part 2)"
            },
            "comment": {
                "value": "### Questions\n\n#### Q1\n\n> What would be the chance-level performance when reconstructing continuous language?\n\nWe are conducting a chance-level baseline, please lend us more time.\n\n> what is the percentage of overlap between random ROIs and whole-brain voxels?\n\nNone overlapping.\n\n> Did the authors repeat the selection of random ROIs multiple times to ensure robustness?\n\nYes, the selection of random ROIs is repeated five times and the results are similarly poor.\n\n#### Q2\n\n> What is the rationale for using 4D volume data from the Narratives dataset while using 2D brain data from the Moth Radio Hour dataset?\n\nWe selected 4D volumetric whole-brain data from the Narratives dataset to align with the settings used in Unicorn, allowing us to compare performance of different models.\n\n#### Q3\n\n> There is no interpretation provided for the two encoders used in PREDFT.\n\nThanks for raising this problem. We think directly evaluating the quality of encoder representation is hard, since we apply a deep learning approach instead of a linear encoding model. \nUsing the change of decoding performance to judge the quality of encoder representation is an alternative and meaningful approach.\nProject the representation to brain map doesn\u2019t seem to lead to results with practical meaning. Previous works didn\u2019t test the encoding quality in this way either.\n\n#### Q4\n\n> Figures 3, 4, 6, and 8 appear redundant. \n\nWe hope showing different components separately can help readers understand the model easily. We also provide the whole framework of PredFT in Figure 11 in the Appendix.\n\n#### Q5\n\n> What does the y-axis represent in Figure 9?\n\nY axis represents the numerical value of score in a percentage-based system (e.g. 40 means 40%). We will polish the figure.\n\n#### Typos\n\nThanks for pointing out. We will fix it.\n\n### BERTScore supplementary\n\n|    |                    |       |\n|----|--------------------|-------|\n|  | Tang's             | 80.84 |\n|    | BrainLLM           | **83.26** |\n|   S1 | MapGuide           | 82.66 |\n|    | PredFT w/o SideNet | 81.35 |\n|    | PredFT             | 82.92 |\n|    |                    |       |\n|  | Tang's             | 81.33 |\n|    | BrainLLM           | **83.4**  |\n|  S2  | MapGuide           | 82.78 |\n|    | PredFT w/o SideNet | 81.42 |\n|    | PredFT             | 82.52 |\n|    |                    |       |\n|  | Tang's             | 81.5  |\n|    | BrainLLM           | **83.82** |\n|  S3  | MapGuide           | 82.84 |\n|    | PredFT w/o SideNet | 81.48 |\n|    | PredFT             | 82.11 |\n||||\n\n|    |                    |       |\n|----|--------------------|-------|\n| | unicorn            | 75.35 |\n| 10   | PredFT w/o SideNet | 75.26 |\n|    | PredFT             | **78.52** |\n|    |                    |       |\n| | unicorn            | 74.88 |\n|  20  | PredFT w/o SideNet | 75.16 |\n|    | PredFT             | **78.2**  |\n|    |                    |       |\n| | unicorn            | 74.4  |\n|  40  | PredFT w/o SideNet | 75.07 |\n|    | PredFT             | **78.63** |\n|    |                    |       |"
            }
        },
        {
            "title": {
                "value": "Rebuttal (part 1)"
            },
            "comment": {
                "value": "We sincerely appreciate your effort in reviewing our paper. We will address your concerns point by point.\n\n### Weaknesses\n\n#### Weakness 1\n\n> There are several major weaknesses in this work, particularly concerning the evaluation of reconstruction results. A major concern is that the current study (PREDFT) does not provide a clear evaluation of reconstruction results.\n\n**We have already provided a case analysis containing reconstruction results in Appendix F.** If you are still confused about the reconstruction results, please refer to our reply to reviewer XUt4.\n\n> For example, the authors did not evaluate the word rate in the generated narrative story.\n\nThere's no word rate model in PredFT. We don't use a fixed LLM to endlessly generate decoded content like Tang's model, in which case a word rate model is needed to judge the end point. Instead, we view fMRI-to-text decoding as sequence-to-sequence translation task and train PredFT in an end2end manner.\nSo how many words should be decoded is already learned by PredFT. We only need to set a sampling method (e.g. greedy decoding).\n\n#### Weakness 2\n\n> However, prior studies have focused on specific ROIs, such as the language, prefrontal, and auditory association cortices...\n\nIn the main decoding network, we apply whole brain data for reconstruction instead of selecting specific ROIs related to language comprehension. In the side network, we use BPC area for language reconstruction, which covers most part of language related areas like auditory cortex (AC), prefrontal cortex (PFC) and Broca area.\n\n> The random selection of ROIs generally leads to low decoding performance. What are these random ROIs? Do they have any overlap with BPC ROIs?\n\nThe specific ROIs we applied are listed in Appendix A.4. For example, for narratives dataset G_and_S_cingul-Ant, G_and_S_subcentral, G_and_S_transv_frontopol, G_orbital, S_front_middle, S_subparietal are selected. For LeBel's dataset, we randomly choose 1000 voxels from brain surface data besides BPC area(because fMRI in this dataset isn't projected to a standard space (e.g. MNI space), so we can't apply a brain atlas for parcellation). There is no overlap between Random ROIs and BPC ROIs.\n\n> this paper does not report any reconstructed stimulus in the main content, nor does it include analysis at the ROI level. \n\nThe reconstructed stimulus is shown in Appendix F. Since we use whole brain data in main decoding network for language reconstruction, there's no available ROI level reconstructed stimulus.\nWe also notice that only Tang's work conducted ROI level reconstructed stimulus, while other previous works (BrainLLM, unicorn, mapguide, BP-GPT) didn't include it.\n\n> Additionally, the authors only used two metrics, and throughout the paper.\n\nWe add another metric BERTScore. Please refer to rebuttal part 2.\n\n#### Weakness 3\n\n> there are no qualitative reconstruction results for these different predictive lengths and distances.\n\nWhen the prediction length and distance is too long or short, it becomes distraction and the decoded content becomes meaningless. No essential concepts or semantics can be decoded. So we think there's no need to show them. The reconstruction results with proper length and distance are shown in Appendix F.\n \n> What type of information is the model forecasting based on brain data?\n\nThe training objective for side network is cross-entropy loss between generated words and label, and the label is ground truth word sequence with certain prediction length and distance from the onset word of each fMRI frame. So the prediction is supposed to be the combination of syntactic and semantic information. We can't show what exactly the model predicts since the predictive information is presented in the form of representation, which is a deep learning approach. We can only judge whether it works through reconstruction performance (e.g. bleu).\n\n#### Weakness 4\n\n> All the figures lack detailed captions.\n\nThe explanation for concepts like \"prediction score\", \"prediction distance\" is presented in the first paragraph of section 2. We will add more captions in the edited version of paper for better understanding.\n\n#### Weakness 5\n\n> it is unclear which component is primarily responsible for reconstructing the language and which component provides the theme and narrative structure.\n\nThe main decoding network in PredFT is responsible for whole text reconstruction, while the side network provides predictive coding information and the decoder in side network is discarded after training. **Therefore, no specific part is claimed to be responsible for reconstructing the language, nor is any particular part claimed to provide the theme and narrative structure, the decoder in main decoding network generates as a whole.**"
            }
        },
        {
            "title": {
                "value": "Thanks for your reply, and we would like to highlight the contribution of this work."
            },
            "comment": {
                "value": "Thanks for the review's reply. We find your primary concern lies in that our model fails to outperform previous methods in a very small part of evaluation metrics.\nModel performance is crucial without doubt. **But we don't agree that a model needs to beat previous methods in every single aspect to be called a \"good\" one.**\nWe can list many top AI conference papers that do not outperform previous methods in every metric (a simple case is a better LLM A doesn't need to beat LLM B in every benchmark).\nMore importantly, besides the improvement of decoding performance, the most significant contribution of our paper is that we first design a model that combines neuroscience theory and deep learning in fMRI-to-text decoding, while previous methods focused on pure deep learning tricks. It has great potential to inspire future work in this domain.\n\nAs to your secondary concern, **we believe the best way to clarify whether predictive coding improves fMRI decoding performance is to evaluate the decoding performance itself, but not to observe the correlation between some layer of PredFT and brain response (i.e. prediction score).** We designed several ablation studies to show the effectiveness of predictive coding: PredFT w/o predictive coding, ROIs selection, prediction length and distance study. In short, we have addressed this concern in our paper.\n\nAs to your final concern, we will add relative discussion in edited version of paper. Here's our explanation: We think it's the model design of PredFT that leads to this phenomenon: We apply a Transformer based SideNet for modeling predictive coding, so too long and far predicted content will lead to poor training because of the noise introduced. A short and close predicted information will lead to better predictive coding representation, thus improving decoding performance. This modeling is similar to the actual predictive coding mechanism in human brain: it only makes prediction about near future.\n\nWe sincerely hope the reviewer may reconsider the rating given our clarification."
            }
        },
        {
            "title": {
                "value": "The experimental results cannot achieve higher rating."
            },
            "comment": {
                "value": "While this article presents strong experimental results that highlight the significance of its motivation, its model performance falls short of the state-of-the-art (SOTA) in several metrics. Specifically, this is evident in BLEU-3, BLEU-4, ROUGE-R, and ROUGE-P scores for certain subjects, as well as the BERTScore across all subjects on LeBel's dataset. These results suggest that there remains room for improvement if the authors aim to leverage Predictive Coding theory to enhance fMRI decoding methods. This performance gap is also a primary reason for my lower rating.\n\nRegarding the use of predictive scores to evaluate the proposed method, I believe the output layer of the decoder in the authors\u2019 model plays a role analogous to the output layer of a language model (LM). Therefore, it could reasonably be treated as an LM for the purpose of calculating predictive scores. Experiments utilizing predictive scores would help demonstrate whether the decoder\u2019s output layer in the main and future networks exhibits the same predictive coding phenomenon as the corresponding fMRI signals. Furthermore, the author provide results on the impact of predictive distance on metrics in the appendix made me curious. Specifically, when predictive distance changes, does the predictive score\uff08or an alternative similarity metric, if the authors find predictive scores inappropriate\uff09 between the decoder's features and fMRI signals align with changes in the metrics? I encourage the authors to consider this experiment and provide explanations for the results, as this would help clarify whether predictive coding improves fMRI decoding performance.\n\nThe authors have their perspective on the use of predictive scores, arguing that such scores must be calculated between an LM and brain responses, and their model is a neural decoding model rather than an LM. However, given the performance issue of their method, I view the question of predictive scores as a secondary concern.\n\nFinally, since the authors have presented results showing the impact of predictive distance on the metrics, it is reasonable for reviewers to question the underlying causes of these effects. This naturally draws attention to the predictive score mentioned in the paper. If the authors do not plan to include additional experiments to explore this issue in future work, it is recommended that they provide a detailed explanation within the paper."
            }
        },
        {
            "title": {
                "value": "Rebuttal (part 2) BERTScore results"
            },
            "comment": {
                "value": "We add BERTScore of different models here.\n\nFor LeBel's dataset:\n\n|    |                    |       |\n|----|--------------------|-------|\n|  | Tang's             | 80.84 |\n|    | BrainLLM           | **83.26** |\n|   S1 | MapGuide           | 82.66 |\n|    | PredFT w/o SideNet | 81.35 |\n|    | PredFT             | 82.92 |\n|    |                    |       |\n|  | Tang's             | 81.33 |\n|    | BrainLLM           | **83.4**  |\n|  S2  | MapGuide           | 82.78 |\n|    | PredFT w/o SideNet | 81.42 |\n|    | PredFT             | 82.52 |\n|    |                    |       |\n|  | Tang's             | 81.5  |\n|    | BrainLLM           | **83.82** |\n|  S3  | MapGuide           | 82.84 |\n|    | PredFT w/o SideNet | 81.48 |\n|    | PredFT             | 82.11 |\n||||\n\nFor Narratives dataset:\n\n|    |                    |       |\n|----|--------------------|-------|\n| | unicorn            | 75.35 |\n| 10   | PredFT w/o SideNet | 75.26 |\n|    | PredFT             | **78.52** |\n|    |                    |       |\n| | unicorn            | 74.88 |\n|  20  | PredFT w/o SideNet | 75.16 |\n|    | PredFT             | **78.2**  |\n|    |                    |       |\n| | unicorn            | 74.4  |\n|  40  | PredFT w/o SideNet | 75.07 |\n|    | PredFT             | **78.63** |\n|    |                    |       |"
            }
        },
        {
            "title": {
                "value": "Rebuttal (part 1)"
            },
            "comment": {
                "value": "We sincerely appreciate your effort in reviewing our paper. We will address your concerns point by point.\n\n> Unconvincing reconstruction result.\n\nWe fully understand your concern. We noticed similar problems during experiments, and here's our explanation.\n\nfMRI-to-text decoding is an important but extremely difficult task due to 1. noisy fMRI signal with latency 2. mismatch between fMRI sampling frequency (2s) and word rate (0.2s). We haven't found any models that can decode satisfying text. If you detailedly read the cases (not cherry picked cases) in previous work (e.g. page S2 in Tang's paper <https://www.biorxiv.org/content/10.1101/2022.09.29.509744v1.full.pdf> ), **you will find that, although the generated content is locally coherent, it's far from being relevant to the ground truth.** We present the decoding results of the first 10 seconds from Tang's paper here for example:\n\n| Truth                                                                           | S1                                                                                                                            | S2 | S3 |\n|---------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|----|----|\n| I had no shoes on I was crying i had no wallet but i was ok because i had my cigarettes | she said she was a little stressed out because she wasn't doing anything wrong and had a lot of anxiety issues but i had been | i don't have any friends that have one and i really don't care if you do i have a girlfriend i don't mind at all   |  we got in my car and i was crying i didn't have my purse i don't have any money to pay for gas i  |\n\nS3 is considered as good case by the authors, while it only decodes the concept of \"crying\" and \"purse\". Not to mention the content of S1 and S2 is completely irrelevant.\nOur model can decode more concepts as shown in our random picked cases. So the question becomes: **Could it be said that a coherent but completely irrelevant text is preferable to an incoherent text but containing more important concepts in fMRI-to-text decoding task?** We believe using metrics to evaluate different models is fair and reliable. BLEU has several drawbacks as you mentioned, which makes BLEU-1 less convincing. But BLEU-3, 4 is reliable. Because there're not many meaningless and repeated trigrams in ground truth, so high BLEU-3,4 prove that model decodes more important concepts. We believe the 5.62 BLEU-3 just shows superiority of our model.\n**Besides, we supplement BERTScore to evaluate semantic similarity for more convincing results in Rebuttal (part 2).** Results show our model performs good in semantic-level reconstruction (the gap compared to the best is narrow), while maintaining the best BLEU and ROUGE score.\n\nMoreover, we have identified the reasons that lead to this phenomenon. We divide existing models into two categories:\n\n1. The parameters in LM are fixed (Tang's method, BrainLLM, MapGuide). \n\n- Advantages\n\n    - Can generate coherent text with the power of pre-trained LM.\n\n- Disadvantages\n\n    - This approach largely restricts model architecture. Only the input representation can be changed. The innovation of previous works is that they try various ways to enhance input representation.\n\n2. The parameters in LM are changed (Unicorn, PredFT)\n\n- Advantages\n\n    - The model design is flexible without restriction. Many innovations might be sparked.\n\n- Disadvantages\n\n    - It's hard to maintain good coherence when finetuning LM or training from scratch due to the very complex relationship between fMRI and text (as mentioned in the beginning).\n\n**Both types of models can decode part of important concepts.**\n\nIn conclusion, while we still have a long way to go towards coherent and accurate fMRI-to-text decoding, we hope the above clarification convinces you that our work represents a solid step.\n\n> The paper is also quite difficult to read for no reason and pointlessly notational...\n\nThe three attention equations represent different meanings:\n\n- Self-attn: q, k, v all come from the input to self-attention layer. It captures relationship between words.\n\n- ED-attn: q comes from the input to self-attention layer, k and v come from the output from main network encoder. It captures relationship between fMRI and words.\n\n- PC-attn: q comes from the input to self-attention layer, k and v come from the output from side network encoder. It captures relationship between predictive information and words.\n\nWe hope readers can have a better understanding of how PredFT operates (especially the input of different attention blocks) with the presented equations. We will only keep the predictive coding attention (PC-attn) equation in the edited version if equations become distractions."
            }
        },
        {
            "title": {
                "value": "Rebuttal"
            },
            "comment": {
                "value": "We sincerely appreciate your effort in reviewing our paper. We will address your concerns point by point.\n\n> Why did the author only use BLEU and ROUGE in the experiment?\n\nWe choose BLEU and ROUGE in the experiment to follow the evaluation setting in UniCoRN [1]. Generally speaking, BLEU, ROUGE, WER, METEOR all measure word-level overlapping between generated content and ground truth. Choosing any of them will lead to similar results, while BERTScore is designed to evaluate semantic-level similarity. **We add the BERTScore result of different models here for supplementary.**\n\n|    |                    |       |\n|----|--------------------|-------|\n|  | Tang's             | 80.84 |\n|    | BrainLLM           | **83.26** |\n|   S1 | MapGuide           | 82.66 |\n|    | PredFT w/o SideNet | 81.35 |\n|    | PredFT             | 82.92 |\n|    |                    |       |\n|  | Tang's             | 81.33 |\n|    | BrainLLM           | **83.4**  |\n|  S2  | MapGuide           | 82.78 |\n|    | PredFT w/o SideNet | 81.42 |\n|    | PredFT             | 82.52 |\n|    |                    |       |\n|  | Tang's             | 81.5  |\n|    | BrainLLM           | **83.82** |\n|  S3  | MapGuide           | 82.84 |\n|    | PredFT w/o SideNet | 81.48 |\n|    | PredFT             | 82.11 |\n||||\n\n|    |                    |       |\n|----|--------------------|-------|\n| | unicorn            | 75.35 |\n| 10   | PredFT w/o SideNet | 75.26 |\n|    | PredFT             | **78.52** |\n|    |                    |       |\n| | unicorn            | 74.88 |\n|  20  | PredFT w/o SideNet | 75.16 |\n|    | PredFT             | **78.2**  |\n|    |                    |       |\n| | unicorn            | 74.4  |\n|  40  | PredFT w/o SideNet | 75.07 |\n|    | PredFT             | **78.63** |\n|    |                    |       |\n\n\nResults show our model performs good in semantic-level reconstruction (the gap compared to the best is narrow), while maintaining the best BLEU and ROUGE score.\n\n[1]UniCoRN: Unified Cognitive Signal ReconstructioN bridging cognitive signals and human language. ACL'23\n\n> Many of the methods compared by the author incorporate LLM, while the author's model is entirely trained with their own transformer. Does this result in the author's method being inferior to the baseline method in terms of semantic similarity?\n\nThe above supplementary BERTScore result confirms that our model achieves similar semantic reconstruction ability as LLM-based method. Moreover, the disadvantage of LLM-based method is that the parameters of LLM is fixed and can't be finetuned. Only the input of LLM could be changed and previous methods (Tang's model, BrainLLM, MapGuide) try different ways to improve input representation. While training Transformer based model from scratch is more flexible (allowing architecture changes).\n\n> The author's method was inspired by predictive coding and validated it on LLM using a prediction score. But can the author's own model still observe the same phenomenon on the prediction score? I haven't seen the same experiment evaluating the author's own model.\n\nWe have to clarify that they're two completely different types of tasks:\n\n- Prediction score\n\n  It belongs to brain-LM alignment research (also termed brain encoding in some paper). The common method is mapping (e.g. via linear model, rsa) the output of LM layer to brain response, and analyzing voxel-level correlation. It aims to discover whether LM has human-like language comprehension ability.\n\n- Language reconstruction\n\n  The task is also known as brain decoding, fMRI-to-text decoding. It aims to decode natural language from brain recordings.\n\nAs a result, it's meaningless to calculate the prediction score for PredFT because it's not a language model.\nEven if we do so, the output result lacks practical meaning.\nWe present the prediction score experiment to highlight how we get the motivation of PredFT: while predictive coding improves brain encoding, could it help improve brain decoding?\n\n> In some parts of the paper, fMRI is spell as FMRI.\n\nfMRI is written as FMRI only in Abstract to highlight how we name the model: the **FT** in PredFT comes of **F**MRI-to-**T**ext."
            }
        },
        {
            "title": {
                "value": "Rebuttal (part 2)"
            },
            "comment": {
                "value": "> In the regions of interests selection experiment, the authors only consider 'random,' 'whole,' and 'BPC' as the ROIs...\n\nThe selected BPC area (superior temporal sulcus, angular gyrus, supramarginal gyrus, and opercular, triangular, orbital part of the inferior frontal gyrus) contributes the most to predictive coding, as indicated in some neuroscience studies [1][2].\n\nThe process for selecting \"BPC\": For the Narratives dataset, Destrieux atlas is applied and the above mentioned ROIs are extracted. For LeBel's dataset, since the fMRI signals are not projected to a standardized space, we use the \u201cAuditory\u201d region provided by the authors', containing parietal-temporal-occipital (PTO) area. The BPC area of both datasets cover highly similar area.\n\nThe process for selecting \"random\": For the Narratives dataset, G_and_S_cingul-Ant, G_and_S_subcentral, G_and_S_transv_frontopol, G_orbital, S_front_middle, S_subparieta are selected. For LeBel's dataset, we randomly choose 1000 voxels from brain surface data.\n\nThe process for selecting \"whole\": We use the whole brain surface data as ROIs for both datasets.\n\nWe believe selecting random and whole ROIs as controlled experiments is sufficient for demonstrating the effectiveness of using predictive coding to improve decoding performance: \n\n1. random vs. BPC demonstrates only ROIs related to predictive coding in human language comprehension can improve decoding.\n\n2. whole vs. BPC not only confirms conclusion in 1, but also shows whole brain surface which contains BPC area still can't contribute to better decoding, because some other brain regions contain too much noise.  \n\n3. none (PREDFT without SideNet) vs. BPC. PREDFT without SideNet is equivalent to not using any ROIs for predictive coding. This comparison shows predictive coding improves decoding accuracy significantly.\n\n**All the above clarifications are included in sec 4.3 and Appendix A.4. We will add more key information in sec 4.3 in the updated version**\n\n[1]. Evidence of a predictive coding hierarchy in the human brain listening to speech. Nature Human Behavior\n[2]. Natural speech reveals the semantic maps that tile human cerebral cortex. Nature\n\n> Could the authors provide pseudocode for the method...\n\nWe will provide pseudocode in the appendix for edited version of paper. The discussion of time complexity is already in Appendix A.4 (line 845) of original paper.\n\n> The results provided by the authors mostly only include the mean value...\n\nWe guess you indicate the experiment of analyzing the impact of prediction length and distance to model performance (sec 4.4), as we have presented results per subject for other experiments. **The per-subject results of analyzing the impact of prediction length and distance are already presented in Figure 16,17,18 in the appendix of original paper.**\n\n> In the methods section, some symbols are not defined...\n\n**A notation table for symbols is presented in Table 3 in the appendix of original paper.**\n\n### Questions\n\nPlease refer to clarification for weaknesses."
            }
        },
        {
            "title": {
                "value": "Rebuttal (part 1)"
            },
            "comment": {
                "value": "We sincerely appreciate your effort in reviewing our paper. We will address your concerns point by point.\n\n### Weaknesses\n\n> In Section 3.3, the authors state, 'During the inference stage, as illustrated in Figure 8, the decoder in the side network is abandoned.' However, they do not provide a detailed explanation of why the decoder is discarded or discuss the potential impact of this decision...\n\nWhat we really want is predictive coding representation, which is produced by side network encoder. The side network decoder is designed to help train the side network encoder (we can't find out how to directly obtain predictive coding representation). \nSpecifically, during the training process, the label for side network decoder is predicted words instead of complete sentences (as shown in Figure 3). The side network learns mapping between specific areas of brain (BPC area) and predicted words.\nHowever, the goal of our task is to decode complete sentences. So the side network decoder is useless after training.\n\n**We will add the motivation and reason for discarding the side network decoder in the methodology section in the updated version.**\n\n> As shown in Table 1, PREDFT does not achieve the best performance on ROUGE1-R...\n\nWe think the different lengths of generated content might contribute to this factor, since we don't apply a word rate model to control the number of generated words.\nAlthough PREDFT fails to outperform other models in ROUGE1-R, the gap is narrow.\nJust like recall and precision to f1 score, ROUGE-Recall measures the extent to which a machine-generated content captures the information contained in a reference content, which is a single perspective assessment. ROUGE-Recall and ROUGE-Precision are characterized by a trade-off. When PREDFT gets a relative low ROUGE-R, it gets a high ROUGE-P.\nInstead, ROUGE-F1 is a more comprehensive indicator, combining both ROUGE-P and ROUGE-R. Our model outperforms other models in this metric. \n\n> As shown in Table 1, PREDFT without SideNet performs similarly to other methods. However...\n\nThe SideNet is designed to obtain predictive coding representation.\nPREDFT without SideNet can be viewed as traditional deep learning approach which directly applies Transformer to decode text from brain recordings, while PREDFT with SideNet combines deep learning and neuroscience findings (predictive coding). The SideNet provides predictive coding representation to the decoder in Main network, and the decoder incorporates both current fMRI representation and predictive coding representation for text decoding.\n\nThe idea of PREDFT is motivated by predictive coding theory in Neuroscience, which indicates human can naturally predict upcoming words. Since predictive coding has been verified to contribute to human language comprehension, we seek to investigate whether such predictive information can help language reconstruction. \nThe improvement of incorporating SideNet highlights 1. the effectiveness of our model design 2. predictive coding has potential to improve brain-to-text decoding. **We will provide the illustration of PREDFT without SideNet for better understanding in the updated version.**\n\n> Although the authors provide a detailed description of the hyperparameter selection...\n\nAll the hyperparameters are chosen to minimize the training & validation loss as much as possible.\nWe don't understand which hyperparameter the reviewer is confused about.\nThe influences of ROIs selection, prediction length, prediction distance,  $ \\lambda $ to model performance are detailedly discussed in sec 4.3, sec 4.4, Appendix E. The learning rate is set to stabilize training. We don't test the influence of model layers (e.g. Transformer layers) due to limited computational resources."
            }
        },
        {
            "summary": {
                "value": "The paper describes a decoding method \"PredFT\" that uses a main decoding network and a side network to perform decoding from fMRI recordings of subjects listening to stories to text. The side network is responsible for obtaining predictive coding representations from specific brain regions and integrating them into the main network, enhancing language decoding. The authors claim that this integration leverages brain regions known for predictive functions (like the parietal-temporal-occipital areas) to better align brain signal decoding with anticipated semantic content. This is supported by results that have claimed the brain performs predictive coding during language stimulation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The attempt to use hypothesized predictive coding representations to enable better text decoding is interesting."
            },
            "weaknesses": {
                "value": "My main concern is that the metric does not seem to produce even locally coherent text, which substantially damages the authors' claims that this method is an advancement over prior work, such as Tang et al., which uses an LM to guarantee local coherence. Consider the following example from the case study: \"He don\u2019t know my girl you of the eyes but his girl sleep he and he said and he said and the to the and and which I not wrong. But the Guy\". Clearly, this has no meaning, and does not even obey basic local grammatical rules (e.g. \"and and\"). The problem seems to be that the model has merely learned repeat short, high-frequency words like \"the\", \"he\" and \"and\", which improves BLEU/ROGUE score but does not actually move forward towards the goal of better language decoding. I imagine if you just had the model repeatedly and randomly output words sampled from the top 100 most common English words that it would behave fairly similarly. My expectation is that a small percentage of the improvement in BLEU score is genuinely derived from brain signals, with most of the benefit deriving from this output bias. The unreasonably high 5.62 BLEU-3 score when compared to other methods is more of a red flag, because its pretty clear that the model is simply guessing every high frequency trigram in the English language.\n\n The paper is also quite difficult to read for no reason and pointlessly notational, for example when the self-attention equation is repeated three separate times in only slightly different ways."
            },
            "questions": {
                "value": "Please see weaknesses. I would need to be convinced that majority of the claimed improvements in the model are not merely from a bias towards outputting high-frequency words, and thereby overfitting the chosen test metrics of BLEU and ROGUE, in order to change my score. Right now, I am fairly convinced that this is the case."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents PREDFT (FMRI-to-Text Decoding with Predictive Coding), a novel framework that utilizes predictive coding to translate fMRI signals into continuous language. This approach combines a primary decoding network with an auxiliary network focused on capturing brain predictive coding, aiming to improve the accuracy of language reconstruction from brain signals. The authors conduct experiments on two established naturalistic language comprehension fMRI datasets, showing that PREDFT achieves state-of-the-art performance across multiple evaluation metrics."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Integrating predictive coding theory into the decoding process offers a fresh perspective on reconstructing language from brain signals.\n2. Experimental results demonstrate that PREDFT outperforms other methods across various evaluation metrics, showing significant improvements."
            },
            "weaknesses": {
                "value": "1. In Section 3.3, the authors state, 'During the inference stage, as illustrated in Figure 8, the decoder in the side network is abandoned.' However, they do not provide a detailed explanation of why the decoder is discarded or discuss the potential impact of this decision. It is recommended to elaborate on the rationale behind this choice and its implications on the overall performance and functionality of the model.\n\n2. As shown in Table 1, PREDFT does not achieve the best performance on ROUGE1-R. The authors should analyze the potential reasons for this and discuss any factors that may have contributed to the lower performance in this specific model. For instance, the model's architecture, training process, or characteristics of the ROUGE1-R metric that might explain the discrepancy. If the authors observed any patterns in the types of language constructs where PREDFT underperformed on ROUGE1-R.\n\n3. As shown in Table 1, PREDFT without SideNet performs similarly to other methods. However, the inclusion of SideNet leads to a significant performance improvement. The authors should provide a detailed analysis of this phenomenon to explain how SideNet contributes to the model's enhanced performance.\n\n4. Although the authors provide a detailed description of the hyperparameter selection, they do not explain the rationale behind these choices. How these choices relate to the model's performance or the underlying theory of predictive coding.\n\n5. \"In the regions of interests selection experiment, the authors only consider 'random,' 'whole,' and 'BPC' as the ROIs, which appears somewhat limited. The paper does not clarify whether there are other potential ROIs associated with predictive coding, nor does it provide supporting neuroscience literature for the selection of BPC. It is recommended to either justify the choice of BPC with relevant references or explore additional ROIs to strengthen the study's validity. If authors can explain the process for selecting these particular ROIs and why authors believe these are sufficient to demonstrate the effectiveness of their approach. Additionally, if authors considered any other ROIs and why those were not included in the study.\n\n6. It is recommended that the authors provide pseudocode for the method and an analysis of its time complexity to enhance the reproducibility of the article.\n\n7. The results provided by the authors mostly only include the meanvalue. The experimental results should provide the mean, variance, and statistical test results.\n\n8. In the methods section, some symbols are not defined. It is recommended that the authors compile a list of symbols used in the paper in an appendix to help readers understand better."
            },
            "questions": {
                "value": "1. In Section 3.3, the authors state that the decoder in the side network is abandoned during the inference stage. Could the authors provide a detailed explanation of why the decoder is discarded and discuss the potential impact of this decision on the overall performance and functionality of the model?\n\n2. As shown in Table 1, PREDFT does not achieve the best performance on ROUGE1-R. Could the authors analyze the potential reasons for this and discuss any factors that may have contributed to the lower performance in this specific model? For instance, how might the model's architecture, training process, or characteristics of the ROUGE1-R metric explain this discrepancy? Did the authors observe any patterns in the types of language constructs where PREDFT underperformed on ROUGE1-R?\n\n3. As shown in Table 1, PREDFT without SideNet performs similarly to other methods, while the inclusion of SideNet leads to a significant performance improvement. Could the authors provide a detailed analysis of this phenomenon to explain how SideNet contributes to the model's enhanced performance?\n\n4. Although the authors provide a detailed description of the hyperparameter selection, could they explain the rationale behind these choices? How do these choices relate to the model's performance or the underlying theory of predictive coding?\n\n5. In the regions of interest selection experiment, the authors only consider 'random,' 'whole,' and 'BPC' as the ROIs. Could the authors clarify whether there are other potential ROIs associated with predictive coding? If so, could they provide supporting neuroscience literature for the selection of BPC? Additionally, can the authors explain the process for selecting these particular ROIs and why they believe these are sufficient to demonstrate the effectiveness of their approach? Did the authors consider any other ROIs, and if so, why were those not included in the study?\n\n6. Could the authors provide pseudocode for the method and an analysis of its time complexity to enhance the reproducibility of the article?\n\n7. The results provided by the authors mostly only include the mean value. Could the authors include the variance and statistical test results in the experimental results?\n\n8. In the methods section, some symbols are not defined. Could the authors compile a list of symbols used in the paper in an appendix to help readers understand better?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In the submission-6263, the authors  propose PREDFT (FMRI-to-Text decoding with Predictive coding) , which was inspired by predictive coding theory. This theory suggests that when humans listen to a certain speech, their subconscious brain predicts the words they may hear next. Then the author validated this theory through a prediction score. The verification method is to first calculate the correlation coefficient between the features extracted by LLM at the current location and the brain features, and then add the features of an upcoming text segment to the current location features, calculate the correlation coefficient again, and observe the changes in the correlation coefficient. The experimental results show that incorporating upcoming text features can increase the correlation coefficient between LLM features and brain features. Based on the above experimental results, the author designed their own model, which includes the side network to decode upcoming text. In the decoding of current text, the feature from the side network is used to incorporate the predictive coding theory into the method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The author provided sufficient experiments to demonstrate the significance of his motivation."
            },
            "weaknesses": {
                "value": "Although the author's explanation of motivation is very sufficient, I still have a few major questions about the author's method and list them in the questions part."
            },
            "questions": {
                "value": "(1) Why did the author only use BLEU and ROUGE in the experiment? Why doesn't the author use WER, METEOR, and BERTScore which is used in the Tang and MapGuide? BLEU and ROUGE both evaluate the matching degree of n-grams, which can easily lead to surface matching but semantic mismatch. METEOR and BERTScore can better reflect semantic similarity.\n(2) Many of the methods compared by the author incorporate LLM, while the author's model is entirely trained with their own transformer. Does this result in the author's method being inferior to the baseline method in terms of semantic similarity?\n(3) The author's method was inspired by predictive coding and validated it on LLM using a prediction score. But can the author's own model still observe the same phenomenon on the prediction score? I haven't seen the same experiment evaluating the author's own model.\n(4) In some parts of the paper, fMRI is spell as FMRI."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Recent brain decoding studies have demonstrated that speech perception can be decoded from fMRI recordings and subsequently reconstructed as continuous language. These studies reconstruct continuous language either from specific regions of interest (ROIs) or from the whole brain, using decoder-based language models like GPT-2. Additionally, recent predictive coding studies reveal that the human brain naturally engages in continuously predicting future words across multiple timescales. Building on recent linguistic brain decoding research and the predictive coding approach, this paper explores predictive coding theory in the context of continuous language reconstruction. To this end, the authors propose PREDFT (fMRI-to-Text decoding with Predictive Coding), which consists of a main decoding network and a side network (the predictive coding component). Experimental results on two naturalistic brain datasets (Moth Radio Hour and Narratives) indicate that PREDFT achieves superior decoding performance when comparing the actual story with the reconstructed story."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The motivation for using predictive coding in continuous language reconstruction is clear and well-explained.\n2. The proposed approach aims to improve the reconstruction of narrative stories from fMRI brain data. This is a very interesting research area because reconstructing language is challenging due to the slowness of the hemodynamic response.\n3. The authors compared the reconstruction performance using evaluation metrics against recent studies. Additionally, ablation studies were conducted on the proposed approach, with and without the predictive coding component."
            },
            "weaknesses": {
                "value": "1. There are several major weaknesses in this work, particularly concerning the evaluation of reconstruction results:\n\t- A major concern is that the current study (PREDFT) does not provide a clear evaluation of reconstruction results compared to the baseline paper by Tang et al. (2023).\n\t- For example, the authors did not evaluate the word rate in the generated narrative story. Since the fMRI data was captured while participants were listening to stories, each word has an onset and offset. Similarly, during decoding, what is the word rate predicted by the proposed model, and does this word rate match the actual word rate of the original stimuli? \n\t- Therefore, comparing the reconstructed stimulus to the ground truth (i.e., the actual transcripts of the stimuli) would provide a good sense of whether the outputs are meaningful, as the dataset includes the ground truth of what words participants heard and when they heard them.\n\n2. Furthermore, the authors performed decoding using either random selections of ROIs, the whole brain, or BPC, which includes language-related ROIs. However, prior studies have focused on specific ROIs, such as the language, prefrontal, and auditory association cortices. Therefore, it is unclear how the proposed method compares with prior methods. Since the authors' main research question revolves around how semantic information is embedded in brain signals to improve decoding, they should consider these ROIs, as they maintain a hierarchy of language processing.\n\t- The random selection of ROIs generally leads to low decoding performance. What are these random ROIs? Do they have any overlap with BPC ROIs?\n\t- Previous studies have conducted both quantitative and qualitative analyses, reporting what the stimulus decoded at each ROI, including language-related regions in both the left and right hemispheres, as well as using four evaluation metrics. However, this paper does not report any reconstructed stimulus in the main content, nor does it include analysis at the ROI level. Additionally, the authors only used two metrics, and throughout the paper, the focus is more on the scores rather than on the main reconstructed language results.\n\n3. Although the authors report some results on predictive length and distance from the current word in Figure 1, there are no qualitative reconstruction results for these different predictive lengths and distances. What type of information is the model forecasting based on brain data? Is it syntactic information, such as nouns and verbs, or semantic content? This analysis is clearly missing from the paper.\n\n4. All the figures lack detailed captions. The results presented in the figures are difficult to understand. For instance, what is the prediction score in each subplot of Figure 1? What does each line in the top plots represent? What does prediction distance \"d\" refer to? Without providing clear details in the figure captions or placing the figures appropriately in the text, it becomes challenging for readers to understand the content and what is being conveyed.\n\n5. Since the authors use two encoders and two decoders in the proposed PREDFT, it is unclear which component is primarily responsible for reconstructing the language and which component provides the theme and narrative structure. It would be interesting if the authors reported the generated stimulus from individual components and from PREDFT as a whole, along with the performance metrics. This would help identify the shared and individual contributions of each component during language reconstruction."
            },
            "questions": {
                "value": "1. What would be the chance-level performance when reconstructing continuous language? Is there a baseline available for comparison? Additionally, what is the percentage of overlap between random ROIs and whole-brain voxels? Did the authors repeat the selection of random ROIs multiple times to ensure robustness, or did they only select a single set of random ROIs?\n2. What is the rationale for using 4D volume data from the Narratives dataset while using 2D brain data from the Moth Radio Hour dataset? Since the Narratives dataset includes both smoothed and unsmoothed versions, along with brain masks to select activated voxels from the 4D volume, why did the authors make these choices regarding data representation?\n3. There is no interpretation provided for the two encoders used in PREDFT. The authors could project these voxels onto brain maps to verify the quality of their encoders.\n4. Figures 3, 4, 6, and 8 appear redundant. The authors could combine these into a single figure with a comprehensive caption, instead of presenting multiple, repetitive figures.\n5. What does the y-axis represent in Figure 9?\n5. Several major questions are raised in the weaknesses section.\n\nTypos:\n\n1. Line 35: Bhattasali et al. (2019); Wang et al. (2020); Affolter et al. (2020); Zouet al. (2021)  - > (Bhattasali et al. 2019; Wang et al. 2020; Affolter et al. 2020; Zouet al. 2021)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}