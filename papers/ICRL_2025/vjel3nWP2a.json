{
    "id": "vjel3nWP2a",
    "title": "Scalable Extraction of Training Data from Aligned, Production Language Models",
    "abstract": "We show that *alignment*---a standard process that tunes LLMs to follow instructions in a harmless manner---seems to prevent existing data extraction attacks. We develop two novel attacks that undo a model's alignment and recover thousands of training examples from the popular proprietary model, OpenAI's ChatGPT.  Our most potent attack causes ChatGPT to emit training data in over 23% of conversations, and enables targeted reconstruction of chosen training documents, including those containing copyrighted or harmful content. Our work highlights the limitations of existing safeguards to prevent training-data leakage in LLMs.",
    "keywords": [
        "privacy",
        "language models",
        "data extraction",
        "security"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We show that aligned, production language models still memorize---and can be made to repeat---their training datasets through two different attacks.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vjel3nWP2a",
    "pdf_link": "https://openreview.net/pdf?id=vjel3nWP2a",
    "comments": [
        {
            "summary": {
                "value": "This paper provides a pioneering study on scaled evaluation of training data memorization issues in aligned Large Language Models (LLMs). The paper effectively defines memorization as the generation of at least 50 tokens that match training data. The authors created AUXDATASET, a 10-terabyte dataset merging four of the largest published language model training datasets, enabling systematic evaluation of the lower bound of training data memorization.\n\nThe study focuses on three aligned models (with 9 open-weight non-aligned models as baselines). GPT-3.5-Turbo/Gemini 1.5 Pro was primarily studied under prompt-based divergence attacks, while both GPT-3.5-Turbo and GPT-4, along with Llama-2-chat, were evaluated using fine-tuning-based divergence attacks to remove chatbot-like behaviors for better assessment.\n\nThe authors discovered that their divergence attacks (causing deviation from typical chatbot behavior) significantly increased the success rate of extracting memorized content from potential training data. Qualitatively, they identified memorization issues in OpenAI models, including OpenAI's proprietary data not released to the public, copyright-protected content from The New York Times, toxic content, and private information."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper addresses a critical problem in LLM development with robust methodology. The authors establish a formal framework by providing a clear definition of memorization (i.e., >50 tokens), creating a comprehensive validation corpus, and presenting results as quantifiable lower bounds on memorization issues.\n\n- The technical innovation in attack methods is compelling (**but might only be one correlated aspect of memorization as it seems the divergence attacks are solely effective to GPT models, see weakness**). The authors propose two effective approaches: a prompt-based method utilizing word repetition to elicit divergent behavior (**which seems to have been fixed by OpenAI**), and a more sophisticated fine-tuning-based divergence attack. Both methods successfully demonstrate how to bypass chatbot-like behaviors to expose memorization from OpenAI models.\n\n- The empirical analysis is thorough and well-structured. The study reveals interesting correlations between memorization and model size and introduces meaningful metrics such as unique 50-grams for measurement. The large-scale evaluation of 10 terabytes of data provides robust evidence for their findings.\n\n- The findings from OpenAI models are compellingly grounded in practical implications, demonstrating memorization of sensitive content including The New York Times' copyrighted material, toxic content, personally identifiable information (PII), and OpenAI's unreleased training data. This connection to real-world concerns enhances the paper's significance.\n\n- The paper is well-structured and clearly written, effectively communicating complex concepts and findings. The logical flow and organization of ideas contribute to its accessibility and impact."
            },
            "weaknesses": {
                "value": "The paper's primary limitation lies in the generalizability and effectiveness of its proposed divergence-based attacks. While innovative, several concerns emerge:\n\n1. Limited Applicability:\nThe prompt-based divergence attack has already been largely addressed by OpenAI and shows limited effectiveness beyond GPT-3.5-Turbo. Similarly, the fine-tuning-based divergence attack demonstrates reduced effectiveness on Llama-2-Chat, suggesting these methods might be model-specific rather than universal.\n2. Correlation Concerns:\nThe relationship between divergence behavior and memorization is not strongly established. The paper would benefit from a deeper analysis of this correlation, as the current results suggest the connection might be specific to OpenAI's training process rather than a general phenomenon across different LLMs.\n3. Methodological Limitations:\nThe heavy reliance on divergence-based attacks as the primary mechanism for revealing memorization might provide an incomplete or potentially misleading picture of the actual memorization behavior."
            },
            "questions": {
                "value": "- Could the authors elaborate additional analysis in the main text on why the divergence-based attacks show varying effectiveness across different models?\n\n- Have you explored alternative attack methods (beyond divergence-based attacks) that might be more universally effective across different LLMs? I wish to learn the authors' thoughts on this. \n\n- Can the authors provide additional analysis over cases where memorization occurs without divergence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper highlights that, despite alignment, large language models still have potential risks of leaking training data. The authors introduce two novel attack techniques, the divergence attack and the finetuning attack, to bypass alignment safeguards. The methods successfully extract thousands of data samples from models like OpenAI's ChatGPT and Google's Gemini."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality & Significance: This paper provides valuable insights into the limitations of current alignment methods in reducing the risk of training data extraction. The proposed extraction methods are both highly scalable and cost-effective. \nClarity: The paper is well-structured and easy to follow, with clear and detailed descriptions of the experiments."
            },
            "weaknesses": {
                "value": "The paper contains experimental details and some analysis of how model capacity influences memorization. The analysis is more empirical than theoretical and lacks a detailed theoretical examination of why model capacity correlates with memorization in this way."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper compares pretrained base models and aligned production models using a simple completion attack to extract training data. The findings indicate that the alignment process appears to prevent models from directly outputting training data when faced with this straightforward attack.\nTo bypass the defense mechanisms introduced by alignment, the paper proposes two novel techniques for extracting training data from aligned production LLMs: the divergence attack and the fine-tuning attack. In the divergence attack, the model is prompted to perform a repetitive task, such as repeating a specific word. This can lead the model to deviate from the original task and potentially output training data. The fine-tuning attack involves fine-tuning the model with a completion task similar to the initial completion attack, using a set of 2,000 data points.\nTo quantitatively assess the effectiveness of these techniques, a 10TB text dataset was constructed as the ground truth for training data comparison. The results demonstrate that the divergence and fine-tuning attacks were able to extract training data from ChatGPT at rates of 3% and 23%, respectively.\nIn addition to extracting training data, these attacks also induced the model to produce harmful content."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This paper underscores an important problem that current alignment techniques do not fully mitigate risks of extracting training data from LLMs.\n- This paper demonstrates the successful extraction of training data from production models in significant quantities and at a feasible cost.\n- This paper introduces a large dataset and a searching algorithm to act as a proxy for unknown training datasets and help matching the data."
            },
            "weaknesses": {
                "value": "- The divergence attack causes the model to output training data as part of its response. An additional step is needed to compare different parts of these responses with the training dataset to verify extraction. The success rate of these attacks remains limited as well. These show a gap between successfully extracting unknown training data and performing an attack similar to the membership inference attack.\n- While testing baseline attacks on 9 open base models, the paper only tests baseline attacks on one aligned model, GPT-3.5. It requires testing on more aligned models to support the claim.\n- The divergence attack proves effective only on ChatGPT and does not transfer to other models, such as Gemini.\n- The finetuning attack has been evaluated solely on LLaMA-2-chat and ChatGPT, despite the existence of many new aligned open-source models that could be used to further assess the attack's effectiveness. The results from LLaMA-2-chat indicate limited effectiveness and the transferability limitations of the attack."
            },
            "questions": {
                "value": "- Conduct more experiments for baseline attacks on more aligned models to support the conclusion---\"Baseline attacks fail against aligned models\".\n- Conduct more comprehensive experiments with more and newer models for the finetuning attack.\n- Estimate the probability of extracting the training data part from the whole response assuming the training data is unknown.\n- Minor problems:\n  - line 071: the broken symbol before \"10,000 examples\"\n  - Figure 2 is never mentioned in the main text."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper pointed out the key reasons of the ineffectiveness of the model alignment and developed two novel techniques to circumvent chatbot alignment guardrails: a divergence attack and a finetuning attack.  The author demonstrated that this is the first large-scale, training-data extraction attacks on proprietary language models using only publicly-available tools and relatively little resources. This work highlights the limitations of existing safeguards to prevent training data leakage in\nproduction language models. And the experiment results show the model alignment is not enough to prevent memorization."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The contributions are valid and significant. This work highlighted the limitations of existing safeguards to prevent training data leakage in\nproduction language models. The author proposed two novel extraction attacks illustrating the limitation of model alignment of training-data extraction. The attacks only require access to tools that are publicly accessible to everyone. In addition, the author proposed a scalable approach to validate memorization. \n\n2. The paper does a comprehensive research showing additional work in long Appendix with sufficient experiments. \n\n3. The paper has good structure by clarifying key definitions and prompting the motivation. In experiments, the author clearly described the scalable approach for validating memorization and what are the production language models, including both aligned, conversational models and instruction-tuned, non-conversation models."
            },
            "weaknesses": {
                "value": "1. (not a weakness but a suggestion). During the reading, I found some figures and conclusions in the Appendix is helpful and may worthwhile to be added or replaced to the main body. For example, Figures in Appendix A.9\n\n2. In section 7 QUALITATIVE ANALYSIS OF EXTRACTED TEXT, it seems the result analysis focuses on the length of the extracted string and memorized text. It may better if the author could add more explanation in terms of the leakage of random training data from divergence attack vs the leakage of specification training data from fine-tuning attack."
            },
            "questions": {
                "value": "1. [line 047] It said they apply divergence attack to ChatGPT and Gemini but apply finetuning attack to ChatGPT only. Is there a particular reason why they doesn\u2019t apply finetuning attack to Gemini?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper considers two approaches to extract a large language model's training data. The first is using repeated words, and the second is by fine-tuning the model to break the safety training. The approach is tested on proprietary models, and shown to regenerate sentences from the open source datasets with verbatim tokens over a threshold. While the first approach does not always work, fine-tuning could easily circumvent the defense mechanism put in the model."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- S1: The approaches are simple and effective without too many assumptions.\n- S2: The attacks are shown to work on the state-of-the-art commercial models.\n- S3: The presentation is good overall and the paper is very easy to read."
            },
            "weaknesses": {
                "value": "- W1: The related work section is missing. Although there is the background section, the paper does not properly cover the related work and its relation to the existing work, as well as potential defenses in the literature. Especially, these attacks are known and discussed in different forums. There is a potential that the authors of the paper might be those who suggested and discussed these approaches early on, but some mention of the context is useful understanding the literature and the significance of this approach.\n- W2: The paper defers a lot of information to the appendix. Although this abundance of information comes from the thorough analysis and investigation, the paper needs to prioritize more essential information and drop potentially duplicate or obvious information."
            },
            "questions": {
                "value": "- Q1: What are the examples of the (near) duplicate generations and their significance?\n\nThe paper is overall well written and the extensive analysis is helpful, the paper can be improved with better use of prioritization in space, and a proper related work section. Especially, discussing how novel the proposed approach would be helpful understanding the impact of this paper. This might need a significant reorganization of the paper, but all the ingredients should be already there. If that can be done, I'm willing to upgrade my recommendation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper conducts a large amount of empirical research and finds that aligned chat models hardly leak training data. However, when the authors implemented the divergence attack and fine-tuning attack, the models leaked some training data, demonstrating significant security vulnerabilities in current large language models. The paper conducted a large number of experiments to validate the various negative effects on the model after being attacked."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper conducted a large number of experiments to reveal the extraction attacks faced by large language models, and the models used in the experiments are very representative.\n- The research problem addressed in the paper is very interesting; extraction attacks are an important topic for large language models.\n- The structure of the paper is very well-organized, with rich details such as explanations and definitions for memorization, making it easy to read."
            },
            "weaknesses": {
                "value": "- I think that the paper lacks innovation or technical contribution. Although the two attack methods proposed in the paper reveal security issues with large language models, I think such contributions may not be sufficient for a top conference like ICLR.\n- The divergence attack proposed in the paper is intriguing, but why does this attack work? Under what circumstances does it work? It seems that this attack may not enable targeted attacks (i.e., leaking specific information from the model). There appears to be a significant random component, which means that the efficiency of this type of attack may be low for the attacker.\n- It seems that the authors did not discuss the relationship with related works. Some adversarial attacks also seem to achieve similar effects. What are the main differences between the authors' work and related works?"
            },
            "questions": {
                "value": "Please refer to the ``Weaknesses`` part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}