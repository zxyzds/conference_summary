{
    "id": "7QDIFrtAsB",
    "title": "Anomaly Detection by Estimating Gradients of the Tabular Data Distribution",
    "abstract": "Detecting anomalies in tabular data from various domains has become increasingly important in deep learning research. Simultaneously, the development of generative models has advanced, offering powerful mechanisms for detecting anomalies by modeling normal data. In this paper, we propose a novel method for anomaly detection using a noise conditional score network (NSCN). NSCNs, which can learn the gradients of log probability density functions over many noise-perturbed data distributions, are known for their diverse sampling even in low-density regions of the training data. This effect can also be utilized, and thus, the NSCN can be used directly as an anomaly indicator with an anomaly score derived from a simplified loss function. This effect will be analyzed in detail. Our method is trained on normal behavior data, enabling it to differentiate between normal and anomalous behaviors in test scenarios. To evaluate our approach extensively, we created the world's largest benchmark for anomaly detection in tabular data with 49 baseline methods consisting of the ADBench benchmark and several more datasets from the literature. Overall, our approach shows state-of-the-art performance across the benchmark.",
    "keywords": [
        "Anomaly detection",
        "Tabular data",
        "Noise Conditional Score-based Networks"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "An one-class anomaly detection method by estimating the log-likelihood of the data distribution",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=7QDIFrtAsB",
    "pdf_link": "https://openreview.net/pdf?id=7QDIFrtAsB",
    "comments": [
        {
            "title": {
                "value": "Part 3"
            },
            "comment": {
                "value": "Xiaohui Yang and Xiang Li. Atdad: One-class adversarial learning for tabular data anomaly detection.\nComputers & Security, 134:103449, 2023.\n\nRoel Bouman, Zaharah Bukhsh, and Tom Heskes. Unsupervised anomaly detection algorithms on real-world data: how many do we need? Journal of Machine Learning Research, 25(105):1\u201334, 2024.\n\nTom Shenkar and Lior Wolf. Anomaly detection for tabular data with internal contrastive learning.\nIn International conference on learning representations, 2022.\n\nVictor Livernoche, Vineet Jain, Yashar Hezaveh, and Siamak Ravanbakhsh. On diffusion modeling for anomaly detection. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=lR3rk7ysXz.\n\nSongqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, and Yue Zhao. Adbench: Anomaly\ndetection benchmark. Advances in Neural Information Processing Systems, 35:32142\u201332159,\n2022."
            }
        },
        {
            "title": {
                "value": "Part 3"
            },
            "comment": {
                "value": "[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A \nsimple framework for contrastive learning of visual representations. In International conference on machine learning. PMLR, 1597\u20131607\n\n[5] Izhak Golan and Ran El-Yaniv. 2018. Deep anomaly detection using geometric \ntransformations. Advances in neural information processing systems 31 (2018)\n\n[6]  Mohammad Sabokrou, Mohammad Khalooei, and Ehsan Adeli. 2019. Self- \nsupervised representation learning via neighborhood-relational encoding. In\nProc. of the IEEE/CVF International Conference on Computer Vision. 8010\u20138019\n\nVictor Livernoche, Vineet Jain, Yashar Hezaveh, and Siamak Ravanbakhsh. On diffusion modeling\nfor anomaly detection. In The Twelfth International Conference on Learning Representations,\n2024. URL https://openreview.net/forum?id=lR3rk7ysXz.\n\nRoel Bouman, Zaharah Bukhsh, and Tom Heskes. Unsupervised anomaly detection algorithms on\nreal-world data: how many do we need? Journal of Machine Learning Research, 25(105):1\u201334,\n2024.\n\nHugo Thimonier, Fabrice Popineau, Arpad Rimmel, and Bich-Lien Doan. Beyond individual input \u02c6\nfor deep anomaly detection on tabular data. arXiv preprint arXiv:2305.15121, 2023."
            }
        },
        {
            "title": {
                "value": "Part 2"
            },
            "comment": {
                "value": "**Q2**  \nAs discussed in *W1* and in response to the first review by Reviewer JCE2, we agree that our method is relatively straightforward and, as detailed in the paper, is based on the concept of NCSBs. Beyond the loss functionn, which in this form can also be used directly as an anomaly score metric and has been further developed for this purpose, we were able to avoid further complexities typically necessary for effective generation, such as variance preserving/exploding and weighting, as outlined in Chapter 3, *Score Network for Tabular Anomaly Detection*. All modifications are theoretically and empirically justified in the paper, and the necessity of each is demonstrated. Additionally, we emphasize the originality of the work, viewing the lack of unnecessary complexity as a substantial benefit. \n\nAs noted in Chapter 8, this foundational work aims to broaden the possibilities and landscape of anomaly detection, without claiming to be optimal in all respects. This approach opens up many new research directions, as is often the case in anomaly detection. Early works such as Sakurada & Yairi (2014), for instance, employed simple autoencoders (without inventing the autoencoder principle itself), setting the stage for countless subsequent studies and receiving numerous citations. We believe that our work, being the first of its kind, can likewise contribute to the field by establishing a new research pathway, making it valuable to the scientific community.\n\nOur key contributions are:\n\n\u2022 We present a novel approach for one-class anomaly detection utilizing a score-based model\nwith a simplified loss function, which operates without needing external knowledge such as\npre-trained models or additional datasets.\n\n\u2022 We perform the first empirical study of NCSN anomaly detection on tabular data, where\nour adaptation approach shows a high performance and interpretability.\n\n\u2022 We demonstrate the inherent capability of the trained score model to effectively identify\nanomalies, and we provide a thorough analysis of how various parameters, including the\nnetwork architecture, impact its performance.\n\n\u2022 Through comprehensive experiments on well-established public benchmarks, including\nADBench and other widely used tabular datasets, we demonstrate that our approach consis\u0002tently achieves state-of-the-art results in tabular anomaly detection, outperforming existing\nmethods across multiple metrics.\n\nFurthermore, we would like to draw attention to the key contributions specified in Chapter 1, *Introduction*, and the summary in Chapter 8, where we believe our contribution and its value are clearly outlined. We also emphasize that all parts of the work that are not original are transparently acknowledged.\n\nWe hope we have adequately addressed all questions and concerns, and once again, we thank you for this valuable review. Your feedback enables us to further improve the paper, for which we are truly grateful. Should further questions arise, we are more than willing to answer them and look forward to continuing this constructive discussion.\n\n**Please note**: We will upload the updated version, incorporating the proposed improvements, in time before the deadline.\n\nBest regards\n\nVictor Livernoche, Vineet Jain, Yashar Hezaveh, and Siamak Ravanbakhsh. On diffusion modeling for anomaly detection. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=lR3rk7ysXz.\n\nRoel Bouman, Zaharah Bukhsh, and Tom Heskes. Unsupervised anomaly detection algorithms on real-world data: how many do we need? Journal of Machine Learning Research, 25(105):1\u201334, 2024.\n\nHugo Thimonier, Fabrice Popineau, Arpad Rimmel, and Bich-Lien Doan. Beyond individual input \u02c6 for deep anomaly detection on tabular data. arXiv preprint arXiv:2305.15121, 2023.\n\nMayu Sakurada and Takehisa Yairi. Anomaly detection using autoencoders with nonlinear dimen\u0002sionality reduction. In Proceedings of the MLSDA 2014 2nd workshop on machine learning for\nsensory data analysis, pp. 4\u201311, 2014."
            }
        },
        {
            "title": {
                "value": "First reply to Reviewer eR8K"
            },
            "comment": {
                "value": "Dear Reviewer eR8K,\n\nfirst we thank the you for the detailed and valuable feedback. We would like to address your questions and comments as follows:\n\n**W1**  \nWe agree that the broader area of diffusion-based models has now become a well-researched field. However, we would like to point out, as explained in Section 5 *Related Work*, that this primarily applies to DDPMs. The works cited in sources 1\u20133 also fall under this category of diffusion-based models. However, this does not apply specifically to NCSBs. As other reviewers\u2014particularly Reviewer AVJq\u2014have rightly noted, NCSBs remain a largely unexplored area and represent the first work of this kind. Cite: \"To the best of my knowledge, this paper showcases the first application of score networks to anomaly detection. The theoretical foundations are well substantiated.\" (Strengths, in review by Reviewer AVJq). Additionally, we would like to refer to our responses to *W2* in the first review by Reviewer JCE2: \u201cAdditionally, we note that our approach significantly outperforms DDPM as reported by Livernoche et al., 2024, both in terms of results and processing times. DTE, on the other hand, represents a markedly different approach, using a time estimation during inference to provide a score instead of a direct scoring method. We would also like to clarify the distinction between DDPMs and NCSB models. Esteemed researchers like Terro Karras and Yang Song, who have laid foundational work in this field, acknowledge the close relationship between these approaches. However, there are substantial implementation differences, also covered in the Related Work and the aforementioned sections. DDPMs, for instance, learn a stepwise denoising process based on a Markov chain. In contrast, as discussed in Chapter 2, NCSBs learn SDEs that correspond to the gradient of the log-likelihood and do not rely on a Markov chain for generation but rather require specific solvers, like the Euler-Maruyama method, to solve the SDE.\u201d\n\n**W2**  \nThis statement is correct. However, most benchmarks and work from previous years, like Livernoche et al. (2024), Bouman et al. (2024) and Thimonier et al., 2023, have this in common. Therefore, we explicitly point this out in Chapter 4 in the Main Results section, lines 378-384. We also refer here to the detailed results provided and the answer to the first review by Reviewer JCE2: \u201cAs explained in Chapter 8 from line 504 onward, this work aims to expand possibilities and is not intended to be optimal in every respect. Furthermore, in Chapter 3, *Main Results* starting from line 378, we emphasize that various methods have their merits from different perspectives and should be tested for new applications, allowing each researcher to make their own evaluations. We believe, and feel reinforced by the strong overall results and top placements in Appendix F4 for individual datasets, that NCSBAD should indeed be considered one of these methods worth testing.\u201d\nWe wanted to make this completely transparent through this statement and the complete results in the appendix.\n\n**Q1**  \nQ1 In the case of AUCROC, we were happy to do this for you (please note that double counting is possible in case of multiple best models and the counting was carried out on the 122 sub-data sets + 15 additional data sets according to the tables in Appendix F4, and only the top models were considered):\n\nModel: NCSBADVAL, Count: 36 (If NCSBAD is not conted in the results, in the fair comparison this would be the case)\n\nModel: NCSBADVAL, Count: 32 (If NCSBAD is also counted in the results)\n\nModel: NCSBAD, Count: 29 (If NCSBADVAL is not conted in the results, in the fair comparison this would be the case)\n\nModel: NCSBAD, Count: 11 (If NCSBADVAL is also counted in the results)\n\nModel: LUNAR, Count: 23\n\nModel: KPCA, Count: 13\n\nModel: GMM, Count: 11\n\nModel: Ensemble, Count: 7\n\nModel: ICL, Count: 7\n\nModel: SLAD, Count: 7\n\nModel: KDE, Count: 7\n\nModel: LOF, Count: 7\n\nWe will also add an appropriate marker in the updated version to help readers with counting."
            }
        },
        {
            "title": {
                "value": "First reply to Reviewer V2GR"
            },
            "comment": {
                "value": "Dear Reviewer V2GR,\n\nwe would like to thank you for the helpful feedback and the positive evaluation. We would like to respond to your comments and questions as follows:\n\n**W1**  \nThe title was chosen as a tribute to the foundational work by Song et al. (2019) titled *\"Generative Modeling by Estimating Gradients of the Data Distribution.\"* The principle and reasoning behind this are thoroughly explained in that work and briefly covered in our own work in Chapter 2, *Background,* from line 111 to 143.\n\n**W2**  \nThank you for this excellent suggestion; we will incorporate this change in the updated version.\n\n**W3**  \nThis statement refers to the fact that no additional knowledge of an underlying data distribution or fixed thresholds defined by experts is required. We will clarify this point in the updated version.\n\n**Q1**  \nThe detailed structure can be found in Figure 1, as well as in Chapter 3, under the section *Network Architecture,* lines 193\u2013197. The requested information is also available in Appendix E1, Table 3. As described in Chapter 3 *Network Architecture*, only the input and output layers are adjusted to accommodate the various data dimensions, while the rest of the network architecture remains fixed. The input and output dimensions are further modified automatically in the accompanying code in the supplementary materials, based on the dimensions of the input data. This approach ensures reproducibility (even with new datasets) without further modification. Additionally, all network structures analyzed in this study, including those that were less successful and briefly discussed in the appendix, are provided in the supplementary materials code, ensuring complete reproducibility and traceability.\n\nWe hope these responses address any outstanding questions and concerns, and we would like to once again thank you for the positive evaluation. Your feedback and suggestions have undoubtedly contributed to an improved version of this work, and we are very grateful for this. If further questions arise, we are more than willing to address them and look forward to continuing this productive discussion.\n\n**Please note**: We will upload the updated version, incorporating the proposed improvements, in time before the deadline.\n\nBest regards"
            }
        },
        {
            "title": {
                "value": "Part 2"
            },
            "comment": {
                "value": "**Q1**  \nWe are not entirely certain if we fully understand your question, but we will do our best to address it. We agree that some methods, such as KPCA and GMM, were not developed explicitly for the one-class classification scenarios; however, they can still be applied effectively in this context, as the results clearly demonstrate. As seen in the code, we did not use the `fit_predict` method. Instead, following the ADBench framework, we trained with the `fit` method on the training data and evaluated with the `decision_function` method on previously unseen test data. The assumption that this approach is invalid would implicitly suggest that the ADBench library\u2019s approach, as well as the methods in Livernoche et al. (2024) and others, would be infeasible or yield incorrect results, we can't confirm this here.\n\n**Q2**  \nThank you for pointing this out. We made here a misleading sentence. We meant that it learns (during training) to distinguish the data during testing. We are very grateful for your note, and we will clarify this in the updated version.\n\n**Q3**  \nInitially, 50,000 random samples are drawn from the entire dataset based on the specified seed. The division into subsets occurs afterward. However, we respectfully disagree with your assumption. This approach maintains the anomaly count in the dataset quite well (validated with the selected seeds). When running the preprocessing script, this process can be transparently verified in the output, where both the number and percentage of anomaly samples are displayed. This common procedure, following the ADBench library and Livernoche et al. (2024), functions as intended and can be confirmed through these metrics. Furthermore, this will be taken into special inspection in the metrics that you have suggested and which we will include in the camera-ready version based on your recommendation. The results will confirm the correctness of our statement and will be visible in the final version.\n\n**Q4 (Seed Clarification)**  \nThe specified seed is used for the undersampling and data splitting. We will clarify this in the updated version. The seeds within the individual methods were preserved from the original implementations.\n\n**Q5**  \nTo clarify this statement further, we need to address it in several parts. LUNAR, KPCA, and GMM are generally ignored in prominent recent works as Livernoche et al. (2024), Thimonier et al., 2023  and many others. In Bouman et al. (2024), we ask the reviewer to consider the following points:\n\n1.\tThe statement holds only for LUNAR and GMM; KPCA is not included.\n2.\tThe work conducts both local and global as well as mixed investigations across three studies.\n3.\tAll datasets (the local and the global ones) used in the work are also included in our benchmark, along with numerous additional ones.\n\nThank you for pointing this out. Detailed results to facilitate exact comparisons are also provided in Appendix F4, and references to these results are made in the main text.\n\nWe hope these responses offer some clarification and have adequately addressed all concerns. Again, we would like to thank the reviewer for the valuable feedback and insights, which we are confident will contribute to significantly improving the paper. Should further questions arise, we are happy to respond and look forward to continuing this constructive discussion.\n\n**Please note**: We will upload the updated version, incorporating the proposed improvements, in time before the deadline.\n\nBest regards\n\n[1] Ahmed et al. 2021. Graph regularized autoencoder and its application in unsupervised anomaly detection. IEEE transactions on pattern analysis and machine intelligence 44, 8 (2021), 4110\u20134124.\n\n[2] Flaborea et al. 2023. Multimodal motion conditioned diffusion model for skeleton-based video anomaly detection. In Proc. of the IEEE/CVF International Conference on Computer Vision. 10318\u201310329\n\n[3] Flaborea et al. 2023. Are we certain it\u2019s anomalous?. In Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2896\u20132906.\n\n[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In International conference on machine learning. PMLR, 1597\u20131607\n\n[5] Izhak Golan and Ran El-Yaniv. 2018. Deep anomaly detection using geometric transformations. Advances in neural information processing systems 31 (2018)\n\n[6] Mohammad Sabokrou, Mohammad Khalooei, and Ehsan Adeli. 2019. Self- supervised representation learning via neighborhood-relational encoding. In Proc. of the IEEE/CVF International Conference on Computer Vision. 8010\u20138019"
            }
        },
        {
            "title": {
                "value": "First reply to Reviewer AVJq"
            },
            "comment": {
                "value": "Dear Reviewer AVJq,\n\nfirst we would like to thank you for their excellent and thorough feedback.\n\n**W1**  \nWe agree with the reviewer\u2019s points and will implement an additional clarification in the updated version, specifically in line 328. Lines 55\u201357, as the reviewer noted, already accurately describe the relevant details and should therefore suffice. The most approaches in this area rely on Learning from Positive Unlabelled Examples (LPUE) [4-6] , where anomaly detectors are trained on positive data only and then validated/tested on both normal and abnormal data. Our approach is also based on this like the most anomaly detection strategies that rely on LPUE (e.g., [1-3]). Additionally, as suggested, we will make an adjustment to the summary to further clarify this point. \n\n**W2**  \nWe agree that the method itself has undergone significant optimization, which is indeed a core aspect of the work, as extensive parameter analysis is essential for each new method. We conducted this optimization across a variety of datasets (only ADBench part and only one seed) to avoid tailoring the method to specific datasets. However, the optimization was performed only on a subset of all datasets to demonstrate transferability to other, unused datasets. We believe that this approach ensures the method is optimized for the general task of anomaly detection rather than for a particular benchmark. It is also correct that no optimization was performed for the baseline methods. As stated in Chapter 4, *Baseline Methods* (\u201cFor all methods used, we apply the default hyperparameters provided by the authors of the original publications\u201d), we used either the hyperparameters (where possible, directly from the original code) or the code from the original works or the PyOD and ADBench libraries. Therefore, we assume that tuning occurred within those works, and that these methods are optimized for general applications. For this reason, methods requiring dataset-specific hyperparameter tuning (e.g., the ATDAD method, Yang & Li, 2023) were partially excluded, as implementing this would not have been feasible within this work. Furthermore, this approach allows a fair comparison with our method, which uses the same (albeit once-optimized) parameters and architectures for all datasets in the benchmark.\n\n**W3**  \nUnfortunately, we are not entirely sure what is meant by \u201cpaired data.\u201d For the preprocessing of MI-F/MI-V data, we used the code provided by Bouman et al. (2024) (as indicated in our paper and marked in the code), where clear sample-wise labels are available. If the question concerns the specific splitting process, please note that, as shown in the accompanying code, the rounding always favors the test data.\n\n**W4**  \nIn making this choice, we aimed to align with the works referenced as guidelines. However, we fully understand and acknowledge the reviewer\u2019s concerns. As suggested, we will swap the two plots in the updated version. Additionally, we will include the suggested adjusted P@n and adjusted AP in the appendix with the appropriate references.\n\n**W5**  \nPlease note that while this is indeed a vision example, the flattened matrix is converted to a vector format akin to tabular data, meaning that no extended spatial patterns remain in 2D and also the MLP2048 is used for this. As described in the chapter, this is merely a toy example selected for its illustrative potential. Providing an intuitive example in the context of tabular data is challenging, as Shenkar & Wolf, 2022, note in their *Discussion* chapter. However, we aimed to go beyond purely numerical statements and offer a tangible example. The requirements for this example also included using a grayscale image with a manageable dimension, ensuring comparability with tabular data. A publication specifically for the vision field is not planned. We kindly ask that the detailed construction of this example be considered. The code is also provided.\n\n**Minor Remarks**  \nWe would like to express our sincere thanks for these suggestions, and we will, of course, correct these in the updated version."
            }
        },
        {
            "title": {
                "value": "Part 2"
            },
            "comment": {
                "value": "**W3.2**  \nAs explained in Chapter 8 from line 504 onward, this work aims to expand possibilities and is not intended to be optimal in every respect. Furthermore, in Chapter 4, *Main Results* starting from line 378, we emphasize that various methods have their merits from different perspectives and should be tested for new applications, allowing each researcher to make their own evaluations. We believe, and feel reinforced by the strong overall results and top placements in Appendix F4 for individual datasets, that NCSBAD should indeed be considered one of these methods worth testing. We also refer you to our response to Q1 in the review from reviewer eR8K. \n\n**W3.3**  \nWe completely agree that different representations and aggregation methods can lead to varied boxplots. However, we find it difficult to fully understand the question regarding fairness toward other methods, as this approach has been applied consistently across all methods. To accommodate individual preferences such as these, we have included detailed results in Appendices F3 and F4, allowing each reader to access the exact results relevant to their interests, regardless of the method or dataset. In this way, we hope to provide added value for every reader. Based on other reviews, however, we plan to further enhance the presentation; please see our response to W2 in the review by Reviewer AVJq. Due to limited space, an entirely different presentation approach is unfortunately not feasible, and we refer here to established practices in recent recognized works such as Livernoche et al., 2024, Bouman et al., 2024, and Thimonier et al., 2023, which also adopt such presentations to avoid limiting the number of datasets in large benchmarks, as in this work.\n\n**W3.4**  \nWe would like to address your comment in two parts. \n\n**Part 1**\n\nThe validation set is not used for tuning the method but rather to determine the best training point and to select the corresponding checkpoint from this epoch. As explained in Chapter 3, *Benefit of Validation Data*, this approach makes sense for our method due to the nature of NCSBs, which do not converge in the traditional sense. Hyperparameter tuning in the conventional sense does not take place. As far as we can observe, most other models do converge; hence, we assume they would not benefit from validation data by selecting an earlier checkpoint during the training process for inference (although we must acknowledge that we cannot completely rule this out). We would also like to point out that our approach NCSBAD performs superior even without validation data and refer to the counted results in the answer to question Q1 of the review of Reviewer V2GR. This is also evident in the result diagrams in Figures 2 and 5 and in the complete results in Appendix F3 Table 5 with an AUCROC =  82.19, F1 = 54.77 and AUCPR = 54.99 for NCSBAD and AUCROC = 83.26, F1 =  56.17 and AUCPR = 56.48 for NCSBADVAL. As in the paper, in Chapter 4, *Main Results* line 372-377, stated this is still the best performance in the AUCROC and the second best in the F1-score and the AUCPR in the overall benchmark. We wanted to show another way to further improve the results and we succeeded in doing so.\n\n**Part 2**  \nIn our practical applications (not covered in this paper), we often encounter a large volume of normal behavior data and a limited number of known anomalies, as is often the case in real-world scenarios. These anomalies typically do not cover every type of anomaly and are also often too few in number to train a supervised approach directly. However, as shown in the results of this paper, such a small validation dataset can still be very useful in assessing the learning of normal behavior. For these reasons, we believe that this approach makes sense in certain scenarios.\n\nWe hope that we have adequately addressed all your questions and would like to thank you again for your review and helpful insights. We believe that your feedback has allowed us to improve the updated version, and we are grateful for that. Should further questions arise, we would be more than happy to address them and look forward to continuing this productive discussion.\n\n**Please note**: We will upload the updated version with the outlined improvements in time before the deadline.\n\nBest regards\n\n[1] Ahmed et al. 2021. Graph regularized autoencoder and its application in unsupervised anomaly detection. IEEE\ntransactions on pattern analysis and machine intelligence 44, 8 (2021), 4110\u20134124.\n\n[2] Flaborea et al. 2023. Multimodal motion conditioned diffusion model for skeleton-based video anomaly detection. In Proc. of the IEEE/CVF International Conference on Computer Vision. 10318\u201310329\n\n[3] Flaborea et al. 2023. Are we certain it\u2019s anomalous?. In Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2896\u20132906."
            }
        },
        {
            "title": {
                "value": "First reply to Reviewer JCE2"
            },
            "comment": {
                "value": "Dear Reviewer JCE2,\n\nwe would like to thank you for the detailed and constructive feedback. We will respond to your questions as follows:\n\n**W1**  \nWhile the statement is not incorrect in principle, we would like to draw attention to the innovations introduced in the loss function and the reasoned selection of parameters and individual components. Without these, the approach would not perform as effectively. Furthermore, we would like to reference feedback from other reviewers who have also confirmed that this study represents the first of its kind. While the idea itself may not be overly complex (a quality that often serves as a positive trait rather than a drawback for an idea), many ideas may seem simple in retrospect yet are valuable precisely it is an initial approach and as we stated in Chapter 8: \"It's a foundational contribution to establishing NCSNs as a competitive method for anomaly\ndetection. We have proposed a robust and generalizable framework, focusing on the fundamental\nprinciples, architectural design, and parameter settings necessary for effective anomaly detection\nusing NCSNs. Future research could explore advanced training strategies, such as sliced score\nmatching (Song et al., 2020a) and maximum likelihood weighting (Song et al., 2021), to optimize\ntraining efficiency and potentially enhance performance further.\n\n**W2**  \nWe would like to highlight that this is briefly addressed in Chapter 3, lines 265\u2013269, and in Chapter 8. A significant difference lies in the straightforward parallelizability that is not present in DDPMs, either in the baseline approach or in generation approaches. The brevity of this discussion is due to length constraints, which we aim to address here. We will attempt to incorporate the explanations presented here into the updated version within the allowed space. Additionally, we note that our approach significantly outperforms DDPM as reported by Livernoche et al., 2024, both in terms of results and processing times. DTE, on the other hand, represents a markedly different approach, using a time estimation during inference to provide a score instead of a direct scoring method. We would also like to clarify the distinction between DDPMs and NCSB models. Esteemed researchers like Terro Karras and Yang Song, who have laid foundational work in this field, acknowledge the close relationship between these approaches. However, there are substantial implementation differences, also covered in the Related Work and the aforementioned sections. DDPMs, for instance, learn a stepwise denoising process based on a Markov chain. In contrast, as discussed in Chapter 2, NCSBs learn SDEs that correspond to the gradient of the log-likelihood and do not rely on a Markov chain for generation but rather require specific solvers, like the Euler-Maruyama method, to solve the SDE. We will address these difference in detail between DDPM and NCSN  at camera-ready.\n\n**W3.1**  \nThis is correct. The goal of our work was to focus on this specific case and not the completely unsupervised scenario. In this, we followed established works such as Shenkar & Wolf, 2022, and Bergman & Hoshen, 2020, as described in Chapter 4, *Experimental Setup*. This approach also defines the single-class classification case and was intended to be the scope of this work. The suggested additional perspective would certainly be interesting; however, due to the limited timeframe and considering the paper's length, it is unfortunately not feasible. We apologize for any inconvenience and request your understanding in this matter. The most approaches in this area rely on Learning from Positive Unlabelled Examples (LPUE) [4-6] , where anomaly detectors are trained on positive data only and then validated/tested on both normal and abnormal data. Our approach is also based on this like the most anomaly detection strategies that rely on LPUE (e.g., [1-3])."
            }
        },
        {
            "summary": {
                "value": "This manuscript proposes to utilise a well-established diffusion model, Noise Conditional Score Network (NCSN), to perform unsupervised anomaly detection (including the semi-supervised one-class anomaly detection setting) in tabular data, leading to an anomaly detection method called NCSNAD. During the training phase, NCSNAD learns a vector field which represents the underlying distribution of (normal) data; while during the inference phase, NCSNAD assigns an anomaly score by estimating the likelihood of staying within the learned vector  filed for each test data instance. Overall, NCSNAD follows the generic principles of one-class anomaly detection, where the novelty of NCSNAD lies in utilising diffusion model to learn the data distribution of normal instances. After establishing NCSNAD, they conduct very extensive experiments (on 57+15 datasets ) to show the effectiveness of NCSNAD and compare it with SOTA baselines (with more than 50 anomaly detectors). The results show that NCSNAD outperforms most baselines in terms of detection accuracy (measured with three different metrics)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Overall, this manuscript is well organised and easy to follow;\n2. The authors conducted very extensive experiments, showing the effectiveness of their method and superiority compared to SOTA baselines;\n3. Although there already exist some work that employ diffusion models to perform anomaly detection in tabular data, this research topic is definitely worthy of more research attention;"
            },
            "weaknesses": {
                "value": "1. The novelty is limited: it seems that the authors simply employ the established model NCSN, with a simplified loss function to perform anomaly detection. It is a very straightforward idea. \n2. NCSNAD is not well motivated. For example, when comparing to the closest related work DTE (which is the only existing diffusion model based anomaly detection method in tabular data), the authors did not explain why they chose to use NCSN rather than DDPM; what are the corresponding pros and cons of each method, etc.?\n3. I appreciate that the authors have conducted very extensive experiments (in the sense there are many datasets and baselines), but I have several major concerns regarding the experiments:\n* 3.1. they only considered the semi-supervised one-class setting in this manuscript: namely they utilise 50% of normal data instances as training while the rest data instances as validation or test set. In other words, they did not consider the truly unsupervised setting, where the training set should contain both normal and abnormal data instances. As far as I know, one-class anomaly detection anomaly detection methods usually do not work well if the training data is contaminated (namely containing abnormal instances);\n* 3.2. the results show that simpler models like LUNAR, KPCA, and especially GMM (which have less training and inference time) achieve comparable detection accuracy (in terms of the box plots of ROC-AUC, F1-Score, or ROC-PR). A natural question raises: why people in anomaly detection community will use NCSNAD? (which is more complicated and computationally more expensive)\n* 3.3. the authors try to show that NCSNAD (or NCSNADVAL) is the best method by comparing the absolute performance metrics by providing the box-plots of ROC-AUC, F1-Score, or ROC-PR. My question is that: is this informative or fair to other methods? To mitigate this issue, I suggest the authors to include the results of relative rankings (namely the ranking of anomaly detectors on each dataset, and then aggregate the results in a similar manner), which I believe is more informative. \n* 3.4. I friendly point out that NCSNADVAL is unfair to other methods: if the authors utilise the validation set with labels to tune NCSNAD, this validation set should also be used to tune all other baselines. A more critical question is that: if I have a validation set with labels, why don\u2019t we directly use it to train the models (by turning unsupervised into semi-supervised with the help of these labels)? \n\nBTW, I am willing to raise my rating if my concerns are well addressed."
            },
            "questions": {
                "value": "Please see the weak points."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a new method for semi-supervised anomaly detection using a noise conditional score network. They demonstrate the efficacy of their new method on a large benchmark of tabular datasets. They showcase the interpretability of their method on computer vision anomaly detection and provide extensive resources for reproducibility of the paper."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "To the best of my knowledge, this paper showcases the first application of score networks to anomaly detection. The theoretical foundations are well substantiated. Similarly, I commend the authors for aiming to make the paper reproducible by providing a readme and all code used in the experiments. The authors have put considerable effort in constructing a large benchmark, including many methods and datasets from various different studies."
            },
            "weaknesses": {
                "value": "While I believe the contribution certainly has merit, there are some minor and major issues with the paper as is. In some cases, this section might overlap with the Questions section of this review. Note that the ordering below does not indicate order of importance of the issues.\n\n- The paper could be more clear with respect to its domain: i.e. **semi-supervised/one-class** anomaly detection. For example at line 55-57 it is stated that training is done unsupervised, in absence of labels. While I agree that the training is done without labels, it is done only on data that is labelled \"normal\", so some information of the labeling is provided to the models. To avoid confusion, at least the abstract should clearly state that the proposed method is semi-supervised in nature. The introduction then can elaborate that this means that the model is trained on only \"normal\" data, in contrast to semi-supervised classification, where access to all labels is more common. Similarly, at line 328 it is, in my opinion, incorrectly stated that this paper concerns the unsupervised setting.\n- The network architecture study, detailed in Appendix A, leads me to believe that the network and method have been thoroughly optimized on the benchmark. While this is not necessarily bad, it leads to a heavily unfair comparison. All other methods in the comparison have not been optimized to a similar degree, and will in many cases perform subpar. It is therefore not strange that the proposed method is the best performing one, as it simply has the highest degree of optimization.\n- Similar to the previous point: the authors show that allowing their method access to a validation set improves performance. Yet, no other methods are allowed the same benefit. This can lead to great discrepancies. GAN, and AE-based methods for example greatly improve with early stopping. Even beyond early stopping, the argument could be made that hyperparameter tuning should be done for many of these methods if a validation set is available. \n- In the experimental setup it is described how the various train/val/test sets are constructed. However, some datasets contain paired data which can't be split in the described manner without introducing cross-contamination. An example is the MI-F/MI-V data from the ex-AE study. \n- Generally, Fbeta scores are hard to compare across datasets, as they are not readily interpretable like AUC scores. Specifically: some problems are inherently harder than others, leading to the great variability observed in Figure 2. The authors could and should consider using the average precision (now shown in appendix) or the adjusted measures proposed by Campos (G. O. Campos, A. Zimek, J. Sander, R. J. Campello, B. Micenkov\u00b4a, E. Schubert, I. Assent,\nand M. E. Houle. On the evaluation of unsupervised outlier detection: measures, datasets,\nand an empirical study. Data Mining and Knowledge Discovery, 30(4):891\u2013927, 2016)\n- Some of the methods used in the comparison are not properly implemented for tabular data, or are insufficiently optimized. I've not thoroughly studied all code provided by the authors, but some examples include the VAE, which uses a sigmoid activation at the last layer, which is not suitable for standardized real-valued tabular data, and DeepSVDD, of which the PyOD implementation does not use many of the needed optimizations/steps the original paper by Ruff et al. introduces.\n- Section 5 concerns interpretability. In contrast to the rest of the paper this only shows how the score map can be used for the intepretation of anomalies in the computer vision domain, but not on tabular data, which is the main focus of the paper. This seems disconnected, and I would urge the authors to either show how to interpret tabular anomaly detection using their method, or include this experiment only in a separate paper showcasing the method on computer vision anomaly detection.\n\n\n\nMinor comments/typographical issues:\n- line 254: benifit -> benefit\n- Throughout the paper: spacing is too large near references: for example Appendix  C -> Appendix C and Algorithms  1 and   2 -> Algorithms 1 and 2.\n- The y-axis labels in Figure 2 are too small to read.\n- The x-axis and y-axis labels in figure 3 are not needed when displaying images"
            },
            "questions": {
                "value": "- Many of the classically unsupervised methods used in this comparison can't readily be used in the typical fit/predict paradigm that corresponds to distinct training, validation, and test sets. This confuses me as to how they are included exactly in the comparison, are the methods applied as is typical in the unsupervised setting: they get access to both train+test data and make a single prediction on the entire collection? If methods from for example PyOD are applied in the fit/predict paradigm on external test data they will yield incorrect results.\n- At lines 59 and 60 it is stated that the network **learns** to differentiate between normal and abnormal data during testing. From the rest of the paper it seems that no network updates are done during testing. This sentence may therefore be misleading, could the authors clarify?\n- in the **main results** subsection the authors first state that they subsample datasets to 50.000 data points. Is this done for the test set, the training set, or is this the total dataset which is then further split according to the procedure described earlier? Are all anomalies still included in this subsample? If so: that make anomalies much less rare than they would originally be. If not: anomalies are generally assumed to be heterogenous, so subsampling might introduce a severe bias.\n- In the **main results** subsection it is stated that five different random seeds are used. Is this the random seed for the methods, or for the dataset subsampling, or both?\n- In the **Main results** subsection it is stated that the notable performance of LUNAR, KPCA, and GMM methods goes overlooked in similar comparison. Yet, the results of Bouman et al. (2024) have observed similar performance of LUNAR and GMM on the collection of Local anomaly datasets. As a different collection of datasets is used in this paper in contrast to their comparison, does this not perhaps indicate that a larger proportion of the datasets used in this research is likely to contain \"local\" anomalies rather than the generally studied \"global\" anomalies?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes an improved unsupervised tabular anomaly detection method based on a diffusion model."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The experiments designed in this paper are extensive.\n\nThe method proposed in this paper is given adequate theoretical derivation and proof."
            },
            "weaknesses": {
                "value": "- The title\u2019s phrase \"Estimating Gradients\" does not seem to be sufficiently reflected throughout the paper; it would be helpful to provide a reasonable explanation.\n\n- Although the paper includes numerous baselines (a commendable aspect), a small suggestion would be to mark the proposed method in all comparative result charts, using an identifier like \"(ours)\".\n\n- The paper claims that the introduced method requires no additional prior knowledge; however, it still seems to be a reconstruction-based framework, which typically involves basic prior assumptions."
            },
            "questions": {
                "value": "What is the detailed structure of the MLP2048? Could it be clearly described through diagrams or text? For instance, the structure used in the experiments, including the number of layers and the parameters of each layer. If different datasets use different configurations, including this information in the appendix would help readers replicate this work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces noise conditional score networks (NCSN) to tabular anomaly detection and propose a new method called NCSBADVAL. However, it just made minor adjustments on NCSN and combines some popular techniques, such as time-step embedding to adapt the standard NCSN to this area. The authors have made extensive experiments to verify that if we aggregate the performances across all the 57 datasets, the average F1 and AUC-ROC of NCSBADVAL are better than the baselines. Also, the authors provided a good example to exhibit the interpretability of it."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. Extensive experiments and good visualization. The authors have made extensive experiments to prove that the proposed method can achieve an overall better performance across 57 datasets compared with tens of baselines. Besides, the authors have made a good visualization of such a mass experiment results and verify the effectiveness of NCSBADVAL.\n2. Good interpretability.  The authors also provide a good example in figure 3 to exhibit the strong interpretability of NCSBADVAL."
            },
            "weaknesses": {
                "value": "Though I really admire the huge experiment workload of this paper, I have some concerns about it.\n\n1. Limited novelty. Actually there are many works have introduced diffusion model into anomaly detection area, for example [1] [2] [3]. Though it may firstly introduce NCSN (a branch of diffusion model), it is not an original idea to introduce this kind of model into anomaly detection. Besides, this work only make little adjustment on NCSN when adapting it to anomaly detection area by combining some popular techniques such as time step embedding and finding a correspondence relationship between the anomaly score and score in diffusion model.\n2. Consistently good performance but not best performance. Though NCSBADVAL can make overall better average performance when aggregating the performances across all the datasets, I found in Table 6- Table 13 that NCSBADVAL actually can not achieve the best performance on majority of the datasets (I have not counted it accurately due to the huge amount). Thus, could I understand it as that NCSBADVAL can only obtain a relatively good results on most datasets, but the best performance is achieved by different methods on different datasets?\n\n[1] Wolleb J, Bieder F, Sandk\u00fchler R, et al. Diffusion models for medical anomaly detection[C]//International Conference on Medical image computing and computer-assisted intervention. Cham: Springer Nature Switzerland, 2022: 35-45.\n\n[2] Wyatt J, Leach A, Schmon S M, et al. Anoddpm: Anomaly detection with denoising diffusion probabilistic models using simplex noise[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 650-656.\n\n[3] Zhang X, Li N, Li J, et al. Unsupervised surface anomaly detection with diffusion probabilistic model[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 6782-6791."
            },
            "questions": {
                "value": "1. How many times that NCSBADVAL have achieved the best performance among 57 datasets?\n2. Could you emphasize the adaptions you have made compared to the standard NCSN?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}