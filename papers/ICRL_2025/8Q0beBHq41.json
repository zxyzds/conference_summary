{
    "id": "8Q0beBHq41",
    "title": "Can VLMs Play Action Role-Playing Games? Take Black Myth Wukong as a Study Case",
    "abstract": "Recently, large language model (LLM)-based agents have made significant advances across various fields. One of the most popular research areas involves applying these agents to video games. Traditionally, these methods have relied on game APIs to access in-game environmental and action data. However, this approach is limited by the availability of APIs and does not reflect how humans play games. With the advent of vision language models (VLMs), agents now have enhanced visual understanding capabilities, enabling them to interact with games using only visual inputs. Despite these advances, current approaches still face challenges in action-oriented tasks, particularly in action role-playing games (ARPGs), where reinforcement learning methods are prevalent but suffer from poor generalization and require extensive training. To address these limitations, we select an ARPG, ``Black Myth: Wukong'', as a research platform to explore the capability boundaries of existing VLMs in scenarios requiring visual-only input and complex action output. We define 13 tasks within the game, with 76.9% focusing on combat, and incorporate several state-of-the-art VLMs into this benchmark. Additionally, we will release a human operation dataset containing recorded gameplay videos and operation logs, including mouse and keyboard actions. Moreover, we propose a novel VARP (Vision Action Role-Playing) agent framework, consisting of an action planning system and a human-guided trajectory system. Our framework demonstrates the ability to perform basic tasks and succeed in 90% of easy and medium-level combat scenarios. This research aims to provide new insights and directions for applying multimodal agents in complex action game environments. The code and datasets will be made available at https://varp-agent.github.io/.",
    "keywords": [
        "VLMs",
        "Agent",
        "ARPGs",
        "Benchmark",
        "Dataset"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "We select an ARPG, ``Black Myth: Wukong'', as a research platform to explore the capability boundaries of existing VLMs in scenarios requiring visual-only input and complex action output. In addition, we propose an evaluation benchmark and a dataset.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=8Q0beBHq41",
    "pdf_link": "https://openreview.net/pdf?id=8Q0beBHq41",
    "comments": [
        {
            "summary": {
                "value": "This work builds an AI agent framework, aiming to deal with Action Role-Playing Games. It successfully demonstrates the huge potential of LMM in decision-making. In addition, it builds a benchmark that may be useful for the future research in this filed."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This work needs lots of engineering efforts and the proposed benchmark will be handy for future research.\n2. This work explores some of the potential of existing large models, offering the audience plenty of room for imagination."
            },
            "weaknesses": {
                "value": "1. The novelty of this work seems very low. The framework is similar to Cradle and many other AI agent works, as well as some tailored modules, mainly in section 3.2.3, for BMW. \n2. The game is not an open-ended world and the skill library can be enumerated. The level of difficulty is still somewhat limited."
            },
            "questions": {
                "value": "I greatly appreciate these AI agent works as they explore the boundaries of VLM capabilities. However, from a methodological point, their contributions to academia are quite limited. Therefore, I believe this type of work should be reorganized into a benchmark-focused effort, dedicated to advancing the field."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a new eval for Multimodal LLMs (those which accept images). The authors take the Action RPG \u201cBlack Myth: Wukong\u201d and produce a benchmark of 13 tasks from the game. The game produces screenshots. The environment asks agents to produce high level actions which are transformed by python functions into specific actions of the model.\n\nThe authors collect human performance on the task and produce a dataset of expert trajectories. This is used as a retrieval system during the agents playthrough \n\nThe authors also simultaneously propose a method for composing LLMs to approach these problems (VARP agent). The VARG agent, using Gemini / Claude / GPT-4-Turbo is able to solve 7/13 tasks completely."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well presented and the task is original \n\nThe writing is clear\n\nThe 5 final tasks are clearly difficult \n\nGood ablations to identify what works well"
            },
            "weaknesses": {
                "value": "Given the baseline (Cradle) is also able to perform well on the 5 easy tasks this reduces the usefulness of those tasks.\n\n\n\nMy biggest concern is that i\u2019m not sure how performance on this evaluation relates to real world capabilities. For example montezuma\u2019s revenge was directly related to hard exploration or hinabi related to identify cooperative policies.  I think some framing to explain what models failing on the final 5 tasks demonstrates. \n\nI think a strong evaluation protocol could be suggested - e.g. have held out tasks or tasks that composed of easier tasks. This would help interpret what models fail at. In particular currently this seems to be a task that measures in-distribution performance, with accessing to expert trajectories. \n\nThe dataset collected is largely skewed towards similar task - given the first 5 are not as useful - how big is the actual dataset?\n\n\nThe Decomposable Task-Specific Auxiliary (DTSA) is very tailored to the game (as mentioned in the limitations).\n\nI think most of the performance is coming from the human-guided trajectory system. Why was removing this subcomponent not part of the ablation?"
            },
            "questions": {
                "value": "You mention both in abstract and introduction that 75% of the tasks are combat based - could you explain why this is important?\n\t\t\t\nSome typos - you say you define 13 tasks in the abstract but 12 in the introduction.\n\nFigure 7 does not have task 1"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a VLM-based agent that can play a AAA action role-playing (ARPG) game \u201cBlack Myth: Wukong\u201d (BMW). Notably, The system features a human-guided trajectory system that generates an action library based on human gameplay data. The contribution includes defining 13 game-playing tasks in BMW, releasing a BMW gameplay dataset, and introducing a new framework for playing ARPG games based on VLM. The experiment shows the introduced VLM-based agent outperforms Cradle, a general-purpose computer control agent, and is competitive against human players."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper not only introduces an agent framework but also introduces a benchmark and a dataset. Notably, the dataset is collected by recruiting 200 human players to play the game. The proposed agent generates an action library from human gameplay data, does not rely on text-based guiding information. The agent directly takes screenshots as input and output mouse and keyboard commands, and achieves good performance in the 13 introduced tasks."
            },
            "weaknesses": {
                "value": "The significance of this work is not so clear. According to the presented results, Cradle, a general computer control agent based on VLMs, can also play BMW. Though the results show the proposed VARP outperforms Cradle, to my understanding, the Cradle framework does not use human gameplay data and is designed for general purposes. It is not surprising that the proposed agent can outperform Cradle. \nMeanwhile, there are some concerns about the experiment part: \n1. The proposed agent is not compared to the RL-based agent for BMW, i.e., \u201cOther project\u201d in Tab. 1 (I guess it\u2019s AI-Wukong), which is also an agent playing BMW\n2. How many trials do the authors repeat for each task? It seems that all success rates are divisible by 10%, so perhaps all tasks are tested for 10 trials. If possible, I would recommend the authors test agents for more trials, or explain why 10 trials are enough/why not test more trials.\n\nMinor issues:\n\nLots of opening brackets are not separated from the texts with a space. For example, \u201caction role-playing games(ARPG),\u201d; \u201cVARP(Vision Action Role-Playing)\u201d."
            },
            "questions": {
                "value": "1.\tHow does AI-Wukong\u2019s performance compare to VARP? Or is there some reason for not comparing AI-Wukong with VARP?\n2.\tWhat is the practical impact of VARP, for example, how can VARP potentially benefit the game industry?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors are exploring the use of visual language models (VLMs) to play action-role playing video games (ARPGs), specifically using one AAA title as a case study. They propose:\n1) a VLM-based agent framework called VARP (Vision Action Role-Playing) which takes game screenshots as inputs and one of the 13 pre-defined tasks and outputs game actions (defined as combinations of atomic game actions)\n2) a human-gameplay dataset with 1000 records, collected in-house with 200 mostly novice human players\n3) an evaluation task set with 13 tasks specific to a AAA ARPG game Black Myth: Wukong (BMW), of different levels of difficulty: 9 easy, 1 medium, 1 hard and 3 very hard."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "It is great to see the complexities of a AAA title being described and addressed, helping the research community make progress beyond simpler 2D environments. The authors do a good job of explaining their data collection process, mentioning the number of participants, as well as the compute resources utilised for the experiments. They also present results on ablating some of the key optimizations they added to the VARP framework to aid action selection. \n\nThe work is original in the sense that it addresses the use of VLMs for ARPGs, providing a framework which is able to update the action library, as opposed to using a static one, as highlighted in the comparison study against the closest baseline, Cradle. The authors are also open to share publicly the code and the datasets to contribute to the community and encourage reproducibility and extension upon their findings."
            },
            "weaknesses": {
                "value": "In evaluating VARP, authors highlight 2 limitations of using VLMs for action video games, such as their slow reasoning speed for the more difficult tasks, as well as the challenges they face when tasked with long horizon navigation queries. Given the premise in the title, I would have expected a more detailed critical analysis of the strengths and limitations of using VLMs used as agents in ARPGs.\n\nAnother opportunity to expand and strengthen the submission would be to present a clearer discussion on the value of the additional modules that distinguish it from the Cradle framework. Sections 4.5 and 4.6 are a great step in this direction, but I feel they could be further strengthened by including the very hard tasks. First by mentioning this in the context of related work, and secondly by considering the very hard tasks in the comparison with Cradle and the ablation study. \n\nEven though the title does mention a case study on a specific game title, it would add a lot more strength to the submission to include a second game environment. One option would be for the authors to consider rephrasing the title. Having the initial question formulated as \"Can VLMs play ARPGs?\" and show insights from only one title, on a series of limited tasks, makes it harder to claim that the question is being comprehensively addressed.\n\nIt would be good to see a more detailed discussion on how the choice of VLM driving VARP makes a difference on the overall success rates. Along similar lines, in order to best support the community, it would be good to see stronger reasoning supporting the authors\u2019 initial choice of VLM models - why were GPT-4o, Claude and Gemini selected in experiments, and not others? What makes them suitable for this type of tasks?"
            },
            "questions": {
                "value": "Clarifying Questions:\n\n-\tIn Section 3.2.1, could you elaborate more on the size of the predefined action set and link to some examples (such as the ones in Figures 5 and 6)? Also, it would be great to know more about the process of selecting those actions.\n-\tIn Section 3.2.3, would the 5 submodules generalize to a wide range of ARPGs?\n-\tIn Section 3.3, it would be good to clarify that the human-guided trajectory system is only being used for the very hard tasks.\n-\tIn Section 4.4, could you specify what were the GPT-4o constraints in terms of maximum token count?\n-\tOn the Defeat WolfSwornsword task, Gemini performed better than the other 2 VLMs, GPT-4o and Claude. Any intuition on why that was the case?\n-\tIn Table 4 in the appendix, did you collect inference performance numbers on the very hard tasks as well that made use of the human-guided trajectory system? It would be interesting to understand the cost of running that additional component. Also, could you elaborate more on why is task 1 (easy) much slower compared to the others?\n-\tWas there an option considered to try out VARP on the Cradle dataset and tasks? It would be good to see if it exceeds the performance of Cradle.\n-\tIn the abstract and conclusion, you mention a 90% success rate in basic and moderate combat scenarios, is this number based on success rates in Table 3? If so, the average for VARP should be 88%.\n\nMinor comments/Suggestions:\n\n-\tIn the introduction of Section 3.2, it would be good to point to which respective section will update the situation library (Section 3.2.1), which one will define the updatable action library (Section 3.2.2) and which one will introduce the self-optimizable action generation module (Section 3.2.3).\n-\tIn section 3.2.1, for readability, you can mention that you are about to start introducing the 5 basic modules from Cradle.\n-\tIn Section 3.3, it would be good to maybe add in the appendix examples of game screenshots and human operation pairs collected in the human dataset.\n-\tIn Figure 2, it would be best to also add the short description of each task as the description for each subfigure/example.\n-\tIn Figure 3, for readability, it would be good to add the numerical task index along with the description (as mapped and defined in Table 2), to make it easier for the reader to keep track of the task numbers referred to in the main body of the paper.\n-\tIn Section 4.4, I would add a link to the additional inference evaluation added in the appendix A.5.\n-\tIt would be good to add a more descriptive caption to Table 3, specifying that the metric depicted in the comparison is the success rate.\n-\tIn Section  4, for consistency, consider standardizing the use of the term GPT-4o to reflect the use of this specific VLM. In some places, it is referred to as GPT-4o and in some, it is referred to as GPT-4o-2024-05-13.\n-\tFor readability, in Figure 5, it would be best to split it into 2 subfigures, (a) for the pathfinding action generated by the human-guided trajectory system and (b) for the action generated by the SOAG system.\n-\tSimilarly, Figure 6 could be more descriptive, highlighting which types of actions are predefined and which are generated.\n\nMinor Corrections:\n\n-\tin the introduction (lines 90 and 104), there are 12 tasks mentioned, with 75% of them focused on combat. In the abstract and the Experiments section, there are 13 tasks described, with a classification of 76.9% combat.\n-\tNoted 3 typos: one on line 100, word \u201ceasy\u201d is misspelled, one on line 176, the word \u201cwe\u201d should be written starting with lowercase, one on line 259 in \u201cin Sec 4.1\u201d instead of 3.1.\n-\tIn Section 4.7, line 454, the choice of GPT-4o model is repeated. It was stated in the same section on line 450."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}