{
    "id": "dIK7GpOwNY",
    "title": "Complexity Matters: Effective Dimensionality as a Measure for Adversarial Robustness",
    "abstract": "Quantifying robustness in a single measure for the purposes of model selection, development of adversarial training methods, and anticipating trends has so far been elusive. The simplest metric to consider is the number of trainable parameters in a model but this has previously been shown to be insufficient at explaining robustness properties. A variety of other metrics, such as ones based on boundary thickness and gradient flatness have been proposed but have been shown to be inadequate proxies for robustness.\n\nIn this work, we investigate the relationship between a model's $\\textit{effective dimensionality}$, which can be thought of as model complexity, and its robustness properties. We run experiments on commercial-scale models that are often used in real-world environments such as YOLO and ResNet. We reveal a near-linear inverse relationship between effective dimensionality and adversarial robustness, that is models with a lower dimensionality exhibit better robustness. We investigate the effect of a variety of adversarial training methods on effective dimensionality and find the same inverse linear relationship present, suggesting that effective dimensionality can serve as a useful criterion for model selection and robustness evaluation, providing a more nuanced and effective metric than parameter count or previously-tested measures.",
    "keywords": [
        "adversarial machine learning",
        "complexity",
        "robustness",
        "scale"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We investigate the relationship between model effective dimensionality and its robustness properties, showing a clear inversly-correlated linear trend. We also investigate adversarial training methods in this context.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=dIK7GpOwNY",
    "pdf_link": "https://openreview.net/pdf?id=dIK7GpOwNY",
    "comments": [
        {
            "summary": {
                "value": "The authors explore the relationship between \"effective dimensionality\" and robustness for a collection of computer vision classification models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is relatively clearly written.\n\nThe question being asked and answered is compelling."
            },
            "weaknesses": {
                "value": "* the paper is quite short and sweet: some experiments are conducted, on small models, on a single seed.\n* there is no \"novel\" contribution from the paper. In itself, this is not a problem at all. But without a novel contribution, I would expect the analysis to be more in-depth (not being able to run larger models is understandable; perhaps running more than one seed and more models is feasible? Or some of the other directions that the authors mention could be explored: looking at other architectures or boundary geometry or loss flatness. Or more exploration of what happens during adversarial training.\n* the paper is too superlative in its writing, almost to make up for interesting but not the strongest-looking results (which is also ok, just means more investigation is needed).\n* overall it seems like valuable work to be presented at a computer vision or robustness workshop, but the contribution is not enough to meet the ICLR main conference bar in my current opinion\n* handful of typos"
            },
            "questions": {
                "value": "## Questions\n* 149: Calculating the hessian is generally super expensive. Is there some trick here to make it scale? Or does this only work on small models?\n* 215: \"clear polynomial trend\" I think this is significantly too generous -- there appears to be a lot of bumpiness in the plots. Do you know what's going on there? Can you get more seeds?\n* 247: it seems much too strong to say that \"models across all datasets\" show a clear inverse trend when several of the lines go up and not down in the right column of figure 2 plots. Am I misunderstanding something?\n* can you get more seeds? it's hard to tell what's going on with only one seed.\n* I don't really understand what's going on with the adversarial training setup and plots (figure 3). Maybe you can give more details on the specific of adversarial training? I think there could be interesting details to look through here.\n\n## Specific things:\n* line 34: \"has been a myriad of\" -> \"have been myriad\"\n* line 35: isn't postprocessing an important thing too?\n* 39: delete \"recently\"\n* 48: it's -> its\n* 53: what does \"most of which are often used in production\" mean?\n* 66: please make it obvious you're in the computer vision setting\n* 68: delete the argmax\n* 72: use \\ell instead of l\n* 74-99: why these three methods? there are many many adversarial example finding techniques. why did you choose these specifically? why no FGSM as a baseline?\n* 103: please define what is in the equation; people won't have necessarily read the previous papers\n* 106: I think it's generous to say adversarial training is SOTA. it's a go-to technique and important tool, but it alone will not get you SOTA robustness\n* 129: it's -> its\n* 173: exact -> specific\n* 212: demonstrated -> show\n* 215: the \":-\" is not something I've seen before and I don't know what it means\n* 233: graphs -> plots\n* 258-263: writing it out like this is hard to follow. make a table and highlight one important result from it if you want?\n* 265: \"we removed outliers\" maybe you can find a different way to include the outliers (maybe make a scatter but don't include them in the fit and make it obvious by using a different color?). Removing data (which I'm grateful you mentioned in the text) raises suspicions of cherry-picking.\n* 269: delete \"above\"\n* 323: slightly mixed -> mixed\n* 380: extensive investigation -> investigation\n* 393: same\n* 395: you mention production-scale models here, but previously you just said you had to do only small models because of computational constraints. I would expect either that these models are production-scale and that's ok, or that you don't have the compute for production-scale and that's ok, but not both?\n\n## General note\nWith 3 seeds per model (possibly even only for one family of models and one adversarial attack method) this could be a great workshop paper. As is, I can't recommend it for main conference. I could imagine increasing my rating to a 5 if more seeds are run, all the data are shown, clearer justification is given to attack methods, and the results look less noisy OR are talked about in a less superlative way. Going deeper into the adversarial training direction could be quite interesting and would start to look more like a main conference paper to me, though I imagine that might be infeasible to do in the rebuttal time-window."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors evaluate adversarial robustness and effective dimension on an array of models of different architectures, trained with different datasets, and trained with various adversarial training algorithms. The authors plot the relationship between robustness and effective dimension and make four claims: (1) Effective dimension sometimes decreases with model size; (2) adversarial robustness is correlated with model size within the same model class, but not outside; (3) adversarial robustness and effective dimension are negative correlated; (4) adversarial training reduces effective dimension."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "* The paper is generally well-written and clear. \n* The paper addresses and important topic: identifying factors that either are causally related to, or correlate very strongly with adversarial robustness is an important area of research, for cheap model selection or for understanding how to train adversarially robust models.\n* The ideas proposed by the paper are novel, to my knowledge."
            },
            "weaknesses": {
                "value": "The primary weakness of this paper is major: the evidence presented in this paper does not sufficiently support the claims made by the authors. The primary premise of the paper is built by the following claims made by the authors: (1) adversarial robustness and effective dimension are negatively correlated; (2) effective dimension decreases with adversarial training.\n1. The negative correlation between adversarial robustness and effective dimension is weak at best, and often yields the opposite trend. The evidence for this claim is given in Figure 2. In this figure, ResNet appears to clearly have the claimed trend in 1 out of 9 plots (bottom right only). ShuffleNetV2 has the opposite trend in 2 out of 9 plots, and appears to have no correlation in another 2 out of 9, leaving only 5 out of 9 plots with the claimed trend. VGG seems to have the opposite trend or appears noisy in all plots in which it appears.  RepVGG has the opposite trend in the bottom right trend. MobileNetV2 seems to have the opposite trend, or noise in 4 out of the 6 plots in which it appears (all but the bottom left two plots). In total, the only model that actually observes the claimed trend is YOLO, which only appears in three of the plots. \n2. The fact that adversarial training decreases effective dimensionality has slightly stronger, but still not sufficient evidence: the adversarial training in Figure 3 left has greater effective dimensionality than the standard training. Further, the relationship between effective dimensionality and robustness does not appear correlated when considered training without AWP in two of the three plots in Figure 4 (center and right, considering the circle points). It seems, therefore, that perhaps AWP leads to both substantially smaller effective dimension and also better robustness, but the claim made by the authors, that robustness in general is negative correlated with effective dimensionality, appears false.\n\nRelated work:\n* AWP is closely related to Sharpness-Aware Minimization, and so it would be beneficial to include some discussion of the relationship between Sharpness-Aware Minimization and effective dimension. For example, see [1].\n\nMinor:\n* The clarity of your plots could be improved by making the legends not overlap with the plotted lines, and by keeping colors consistent across nearby plots (e.g., the models are given different colors in different plots in Figure 2)\n\n[1] Andriushchenko, Maksym, et al. \"Sharpness-aware minimization leads to low-rank features.\" Advances in Neural Information Processing Systems 36 (2023): 47032-47051."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors have investigated the relationship between the effective dimensionality and the model robustness, where the effective dimensionality shows a negative linear correlation with the model robustness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "**Strength 1:** This paper provides the empirical findings to understand the robustness capability of various deep model architectures.\n- Evaluating and understanding the robustness of deep models, or the generalization capability with a wide viewpoint, are long-lasting issues crucial in the theoretical understanding of the parametric model's capability and in realizing robust AI models in applications. From this perspective, the main strength of this paper is that it provides a novel empirical finding to understand model robustness."
            },
            "weaknesses": {
                "value": "**Weakness 1:** Limited theoretical understanding of the effective dimensionality\n- This paper only investigates the empirical results to show the relationships between the effective dimensionality and the model robustness, but its theoretical understanding is not yet explored.\n- Specifically, to further provide in-depth understanding, it would be better to explain why the effective dimensionality correlates more clearly with the model robustness than other measurements."
            },
            "questions": {
                "value": "**Question 1:** Any conjecture to explain the clear correlations between the effective dimensionality and the model robustness\n- Would you explain why clear relationships are observed, which is not shown in the prior metrics?\n\n**Question 2:** Relationships between the effective dimensionality and the model flatness\n- In Eq. 3, the effective dimensionality seems to be strongly related to the eigenvalues of Hessian. When revisiting the traditional measurements of model flatness on the parameter space, i.e., the maximum or summation of the eigenvalues of Hessian, the effective dimensionality increases when the model flatness decreases. Thus, it is hard to figure out why the effective dimensionality is particularly showing the more clear relationships to the model robustness. Would you share your thoughts on the relationships between the effective dimensionality and the model flatness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper links adversarial robustness to the models' effective dimensionality, using empirical experiment to investigate the relationship between these two aspect of the model.  The paper perform various experiments on different datasets (e.g. CIFAR10, CIFAR100, ImageNet) across different model architectures (e.g. ResNet, ShuffleNet, Yolo) and tries the prove the negative relationship between effective dimensionality and adversarial robustness. However, empirical results cannot validate such claim."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper investigates the relationship between two metrics, models' effective dimensionality and adversarial robustness. Various empirical results are presented, which are helpful for readers to get the relationship of two these results.\n\n2. The paper is generally easy to read."
            },
            "weaknesses": {
                "value": "1. The definition of effective dimensionality is highly correlated to the adversarial accuracy itself. Actually, it seems to be a tautology of adversarial robustness. effective dimensionality measures the flatness of loss landscape on the test data using Hessian Matrix while adversarial robustness reflects the flatness of loss landscape using the accuracy. \n\n2. The empirical result cannot prove the claim of the paper. As shown in Figure 2, the relationship between effective dimensionality and adversarial robustness is chaotic without clear trend."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}