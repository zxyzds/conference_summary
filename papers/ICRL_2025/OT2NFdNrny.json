{
    "id": "OT2NFdNrny",
    "title": "Improving Denoising Diffusion with Efficient Conditional Entropy Reduction",
    "abstract": "Diffusion models (DMs) have achieved significant success in generative modeling, but their iterative denoising process is  computationally expensive. Training-free samplers,  such as DPM-Solver, accelerate this process through gradient estimation-based numerical iterations.  However, the mechanisms behind this acceleration remain insufficiently understood. In this paper, we demonstrate  gradient estimation-based iterations enhance the denoising process by  effectively \\emph{\\textbf{r}educing the conditional \\textbf{e}ntropy} of reverse transition distribution.  Building on this analysis,  we introduce  streamlined denoising iterations for DMs  that optimize   conditional entropy in score-integral estimation to improve the denoising iterations.  Experiments on benchmark pre-trained models in both pixel and latent spaces validate our theoretical insights,  demonstrating that numerical iterations based on conditional entropy reduction improve the reverse denoising diffusion process of DMs.",
    "keywords": [
        "Denoising Diffusion",
        "Conditional Entropy"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-20",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=OT2NFdNrny",
    "pdf_link": "https://openreview.net/pdf?id=OT2NFdNrny",
    "comments": [
        {
            "summary": {
                "value": "This paper presents an interesting approach for reducing entropy during reverse sampling.  It theoretically derived the coefficients for sampling that reduce the conditional entropy the most. Results showing improvement in FID over samplers like DPM++."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Well written\n2. Clear to understand\n3. Mathematical sound\n4. Interesting perspective on improving sampling."
            },
            "weaknesses": {
                "value": "1. The motivation of reducing entropy may need elaboration. Why this necessarily improves sampling quality?\n2. The result shows significant degradation on FID when NFE reduces from 20 to 5, which casts doubt on how this method can actually helps with few-step sampling. This paper needs to compare with other distillation methods such as CM [1] or LCM [2].\n3. More results with different CFG should be desirable. It is known that higher CFG may cause instability in sampling. How is the FID performing at different CFG scales\n\n\n\n[1] Song, Y., Dhariwal, P., Chen, M., & Sutskever, I. (2023, July). Consistency Models. In International Conference on Machine Learning (pp. 32211-32252). PMLR.\n\n[2] Luo, Simian, et al. \"Latent consistency models: Synthesizing high-resolution images with few-step inference.\" arXiv preprint arXiv:2310.04378 (2023)."
            },
            "questions": {
                "value": "1. The motivation of reducing entropy may need elaboration. Why this necessarily improves sampling quality?\n2. The result shows significant degradation on FID when NFE reduces from 20 to 5, which casts doubt on how this method can actually helps with few-step sampling. This paper needs to compare with other distillation methods such as CM [1] or LCM [2].\n3. More results with different CFG should be desirable. It is known that higher CFG may cause instability in sampling. How is the FID performing at different CFG scales\n4. Can this method be applied to image editing tasks with inversion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposed two methods for solving the ODE formulation of the reversed diffusion flow.\nThe methods are motivated by claims on reduction of conditional entropy."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The proposed iteration scheme seems to be empirically useful when comparing with existing ODE-based methods."
            },
            "weaknesses": {
                "value": "- The presentation is not good enough.  \nClearly, it also lacks discussion in the experiments section and conclusion.\n\n- Many claims do not seem to be well justified and some formal statements are not written in a clear and explicit way.\n\n- Most of the results are based on assumption in the reverse flow that is not justified.\nTherefore, the proposed methods are not really mathematically backed."
            },
            "questions": {
                "value": "- I tend to believe that in the *reversed flow* you cannot assume that x_{t} conditioned on x_{t+1} is Gaussian (with expectation x_{t+1}).  \nNote that conditioned on both x_{t+1} and x_{0}, we have that x_{t} is Gaussian (with suitable expectation and variance), but not when dropping the conditioning on x_{0}.  \nThis makes Eq 3.9 (using the formula of entropy of Gaussian RV) unjustified. \n\n- Furthermore, in practice, the reversed flow is conducted by discretization and integration, which means that analyzing it by properties of the continuous ODE are not rigorous.\n\n- The following sentence seems like hand-waiving:  \n\"Since the injected noise at different time steps in a DM is mutually independent, the estimated noise by the model at different time steps can also be regarded as mutually independent.\"  \nIn the sequential denoising operations of the solvers I tend to believe that you will see correlation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper finds that gradient estimation\u0002based iterations enhance the denoising process by effectively reducing the conditional entropy of reverse transition distribution. Therefore, this paper introduces streamlined denoising iterations for DMs that optimize conditional entropy in score-integral estimation to improve the denoising iterations. The effectiveness of this method is verified on both pixel and latent spaces diffusion models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The concept that the condition entropy and the inference acceleration are correlated is interesting.\n2. The theoretical derivation is sufficient.\n3. Superior performance over the previous samplers on the pixal and latent space diffusin models."
            },
            "weaknesses": {
                "value": "1. The writing of this paper is poor. Certain key concepts are not defined or defined at latter page, for example, the concept of conditional entropy. Too much theoretical derivation and proof hinders the better expression of this paper as well as the understanding of readers. It is suggested to leave proof in the appendix, and highlights the crucial theroical findings and conclusions.\n2. The intorduction of conditional entropy is abrupt. Uncertainty reduction seems more like the result of inference convergence, instead of the inner reason. This also applies to the concept of conditional entropy. Compared to x_{t+1}, x_t has lower noise level. It is thus also natural that x_t has less uncertainty than x_{t+1}.\n3. Among all the proposed assumptions, how to detailly inplement the proposed method seems uncler and less strengthed.\n4. The visual results are embarrassedly absent. Besides the quantitative results, the visual results are also highly required. \n5. For the only proposed visual results, it is found that the proposed method may destroy the determinestic nature of ODE sampler. For example, compared to DDIM, the proposed method has different structures."
            },
            "questions": {
                "value": "Refer to the weakness part. It is highly suggest to improve the fluency and cores of this paper. More visual results are also encouraged."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper examines the efficiency of denoising diffusion models (DMs) by introducing an entropy-focused approach to improve denoising iterations. It proposes conditional entropy reduction as a means to streamline denoising iterations, leveraging gradient estimation techniques without additional model training. This method aims to enhance the denoising efficiency, theoretically and empirically validating it on pre-trained models, thereby promising a more optimized and effective denoising process. The paper\u2019s contributions include an analysis of entropy's role in denoising, proposing improved iterations for entropy reduction and empirically confirming the benefits of this method on image and text-to-image models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper proposes an innovative approach to enhancing denoising diffusion models (DMs) by introducing **conditional entropy reduction** as a framework for improving denoising efficiency. This concept shifts the focus from purely model-centric improvements to a novel mathematical approach, treating entropy as a direct metric for accelerating diffusion processes. By employing **RE-based (entropy reduction-based) iterations**, the authors effectively reduce computational demands and improve output quality. This approach creatively combines elements from information theory and generative modeling, marking a significant departure from traditional DM acceleration techniques. Such application of entropy, directly aimed at process optimization within diffusion models, demonstrates originality and has not been explicitly explored in prior works.\n\nThe paper's quality is evident in its rigorous theoretical framework, well-documented experimental design, and thorough comparative analysis. The authors carefully derive the principles of entropy reduction in diffusion models, offering clear justifications for each proposed method, including the **gradient estimation-based iteration** to achieve conditional entropy reduction. Theoretical insights are validated across diverse benchmarks, such as **CIFAR-10** and **ImageNet**, showcasing improvements in **Fr\u00e9chet Inception Distance (FID)** scores. The comparisons with established methods, including **DDIM** and **DPM-Solver**, reflect a robust and comprehensive experimental framework. By testing across both **single-step and multi-step scenarios**, the paper demonstrates the practical reliability and consistency of its proposed methods, underscoring a strong commitment to research quality.\n\nThe clarity of the paper is generally commendable, particularly in its explanations of complex mathematical derivations and the entropy reduction approach applied to the denoising process. Key concepts, including **conditional entropy, score-integral estimation, and gradient-based iterative improvements**, are introduced systematically, making them accessible to readers familiar with diffusion models. Visual aids, such as tables and diagrams comparing FID scores across conditions, effectively illustrate performance improvements, enhancing readability. However, the technical depth, especially in sections like the proofs of **Propositions 3.1 and 3.2**, may present a challenge to those without a specialized background. Despite this, the structured presentation helps readers grasp the main contributions of the proposed methods to the denoising process.\n\nThe paper\u2019s significance is substantial, addressing critical limitations of current diffusion models, notably their high computational costs and the iterative inefficiency in denoising. By reducing conditional entropy at each step, the paper achieves enhanced denoising quality and efficiency, while also providing a valuable entropy-centric framework that could apply to other generative modeling tasks. This entropy reduction perspective provides a foundational shift that could potentially expand to broader applications requiring high-quality generative outputs in fields like **image and video synthesis, text-to-image generation, and voice synthesis**. The methodology outlined here holds promise for influencing future model design by establishing entropy as a target for iterative refinement, potentially impacting other AI subfields that prioritize both efficiency and quality in generative modeling.\n\nIn conclusion, this paper demonstrates a high degree of originality through its entropy-focused approach, maintains rigorous research standards, communicates complex methodologies with considerable clarity, and offers significant potential for advancing the field of diffusion models and generative modeling. By introducing conditional entropy reduction as a tool for accelerating and enhancing denoising diffusion, the paper presents a compelling framework with wide-ranging implications for applications demanding efficient and high-fidelity denoising processes."
            },
            "weaknesses": {
                "value": "This paper offers a robust theoretical framework with innovative contributions to diffusion models (DMs) through the use of conditional entropy reduction for enhanced denoising efficiency. However, there are several areas where improvements could significantly increase the paper's accessibility, generalizability, and practical utility.\n\nFirstly, while the paper provides detailed theoretical support, certain sections, particularly the proofs for **Propositions 3.1 and 3.2** and the derivation of gradient estimation-based iterations, are dense and technically complex. This high level of mathematical rigor may make the work less accessible to readers who lack a specialized background in advanced calculus or information theory. Simplifying or supplementing these proofs with intuitive explanations and illustrative diagrams could make the core ideas more accessible, potentially widening the audience and facilitating broader adoption of the methodology.\n\nSecondly, the empirical validation, though robust on image-based benchmarks like **CIFAR-10** and **ImageNet**, lacks diversity across different types of generative tasks and data. The experiments focus primarily on image data, which limits the conclusions that can be drawn about the method\u2019s effectiveness in other areas where DMs are applied, such as **text-to-image, audio, or video generation**. Extending the validation to these domains would provide stronger evidence of the proposed method\u2019s versatility and demonstrate its broader applicability to different generative modeling challenges.\n\nFurthermore, the concept of **entropy reduction** is central to the proposed approach, but the paper could further elucidate how this metric directly influences model performance at each iterative step. Expanding on the relationship between conditional entropy reduction and the quality of denoising could offer readers a clearer understanding of the metric's impact on performance. Including visualizations that show the changes in entropy and their correlation with output quality at each iteration could deepen insights into this relationship and enhance the clarity of the work.\n\nAnother practical limitation lies in the adaptability of **RE-based iterations**. Researchers and practitioners who aim to incorporate these iterations into diverse architectures or applications may face challenges, especially if existing infrastructures require substantial adjustments for entropy reduction integration. More explicit guidance on adapting these iterations for different DM variants would improve the paper\u2019s practical utility. Adding modular guidelines or pseudocode, as well as detailing compatibility with standard DM frameworks, would facilitate adaptation and encourage further experimentation in real-world applications.\n\nIn summary, this paper would benefit from more accessible explanations of its mathematical rigor, broader empirical validation across diverse generative tasks, a clearer discussion of the role of entropy in denoising, and practical guidance on integrating RE-based iterations into different DM architectures. Addressing these areas could significantly broaden the appeal and applicability of the work, allowing it to better fulfill its potential as a foundational advancement in efficient, high-quality diffusion modeling."
            },
            "questions": {
                "value": "-  Could you provide a more detailed explanation of how conditional entropy reduction directly influences the quality of denoising at each iterative step? Specifically, how does lower conditional entropy correlate with higher-quality generative outputs in a measurable way?\n\n\n\n-  How adaptable are the RE-based iterations to different types of diffusion models, such as those used in text-to-image generation or audio synthesis? Are there any known limitations or additional requirements when applying RE-based iterations to these variations?\n  \n\n - Do you have plans to test RE-based iterations on tasks beyond image synthesis, such as text-to-image, audio, or video generation? If not, could you discuss any anticipated challenges in applying the method to these domains?\n  \n-  The mathematical proofs, especially for **Propositions 3.1 and 3.2**, require a strong background in advanced calculus and information theory. Could you provide more intuitive explanations or visual aids to support readers who may find these sections challenging?\n   \n\n - Have you measured the computational overhead introduced by the RE-based iterations? How does the entropy reduction approach impact memory usage, runtime, and scalability for larger datasets or more complex models?\n  \n\n- For researchers interested in implementing RE-based iterations in their diffusion models, could you provide pseudocode or a high-level description of the integration steps?\n   \n\n\n-  Did you observe any trade-offs between denoising speed and output quality when using RE-based iterations? If so, how might these trade-offs vary across different types of models or data?\n \n\n\n- Have you established any baseline or threshold levels for conditional entropy across typical datasets (e.g., CIFAR-10, ImageNet) that could serve as reference points for evaluating model performance?\n  \n- Beyond FID scores, have you evaluated RE-based iterations on other metrics (e.g., perceptual quality, accuracy of specific content features)? If not, are there particular metrics that may benefit from future analysis?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There are no ethical concerns with this research."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 1
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}