{
    "id": "KkALFpRWSV",
    "title": "Skill-based Safe Reinforcement Learning with Risk Planning",
    "abstract": "Safe Reinforcement Learning (Safe RL) aims to ensure safety when an RL agent conducts learning by interacting with real-world environments where improper actions can induce high costs or lead to severe consequences. In this paper, we propose a novel Safe Skill Planning (SSkP) approach to enhance effective safe RL by exploiting auxiliary offline demonstration data. SSkP involves a two-stage process. First, we employ PU learning to learn a skill risk predictor from the offline demonstration data. Then, based on the learned skill risk predictor, we develop a novel risk planning process to enhance online safe RL and learn a risk-averse safe policy efficiently through interactions with the online RL environment, while simultaneously adapting the skill risk predictor to the environment. We conduct experiments in several benchmark robotic simulation environments. The experimental results demonstrate that the proposed approach consistently outperforms previous state-of-the-art safe RL methods.",
    "keywords": [
        "Safe Reinforcement Learning",
        "Skill-based",
        "Risk Planning"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "This paper presents a novel approach in safe RL, learning safe skills from offline data and optimizing them through a risk planning process using a skill risk predictor derived via PU learning to enhance online safe RL.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=KkALFpRWSV",
    "pdf_link": "https://openreview.net/pdf?id=KkALFpRWSV",
    "comments": [
        {
            "summary": {
                "value": "The paper presents a novel approach using learned skills to improve online exploration in CMDPs given available offline data. In the offline phase, the presented method leverages the algorithm presented in [1] to learn skills as high-level actions. To enable safe learning, the authors employ a safety classifier $P_\\xi(c|s_t,z_t)$ that takes as input $s_t$ and the skill $z_t$ using a similar PU learning approach similar to [2]. In the online exploration phase, the authors use SAC to learn a skills policy $\\pi_\\theta(z_t | s_t)$ to maximize rewards. In each environment step, the learned skills policy $\\pi_\\theta(z_t | s_t)$ is used to initialize the plan for a risk planner based on CEM. The planner optimizes for safe skills by minimizing the safety classifier cost. In the experimental section, the paper is compared to recovery RL[3], CPQ[4] and SMBPO[5]. \n\nThe contributions of the paper can be summarized in the following points\n- Exploring the skill learning method from [1] in a safe RL setting.\n- Learning safety classifier using PU sampling.\n- Introducing a risk planner to optimize safe skills by minimizing safety classifier costs.\n\n[1] Pertsch, Karl, Youngwoon Lee, and Joseph Lim. \"Accelerating reinforcement learning with learned skill priors.\" Conference on robot learning. PMLR, 2021.\n\n[2] Xu, Danfei, and Misha Denil. \"Positive-unlabeled reward learning.\" Conference on Robot Learning. PMLR, 2021.\n\n[3] Thananjeyan, Brijen, et al. \"Recovery rl: Safe reinforcement learning with learned recovery zones.\" IEEE Robotics and Automation Letters 6.3 (2021): 4915-4922.\n\n[4] Xu, Haoran, Xianyuan Zhan, and Xiangyu Zhu. \"Constraints penalized q-learning for safe offline reinforcement learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 8. 2022.\n\n[5] Thomas, Garrett, Yuping Luo, and Tengyu Ma. \"Safe reinforcement learning by imagining the near future.\" Advances in Neural Information Processing Systems 34 (2021): 13859-13869."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper investigates an interesting setting of using offline data to enable RL for online exploration in constrained settings. This topic is crucial for enabling RL to explore in constrained real-world scenarios. In this setting, the paper explores using skills in safe RL, which is a relatively unexplored combination. The paper provides a clear description of the algorithm, which aids in understanding the advantages and disadvantages of the proposed method."
            },
            "weaknesses": {
                "value": "- The impact of using skills needs to be ablated. It is unclear whether using skills in this context is useful. Is it beneficial to only act on each H step in a constrained setting? The authors should ablate using the presented planning and policy learning combination on low-level actions. \n\n- The motivation behind using PU learning to learn the safety classifier is unclear. The authors use the classifier as a cost-to-go function, which would typically be learned as a safety critic using approximate dynamic programming [1,2]. It's unclear whether PU learning can be better than approximate dynamic programming in capturing the temporal aspect of the cost. \n\n- The paper claims to learn a safe policy when the policy is not optimized to learn safe behavior but to maximize rewards. This is clear in the ablation studies where the policy has a much worse behavior regarding adherence to constraints without the risk planner. However, the paper mentions learning a safe policy on multiple occasions, which needs to be clarified.\n\n- Unfair comparison with baselines. SMBPO should also be pretrained to ensure a fair comparison. Even training it from scratch, SMBPO has very similar performance in 2 environments. Additionally, the recovery RL baseline is not tuned to the benchmark used. However, recovery RL is sensitive to the $\\epsilon$ parameter that triggers the recovery policy [1]. Even with an unfair comparison, recovery RL has a similar performance to the presented method in 2 environments. Also, recovery RL only uses the recovery policy (its version of the risk planner) when a trigger condition is fulfilled, unlike the presented method, where the risk planner is used in every step. It's thus hard to judge whether the use of skills is the real reason for the difference in performance or the use of planning in every environment step.\n\n- The absence of an explanation on how the offline data was collected is a significant gap. Details such as the number of samples, the ratio of samples with constraint violations, and other relevant information are crucial for the sake of reproducibility and should be provided.\n\n[1] Thananjeyan, Brijen, et al. \"Recovery rl: Safe reinforcement learning with learned recovery zones.\" IEEE Robotics and Automation Letters 6.3 (2021): 4915-4922.\n\n[2] Srinivasan, Krishnan, et al. \"Learning to be safe: Deep rl with a safety critic.\" arXiv preprint arXiv:2010.14603 (2020)."
            },
            "questions": {
                "value": "- How would the method perform if we replaced skills with low-level actions? It's unclear whether the performance is due to using skills or a CEM planner to minimize constraint cost in every environment step.\n- Why not use a safety critic similar to [1,2] instead of using PU learning for the safety classifier?\n- In lines (19, 128, 307), you mention learning a safe policy $\\pi_\\theta$; however, as far as I understand, the policy only optimizes the policy to maximize rewards. Can you explain what makes the policy safe?\n- Can you explain equation 7?\n- Can you provide more details on offline data, including how it was collected and the percentages of constraint-violating samples in the data?\n\nAdditional remarks:\n\n- The abbreviation PU in used in the abstract without prior introduction\n- LfD is an abbreviation for Learning from Demonstrations, not Reinforcement Learning from Demonstrations (line 42)\n- In line 218 in algorithm1, you mention you sample ($\\mu_0, \\Sigma_0$) from $q_\\psi$, which contradicts the second paragraph in 4.2.1.\n- in line 9 in algorithm, what exactly is pair $\\mathcal{P}$?\n- In the conclusion, you mention learning safe behavior patterns from demonstrations. However, the skill model is not optimized to teach safe skills, just skills as high-level representations of actions. I think this can be confusing.\n\n[1] Thananjeyan, Brijen, et al. \"Recovery rl: Safe reinforcement learning with learned recovery zones.\" IEEE Robotics and Automation Letters 6.3 (2021): 4915-4922.\n\n[2] Srinivasan, Krishnan, et al. \"Learning to be safe: Deep rl with a safety critic.\" arXiv preprint arXiv:2010.14603 (2020)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a method that leverages additional offline data to train skills and a risk predictor and refines the risk predictor while collecting online data, ultimately training a skill-based policy. \nFor safe exploration, the proposed method uses a risk planning strategy, similar to a cross-entropy method (CEM)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The paper introduces the idea of using offline data for training skills and a risk predictor.\n- The paper is well-organized and easy to read."
            },
            "weaknesses": {
                "value": "# Major Weaknesses\n- The contributions are relatively limited.\n    - Each proposed module (skills [1,2], risk predictors [3, 4, 5]) is from existing methods.\n        - If there were any \bnovel training techniques, the author should have highlighted them, but it seems there is nothing new.\n    - The motivation for the proposed method is ambiguous, and in particular, it is unclear which parts of the proposed method improve upon existing methods (lines 44\u201348).\n    - Also, there is no theoretical analysis to motivate it.\n- The experimental validation is weak, with only four tasks and using just three seeds per task, which is insufficient for robust evaluation.\n    - Additional experiments on safe RL benchmarks, such as Safety Gymnasium [6], are necessary for a comprehensive evaluation.\n    - The baseline methods, primarily from 2021 to 2022, appear somewhat outdated, so it would be nice to include a state-of-the-art method.\n- Ablation studies are not sufficient to show the roles of each module.\n    - Although the current ablation study has focused on risk planning, it seems obvious that eliminating risk planning would result in unsafe outcomes, as it does not take into account costs when training the skill-based policy.\n    - The training of the risk predictor and the learning of skills appear to be significantly influenced by the proportion of cost violations or the diversity of demonstrations. Therefore, a performance comparison based on different demonstrations seems essential.\n    - Additionally, since using skills represents a distinct contribution, comparative experiments between employing a skill-based policy and using a direct-action policy are also necessary.\n\n# Minor Weaknesses\n- In line 178, it is essential to clarify that $\\zeta$ in $P_\\zeta(C|s_t,z_t)$ represents the neural network parameters to avoid confusion. \n- For reproducibility, Section 4.1 should include at least loss functions or the algorithms used for training the encoder and decoder of the skills.\n\n# References\n- [1] Campos, V\u00edctor, et al. \"Explore, discover and learn: Unsupervised discovery of state-covering skills.\"\u00a0International Conference on Machine Learning. PMLR, 2020.\n- [2] Eysenbach, Benjamin, et al. \"Diversity is all you need: Learning skills without a reward function.\"\u00a0arXiv preprint arXiv:1802.06070 (2018).\n- [3] Bharadhwaj, Homanga, et al. \"Conservative safety critics for exploration.\"\u00a0arXiv preprint arXiv:2010.14497 (2020).\n- [4] Srinivasan, Krishnan, et al. \"Learning to be safe: Deep RL with a safety critic.\"\u00a0arXiv preprint arXiv:2010.14603 (2020).\n- [5] Thananjeyan, Brijen, et al. \"Recovery RL: Safe reinforcement learning with learned recovery zones.\"\u00a0IEEE Robotics and Automation Letters 6.3 (2021): 4915-4922.\n- [6] Ji, Jiaming, et al. \"Safety gymnasium: A unified safe reinforcement learning benchmark.\"\u00a0Advances in Neural Information Processing Systems\u00a036 (2023)."
            },
            "questions": {
                "value": "1. Given the inclusion of a planning module, this paper appears to suggest that execution time may increase as a potential drawback. Are there any results measuring execution times based on different planning parameters?\n\n2. Is there a part of the method that improves skills online? If not, it seems that the initial skill sets the upper bound for performance.\n\n3. When training a skill-based policy, there appears to be no component accounting for safety, which might result in a high likelihood of choosing actions far from safety during risk planning. How has this issue been addressed? According to lines 334\u2013336, penalties are applied to encourage actions that are closer to a prior. Does this approach effectively resolve the concern?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel approach called Safe Skill Planning (SSkP) for safe reinforcement learning (Safe RL). SSkP enhances safe RL by leveraging offline demonstration data through a two-stage process. The first stage involves learning a skill risk predictor using Positive-Unlabeled (PU) learning from offline demonstrations. The second stage uses this predictor to develop a risk planning process that enhances online safe RL, learning a risk-averse safe policy efficiently while adapting the skill risk predictor to the environment. Experiments in various robotic simulation environments show that SSkP outperforms other state-of-the-art safe RL methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper proposes a new method, SSkP, that combines skill learning with risk planning for safe RL.\n- The approach uses a two-stage process that first learns from offline data and then applies it to online environments, which is efficient and reduces potential damage to physical environments.\n- The risk planning process is a simple yet effective method for generating safer skill decisions, enhancing safe exploration and learning.\n- The method adapts the skill risk predictor to online environments in real-time, showing the ability to learn and adjust on the fly."
            },
            "weaknesses": {
                "value": "- The effectiveness of SSkP relies heavily on the quality and quantity of offline demonstration data, which may not always be available or reliable.\n- The two-stage process and the integration of multiple components (skill model, risk predictor, risk planning) might make the approach more complex to implement and understand.\n- The paper primarily focuses on robotic simulation environments, and it's unclear how well SSkP would generalize to other types of environments or real-world applications.\n- The paper does not discuss the computational cost of the risk planning process, which could be high, especially with large state and action spaces.\n-  As shown in the sensitivity analysis, the performance of SSkP decreases with smaller offline data sizes, indicating a potential weakness in scenarios with limited data."
            },
            "questions": {
                "value": "- How does SSkP handle partially incorrect or noisy offline demonstration data?\n- What are the computational requirements of SSkP, and how does it scale with the size of the state and action spaces?\n- How does SSkP compare to other safe RL methods in terms of computational efficiency?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper considers safe reinforcement learning from demonstration problem where there is offline demonstration dataset available. The proposed method, SSkP, seeks to draw both skill encoder/policy and skill safety information from the offline data, and during online learning plan with the skill safety information to lower safety violations. The authors conduct experiments on four simulated MuJoCo environments and show SSkP can achieve higher performance in terms of performance-risk ratio, i.e., given the same amount of safety violation, SSkP\u2019s performance outperforms. The paper is generally well-written and the experiments support the the paper\u2019s claims, but I find important details about the method missing, and therefore wish authors to clarify."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper looks into an important topic of safety in RL learning process (not only the execution phase but also the training phase), and presents the novel algorithm, SSkP, with mild assumptions of offline data available (some data is labeled as unsafe while all other data is assumed unlabeled). \n\n2. The writing is high-quality; therefore, it is easy for me to follow the whole paper\u2019s logic. The algorithm pseudocodes were very clear about what the procedures are."
            },
            "weaknesses": {
                "value": "1. [biggest question] Based on Algorithm 1 and 2, it seems the risk planning is just choosing the safest skill to activate given the current state without considering how the skill contributes to the task completion. Line 4 of algorithm2 passes in the policy and the risk predictor, but algorithm 1 does not utilize the policy at all and rather samples from the skill prior q_\\psi. In this case, wouldn\u2019t the agent just choose safe but task-irrelevant skills? For example, to avoid colliding with anyone, an autonomous vehicle would just stop and stay. \n\n2. From my understanding, the online RL only updates the skill policy network that chooses skills based on state and keeps the skill policy that chooses actions based on state and skill fixed (this only learns from the offline dataset). Why? Do you assume the action policies learned from offline dataset are very good? How much demonstration was used in the experiments and has the authors done scale experiments with different amount of offline data? \n\n3. The paper seems to be lacking a \u201cpreliminary\u201d section where the authors should show prior work that is directly utilized in the method, such as the PU learning and skill learning via skill-conditioned learning from demonstration. These are important prior knowledge to understand the whole method. More importantly, the paper currently mixes previous methods with its own, novel components, making it hard to judge the novel parts of the work. For example, it seems like the skill risk predictor is novel and all other skill learning from demonstration is using the previous work. However, the method to learn the skill risk predictor again is a prior method PU. \n\n3.1. How is \\lambda determined? It seems an essential hyperparameter for PU to work. \n\n3.2. In Equation 6, why don\u2019t the authors consider allowing covariance? Covariance is an important factor for CEM to accurately represent the correct shape of the distribution. \n\n4. The experiment domains seem not diverse enough (all locomotion tasks) while there are many good safe RL benchmarks such as safety-gym and safety-gymnasium. \n\n4.1. Line 415 states \u201cSMBPO demonstrates a similar inability as CPQ in terms of learning a good policy to maximize the expected reward\u201d. I believe this is not true as the SMBPO final performance is very close to SSkP (much higher than CPQ), and the Table 1\u2019s result on Ant shows that too. \n\n5. The related work is mostly comprehensive but lacks a clear description of how the current work distinguishes itself from prior work, in each of the paragraphs of the related work. I strongly recommend authors discuss this to show the uniqueness of the proposed approach. \n\n5.1. On line 96, the authors stated \u201cReinforcement Learning from Demonstration, also known as Imitation Learning\u201d \u2013 I believe this is generally not the well-accepted definition of imitation learning. Imitation Learning generally refers to methods like behavior cloning and inverse reinforcement learning. Reinforcement Learning from Demonstration is a series of work on its own. \n\n5.2. There has been work on extracting safety information from demonstrations, such as [1]. The authors could discuss the differences between their work and prior work. \n\n[1] Yang, Y., Chen, L., Zaidi, Z., van Waveren, S., Krishna, A., & Gombolay, M. (2024, March). Enhancing Safety in Learning from Demonstration Algorithms via Control Barrier Function Shielding. In Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction (pp. 820-829)."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}