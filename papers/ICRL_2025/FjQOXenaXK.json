{
    "id": "FjQOXenaXK",
    "title": "Do Large Language Models Truly Understand Geometric Structures?",
    "abstract": "Geometric ability is a significant challenge for large language models (LLMs) due to the need for advanced spatial comprehension and abstract thinking. Existing datasets primarily evaluate LLMs on their final answers, but they cannot truly measure their true understanding of geometric structures, as LLMs can arrive at correct answers by coincidence. To fill this gap, we introduce the GeomRel dataset, designed to evaluate LLMs\u2019 understanding of geometric structures by isolating the core step of geometric relationship identification in problem-solving. Using this benchmark, we conduct thorough evaluations of diverse LLMs and identify key limitations in understanding geometric structures. We further propose the Geometry Chain-of-Thought (GeoCoT) method, which enhances LLMs\u2019 ability to identify geometric relationships, resulting in significant performance improvements.",
    "keywords": [
        "Large Language Models",
        "Geometric Ability Evaluation",
        "Geometric Relationship Identification"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "This paper introduces a dataset to evaluate large language models\u2019 understanding of geometric structures and proposes a method to enhance their ability to identify geometric relationships.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=FjQOXenaXK",
    "pdf_link": "https://openreview.net/pdf?id=FjQOXenaXK",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates whether large language models can truly understand geometric structure and solve geometric problems. It introduces a new benchmark, **GeomRel**, focused on geometric relationships in three categories: line-based, angle-based, and shape-based relationships. To expand the dataset\u2019s depth and complexity, the authors construct advanced and diverse data subsets within GeomRel. Evaluations on this benchmark reveal that even advanced models like GPT-4o outperform Random Guess by only 20.34% on complex tasks in the advanced GeomRel subset. Building on insights from these evaluations, the paper proposes a two-stage pipeline that guides models to decompose geometric structures and perform relationship observation. Experimental results demonstrate the effectiveness of the proposed pipeline in enhancing model performance on geometric tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper introduces a new benchmark that reveals current LLMs struggle to effectively recognize geometric structures.\n- It demonstrates that even with few-shot prompting or fine-tuning, current LLMs do not perform well in recognizing geometric relationships. To address this, the paper proposes a two-stage pipeline that guides LLMs to decompose and observe geometric structures.\n- Experimental results in the final section show that the proposed two-stage pipeline effectively enhances LLM performance on geometric tasks."
            },
            "weaknesses": {
                "value": "- This paper dedicates significant space to describing the rules and various data augmentation methods used in constructing the benchmark. However, the overall process and rationale for construction could be presented more clearly.\n- In Section 3.5, the paper uses the LLaMA-3-8B-Instruct model as the base model for fine-tuning, which is somewhat unconventional, as it is more typical to fine-tune base models on math-related datasets.\n- Including additional experimental metrics commonly used in math domains, such as pass@k, majority voting, or best-of-n, could improve the depth of analysis and provide a more comprehensive evaluation of model performance in this paper."
            },
            "questions": {
                "value": "- Could you elaborate on how the benchmark was constructed and clarify how the different relationships, difficulty levels, and diversity augmentation methods were integrated? And for instance do you leverage LLMs to help generate the benchmark or all the questions are based on rules?\n- Could you provide results from fine-tuning on the LLaMA-3-8B base model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the limitations of large language models (LLMs) in understanding geometric structures, proposing a new benchmark, GeomRel, specifically designed to assess LLMs' ability to identify geometric relationships. The authors identify that while existing datasets mainly measure final answer accuracy, they fail to capture whether LLMs truly understand underlying geometric structures. The GeomRel benchmark isolates the task of geometric relationship identification (GRI) as a foundational skill for geometric reasoning. Using GeomRel, the paper evaluates several LLMs and finds that most perform well on simple geometric relationships but struggle with more complex structures, particularly those involving angle relationships. The authors introduce a new method, Geometry Chain-of-Thought (GeoCoT), which decomposes geometric reasoning into step-by-step relationship identification, significantly improving model performance on GeomRel by over 9% on basic tasks and nearly 15% on advanced tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The GeomRel dataset, which isolates geometric relationship identification as a key step, provides a novel and focused way to evaluate LLMs' geometric understanding. This dataset addresses a unique gap in the field and enables more focused evaluation of geometric reasoning capabilities.\n\n2. The Geometry Chain-of-Thought (GeoCoT) method improves LLMs\u2019 performance in identifying geometric relationships by breaking down problems into reasoning steps. The proposed method significantly improved both basic and advanced variants of the dataset.\n\n3. The authors evaluates a range of LLMs, including both proprietary and open-source models, providing a rounded view of how different models handle geometric reasoning. This extensive evaluation is valuable for identifying model-specific strengths and weaknesses in geometric tasks.\n\n4. The paper investigates multiple prompting methods, including Zero-Shot and Chain-of-Thought (CoT), analyzing their effectiveness in GRI tasks. This analysis highlights that while traditional CoT is less effective for geometric tasks, the GeoCoT adaptation provides usefgul improvements for geometric reasoning."
            },
            "weaknesses": {
                "value": "1. Although fine-tuning on GeomRel was attempted, the results only report results of a single model. More experiments exploring different models could clarify which models has performance improvements with fine-tuning.\n2. It is difficult to truly understand the difficulty of the dataset for human intelligence. This can be understood by sampling the dataset and carrying out a systematic human evaluation to report human baselines on these geometrics reasoning problems.\n3. GeomRel, while valuable for evaluating LLMs on basic geometric relationships, covers a limited range of geometric concepts, primarily focusing on 2D relationships involving lines, angles, and shapes. This narrow scope may not fully represent the complexity of real-world geometric tasks and it will further help evaluate reasoning capabilities particularly those that involve 3D relationships, transformations (like rotations and reflections), or coordinate-based reasoning."
            },
            "questions": {
                "value": "1. It would help the readers better, if the authors can provide the exact versions of proprietary LLMs that was used as well as other hyper parameters set for these models for inferences as well as for the fine tuning experiments."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates LLMs abilities to understand geometric relationships and spatial reasoning based on textual descriptions. The authors observe that LLMs often arrive at correct answers without grasping the underlying geometric relationships. To address this, they propose a new task called Geometric Relationship Identification (GRI) and introduce GeomRel, a dataset specifically designed to evaluate models' abilities in GRI. For dataset construction, they create a collection of basic geometric relationships and develop more complex examples by combining and enhancing these foundational relationships.\n\nThe authors conduct extensive experiments across several models and strategies, including zero-shot, few-shot, Chain-of-Thought (CoT), and fine-tuning methods. Their findings indicate that current LLMs have significant limitations in spatial understanding and that most strategies yield limited improvements. They propose a new prompting technique, GeoCoT, which decomposes geometric observations and employs reverse reasoning."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The paper establishes GRI as a new task, shifting focus beyond answer accuracy to intermediate steps in spatial reasoning. \n* The proposed dataset, GeomRel, is valuable, especially with its advanced split that incorporates logical chains, indeterminate cases, and extraneous information, mimicking real-world problem complexity and ambiguity. \n* The paper provides a thorough evaluation across multiple LLMs and reasoning methods, yielding insights into their spatial reasoning capacities and limitations."
            },
            "weaknesses": {
                "value": "* For disambiguation, the authors manually reviewed and excluded ambiguous data throughout the data construction process, which may reduce scalability and limit others\u2019 ability to expand the benchmark.\n* The paper\u2019s focus on GRI as an isolated skill may be too narrow, leaving it uncertain if success in GRI tasks will translate to general spatial reasoning or even multi-step problem-solving abilities.\n* The two-stage GeoCoT method, which involves decomposing geometry problems and reverse reasoning, may be challenging to scale to more complex tasks."
            },
            "questions": {
                "value": "* While the advanced split of GeomRel does require managing multiple geometric relationships, it still treats each relationship as relatively independent. Real-world geometric problems, however, often demand that LLMs execute step-by-step calculations in sequence, where each conclusion logically affects the next. \n* Some analyses in Appendix F are particularly insightful, demonstrating the reasoning path and showing how the vanilla model fails while reverse reasoning aids understanding. It would be valuable to provide experimental results to support these explanations.\n*  the paper show marginal gains from fine-tuning. It would be valuable to explore the reasons behind this limited improvement, possibly factors like dataset size, or the nature of the fine-tuning data. \n* In explaining the performance improvements from point relabeling, the authors hypothesize that \"using more complex descriptions may stimulate LLMs\u2019 reasoning abilities.\" This point could benefit from further explanation."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}