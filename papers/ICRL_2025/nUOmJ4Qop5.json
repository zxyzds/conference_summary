{
    "id": "nUOmJ4Qop5",
    "title": "Peacock: Multi-Objective Optimization for Deep Neural Network Calibration",
    "abstract": "The rapid adoption of deep neural networks underscores an urgent need for models to be safe, trustworthy and well-calibrated. Despite recent advancements in network calibration, the optimal combination of techniques remains relatively unexplored. By framing the task as a multi-objective optimization problem, we demonstrate that combining state-of-the-art methods can further boost calibration performance. We feature a total of seven state-of-the-art calibration algorithms and provide both theoretical and empirical motivation for their equal and weighted importance unification. We conduct experiments on both in and out-of-distribution computer vision and natural language benchmarks, investigating the speeds and contributions of different components.",
    "keywords": [
        "Deep Neural Network Calibration",
        "Uncertainty Calibration",
        "Robustness",
        "Safety",
        "Out-of-Distribution"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We propose Peacock a novel multi-objective calibration framework motivated by the strengths of recent contributions. We demonstrate how to effectively combine different methods, into a fast and well-optimized calibration algorithm.",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=nUOmJ4Qop5",
    "pdf_link": "https://openreview.net/pdf?id=nUOmJ4Qop5",
    "comments": [
        {
            "summary": {
                "value": "The paper presents Peacock, a calibration algorithm that performs multi-objective optimization of existing calibration techniques. The method is shown to improve in and out-of-distribution calibration while retaining performance and speed on computer vision and natural language benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "\u2013 The paper is very well written and easy to follow\n\n\u2013 The paper does a good job of thoroughly reviewing prior work\n\n\u2013 The paper includes a comprehensive appendix with extensive details as well as a detailed codebase which will aid in reproducibility\n\n\u2013 The theoretical justification for the multi-objective formulation is intuitive and well-explained\n\n\u2013 The proposed method seems effective on a range of datasets without adding a computational overhead"
            },
            "weaknesses": {
                "value": "\u2013 The paper framing refers to calibrating \u201cdeep neural networks\u201d in general rather than classification models. From what I can tell the proposed approach is designed only with classification in mind \u2013 it would be helpful to either update the paper\u2019s framing to reflect this, or to include additional discussion (or if possible, even a proof of concept experiment) demonstrating how Peacock may be generalized to other tasks (or even to specialized classification tasks such as dense prediction)\n\n\u2013 The technical contribution of the paper is somewhat limited \u2013 1) it shows that multi-objective optimization of existing calibration strategies is both theoretically and empirically better than or equivalent to any one in isolation (which is consistent with related findings in loss and model ensembling, as the paper acknowledges), and 2) it proposes an algorithm to compute \u201cpareto optimal importance weights\u201d efficiently. That said, it does a very thorough job of exploring various design choices, motivating its method, and reviewing the literature, due to which I do not consider this a significant weakness.\n\n\u2013 The \u201cweighted importance formulation\u201d would benefit from additional motivation and . L306 claims that \u201closs terms can often be conflicting\u201d as the motivation for importance weighting \u2013 it would be good to experimentally demonstrate this. Further, do any of the theoretical guarantees for the equal weighting formulation apply to the importance weighted version? Finally, why does importance weighting not consistently outperform equal weighting in Table 1 and 2? \n\n\u2013 Some aspects of the method design need elaboration. For eg. why is \u201cdirection weighted self-attention\u201d required to learn loss weights, rather than say a simple linear layer?"
            },
            "questions": {
                "value": "Please see weaknesses above for a detailed list of questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a comprehensive survey on \u201cMulti-objective optimization for deep neural network optimization\u201d. Motivated from the previous literature, this paper advocate a method of unifying the previous baselines into one, dubbed the method \u201cPeacock\u201d. \n\nThis method, according to figure 1(a), encompasses a few recent works: AdaFocal, Dual Focal, and MaxEnt for entropy based one, as well as margin-based CPC and regularizers based RankMixup and Post-hoc processing AdaTS. \n\nExperiments are conducted on many domain shift datasets, like CIFAR10/100-C, FmoW, CivilComments and so on."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "+ This paper presents a complete survey of previous works. \n+ It seems to be a good reading material for the newcomers to this field. \n+ Experiments shown in this paper indicates averaging some of the good methods can improve the performance further. This is good to be known for the community"
            },
            "weaknesses": {
                "value": "There is no solid scientific contribution in terms of novel methods. Instead, it basically composes several state-of-the-art methods and constitutes as an engineering effort to unify previous methods. I think in general this paper does not have technical algorithmic novelty, and seems to be a technical report rather than a scientific paper. \n\nThere exists some presentation mistakes:\n- In the figure of page 2, It should be Figure 1, but this caption is missing. \n- I am not sure why this method is called \u201cPeacock\u201d, it lacks of any interpretation but seems to be a meaningless shortcut."
            },
            "questions": {
                "value": "As above. \n\nMy biggest concern is this paper is taking a short cut, summarizing many previous works and put them into one in a naive manner. This might take some of the works, but not the actual contribution that excites the community."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a framework (Peacock) designed to improve neural network calibration through multi-objective optimization. Emphasizing the importance of accurate model confidence in safety-critical applications, the framework proposes combining multiple calibration methods into a single framework. Peacock optimizes these methods to reduce calibration errors for in-distribution (ID) and OOD data. They operate in two modes: Equal Importance, where all methods are weighted equally, and Weighted Importance, where weights are dynamically adjusted using a self-attention mechanism. The results show that Peacock achieves better calibration accuracy across various datasets than any individual method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Peacock\u2019s approach to combining calibration methods in a multi-objective framework is novel and reflects a strong and thoughtful attempt to address the limitations of single-method approaches.\n2. Provide a theoretical foundation, arguing that Peacock\u2019s calibration error is bounded by the average error of its components. This mathematical rigor adds to the credibility and applicability of their approach however I need to see other reviewers\u2019 opinion on it.\n3. Peacock\u2019s use of a self-attention-based weighting mechanism in its Weighted Importance formulation is a valuable addition, allowing the framework to adapt to different calibration needs.\n4. The extensive evaluation on synthetic and real-world datasets, including CIFAR variants and iWildCam, demonstrates Peacock\u2019s potential for generalizable performance in ID and OOD contexts."
            },
            "weaknesses": {
                "value": "1. The work could improve clarity around why certain calibration methods were selected and how they interact synergistically within Peacock. It would be helpful if the authors compared Peacock with other excluded methods to understand its compatibility and potential flexibility.\n2. While the self-attention mechanism for adaptive weighting is innovative, the authors do not provide a clear explanation of how specific weights are adjusted in various calibration scenarios. Detailed case studies or visualizations could strengthen the interpretability of the model\u2019s weighting adjustments.\n3. Peacock\u2019s performance is documented on many typical benchmarks, but it remains unclear if it scales efficiently in more resource-constrained settings (less human supervision specially) or real-time applications. Details on Peacock\u2019s resource usage across varying computational capacities could improve the overall proposal of framework.\n4. The dependence on Expected Calibration Error (ECE) and related metrics is standard, yet calibration results could benefit from additional metrics or more especially contextual tests. It would be great to see the limitation of Peacock in unusual or extreme OOD scenarios."
            },
            "questions": {
                "value": "1. Please clarify the selection of these specific calibration techniques. How does Peacock handle any dependencies or conflicting effects among these methods, and what might its performance look like if additional calibration techniques (e.g., ensemble-based or regularization-based) were included? It will be helpful to justify the proposed framework and provide comprehensive details for future research.\n2. Interpretability of Adaptive Weighting: For the self-attention mechanism, can the authors provide insights into how weight adjustments occur across different datasets? Examples of how specific weights change or stay consistent across OOD shifts would clarify the mechanism\u2019s adaptability.\n3. Handling of Adversarial and Non-Standard OOD Scenarios: Peacock performs well in ID and standard OOD settings, but could the authors elaborate on its behavior in more adversarial or highly unstructured OOD contexts? Even very limited empirical evidence will be suffice for it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a multi-objective calibration strategy that relies on Pareto optimal solutions that combines several losses that have been shown to provide good calibrated networks. The approach is evaluated on several small size problems (image and text)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The principle of combining multiple losses designed for good calibration makes sense.\n\n- Comprehensive evaluation and ablation study on multiple architectures and datasets.\n\n- The results are constantly good on the evaluated problems (but only small size problems).\n\nI rate the work as \u201cmarginally below the acceptance threshold\u201d: I may change my score after reading the rebuttal and other reviewers' comments. ."
            },
            "weaknesses": {
                "value": "- The proposed approach is basically a multi-objective optimization using the latest calibration losses from the literature: the algorithm novelty is limited.\n\n- I had a problem with the mathematical notations which are often not clearly defined. This makes the paper difficult to check. See the questions section.\n\n- Evaluation is only for small size problems (according to current standards)."
            },
            "questions": {
                "value": "- I had problems with the mathematical notations. For example (but not only): \n   - l.139 and 141: is $P_i(y_k | x)=P(\\{y=y_k\\}|x_i)$ the likelihood or score of hypothesis $k$ when $x_i$ is the input ? What is $h_i$?\n  - Eq. 1: ECE is a scalar measure that characterizes globally the predictor (Naeini et al., 2015): How can it depend on a sample $x$?\n  - l. 281: what is $\\bar{y}$?\n  - Eq.11:  I don\u2019t understand this equation (expectation of a non random scalar).\n  - Please make your mathematical notations sound.\n\n- How does your approach scale with larger models and datasets (ImageNet) ?\n\n- How does the approach compare with post-hoc calibration, which is a much lighter calibration strategy, for instance [1] cited in the paper or [2] not cited? \n\n[1] Jize Zhang, Bhavya Kailkhura, and T Yong-Jin Han. Mix-n-match: Ensemble and compositional methods\nfor uncertainty calibration in deep learning. In ICML, 2020.\n\n[2] Kanil Patel, William H Beluch, Bin Yang, Michael Pfeiffer, and Dan Zhang. Multi-class uncertainty\ncalibration via mutual information maximization-based binning. In ICLR, 2021"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}