{
    "id": "stK7iOPH9Q",
    "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
    "abstract": "Leveraging the visual priors of pre-trained text-to-image diffusion models offers a promising solution to enhance zero-shot generalization in dense prediction tasks. However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation. In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising/denoising diffusion process is also unnecessary and challenging to optimize. Based on these insights, we introduce $\\textbf{Lotus}$, a diffusion-based visual foundation model with a simple yet effective adaptation protocol for dense prediction. Specifically, Lotus is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into a single-step procedure, simplifying optimization and significantly boosting inference speed. Additionally, we introduce a novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions. Without scaling up the training data or model capacity, Lotus achieves SoTA performance in zero-shot depth and normal estimation across various datasets. It also enhances efficiency, being significantly faster than most existing diffusion-based methods. Lotus' superior quality and efficiency also enable a wide range of practical applications, such as joint estimation, single/multi-view 3D reconstruction, etc.",
    "keywords": [
        "Diffusion Models; Dense Prediction; Monocular Depth Estimation; Monocular Normal Estimation"
    ],
    "primary_area": "generative models",
    "TLDR": "Based on pre-trained Stable Diffusion, Lotus delivers SoTA performance on monocular depth & normal estimation with simple yet effective fine-tuning protocol.",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=stK7iOPH9Q",
    "pdf_link": "https://openreview.net/pdf?id=stK7iOPH9Q",
    "comments": [
        {
            "summary": {
                "value": "The authors simply told us diffusion-based method is not suitable, not necessary, and even harm dense prediction quality. They form a story that they reduce the diffusion step to 1, while adding a regularization of detail preserver, and finetune from a large-scale UNet pretrained on T2I task, to achieve good dense prediction performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The authors presented an interesting story of removing 'diffusion' from dense prediction to make things less complicated. \n- If the proposed method and claims are proved to be effective, it shows using a diffusion-based dense prediction method is overkill compared to the efficiency of a 1-step direct prediction. It can be a nice finding."
            },
            "weaknesses": {
                "value": "- What is the fundamental difference between the proposed method with a model trained with UNet using latent-space? If there is no diffusion process and the model is doing x0 prediction, why it is still called diffusion method, especially when the authors proposed a discriminator version, does it just fall back to a simple UNet-based method?\n- If the authors claim the powerful prediction quality can come from the strong prior of the pretrained model, but the currently proposed detail preserver, is just learning the reconstruction, and can be lazily learned via the skip connection within the UNet structure, it does not seem to preserve the original generator capability. The reviewer believe after longer training or large batch size training, the performance will degrade more. It is interesting that the detail preserver can enhance the detail by a large margin, given the reconstruction can be easily learned by the residual connection in each layer. \n- If the authors claim it can be served as a visual foundation model, it needs to show more capability of generalization or potentials of other tasks, either through finetuning or adaptation. \n- The qualitative results seem still not strong enough compared with other baselines, and are very close to Marigold."
            },
            "questions": {
                "value": "- Do we need to finetune the VAE or decoder for dense representation given the original VAE can be only trained on image data?\n- How did the author evaluate the generative methods given the randomness?\n- Does the conclusions from this paper also apply to other simpler vision tasks? How do the author feel about segmentation task? Also whether diffusion is just not suitable for any tasks requiring pixel-level preservation?\n\nOverall I feel the authors proposed a good observation for dense prediction, but a couple of statements are over-claimed, like foundation model, 1-step 'diffusion' is enough, or detail preserver is necessary."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel method for dense prediction tasks by fine-tuning a pre-trained diffusion model. To achieve stable and consistent predictions, the method shifts the model\u2019s prediction from epsilon to x0 (clean image) prediction, significantly reducing the variance in outputs. This transition allows for accurate single-step inference, which simplifies the process while maintaining high-quality results.\n\nTo avoid generating excessively smoothed outputs\u2014a common issue in dense prediction with diffusion models\u2014the approach incorporates input frame reconstruction into the training process. This addition serves to preserve finer details and structure in the output predictions, balancing accuracy with realistic visual representation.\n\nThe proposed method is evaluated in a challenging zero-shot scenario, where its performance is benchmarked against both discriminative and generative approaches. Despite being fine-tuned on limited data, the model demonstrates competitive, promising performance, highlighting its robustness and adaptability for dense prediction tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper presents its motivation clearly, leading readers through a coherent explanation of the research objectives and the rationale behind the proposed method. Supported by carefully crafted figures, the narrative flows in a way that helps readers follow the progression of ideas and understand the purpose of each methodological contribution. Each figure complements the text by visually illustrating the transition from epsilon prediction to x0 prediction, the integration of input frame reconstruction, and the steps involved in single-step inference. This combination of clear motivation and visual aids effectively demonstrates how the approach stabilizes predictions and reduces variance, helping readers grasp the value and impact of the proposed method for dense prediction tasks, even in zero-shot settings with limited data."
            },
            "weaknesses": {
                "value": "The proposed method, while effective in its application, appears relatively simplistic and may lack substantial innovation. Both the use of  \\mathbf{x}_0  prediction and few-step inference have been previously explored in the diffusion model literature, which limits the originality of these aspects. The  \\mathbf{x}_0  prediction has been extensively discussed in seminal works such as \u201cDenoising Diffusion Probabilistic Models\u201d and Stable Diffusion. Similarly, few-step inference has been considered in studies like \u201cUFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs\u201d and \u201cOne-step Diffusion with Distribution Matching Distillation.\u201d Consequently, these elements alone may not constitute a significant contribution to the field.\n\nFurthermore, the paper\u2019s evaluation and analysis are narrowly focused on the zero-shot scenario, which, while valuable, provides a limited perspective on the method\u2019s potential. To fully illustrate the strengths of this approach for generative dense prediction, a broader evaluation across diverse scenarios beyond zero-shot would enhance the analysis. Expanding the experiments to include intra-dataset inference, or testing in varying contexts, such as semantic segmentation or optical flow, could highlight the model\u2019s generalization abilities and its versatility in handling different levels of supervision.\n\nI also acknowledge the authors\u2019 claims regarding the model\u2019s performance in minimal-data contexts. Exploring the scalability of the model could offer significant insights, especially given the inherent scalability of diffusion-based methods. Demonstrating the model\u2019s performance with larger, more complex training datasets, similar to \u201cDepthAnything,\u201d would facilitate a deeper understanding of its robustness and computational efficiency. This could be achieved by training the model with datasets of varying sizes and evaluating the performance gains of such a diffusion-based approach. An expanded evaluation would provide a more balanced perspective, showcasing the model\u2019s adaptability beyond minimal-data contexts and emphasizing its potential applicability in real-world, data-rich environments."
            },
            "questions": {
                "value": "I\u2019m curious about the mention of \u201cremoving noise\u201d on line 430. In the figure, the noise appears to be concatenated with the input frame\u2014how exactly is this term removed? What value, if any, is set in its place? Does this modification affect any underlying assumptions of diffusion models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel approach to dense prediction tasks in computer vision. The authors analyze existing diffusion formulations and identify that traditional noise prediction methods are suboptimal for dense prediction, leading to significant prediction errors. They propose Lotus, a diffusion-based model that directly predicts annotations instead of noise, simplifying the optimization process and enhancing inference speed. The model employs a single-step diffusion process and introduces a detail preserver mechanism to improve accuracy in detail-rich areas. Experimental results demonstrate that Lotus achieves state-of-the-art performance in zero-shot depth and normal estimation tasks, significantly outperforming existing methods while requiring minimal training data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ This paper introduces a dense prediction method that directly predicts annotations rather than using traditional noise prediction, enhancing prediction stability and enabling one-step inference.\n+ The paper provides an in-depth analysis of existing diffusion models, highlighting instability issues in dense prediction and examining the relationship between prediction variance and time steps.\n+ The proposed method achieves competitive performance across multiple benchmarks, rivaling state-of-the-art approaches."
            },
            "weaknesses": {
                "value": "- In this work, the diffusion model functions more like a single-step restoration network, employing a one-step strategy for both training and inference. It may be beneficial to explore using ZtZ_tZt with different ttt values in one-step training and inference.\n- For the detail-preserving component, a task switcher s is selected during each training iteration. Since predicting annotations and reconstructing images are mutually exclusive tasks, the loss function should be adjusted to reflect this \"either-or\" relationship."
            },
            "questions": {
                "value": "What is the relationship between the proposed network and existing conditional restoration methods? Could this design be extended to other dense prediction tasks, such as segmentation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper follows a recent trend to formulate dense per-pixel regression tasks (in particular, monocular depth and normal estimation) as conditional image generation, so as to benefit from the strong prior of (latent) denoising diffusion models. Building on Marigold [Ke et al., 2024] and its derivative GeoWizard [Fu et al., 2024], the paper proposes the following technical improvements.\n1) Training the diffusion process to predict the denoised image, rather than the noise, as commonly done for image generation; Arguing that the increased variability when predicting the noise is undesirable for regression tasks.\n2) Faster and less variable one-step inference, including a deterministic variant without initial random noise.\n3) Additional regularisation with the image generation task to enhance the preservation of details."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The analysis of predicting the clean image vs. predicting the noise in conditional regression tasks is interesting. It is a relevant observation that, empirically, predicting the clean output directly may perform better in terms of mean prediction error. This could contribute to a better understanding of the mechanisms in diffusion models, and may potentially be interesting also for other conditional uses of those models.\n\nThe regularisation, by switching between predicting the regression target and reconstructing the input image, is plausible. While it is really a simple implementation trick with a fairly hand-wavy explanation, it might be useful for a whole range of applications that repurpose generative models for image analysis tasks.\n\nThe paper provides further evidence for the confluence of conditional generation and discriminative prediction - the evidence is thickening in the last few months that one can move to one-step prediction and even drop the random initialisation (or replace it with a constant zero-input), effectively using a pre-trained multi-step generative model as a \u201ctraining scheme\u201d or \u201cteacher model\u201d for a discriminative one."
            },
            "weaknesses": {
                "value": "The paper does not go very deep in its analysis of the difference between predicting the clean target or the noise. It is a plausible and important message that clean image prediction yields lower errors for certain conditional regression tasks. But the slightly higher mean error of noise prediction could, to some degree, also be an inevitable price to pay for the benefit of proper probabilistic modelling. Is the larger variability really just an algorithmic artefact, or is it perhaps a correct rendition of the underlying predictive uncertainty in the ill-posed task? The findings in the paper are an interesting starting point, but a deeper and more neutral analysis would be helpful.\n\n\nThe findings regarding one-step inference, as well as deterministic prediction without initial noise, are credible and important for the further development of the field. That being said, they should be discussed in the context of other recent literature, in particular [Garcia et al., 2024]. Yes, that paper only came out as a preprint 2 weeks before the ICLR deadline. I am aware that it does not qualify as \"prior art\", and I am not suggesting that there would be any novelty issue. But I would opine that, in this specific case, the work of Garcia et al. should be discussed and not ignored altogether. The findings - one-step training and inference, zero-noise initialisation - are very similar to one of the present paper's main messages. To maximally benefit the research community it will be important to paint a complete picture. Importantly, Garcia et al. show that these improvements are equally possible with the standard formulation (with noise prediction), after fixing a small but impactful bug in the DDIM scheduler of the most popular implementation (HuggingFace Diffusers). Again: I applaud the authors of the present paper for independently confirming that 1-step and deterministic inference are possible based on a pre-trained diffusion model. Still, since [Garcia et al.] is in some sense a bugfix and shows that these improvements apply to pretty much all of the conditional diffusion schemes of the past year if it were not for an incorrect implementation of the scheduler, I really think the authors should relate, compare and discuss that work in the final version of the paper.\n- [Garcia et al., 2024] Garcia, GM, Abou Zeid, K, Schmidt, C, de Geus, D, Hermans, A, Leibe, B (2024). Fine-tuning image-conditional diffusion models is easier than you think. arXiv:2409.11355\n(for the record: I am not a co-author)\n\nMy main technical criticism, where I really hope the authors could provide some insight during the discussion, comes from the ablation study in Table 3. According to that table, the \u201cdirect adaptation\u201d of the diffusion model works a lot worse (2-3 times higher AbsRel), and both x0-prediction and single-timestep inference are needed to reach good performance. But the baselines Marigold and GeoWizard both reach comparable performance (NYU ~5.5, KITTI ~11, ETH3D ~6.5, ScanNet ~6.5) without those modifications, just doing \u201cdirect adaptation\u201d. Why is the Lotus version of \u201cdirect adaptation\u201d so much worse than prior art, and would the modifications have the same impact if one started from one of the competitors that already achieves low errors with direct adaptation?\n\nI also find the \"league table mentality\" in the experiments (which the paper shares with many recent ones) really irritating. I would urge the authors to take a more objective, scientific stance and interpret the experimental results in a sensible and candid manner instead of desperately looking for a \u201cwin\u201d. My interpretation of the bottom 4 lines in Table 1 is not \u201cLotus-G is the best\u201d, but rather \u201call methods based on Stable Diffusion have comparable performance\u201d. And it is totally ok to say that - your method is still fast, elegant and efficient in terms of training data and brings interesting insights, there is no need to over-interpret differences of mostly <0.5 percent points (mischievously, one could say that the only significant differences are on KITTI, where Lotus does not win - but then again that dataset is problematic anyway and should not be taken too serious).\n\nThe language and formulations are sometimes slightly imprecise or awkward, perhaps this can still be improved. Nothing serious that would seriously impair understanding, but still. Random examples:\n- \u201cfirst, their is a pair of autoencoders E(.), D(.)\u201d. I would argue that there is only a single auto-encoder: encoder and decoder parts together form one auto-encoder, not a pair.\n- \u201cthe type of parameterization is a vital configuration\u201d. What exactly does this mean? The type of parametrisation may be important, or the configuration of the diffusion loop may be important, but something seems to be redundant in this sentence.\n\n(there are more such small language and style issues, please check)"
            },
            "questions": {
                "value": "Most important for the rebuttal: please clarify the performance gap of \"direct adaptation\" in Table 3 compared to other recent conditional diffusion approaches. Why do they almost match your best performance despite not using x0-prediction and single-step inference? Or, conversely, why does Lotus not perform decently without those extensions? The answer to this question is likely to significantly influence my final rating (either way).\n\nWhy did you drop DIODE from the test datasets? I am aware that it has certain issues, but since many papers (e.g., Marigold, GeoWizard, BetterDepth, etc.) show DIODE results in addition to exactly the datasets used in the paper, it would be better to also include it, for completeness. Once more, it is not a problem if, for once, you wouldn't get the bold numbers.\n\nSimilarly, why not also include DIODE and OASIS for normals, given that it does not require any training effort and makes the comparisons more complete? Also, some numbers in Table 2 differ from those I have seen floating around (although for some methods, e.g. Marigold, there do not seem to be author-approved or peer-reviewed numbers available at the time of writing). I would recommend to double-check the literature and available code bases for the rebuttal, and again for a possible final submission, to avoid swamping the literature with preliminary and potentially contradictory numbers."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new way of using pre-trained text-to-image diffusion model for dense prediction tasks, including monocular depth estimation and surface normal estimation. Different from existing methods that use the pre-trained diffusion model without considering the dense prediction tasks, authors present several meaningful modifications. First, the parameterization type is changed in a way of predicting original image or annotation ($x_o$) instead of estimating noise. Second, the number of time-steps is reduced (even to one). Third, to enhance the capability of dealing with image details, authors utilize a task switcher that enables the proposed denoiser to generate annotation or reconstruct an input image."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1) The paper was written clearly and is easy to understand.\n2) The three modifications tailored to dense prediction tasks seem to be effective."
            },
            "weaknesses": {
                "value": "1) The two tasks (depth and surface normal estimation) presented in this paper are rather insufficient. It would be better to show the possibility of using this framework in other tasks such as segmentation and detection.\n\n2) More explanations are necessary for the detail preserver, as it is hard to understand why the image reconstruction task improves the details of estimated annotations. The image reconstruction and dense prediction tasks share the U-net denoiser, so it would be harmful to perform the two heterogeneous tasks within the single network due to some unexpected interferences. This part needs to be clarified with more experiments and detailed analysis.\n\n3) It was shown experimentally that decreasing the number of denoising steps is more effective in the dense prediction tasks, but no theoretic analysis is given. For instance, what attributes of dense prediction tasks make it possible to reduce the number of denoising steps, unlike the image generation task? Also, does it work for other prediction tasks such as segmentation and detection?\n\nOverall, this work introduces an interesting framework for dense prediction tasks based on pre-trained text-to-image diffusion model, demonstrating the effectiveness in the monocular depth estimation and surface normal estimation tasks. However, the performance should also be validated in different tasks (segmentation and detection), and more analysis on the detail preserver and the number of denoising steps are necessary."
            },
            "questions": {
                "value": "Refer to the comments in the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N.A."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}