{
    "id": "zqtql1YmlS",
    "title": "Fewer May Be Better: Enhancing Offline Reinforcement Learning with Reduced Dataset",
    "abstract": "Research in offline reinforcement learning (RL) marks a paradigm shift in RL. However, a critical yet under-investigated aspect of offline RL is determining the subset of the offline dataset, which is used to improve algorithm performance while accelerating algorithm training. Moreover, the size of reduced datasets can uncover the requisite offline data volume essential for addressing analogous challenges. Based on the above considerations, we propose identifying Reduced Datasets for Offline RL (ReDOR) by formulating it as a gradient approximation optimization problem.  We prove that the common actor-critic framework in reinforcement learning can be transformed into a submodular objective. This insight enables us to construct a subset by adopting the orthogonal matching pursuit (OMP). Specifically, we have made several critical modifications to OMP to enable successful adaptation with Offline RL algorithms. The experimental results indicate that the data subsets constructed by the ReDOR can significantly improve algorithm performance with low computational complexity.",
    "keywords": [
        "Offline Reinforcement Learning; Data Selection; Grad Match"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "Contruct the reduced dataset to improve algorithm performance while accelerating algorithm training.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zqtql1YmlS",
    "pdf_link": "https://openreview.net/pdf?id=zqtql1YmlS",
    "comments": [
        {
            "summary": {
                "value": "Motivated by the large size of the offline dataset as well as suboptimal data quality in offline RL, this paper considers the problem of finding a coreset out of the given dataset. The authors first formulate such problem as a task to approximate the actual gradients (from the complete dataset) in the offline training process. And a line of of results are provided to support the low approximation errors. Then the method named Reduced Datasets for Offline RL (REDOR) is proposed, inspired by the orthogonal matching pursuit (OMP). Finally, the method is compared with several baseline methods on D4RL data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "> **Originality**\n- Such new method is proposed to select a coreset from the raw offline dataset, which could contribute as an alternative approach in offline RL.\n\n> **Clarity**\n- Several informative figures are provided. Especially the one by t-SNE provides a straightforward way to understand the behaviour of such selection process.\n\n> **Significance**\n- In some of the settings concerned in the experiments, such method is quite efficient."
            },
            "weaknesses": {
                "value": "> **Quality**\n- Several assumptions in Theorem 4.1 are rather stronger than scenarios in actual implementations. One observation often seen in offline RL is the diverging gradients (if without proper training techniques), which, however, are assumed to be uniformly bounded in the paper, w.r.t parameters in respectively policies and Q-functions.\n- Despite the multi-round selection strategy introduced in Section 4.2, as long as the empirical returns are used, as depicted in equation (13), the targets in training steps are relatively fixed (in the sense of distributions due to behaviour policies), which then makes (13) no longer an approximation of Bellman backup errors. As a result, it is currently not clear if such approach would lead to a guaranteed good estimation of values/Q-functions.  \n- According to what the reviewer can understand about the statements and proof for results in Section 5, the theorems only consider the proposed method defined with classic TD loss, while do not consider the techniques emphasized in Section 4.2 - 4.3. As a result, such theoretical discussion is not an actual analysis of the proposed algorithm (feel free to correct me).\n- In Line 766, within the proof for Theorem 5.2, it is not justified why $S\\^k$ can always start from the cluster center ${c\\_k}$ of gradients.\n\n> **Clarity**\n- According to the way a Q-function is defined in Line 99, some index of $t$ should be included in the notation of $Q$.\n- Horizon $H$ is not explicitly defined.\n- There is not enough information for $L\\_{\\text{max}}$.\n- There lacks for an introduction to how KRLS, Log-Det and BlockGreedy are implemented in such offline RL settings.\n\n> **Significance**\n- As explained in the 'Quality' part, the theoretical results seem not to be exactly for the proposed method."
            },
            "questions": {
                "value": "None"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a method for dataset selection in offline reinforcement learning (RL) using the Orthogonal Matching Pursuit (OMP) algorithm and Monte Carlo Q losses. The proposed approach selects full trajectories whose loss gradients align well with the residuals."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The method demonstrates improved performance compared to the baselines.\n2. The paper includes a theoretical analysis that provides a solid grounding for the approach."
            },
            "weaknesses": {
                "value": "1. Some key elements of the proposed algorithm are either missing or unclear, and there are some discrepancies between the paper and the accompanying codebase. For instance, the method used to generate \"hard\" datasets is not fully discussed in the paper, and the percentile $m$ mentioned in the paper differs from that in the codebase. More details are provided in the questions below.\n\n2. Certain parts of the proposed algorithm may contain logical errors or inconsistencies. For example, in Line 4 of Algorithm 2, $r$ is a scalar, yet an inner product operation is applied to it. More details are provided in the questions below.\n\n3. The baselines chosen for comparison seem somewhat outdated, which could affect the perceived significance of the performance improvements demonstrated by the proposed method."
            },
            "questions": {
                "value": "1. Could you please clarify how the suboptimal datasets for MuJoCo, namely \"hard\", were generated? The paper mentioned that they were generated by adding low-quality data, but the quality or source of such data and mix ratio should also be introduced.\n\n2. Regarding $Q_\\theta$ in Algorithm 1, could you explain how $Q_{\\theta_t}$ was formulated? There is no update term in either the pseudocode or the codebase. Was $Q_\\theta$ pretrained or trained simultaneously but omitted? It would be best if the pseudocode or thorough explanations were provided.\n\n3. In Equation 14, it is stated that trajectories in the top $m%$ based on return are filtered, with $m$ set to 50, which would seem to exclude almost the entire random dataset. Could you provide the result of simply selecting trajectories with top $m (=50)\\%$ returns for comparison?\n\n4. In the codebase, it seems that in addition to the evaluation of Monte Carlo Q targets, the selection of candidate trajectories via OMP is filtered based on trajectory returns. What is the exact search space of the selected trajectories? If it is the filtered one with trajectory returns, then how can we ensure the fairness of the comparison to baselines that do not utilize such a filter?\n\n5. In the paper, the percentile $m$ is specified as 50 (Top), but in the codebase, it varies (Bottom 50, 70, and 95). Could you clarify the reason for this difference?\n\n6. In Algorithm 2, $r$ is defined as a scalar, but in Line 4, an inner product is applied. Could you kindly explain this?\n\n7. In Line 3 of Algorithm 2, the inequality appears to be reversed. Is this correct?\n\n8. Is there a reason why TD3+BC was chosen as the backbone offline RL algorithm for the MuJoCo tasks? Would using IQL, as in the Antmaze tasks, provide a more consistent comparison?\n\n9. For the MuJoCo tasks, the authors used the \"-v0\" versions, which are now outdated and differ from the more recent \"-v2\" versions. Could you explain the reasoning behind using \"-v0\"?\n\n10. For the \"Complete Dataset\" scores in the Antmaze tasks, it seems that these values are taken from the IQL paper, which does not provide standard deviations. Could you clarify how these scores were derived?\n\n11. While the baselines used in the experiments appear somewhat dated, dataset selection has recently gained increased attention in offline RL. Hence, it seems that recent algorithms should be contained as baselines. For example, \"Improving Generalization in Offline Reinforcement Learning via Adversarial Data Splitting (Wang et al., 2024)\" provides a codebase, which could allow for a straightforward comparison. Or, is there any reason why such comparisons are inappropriate?\n\n12. Could you provide more details on what is meant by the \"Complete Dataset\" baseline? Specifically, is it the original mixture of the desired dataset and the suboptimal dataset, or is it just the original dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores the interesting concept of finding a subset of the offline dataset to improve the performance of offline RL algorithms using orthogonal matching pursuit. The authors provide empirical and theoretical evidence of performance improvement on benchmark datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is well written and the idea is easy to follow.\n2. The idea of subset selection is novel and interesting.\n3. The paper provides both strong theoretical study and empirical analysis of the proposed method."
            },
            "weaknesses": {
                "value": "1. The authors characterize the field of offline RL only in terms of OOD action penalization and constraints on the behavior policy. There should also be a short discussion on model-based methods like MOPO [1] and MoERL [2], as some of these approaches have been shown to outperform model-free methods.\n\n2. Some parts of the paper are difficult to understand without prior knowledge of orthogonal matching pursuit. Specifically, how is $F\\lambda(s) = L_{max} - min_w Err_{\\lambda} (w, S, L, \\theta)$ used in the OMP.\n\n3. If I understand correctly this method may not lead to the claimed reduction in complexity, as training $Q_{\\theta}$ and $\\pi_{\\phi}$ till requires the full dataset.\n\nMinor \n\nThe table references do not match the table numbers. On line 420, I believe the authors are referring to Table 1 instead of 6.2.\n\nSuggestion : If the authors could include a notations table in Appendix it will help in readability and understanding the proofs.\n\nReferences:\n[1] Kidambi, Rahul, et al. \"Morel: Model-based offline reinforcement learning.\" Advances in neural information processing systems 33 (2020): 21810-21823.\n[2] Yu, Tianhe, et al. \"Mopo: Model-based offline policy optimization.\" Advances in Neural Information Processing Systems 33 (2020): 14129-14142."
            },
            "questions": {
                "value": "Q1. How is the weight $w_i$ or $\\lambda$ decided during training and the parameters $L_{max}$, $m$,$\\epsilon$ chosen in practice?\n\nQ2. Are the networks Q\u03b8, \u03c0\u03d5 networks first trained on the full dataset before starting with the subset selection?\n\nQ3. What is the empirical reduction percentage achieved in each dataset?\n\nQ4. In Figure 1 for the walker2d-expert-v0 environment, the reward first increases and then drops. It is also counterintuitive that the subset selected in ReDOR would perform better than a dataset containing only expert trajectories. Could the authors provide an explanation for this behavior?\n\nQ5. Q5. Could the authors elaborate more on the Prioritize baseline, what do samples with highest TD Loss mean?\n\nQ6. How does ReDOR perform on random datasets such as halfcheetah-random-v2?\n\nQ7. I could not understand Fig 3. Why are the reduced dataset points more for category 6 when it is a subset of complete dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce an approach for reducing the size of a dataset for offline RL by defining this reduction as a submodular set cover problem and using orthogonal matching pursuit. The resulting algorithm is evaluated on a modified version of D4RL locomotion tasks and the original antmaze tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This is an interesting and novel approach for data selection in RL. The high-level approach/formulation of the problem may be useful as a foundation for extensions. \n- Strong results on a modified version of D4RL, and the unmodified antmaze."
            },
            "weaknesses": {
                "value": "There is a discrepancy between the proposed objectives and the resulting objectives that makes me question where the effectiveness of the proposed approach comes from. \n\nThe problem is initially defined as finding a subset of the data which results in a higher performing policy than the policy determined by training on the original dataset (Eqn 3). However, this is immediately discarded for another optimization problem, which instead tries to limit change in the value function (Eqn 5). While discovering a smaller dataset which achieves the same performance as the original dataset is an interesting problem, the authors claim in several places (and demonstrate) that their reduced dataset actually improves the performance. So where does the performance gain come from? \n\nOne possible cause for the performance increase is how the evaluation is done (add noisy/low performing trajectories to the D4RL dataset) and the filtering of low performing trajectories (Eqn 14). I would be very curious if this filtering alone is sufficient to also recover the performance of the algorithm. This concern, along with some missing key experimental details, makes me cautious about the experimental claims made in the paper. \n\nMissing References which also filter the dataset using returns:\n- [1] Chen, Xinyue, et al. \"Bail: Best-action imitation learning for batch deep reinforcement learning.\" Advances in Neural Information Processing Systems 33 (2020): 18353-18363.\n- [2] Yue, Yang, et al. \"Boosting offline reinforcement learning via data rebalancing.\" arXiv preprint arXiv:2210.09241 (2022)."
            },
            "questions": {
                "value": "Additional experiments:\n- Does simply filtering the dataset by high returns recover the same performance?\n- What is the performance of ReDOR on the original version of D4RL? One might expect that reducing mixed quality datasets like medium-expert, or medium, could also result in a high performance. \n\nMissing experimental details:\n- How is the hard dataset generated? How many datapoints are added to the dataset?\n- How many datapoints are removed by ReDOR? What is the size of the reduced datasets?\n\nGeneral:\n- Is there a way to tune the resulting dataset size? \n- Is Fig 3, episode return = 99.5 for behaviors [2-7] correct or a bug?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}