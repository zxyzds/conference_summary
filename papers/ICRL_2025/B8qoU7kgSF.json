{
    "id": "B8qoU7kgSF",
    "title": "Generalization  Bounds for  Neural Ordinary Differential Equations and Residual Neural Networks",
    "abstract": "Neural ordinary differential equations (neural ODEs) represent a widely-used\nclass of deep learning models characterized by continuous depth. Understand-\ning the generalization error bound is important to evaluate how well a model is\nexpected to perform on new, unseen data. Earlier works in this direction involved\nconsidering the linear case on the dynamics function (a function that models the\nevolution of state variables) of Neural ODE Marion (2024). Other related work\nis on bound for Neural Controlled ODE Bleistein & Guilloux (2023) that de-\npends on the sampling gap. We consider a class of neural ordinary differential\nequations (ODEs) with a general nonlinear function for time-dependent and time-\nindependent cases which is Lipschitz with respect to state variables. We observed\nthat the solution of the neural ODEs would be of bound variations if we assume\nthat the dynamics function of Neural ODEs is Lipschitz continuous with respect\nto the hidden state. We derive a generalization bound for the time-dependent\nand time-independent Neural ODEs.Using the fact that Neural ODEs are limiting\ncases of time-dependent Neural ODEs we obtained a bound for the residual neural\nnetworks. We showed the effect of overparameterization and domain bound in the\ngeneralization error bound. This is the first time, the generalization bound for the\nNeural ODE with a more general non-linear function has been found.",
    "keywords": [
        "NEURAL ORDINARY DIFFERENTIAL EQUATIONS",
        "GENERALIZATION  BOUNDS",
        "BOUNDED VARIATION FUNCTIONS",
        "MACHINE LEARNING"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=B8qoU7kgSF",
    "pdf_link": "https://openreview.net/pdf?id=B8qoU7kgSF",
    "comments": [
        {
            "summary": {
                "value": "This submission proposes generalization bounds for neural ODEs and residual neural networks (the latter being a discretization of the first). \nThe first 5 pages of the paper are dedicated to related works and preliminary results. The main results are in Theorems 5.9 and 6.9, generalizing results from Marion (2024), where the author only consider linear parametrization of the residuals (while still having non linear residuals with respect to the activations). Experiments on synthetic data are conducted."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper studies the interesting problem of deriving generalization bounds for residual architectures which are at the core of most successful deep learning methods."
            },
            "weaknesses": {
                "value": "In my opinion, this paper is not suitable for acceptance at ICLR. It appears incomplete and lacking in polish. Below are specific points:\n\n- The paper shares almost the exact same title as Marion (2024), with only the term \"deep\" removed. This is inappropriate.\n- The obtained generalization bounds in Theorems 5.9 and 6.1) are not commented on and, most importantly, are not compared with existing ones from Marion (2024).\n- The second experiment (Fig. 2) appears very similar to experiments shown in Marion (2024) (Figs. 1 and 2), yet Marion\u2019s work is not cited here.\n- The paper references only around 20 prior works, which is insufficient. A broader acknowledgment of previous studies is necessary (see the references cited by Marion (2024) as a comparison).\n- The bibliography is poorly presented and lacks formatting consistency.\n- There are no experimental details provided. I looked in the appendix, but none were included.\n\n\nOverall, the paper appears to have been submitted without adequate proofreading (see, for instance, the last sentence of the abstract). In addition:\n\n- In Assumption 1, there is an unexpected dependence on time\u2014this should be clarified.\n- The symbol $z$ in line 227 is the same as the notation for the ODE solution. This is confusing. \n- Line 230: The expression involving the $\\arg\\min$ is difficult to understand. The $\\arg\\min$ is taken over $\\theta$, yet it is denoted as a function $f$ (that is itself parametrized by $\\theta$). Furthermore, $\\arg\\min$ is applied to $\\theta(t) \\in \\theta(t)$, which is extremely unclear and problematic.\n- Line 235: Typo present.\n- In Lemma 5.1, it would be helpful to explicitly state that Assumption 1 is being used. \n- In Lemma 5.1, the structure is confusing. It is hard to tell what is an assumption and what is a result. Key definitions (e.g., for $f$) are also missing. Can you clarify ? \n- Line 265: The presentation here lacks rigor. Can you please specify the assumptions?\n- There are multiple typos in the use of parentheses in lines 262, 267, and 272, which is unacceptable.\n- Multiple typos are present in the experimental section (e.g., lines 423 and 429)."
            },
            "questions": {
                "value": "- How different are your experimental results from those of Marion (2024)? \n- Why is there sometimes an additional t argument within the parametrized function in the neural ODES? \n- Both theorems provide the same bounds, which requires clarification\u2014how is this possible?\n- Please also see the questions and remarks in the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides generalization bounds for neural ODEs. It extends the class of neural networks used as the dynamics function. The bound is applied also to residual neural networks. Numerical experiments are used to show the effect of hyperparameters on the generalization gap."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The results are applicable to a much larger class of neural ODEs than prior work, such as Marion's paper which is only for when f depends linearly on the parameters. \n* The prior work is well explained. Important lemmas from the prior work as used.\n* I did not find errors in any of the proofs themselves."
            },
            "weaknesses": {
                "value": "* Some of the notation is conflicting/confusing. Please see the questions. \n* The prior work of Bleistein and Guilloux, and Marion is referenced. However, the bounds derived in these paper and in this paper are not compared.\n* The main theorems 5.9 and 6.1 have only an outline of the proof, and there is not a full proof in the appendix.\n* The numerically illustrations are missing details. Please see the questions."
            },
            "questions": {
                "value": "* On lines 89 and 93, the initial condition is given by $z(0)=\\phi_{\\theta(t)}(u)$. Why does the initial condition depend on the parameters at time t instead of time 0?\n* On line 168, should it be \"if\" instead of \"then\"? Is line 169 an assumption of the lemma?\n* In Assumption 3 (line 236), the outcome y is said to be in R. Is y a single number (say the solution at a final time) or is y a function of time? Same question for Assumption 4 and the loss function. Does this theory only apply for one-dimensional ODEs?\n* Why is the risk a function of f in Definition 3.9, but is a function of z in section 4?\n* Why is f a function of z, t, and theta in Assumption 1, but only a function of z and theta in equation (4.3)?\n* In the numerical illustrations, what are the  regularization loss functions for either case?\n* In the numerical illustration, how is the synethic data generated? Is it from numerical solutions of an ODE? If so, what ODE?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Generalization bound of neural ODEs whose vector field is an MLP"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. give an bound estimation of $z(t)$ based on the Lipschitz constant of an MLP $f(z)$\n2. follow the similar technique as [1] to derive the generalization bound of $\\dot z= f(z) $ \n\n[1] Bartlett et, al. Spectrally-normalized margin bounds for neural networks, NeurIPS 2017."
            },
            "weaknesses": {
                "value": "1. The bound estimation of $\\|z(t)\\|$ is very loose due to the Gronwall's inequality $u(t)\\leq \\alpha(t)+\\int_a^t\\beta(s)u(s)ds$. In this case, $\\beta$ is the Lipschitz bound $\\mathrm{Lip}(f)$ and $\\alpha$ is a bound related to $\\mathrm{Lip}(f)$ and bias norm. Thus, the downstream analysis of generalization bound could be very conservative. \n2. $\\mathrm{Lip}(f)$ is estimated using the product of spectral norm bounds, which is again very loose. The SOTA estimation is based on some semidefinite programming formulation, see [2].\n3. The assumption of globally Lipschitz $f$ is quite strong as the popular transformer architecture is only locally Lipschitz. \n4. The experimental results are quite weak, lacking of extensive comparison study. \n5. The presentation is poor. A few (not all) examples are listed as follows:\n\n- The mapping $z(t)\\rightarrow y$ is not defined. \n- Assumption 2 involves $A_i(t)$ and $b_i(t)$ which are introduced later in Section 5.\n- A right ) is missing in 5.4.\n- [1] appears twice in the reference.\n\n[2] P. Pauli et. al. Novel quadratic constraints for extending lipsdp beyond slope-restricted activations, ICLR 2024."
            },
            "questions": {
                "value": "1. There exist many generalization bound analysis of residual networks. Can you provide some comparison studies with Thm. 6.1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors claim to prove new generalization bounds for neural ODEs and residual neural networks. However, these claims are largely unsupported since their work does not significantly improve on Marion (2023) and Bleistein and Guilloux (2024). Some lemmas and proofs are directly borrowed along with notations from these two works without sufficient citations, which might be considered as a case of light plagiarism. The title is almost identical to the work of Marion (2023)."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "I do not believe that this paper has significant strengths."
            },
            "weaknesses": {
                "value": "**Section 2.** The related work is close to insufficient and misses several recent contributions to the field, such as Marion, Wu et al. (2024) and Chen (2024). \n\n**Section 3.** This section compiles a list of definitions and Lemmas without sufficiently motivating their introduction. I would suggest a major rewriting of this section in order to guide the reader through the proofs. \n* Also, Lemma 3.8 does not exist in Bartlett 2017b, and I believe that the Lemma as stated is wrong: there should not be a factor $1/\\sqrt{n}$ in the integral, but rather a factor $1/\\varepsilon$. See Bartlett 2017b Lemma A.5. \n\n**Section 4.** \n* The learning setup is unclear. The authors write \"Let z be the solution of Neural ODE with x as the initial condition and let y be the true solution of the true differential equation learned by Neural ODE given by equation (4.3)\" (l. 221-223): it is unclear what is meant by the \"true solution\". Do the authors assume a generative model for the data ? In this case, it should be introduced.\n* The authors write that the empirical risk \"cannot be optimized, since we do not have access to the continuous data.\" (l. 229 - 230). The authors have not introduced any form of continuous data, nor do they explain why the empirical risk cannot be optimized.\n* This section seems to plagiarize Bleistein & Guilloux (2024) section 3.2, who consider a generative model where a continuous function, which is only observed at a discrete set of sampling times, generates the outcome through an unknown neural ODE. I believe that the authors have carelessly copied this text, hence introducing the two confusing sentences mentioned above which make no sens in their setting as it stands. \n\n**Section 5.**    \n\n* The main contribution of this part seems to be an adaptation of Proposition 2 of Marion (2023) to the general non-linear case. The results in the section strongly resemble the results of Bleistein and Guilloux (2024) --- see Lemma 3.3. These results should be in my opinion at least ackowledged in the main text. \n* Settings concerns about plagiarism aside, the result cited here is directly implied by the results from Bleistein & Guilloux (2024), who establish generalization bounds for neural CDEs of the form  $ dz(t)  = \\mathbf{G}(z(t))dx(t)$ , where $ \\mathbf{G} $ is a generic neural network. Indeed, by setting $ x(t) = t $, one recovers a generic neural ODE. The authors of the aforementioned paper highlight the proximity between both models in Figure 2 of their paper.  \n* In the abstract, the authors claim to have \"showed the effect of overparameterization and domain bound in the generalization error bound\". This is a strong overstatement, since the type of arguments used by the authors only work in the case where $n$ is taken to be sufficiently large to obtain concentration ; even if in this case the number of parameters exceeds the number of observations, these bounds become vacuous in this setting, since the bound presented in Theorem 5.9 does not tend to $0$ when $d$ grows at the same rate than $n$. Hence these bounds say nothing about the overparametrized regime, in which it is typically observed that neural network achieve good prediction performance even if they completely overfit the training data (see for instance Bartlett 2019).  \n\n**Section 6.**\n* Both Marion (2023) and Bleistein and Guilloux (2024) invoke discretization based arguments to go from continuous neural-ODE like architectures to discrete ResNet-type architectures. I do not see such an argument here, and am hence unconvinced by the soundness of Theorem 6.1. In particular, the authors simply write that \"a neural ODE with an euler solver and $\\Delta t= 1$ replicated the ResNet updates, it follows that the solution space of ResNets is contained within the solution space of Neural ODEs.\" (l. 366-368).\n\n**Section 7.** \n\n* The authors claim to perform these experiments on neural ODEs. Given the previous approximations in the paper and the strong similarities of experiments displayed in figures 2 and 3 with the experimental section of Marion (2023), I strongly suspect that these experiments are carried out on ResNets rather than **continuous** neural ODEs. \n* Writing that experiment 1 validates Theorem 5.9 (l. 385) is an overstatement: the authors show (without any confidence intervals) on a purely synthetic dataset that the generalization error increases with the number of hidden units. However, the details on the model are insufficient (what is exactly meant by the number of hidden units ?). Also, since no precision is given on the training data, it is unclear whether the model operates in an overparametrized or underparametrized setting. \n* The experiment displayed in Figure 3 is directly copied from Marion (2023). This article should at least be acknowledged here in my opinion.  \n* The experiment displayed in Figure 2 is novel and investigates the effect of penalizing the loss of a neural ODE with the bound of the solution, hence favoring solutions with a low euclidian norm. However, the experiment is not conclusive due to the high variance and the little variability of the mean for every choice of regularization. \n* Figures should be included in a vectorized format (PNG or PDF).   \n* A experimental appendix should be added, that includes a detailed overview of the experiments.\n\n**References**\n\nBartlett, Peter et al., \"Spectrally-normalized margin bounds for neural networks\", Neurips 2017. \n\nBartlett, Peter et al., \"Benign overfitting in Linear Regression\", Proceedings of the National Academy of Sciences, 2020.\n\nBleistein, Linus and Guilloux, Agathe, \"On the Generalization and Approximation Capacities of Neural Controlled Differential Equations\", ICLR 2024.\n\nChen, Yihang et al., Generalization of Scaled Deep ResNets in the Mean-Field Regime, ICLR 2024. \n\nMarion, Pierre, \"Generalization bounds for neural ordinary differential equations and deep residual networks\", Neurips 2023.\n\nMarion, Pierre, Wu, Yu-Han, et al., Implicit regularization of deep residual networks towards neural ODEs, ICLR 2024."
            },
            "questions": {
                "value": "I believe that this paper is largely insufficient for publication as it stands due to a lack of novelty, and often teeters on the brink of plagiarism. I strongly encourage the authors not to submit this work at the moment and to read the ethics requirements of ICLR 2025. Many points can be improved (see above). I list a few questions bellow. \n\n* Can you provide extensive details on the experiments carried out ? In particular, I would appreciate a mathematical formulation of the model use to generate your data and architectural details. Also, please carefully check that the experiments are run with neural ODEs instead of ResNets. \n* Please provide more mathematical details on your neural ODE to ResNet conversion (Theorem 6.1)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper seems to partially plagiarize Marion (2023) and Bleistein and Guilloux (2024), without any significant contribution. I have provided more details above."
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}