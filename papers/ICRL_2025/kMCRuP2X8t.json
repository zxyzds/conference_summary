{
    "id": "kMCRuP2X8t",
    "title": "Multi-View Graph Neural Networks with Language Models for Mutli-Source Recommender Systems",
    "abstract": "Graph Neural Networks (GNNs) have become increasingly popular in recommender systems due to their ability to model complex user-item relationships. However, current GNN-based approaches face several challenges: They primarily rely on sparse user-item interaction data, which can lead to overfitting and limit generalization performance. Moreover, they often overlook additional valuable information sources, such as social trust and user reviews, which can provide deeper insights into user preferences and enhance recommendation accuracy. To address these limitations, we propose a multi-view GNN framework that integrates diverse information sources using contrastive learning and language models. Our method employs a lightweight Graph Convolutional Network (LightGCN) on user-item interactions to generate initial user and item representations. We use an attention mechanism for the user view to integrate social trust information with user-generated textual reviews, which are transformed into high-dimensional vectors using a pre-trained language model. Similarly, we aggregate all reviews associated with each item and use language models to generate item representations for the item view. We then construct an item graph by applying a meta-path to the user-item interactions. GCNs are applied to both the social trust network and the item graph, generating enriched embeddings for users and items. To align and unify these heterogeneous data sources, we employ a contrastive learning mechanism that ensures consistent and complementary representations across different views. Experimental results on multiple real-world datasets such as Epinions, Yelp, and Ciao demonstrate significant performance improvements over state-of-the-art methods.",
    "keywords": [
        "Graph Neural Networks",
        "Contrastive Learning",
        "Self-Supervised Learning",
        "Language Models",
        "Social trust",
        "Textual Reviews",
        "Recommender Systems"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kMCRuP2X8t",
    "pdf_link": "https://openreview.net/pdf?id=kMCRuP2X8t",
    "comments": [
        {
            "summary": {
                "value": "This paper studies the problem of enhancing recommender systems by incorporating multi-source data to improve recommendation accuracy and robustness. The authors propose a multi-view graph neural network framework that leverages contrastive learning and pre-trained language models to integrate user-item interactions, social trust networks, and user reviews."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1. The majority of the paper is easy to follow.\n\nS2. The proposed multi-view framework effectively integrates diverse information sources, including user-item interactions, trust networks, and review data."
            },
            "weaknesses": {
                "value": "W1. I have some concerns regarding the motivation. The inclusion and processing of raw textual data (such as user reviews) inherently add complexity and computational cost to the recommendation system, particularly in large-scale datasets. It would benefit the paper to include a detailed cost-benefit analysis to justify the addition of textual information.\n\nW2. The baseline models do not incorporate review data, which raises questions about the fairness of the comparisons. It is difficult to discern whether the observed performance improvements caused by the added information or the multi-view design itself. Additionally, the performance gains achieved seem marginal.\n\nW3. In Table 1, the number of nodes and edges in the Epinions dataset is notably lower than in prior studies. The authors should detail the data processing approach for Epinions to clarify the reasons for this discrepancy.\n\nW4. The presentation of the paper needs some improvements. For example, there are several typos in the conclusion."
            },
            "questions": {
                "value": "See W1-W4."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces MvL-GNN, a model that leverages user-user and item-item graphs to generate two additional contrastive views for contrastive learning, thereby improving model performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper is easy to follow, and the proposed method is clearly presented.\n2. The experiments conducted appear to demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "1. There is a lack of performance comparison with methods that only consider the user-item graph, such as LightGCN and NGCF.\n2. There is a lack of significance analysis (e.g., p-value) regarding the performance improvement of the proposed method compared to the baselines."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes a multi-view GNN framework that integrates diverse information sources using contrastive learning and language models. The method employs a lightweight Graph Convolutional Network (LightGCN) on user-item interactions to generate initial user and item representations. An attention mechanism is utilized for the user view to incorporate social trust information with user-generated textual reviews, which are transformed into high-dimensional vectors using a pre-trained language model. Similarly, all reviews associated with each item are aggregated, and language models are used to generate item representations for the item view. An item graph is constructed by applying a meta-path to the user-item interactions. GCNs are applied to both the social trust network and the item graph, generating enriched embeddings for users and items. To align and unify these heterogeneous data sources, a contrastive learning mechanism is employed to ensure consistent and complementary representations across different views."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The dataset and code have been provided, ensuring reproducibility.\n\n* The article has a clear structure and is easy to follow."
            },
            "weaknesses": {
                "value": "* The innovation of this article is limited; techniques like LightGCN, attention mechanisms, item graphs based on meta-paths, and contrastive learning are several years old and should not be the main selling points of the current work.\n\n* This article is too basic in terms of idea, method, and experiments, making it unsuitable for submission to ICLR.\n\n* Such a basic work should have advantages in time complexity and runtime efficiency, but it appears to lack these, not aligning with the style of industry papers."
            },
            "questions": {
                "value": "* Do you have the latest baselines for heterogeneous relationships?\n\n* What advantages does this work have compared to HGCL?\n\n* Is the pre-trained language model specifically trained by you, or is it directly using the Transformers API with the original Hugging Face model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "To ease the sparse user-item interaction problem in recommendation, the authors involve additional information sources (social trust and user reviews) for better modeling and propose a multi-view graph representation learning method. A pre-trained language models BERT is used to obtain review embeddings, and a GCN is used to encode additional knowledge from U-U/I-I networks that are built on the user\u2019s trust network and item-item relationships. The experiments over three datasets show its effectiveness over the compared baselines. However, the novelty of the motivation of introducing addition information (such as user\u2019s social network and reviews) and proposed model is weak. Besides, the experiments are also insufficient."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1)\tThe description of the paper is clear and easy to follow.\n2)\tThe prediction accuracy over the compared baselines shows its effectiveness and the authors also perform ablation study to evaluate the effect of each component."
            },
            "weaknesses": {
                "value": "1)\tThe motivation of introducing addition information (such as user\u2019s social network and reviews) for easing the user-item sparse interaction problem is well studied and lots of previous work in the cold-start and transfer learning domain are explored.\n2)\tThe designed methods that deploys the BERT and GCN to encode textual review information and social network representation are widely-used in recommender system, also for contrastive learning. The novelty of the proposed model is weak.\n3)\tThe experiments compare the proposed model with several baselines without additional information (social trust and user reviews), which is unfair. It is important to provide these information to the baselines as well.\n4)\tThe improvement in Table 2 is too small given additional information used and no significant test were conducted. Besides, the comparison with backbone model lightGCN is missing.\n5)\tInsufficient analytical experiments and lack of experiments on the effectiveness of models under different sparsity."
            },
            "questions": {
                "value": "1)\tWhat is the innovation of this paper and what is the difference from existing work?\n2)\tIt is important to provide these information (social trust and user reviews) to the baselines and show their performance. \n3)\tWhat is the effect of the backbone model lightGCN?\n4)\tWhat is the effect of the model in different sparsity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces MvL-GNN, a multi-view graph neural network framework designed to enhance recommender systems by integrating diverse data sources, including user trust networks, item-wise meta-path based relationships, and user-generated textual reviews. Utilizing LightGCN for capturing user-item interactions, BERT for encoding textual reviews, and an attention mechanism to merge multiple views, MvL-GNN generates enriched embeddings for users and items. The framework employs contrastive learning to align and unify representations across different views, ensuring consistency and complementarity. Extensive experiments on real-world datasets such as Yelp, Ciao, and Epinions demonstrate that MvL-GNN significantly outperforms state-of-the-art methods in terms of Hit Ratio and NDCG, highlighting its effectiveness in addressing challenges like data sparsity and cold start problems in recommendation tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. **Comprehensive Integration of Multiple Data Sources:**\nThe paper presents a robust multi-view approach that incorporates user trust networks, meta-path based item interactions, and textual reviews into the recommendation framework. By leveraging these diverse data sources, MvL-GNN captures a more holistic view of user preferences and item characteristics, which potentially leads to higher recommendation accuracies compared to models that rely solely on user-item interactions.\n\n2. **Effective Utilization of Language Models and Contrastive Learning:**\nThe integration of pre-trained language models such as BERT for encoding user and item reviews allows the model to extract rich semantic information from unstructured text data. Additionally, the use of contrastive learning mechanisms to align and unify representations across different views ensures that the embeddings are both consistent and complementary. The experimental results, showing significant performance improvements on multiple real-world datasets, underscore the effectiveness of incorporating textual features and advanced learning techniques to enhance recommendation accuracy."
            },
            "weaknesses": {
                "value": "1. **Limited Novelty:**\nWhile the integration of multiple data sources is a notable aspect of the proposed method, the core components\u2014namely, the use of language models for textual encoding and the application of contrastive learning techniques\u2014are well-explored in recent literature (references [1-4]). The paper does not sufficiently highlight the fundamental differences, unique contributions, or novel innovations of MvL-GNN that set it apart from existing methodologies, thereby limiting the perceived novelty of the approach.\n\n[1] Representation learning with large language models for recommendation\n[2] Text is all you need: Learning language representations for sequential recommendation\n[3] Heterogeneous graph contrastive learning for recommendation\n[4] Self-supervised heterogeneous graph neural network with co-contrastive learning\n\n2. **Insufficient Analysis of Performance Gains and Statistical Significance:**\nAlthough the experiments demonstrate that MvL-GNN outperforms state-of-the-art methods across various datasets and metrics such as Hit Ratio and NDCG, the magnitude of these improvements may not be substantial enough to warrant the additional complexity introduced by the multi-view framework. Moreover, the paper lacks statistical significance tests to validate whether the observed performance gains are robust and not merely due to random variations, thereby weakening the strength of the experimental claims."
            },
            "questions": {
                "value": "1. **Scalability and Efficiency**: How is the model efficiency of the proposed methods, in comparison to the compared baselines?\n2. **Concrete Examples for Using Textual Reviews to Enhance Recommendation**: Can the authors provide some examples on how textual review data benefits the recommendation accuracy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}