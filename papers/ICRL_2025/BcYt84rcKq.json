{
    "id": "BcYt84rcKq",
    "title": "Fourier Sliced-Wasserstein Embedding for Multisets and Measures",
    "abstract": "We present the _Fourier Sliced Wasserstein (FSW) embedding_\u2014a novel method to embed multisets and measures over $\\mathbb{R}^d$ into Euclidean space.\n\nOur proposed embedding approximately preserves the sliced Wasserstein distance on distributions, thereby yielding geometrically meaningful representations that better capture the structure of the input. Moreover, it is injective on measures and _bi-Lipschitz_ on multisets\u2014a significant advantage over prevalent embedding methods based on sum- or max-pooling, which are provably not bi-Lipschitz, and in many cases, not even injective.\nThe required output dimension for these guarantees is near optimal: roughly $2 n d$, where $n$ is the maximal number of support points in the input.\n\nConversely, we prove that it is _impossible_ to embed distributions over $\\mathbb{R}^d$ into Euclidean space in a bi-Lipschitz manner. Thus, the metric properties of our embedding are, in a sense, the best achievable.\n\nThrough numerical experiments, we demonstrate that our method yields superior representations of input multisets and offers practical advantage for learning on multiset data. Specifically, we show that (a) the FSW embedding induces significantly lower distortion on the space of multisets, compared to the leading method for computing sliced-Wasserstein-preserving embeddings; and (b) a simple combination of the FSW embedding and an MLP achieves state-of-the-art performance in learning the (non-sliced) Wasserstein distance.",
    "keywords": [
        "Sliced Wasserstein distance",
        "Euclidean embedding",
        "bi-Lipschitz",
        "permutation invariance",
        "multisets",
        "optimal transport"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "Euclidean embedding for multisets and measures with injectivity and bi-Lipschitzness guarantees, based on the sliced Wasserstein distance",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=BcYt84rcKq",
    "pdf_link": "https://openreview.net/pdf?id=BcYt84rcKq",
    "comments": [
        {
            "title": {
                "value": "Response to Reviewer styv (Part 2 of 2)"
            },
            "comment": {
                "value": "**Weakness 3: Why bi-Lipschitzness is important**\n\nThe lack of bi-Lipschitzness, which plagues most prevalent multiset architectures to date, practically implies that there inevitably exist pairs of different input multisets that appear numerically identical to the architecture. This poses a problem, for example, in classification tasks where such pairs need to be assigned different labels. Any multiset architecture based on sum- or max-pooling is provably affected by this problem [FWT]. \n\nAchieving a bi-Lipschitz embedding for multisets has been recognized as an important goal in previous works. Our work is the first to fully achieve this goal. Below is a selection of previous works that underscore the importance of bi-Lipschitzness:\n\n> \"The question of which metric spaces admit a bilipschitz embedding into some (finite-dimensional) Euclidean space is natural, and has received a lot of attention in recent work. The results obtained so far indicate that there is no simple answer to this question.\"\n>\n> \u2014 Lang, Urs, and Conrad Plaut. \"Bilipschitz embeddings of metric spaces into space forms.\" _Geometriae Dedicata 87_ (2001): 285-307.\n\n\n> \"Since the late 1990\u2019s, it has become apparent that designing efficient approximate nearest neighbor algorithms, at least for high-dimensional data, is closely related to the task of designing _low-distortion embeddings_. A _bi-Lipschitz embedding_ between two metric spaces $(X,d\\_X)$ $(X',d\\_{X'})$ is a mapping $f : X \\to X'$ such that ... where the parameter $D \\geq 1$ called [_sic_] the _distortion_ of $f$.\"\n>\n> \u2014 Indyk, Piotr, and Assaf Naor. \"Nearest-neighbor-preserving embeddings.\" _ACM Transactions on Algorithms (TALG)_ 3.3 (2007): 31-es.\n\n> \"The second negative result is that while moments of MLPs with analytic activations can be injective, they can never be stable in the bi-Lipschitz sense. This points to a possible advantage of injective multiset functions that are not based on moments, but rather on sorting or max-filters.\"\n>\n> \u2014 Amir, T., Gortler, S., Avni, I., Ravina, R., & Dym, N. (2023). \"Neural injective functions for multisets, measures and graphs via a finite witness theorem.\" _Advances in Neural Information Processing Systems (NeurIPS)_ 37 (2023)\n\n\n> \u201cWe propose developing fine-grained expressivity results, namely metric equivalencies between explicit graph metrics and feature metrics for GNNs on graphs with features. An ideal result would derive a bi-Lipschitz correspondence between such metrics.\u201d\n> \n> \u2014 Christopher Morris, Nadav Dym, Haggai Maron, \u0130smail \u0130lkan Ceylan, Fabrizio Frasca, Ron Levie, Derek Lim, Michael Bronstein, Martin Grohe, and Stefanie Jegelka. \"Future Directions in Foundations of Graph Machine Learning.\" _International Conference on Machine Learning (ICML)_ (2024)\n\n**Question 1: Regarding point (2), is \"Multisets\" simply another term for \"discrete distributions\"?**\n\n_Multisets_ and _discrete distributions_ refer to different concepts. Multisets are essentially sets that account for repetitions. For instance, $\\\\{b,a,b\\\\} = \\\\{a,b,b\\\\} \\neq \\\\{a,b\\\\}$. Discrete distributions, on the other hand, are probability distributions with finite support. However, multisets can be idenitified with the subset of of discrete distributions with uniform weights, as discussed beginning at line 183.\n\n**Question 2: Could you clarify what \"E2\" refers to in lines 473-474?**\n\n$E_1$ and $E_2$ are two independent instances of the FSW embedding, with different input and output dimensions. $E_1$ maps distributions over $\\mathbb{R}^d$ into $\\mathbb{R}^{m_1}$, and $E_2$ maps distributions over $\\mathbb{R}^{m_1}$ into $\\mathbb{R}^{m_2}$, with $m_1$, $m_2$ being architecture hyperparameters. This is detailed in the manuscript (lines 481-485 in the revised version).\n\n**References:**\n\n[Chen] Chen, S., Wang, Y. \"Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions.\" _Advances in Neural Information Processing Systems (NeurIPS)_ 37 (2023).\n\n[FWT] Amir, T., Gortler, S., Avni, I., Ravina, R., & Dym, N. (2023). \"Neural injective functions for multisets, measures and graphs via a finite witness theorem.\" _Advances in Neural Information Processing Systems (NeurIPS)_ 37 (2023)."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer styv (Part 1 of 2)"
            },
            "comment": {
                "value": "**Response to summary:**\n\nWe would like to highlight that, in addition to the theoretical guarantees for our embedding, we present the impossibility result stated in Theorem 4.4. This result proves that it is impossible to embed discrete distributions into any finite-dimensional Euclidean space in a bi-Lipschitz manner. This saves the community further efforts in that direction, and essesntially shows that an embedding with substantially better analytical properties than the FSW does not exist.\n\n  \n\n**Weakness 1: Introducing baseling methods**\n\nThank you for your suggestion. We added to the revised manuscript a paragraph describing all the baseline methods (lines 498-504).\n\nWe would like to stress that the methods presented in Table 2 are those introduced in [Chen] and earlier papers, and were not implemented by us. Specifically: $\\mathcal{N}\\_{\\textup{ProductNet}}$, $\\mathcal{N}\\_{\\textup{SDeepSets}}$ and WPCE use their own architectures, and Sinkhorn is an approximation algorithm specifically designed to approximate the $p$-Wasserstein distance.\n\nOur architecture based on $E_1$, $E_2$, $\\Phi$, described in line 485,  achieves state of the art results with a simple combination of our embedding and one MLP. In comparison, $\\mathcal{N}{\\_\\textup{ProductNet}}$ produces inferior results using three MLPs.\n\nThe only method other than ours that we tested with our architecture was PSWE, which is designed to compute Sliced-Wasserstein preserving embeddings.\n\nLastly, the reason why Sinkhorn cannot be incorporated into our architecture is due to its own inherent limitation: it takes _pairs_ of input distributions and estimates their distances, rather than computing a distance-preserving embedding for individual distributions. This significantly limits its applicability to practical learning tasks, as we further discuss in our response to Reviewer Ew1o. We also added a brief explanation in line 307.\n\n**Weakness 2: Real-data application of our method**\n\nThe experiment presented in Table 2 illustrates the utility of our embedding in a learning task on real-world data (ModelNet-40). We note that our paper includes all experiments from the NeurIPS-accepted work by Chen and Wang [Chen], as well as theoretical results that in our opinion merit acceptance in their own right.\n\n_To be continued..._"
            }
        },
        {
            "title": {
                "value": "Response to Reviewer gmod"
            },
            "comment": {
                "value": "Thank you :)"
            }
        },
        {
            "title": {
                "value": "Response to Reviewer kJBi"
            },
            "comment": {
                "value": "We thank the reviewer for the comments. We have uploaded a revised version of the manuscript, where the comments have been addressed and incorporated.\n\n**Response to summary:**\n\nWe would like to highlight an additional theoretical contribution in our paper: the impossibility result of Theorem 4.4, which proves that it is impossible to embed discrete distributions into any finite-dimensional Euclidean space in a bi-Lipschitz manner. This saves the community further efforts in that direction, and essesntially shows that an embedding with substantially better analytical properties than the FSW does not exist.\n\n**Response to weaknesses:**\n\n1. **On the inclusion of $\\mathcal{W}_{\\infty}$:** We included the definition of the $p$-Wasserstein distance with $p = \\infty$ to allow our results to be stated across all $p \\in [0, \\infty]$. Our bi-Lipschitzness guarantee and impossibility result apply uniformly for all $p$ in this range.\n2. **Complexity of Wasserstein when $d=1$**: The complexity in this case is the computational complexity of the sort function, which is $\\mathcal{O}(n \\log n)$. This is stated in lines 223-224.\n3. **Definition of STD:** STD here is the Standard Deviation. We clarified this in the revised manuscript, l. 341-342.\n4. **Proof ideas in the main text:** We added an overview of the proof ideas of Theorems 4.1, 4.2 and 4.4 to the revised manuscript. Thank you for this comment.\n\n**Response to question on the practical motivation:**\n\nTo illustrate the advantage of our approach for practical applications, consider a learning task on multisets handled by traditional architectures based on sum- or max-pooling. With these methods, certain pairs of input multisets may appear numerically identical, meaning the architecture will not be able to distinguish between them\u2014even if they represent different underlying data. In contrast, our approach, due to its bi-Lipschitzness guarantee, can distinguish between these multisets in a way that reflects their actual differences. This practical advantage is evidenced in our experimental results shown in Table 2."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer Ew1o"
            },
            "comment": {
                "value": "There is a fundamental difference between our embedding approach and approaches such as Distributional Sliced Wasserstein and Max-Sliced Wasserstein. Our approach takes one input distribution at a time and computes an _embedding_, whereas the aforementioned approaches take two input distributions and estimate a _distance_ between them. Pairwise methods have two disadvantages in comparison with embeddings: (i) higher computational complexity when computing multiple pairwise disances, and (ii) limited applicability to real-world learning problems.\n\nIn terms of applicability, a pairwise method cannot be directly applied to common learning tasks, such as object classification, where the inputs are typically individual distributions. In contrast, an embedding is readily applicable to such tasks, as demonstrated in our experiments.\n\nIn terms of computation, pairwise methods to estimate sliced optimal transport distances typically require $\\tilde{\\mathcal{O}}(mnd)$ time (neglecting logarithmic factors), where $m$ is the number of slices, $n$ is the maximal number of support points, and $d$ is the ambient dimension of the support. Thus, computing all pairwise distances for a set of $k$ distributions would take $\\tilde{\\mathcal{O}}(k^2 mnd)$ time. In contrast, computing our embedding takes $\\tilde{\\mathcal{O}}(mnd)$ time for each input distribution, and pairwise distances can then be computed in the Euclidean space $\\mathbb{R}^m$, resulting in a considerably lower total complexity of $\\tilde{\\mathcal{O}}(k mnd + k^2 m)$. This approach is therefore significantly more scalable for large datasets where pairwise distance computations are required.\n\nWe appreciate the reviewer's comment and will clarify this in the paper."
            }
        },
        {
            "summary": {
                "value": "This paper considers Fourier slicing embedding both for a collection of probability distributions and multisets over $\\mathbb{R}^d$ and supported at $n$ points. The embedding consists of a projection sample on a 1-dimensional vector on the sphere then calculates a cosine transform of the projected quantile function. Under a specific probability distribution of the frequency, the authors prove that the expectation of the estimation error between the embedded measures is exactly the sliced Wasserstein distance. A second part of the theoretical results consists of proving the injectivity of the embedding under the assumption that the dimension embedding $m \\geq 2n(d+1) +1$. Numerical experiments are conducted on point cloud classification."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow. Proofs are rigorous. \n- Proposing the sliced embedding Wasserstein (SEW) through a cosine transform of the projected quantile function. Sampling the quantile function via cosine transform is novel.\n- Injectivity and bi-Lipschitz properties of FSEW on the collection of multisets.\n- Numerical experiments showcase better Wasserstein approximation on simulated datasets and three real datasets than NProductNet, WPCE, NSDeepSets, and Sinkhorn."
            },
            "weaknesses": {
                "value": "- Several approaches for the derivative of sliced Wasserstein distance like, distributional sliced Wasserstein (Nguen et al, ICLR'21), max-sliced Wasserstein, etc ... Could you highlight the difference between FSW and the SOTA derivative of sliced Wasserstein?"
            },
            "questions": {
                "value": "See Weaknes section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel approach to high-dimensional dataset embedding. The authors provided theoretical performance guarantees and numerical study to show the superior performance of the framework."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The theoretical contribution seems to be sound, with explicitly stated technical assumptions and results. Numerical study is solid."
            },
            "weaknesses": {
                "value": "1. The authors denovted much space to describe the p-wasserstein and infinity-type Wasserstein distance. Why it is necessary to introduce infinity-type Wasserstein distance?\n2. In line 222, the authors mentioned that in the special case of d=1, Wasserstien can be computed significantly fast. So what is the complexity rate?\n3. In line 344, what is the definition of STD???\n4. The authors should provide proof ideas for the main technical results in the main content."
            },
            "questions": {
                "value": "I am new to this field. Could the authors elaborate more on the practical motivation and applications of this approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper seeks to establish a mapping from multisets and measures over $ \\mathbb{R}^d $ into Euclidean space, ensuring that the sliced Wasserstein distance corresponds to the distance between their mappings in the target space. The authors propose a mapping that is bi-Lipschitz for multisets and injective for measures. Additionally, they demonstrate that a bi-Lipschitz map for measures does not exist."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is well-structured, and its message is clear. The proofs provided are rigorous and exceptionally clear. This particular problem is quite interesting. I really enjoyed reading the paper."
            },
            "weaknesses": {
                "value": "I don't see any weaknesses. Therefore, I recommend accepting it."
            },
            "questions": {
                "value": "I haven't any question."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "**Summary:**  \nThe paper introduces the \"Fourier Sliced Wasserstein (FSW) embedding\" for data in \\(\\mathbb{R}^d\\).\n\n**Theoretical Contributions:**  \n1. The authors prove that the embedding preserves or approximates the sliced Wasserstein distance.  \n2. They also demonstrate that the embedding technique is injective and bi-Lipschitz.\n\n**Numerical Experiments:**  \n1. The authors evaluate the approximation error of the proposed Fourier Sliced Wasserstein embedding.  \n2. They showcase an application of FSW for approximating the Wasserstein distance using a Multi-Layer Perceptron (MLP)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The combination of the Fourier/cosine transform and the sliced Wasserstein distance (see Eq. (6)) is a novel approach.\n2. Theoretical properties for this new technique with respect to the uniform distribution, along with its empirical approximation, are proposed (see Theorem 3.2, Corollary 3.3).\n3. Injectivity and bi-Lipschitz properties of the embedding have been investigated."
            },
            "weaknesses": {
                "value": "1. I recommend adding a section to introduce baseline methods. For example, explaining how Sinkhorn [Cuturi, 2013] can be used to train a neural network as a Wasserstein distance estimator. Currently, the experimental setup (E1, E2, Phi, Leaky-ReLU) appears tailored only to the proposed method in this paper.\n2. It would be beneficial to introduce a real-data application of the proposed Sliced Wasserstein distance embedding technique to illustrate its practical utility.\n3. I\u2019m unclear on why 'bi-Lipschitz' is considered a crucial property. Could you provide an example to clarify? For instance, in which applications would the lack of a bi-Lipschitz property cause issues, and where having this property could offer distinct advantages?"
            },
            "questions": {
                "value": "1. Regarding point (2), is \"Multisets\" simply another term for \"discrete distributions\"?  \n2. Could you clarify what \"E2\" refers to in lines 473-474?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}