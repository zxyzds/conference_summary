{
    "id": "T2d0geb6y0",
    "title": "Fundamental Limitations on Subquadratic Alternatives to Transformers",
    "abstract": "The Transformer architecture is widely deployed in many popular and impactful Large Language Models. At its core is the attention mechanism for calculating correlations between pairs of tokens. Performing an attention computation takes quadratic time in the input size, and had become the time bottleneck for transformer operations. In order to circumvent this, researchers have used a variety of approaches, including designing heuristic algorithms for performing attention computations faster, and proposing alternatives to the attention mechanism which can be computed more quickly. For instance, state space models  such as Mamba were designed to replace attention with an almost linear time alternative.\n\nIn this paper, we prove that any such approach cannot perform important tasks that Transformer is able to perform (assuming a popular conjecture from fine-grained complexity theory). We focus on document similarity tasks, where one is given as input many documents and would like to find a pair which is (approximately) the most similar. We prove that Transformer is able to perform this task, and we prove that this task cannot be performed in truly subquadratic time by any algorithm. Thus, any model which can be evaluated in subquadratic time \u2013 whether because of subquadratic-time heuristics for attention, faster attention replacements like Mamba, or any other reason \u2013 cannot perform this task. In other words, in order to perform tasks that (implicitly or explicitly) involve document similarity, one may as well use Transformer and cannot avoid its quadratic running time.",
    "keywords": [
        "Large Language Models",
        "Transformers",
        "Fine-grained complexity theory",
        "Document similarity",
        "Hardness of Approximation",
        "Fast attention computation"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=T2d0geb6y0",
    "pdf_link": "https://openreview.net/pdf?id=T2d0geb6y0",
    "comments": [
        {
            "title": {
                "value": "Rebuttal by Authors"
            },
            "comment": {
                "value": "We thank the reviewer for the thoughtful comments and questions. \n\n**Approximation results:** In addition to proving the hardness of exact document similarity, we also proved in this paper the hardness of approximate document similarity. We proved that approximation versions of MSD, LSD (which we denoted by $\\gamma$-MSD, $\\gamma$-LSD) also require quadratic time in our main theorems (Theorem 1.1 and 1.2). Here, approximation means we only need to find one pair of documents whose distance is at most $\\gamma$ times the distance of the optimal pair for LSD and at least $1/\\gamma$ times the distance of the optimal pair for MSD. As a result, they are also hard for subquadratic models, and this is why we refer to approximation versions throughout the abstract and introduction.\n\n**SETH and approximation:** Indeed, SETH conjectures the hardness of an exact problem, but it has since been used many times in the literature to prove hardness of approximation problems as well. For example, we cite the papers [1,2,3] which all proved hardness of approximation problems, including closest pair, Max-IP etc, from SETH. We used these results to prove our own hardness of approximate document similarity.\n\n**Bag-of-Words embedding:** As mentioned above, we also show hardness of approximation, which could mitigate the impact of an embedding like bag-of-words not completely capturing the document. That said, our results hold for many other embeddings. For example, they apply almost directly to TF-IDF without needing to modify the proofs much.\n\n**Simple reductions:** You\u2019re right that many of our reductions are fairly simple, but we view this as a merit: it means that the practical hardness of problems like SAT, OV, etc translate directly to giving practical hardness here as well. Complicated reductions could otherwise make instances become impractical, but we avoid that here.\n\n[1] C. S. Karthik and Pasin Manurangsi. On closest pair in euclidean metric: Monochromatic is as\nhard as bichromatic. Combinatorica, 40(4):539\u2013573, aug 2020. ISSN 0209-9683. doi: 10.1007/\ns00493-019-4113-1. \n\n[2] Lijie Chen. On the hardness of approximate and exact (bichromatic) maximum inner product. Theory Of Computing, 16(4):1\u201350, 2020.\n\n[3] Aviad Rubinstein. Hardness of approximate nearest neighbor search. In Proceedings of the 50th\nAnnual ACM SIGACT Symposium on Theory of Computing, STOC 2018, pp. 1260\u20131268, New\nYork, NY, USA, 2018. Association for Computing Machinery. ISBN 9781450355599. doi: 10.\n1145/3188745.3188916."
            }
        },
        {
            "title": {
                "value": "Rebuttal by Authors"
            },
            "comment": {
                "value": "We thank the reviewer for the insightful comments.\n\n**Practical impacts:** Indeed, most of our hardness results come from simple reductions. For example, if we have an algorithm for LSD that runs in time $T$, then that gives us an algorithm for OV that runs in time at most $2T$. Thus, the practical hardness of problems like SAT and OV carries over to yield practical hardness here as well. We will expand on this in the final version.\n\n**Performance of other models solving OV:** Indeed, it could be interesting to see how to pick the depth for certain subquadratic models like SSMs to solve document similarity tasks. Since our results show that these tasks need quadratic time, one would need to pick a prohibitively large depth for larger $n$ which scales linearly with $n$ (to get a final quadratic running time).\n\n**Broader range of tasks:** We aimed to study a wide variety of similarity search and document similarity problems in this paper (exact vs approximate, monochromatic vs bichromatic, closest vs farthest, optimization vs decision, etc), which appear (either explicitly or implicitly) in many different tasks that language models are intended to solve. That said, we fully agree that finding more examples and other types of tasks would be exciting.\n\n**Other writing suggestions:** Thank you, we agree with all the suggestions and corrections you made and will update them in the final version. In particular, for line 528 in the proof of Theorem 4.1, you\u2019re right that we are missing the $\\ell_1$ norm of $v_j$ in the summation, although it does not impact the correctness of the proof since it is upper bounded by $d$, which is much smaller than the multiplicative gap of $\\sqrt{n}$."
            }
        },
        {
            "title": {
                "value": "Rebuttal by Authors"
            },
            "comment": {
                "value": "We thank the reviewer for the thoughtful review.\n\n**Empirical interpretation:** Indeed, as the reviewer explains, the papers introducing subquadratic alternatives typically justify them with experiments showing that they are empirically fast and accurate. The experiments differ in details in different papers, but roughly, they show that their new model runs faster than transformer and achieves nearly the same accuracy, typically slightly worse. (See, for instance, figure 3 in [2], figure 2 of [3]) One may wonder whether it is possible to design a subquadratic model that actually achieves the same accuracy, or whether an accuracy loss is necessary. Our results show that, for document similarity tasks, it is not possible for any subquadratic model to achieve the same accuracy as transformer, and so these accuracy losses observed in all these experimental papers must necessarily appear for document similarity tasks. In other words, our paper is giving a theoretical explanation for exactly the empirical phenomenon that the reviewer is mentioning.\n\n**Other limitations on transformers:** Indeed, in addition to [1], there has been much work studying problems that transformer is unable to solve. For instance, the papers [4] and [5] which we reference in our paper prove that transformer cannot solve problems based on triple-wise correlations or on tasks that are hard to parallelize. More in the spirit of our paper, one can also take problems which require more than quadratic time (such as problems about finding correlations in tensors that require cubic time), and these also cannot be solved by transformer.\n\nThis is why it is important that we also prove transformer can solve the document similarity tasks we discuss in this paper. We are highlighting a class of problems that transformer can solve, but subquadratic models cannot, thus showing subquadratic models must be worse than transformer at these tasks.\n\n**Ranking or gaps with subquadratic alternatives:** Our results don\u2019t show how to rank subquadratic alternatives, they only show that these alternatives cannot solve document similarity problems. Finding problems that some variants can solve and others cannot would give an interesting way to rank these alternatives, although we note that one would need to focus on problems that can be solved in linear time in order to rank alternatives which run in linear time. Using such problems to quantify the gaps between different models could also be interesting future work.\n\n**Approximation algorithms:** Unfortunately, approximating the minimum inner product is just as hard as computing it exactly. See Theorem 3.1 in our paper where we prove this. (We prove it for approximate LSD instead of approximate Min-IP, but the essentially identical proof works for approximate Min-IP as well.)\n\n\n[2] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. CoRR, abs/2006.04768, 2020. \n\n[3] Praneeth Kacham, Vahab Mirrokni, and Peilin Zhong. Polysketchformer: Fast transformers\nvia sketching polynomial kernels. In Forty-first International Conference on Machine Learn-\ning, ICML 2024, Vienna, Austria, July 21-27, 2024. \n\n[4] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Transformers, parallel computation, and logarithmic depth. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 43276\u201343327. PMLR, 21\u201327 Jul 2024a. \n\n[5] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations of transformers. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS \u201923, Red Hook, NY, USA, 2024b. Curran Associates Inc."
            }
        },
        {
            "title": {
                "value": "Rebuttal by Authors"
            },
            "comment": {
                "value": "We thank the reviewer for the thoughtful review. \n\n**Empirical evidence:** We agree that experiments showing particular models failing to solve document similarity tasks could be interesting in future work. However, we want to emphasize that our results, which mathematically prove that subquadratic models cannot solve document similarity, are more general than any such experiment. For instance, an experiment showing that one particular model cannot solve document similarity may not be very convincing, since one could imagine slightly changing the model to get around the issue. Our results show that it is mathematically impossible to do any such modification without using quadratic time.\n\n**Practical applications on recommendation systems:** Indeed, our work is directed related to the maximum inner product search (MIPS) problem in recommendation systems. In the MIPS problem we are given a dataset $P$ of $n$ points and a query $q$ such that the goal is to return a data point $p \\in P$ such that $\\langle p,q\\rangle$ is the largest. In recommendation systems, $P$ could be a set of items and $q$ could be a customer, such that each time a customer comes, we want to know which item the customer likes the most. This problem is essentially equivalent to the Max-IP problem we study and prove lower bounds with in this paper. We will add another paragraph on this application in later versions.\n\n**Varying sequence length and model parameters:** In general, increasing the model parameters in terms of $n$ would make the model more powerful. However, our results show the hardness of these document similarity problems, which are defined independently of what the model parameters are (see our footnote 2 on page 4). So, one would at least need to very substantially increase the parameters, to a regime where the running time becomes quadratic, in order to overcome our lower bound."
            }
        },
        {
            "summary": {
                "value": "This paper introduces a theoretical perspective on the ability of transformers to solve document similarity tasks. Specifically, by relying on the SETH conjecture, the paper connects complexity theory with transformers, and shows that subquadractic models cannot solve a class of problems that are solvable by transformers, which are quadractic."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Although this is not my area of expertise, I believe the paper's main findings, particularly regarding document similarity tasks, have the potential to impact the literature on architectural development. Additionally, to the best of my knowledge, the paper is well-written, the math is accurate and concise."
            },
            "weaknesses": {
                "value": "While the theoretical coverage seems accurate to me, I was disappointed by the lack of empirical evidence supporting the paper's main findings. For instance, experiments demonstrating that subquadratic models cannot solve Max-IP, Min-IP, MSD, and LSD, whereas transformers can, would have been valuable."
            },
            "questions": {
                "value": "For MSD and LSD, how relevant are your findings in practical applications, such as recommendation systems? Could varying values of sequence length $n$ and model parameters lead to different practical conclusions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 1
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies a fundamental issue whether any subquadratic approximations to the transformers' quadratic attention mechanisms can solve perfectly the minimum inner product problem (or its variants (such as the maximum inner product one)) among a set of binary vectors. Built on results about the inherent complexity of such problems, the authors show that such problems can not be solved by any subquadratic approximations due to the complexity mismatch. They also show the problems can be solved by a transformer using the quadratic attention mechanisms by constructing such a network."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The results are theoretical, demonstrating a fundamental limitation of any subquadratic approximations of the transformers' quadratic attention mechanisms. The involved steps seem sound."
            },
            "weaknesses": {
                "value": "I believe the results, while rigorous and sound, have almost no connection with the transformers being used in practice. Generally speaking, the transformers are shown to be very capable of solving different kinds of problems empirically. For example, subquadratic approximations try to show that they can perform similarly to the original transformers but more efficiently, which is orthogonal to the results in the paper. Due to the nature of the results, there are no experimental results. But the variants of the transformers being used are mainly justified via results. Furthermore, it is known that at least some of the transformers (such as the decoder-only ones) can not solve counting and copying problems [1] perfectly and the additional impact of the results in the paper to the research on transformers may be very limited. \n\n[1] Federico Barbero, Andrea Banino, Steven Kapturowski, Dharshan Kumaran, Jo\u02dcao GM Ara\u00b4ujo, Alex Vitvitskyi, Razvan Pascanu, and Petar Veli\u02c7ckovi\u00b4c. Transformers need glasses! information oversquashing in language tasks. arXiv preprint arXiv:2406.04267, 2024 (also in NeurIPS 2024)."
            },
            "questions": {
                "value": "1. Can the results be enhanced to rank different subquadratic approximations?\n2. Similarly, can the results be enhanced to quantify the gap between the quadratic transformers and the subquadratic approximations?\n3. Are there approximate algorithms for solving the minimum inner product problem or its variants and how would these relate to the subquadratic approximations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Attention is a computationally bottleneck of the prominent transformer models used for language modelling. The classic variant is in quadratic (on the input-dimension) complexity class, which is why sub-quadratic variants such as state-space models emerged, to enable e.g. longer document processing.\nThe authors claim that subquadratic alternatives to transformers face inherent limitations in performing specific NLP tasks, namely document similarity. Its contributions are theoretically based on complexity theory; namely, 1) the prominent SETH problam cannot be solved sub-quadratic, and 2) a constructed transformer can solve SETH."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "Finally some more mathematically theoretically founded analysis of the prominent architecture and its limitations.\nIt analysis seems rigorous and its offering insights that can influence future research on alternative architectures in NLP."
            },
            "weaknesses": {
                "value": "minors/ missing discussions:\n\n1.1) The paper would benefit from discussing the practical impacts of its findings. I.p. the bounds of its proof seem to be quite practically relevant and not 'entirely asymptotic' - in praxis it could be more relevant to have 'bad asymptotic with good bounds'.\n\n1.2) It would be beneficial to explore the performance of alternative architectures i.p. w.r.t. practicability - i am not sure if a state space machine can't handle OVC in a reasonable depth like 10 layers. \n\n1.3) it would be beneficial to discuss a broader range of tasks\n\nmediocores:\n\n2.1) i find the main paper pretty hard to read and would advice a bit of restructering i.p.\n- abstract line 22ff are quite redundant\n- intro line 132ff seem pretty random/ not needed/ more distractive\n- repetitively bichromatic versions are mentioned but not required in the core eg 2.2.1, 2.2.2, 2.3 -> i would love to see that in a final/following discussion sections once. Similarly Min/Max-IP are pretty confusing as not required for the main result -> discussion afterwards\n- you define in 2.3 MSD, but actually require LSD for the core proof. that is one part that actually should be written explicitly twice (or the other version) :)\n\n2.2) the core result is 4. it, and its impact should be discussed further, in particular recall what sub-quadratic methods suffer from.\nthe proof could use a rewrite. i.p.: 4.1. is a transformer from your definition (not just attention~...). i suffer a bit to understand the step 529: A_{Q,K,V}(X) IS NOT line 528, but it is upperbounded by it which is all you need (?) you miss the factors XV = in this construction #ones in v_i.. or did i miss something?"
            },
            "questions": {
                "value": "- 259: finds\n- 310: multi-layer\n- 434 write the numbers explicitly, 'very large' is random here\n- 534 left side v_i^*  (the star)\n- line 529 as mentioned above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Paper is well written and brings interesting results from complexity theory to the deep learning community. It describes theoretical  limitations of sub-quadratic attention architectures for similarity and related problems. Paper shows that MSD, LSD and their variants require quadratic time assuming SETH and therefore any subquadratic alternatives to transformers are not able to solve them due to computational constraints. Also the paper explicitly constructs one head / one layer transformer which is able to solve such a problem, therefore making clear an important difference between standard attention and subquadratic alternatives."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. General results on limitations on sub-quadratic  architecture or heuristic for most similar, least similar and related problems. Proved that accuracy loss for any task relating to document similarity is unavoidable for any sub-quadratic approach.\n2. Shown that one head / one layer transformer can solve those tasks.  This includes explicit transformer construction.\n3. Bringing complexity results (Strong Exponential Time Hypothesis) to transformer architectures."
            },
            "weaknesses": {
                "value": "1. It is not exactly clear when the paper talks about approximate or exact results.\n2. It relies on the BOW (bag of words) model, which already introduces approximation in practical settings. This makes results (which are about exact solution) weaker.\n3. Considering that finding closest pairs in Euclidean or Manhattan distance both require quadratic time (assuming SETH) is well know, results for MSD, LSD are fairly incremental."
            },
            "questions": {
                "value": "> We focus on document similarity tasks, where one is given\nas input many documents and would like to find a pair which is (approximately)\nthe most similar.\n\nCould you please elaborate, why 'approximately' mentioned here? Is it not SETH and derived conjectures about exact results only?\n\n> In other words, in order to perform tasks that (implicitly or explicitly) involve document similarity, one may as well use Transformer and cannot avoid its quadratic running time.\n\nApproximate similarity search involves tradeoff precision and run time. If we talk about the exact solution of the similarity search, it would be nice to make it explicit.\n\n> bag-of-words embeddings\nHow important is usage of bag of words embedding?  Would it be possible to use embedding / technique which takes into account word ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}