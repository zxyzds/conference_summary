{
    "id": "0h6v4SpLCY",
    "title": "Universal generalization guarantees for Wasserstein distributionally robust models",
    "abstract": "Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. Recent statistical analyses have proved that generalization guarantees of robust models based on the Wasserstein distance have generalization guarantees that do not suffer from the curse of dimensionality. However, these results are either approximate, obtained in specific cases, or based on assumptions difficult to verify in practice. In contrast, we establish exact generalization guarantees that cover a wide range of cases, with arbitrary transport costs and parametric loss functions, including deep learning objectives with nonsmooth activations. We complete our analysis with an excess bound on the robust objective and an extension to Wasserstein robust models with entropic regularizations.",
    "keywords": [
        "generalization guarantees",
        "optimal transport",
        "distributionally robust optimization",
        "nonsmooth analysis"
    ],
    "primary_area": "optimization",
    "TLDR": "Exact generalization guarantees for Wasserstein distributionally robust models with dimension-free sample rates.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=0h6v4SpLCY",
    "pdf_link": "https://openreview.net/pdf?id=0h6v4SpLCY",
    "comments": [
        {
            "summary": {
                "value": "This paper presents novel bounds on for the DRO loss using the Wasserstein distance. In particular, they address the question of finding the minimal $\\rho$ used by the empirical robust loss such that the loss is an upper bound for the actual population loss. The main challenge is to overcome the dependency on the distance between W(P_n, P) \\sim n^{1/d}. While this problem has been studied in the literature, and dimension free bounds exist, this paper presents a proof requiring weaker assumptions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper addresses an important problem is generalization bounds/theoretic ML. In particular:\n\n- The paper is well written and the results are nicely presented. \n- The proof sketch in Section 4 is excellent. It is very easy to follow and often neglected in these types of papers\n- The proof idea is smart, non-trivial and interesting."
            },
            "weaknesses": {
                "value": "Given that this is a more traditional field, I would expect a clearer comparison with the existing works. While the authors do a very good job in presenting the proof idea, it is not so clear how the proof fundamentally differs from existing works."
            },
            "questions": {
                "value": "I am happy to increase my score and support this paper with a high confidence if the authors can provide an extensive discussion during the rebuttal on the assumptions in Azizian et al. (2023a) . In particular, my two major questions are: can the authors be more precise in which cases their assumptions are weaker than the ones in  Azizian et al. (2023a). In particular, can you give an example for a class of distributions that are covered by this paper but not by  Azizian et al. (2023a)? Moreover, can the authors explain why the proof in  Azizian et al. (2023a) breaks for your assumptions and why it is not trivial to extend the proof?\n\n\n\nSmaller question:\nIsn't assumption 3.1 (1) always true satisfied by w<=1. Is it possible that this is a typo?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "-"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides exact generalization guarantees for Wasserstein Distributionally Robust Optimization (WDRO) for a wide variety of models with compactness and finite Dudley's entropy assumptions. The results apply to radius $\\rho$ scaling as $O(1/\\sqrt{n})$, which does not suffer from the curse of dimensionality. The results also cover the double regularization case."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The generalization guarantees of this work do not rely on restrictive assumptions like smoothness compared to the previous work (Azizian et al. 2023a). \n- This paper is well-structured, and the theoretical results and proof sketches are clearly presented."
            },
            "weaknesses": {
                "value": "- In Section 3.2, the authors discussed how their results on generalization guarantees apply to linear regression and logistic regression. However, more complicated models such as neural networks with ReLU or other smooth activation functions (e.g. GELU) are not discussed. \n- The theoretical results require a lower bound on $n$, while Theorem 3.4 of Azizian et al. (2023a) applies to all $n \\ge 1$. The implications of this requirement should be discussed."
            },
            "questions": {
                "value": "- What are the practical implications of the generalization guarantees compared to Azizian et al. (2023a)? Can you provide some numerical results analogous to Appendix H of Azizian et al. (2023a)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The proposed paper provides lower bounds on the robust empirical risk under unorthodox but interesting scaling limits on the radius of the Wasserstein ball around the empirical risk.  The paper uses some cool techniques which are not often seen in machine learning.  \n\nThe paper is relatively clearly written.  However, I think there are a few little things here and there which are either difficult to justify (in the current form) or perhaps not well-defined (see below). Also, the introduction is excessively general while the setting rapidly collapses to a much more specific setting shortly after."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is well written, interesting, and theoretical and provides very nice lower bounds on the robust empirical risk.  The results are nice, and so is the use of set-valued analysis to derive them.  Several relevant examples are considered, making a large portion of how these results can be used nearly transparent."
            },
            "weaknesses": {
                "value": "Nevertheless,  I think some of the assumptions are a bit opaque (see below), and I'm not certain some quantities are well-defined.\n\n*Minor*\n- Citing Cuturi and Per\u00e9e's book is odd when mentioning the Wasserstein distance.  Perhaps the original source or a book on optimal transport such as Villani's book, would be more natural, IMO.\n\n- The definition of Wasserstein distance circa (1) is incorrect, $\\Xi$ must be Polish, and  c *must* be a power $p\\in [1,\\infty)$ of a *metric* topologizing $\\Xi$; what you write is just some transport problem.  Eg if c is not symmetric, then $W_c$ is not a metric in general, or if $c(x,y)=0$ for all $x,y$ then $W_c$ cannot separate points.\n\n- Perhaps \"suitable\" distributions is more appropriate before (1), since the distance explodes if these have no finite moment. \n\n- Line 65: bad grammar: \"it does not introduce approximate term\" also imprecise.\n\n- Line 66: Is Wainright's book and Boucheron the best reference?  Perhaps older papers, e.g. on VC dimension, Bartlett's old papers on Rademacher complexity, or old papers on chaining are more natural references?\n\n- Line 69: \"This theoretical feature is specific to WDRO and highlights its potential to give more resilient models.\" This can be **much** less hand-wavy.  Please explain more precisely/mathematically.\n\n- Line 149: In a metric space $(x,..)$ not \"In (X,..) a metric space\".\n\n- Assumption 1 vs. Line 145: You say that $\\Xi$ is just a measurable space, then later you say its a compact metric space.  Why no   be forthright and say its a metric space on line 145.  Similarly, why is $\\mathcal{F}$ an arbitrary family of functions, then straightaway after is actually a compact set of continuous functions.\n\n- Line 176: Why jointly Lipschitz?  If $\\Theta$ is compact, then since you already assumed $\\Xi$ is compact, then it is enough for $\\Theta\\times\\Xi\\ni (\\theta,\\xi)\\mapsto f(\\theta,\\xi)\\in \\mathbb{R}$ to be continuous; to deduce the compactness of $\\{f(\\theta,\\cdot):\\,\\theta \\in \\Theta\\}$ by the currying Lemma.  \n\n- Line 176: Not sure why you say \"if $\\Xi$ is compact, since this was assumed a few lines earlier on the same page.\n\n- Maybe more natural examples come from Arzela-Ascoli...\n\n- Should the definition of the Dudley entropy integral really be in a footnote, while more basic ideas are in the main text.\n\n- Line 221: The words \"the metric\" are missing.\n\n- Line 223: There are many more references of the use of this type of metric, especially in exponential convergence rate results for Markov chains (wrt $W_1$ over countable metric spaces with this distance).\n\n- Line 245: \"sample randomness\" (I know what you mean...but the word independent is misleading as this \n\n- Assumption 1: Why call (2) jointly continuous, it is just standard continuity (actually inform continuity by compactness)."
            },
            "questions": {
                "value": "- Why not submit to JMLR?  The paper is very rigorous and rather long and technical for an ML conference?  You also examine the problem in good detail.\n\n- Could you provide a simple example in Theorem 3.2, where the optimal coupling is known under (say) Gaussianity assumptions?\n\n- I'm a bit confused.  What does $\\operatorname{argmax}_{\\Xi}\\,f$ mean in (5) a sup norm or something?  \n\n- Why is $\\min\\{ c(\\xi,\\zeta): ... \\}$ measurable?  In particular, (independent of the meaning of the argmax, above question), why is there a measurable selection $\\xi\\mapsto \\zeta$?  Without this, its not clear that $\\rho_{\\operatorname{crit}}$ is well defined.  I'm guessing this is Berge's theorem (which is in Aliprantis & Border) somehow, but please spell it out for us :)\n\n- Each result assumes that the (difficult to interpret) $\\rho_{\\operatorname{crit}}$ is \"large enough\".  Can you please provide a general set of conditions ensuring that $\\rho_{\\operatorname{crit}}$ can be bounded away from $0$. \n\n- Is it fair to compare, verbally, our results to those of Fournier et al. (and similar bounds, say, found in [1])?  Since you are considering a small ball around the empirical measure while their results guarantee a minimal radius such that the empirical measure contains the true measure whp.   Furthermore, those rates are only tight (afaik) when the measure is very spread out; more precisely, it is Alhors $d$-regular, see e.g. [3] for a nice clean proof.  \n\n- In theorem 3.2, why is $\\pi^{P,Q}\\ll \\pi_0$?  To be this isn't directly evident... I.e.\\ why is the RHS not trivially $-\\infty$ in general?\n\n\n\n[1] Graf, Siegfried, and Harald Luschgy. Foundations of quantization for probability distributions. Springer Science & Business Media, 2000.\n[2] Otto, Felix, and C\u00e9dric Villani. \"Generalization of an inequality by Talagrand and links with the logarithmic Sobolev inequality.\" Journal of Functional Analysis 173.2 (2000): 361-400.\n[3] Kloeckner, Benoit. \"Approximation by finitely supported measures.\" ESAIM: Control, Optimisation and Calculus of Variations 18.2 (2012): 343-359."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}