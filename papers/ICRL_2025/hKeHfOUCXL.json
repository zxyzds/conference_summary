{
    "id": "hKeHfOUCXL",
    "title": "Benchmarks and Custom Package for Energy Forecasting",
    "abstract": "Energy (load, wind, photovoltaic) forecasting is significant in the power industry as it can provide a reference for subsequent tasks such as power grid dispatch, thus bringing huge economic benefits. However, there are many differences between energy forecasting and traditional time series forecasting. On the one hand, traditional time series mainly focus on capturing characteristics like trends and cycles. In contrast, the energy series is largely influenced by many external factors, such as meteorological and calendar variables. On the other hand, energy forecasting aims to minimize the cost of subsequent tasks such as power grid dispatch, rather than simply pursuing prediction accuracy. In addition, the scale of energy data can also significantly impact the predicted results. In this paper, we collected large-scale load datasets and released a new renewable energy dataset that contains both station-level and region-level renewable generation data with meteorological data. For load data, we also included load domain-specific feature engineering and provided a method to customize the loss function and link the forecasting error to requirements related to subsequent tasks (such as power grid dispatching costs), integrating it into our forecasting framework. Based on such a situation, we conducted extensive experiments with 21 forecasting methods in these energy datasets at different levels under 11 evaluation metrics, providing a comprehensive reference for researchers to compare different energy forecasting models.",
    "keywords": [
        "Energy Forecasting.+Benchmark.+Dataset"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=hKeHfOUCXL",
    "pdf_link": "https://openreview.net/pdf?id=hKeHfOUCXL",
    "comments": [
        {
            "summary": {
                "value": "This paper is on an open source package for benchamarking energy-forecasting time-series models. Authors have integrated a framework which can handle and showed performance of widely varying 11 energy public datasets and integrated both SOTA deep-learning models(mostly time-series forecasting) and other machine learning models. The framework can handle missing data features, and various forecasting metrics and point-based and probabilistic loss. The framework looks similar idea to open graph benchmark except on energy forecasting time-series."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "An important and impactful application\n- framework can handle feature engineering, missing data, common pre-processing pipeline\n- can be customized to integrate more models and loss functions. designed to handle custom loss functions.\n- multiple evaluation metrics, loss, public datasets have been integrated."
            },
            "weaknesses": {
                "value": "The work is a very timely application for energy forecasting. The reviewer has some concern regarding the results and reproducability on new datasets and integration of new models.\n\nComments:\n1. Are the model features been handled as extrinsic and intrinsic features?, e.g. Weekday vs weekend, #occupancies, static features\n\n2. If the framework code can follow OOP format to aggregate all models, would be more convenient. I was looking for the base model in the repository, could not find it. Not sure if OoP is integrated. \n- E.g., MyQuantileModel can be used as base model and all different deep-learning models can inherit the base model, forward traning can be same as the base model or can be overridden. \n- Similar thing can be done for loss functions.\n\n3. Is there a way to customize hyperparameters like time-lag, forecast horizon,  \n\n4. A json config file for chyperparameters, and parameters selction for model and datasets would be a useful integration.\n5. Can this repo be usable and reproducable for evaluating other time-series datasets? What things to be fixed for such cases?\n\n6. pinball Loss are extremely high, which is unususal. \n\t- How are the train and test datasets selected, is the test on unseen domain or same domain data?\n\t- how are the training data for building-level and aggregated level? Seems building level data is not multivariate forecasting. Are each building data trained through different models? Would be easier if authors can clarify on train and test dataset selection?\n\t- Can you show the forecasted vs ground-truth comaprison plot for different models?\n\n7. The repository dont seem to include result plots for visualizing training and test forecast vs ground-truth comparison plots, etc. These plots are also missing in the paper. \n\n8. As the framework is designed to handle both building level and aggregated performance, showing some analysis plots on the performance of building vs aggregation would be convincing to understand the models capability."
            },
            "questions": {
                "value": "Please see the comments in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper seem to have used all public datasets."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces an open-source package and benchmark specifically designed for energy forecasting, addressing the challenges in the field such as the significant impact of external factors like temperature and calendar variables. The authors highlight the differences between energy forecasting and general time series forecasting, emphasizing the need for targeted feature engineering and custom loss functions that consider the costs associated with power grid dispatch. They provide a modular framework that includes data pre-processing, feature engineering, forecasting methods, post-processing, and evaluation metrics. Additionally, they release a high-quality renewable energy dataset with corresponding meteorological data to facilitate research in this area."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "In this work, the authors develop an open-source forecasting package for energy forecasting. They also provide a dataset that includes different types of renewable energy generation and key meteorological factors."
            },
            "weaknesses": {
                "value": "While the authors claim that energy forecasting is different from general time series forecasting, they do not provide a detailed explanation of these differences. \n\nThe feature engineering techniques are heavily centered on temperature and calendar variables. Other external factors that might affect energy forecasting, such as economic indicators or unexpected events (e.g., pandemics), are not extensively explored. \n\nThe effectiveness of the proposed temperature-calendar feature engineering and custom loss function may be dataset-specific. \n\nThe custom loss function is derived from simulations using an IEEE 30-bus test system. Due to its simplified structure and limited scale, this system may not fully capture the complexity and variability of real-world power networks, limiting the generalizability of the results to more complex systems.\n\nWhile the feature engineering strategy is well-motivated, the paper could provide more justification for its generalization across different datasets and energy types. \n\nThe overall clarity of writing needs improvement."
            },
            "questions": {
                "value": "Please see the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose a novel renewable electricity generation dataset that can be utilized in various time series forecasting tasks. This dataset is embedded in a Git Repository, along with other datasets and accompanied by several pre- and post-processing techniques, metrics, point and probabilistic forecasting models, and feature engineering methods. \n\nThe authors used this dataset to benchmark different forecasting models and test their proposed loss function, aiming to train models not only for optimal accuracy but also to comply with specific tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Provision of a Renewable Energy dataset.\n- Important collection of Energy datasets.\n- Combination of various techniques, models, and metrics."
            },
            "weaknesses": {
                "value": "- Clarity could be improved to better understand the different parts/experiments of the proposal.\n- Paper flow could be enhanced.\n- Lack of details for some parts, even with the appendix.\n- Needs thorough proofreading.\n- Missing important state-of-the-art (SOTA) Time Series Forecasting baselines."
            },
            "questions": {
                "value": "## Goal\nWhat is the objective of this package?\n- To gather datasets and models related to \u201cEnergy (load, wind, photovoltaic)\u201d: However, why not include datasets like the London smart meter datasets or NREL Solar energy, or other existing datasets? Additionally, why not include the latest SOTA models? Such choices might prevent future users to select your repository.\n- To provide datasets that have both energy and weather data: Yet why include datasets with only energy data (ELD and ELF)?\n- To introduce a specific loss including subsequent tasks: But without real \u201ctasks\u201d in the provided dataset, how can this loss be effectively tested? Only the simulation is not sufficient.\nFurthermore, what about the negative log-likelihood (NLL) loss that provides uncertainty (or probabilistic prediction) to point forecast models? Models using architecture based on diffusion or traditional models minimizing the NLL can provide the uncertainty of a given prediction (range of values for each time step). Isn\u2019t this the solution to account for any potential task? Why would authors loss should be preferred compared to these options?\n\n## Questions\n\n### Q1\nI agree that energy forecasting requires external factors' information to achieve better performance. However, having the information in the input is one thing; the model can learn the relation between the external factor and the target. But then comes the availability for the prediction window. Does the model have to predict the external factors along with the energy forecasting, or does the model receive the forecast of these external factors? For the latter, are the forecasts reliable? If there is a mistake in the forecast, there will be a mistake in the prediction. How is this taken into consideration in the metrics when it comes to benchmarking models?\n\n> We will input the renewable energy sequence and meteorological factors of the past 24 hours, as well as the meteorological factors of the next 24 hours.\n\nThis sentence implies that your scenario has a perfect forecast of future weather, which is highly unfeasible in real scenarios. Noise should have been included to account for errors in weather forecasts, if authors want to provide a package that is close to real case scenarios.\n\n### Q2\n> [\u2026] ensuring minimum data information distortion.\n\nDo we have a guarantee on that?\n\n\n## Experiment Setting\n\n### E1\n> It is beneficial for the community to study the relationship between different renewable energy types under similar climate conditions in a certain area.\n\nWhy did the authors not demonstrate such an advantage in the experiment section, with a multivariate-to-multivariate scenario of different types of renewables?\n\n### E2\nThere is a lack of information on the experiment setting. Are we in a Univariate-to-univariate, multivariate-to-univariate, or multivariate-to-multivariate forecasting? This uncertainty poses reproducibility problems. Figure 3 and Table 3 results would seem to be U2U, but it needs to be confirmed for each experiment. Authors should clearly state the forecasting task of each experiment.\n\n### E3\nWhat does MQ[model_name] stand for in Figure 3?\n\n### E4\nWhat does $123 456 789$ in Table 4's first columns stand for?\n\n## Feature Engineering\n\n### F1\nHow will feature engineering based on calendar info, such as workdays, benefit REF? Are there options for tailoring the engineered features according to the user's needs and datasets capabilities? Or it only provides the combination described in Section 3.2.1? Which definitely limit the usage of such features.\n\n### F2\nHow are engineered features integrated into transformer-based models (informer, etc.)? Are they considered as channels? The usage of such features limits the forecasting task to univariate-to-univariate forecasting, which impedes the usage of this framework. Such features should have been better integrated into the models for easier usage.\n\n## Dataset\n\n### D1\n> The UCI dataset is a power load dataset from the UCI database, [\u2026]\n\nNot enough details. I assume it is the Electricity Load Diagrams, and it should be clearly mentioned.\n\n### D2\nWhy is UCI ELD not in the results of Figure 3 and Table 3? It is the most well-known and used dataset in the TSF domain. Is it because there is no weather data to do your feature engineering? If yes, your platform enables feature engineering only with calendar data, right? If so, it should be possible to give results with and without feature engineering considering only calendar data.\n\n### D3\nWhy, for the spatial aggregation of REF, are there no external variables? It would be better to provide min/max/std/mean of the weather indicators of the considered area to improve forecasts, as this is one of the main goals/advantages of this dataset/paper.\n\n### D4\nWhere are these regions/cities located? This information might be necessary to perform some feature engineering for given tasks.\n\n### D5\nThe Git repository does not contain all the datasets mentioned in the paper. Does ```load_with_weather.pkl``` correspond to REF?\n\n\n## Limitations\nThe SOTA of transformer models is outdated. It would be more pertinent to have models like iTransformer or Pathformer.\n\n## Revision\n$L(\\epsilon)$ is not introduced before being used.\n\n## Citation Formatting Issue\n * \u201cAs stated in (Menezes et al., 2020), [\u2026]\u201d should be \u201cAs stated by Menezes et al. (2020), [\u2026]\u201d\n * \u201c(Wang & Wu, 2017) discovered the asymmetry [\u2026]\u201d\n * \u201cBased on (Zhang et al., 2022), our framework [\u2026]\u201d\n\nAuthors should use \\citet{} as mentioned in ICLR guidelines:\n> When the authors or the publication are included in the sentence, the citation should not be in parenthesis using \\citet{} (as in \u201cSee Hinton et al. (2006) for more information.\u201d). Otherwise, the citation should be in parenthesis using \\citep{} (as in \u201cDeep learning shows promise to make progress towards AI (Bengio & LeCun, 2007).\u201d)\n\n## Proofreading\n- \u201c[\u2026] such as PV,onshore wind, and offshore wind, [\u2026]\u201d missing space.\n- \u201c[\u2026] on traditional machine learning models and deep learning models.\u201d -> \u201c[\u2026] on traditional machine learning and deep learning models.\u201d\n- \u201c[\u2026] in data at this level(Jeong et al., 2021a).\u201d missing space before citation.\n- \u201c[\u2026] the real requirement(here we mainly refer [\u2026]\u201d\n- \u201cTo our knowledge, [\u2026]\u201d -> To the best of our knowledge,"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a modular package for energy forecasting that incorporates datasets for load, wind, and photovoltaic (PV) energy. It emphasizes feature engineering techniques and custom loss functions aimed at aligning forecasting error with real-world dispatch costs. The package supports both probabilistic and point forecasting methods, evaluated across a set of 21 machine learning and deep learning models. The authors highlight their contribution in terms of feature engineering for load data, customization of loss functions for energy forecasting, and extensive benchmarking results."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The paper's package allows for customized forecasting pipelines\n- The paper collects many datasets and benchmarked on 21 models."
            },
            "weaknesses": {
                "value": "My major concern for this paper is as follows. \n\n- Scale of Datasets:\nThe datasets included are relatively small and may not fully capture the scale or complexity of modern power systems. For instance, the load and renewable datasets lack the granularity typically needed at operational levels, such as bus-level load forecasting or wind farm-level generation forecasts. A large-scale dataset, such as the ARPA-E PERFORM dataset provided by NREL, which includes hundreds of time series, would serve as a more relevant benchmark for modern power systems.\n- External Factors:\nThe authors claim that external factors for renewables have not been extensively studied, which is inaccurate. In the energy sector, weather-informed and physics-informed models for load and renewable forecasting are well-established. Commercial software solutions, like Solcast and NREL's System Advisor Model (SAM), already rely heavily on meteorological factors and hybrid physical and machine learning models. These approaches are widely adopted and should be considered as relevant benchmarks. The software and their predictions are publicly available. \n- Limitations in Probabilistic Forecasting Approach:\nWhile the paper extends to probabilistic forecasting through quantile regression, this approach does not account for the correlation between errors across time series, which is essential in multivariate contexts like interconnected power systems. Effective probabilistic forecasting for power systems often requires capturing correlation structures to support stochastic optimization of power dispatch problems (since the paper emphasizes power grid dispatch). The metrics used to evaluate probabilistic forecasts (e.g., Pinball Loss and CRPS) are limited. Important metrics like the energy score (I agree this is similar to CRPS, but I believe it is more general than CRPS), which handles multivariate distributions, and the variogram score, which captures spatial or temporal correlation, are missing. These metrics are commonly used in power systems to assess probabilistic forecasts and would improve the package's relevance for real-world applications."
            },
            "questions": {
                "value": "- Including datasets that mirror the complexity of operational power systems (such as the ARPA-E PERFORM dataset) would improve the package's utility and scalability. \n- It would be nice to compare with physical/hybrid models that have been well-established in the industry. The PERFORM dataset has predictions from the SAM model with outstanding accuracy for PV power prediction. \n- To align probabilistic forecasts with real-world power system applications, quantile regression should be supplemented with models that capture inter-series correlation, for example [1] and [2]. \n- Additionally, introducing metrics such as the energy score and variogram score would enable a more robust evaluation of probabilistic forecasts, especially in multivariate contexts.\n\n[1] Hauzenberger, Niko, et al. \"Gaussian process vector autoregressions and macroeconomic uncertainty.\" Journal of Business & Economic Statistics (2024): 1-17.\n[2]Ashok, Arjun, et al. \"TACTis-2: Better, faster, simpler attentional copulas for multivariate time series.\" arXiv preprint arXiv:2310.01327 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}