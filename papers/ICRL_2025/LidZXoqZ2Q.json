{
    "id": "LidZXoqZ2Q",
    "title": "Direct Preference Optimization Using Sparse Feature-level Constraints",
    "abstract": "The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often introduce computational inefficiencies and training instability. In this paper, we propose **F**eature-level constrained **P**reference **O**ptimization (FPO), a novel method designed to simplify the alignment process while ensuring stability. FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using sparse features activated in a well-trained sparse autoencoder and the quality of sequential KL divergence by using the feature-level offline reference. Experimental results on benchmark datasets demonstrate that FPO achieves a 5.08\\% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines, making it a promising solution for efficient and controllable LLM alignments.",
    "keywords": [
        "Preference Optimization",
        "Reinforcement Learning",
        "LLM",
        "Sparse AutoEncoder"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We propose a direct preference optimization with feature-level constraints method for efficient and controllable LLM alignment.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=LidZXoqZ2Q",
    "pdf_link": "https://openreview.net/pdf?id=LidZXoqZ2Q",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces feature-level constraints aimed at  improving the stability of alignment training process and reducing memory usage. However, the improvements are marginal, at the cost of increased complexity. And the paper lacks justification for some crucial design decisions and fails to provide analysis on hyperparameter tuning, especially for the KL divergence coefficient."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper incorporates feature-level constraints into DPO and use sparse autoencoders to identify the features used for constructing the constraints, which is novel to my knowledge. \n\n2. The paper is well-written and easy-to-follow."
            },
            "weaknesses": {
                "value": "1. This paper essentially builds upon SimPO by introducing feature-level constraints to improve the stability of alignment training. However, it does not discuss the advantages of the proposed constraints over the widely used forward KL divergence. Additionally, SimPO+KL should be included as a baseline in the experimental section.\n\n2. As claimed by the authors, the proposed feature-level constraint offers efficiency advantages over the token-level KL divergence constraint. However, while the proposed method reduces memory usage by 5G, it significantly increases the algorithm's complexity by introducing at least two additional hyperparameters, the target layer and SAE type, both of which are challenging to tune in practice and can vary across different network architectures and sizes.\n\n3. According to Table 2, the improvement achieved by the proposed method is marginal. For instance, on the Gemma-2-2B model and the benchmark AlpacaEval-2 (805 questions), the proposed method yields better responses than SimPO in only up to eight samples, while significantly increasing the algorithm's complexity. On the Gemma-2-9B model, the number of superior samples does not even exceed three samples compared to SimPO. Furthermore, as shown in Figure 3, the proposed method reduces the KL divergence on positive samples, raising doubts on its effectiveness.\n\n4. By introducing Sparse Autoencoders, the authors achieve a 5-8G reduction in memory usage compared to TDPO. However, based on my experience, this reduction could be accomplished using engineering techniques, such as offline storage or off-load, without introducing additional modules that increase algorithmic complexity. Could the authors elaborate more on this?\n\n5. The algorithm lacks in-depth clarification and analysis of some design choices. e.g.,\n\n5.1. Why the 2-norm is used in the proposed loss function instead of other distance metrics? Could the authors provide a deeper insight, potentially from the theoretical perspective?\n\n5.2. Why sparse encoding is applied only to a single layer\u2019s hidden state l  as shown in Eq (12-13) rather than certain layers of the network? And how to choose this layer?\n\n6. It is not discussed whether the proposed method can be generalized across different model architectures. \n\n7. The comparison methods in this paper involve many hyperparameters, especially the KL divergence coefficient. How did the authors tune these parameters? Could the authors present and compare the performance of the methods under different KL divergence coefficients?"
            },
            "questions": {
                "value": "Please refer to \"Weaknesses\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a method, Feature-level constrained Preference Optimization (FPO) for human preference alignment.  FPO replaces the KL divergence regularization with l2 regularzaion in  Token-Level Direct Preference Optimization (TDPO) and uses a sparse autoencoders (SAEs) for efficiency and stability. The length control trick is also applied. Numerical results on Gemma-2-2B and Gemma-2-9B are presented."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This paper addresses an important problem in LLM post-training, focusing on improving efficiency and stability in alignment with human preferences."
            },
            "weaknesses": {
                "value": "1. The sample code is not provided, which limits the reviewers' ability to verify reproducibility.\n\n2. Could the authors elaborate on why results are provided only for Gemma models? What about results for Llama models?\n\n3. Based on the current presentation, I would consider it as a relatively simple extension of DTPO, as both methods incorporate token-level knowledge and use regularization to integrate it back into DPO. A more detailed discussion clarifying the differences would be beneficial."
            },
            "questions": {
                "value": "Please see the weaknesses part.\n\nAt this stage, I tend to recommend rejection. However, I am open to reevaluating this work based the furture discussions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper try to address the challenge of aligning large language models (LLMs) with human preferences, which is a critical task. It acknowledges the limitations of post-training techniques like RLHF and DPO, which, despite their success, can be computationally inefficient and unstable. To address this, the authors propose Feature-level constrained Preference Optimization (FPO), a novel method that aims to simplify the alignment process while maintaining stability. FPO uses pre-trained Sparse Autoencoders (SAEs) and incorporates feature-level constraints to enable efficient and sparsity-enforced alignment. The approach benefits from the use of sparse features activated within a well-trained sparse autoencoder and leverages the quality of sequential KL divergence through feature-level offline reference. The experimental results on benchmark datasets show that FPO achieves a 5.08% absolute improvement in win rate with a significantly lower computational cost than state-of-the-art methods, suggesting it as a promising solution for efficient and controllable LLM alignments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe paper introduces an innovative Feature-level Constrained Preference Optimization (FPO) methodology, aimed at addressing the alignment challenge between Large Language Models (LLM) and human preferences\u2014a pivotal issue in ongoing research endeavors.\n2.\tThe FPO approach harnesses pre-trained sparse autoencoders (SAEs) and feature-level constraints, thereby offering a novel perspective on an efficient and robust alignment process.\n3.\tThe experimental outcomes reveal a significant absolute victory rate enhancement on the benchmark dataset, while the computational expense is beneath that of the current state-of-the-art benchmarks. This indicates that the methodology holds promise in terms of efficiency and manageability."
            },
            "weaknesses": {
                "value": "1.\tI continue to harbor confusion regarding the \"feature\" in the context of Feature-level Constrained Direct Preference Optimization. Could you elucidate the specific characteristics of the extracted features from the text? Furthermore, what distinguishes the essence of feature-level from token-level, fundamentally; and why are they juxtaposed for comparison? \n2.\tI find myself deeply perplexed by Figure 3 and middle figure of Figure 4. What the authors intend to convey? How does achieving the minimum KL divergence in FPO's margin manifest \"Enhanced Controllability\"? Could the author provide further elucidation on Figure 3 and middle figure of Figure 4 ?\n3.\tIn the right panel of Figure 4, there are a total of twenty points, with six situated below the \"Tie Line\" (representing 30%). This does evoke a measure of skepticism regarding the efficacy of the method in question. Furthermore, as indicated by the data presented in Table 3, the performance of FPO failed to surpass that of TDPO-2, according to the authors' findings. Although FPO does exhibit greater efficiency in terms of GPU utilization, this discrepancy appears to be of somewhat negligible significance (given our graphics cards typically operate at 80GB). \n4.\tThe author offers a meager synthesis of offline preference alignment methodologies, with the present article notably lacking a comprehensive summary and description of pertinent prior work. This deficiency hinders the understanding of the broader offline preference alignment field by other researchers. I suggest the author supplement this with a more extensive synthesis in the form of a review and consider the citation of following literatures:\n\n[1]\tWang Z, Bi B, Pentyala S K, et al. A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More[J]. arXiv preprint arXiv:2407.16216, 2024.\n\n[2]\tShen T, Jin R, Huang Y, et al. Large language model alignment: A survey[J]. arXiv preprint arXiv:2309.15025, 2023.\n\n[3]\tAzar M G, Guo Z D, Piot B, et al. A general theoretical paradigm to understand learning from human preferences[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2024: 4447-4455.\n\n[4]\tWang C, Jiang Y, Yang C, et al. Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints[J]. arXiv preprint arXiv:2309.16240, 2023\n\n[5]\tSun H, Zheng Y, Zhao Y, et al. Generalizing Offline Alignment Theoretical Paradigm with Diverse Divergence Constraints[C]//ICML 2024 Workshop on Models of Human Feedback for AI Alignment. 2024.\n\n[6]\tChen H, Zhao H, Lam H, et al. Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions[J]. arXiv preprint arXiv:2405.14953, 2024.\n\n[7]\tRafailov R, Hejna J, Park R, et al. From $ r $ to $ Q^* $: Your Language Model is Secretly a Q-Function[J]. arXiv preprint arXiv:2404.12358, 2024.\n\nGenerally speaking, the paper's proposed methodology is innovative, presenting an intriguing perspective; however, the author should elucidate the issues surrounding the 'weakness' aspect. Should the author address my concerns, I would be inclined to enhance my rating."
            },
            "questions": {
                "value": "See 'Weaknesses' Part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}