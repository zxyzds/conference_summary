{
    "id": "77gQUdQhE7",
    "title": "Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models",
    "abstract": "Recent studies indicate that effectively utilizing inference-time compute is crucial for attaining good performance from large language models (LLMs). Specifically, the Best-of-N (BoN) inference strategy, where an LLM generates multiple responses and a verifier selects the best, has shown strong empirical performance. Motivated by this, we develop a novel inference-aware fine-tuning paradigm, which encompasses the BoN-aware inference framework as a special case. We devise the first imitation learning and reinforcement learning (RL) methods for fine-tuning LLMs using BoN, overcoming the challenging, non-differentiable argmax operator in BoN. We empirically demonstrate that our BoN-aware models implicitly learn a per-example \"meta-strategy\", which interleaves best responses with more diverse responses that might be better suited to a test-time input\u2014a process reminiscent of the exploration-exploitation trade-off in RL. Our experiments demonstrate the effectiveness of BoN-aware fine-tuning in terms of improved performance and inference-time compute. In particular, we show that our methods improve the BoN performance of Gemma 2B on Hendrycks MATH from 26.8% to 30.8%, and Pass@K from 60% to 67%.",
    "keywords": [
        "Best-of-N sampling",
        "Reinforcement Learning",
        "Language models"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=77gQUdQhE7",
    "pdf_link": "https://openreview.net/pdf?id=77gQUdQhE7",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a novel inference-aware fine-tuning paradigm, aiming to enhance the inference performance when scaling the compute. \n\nTo overcome the non-differential argmax operator within best-of-N (BoN), the paper proposes the first imitation learning and reinforcement learning methods for fine-tuning language models with BoN. \n\nThe experimental results show the BoN improvement of Gemma 2B on Hendrycks MATH from 26.8% to 30.8%, and Pass@K from 60% to 67%."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The studied problem is interesting and important: how to improve best-of-N sampling in the test time through advanced training methods applied to fine-tune the models. \n\n2. The proposed method is reasonable and could be sound and useful if the method can be verified carefully and thoroughly."
            },
            "weaknesses": {
                "value": "- The paper writing can be improved: it is not easy to follow the core idea of the proposed methods. A visual Figure showing the core mechanism of the method is strongly suggested. \n\n- The experiments are not sound enough to demonstrate the effectiveness of the proposed method, specifically:\n\n**Regarding the co-scaling behavior of sample number N and temperature**: this analysis was conducted using only a single policy model and a single reward model.\n\nHow might the curves in Figure 2 appear if different language models or stronger reward models were used?\n\nThe conclusions drawn here cannot be readily generalized to other language models or reward models.\n\n\n**The results presented in Figure 4 make it challenging to conclude that the proposed inference-aware fine-tuning significantly improves performance.**\n\nOne potential issue lies in the weakness of the reward model: the solution selected as having the highest reward is often inaccurate, and fine-tuning solely on this data may result in suboptimal model performance.\n\nSelecting a broader range of solutions could benefit fine-tuning, as a correct answer might be found within these additional options.\n\nTherefore, the observed improvement may stem from identifying a correct solution through an increased sample selection rather than from the inference-aware fine-tuning itself.\n\n\n- I would also suggest conducting experiments on the alignment tasks and testing the models on AlpacaEval or Arena-Hard test sets."
            },
            "questions": {
                "value": "1. How is the accuracy of the used reward model? \n\n2. How does best-of-N sampling relate to majority vote in math reasoning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a finetuning approach for large language models (LLMs) that incorporates the inference strategy directly into the model\u2019s training process, with a particular focus on Best-of-N (BoN) inference. The authors argue that there is a disparity between the best-of-N policy typically used at inference time and the policy learned through standard supervised fine-tuning (SFT). To address this, they formalize an inference-aware fine-tuning problem that integrates the inference strategy into the training objective. Specifically, they develop a supervised BoN-aware fine-tuning method and a BoN-aware reinforcement learning (RL) with binary rewards, using policy gradients derived only from positive examples to mitigate data inefficiency. The proposed methods are evaluated on the Gemma 2B model, primarily using the Hendrycks MATH benchmark, with an emphasis on understanding the relationship between the number of samples and temperature."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper addresses an important and timely problem, as best-of-N inference with verifiers has become increasingly popular for reasoning tasks.\n- The paper provides a solid theoretical framework supporting the proposed methods with a clear formalization of inference-aware training objectives."
            },
            "weaknesses": {
                "value": "- The method is only evaluated on a single model and a single task, which limits insights into its broader applicability.\n- Although BoN-aware fine-tuning is a key contribution, the experiments predominantly examine the relationship between the number of samples and temperature. This leans more toward an analysis of the proposed methods than a comparative assessment against strong baselines, leaving questions about its competitive advantages unanswered."
            },
            "questions": {
                "value": "1. Will inference-aware SFT and inference-aware RL reduce the model\u2019s generalizability? How does the model perform when using beam search alone, without a verifier?\n2. For inference-aware SFT experiments, what type of verifier is used? How does the method perform if there\u2019s a mismatch between the training and testing verifiers?\n3. In Figure 1, how is the empirical frequency calculated? What defines the best BoN performance? Also, what are the definitions of \"easy\" and \"difficult\" problems, given that the figure is based on the same set of MATH benchmark problems?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an inference aware fine-tuning strategy for Best-of-N (BoN) to overcome the non-differentiable argmax operator for BoN. The proposed strategy incentivizes the model to balance the best and diverse responses for fine-tuning, achieves better performance compared to the base Gemma 2B model on the MATH dataset."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The incorporating of inference strategy into training is a compelling and well-founded idea.\n2. The proposed BoN-aware fine-tuning to balance the trade-off between the best and diverse responses is well-motivated."
            },
            "weaknesses": {
                "value": "1. The experiments are conducted on a base model (Gemma 2B) and an evaluation dataset (MATH). More evaluation results on different base models and evaluation datasets are needed.\n2. What is the underlying rationale for the BoN-SFT loss function presented in Equation (7)? The function appears to penalize y', even in scenarios where the sampled y' demonstrates high quality, as evidenced by r(x, y') approaching r(x, y). This raises a question: If the sampled y' exhibits desirable characteristics, wouldn't it be more beneficial for the policy model to learn from these high-quality responses?\n3. It would be valuable to conduct a comparative analysis of BoN-SFT against several straightforward baseline approaches. These could include:\n\n(1) Fine-tuning the base model using the highest-quality response selected from N generated samples.\n\n(2) Fine-tuning the base model utilizing the ground truth responses.\n\n(3) Fine-tuning the base model with all the sampled N responses.\n\n(4) Fine-tuning the base model with the combination of all the sampled N responses and the ground truth responses."
            },
            "questions": {
                "value": "See \"Weaknesses\".\n\nI would be happy to discuss further with the authors and reassess my score based on the rebuttal stage."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}