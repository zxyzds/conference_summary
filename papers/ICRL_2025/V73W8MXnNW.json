{
    "id": "V73W8MXnNW",
    "title": "Progressive Visual Relationship Inference",
    "abstract": "As an important component of visual scene, visual relationship has received extensive attention in recent years.\nMost existing works directly utilize the rough visual appearance to represent visual relationships.\nAlthough they have been made tremendous progress, the study of visual relationship may be still far from perfect.\nThis common idea may have three problems.\n1) The similarity of space aggravates the ambiguity of predicate representation.\n2) The differences between many visual relationships are subtle.\n3) It lacks interpretability.\nTo address these problems, we propose a novel method - Progressive Visual Relationship Inference(\\PVRI) - which considers both rough visual appearance and fine-grained visual cues to gradually infer visual relationships.\nIt includes the following three steps.\n1) Known Cues Collection:\nfirstly, we utilize Large Language Model(LLM) to collect the cues that may help infer visual relationships;\n2) Unknown Cues Extraction:\nsecondly, we design UCE strategy to extract the cues that are not defined by the text.\n3) Progressive Inference:\nthirdly, we utilize the obtained cues to infer visual relationships.\nWe demonstrate the effectiveness and efficiency of our method for the Visual Genome, Open Image V6 datasets.",
    "keywords": [
        "scene graph generation",
        "visual relationship detection",
        "visual scene understanding"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=V73W8MXnNW",
    "pdf_link": "https://openreview.net/pdf?id=V73W8MXnNW",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes Progressive Visual Relationship Inference (PVRI), a method to improve visual relationship understanding by combining rough appearances with detailed visual cues. PVRI includes three steps: gathering known cues from language models, extracting unknown cues directly from images, and progressively refining relationship inferences. Tested on Visual Genome and Open Image V6 datasets, PVRI shows strong performance in distinguishing subtle visual relationships, enhancing both accuracy and interpretability in scene understanding."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The overall structure of the paper is complete.\n2. The content of the paper is quite substantial."
            },
            "weaknesses": {
                "value": "1. Poor presentation. Line 93\uff5e97 are almost the same as abstract and line 161-192. Moreover, the interpretation of \u201cProgressive Inference\u201d is very confused and DO NOT reflect the concept of \u201cprogressive\u201d at all.\n2. Novelty is limited. There are many works[1][2][3] using LLM to generate visual cues to assist SGG model to distinguish similar relation categories. The author does not even discuss these efforts.\n[1] LLM4SGG: Large language models for weakly supervised scene graph generation, CVPR 2024.\n[2] Zero-shot visual relation detection via composite visual cues from large language models, NeurIPS 2023.\n[3] Less is more: Toward zero-shot local scene graph generation via foundation models, ArXiv 2023.\n3. Low performance. Both the recall@K and mean recall@K are lower than PE-Net-Reweight in Table 1 of [1].\n[1] Prototype-based embedding network for scene graph generation, CVPR 2023.\n4. Rough drawing. Fig 1 (b) is not a vector image, and the text cannot be seen clearly.\n5. Insufficient experiments. Lack of ablation study of PI and the robustness of the multiple description generation. \n6. Due to the lack of comparison between training and inference time. the calculation similarity of the union region requires N*N cropping (N is the number of the object in an image), which is very time-consuming."
            },
            "questions": {
                "value": "Please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new method named PVRI for scene graph generation, which integrates both rough visual appearance and fine-grained visual cues. It introduces Known Cue Collections (KCC), which uses Large Language Models to gather detailed descriptions, and Unknown Cue Extraction (UCE), which focuses on visual cues. The author conducts comprehensive experiments to demonstrate the effectiveness of these methods using the Open Images V6 and Visual Genome datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper proposes a new method that complements the rough visual appearances by using a Large Language Model to extract the detailed descriptions of cues.\n2. The author demonstrates the effectiveness of this proposed method through extensive experiments.\n3. This is an innovative attempt to capture fine-grained features using the Large Language Model."
            },
            "weaknesses": {
                "value": "1. There is no analysis regarding complexity or cost at all. It seems that the author utilizes the costly GPT-4 to extract a detailed description of visual cues for *all triplets*. Therefore, I'm afraid of the significant costs involved in using the GPT-4 for the scene graph generation task, considering it deals with numerous triplets. Meanwhile, the author utilizes the Large Language Model, large vision-language model (i.e., CLIP), and BGNN [1], which are all heavy. In this regard, the complexity analysis is necessary.\n\n2. The trade-off between mR@K and R@K appears to be more pronounced than in other baselines, leading us to question the effectiveness of the proposed model. Therefore, the author could use the harmonic average or simple average of R@K and mR@K, which are widely used for showing the trade-off in the nature of scene graph generation [2,3].\n\n3. The author should include state-of-the-art baselines in the comparison. While there are several recent methods for scene graph generation, the author only compares performance with a few others. For example, the author evaluates head, body, and tail performance against BGNN, which is from 2022. To clearly validate the effectiveness of the proposed method, it is essential to incorporate state-of-the-art baselines like VETO [3] and HIKER-SGG [4]. Additionally, since the author discusses the problem of the hierarchical tree structure of HIKER-SGG, it makes sense to compare performance with that model as well.\n\n4. There is no evidence supporting the effectiveness of the interpretability of the proposed method. Although the author mentions in the Introduction that existing works lack interpretability and attempts to address this issue, there is no analysis provided. This absence raises doubts about how the proposed method actually enhances interpretability.\n\n[1] Bipartite Graph Network with Adaptive Message Passing for Unbiased Scene Graph Generation. Li et al. CVPR'21  \n[2] Fine-Grained Scene Graph Generation with Data Transfer. Zhang et al. ECCV'22  \n[3] Vision Relation Transformer for Unbiased Scene Graph Generation. Sudhakaran et al. ICCV'23  \n[4] Leveraging Predicate and Triplet Learning for Scene Graph Generation. Li et al. CVPR'24"
            },
            "questions": {
                "value": "1. What would happen if GPT-4 were replaced with an open-source large language model or another model? I\u2019m interested in whether the proposed method can be generalized to other large language models, which would enhance its robustness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper propose a novel method - Progressive Visual Relationship Inference(PVRI) - which considers both rough visual appearance and fine-grained visual cues to gradually infer visual relationships. It conducted experiments on Visual Genome, OpenImages datasets. The experiments show the effectiveness of its method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors designed ablation studies to verify the importance of the two strategies, KCC and UCE."
            },
            "weaknesses": {
                "value": "1. The experiments of this paper is not as substantial as other state-of-the-art papers in this field. There is a lack of comparison with the latest methods (e.g. HiKER-SGG: Hierarchical Knowledge Enhanced Robust Scene Graph Generation).\n2. The writing of this paper is very poor. The whole article is messy and does not highlight the key points. Some of the descriptions are redundant, such as the experimental part (lines 470-485), which is obvious in the table 1. However, the description of the experimental background and the reasons for the experimental results is not enough. There are many similar problems.\n3. The main innovations of this paper are concentrated in two points: one is the geometric space representation of relationship, and the other is the clues provided by LLM. However, these two innovations are not the latest, referring to Li's paper (Zero-shot Visual Relation Detection via Composite Visual Cues from Large Language Models).\n4. There are few pictures in the paper, which are not intuitive. The pictures of the pipeline do not explain the method very well. Even the borders of some pictures(e.g. fig 1) are not processed. The author should re-work this part."
            },
            "questions": {
                "value": "The experiments lack comparisons with some of the latest state-of-the-art methods, such as HiKER-SGG. Why it happens?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}