{
    "id": "kc3QtI6NBF",
    "title": "Actionable Inverse Classification with Action Fairness Guarantees",
    "abstract": "Machine learning (ML) classifiers are increasingly used in critical decision-making domains such as finance, healthcare, and the judiciary. However, their interpretability and fairness remain significant challenges, often leaving users without clear guidance on how to improve unfavourable outcomes. This paper introduces an actionable ML framework that provides minimal, explainable modifications to input data to change classification results. We also propose a novel concept of \"action fairness,\" which ensures that users from different subgroups incur similar costs when altering their classification outcomes. Our approach identifies the nearest decision boundary point to a given query, allowing for the determination of minimal cost actions. We demonstrate the effectiveness of this method using real-world credit assessment data, showing that our solution not only improves the fairness of classifier outcomes but also enhances their usability and interpretability.",
    "keywords": [
        "Algorithm Fairness",
        "Explainability"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kc3QtI6NBF",
    "pdf_link": "https://openreview.net/pdf?id=kc3QtI6NBF",
    "comments": [
        {
            "summary": {
                "value": "This work"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is very straightforward and simple. The problem is well motivated and illustrated. \n\n2. The authors evaluate on simple, interpretable ML models"
            },
            "weaknesses": {
                "value": "1. While the authors focus on inverse classification, I actually can't distinguish the problem statement from counterfactuals and recourse. https://arxiv.org/abs/2002.06278 . Contrasting with recourse and counterfactual works, the proposed framework seems limited as it requires integer costs. In large feature-sets, setting these relative costs may be very difficult, r.e. user feasibility.\n\n2. The method is quite weakly evaluated in several regards: (a) evaluating on two fairly simple tabular datasets, (b) not evaluating against any baseline (there are many in recourse, and (c) the proposed 'action fairness' seems just a differencing measure. this need not be proposed as a novel fairness (as any statistic could be use difference of means etc.) This evaluation is simply insufficient for publication.\n\n3. Overall, the contribution is limited. The authors do not provide analysis on this action fairness problem, or present a novel model for achieving it in pre-processing."
            },
            "questions": {
                "value": "1. How do you handle varying costs over a large set of features? Must the user specify such feature cost functions? i.e. perhaps it's easier to change a FICO feature from 550 to 570, than 680 to 700."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper looks at the problem of altering decisions (Given an ML classifier, how could people with unfavorable decision change the prediction) and then focus on Action Fairness (is that cost the same across groups?).\n\nThe authors propose a metric as well as a technique to improve it, and then provide empirical evidence on two academic datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This was afun read, as the underlying techniques are quite interesting (in particular the technique of inverse classification). I have a lot of questions on the metrics, but the methodology is sound and well explained."
            },
            "weaknesses": {
                "value": "The main weakness lies in the fairness definition proposed:\n\n1- Why is \"action Fairness a desired property? in particular since it is not conditioned on any credit worthiness of the underlying group/individuals.\nOther established fairness metrics focus on outcome and typically rely on economic values. For instance Equality of opportunity: if credit worthy, both people should have the same chance or Statistical parity: both groups should have the same chance. \nFor instance, if females need to reduce their credit cards and increase their account balance to meet fairness constraints, males should face comparable requirements. --> This is not very intuitive to me. What would be the motivation for this? \n\n2- How does this fairness definition compare to Fairness of outcome (as measured by SP, EOP or other)?\nFor instance, if a system achieves higher action fairness but lower fairness of outcome, do we really consider this as an improvement?\n\n3- A core concept of this definition is the cost function, which seems very hard to determine in practice. The authors say that it would be \"established by an expert or learned from historical data\". Could they share more information on this? I want to understand more the feasibility of establishing such cost function.\n\n\n\nSome more minor weaknesses:\n1- Intro is a bit  unclear. Need to justify right away the concept of \"altering their outcomes\", and  the associated cost. Also needs to justify for their fairness definition. Maybe walk the reader through some examples.\n\n2- Missing relevant literature: Some key papers of the field are missing (just listing a few below):\nStandard approaches, such as removing sensitive attributes, may not always achieve fair outcomes due to hidden correlations. --> Cite adversarial fairness egg https://arxiv.org/abs/1707.00075\nFairness in classifiers: http://arxiv.org/abs/1803 +  Fairness constraints: A mechanism for fair classification.  + Fairness through awareness+  Preventing fairness gerrymandering: Auditing and learning for subgroup fairnes"
            },
            "questions": {
                "value": "Re-listing some questions from the weakness section.\n\n1- Can authors provide further justification for the desirability of this fairness definition>\n\n2- What are the relationship of such metric compared to fairness of outcomes: is it a second order property or should it supersede it?\nOverall I could be convinced if we say that the goal is to have fairness of outcome + fairness of actions.\n\n3- Can they add metrics of fairness of outcome in experimental section?\n\n4- Would it be possible to provide further details on how to establish the cost function?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a framework for algorithmic recourse, which considers the outcome's fairness. Experiments on two datasets confirm the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The current status of the paper does not allow me to understand the actual contribution, hence its strengths.\nE.g., in lines 65-67 the authors claim that their method is better than Ustun et al., (2019) in terms of execution time and real-life cost values. Still, a detailed description of their approach is never presented, so I cannot evaluate whether this is true."
            },
            "weaknesses": {
                "value": "The paper has several shortcomings, which (in my opinion) cannot be addressed in the current version of the paper. The weaknesses I see are:\n\n1) the paper lacks an actual contribution, or at least, it is not adequately presented. Here are a few examples of this:\n- Algorithm 0 is only referenced in the submitted pdf but never presented. Moreover, I could not retrieve any supplementary material detailing such an algorithm;\n- Several variables are never introduced (e.g., what are $y'=0$ and $S$ in equation 2?)\nThis makes it very difficult to evaluate the current paper, especially regarding the novelty and the significance of the contribution.\n\n2) the paper's related work is not adequate, with the most recent work dating back to 2019. The research on algorithmic recourse has instead advanced quite fast in the last five years. See for instance,\n\n    - Karimi, Amir-Hossein, Gilles Barthe, Bernhard Sch\u00f6lkopf, and Isabel Valera. \"A survey of algorithmic recourse: contrastive explanations and consequential recommendations.\" ACM Computing Surveys 55, no. 5 (2022): 1-29.\n    - Karimi, Amir-Hossein, Bernhard Sch\u00f6lkopf, and Isabel Valera. \"Algorithmic recourse: from counterfactual explanations to interventions.\" In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pp. 353-362. 2021.\n    - Karimi, Amir-Hossein, Julius Von K\u00fcgelgen, Bernhard Sch\u00f6lkopf, and Isabel Valera. \"Algorithmic recourse under imperfect causal knowledge: a probabilistic approach.\" Advances in neural information processing systems 33 (2020): 265-277.\n    - Beretta, Isacco, and Martina Cinquini. \"The Importance of Time in Causal Algorithmic Recourse.\" In World Conference on Explainable Artificial Intelligence, pp. 283-298. Cham: Springer Nature Switzerland, 2023.\n\n\n3) The empirical evaluation has several shortcomings:\n- it is limited to two tabular datasets and focuses on two simple algorithms, i.e., logistic regression and an SVM;\n- it does not compare the approach with existing methods that perform algorithmic recourse;\n-  it lacks any form of uncertainty related to the outcomes. For instance, I think the authors should perform statistical significance tests across different methods to claim the superiority of their approach. See [Demsar, 2006] for evaluating different methodologies.\n\n[Demsar, 2006] - Dem\u0161ar, Janez. \"Statistical comparisons of classifiers over multiple data sets.\" The Journal of Machine learning research 7 (2006): 1-30."
            },
            "questions": {
                "value": "I think the authors should address the four weaknesses presented in the previous space.\nMore precisely, the questions are:\n\n- q1) how do the authors differentiate from previous works on algorithmic recourse?\n\n- q2) why did they not include works from 2020 onwards?\n\n- q3) why did they not compare their approach with other existing methods?\n\n- q4) what is the required time for the proposed approach compared to existing approaches?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors address the problem of inverse classification under fairness constraints, proposing a machine learning framework that offers minimal and explainable modifications to instances to change classification outcomes, thereby providing recourse. They introduce a fairness notion, i.e., \u201caction fairness\u201d, aimed at equalizing the difficulty of achieving recourse across different groups. Their approach focuses on linear classifiers, where the decision boundary is clearly defined, enabling the identification of minimal-cost actions. Through experimental evaluation, they demonstrate that the framework enhances the fairness of classifier outcomes via a post-processing technique."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper addresses the pressing issue of fairness and interpretability in machine learning applications within high-stakes decision-making domains such as finance, healthcare, etc. Given the potential impact of ML-based decisions on individuals' lives, this research tackles a crucial problem.\n2. The framework successfully reduces fairness disparities without significantly sacrificing classification accuracy (balance of fairness-accuracy)."
            },
            "weaknesses": {
                "value": "1. The literature review does not provide a comprehensive overview of relevant work. \n(a)The framework is described as providing \"minimal, explainable modifications to input data to change classification results,\" i.e., the actions leading to local counterfactuals. However, the authors do not make such a connection. The authors make a brief connection between the notion of recourse (Ustun et al., 2019) and their work, claiming that they extend this concept by incorporating fairness requirements. However, no other information is given.\n(b)The authors propose the concept of \"action fairness.\" However, they appear to overlook relevant work, particularly fairness notions related to the challenge of achieving recourse, such as \"fairness of recourse,\" initially introduced by Ustun et al. (2019) and later formalized by Gupta et al. (2019). Gupta et al. introduced the idea of Equalizing Recourse for different groups, aiming to provide classifiers that maintain good performance while ensuring feasible recourse across groups. Subsequent works refer to the disparity in the mean cost to achieve recourse as \"burden.\" Additionally, Kavouras et al. (2023) offer alternative definitions to capture both micro and macro perspectives of fairness of recourse, particularly for auditing machine learning models.\n2. The notation and formalization of the problem, including conditions and definitions, require improvement for clarity.\n(a) The formalization of the problem and the definition of their fairness condition lack clarity. It is difficult to identify the cost in the problem definition (cumulative over n actions), and it is not explicitly clear that P_n(y\u2019=0|S=1) represents the probability of an individual X_n (belonging to the subgroup S=1) achieving a positive outcome with action A_n. Additionally, the method for determining this probability remains unclear.\n(b) The description of the weighted Euclidean distance is ambiguous. Specifically, the definition of cost(a_n) is unclear\u2014is it intended to be the cost of an action multiplied by the distance from x to the boundary (which already appears to define the cost)? It would be helpful to clarify whether cost(a_n) corresponds to a weight vector assigned to each feature. Furthermore, the definitions should be expressed as vector operations, where x_n represents an instance of X with multiple features (typically, vectors are denoted in bold). For an example of appropriate notation and problem definitions, please refer for example to Ustun et al. (2019).\n(c) The problem definition for inverse classification does not appear to impose any restrictions related to linear classifiers. Consequently, the introduction of weights may not be relevant to inverse classification (as noted on line 58). However, equation (7) defines the problem specifically for linear classifiers, where the introduction of weights would probably be more appropriate.\n3. Algorithm 0 is not provided, resulting in a lack of explanation regarding the framework and its methods.\n4. The methods used to ensure fairness in the results during the experimental section are not clearly explained. Specifically, it is not clear if the authors find the weights for a new fair classifier or something different.\n---\nGupta, Vivek, et al. \"Equalizing recourse across groups.\" arXiv preprint arXiv:1909.03166 (2019).\nKavouras, Loukas, et al. \"Fairness aware counterfactuals for subgroups.\" Advances in Neural Information Processing Systems 36 (2024)."
            },
            "questions": {
                "value": "1. The post-cost-diff method lacks a clear explanation. How is the group assignment of an instance determined if not based on its attributes?\n2. The purpose of the post-processing with the ranking is not clearly articulated. How does this process contribute to fairness? Does it involve removing the most costly actions? Additionally, how does it affect the accuracy of the classifier? Are new weights determined by solving an optimization problem? If so, why not directly incorporate the fairness criterion as an additional constraint in the optimization problem? \n3. Could you clarify why training time is considered an issue? Additionally, why was not a data split performed? How is accuracy calculated in this context? \n4. Please refer to the weaknesses section for details. The paper would benefit from a more comprehensive literature review, including connections to relevant fairness concepts (e.g., the cited papers). Additionally, improvements are needed in the notation, problem formulation, and the formalization of the algorithm and its description. The absence of a formal algorithm is a significant issue, as it contributes to the questions surrounding both the fairness concepts and the framework proposed by the authors. In general, a major revision is required."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}