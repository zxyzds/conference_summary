{
    "id": "HNOo4UNPBF",
    "title": "Scale-Aware Contrastive Reverse Distillation for Unsupervised Anomaly Detection",
    "abstract": "Unsupervised anomaly detection using deep learning has garnered significant research attention due to its broad applicability, particularly in medical imaging where labeled anomalous data are scarce. While earlier approaches leverage generative models like autoencoders and generative adversarial networks (GANs), they often fall short due to overgeneralization. Recent methods explore various strategies, including memory banks, normalizing flows, self-supervised learning, and knowledge distillation, to enhance discrimination. Among these, knowledge distillation, particularly reverse distillation, has shown promise. Following this paradigm, we propose a novel scale-aware contrastive reverse distillation model that addresses two key limitations of existing reverse distillation methods: insufficient feature discriminability and inability to handle anomaly scale variations. Specifically, we introduce a contrastive student-teacher learning approach to derive more discriminative representations by generating and exploring out-of-normal distributions. Further, we design a scale adaptation mechanism to softly weight contrastive distillation losses at different scales to account for the scale variation issue. Extensive experiments on benchmark datasets demonstrate state-of-the-art performance, validating the efficacy of the proposed method. The code will be made publicly available.",
    "keywords": [
        "unsupervised anomaly detection",
        "medical images",
        "contrastive reverse distillation",
        "student-teacher"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=HNOo4UNPBF",
    "pdf_link": "https://openreview.net/pdf?id=HNOo4UNPBF",
    "comments": [
        {
            "summary": {
                "value": "The paper presents a scale-aware contrastive reverse distillation model for unsupervised anomaly detection, focusing on improving performance in fields like medical imaging, where labeled anomalous data is often scarce. Building on this, the authors introduce a contrastive student-teacher learning approach that generates out-of-normal distributions to enhance feature discriminability. Additionally, they propose a scale adaptation mechanism that adjusts contrastive distillation losses at various scales to account for variations in anomaly size, addressing limitations in existing methods that struggle with scale variation and feature discrimination. Evaluation on benchmark datasets shows that this model achieves reasonable performance, validating its effectiveness."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+) Anomaly detection is an important topic\n\n+) The paper is well-structured and easy to follow"
            },
            "weaknesses": {
                "value": "-) Novelty of this work seems incremental\n\n-) The domain is limited to medical image analysis"
            },
            "questions": {
                "value": "a) Anomaly detection is an important topic where identifying unusual patterns can have a direct impact on various applications. In this sense, this paper researches a domain-specific area in anomaly detection, i.e., medical images, using reverse-distillation. \n\nb) While this paper provides valuable refinements in the area of anomaly detection, particularly through its scale-aware contrastive reverse distillation model, the novelty appears somewhat incremental. Firstly, the study primarily builds on and combines well-established methods, reverse distillation and contrastive learning, rather than introducing a fundamentally new approach. Secondly, the approach to enhance feature discriminability by generating synthetic or perturbed anomaly data is not novel; similar methods have been previously explored, such as in [1]. The effectiveness of this technique can also vary significantly depending on the choice of noise or perturbation (rather than learnable in [1]), which may limit its reliability. Thirdly, fusing multi-scale features is a long-standing topic in computer vision, which further contributes to the incremental nature of the contribution. While these refinements have practical value, they represent an evolution of existing methods rather than a new innovation in anomaly detection.\n\n[1] Cai, Jinyu, and Jicong Fan. \"Perturbation learning based anomaly detection.\" Advances in Neural Information Processing Systems 35 (2022): 14317-14330.\n\nc) The evaluation of the proposed method is performed on three medical datasets, it is not clear how good it can be generalized to other domains?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a twofold approach: on one hand, data augmentation for unlabeled information is achieved by injecting noise, which then serves as the basis for contrastive learning. On the other hand, parameters are extracted at each layer to achieve multi-scale information fusion, enabling the model to better distinguish between positive and negative samples."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The methodology section of this paper is clearly written and well-presented."
            },
            "weaknesses": {
                "value": "1. In terms of innovation, this article presents two components that are commonly seen in other works. A straightforward suggestion would be for the authors to explore a more adaptable data augmentation method to address the issue raised in \u201cQuestion 1.\u201d\n\n2. Regarding the writing, the **Introduction** section of this article is somewhat confusing. Firstly, the paragraphing is suboptimal. In the second paragraph of the introduction, the author discusses the shortcomings of anomaly detection models based on generation. The third paragraph explains existing work aimed at improving these models, followed by an exploration of the development of self-supervised learning. Although self-supervised learning indeed enriches unsupervised learning within deep learning, the third paragraph would be clearer if it unified these two parts, focusing on how self-supervised learning specifically addresses the issues raised in the second paragraph. Secondly, and more importantly, an excessive portion of the introduction is dedicated to discussing related work, lacking sufficient focus on the contributions of this paper. Maybe you could merging the discussion of existing work and self-supervised learning, followed by a clear transition to the paper's contributions. Additionally, you could recommend what key points about the paper's contributions should be emphasized earlier in the introduction."
            },
            "questions": {
                "value": "1. In the noise generation process, the intensity of each position is sampled from a uniform distribution. However, in real-world medical imaging, anomalous regions in disease cases appear in specific locations; for example, abnormalities in a chest image typically do not appear outside body regions. For instance, you could suggest incorporating prior knowledge about typical anomaly locations in different types of medical images.\n\n2. Additionally, is each test sample measured at the image level or at the patch level? If it is at the image level, the positive-to-negative anomaly ratio in the dataset is relatively high, which does not quite align with the extremely imbalanced nature of anomaly detection tasks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Two issues arise in reverse knowledge distillation for medical imaging, an inability to properly distinguish between different features and an inability to deal with different scales. This work proposes two methods to address these issues: a contrastive student-teacher learning approach that involves using both a \u201cgood\u201d and a \u201cbad\u201d teacher, as well as a scale adaptation mechanism. Results demonstrate that on almost every dataset the proposed approach is able to outperform existing techniques."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well written and the ideas are easy to follow. I appreciate how the authors identify the two problems facing medical imaging in this domain and then propose solutions. The new approach outperforms all other methods that the authors compare with in table 1."
            },
            "weaknesses": {
                "value": "Issue 1: This paper doesn\u2019t have any formal proofs or theorems to guarantee its effectiveness, therefore the strengths of the paper must rely on the experiments and empirical results. For the datasets I will admit as a review I am not very familiar with them. For example, on the image dataset CIFAR-10, a few years ago a jump from 93% to 98% was considered significant. Today an increase from 99% to 99.9% on CIFAR-10 would not even be considered a major contribution. Likewise, for these datasets it is hard to determine whether a jump of 3.01% and 4.38% (as the authors show) is worthy of publication. Could the authors please clarify how much other papers pushed the accuracy forward in other works, as compared to the increase their method is offering?\n\nIssue 2: The authors say they will release their code upon publication, but I did not see any code given in the supplemental material. This might be an oversight on my part, but where is the code? I would like to see a good faith gesture of releasing an anonymous github for the code or at least a zip file of the code. Too many papers claim SOTA results and never release code. In this day and age it is simply unacceptable to continue this trend. \n\nIssue 3: The authors also don\u2019t mention anything about the runtime of their algorithm. How does the student-teacher setup and training time compare to other existing methods in the field? While this is not a HUGE concern, I think it would be good to have some discussion and analysis of this. For example could you show the training time complexity of your algorithm and the next two best methods in the field? Or even an experimental runtime of the training in seconds of your algorithm and the next two best methods? Without this I feel we don\u2019t have the complete story when comparing methods. \n\nIssue 4: Please see my minor comments about uncited claims in the papers. For acceptance I would like to see citations for all the claims you have made. \n\nIssue 5: I don\u2019t like the terminology \u201cgood\u201d and \u201cbad\u201d teacher. It is very non-technical and hard to follow. Can you please change the terminology in your paper and name the teachers more appropriately? \n\nMinor Comments:\n\n=Line 36, \u201canomalous samples is often prohibitively expensive and time-consuming\u2026\u201d I believe this is correct but I want to see a citation to backup this claim?\n\n=Line 45, \u201cthey tend to generalize too well, thereby risking the reconstruction of abnormal regions.\u201d Citation for this claim? \n\n=Table 1 has many entries that are unsorted (aside from the bolded number indicating the best method). It would be better if this table was sorted in order of increasing AUC for the methods.\n\n=Line 167 \u201cMedical images typically exhibit a power law distribution of frequencies, with lower frequency components dominating the image content.\u201d Please give a citation for this bold claim?"
            },
            "questions": {
                "value": "Q1: Please address each of my issues 1-5. If they are adequately addressed I would be willing to increase my score for the paper accordingly."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a new method for anomaly detection in medical data setting using contrastive reverse distillation. The method uses knowledge distillation as it's foundation, implementing pre-trained teachers models along with a student model trained to either minimize or maximize its distance from the teachers in a feature-map space. They additionally utilize a scale adaptive mechanism to allow for better generalizations across different sizes of anomaly.\n\nTheir method's specific construction utilizes two teacher models with the same weights, one \"bad\" and one \"good\". The good model is given benign images while the bad model is given images with synthetic anomalies. The student model is then trained to maximize its cosine similarity with the per-layer features of the good teacher while minimizing the cosine similarity with the bad teacher. At evaluation time anomaly scores are computed by comparing the scaled, per-layer feature spaces of the good teacher against those of the student. Inputs with high levels of dissimilarity between the good model and student are given high anomaly scores.\n\nTheir evaluation against prior work is very comprehensive, comparing against a total of 18 other detectors. They out perform all other detectors on all three datasets (lung, brain, and skin anomalies) in terms of their main metrics (AUC, F1, and Accuracy). The authors further perform ablations with respect to their main contributions of Contrastive Reverse Distilation and Scale Adaptive Mechanisms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Anomaly detection is somewhat outside my area of expertise, but the paper seems to frame itself well within the current body of literature. \n* The paper is well written and easy to follow. Figures are also informative and easy to understand.\n* The idea of contrastive reverse distillation seems novel (to the best of my knowledge) and intuitive. I'd like to see how future works can iterate upon this framework in other settings or with improved constructions.\n* Motivation is well established for the usage synthetic anomalies, and the choice of simplex noise is clever.\n* Empirical analysis is very comprehensive in terms of datasets, baselines, and ablations.\n* The proposed method out-performs all baselines in terms of AUC, F1, and Accuracy.\n* The proposed method scales well with model architecture size, which is a good sign that the method is sensible."
            },
            "weaknesses": {
                "value": "In general, within my somewhat limited knowledge of this sub-field, I like this paper. In spite of that I think there are a few things the authors may want to consider to further strengthen their work.\n\nI think when discussing detection and AUC scores it is important to give them context within the given domain. See section B of [1] for some example discussion on this topic from a different field. I'd like the authors to consider the following questions:\n\n* Is it more important to maximize the true positive rate in spite of inducing some false positives? \n* Is it better to minimize false positives to increase confidence in the case of a positive prediction?\n* How do you anticipate doctors using these tools in their analysis?\n\nGiven this, it would also be nice if the authors could plot some of the ROCs between their method and some of the baselines (maybe including all of them is too visually messy, but at least the top 5). This way people can compare their method to the baselines at different levels of false positive rate.\n\nI think it would be good if the authors could include some extended discussion, either in the main body or the supplementary material, analyzing the more subtle features of their results. See the next section of my review \"Questions\" for some examples of what I think they can further analyze.\n\nLastly, the results of table 1 are good, but a bit overwhelming. I think it would help if the second best score in each metric is underlined and if the methods are visually grouped based on detector type (e.g. RD4AD, RD++, and ReContrast could be grouped together). Optionally it would be nice if all experiments were averaged over a few runs and given standard deviations or $\\pm$ scores, at least in the supplementary material. I understand if these experiments are too extensive to replicate too many times, though.\n\n [1] \"Membership Inference Attacks From First Principles\" https://arxiv.org/abs/2112.03570"
            },
            "questions": {
                "value": "Here I include some questions for the authors to consider.\n\n* Why does ISIC seem like the hardest dataset for all the detectors? Why is Brain Tumor the easiest?\n* Why is SAM more impactful in the Brain Tumor dataset while it seems minimally impactful in the RSNA dataset in table 2? Is this related at all to the results of table 3 - where Gaussian noise performs similar to simplex noise on RSNA, but much worse on Brain Tumor?\n* Can the method achieve better generalization by using a mixture of synthetic noise types, perhaps 10% gaussian, 90% simplex? \n* Can we improve performance by including some real world data in the training set, or will that harm generalization?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}