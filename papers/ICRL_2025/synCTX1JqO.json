{
    "id": "synCTX1JqO",
    "title": "RaCNN: Region-aware Convolutional Neural Network with Global Receptive Field",
    "abstract": "Recent Convolutional Neural Networks (CNNs) utilize large-kernel convolutions (e.g., 101 kernel convolutions) to simulate a large receptive field of Vision Transformers (ViTs). \n    However, these models introduce specialized techniques like re-parameterization, sparsity, and weight decomposition, increasing the complexity of the training and inference stages. \n    To address this challenge, we propose Region-aware CNN (RaCNN), which achieves a global receptive field without requiring extra complexity, yet surpasses state-of-the-art models. \n    Specifically, we design two novel modules to capture global visual dependencies. \n    The first is the Region-aware Feed Forward Network (RaFFN). \n    It uses a novel Region Point-Wise Convolution (RPWConv) to capture global visual cues in a region-aware manner. \n    In contrast, traditional PWConv shares the same weights for all spatial pixels and cannot capture spatial information. \n    The second is the Region-aware Gated Linear Unit (RaGLU). \n    This channel mixer captures long-range visual dependencies in a sparse global manner and can become a better substitute for the original FFN. \n    Under only 84\\% computational complexity, RaCNN significantly outperforms the state-of-the-art CNN model MogaNet (83.9\\% vs. 83.4\\%). \n    It also demonstrates good scalability and surpasses existing state-of-the-art lightweight models. \n    Furthermore, our RaCNN shows comparability with state-of-the-art ViTs, MLPs, and Mambas in object detection, instance segmentation, and semantic segmentation.  \n    All codes and logs are released in the supplementary materials.",
    "keywords": [
        "Convolutional Neural Network",
        "Global Receptive Field",
        "Backbone"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=synCTX1JqO",
    "pdf_link": "https://openreview.net/pdf?id=synCTX1JqO",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces Region-aware CNN (RaCNN), a novel architecture that achieves a global receptive field without added complexity, outperforming state-of-the-art models. RaCNN features two key modules: the Region-aware Feed Forward Network (RaFFN) using Region Point-Wise Convolution (RPWConv) to capture global visual cues, and the Region-aware Gated Linear Unit (RaGLU) for capturing long-range dependencies. With only 84% of the computational complexity, RaCNN surpasses the performance of MogaNet (83.9% vs. 83.4%) and demonstrates strong scalability, excelling in object detection, instance segmentation, and semantic segmentation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. RaCNN includes two critical modules: the Region-aware Pointwise Convolution (RPWConv) for capturing global visual information and the Region-aware Gated Linear Unit (RaGLU) for capturing long-range dependencies.\n2. RaCNN outperforms state-of-the-art models like MogaNet, achieving higher performance (83.9% vs. 83.4%) with only 84% of the computational complexity.\n3.  The model demonstrates excellent performance across various tasks, including object detection, instance segmentation, and semantic segmentation.The paper provides a thorough comparison with other methods across multiple downstream tasks, showcasing the robustness and effectiveness of RaCNN."
            },
            "weaknesses": {
                "value": "1. While RaCNN shows improvements over existing models, the performance gains might be considered incremental. \n2. The ablation experiments are insufficient, lacking further analysis on the effectiveness of the proposed modules. For example, the dynamic PWconv use cosine distance to measure similarity instead of inner-product, how about the difference affect the performance ?\n3. From the definition of the dynamic PWconv and RaGLU in the paper, and its implementation in the code, the proposed method does not capture the global receptive field of imagesm which is inconsistent with the paper's title.  Since the dynamic PWconv are improvement upon the basic of Pointwise Convolution, the dynamic wieghts generated for different region just provide different ways for gatherring features along the channel dimension.  The region attention in RaGLU is also a kind of channel attention .All the operation about the spatial gathering remains conventional 3x3 convolution. So where the global receptive field exhibits?"
            },
            "questions": {
                "value": "The questions have been listed in the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper titled \"RaCNN: Region-Aware Convolutional Neural Network with Global Receptive Field\" proposes a new CNN model, RaCNN, that enhances the receptive field to capture global visual dependencies without increasing computational complexity. The key innovations of RaCNN include two modules: the Region-aware Feed Forward Network (RaFFN) and the Region-aware Gated Linear Unit (RaGLU).\n\nRaFFN uses Region Point-Wise Convolution (RPWConv) to capture global cues by dividing spatial feature maps into sparse global regions, allowing it to dynamically adjust weights per region. Meanwhile, RaGLU serves as a channel mixer, capturing long-range dependencies in a sparse, global manner, improving spatial information aggregation. Compared to conventional CNNs, RaCNN demonstrates state-of-the-art performance, surpassing models like MogaNet, while requiring fewer computational resources."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe paper proposes region-aware CNN. The author suggest RaCNN captures both local and global information to better feature extraction\n2.\tExperiments show RaCNN achieves better results on various tasks, including image classification (ImageNet-1K), object detection (COCOval2017), instance segmentation (COCOval2017), and semantic segmentation (ADE20K)."
            },
            "weaknesses": {
                "value": "1.\tIt is confused to figure out how RaCNN captures global information. For instance, Region PWConv in figure 4c takes dilated windows and Region Attention in figure 5 averages spatial feature in a dilated manner. However, dilation is not equivalent to the so-called \u2018global\u2019. From my point of view, it is more proper to say it captures a larger local receptive field. \n2.\tThe author suggests RaCNN is better because it captures local and global information. However, in figure 2 we can see SLak, UniRepLKNet, and InceptionNeXT all captures local and global receptive field, even with a more \u2018global\u2019 field. Therefore, I am suspecting local and global receptive field is not the true reason for RaCNN performance. \n3.\tFor figure 1, there is no margin between the figure 1 caption and main text. \n4.\tMore tasks [1] should be added to verify the effectiveness of RaCNN, such as 2D and 3D human pose estimation, and video prediction. \n[1] Li, Siyuan, Zedong Wang, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu, Zhiyuan Chen, Jiangbin Zheng, and Stan Z. Li. \"Moganet: Multi-order gated aggregation network.\" In The Twelfth International Conference on Learning Representations. 2023."
            },
            "questions": {
                "value": "Question: While RaCNN claims to achieve a global receptive field without additional complexity, it would be beneficial to understand the exact computational trade-offs compared to recent models with large-kernel convolutions. Could the authors provide further detail on any specific optimizations that contribute to this efficiency?\nSuggestion: Including a breakdown of computational complexity relative to specific design choices (like RPWConv and RaGLU) and a comparison against representative models would enhance clarity on how RaCNN maintains low computational cost.\nScalability Across Tasks and Architectures:\n\nQuestion: RaCNN demonstrates strong performance across multiple vision tasks. How does its performance and efficiency vary when scaling to larger or smaller model variants, particularly in dense prediction tasks? Are there configurations where RaCNN\u2019s advantages diminish?\nSuggestion: It would be helpful if the authors provided more details on the model's performance when scaled to different architectures and task demands, particularly in cases where efficiency gains may be less pronounced.\nImpact of Region Size and Sparse Global Regions:\n\nQuestion: RaCNN\u2019s RPWConv operates on sparse global regions, which raises questions about the impact of different region sizes on performance and the possibility of information loss. Could the authors clarify how they determine optimal region sizes and the robustness of RaCNN to varying region granularity?\nSuggestion: An ablation study on the impact of different region sizes and sparsity levels on accuracy and computation might help clarify the balance between global context capture and computational efficiency."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents the RaCNN for vision recognition. The proposed RaCNN achieves a global receptive field with two designs: the region-aware FFN that uses a variant of dilated attention to model global relations, and the region-aware GLU that uses DWConv and a regional SE module to improve model capacity. The model is evaluated on widely used visual recognition benchmarks and compared to recent SoTA methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The method achieves competitive performance on various visual recognition tasks including classification and dense prediction. The throughput also looks good.\n\n- The authors conducted extensive experiments to verify the model."
            },
            "weaknesses": {
                "value": "- The proposed model actually is not a convolutional neural network. A convolution operation should be transition-equivariant. The proposed operation is closer to a variant of local attention instead of convolution. The authors may consider changing the name of the model and some claims in the paper to avoid misunderstanding.\n\n- The proposed design is a mixture of some existing techniques.  The Region Point-Wise Convolution is not a convolutional operation and it is very similar to [r1]. The RA operation is a variant of the SE module. Although the paper shows that the mixture can achieve good performance, the technical contribution is quite limited. \n\n[r1] Glance-and-Gaze Vision Transformer, NeurIPS 2021. \n\n- The motivation of the paper is not quite clear. According to the abstract, the paper wants to propose a new solution because \"these models introduce specialized techniques like re-parameterization, sparsity, and weight decomposition, increasing the complexity of the training and inference stages\". However, I think the proposed solution also introduces many complex operations and may make the solution less general. \n\n- The presentation of the paper needs improvement. Many claims are not clear. For example, in the caption of Figure 1, it is mentioned that some existing methods \"could capture long-range dependency but introduce excessive visual noises\". I don't found there is evidence to show these methods can introduce extra noise. Besides, the descriptions of the core contribution: RPWConv and RA, are quite confusing. Figure 4 didn't clearly the RPWConv operation. Equations 5-7 are also misleading since both \"x \\dot x\" and \"xw\" actually are matrix multiplication. I need to check the provided code to clearly understand how the operation actually works."
            },
            "questions": {
                "value": "Please refer to my comments above. Considering there are many issues about the presentation, positioning, technical contribution, and motivation, I cannot recommend acceptance for this paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents RaCNN, a convolutional neural network designed to capture both long-range dependencies and local contextual information in images. The architecture introduces two key components: the Region-aware Feed Forward Network (RaFFN) and the Region-aware Gated Linear Unit (RaGLU). RaFFN employs a specialized Region Point-Wise Convolution (RPWConv), which divides spatial feature maps into global regions and applies dynamic weights within each, allowing for the capture of both global and local features. RaGLU enhances spatial information mixing through region-specific attention mechanisms. The authors claim that RaCNN outperforms state-of-the-art models such as MogaNet and Swin Transformer in tasks like image classification, object detection, and instance segmentation, while maintaining computational efficiency. Extensive experiments and ablation studies are provided to validate RaCNN's effectiveness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "RaFFN and RaGLU introduce a novel region-based dynamic weighting approach in CNNs, allowing RaCNN to capture multiscale spatial dependencies compared to traditional convolutional methods more effectively.\n\nThe experimental results across multiple vision tasks, such as object detection and instance segmentation, demonstrate RaCNN's robustness. This approach efficiently captures global dependencies while maintaining computational efficiency, representing an advancement over previous CNNs and ViTs.\n\nRaCNN demonstrates scalability across various model sizes and tasks."
            },
            "weaknesses": {
                "value": "The claim that RaCNN \"can capture long-range dependencies and local context features simultaneously without excessive noise\" is based on a single ERF visualization. However, visualizations from a single layer are insufficient to support such broad conclusions about noise reduction and contextual feature capture, as ERF patterns vary across layers in complex networks. A more comprehensive analysis across multiple layers or the use of additional metrics would be necessary to fully substantiate this claim.\n\nAlthough the paper highlights RaCNN\u2019s computational efficiency, it lacks a detailed comparison of how this efficiency holds up against similar models in real-world scenarios, such as inference time or memory usage.\n\nThe RPWConv design, which divides the spatial feature map into predefined regions, may limit flexibility across different image types. Fixed partitioning could reduce generalizability for diverse visual tasks that benefit from more adaptive receptive fields. Further investigation into alternative partitioning strategies, like adaptive region sizes, would strengthen the proposed approach.\n\nWhile the ablation studies focus on the impact of individual components (e.g., RPWConv, RaGLU), they do not sufficiently explore the interactions between these modules. Expanding the ablation studies to analyze how these components interact could provide deeper insights into RaCNN\u2019s overall design.\n\nThe paper\u2019s focus on FLOPs as the primary metric for efficiency may overlook other crucial factors, such as memory usage and inference time. These metrics are critical for real-world deployment, and their inclusion would provide a more balanced perspective on RaCNN\u2019s efficiency."
            },
            "questions": {
                "value": "1. Could the authors provide additional evidence to support RaCNN\u2019s claims of noise resistance and contextual feature capture? Specifically, how does the ERF visualization change across different layers of RaCNN, and would deeper layer results reinforce the conclusions drawn from Figure 2?\n\n2. How does RaCNN perform in terms of inference speed and memory usage compared to models like Swin Transformer and MogaNet in real-time applications?\n\n3. Have the authors considered exploring alternative region partitioning strategies for RPWConv, such as using adaptive region sizes, to enhance flexibility across different image types?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel convolutional neural network (CNN) model called Region-aware CNN (RaCNN), designed to simulate a large receptive field with minimal computational overhead. RaCNN achieves this by replacing the standard feedforward neural network (FNN) with RaGLU and the original depthwise convolution (DWConv) with Region PWConv (RPWConv), allowing the model to capture long-range visual dependencies. The paper demonstrates the effectiveness of RaCNN through experiments on classification, detection, and segmentation tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The motivation of the paper is valuable. The high computational complexity of large kernel size is indeed a barrier in the field of computer vision.\n\nThe proposed RaCNN model shows effectiveness across various tasks, as evidenced by the experimental results."
            },
            "weaknesses": {
                "value": "The contribution of this work is not particularly groundbreaking. Compared to previous methods like [], the advantages of RaCNN are not significantly clear.\n\nSome expressions in the paper lack precision. For example, the distinction between \u201cFLOPS\u201d and \u201cFLOPs\u201d on page 8, line 421, is not handled correctly. I recommend a thorough review of the manuscript.\n\nEquation (6) appears to be incorrect; \u201cx\u00b7x\u201d cannot be computed as written. Please verify all equations."
            },
            "questions": {
                "value": "The paper lacks detailed explanation regarding the advantages of dilated windows in Region PWConv. Could you provide more clarification on this?\n\nWhat would be the impact of using Dynamic PWConv instead of Region PWConv? Additional ablation studies would strengthen the validation of your method.\n\nIn my view, Figure 2(e) does not effectively demonstrate RaCNN\u2019s ability to capture long-range dependencies.\n\nIn Table 8, row 4 (which uses both ReFFN and RaGLU) reports fewer FLOPs and parameters than rows 2 and 3, which is puzzling. Could you explain this discrepancy?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}