{
    "id": "dWEDLUmddV",
    "title": "Enhancing Dataset Distillation with Concurrent Learning: Addressing Negative Correlations and Catastrophic Forgetting in Trajectory Matching",
    "abstract": "Dataset distillation generates a small synthetic dataset on which a model is trained to achieve performance comparable to that obtained on a complete dataset. Current state-of-the-art methods primarily focus on Trajectory Matching (TM), which optimizes the synthetic dataset by matching its training trajectory with that from the real dataset. Due to convergence issues and numerical stability, it is impractical to match the entire trajectory in one go; typically, a segment is sampled for matching at each iteration. However, previous TM-based methods overlook the potential interactions between matching different segments, particularly the presence of negative correlations. To study this problem, we conduct a quantitative analysis of the correlation between matching different segments and discover varying degrees of negative correlation depending on the image per class (IPC). Such negative correlation could lead to an increase in accumulated trajectory error and transform trajectory matching into a continual learning paradigm, potentially causing catastrophic forgetting. To tackle this issue, we propose a concurrent learning-based trajectory matching that simultaneously matches multiple segments. Extensive experiments demonstrate that our method consistently surpasses previous TM-based methods on CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet-1K.",
    "keywords": [
        "Dataset Distillation; Efficient Machine Learning; Data-centric AI"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=dWEDLUmddV",
    "pdf_link": "https://openreview.net/pdf?id=dWEDLUmddV",
    "comments": [
        {
            "title": {
                "value": "First reply to reviewer kWAo"
            },
            "comment": {
                "value": "We appreciate the reviewer's constructive feedback, which helps enhance the clarity of our paper! Let us answer your question below.\n\n>**The description of the accumulated trajectory error is not intuitive.**\n\nAccumulated trajectory error represents the distance between the model optimized on the synthetic dataset and the model trained on the real dataset, and it is our ultimate optimization objective. It can be decomposed into two parts: initialization error and the matching error across all segments, and trajectory matching aims at decreasing the matching error. However, previous TM-based methods only optimized the error of a single segment at a time. Reducing the error of a single segment can be seen as one task, and therefore, our goal of reducing the overall matching error can be viewed as a multi-task, multi-objective optimization problem. When there are negative correlations among the objectives (tasks), optimizing only one of them will prevent the overall optimization goal from being effectively reduced.\n\nIn the Appendix E.6 of the rebuttal revision, we add a table to show the definition of notations.\n\n>**The explanation of the negative correlation is not clear, especially when the IPC is low**\n\nIn this work, negative correlation refers to the phenomenon where reducing the matching error of one segment causes the matching error of other parts to increase. At low IPC, the synthetic dataset can contain less information, making this issue more pronounced. Intuitively, we can think of the synthetic dataset as a hard drive, where IPC determines the amount of information that can be stored. When IPC is very low, if there are no constraints, the hard drive, in order to store new content (the trajectory matching of a new segment), will simply forget previously stored information (the segments that have already been matched).\n\n>**The novelty of the concurrent training to tackle the problem is rather limited, and improvement compared to previous state-of-the-art methods is not very significant**\n\n**[novelty of the proposed method]** As mentioned in Lines 347-348, we explored several other techniques. These solutions are clearly more technically novel and complex. However, we ultimately chose the simplest approach\u2014concurrent training. On one hand, concurrent training is simple and effective, and it can be combined with any TM-based methods (Table 3) to yield improvements. We believe that the simpler the method, the stronger the underlying idea\u2019s generality and novelty.\n\nOn the other hand, another core novelty of this work is the theoretical analysis and empirical validation of the existence of negative correlations in trajectory matching, and our method is directly derived from this observation. We believe the discovery of negative correlation can offer valuable insights to the dataset distillation community.\n\n**[improvements compared with SOTA]** ConTra, as a plug-in method, can improve all trajectory matching methods. Compared to another plug-in method, PDD [4], the improvements we achieve are more substantial. For example, when the IPC is 10 on CIFAR-10, the improvement brought by PDD to MTT is 1.5%, whereas ConTra achieves an improvement of 2.5%. Moreover, DATM has already achieved lossless condensation at high IPC, and building upon this, but we still bring further improvements, which is quite challenging. And at small IPC, our improvement is highly significant. When IPC = 1, DATM shows a 0.7% improvement compared to the basic MTT, but we achieved a 3.1% improvement over the SOTA.\n\nIf you have any other questions, we are happy to have further discussions.\n\n## References\n\n[1] Chen, Xuxi, et al. \"Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality.\" The Twelfth International Conference on Learning Representations."
            }
        },
        {
            "title": {
                "value": "First reply to reviewer 1dFc"
            },
            "comment": {
                "value": "We would like to Thank you for your constructive review, as well as your positive feedback! Let us answer your question below.\n\n>**Typos**\n\nThanks for pointing this out. We carefully check the draft and correct these typos in the rebuttal revision.\n\n>**It would be more intuitive if there is a figure to show the differences/advantages of your proposed ConTra compared to the previous TM methods.**\n\nWe did not include a figure to show ConTra in the main text due to page limitations. The core difference between our approach and previous TM-based methods can be summarized in one sentence: previous methods sample and match one segment at a time, while we match multiple segments simultaneously to mitigate the issue of negative correlation\n\n>**The paper does not test the proposed method using different distillation and evaluation model size.**\n\nThanks for this suggestion. In Table 2, we present the performance evaluated with three other convolutional networks of different sizes. In Table 8, we show the results for ViTs of varying sizes.\n\nTo further explore the performance of ConTra using different distillation and evaluation model sizes, we conduct the following experiments (CIFAR-10, IPC=10). We observe that when IPC is small, distilling from ConvNet to ConvNet yields the best results, where D refers to distill, and E refers to evaluate. However, [1] found experimentally that when IPC = 1000, using a larger ResNet for distillation can achieve better performance. This may be because larger and more complex models contain more information, which requires a higher IPC to fully capture.\n\n|D\\E|ConvNet|VGG11|ResNet18|\n|-|-|-|-|\n|ConvNet|68.3|50.6|52.9|\n|VGG11|47.3|42.7|40.6|\n|ResNet18|49.0|41.4|48.1|\n\nIf you have any other questions, we are happy to have further discussions.\n\n### References\n[1] Guo, Ziyao, et al. \"Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching.\" The Twelfth International Conference on Learning Representations."
            }
        },
        {
            "title": {
                "value": "First reply to reviewer cujf"
            },
            "comment": {
                "value": "We would like to thank the reviewer for the constructive comments! Let us answer the questions below.\n\n>**In Section 4.1,  A more detailed exploration of how these relate to negative correlation**\n\nBy deriving the accumulated error, it becomes clear that trajectory matching aims to reduce the matching error of all segments. However, previous TM-based methods only optimized the error of a single segment at a time. Reducing the error of a single segment can be seen as one task, and therefore, our goal of reducing the overall matching error can be viewed as a multi-task, multi-objective optimization problem. When there are negative correlations among the objectives (tasks), optimizing only one of them will prevent the overall optimization goal from being effectively reduced. We do not further define assumptions and theoretically prove this point mainly because this conclusion is common, as similar results are found in continual learning [1], multi-objective optimization [2], and test-time training [3]. Section 4.1 theoretically demonstrates the multi-task nature of trajectory matching, while Section 4.2 empirically verifies the existence of negative correlations across multiple tasks, which leads to our proposed solution.\n\n>**The novelty of the proposed approach**\n\nAs mentioned in Lines 347-348, we explored several other techniques. These solutions are clearly more technically novel and complex. However, we ultimately chose the simplest approach\u2014concurrent training. On one hand, concurrent training is simple and effective, and it can be combined with any TM-based methods (Table 3) to yield improvements. We believe that the simpler the method, the stronger the underlying idea\u2019s generality and novelty. On the other hand, another core novelty of this work is the analysis and empirical validation of the existence of negative correlations in trajectory matching, and our method is directly derived from this observation. We believe the discovery of negative correlation can offer valuable insights to the dataset distillation community.\n\n>**The performance gains observed in the experiments are somewhat disappointing**\n\nConTra, as a plug-in method, can improve all trajectory matching methods. Compared to another plug-in method, PDD [4], the improvements we achieve are more substantial. For example, when the IPC is 10 on CIFAR-10, the improvement brought by PDD to MTT is 1.5%, whereas ConTra achieves an improvement of 2.5%. Moreover, DATM has already achieved lossless condensation at high IPC, and building upon this, but we still bring further improvements, which is quite challenging. And at small IPC, our improvement is highly significant. When IPC = 1, DATM shows a 0.7% improvement compared to the basic MTT, but we achieved a 3.1% improvement over the SOTA.\n\nCompared to DATM, a single iteration of ConTra is slower, but due to its faster convergence (according to Table 2) and more stable training (Figure 5), it does not incur much additional cost in practice.\n\nIf you have any other questions, we are happy to have further discussions.\n\n## References\n\n[1] Kirkpatrick, James, et al. \"Overcoming catastrophic forgetting in neural networks.\" Proceedings of the national academy of sciences 114.13 (2017): 3521-3526.\n\n[2] Crawshaw, Michael. \"Multi-task learning with deep neural networks: A survey.\" arXiv preprint arXiv:2009.09796 (2020).\n\n[3] Sun, Yu, et al. \"Test-time training with self-supervision for generalization under distribution shifts.\" International conference on machine learning. PMLR, 2020.\n\n[4] Chen, Xuxi, et al. \"Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality.\" The Twelfth International Conference on Learning Representations."
            }
        },
        {
            "summary": {
                "value": "This paper addresses dataset distillation, aiming to create small synthetic datasets that enable models to achieve comparable performance to training on complete datasets. Due to convergence and stability challenges, Trajectory Matching methods typically match only segments of training trajectories, but they overlook negative correlations between different segments. The authors quantitatively analyze these correlations, finding that negative correlations can increase trajectory error and lead to catastrophic forgetting. To address this, they propose a concurrent learning-based TM approach that matches multiple segments simultaneously. Experiments show that this method outperforms previous approaches across various datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper clearly identifies the problem, providing both theoretical and experimental analysis to demonstrate the existence of negative correlations between trajectory segments.\n\n2. All the discussions in the paper are clear and straightforward.\n\n3. The experiments contains all the necessary components with enough discussion"
            },
            "weaknesses": {
                "value": "1. This paper includes a theoretical analysis of negative correlation; however, the theory presented in Section 4.1 primarily illustrates that training errors can accumulate across segments. While this is an important observation, it is not directly related to the negative correlation itself. A more detailed exploration of how these relate to negative correlation would strengthen the theoretical foundation of the proposed method.\n\n2. The novelty of the proposed approach appears to be somewhat limited. To mitigate negative correlation, the method simply trains multiple segments concurrently, which is a strategy commonly employed as a baseline in continual learning scenarios.\n\n3. The performance gains observed in the experiments are somewhat disappointing, particularly when considering the increased computational requirements associated with the proposed method. Given the additional complexity introduced, one would expect a more substantial improvement in performance to justify the costs involved (especially when compared to DATM). This raises questions about the effectiveness of the approach in real-world applications."
            },
            "questions": {
                "value": "See strengths and weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper analyzes the challenges of dataset distillation, i.e., negative correlation between different segments of a trajectory to match and catastrophic forgetting problem. To address this, the authors formulate trajectory matching as a continual learning problem and propose a method called Concurrent Training-based Trajectory Matching (ConTra). It employs multi-task learning to simultaneously match multiple segments, rather than sequential learning used in previous works. Experimental results show that ConTra consistently outperforms existing trajectory matching methods on various datasets, thus demonstrating its ability to minimize accumulated matching errors and achieve lossless condensation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe analysis on negative correlation between different trajectory segments is detailed and solid, enriching the discourse on continual learning.\n2.\tThe idea of utilizing concurrent learning to tackle negative correlation is simple but novel.\n3.\tExtensive experiments on multiple datasets and downstream tasks are quite convincing."
            },
            "weaknesses": {
                "value": "1.\tThe paper needs to be further polished. There are numorous typos, such as line 140 \u2018a expert\u2019->\u2019an expert\u2019, line 457 \u2018s the range\u2019->\u2019so the range\u2019.\n2.\tIt would be more intuitive if there is a figure to show the differences/advantages of your proposed ConTra compared to the previous TM methods. \n3.\tThe paper does not test the proposed method using different distillation and evaluation model size."
            },
            "questions": {
                "value": "1.\tIs the proposed method sensitive to the model size used for distillation and evaluation? \n2.\tDoes the distillation training time decease when using concurrent learning compared to sequential learning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tackles the problem that matching between different segments of the training trajectory may be negatively correlated. Existing methods use Trajectory Matching (TM), which optimizes the synthetic dataset by matching its training trajectory with the real dataset. However, they overlook the negative correlation between different trajectory segments, leading to performance degradation. The authors propose a concurrent learning-based TM method that matches multiple segments simultaneously, reducing errors. With exhaustive experiments, their approach outperforms previous methods across several benchmark datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "[1] The writing of paper is good.\n\n[2] The analysis of the negative correlation on accumulated trajectory error is comprehensive. For example, the author clearly specifies the accumulated error to initialization error and matching error. Afterwards, the authors calculate the correlation to validate the phenomenon.\n\n[3] The proposed cocurrent training methods is reasonable. The experimental results validate the effectiveness of the proposed methods."
            },
            "weaknesses": {
                "value": "[1] The description of the accumulated trajectory error is not intuitive. The notations defined in Sec. 4.1 are complex and there is no graphic illustration of these notations.\n\n[2] The explanation of the negative correlation is not clear, especially when the IPC is low. In addition, the roles of training dynamics are not validated with experimental evidence.\n\n[3] The novelty of the concurrent training to tackle the problem is rather limited. While reasonable, the author only simply leverages the multitask learning to tackle the problem. Since the improvement compared to previous state-of-the-art methods is not very significant, the contribution is not above the acceptance bar of ICLR."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}