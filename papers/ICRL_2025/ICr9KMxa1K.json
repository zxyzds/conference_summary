{
    "id": "ICr9KMxa1K",
    "title": "ART: Actor-Related Tubelet for Detecting Complex-shaped Action Tubes",
    "abstract": "This paper focuses on detecting complex-shaped action tubes in videos. Existing methods are based on the assumption that actor's position changes slightly in short video clips. These methods either oversimplify the shape of action tubes by representing them as cuboids or conjecture that action tubes can be summarized into a set of learnable positional patterns. However, these solutions may be insufficient when actor trajectories become more complex. This limitation arises because these methods rely solely on position information to determine action tubes, lacking the ability to trace the same actor when their movement patterns are intricate. To address this issue, we propose Actor-related Tubelet (ART), which incorporates actor-specific information when generating action tubes. Regardless of the complexity of an actor's trajectory, ART ensures that an action tube consistently tracks the same actor, relying on actor-specific cues rather than solely on positional information. To evaluate the effectiveness of ART in handling complex-shaped action tubes, we introduce a dedicated metric that quantifies tube shape complexity. We conduct experiments on three commonly used tube detection datasets: MultiSports, UCF101-24 and JHMDB51-21. ART presents remarkable improvements on all the datasets.",
    "keywords": [
        "human action recognition",
        "action tube localization"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "We propose Actor-related Tubelet (ART), which incorporates actor-specific information for generating action tubes with complex shapes.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ICr9KMxa1K",
    "pdf_link": "https://openreview.net/pdf?id=ICr9KMxa1K",
    "comments": [
        {
            "summary": {
                "value": "The paper proposed an approach to solve action detection via tubelet based approach. The paper proposes a metric to compute the complexity of the datasets based on tubelet IoUs. Tubelet query Generator helps for actor tubelet queries to boost the performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed approach outperforms previous approaches on all datasets."
            },
            "weaknesses": {
                "value": "Novelty - \n- (Section 1 Introduction - Lines: 98-102) In the current settings, precise information about the actor location is provided by analyzing the video beforehand. What does this mean? Does the model know the actor locations in all the keyframes? These settings are not consistent with previous approaches [TubeR, EVAD, STAR, BMViT]. Are the previous approaches evaluated in similar settings - where the actor/action locations are provided and then the model is trained and evaluated? How would the model work without these annotations provided beforehand?\n- (Section 3.1 Tube shape Complexity) How is the proposed metric different from [1]? Is it a direct extension from [1]? Can authors please elaborate on this?\n- (Section 3.3 Actor-Related Tubelet) The proposed approach looks very similar to TubeR each module. I understood the difference between position vs actor against TubeR. Tubelet Query Generator module - It\u2019s not clear how it is different from TubeR\u2019s Tubelet attention module. Can authors please clarify? Also, the complexity analysis is also not clear. Looks like TubeR should have the similar complexity. \n\n(Results & Ablations) \n- Ablation Study - There is only one ablation study on temporal compensation for the whole model with just one model and one dataset. Can authors please provide more ablations for different datasets or with different models? The proposed module shows a very minor performance boost for 0.5%. It will be helpful to see if this is the same pattern with other models/datasets.\n- Is the results on AVA comparable since the paper is using a better visual backbone?  Without ablations, it is hard to prove that it\u2019s not just a good performance due to better backbone. AVA is a very difficult dataset where an actor comes and goes out of the scene. How are the tubelet of an actor maintained across time?\n- (Sec 4.1 - Scenario with Multiple Actors) Does knowing the location of the bounding box helps in performance boost for difficult classes such as Basketball, Basketball Dunk, etc.? Is there any analysis of how prior knowledge helps?\n- UCF and JHMDB don't have the issue of actors appearing and disappearing from the video. Do Multisports have this issue? If yes, how is the paper handling the issue that the same tubelet id is assigned similar to any offline tracker?\n- (Section 4.1 Actor decoder vs Offline person detector) Does the ablation study show the detector+tracker with some base ReID model to track appearance features? Without a ReID model, the tracker mixes up IDs and leads to false tubelet generation because of ID switching."
            },
            "questions": {
                "value": "- Could the approach of the tubelet query generator be applied to other works? If so, how? If not, why not? The module looks pretty flexible to be adaptable for video action detection modules. \n- Can authors also please show or share f-mAP results on UCF, JHMDB, and Multisports? It will be helpful to compare it to recent works that use frame-based approaches.\n\nPlease look into the weakness section for other queries. The weaknesses are listed in priority from high to low."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces actor-related tubelet (ART) for spatial-temporal action localization. It aim to improve the performance in complex scenes. The framework is based on transformer architectures.  To distinguish different actors, the authors employ ROI align to extract actor features using key frame features from the decoder and extend these features temporally to build actor-related queries. The proposed method obtained competitive results on JHMDB51-21, UCF101-24, and MultiSports."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) Different from position-based methods, the proposed ART leverages actor features to distinguish different actors in complex scenes\n2) The proposed method shows significant improvements on complex activities."
            },
            "weaknesses": {
                "value": "1) Notations in Section 3.1 are confusing, variable names and indexes are tangled together.\n2) The loss function in Eq. 9 is incorrect, please proofread the equations.\n3) Simple action with large motion can also lead to high complexity in Eq. 2\n4) The key of the method is actor tubelet query, but there are few discussion of actor-related feature except using the ROI align method in (He et al., 2017)\n5) In Figure 6, the performance of many actions using actor-related methods are worse that the ones using position-related methods. And which method is used for the blue bar?"
            },
            "questions": {
                "value": "1) Previous also use DETR to detect actors for generating queries, why they cannot capture actor related information?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Actor-related Tubelets (ART), an action tublet centric approach for detecting complex-shaped action tubes in videos. ART generates \"actor tubelet queries\" by extracting ROI-aligned keyframe features with temporal compensation, which are then passed to an Action Decoder to produce action tubelet boxes and scores. The model is evaluated on the MultiSports, UCF101-24, and JHMDB51-21 datasets, where it demonstrates decent results."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. Performance and Analysis:\n\nThe reported performance of ART on key datasets is at least decent, and the analysis provided is informative, especially the performance breakdown based on the proposed tube complexity criterion.\n\n2. Metric Innovation:\n\nThe proposed metric for measuring tube complexity is a valuable addition."
            },
            "weaknesses": {
                "value": "1. Questionable Omission of Frame-mAP:\n\nThe authors\u2019 decision to exclude frame-mAP results is questionable and limits comparability with prior work in spatio-temporal action recognition. While ART focuses on action tubelets, the tubelet output can certainly be converted to framewise bounding boxes. If the tubelet predictions are indeed high-quality, one would expect the framewise mAP to reflect that as well. Since ART does not propose a new task or benchmark, adhering to conventional evaluation schemes, including frame-mAP, would offer a more transparent assessment of the model\u2019s performance. This addition would also facilitate direct comparison with existing literature, adding further validity to the claims made in the paper.\n\n\n2. Potentially Misleading Omission of Key Comparisons:\n\nTable 5 primarily compares ART with older works, despite the authors having cited recent influential methods such as \u201cEnd-to-End Spatio-Temporal Action Localisation with Video Transformers\u201d and \u201cHolistic Interaction Transformer Network for Action Detection.\u201d The omission of these contemporary approaches from the main performance table is problematic, as it may obscure ART's comparative impact. Including these methods would present a more comprehensive and honest performance evaluation.\n\n3. Moderate Impact of Architectural Changes:\n\nWhile ART introduces an architectural modification that addresses positional constraints, this adaptation alone may not be groundbreaking. ART\u2019s performance gains, while respectable, do not demonstrate a transformative leap, especially if compared directly to the omitted recent works. The improvements seen with ART suggest more of an incremental contribution rather than a paradigm shift in action tube detection."
            },
            "questions": {
                "value": "Please see weakness section. Not that important but too many underbars in Sec 3.1 equations seem somewhat ackward."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces an interesting problem on detecting the complex-shaped  action tubes.\n\nThe main motivation is that the authors notice that existing methods rely only on the position information to determine the action tubes, lacking the ability to trace the same actor when their movement patterns are complicated.\n\nThey introduce actor-related tubelet (ART) which incorporates the actor-specific information when generating the action cubes.\n\nThey perform evaluation on 3 benchmark datasets and show that the proposed method achieves better performance compared to existing methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+ Good and clear motivation, with nice visualisations presented in the paper, eg, Fig. 1\n\n+ The authors conduct some interesting experiments, analysis and comparisons, such as Fig. 3.\n\n+ Experimental results, especially those presented in tables, show that the proposed model achieves better performance. The authors also provide some insights and discussions regarding the proposed model."
            },
            "weaknesses": {
                "value": "Major:\n\n- Page 1, why \u201ceasy actions like brushing hair/teeth \u2026 that are spatially stationary or follow predictable patterns\u201d? These motions are just subtle, still considered to be complicated eg the hair movements during brushing process, etc. The descriptions and claims make in the paper needs to be more precise and clear to readers.\n\n- What are the main reasons that cause eg deformable shape in tubes? Would the camera distance, and human subject movement eg far away from camera, or move close camera, can be considered as deformable shape in tubes where human subject sizes changed during the process and eg when the human subject is partially inside the video frame? The authors should make these concepts clear enough.\n\n- Related work section is not polished. It shows significant redundancies especially when the name and citation being repeated in the whole section. The review of closely related works are a bit limited. What are the existing works that doing the action tube detection. The review is also not comprehensive and thorough, a good review should outline the major differences between the method introduced and the existing closely related works.\n\n- Sec. 3, while these metrics and concepts are explained, they are not clear enough to readers. The authors are encouraged to use figures to explain these concepts. Moreover, the method section is not that well-written and well-structured. It is suggested to add a notation section detailing the maths symbols and operations used in the paper to make them clear to readers. The maths symbols presented are also quite raw. What is \u201c*\u201d in Eq. (5), and is it the same as Line 282-283? In Eq. (6), would a weighted sum works? \n\n- Page 5 top, what does it mean \u201cJHMDB is excluded from this plot as it features only a single actor per video\u201d? Does that mean it is unable to visualise it in Fig. 3b?\n\n- The paper is presented in an unnecessarily complicated way, for example, Fig. 4 is quite confusing. The authors should be more specific regarding each maths symbol introduced in texts and figures.  The reviewer gets lost many times and still cannot get the core innovations and insights from the method section, although the motivation is quite interesting.\n\n- Regarding the evaluation metrics, the authors do not clearly explain what is video-mAP, and how this evaluation metric contributes to the performance evaluation of the method.\n\n- Section 4.3, although the authors presented some visualisations, the analysis, evaluations, comparisons and discussions are very limited to 2 sentences. An excellent paper should provide enough insights that are able to deliver powerful and useful information/insights to readers.\n\nMinor: \n\n- Page 6, what is the \u201cps\u201d value exactly?\n\n- How those hyperparameters are selected, eg, in Eq (9), a reference referring to existing works or how empirically choosing these values would be much clearer and useful."
            },
            "questions": {
                "value": "Please refer to my main comments above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The research experiments conducted in this paper use publicly available datasets. No ethics review needed."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}