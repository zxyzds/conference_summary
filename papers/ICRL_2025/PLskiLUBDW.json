{
    "id": "PLskiLUBDW",
    "title": "Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional, Black-box Systems",
    "abstract": "Efficient inference in high-dimensional models is a central challenge in machine learning.\nWe introduce the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, which combines the strengths of the Ensemble Kalman Filter (EnKF) and Gaussian Belief Propagation (GaBP) to address this challenge.\nGEnBP updates ensembles of prior samples into posterior samples by passing low-rank local messages over the edges of a graphical model, enabling efficient handling of high-dimensional states, parameters, and complex, noisy, black-box generation processes.\nBy utilizing local message passing within a graphical model structure, GEnBP effectively manages complex dependency structures and remains computationally efficient even when the ensemble size is much smaller than the inference dimension--a common scenario in spatiotemporal modeling, image processing, and physical model inversion.\nWe demonstrate that GEnBP can be applied to various problem structures, including data assimilation, system identification, and hierarchical models, and show through experiments that it outperforms existing methods in terms of accuracy and computational efficiency.",
    "keywords": [
        "factor graph",
        "data assimilation",
        "ensemble kalman filter",
        "gaussian belief propagation",
        "geospatial",
        "probabilistic graphical model",
        "hierarchical model"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "If we carefull combine the structures of the Ensemble Kalman Gfilter and Gaussian Belief Propagation, the result is an efficient method for inference in hierarchical spatial systems",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=PLskiLUBDW",
    "pdf_link": "https://openreview.net/pdf?id=PLskiLUBDW",
    "comments": [
        {
            "title": {
                "value": "Part 1b: Style and language"
            },
            "comment": {
                "value": "Continuing part 1a\n\n- Line 353: \u201calgorithm\u201d not capitalised. This is also the case in many other places.\n    - We have updated the manuscript so that Table, Section, Figure, Appendix, Equation, Definition, and Algorithm are all upper-case, as they are names.\n- Line 376: Incorrect grammar: \u201cFor a variable with K neighbors, in the worst case M = KN , when the cost becomes\u2026\u201d and missing punctuation.\n    - Corrected this sentence to read \"For a variable with $K$ neighbors, in the worst case $M = K N$, and the cost becomes $\\mathcal{O}(N^3 + D N^2 K^2)$.\"\n- Line 381: How about \u201cIn many practical applications it holds that N \u226a D, resulting in significant computational savings compared to GaBP which scales poorly with D.\u201d.\n    - Changed as per your suggestion.\n- Line 385: How about \u201cbetter suited\u201d?\n    - Changed as per your suggestion.\n- Line 404/405: \"While GEnBP scales favorably with D, but unfavourably with respect to the node degree K, scaling as O(K3) for some operation\n    - Agreed, this is unnecessarily muddy. We have update it to read \u201cGEnBP operations scale favourably with $D$, at worst $\\mathcal{O}(D)$, compared to the worst $\\mathcal{O}(D^3)$ for GaBP. They scale unfavourably with respect to the node degree, at worst $\\mathcal{O}(K^3)$, compared to GaBP\u2019s $\\mathcal{O}(1)$.\u201d\n\nWe look forward to further engaging with you during the rebuttal period, should you have any additional queries. Please let us know if you have any further concerns."
            }
        },
        {
            "title": {
                "value": "Part 1a: Style and language"
            },
            "comment": {
                "value": "Thanks very much for your detailed review. We are glad that you think the paper makes valuable contributions, and presents a novel methodology with well motivated approximations and algorithmic structure.\nWe address your individual comments below.\nWe hope that this satisfies your concerns about clarity of the experiments and language errors.\n\n- > Line 22: no spaces around \u201c\u2013\u201d.\n    - Fixed, and now correctly use an em dash ---\n- Line 48: why \u201cseems\u201d novel and not \u201cis\u201d novel, if that is the case?\n    - Indeed, their combination *is* novel, as demonstrated in the manuscript and mentioned by you and other reviewers. We have corrected this.\n- Line 68: \u201ctable\u201d is not capitalised. This is also the case in many other places. Note that it should be capitalised since \u201cTable 1\u201d is a name.\n    - We have updated the manuscript so that Table, Section, Figure, Appendix, Equation, Definition, and Algorithm are all upper-case, as they are names.\n- Line 71: \u201csection\u201d not capitalised. This is also the case in many other places.\n    - We have updated the manuscript so that Table, Section, Figure, Appendix, Equation, Definition, and Algorithm are all upper-case, as they are names.\n- Line 72: missing punctuation after \u201capproximations\u201d.\n    - We have added a full stop after \"approximations\".\n- Line 81-95: In my opinion, excessive use of bolds. Is it needed more than say a couple of times here, or even at all?\n    - Agreed. We have removed the bolds, as the itemised list already makes the statements stand out sufficiently well.\n- Line 83: How about \u201ca novel message-passing method for inference in high-dimensional graphical models\u201d?\n    - Agree this wording is cleaner. Have updated this phrase as you suggest.\n- Line 91: \u201cRank\u201d is (incorrectly) capitalised even though the list is written as a sentence.\n    -We have fixed capitalisation of \"rank\"\n- Line 93: Same as above but for \u201cCompute\u201d.\n    - We have fixed capitalisation of \"compute\"\n- Line 99: How about \u201cWe will now introduce our notation and essential concepts\u201d?\n    - Fixed, as per your suggestion.\n- Line 100: \u201cappendix\u201d is not capitalized. This is also the case in many other places.\n    - We have updated the manuscript so that Table, Section, Figure, Appendix, Equation, Definition, and Algorithm are all upper-case, as they are names.\n- Line 118: How about \u201cWe will assume that queries are always ancestral variables, i.e. that \u2026\u201d?\n    - Agree that this better describes our setting. We have updated as you suggest.\n- Line 134: \u201cequation\u201d is not capitalized. This is also the case in many other places.\n    - We have updated the manuscript so that Table, Section, Figure, Appendix, Equation, Definition, and Algorithm are all upper-case, as they are names.\n- Line 146: Missing word 'with'. \u201cEdges connect each factor node j with each variable node\u2026\u201d?\n    - Well-spotted. We have added the missing \"with\".\n- Line 173: Unnecessary abbreviations, \u201cwe\u2019ve\u201d and \u201cwe\u2019re\u201d.\n    - Removed abbreviations and contractions so \"we've\" is now we have and \"we're\" is now \"we are\".\n- Line 180: observed is misspelt.\n    - Updated to correct spell \"observed\".\n- Line 184: \u201cfigure\u201d not capitalised. This is also the case in many other places.\n    - We have updated the manuscript so that Table, Section, Figure, Appendix, Equation, Definition, and Algorithm are all upper-case, as they are names.\n- Line 191: Missing punctuation before \u201cThe following\u201d.\n    - Added missing full stop before \"The following\".\n- Line 202: \u201cdefinition\u201d not capitalised. This is also the case in many other places.\n    - We have updated the manuscript so that Table, Section, Figure, Appendix, Equation, Definition, and Algorithm are all upper-case, as they are names.\n- Line 227: Double punctuation.\n    - Removed the extra full stop.\n- Line 261: Missing punctuation before \u201cMarginalization\u201d.\n    - Added the missing full stop before \"Marginalization\".\n- Line 294: Missing punctuation before \u201cThroughout\u201d.\n    - Verified that in the updated manuscript, \"Throughout\" is preceeded by a full stop.\n- Line 312: Missing punctuation before \u201cHere\u201d.\n    - Verified correct placement of punctuation before \"Here\".\n- Line 314: Misspelt equivalently.\n    - Fixed spelling of equivalently.\n- Line 323: Incorrect grammar, \u201cSee appendix I for a comparison with a naive attempt to do without the ensemble\u201d.\n    - Whilst there is a grammatical parse of this sentence, we agree that it is unnecessarily difficult to deduce it. Updated to read \u201cAppendix 1 compares the computational cost of the GEnBP with an attempt to exploit DLR factorisations without using an ensemble.\u201d\n- Line 325: Missing punctuation before \u201cThen\u201d.\n    - Added full stop before \"Then\".\n- Line 336: Missing punctuation after \u201chigh dimensions\u201d.\n    - Added full stop after \"high dimensions\"."
            }
        },
        {
            "summary": {
                "value": "The authors introduce a novel method named Gaussian Ensemble Belief Propagation (GEnBP). They combine EnKF and GaBP together, in order to improve the efficiency in high-dimensional and non-linear cases. The method uses low-rank message-passing algorithms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The authors try to use the strength of GaBP to solve (or I would like to say mitigate) the bad performance of EnKP in high-dim and non-linear problems. In the given examples, the new method works well. The computational complexity reduces largely for the dimensions of the data. It seems that the paper demonstrates superiority over existing methods in various benchmarks (but the figures are hard to read). I believe this method is potential for real-world applications for example in geospatial modeling."
            },
            "weaknesses": {
                "value": "1. Most important question for me: the scalability of the method to extremely large models, like weather simulations, remains to be fully demonstrated. Computational complexity still depends heavily on the degree of nodes in the graphical model, which means you must restrict the scale of the model. However, models in climate research always be extremely huge. How to choose N, how to choose K? If too small, can the model catches the nonlinearity well?\n2. EnKF is somehow popular in climate or geophysical models, because of its computational complexity. But GaBP, in my impression, is a little bit outdated. For the nonlinear cases, we may use variational inference, or particle filter. Have you compared with them?\n3. The innovation: GaBP mostly for low-dim problem. I am not sure it can help to improve the EnKF itself. More importantly, please see Bickson et al (2008) for the similar idea.\n4. Even for small model, for example, Laplace seems better after consideration of trade-off. And Fig 2(c) seems not complete."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces the GEnBP algorithm, a method that combines Ensemble Kalman Filter (EnKF) and Gaussian Belief Propagation (GaBP) to efficiently perform inference in high-dimensional systems. GEnBP updates ensembles of prior samples into posterior samples using a message-passing strategy on a graphical model, making it suitable for applications with complex dependencies and high-dimensional variables, such as in geospatial and physical models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-structured, making it easy to follow the ideas presented. The proposed method can be considered to be novel in its approach to addressing the computational challenges of GaBP in high-dimensional systems."
            },
            "weaknesses": {
                "value": "While GEnBP demonstrates strong performance in high-dimensional contexts, its complexity scales disproportionately with the node degree in graphical models. This presents a significant limitation in highly connected graphs, where methods such as Forney factorization have been proposed by the authors as partial solutions. However, the effectiveness of these methods remains uncertain. Can some simple numerical results or insightful analyses be added to showcase this potential solution?\n\nAdditionally, a notable drawback is the absence of comparative analysis with other related works, both those employing and not employing EnKF, for efficient inference in high-dimensional, black-box systems. Key related works include:\n\n  - Chen Y, Sanz-Alonso D, Willett R. \"Autodifferentiable Ensemble Kalman Filters,\" *SIAM Journal on Mathematics of Data Science*, 2022; 4(2):801-833.\n\n  - Chen Y, Sanz-Alonso D, Willett R. \"Reduced-order Autodifferentiable Ensemble Kalman Filters,\" *Inverse Problems*, 2023; 39(12):124001.\n\n  - Lin Z, Sun Y, Yin F, Thi\u00e9ry A. \"Ensemble Kalman Filtering-Aided Variational Inference for Gaussian Process State-Space Models.\" *arXiv preprint arXiv:2312.05910*, 2023.\n\n  - Girin L, Leglaive S, Bie X, Diard J, Hueber T, Alameda-Pineda X. \"Dynamical variational autoencoders: A comprehensive review.\" *arXiv preprint arXiv:2008.12595*, 2020.\n\nIncluding a discussion or comparison on system identification and data assimilation from these (and other existing) works would enhance the comprehensiveness of the proposed method's positioning."
            },
            "questions": {
                "value": "1. Figure 2 is not cited in the text, which may confuse readers\n2. Consider put Figure 1 in the experimental section\n3. The paper highlights potential applications in fields like geospatial prediction and physical model inversion, which may involve real-time data. How does GEnBP handle streaming data, and what would be required to extend it to real-time applications with time-varying dependencies?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a generalisation of the ensemble Kalman filter to make inference on arbitrary graphical models using belief propagation  (BP) with ensembles. By relying on ensembles rather than keeping track of the full moments, the method is able to scale to problems where the state dimension of the nodes are high dimensional, e.g. weather forecasting. The authors propose several numerical tricks that make this possible while retaining scalability. This is tested against a standard Gaussian BP on some examples, showcasing its computational efficiency and performance improvements, offered by its improved handling of nonlinear conditional dependence between nodes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is generally well-written and well-motivated. The methodology presented, while seemingly a small step beyond standard BP, is not at all trivial to execute, requiring various tricks to make it work in practice. In particular, I find the efficient computation of the factor-to-variable message and the ensemble conformation step to be an interesting contribution that is essential to retain the ensemble-based representation of the beliefs. The experiments showcase interesting settings where this technique could be applied, such as latent forcing identification in fluid models. In addition, it shows clear benefits of the approach over vanilla BP, with orders of magnitude increase in computation speed and performance gains."
            },
            "weaknesses": {
                "value": "While the paper is clearly written for the most part, there are some parts that I find need more explanation. See my questions below for details. I also find that the experiments can be improved with further baseline comparisons. Is GaBP really the only baseline that can solve the system identification problems in the experiments? One may probably also consider methods like the integrated nested Laplace approximation (INLA), or particle MC methods (e.g. the particle marginal Metropolis-Hastings algorithm)."
            },
            "questions": {
                "value": "- It would be great if the authors could please clarify why we need a particle representation of the beliefs when we can just make computations with the DLR representation? My understanding is that in EnKF, it is used to propagate the particles by the nonlinear dynamics model and update them using Matheron's rule, but what does this correspond to in the general BP framework (in terms of steps (5)-(7))?\n- Do we need tricks like covariance localisation and inflation in this general BP setting? These are almost always used to make EnKF work robustly in high dimensions.\n- In Figure 2, why are some results missing for GaBP and Laplace method in the log-likelihood comparison? Was it producing NaNs due to overconfident predictions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Efficient inference is challenging in graphical models with high-dimensional variables. \nThe authors introduce an inference algorithm based on Gaussian Ensemble Belief Propagation (GEnBP) and the Ensemble Kalman Filter (EnKF), combining strengths of both algorithms to arrive at an algorithm which is competitive in accuracy, in scalability with respect to the dimensionality of the variables, and which can be applied to a wide range of inference problem structures."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The authors present a novel methodology with well motivated approximations and algorithmic structure, where important connections and details are highlighted. The methodology contains contributions and explanations that are valuable also on their own. Graphical illustrations and notation is used in an effective manner to improve clarity, and the text is well structured."
            },
            "weaknesses": {
                "value": "Although I find the text to be well structured in terms of its content, the text contains many language errors, forgotten punctuation, and incorrect figure references. I have listed some errors I encountered below which the authors may find helpful for improvement of the manuscript.  \n\nList errors/suggestions style and language:\n- Line 22: no spaces around \u201c\u2013\u201d. \n- Line 48: why \u201cseems\u201d novel and not \u201cis\u201d novel, if that is the case? \n- Line 68: \u201ctable\u201d is not capitalised.  This is also the case in many other places. Note that it should be capitalised since \u201cTable 1\u201d is a name.   \n- Line 71: \u201csection\u201d not capitalised.  This is also the case in many other places. \n- Line 72: missing punctuation after \u201capproximations\u201d.\n- Line 81-95: In my opinion, excessive use of bolds. Is it needed more than say a couple of times here, or even at all?\n- Line 83: How about \u201ca novel message-passing method for inference in high-dimensional graphical models\u201d?\n- Line 91: \u201cRank\u201d is (incorrectly) capitalised even though the list is written as a sentence. \n- Line 93: Same as above but for \u201cCompute\u201d. \n- Line 99: How about \u201cWe will now introduce our notation and essential concepts\u201d?\n- Line 100: \u201cappendix\u201d is not capitalized. This is also the case in many other places. \n- Line 118: How about \u201cWe will assume that queries are always ancestral variables, i.e. that \u2026\u201d?\n- Line 134: \u201cequation\u201d is not capitalized.  This is also the case in many other places. \n- Line 146: Missing word 'with'. \u201cEdges connect each factor node j with each variable node\u2026\u201d? \n- Line 173: Unnecessary abbreviations, \u201cwe\u2019ve\u201d and \u201cwe\u2019re\u201d.\n- Line 180: observed is misspelt.\n- Line 184: \u201cfigure\u201d not capitalised.  This is also the case in many other places. \n- Line 191: Missing punctuation before \u201cThe following\u201d.\n- Line 202: \u201cdefinition\u201d not capitalised. This is also the case in many other places. \n- Line 227: Double punctuation. \n- Line 261: Missing punctuation before \u201cMarginalization\u201d. \n- Line 294: Missing punctuation before \u201cThroughout\u201d. \n- Line 312: Missing punctuation before \u201cHere\u201d. \n- Line 314: Misspelt equivalently. \n- Line 323: Incorrect grammar, \u201cSee appendix I for a comparison with a naive attempt to do without the ensemble\u201d.\n- Line 325: Missing punctuation before \u201cThen\u201d. \n- Line 336: Missing punctuation after \u201chigh dimensions\u201d. \n- Line 353: \u201calgorithm\u201d not capitalised. This is also the case in many other places. \n- Line 376: Incorrect grammar: \u201cFor a variable with K neighbors, in the worst case M = KN , when the cost becomes\u2026\u201d and missing punctuation. \n- Line 381: How about \u201cIn many practical applications it holds that N \u226a D, resulting in significant computational savings compared to GaBP which scales poorly with D.\u201d. \n- Line 385: How about \u201cbetter suited\u201d?\n- Line 404: Incorrect grammar \u201cWhile GEnBP scales favorably with D, but unfavourably with respect to the node degree K, scaling as O(K 3 ) for some operations.\u201d\n\n\nThe presentation of the experiments is unclear at times, please see Questions."
            },
            "questions": {
                "value": "Regarding Section 4.1:\n- Figure 2 is never referenced. Although I assume the first paragraph describes observations made from it? \n- Why is the log likelihood only shown for GaBP in the case of the lowest dimensionality setting used? \n- I struggle to reach the following interpretation based on Figure 2: \u201cWe see that the Laplace approximation \u2026 but its posterior likelihood, while similar to GaBP, is even less stable, and both are inferior to GEnBP\u201d:\n- - For dimensionalities greater than around 100, it seems GEnBP is underperforming Laplace in terms of log likelihood (while the results provides no comparison to GaBP)? \n- - Why is GaBP and Laplace interpreted as less stable than GEnBP based on this figure? \n\nSection 4.2:\n- Figure 3 is from the text implied to contain results using Langevin MC, but it does not --- only Figure 4 does. \n- The results show (and the text says) that GaBP performs better in terms of log likelihood for high viscosity settings. What do you suspect is the explanation for this? \n\nOverall, I think the paper is good and makes valuable contributions. However, I think the manuscript needs an overhaul to fix the language errors, missing punctuation, and figure references before it is ready for publication. If this is done and the experiments section is clarified I intend to raise my evaluation score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}