{
    "id": "IS7kW28VVt",
    "title": "AlphaQCM: Alpha Discovery with Distributional Reinforcement Learning",
    "abstract": "Finding synergistic formulaic alphas is very important but challenging for researchers and practitioners in finance. In this paper, we reconsider the discovery of formulaic alphas from the viewpoint of sequential decision-making, and conceptualize the entire alpha-mining process as a non-stationary and reward-sparse Markov decision process. To overcome the challenges of non-stationarity and reward-sparsity, we propose the AlphaQCM method, a novel distributional reinforcement learning method designed to search for synergistic formulaic alphas efficiently. The AlphaQCM method first learns the Q function and quantiles via a Q network and a quantile network, respectively. Then, the AlphaQCM method applies the quantiled conditional moment method to learn unbiased variance from the potentially biased quantiles. Guided by the learned Q function and variance, the AlphaQCM method navigates the non-stationarity and reward-sparsity to explore the vast search space of formulaic alphas with high efficacy. Empirical applications to real-world datasets demonstrate that our AlphaQCM method significantly outperforms its competitors, particularly when dealing with large datasets comprising numerous stocks.",
    "keywords": [
        "Distributional Reinforcement Learning",
        "Computational Finance",
        "Formulaic Alpha",
        "Quantiled Conditional Moments",
        "Stock Trend Forecasting"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=IS7kW28VVt",
    "pdf_link": "https://openreview.net/pdf?id=IS7kW28VVt",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces AlphaQCM, a distributional reinforcement learning method designed for the discovery of  the so-called formulaic alphas in financial markets, with a focus on overcoming the challenges of non-stationarity and reward sparsity inherent in alpha-mining. The authors conceptualize alpha discovery as a non-stationary and reward-sparse MDP and employ the quantiled conditional moments (QCM) method to estimate unbiased variances as exploration bonuses. This setup enables efficient exploration of the vast search space in formulaic alpha generation. The paper further demonstrates that AlphaQCM outperforms existing alpha-mining methods, such as AlphaGen and genetic programming, across multiple financial datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The strength of this paper include the development of a new RL-based framework that could potentially address the mentioned financial problem."
            },
            "weaknesses": {
                "value": "I believe this paper has several fundamental weaknesses.\n\nFirst, I find the paper poorly organized and hard to read. This includes several aspects:\n\n1. The authors frequently refer to other sources for definitions of financial terminologies. Given the niche nature of this topic, many researchers are unlikely to be familiar with the setup, and they may not consult external references to understand it fully. As a result, even after multiple readings of Figure 1, I am still unclear on the role of certain tokens.\n\n2. As an expert in theoretical reinforcement learning, I struggled to understand the setup after reading Section 3.2 multiple times. The paper lacks a foundational formulations of the underlying MDP (For example, I didn't see a formal definition of the transition kernel, let alone an explanation of the non-stationarity issue). In my opinion, this significantly affects the paper\u2019s readability and clarity.\n\nSecond, I feel that this paper mainly applies off-the-shelf RL algorithms to a specific financial application. Furthermore, it offers very little insight into why RL is particularly beneficial in this setting or how it advances researchers' understanding of the underlying problems. In other words, the paper lacks depth.\n\nThird, the experiments rely solely on data from the Chinese market, which falls outside the traditional scope of empirical studies in finance, financial economics, or asset pricing. While using Chinese financial data is not inherently problematic, including comparisons with well-established financial datasets would aid in communicating the findings, as researchers could more easily relate the results to their existing understanding. Given that many of these datasets are publicly available, this additional comparison would be straightforward to implement."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a novel approach combining Distributional Reinforcement Learning (DRL) and quantiled conditional moments (QCMs) to derive synergistic formulaic alphas. The alpha-mining problem is important in finance, and the proposed DRL algorithm could be adaptive for non-stationary and reward-sparse environments. Tests on multiple real-world datasets benchmarked against baseline methods highlight the model's improved performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-organized and generally accessible, with clear explanations of most components used in the proposed methods. Notably, this work is among the first to apply Distributional Reinforcement Learning to the alpha-mining problem. Experimental results demonstrate that the proposed method consistently outperforms all baseline models."
            },
            "weaknesses": {
                "value": "1. The primary distinction between this work and AlphaGen lies in the use of a Distributional RL algorithm rather than a Proximal Policy Optimization (PPO) approach. The authors argue that their DRL algorithm outperforms PPO by better handling non-stationary and reward-sparse environments, which they claim accounts for the observed performance improvement over AlphaGen. However, the paper lacks empirical evidence, explanation, or references to substantiate this claim. The improvement could also stem from factors such as network parameter sizes or other intrinsic properties of the alpha-mining environment.\n\n2. Certain parts of the paper lack clarity. For instance, while \u201cDistributional Reinforcement Learning\u201d appears in the title, it is not explicitly referenced in the abstract or introduction; instead, it is only implicitly referenced via terms such as \"quantile\" and the IQN algorithm, which may lead to confusion.\n\n3. There is also a problem in the design of the algorithm. According to Equation (5), the algorithm aims to encourage exploration by incorporating $h$ as the variance of $Q$. The UCB algorithm, for example, used in Chen et al. (2017) [1], leverages a Q-network ensemble variance to guide exploration, aligning with UCB theory as the ensemble variance correlates with the estimation error of the Q function. In contrast, if I understand correctly, the variance in this work represents the intrinsic variance of the $Z$ function, which may not directly correlate with the Q-estimation error.\n\n\n[1] Chen, Richard Y., et al. \"Ucb exploration via q-ensembles.\" *arXiv preprint arXiv:1706.01502* (2017)."
            },
            "questions": {
                "value": "1. Do the authors have evidence or references supporting that the improved performance is due to the DRL algorithm\u2019s superior handling of non-stationary and reward-sparse environments compared to PPO?\n\n2. In the appendix, it is mentioned that hyperparameters were kept consistent with AlphaGen for a fair comparison. However, given that the PPO policy network in AlphaGen is not directly comparable with the Q-network and quantile network in this paper, could the authors clarify the number of networks and parameters used in both their method and the AlphaGen baseline?\n\n3. In Section 3.3.2, only $\\hat{h}$ is estimated from the quantile network. Does this mean that the parameters $s$, $k$, and $\\zeta$ discussed in Section 3.3.1 are not utilized in the proposed algorithm? If so, would it be possible to reduce Section 3.3.1 to improve conciseness?\n\n4. According to Equation (5), the algorithm aims to encourage exploration by incorporating $h$ as the variance of $Q$. Are there references supporting this approach? As mentioned in the weakness part, the variance in your work represents the intrinsic variance of the $Z$ function, which may not directly correlate with the Q-estimation error. How does adding $h$ enhance exploration in this context?\n\n\n[1] Chen, Richard Y., et al. \"Ucb exploration via q-ensembles.\" *arXiv preprint arXiv:1706.01502* (2017)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a new approach to \"alpha discovery\" in finance using deep reinforcement learning. Alpha discovery is the problem of finding a function that maps stock histories onto predictive signals for future returns. The authors claim that this problem is challenging because the domain is non stationary and reward stationarity. The authors propose a distributional reinforcement learning approach called AlphaQCM. Some theoretical analysis is provided and experiments compare the proposed method with some baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The writing is generally clear. \n- The problems of nonstationarity and reward sparsity are interesting."
            },
            "weaknesses": {
                "value": "The most salient weakness is that the paper appears to be written for a finance audience. Please see questions below."
            },
            "questions": {
                "value": "Questions:\n- What is a synergistic formulaic alpha?\n- What is alpha mining? \n- It is unclear whether the paper aims to contribute to financial modeling or reinforcement learning. \n- Why is the definition of \"alpha\" in a footnote. For a technical conference, given that this is the goal of the work, I would expect the authors to explain what this is and why it is important first. \n- What does it mean for alpha to be subtle and intricate? \n- Line 035, I don't know that \"surpass\" conveys the intended meaning here. \n- What is AlphaGen? \n- Neither nonstationarity nor reward sparsity are adequately defined in the introduction. \n- It is not clear why these methods are chosen for comparison. I see that we are supposed to look in an Appendix for the justification, but why wouldn't this be in the main text? \n- Specifically, I would expect to see, for a machine learning audience, a justification for the alternative models in terms of their prior performance in non stationary and/or sparse reward settings. \n- Why are the only datasets financial in nature? There are other examples of non stationary sparse domains. I would expect those to be included, if the goal of the algorithm is to overcome those challenges."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}