{
    "id": "c5JZEPyFUE",
    "title": "Dynamical Diffusion: Learning Temporal Dynamics with Diffusion Models",
    "abstract": "Diffusion models have emerged as powerful generative frameworks by progressively adding noise to data through a forward process and then reversing this process to generate realistic samples. While these models have achieved strong performance across various tasks and modalities, their application to temporal predictive learning remains underexplored. Existing approaches treat predictive learning as a conditional generation problem, but often fail to fully exploit the temporal dynamics inherent in the data, leading to challenges in generating temporally coherent sequences. To address this, we introduce Dynamical Diffusion (DyDiff), a theoretically sound framework that incorporates temporally aware forward and reverse processes. Dynamical Diffusion explicitly models temporal transitions at each diffusion step, establishing dependencies on preceding states to better capture temporal dynamics. Through the reparameterization trick, Dynamical Diffusion achieves efficient training and inference similar to any standard diffusion model. Extensive experiments across scientific spatiotemporal forecasting, video prediction, and time series forecasting demonstrate that Dynamical Diffusion consistently improves performance in temporal predictive tasks, filling a crucial gap in existing methodologies.",
    "keywords": [
        "Diffusion Model",
        "Generative Model",
        "Prediction Learning",
        "Dynamics"
    ],
    "primary_area": "generative models",
    "TLDR": "We propose Dynamical Diffusion, which incorporates temporally aware forward and reverse processes and improves performance in general temporal predictive tasks.",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=c5JZEPyFUE",
    "pdf_link": "https://openreview.net/pdf?id=c5JZEPyFUE",
    "comments": [
        {
            "summary": {
                "value": "The authors proposed a new method to approach the problem of predictive learning in the context of diffusion models. Instead of utilizing conditional generation, in which the generation is conditioned on the history, their method devises a forward process that gradually incorporates the history while simultaneously adds noise to the data. In this way, the process mixes the \"diffusion time\" and the \"data time\". Training can be done using the usual denoising score matching method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Mixing temporal transitions with diffusion process is a novel idea. The experiments seem extensively done and the proposed method does deliver better performance when compared to DPM in many cases, though not as powerful as heavy models such as iVideoGPT"
            },
            "weaknesses": {
                "value": "1. The writing can at various places uses some improvements. For instance, section 1, from the third paragraph onwards, appear to have large overlap with the abstract. In section 5, why the papers mentioned as \"related work\" described in the 1st paragraph are related seem to be obscure.  \n2. The paper does not contain much insight about why iterative scheme like the one described in (4)-(5) is a helpful one, or more precisely, why it is a good structure to impose. Even adding some simple explanations on the basic features of the scheme, such as \" larger values of $t$ correspond to a stronger emphasis on historical states\", will help bringing some insights."
            },
            "questions": {
                "value": "1. The author(s) stressed that the method is \"theoretically sound\" (repeated twice) and \"theoretically guaranteed\". It'd be good if the author(s) can clarify a. in which sense is it theoretically sound? b. What precisely is \"guaranteed\" theoretically? \n2. I'm confused by the sentence \"Notably, the noise factor $\\sqrt{1-\\alpha_t}$ remains consistent with theoriginal diffusion process along the denoising axis. Hence, the new forward process preserves the same signal-to-noise ratio for any diffusion step $t$\". Surely the authors do not mean $\\sqrt{1-\\alpha_t}$ is independent of $t$, as opposed to what the notation suggests?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Dynamical Diffusion (DyDiff) is a novel framework that enhances diffusion models for temporal predictive learning tasks. It introduces temporally aware forward and reverse processes that explicitly model temporal transitions at each diffusion step, addressing the challenge of integrating temporal dynamics into diffusion processes. DyDiff achieves this by incorporating a mixture of historical states controlled by a new schedule parameter \u03b3, allowing it to capture temporal dependencies more effectively. The framework is theoretically sound and can be efficiently trained and sampled similar to standard diffusion models. Experiments across scientific spatiotemporal forecasting, video prediction, and time series forecasting demonstrate that DyDiff consistently outperforms standard diffusion models, particularly in generating temporally coherent sequences and improving performance over longer time horizons."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper introduces a novel framework called Dynamical Diffusion (DyDiff) that incorporates temporal dynamics into diffusion models for predictive learning tasks. This represents an original approach to addressing limitations of existing diffusion-based methods for temporal data. The authors creatively combine ideas from standard diffusion models with explicit modeling of temporal transitions.\n\nThe paper is generally well-written and structured logically. Key concepts and the proposed method are explained clearly with helpful illustrations. The authors provide pseudocode for the algorithms, which aids in understanding the implementation. Some technical details are relegated to appendices to maintain flow in the main text."
            },
            "weaknesses": {
                "value": "1. The paper does not adequately address why temporal dynamics need to be explicitly modeled, given that neural network architectures like transformers can inherently learn such relationships. Specifically:\n\t-\tLack of justification: The authors do not provide a clear explanation or empirical evidence for why explicitly modeling temporal dynamics is necessary, given that modern architectures like transformers are theoretically capable of learning temporal relationships on their own.\n\t-\tLimited comparison: There is no direct comparison between the proposed Dynamical Diffusion method and transformer-based approaches that implicitly learn temporal dynamics. This makes it difficult to assess the true value added by the explicit modeling of temporal relationships.\n\t-\tInsufficient analysis: The paper does not thoroughly analyze the limitations of existing architectures in capturing temporal dynamics, which would help justify the need for the proposed approach.\n\t-\tOverlooked alternatives: The authors do not discuss potential alternatives for improving temporal modeling within existing frameworks, such as modifications to transformer architectures or attention mechanisms.\n\t-\tUnclear efficiency trade-offs: The paper does not address whether the explicit modeling of temporal dynamics introduces additional computational overhead compared to letting a neural network learn these relationships implicitly.\n\n2. The paper primarily compares Dynamical Diffusion with standard diffusion models. To better establish its significance, the authors should:\n- Include comparisons with other state-of-the-art methods in temporal predictive learning, not just diffusion-based approaches.\n- Provide a more comprehensive literature review to contextualize their work within the broader field of temporal predictive learning.\n\n3. The paper could benefit from more extensive ablation studies to understand the contribution of different components of the proposed method. Additionally, a thorough analysis of hyperparameter sensitivity would strengthen the work. The authors could:\n- Conduct ablation studies on the key components of Dynamical Diffusion.\n- Analyze the sensitivity of the method to different hyperparameters, especially the newly introduced \u03b3\u0304_t schedule.\n- Provide guidelines for selecting optimal hyperparameters for different types of temporal data."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors address temporal learning dynamics - an underexplored area in diffusion models. They introduce Dynamical Diffusion, a simple (essentially one term) modification of existing diffusion dynamics to directly incorporate this temporal dependency in both the forward and backward processes. They show that tracking the temporal similarities results in higher quality samples across multiple modalities."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I think this is a good paper. In my view, the motivation is more akin to how momentum methods developed in optimization - in both the cases, tracking the temporal dynamics was important. The idea, in that regard, isn't too novel, but I am glad to see the large number of experiments the authors have performed. I think this will be a paper whose method will be used as benchmark by future papers."
            },
            "weaknesses": {
                "value": "N/A"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a new diffusion model for time series data incoporating the temporal dependencies between states in the forward and backward processes. Extensive empirical evidence shows that the model indeed outperforms regular denoising diffusion most notably on the SEVIR and Turbulence Flow datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- **A clear problem and a sound solution**: The paper identifies a significant gap in how current diffusion models handle temporal data - they treat predictive learning primarily as conditional generation without fully leveraging temporal dependencies in the data. The authors address this limitation by modelling explicitly temporal transitions at each diffusion step (in both the forward and backward processes) through a mixture process.\n- **Thorough evaluation**: The authors conduct a strong empirical evaluation using datasets from various domains: scientific spatiotemporal forecasting, video prediction, and time series forecasting. The experiments also include ablations and analyses of key components like dependent noise and the $\u03b7$ parameter. In addition to the reported metrics, qualitative visualizations illustrate the strengths of the model espcially compared to regular diffusion.\n- **Good presentation**: The paper is well-written, with a logical flow from motivation to results."
            },
            "weaknesses": {
                "value": "- **Why diffusion models?**: I understand that the main contribution of the paper is extending the capability of diffusion models, and as such it makes sense to focus the evaluation of DyDiff vs DPM. However, I think a general motivation for diffusion/generative models for dynamical data is needed (see questions). Additionally, the authors could discuss how their approach in general relates to previous methods (see questions).\n\n- **Derivations for the reverse process and conditional reverse process are incomplete**: I have a hard time following the derivations in appendix A2 for $q(x_{t-1}^1|x^1_t)$ and $q(x^1_{t-1}|x^1_t, x_0^{-P:1})$, which I find to be too brief. Can the authors explain how $q(x_{t-1}|x_t, x_0^{-P:1})$ is obtained from the forward process?\n \n- **Dataset descriptions lack context**: The Turbulence Flow dataset is introduced without context about what it represents (including what each frame represents physically) or why it's relevant for evaluation. For SEVIR, while mentioned as a \"spatiotemporal Earth observation dataset\", the significance of Vertically Integrated Liquid (VIL) prediction is not explained. The same goes for the RoboNET, BAIR, and time-series datasets. A few sentences introducing the datasets used are needed in each experimental subsection. \n\n- **The metrics used are not defined**: Critical evaluation metrics (CRPS, CSI, FVD, PSNR, SSIM, LPIPS) are used without proper definition or motivation. This makes interpreting the results difficult. A formal (brief, if needed) definition of these metrics is crucial for the assessment of the empirical evaluation of the paper. \n\n- **Important baselines are missing**: The evaluation is missing important baselines like DYffusion [1], which is mentioned when motivating the present work, but not compared to it empirically. Other more recent relevant work is Rolling Diffusion [2], which the authors may choose to not discuss given how recently it was published.\n\n- **The code is not provided as part of the submission**: while this is optional, I believe it is a useful practice to ensure reproducibility. \n\n**nitpicks**\n- could you bold the best numbers in table 8 appendix D2?\n- 'Dyffusion' is misspelled in page 2, last paragraph\n\n[1] \"DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting\", Salva R\u00fchling Cachay, Bo Zhao, Hailey Joren, Rose Yu, NeurIPS 2023\n\n[2] \"Rolling Diffusion Models\", David Ruhe, Jonathan Heek, Tim Salimans, Emiel Hoogeboom, ICML 2024"
            },
            "questions": {
                "value": "- What are the advantages of using diffusion-like models for dynamical data compared to previous predictive approaches?\n- Are there any features of diffusion models that previous methods cannot achieve? can you demonstrare these empirically?\n- Can you extend your derivations to show how $q(x_{t-1}|x_0^{-P:s})$ is obtained from the forward process for both the ddpm and ddim sampling methods?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}