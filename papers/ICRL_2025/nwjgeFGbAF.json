{
    "id": "nwjgeFGbAF",
    "title": "Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification",
    "abstract": "Malware classification by using function call graphs (FCG) is an important task in cybersecurity.\nOne big challenge in this direction is the lack of representative, large, and unique FCG datasets.\nExisting datasets typically contain obsolete Android application packages (APKs), largely consist of small graphs, and include many duplicate FCGs due to repackaging.\nThis results in misleading graph classification performance.\nIn this paper, we propose a new comprehensive dataset, Better Call Graphs (BCG), that contains large and unique FCGs from recent APKs, along with graph-level APK features, with benign and malware samples from different types and families.\nWe establish the necessity of BCG through the evaluation of several baseline approaches on existing datasets.  \nBCG is available at https://iclr.me.",
    "keywords": [
        "Malware classification",
        "FCG"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "New Dataset for Malware Classification and Graph Classification",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=nwjgeFGbAF",
    "pdf_link": "https://openreview.net/pdf?id=nwjgeFGbAF",
    "comments": [
        {
            "summary": {
                "value": "This paper creates a new dataset for malware. Current Android datasets are outdated and limited in size. Importantly, they often also have duplication, which can be harmful for research. For example, baseline tests on existing datasets highlighted how outdated samples can skew classifier performance. These researchers downloaded malware from AndroZoo and VirusShare, filtered them to find suitable current data, and then used various tools to categorize them."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This seems to be a sound way to filter APKs and featurizing them, especially combining graph-level features with the function call graph features. Definitely is a necessary advancement to research, as malware datasets are currently quite old."
            },
            "weaknesses": {
                "value": "It would be nice to have non-Android (x86) malware. \nidentifying specific changes in APK structures over time: this is often an important part of malware research - how do these features and graph structures change over time? Similarly, how does this do on unseen data (new families that arise)? \nThis dataset relies on existing tools like VirusTotal for malware classification and AVClass for label assignment. While I personally don't think this is necessarily an issue, I do think it's relatively a weakness in terms of novelty of methodology."
            },
            "questions": {
                "value": "Are GIN and GraphSage really the most advanced baselines you have? I think there are malware-specific works that could be useful to this discussion rather than relatively old graph baselines. \nWhat are the statistics on how the classifications/family distrubtions change over time."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a dataset for function call graphs (FCGs) from\nAndroid APKs in the task of malware classifcation.  They downloaded\nmore recent APKs, determined family and type, constructed FCGs, and\nremoved graphs with fewer than 100 edges and duplicates.  The\nresulting dataset has 9938 graphs, with an average of 25k nodes and\n54k edges. It contains 29 types and 118 families.\n\nThey extracted non-graph APK features (AF), such as servies,\nreceivers, and libraries.  They also extracted graph features (GF)\nsuch as number of nodes/edges, largest connected component size, and\ncentrality metrics.  Finally, they extracted node representation based\non LDP (Cai and Wang, 2018) for some of their experiments.\n\nThey compare Random Forrest with different combination of features\nwith 3 GNN algorithms.  Empirical result indicate Random Forest using\nall 3 types of features generally outperforms."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors propose a dataset for function call graphs (FCGs) from\nAndroid APKs in the task of malware classifcation.  They downloaded\nmore recent APKs, determined family and type, constructed FCGs, and\nremoved graphs with fewer than 100 edges and duplicates.  The\nresulting dataset has 9938 graphs, with an average of 25k nodes and\n54k edges. It contains 29 types and 118 families.\n\nThey extracted non-graph APK features (AF), such as servies,\nreceivers, and libraries.  They also extracted graph features (GF)\nsuch as number of nodes/edges, largest connected component size, and\ncentrality metrics.  Finally, they extracted node representation based\non LDP (Cai and Wang, 2018) for some of their experiments.\n\nThey compare Random Forrest with different combination of features\nwith 3 GNN algorithms.  Empirical result indicate Random Forrest using\nall 3 types of features generally outperforms."
            },
            "weaknesses": {
                "value": "New algorithmic methods were not proposed and evaluated.\n\nMore non-graph features could have been extracted, such as n-grams of instructions."
            },
            "questions": {
                "value": "Why LDP was chosen to generate node embeddings?\n\nThe number of nodes across graphs is a variable, how are the node embeddings of a graph converted into a fixed-length feature vector for Random Forest?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a new comprehensive datasetthat contains large and unique FCGs from recent APKs, along with graph-level APK features."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. A new dataset is proposed.\n2. The paper is well organized."
            },
            "weaknesses": {
                "value": "The collection and construction of the dataset lack distinctiveness. The inclusion of new software and non-repetitive samples does not constitute the primary contribution of the paper."
            },
            "questions": {
                "value": "1. I am not entirely sure why the authors refer to their dataset as the FCG dataset. Other FCG datasets mentioned by the authors, such as Drebin and CIC, provide SHA256 hashes or APK files, which are not directly related to FCG. Additionally, the Drebin dataset paper introduces a binary feature, not an FCG feature. Therefore, I am unsure why the authors classify their dataset as an FCG dataset.\n2. The dataset relies heavily on VirusTotal's labeling. Although VirusTotal labeling remains the mainstream method, the authors mention, 'To ensure reliability, we only consider the APKs flagged by multiple antivirus engines in VirusTotal.' How many are considered 'multiple'? Which ones are they? Why were these specific ones chosen?\n3. When labeling families using AVClass, there is significant noise. How should samples that cannot be consistently labeled with a family tag be handled?\n4. The distribution of year categories is highly imbalanced. Why is this the case? It appears that the sample count is highest for the year 2021.\n5. It is hoped that the authors can include the results of common detection methods for function call graph such as Mamadroid and APIGraph on BCG.\n\n[1] Enhancing State-of-the-art Classifiers with API Semantics to Detect Evolved Android Malware\n[2] MAMADROID: Detecting Android Malware by Building Markov Chains of Behavioral Models\n\n\n6. Similarly, when evaluating the detection performance for family classification, I anticipate the detection results of common malicious family classification methods (FalDroid [3] and MDMC [4]), rather than results based on common graph processing methods.\n\n[3] Android Malware Familial Classification and Representative Sample Selection via Frequent Subgraph Analysis\n[4] Byte-level malware classification based on Markov images and deep learning.\n\n7. It is unclear whether the authors have provided temporal information for each APK. This would aid readers in conducting research on concept drift ([5]). Additionally, the authors could conduct additional experiments on concept drift, such as investigating how the model classification accuracy decreases with time on this dataset.\n\n[5]Transcending Transcend: Revisiting Malware Classification in the Presence of Concept Drift\n\n\nSome related papers that I think need to be studied\uff1a\nMalRadar:\u00a0Demystifying Android Malware in the New Era"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new comprehensive dataset, Better Call Graphs (BCG), that contains large and unique FCGs from recent APKs,\nalong with graph-level APK features, with benign and malware samples from different types and families. The BCG datasets and existing datasets are evaluated on several baseline approaches to demonstrate the necessity of the BCG dataset."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper addresses the limitations of existing malware classification datasets, particularly the issue of outdated malware samples. A new, open-source dataset has been created, covering malware samples from 2017 to 2023."
            },
            "weaknesses": {
                "value": "First, the primary contribution of this work is the creation of a new dataset covering recent malware samples. However, I do not think this contribution meets the expectation of ICLR. The creation of a new dataset is largely engineering efforts and lacks sufficient scientific contribution. \n\nSecond, the evaluation shows that baseline methods perform poorly on this newly created dataset. I would appreciate the author to design a new model that can work well on the new dataset to enhance the scientific contribution.\n\nThird, I find the poor performance of the baseline methods somewhat unclear. Could the authors clarify the experimental setup? Specifically, was the baseline trained on a portion of the new dataset (e.g., 70%) and then tested on the remaining samples? \n\nFinally, the new dataset spans malware samples from 2017 to 2023, yet there are several other datasets that include samples up to 2021. Do the samples in your dataset overlap with those in existing datasets? If overlaps exist, would collecting malware samples only from 2021 to 2023 have been sufficient?"
            },
            "questions": {
                "value": "1. Clarify the contribution of this work, especially scientific contribution. \n2. Explain the experimental setup: whether baseline models are trained on the new created dataset?\n3. Clarify the overlap between the new created datasets and existing datasets (especially those includes samples up to 2021)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}