{
    "id": "HSGCCUwH7r",
    "title": "Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence",
    "abstract": "We propose Model Swarms, a collaborative search algorithm to adapt LLMs via swarm intelligence, the collective behavior guiding individual systems. Specifically, Model Swarms starts with a pool of LLM experts and a utility function. Guided by the best-found checkpoints across models, diverse LLM experts collaboratively move in the weight space and optimize a utility function representing model adaptation objectives. Compared to existing model composition approaches, Model Swarms offers tuning-free model adaptation, works in low-data regimes with as few as 200 examples, and does not require assumptions about specific experts in the swarm or how they should be composed. Extensive experiments demonstrate that Model Swarms could flexibly adapt LLM experts to a single task, multi-task domains, reward models, as well as diverse human interests, improving over 12 model composition baselines by up to 21.0% across tasks and contexts. Further analysis reveals that LLM experts discover previously unseen capabilities in initial checkpoints and that Model Swarms enable the weak-to-strong transition of experts through the collaborative search process.",
    "keywords": [
        "evolutionary algorithm",
        "model adaptation",
        "model merging"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=HSGCCUwH7r",
    "pdf_link": "https://openreview.net/pdf?id=HSGCCUwH7r",
    "comments": [
        {
            "title": {
                "value": "Author Response (2/2)"
            },
            "comment": {
                "value": "> line 318 - \"in LLM-as-a-judge\" Here, I think we need more details on the prompts, etc. as there is evidence that LLM as a judge evaluation methods can be gamed easily. Could you provide more detail about the prompt and evaluation process? Perhaps discussing how you mitigate potential biases or gaming of this evaluation method.\n\nTable 11 presents the prompt for the LLM-as-a-judge evaluation. Yes automatic evals could be gamed, we take the following measure to enhance reliability: As you can see in Table 11, there is an (instruction, response) pair. The response is curated by querying PerplexityAI and then manually verified and edited. This would provide a high-quality instance for each domain for LLM grounding. We did some in-house quick eval to show that this is more stable than evaluation without a \"gold\" example.\n\nTable 4 also shows that this LLM eval is consistent with the conclusion of the human eval, that Model Swarms is consistently outperforming pre-swarm experts on both metrics.\n\n> line 367 \"Averaged across the four datasets, we found that only 10.4% of the ending-best particles also started as the best (#1), while surprisingly, the bottom half of the starting experts were able to rise to the top in 56.9% of the MODEL SWARMS searches.\" This would be a good opportunity to further discuss the importance of the \"global\" values in the utility function, especially in the early iterations.\n\nYes, we are adding the following sentence to line 372:\n\n\u201cThis indicates that the global best status is switching between experts frequently, suggesting that models are vibrantly and collectively improving and the top spot is constantly overtaken.\u201d\n\n> Paragraph following line 67 - This should be explained at a high level at this point: What does velocity map to? What does \"best found location\" map to? Why is it important to be in a \"best\" neighbourhood, and what does \"best\" refer to? I suggest providing a high-level explanation of these concepts early in the paper, as this would help readers better understand the method.\n\nSo \u201cbest\u201d means \u201chighest score on the utility function\u201d, most commonly \u201chighest score on the validation set\u201d. In this sense:\n\n\u201cVelocity\u201d is the aggregation of model parameter differences. Say model 1 has weight $w_1$, model 2 has weight $w_2$, the $w_1-w_2$ is a \u201cmodel parameter difference\u201d, and velocity is an aggregation of various such terms (lines 150). This velocity term then guides where the model should move towards next.\n\n\u201cBest found location\u201d is then the model weight that achieves the highest utility function score across all experts\u2019 search history.\n\nThe \u201cbest neighborhood\u201d is then valuable since \u201cthe weight neighborhoods of good model checkpoints might be promising to explore\u201d (lines 146-147, [1])\n\nWe add \u201cbest/worst meaning the best/worst-performing expert on the utility function\u201d towards the end of line 69.\n\n[2] Eilertsen, Gabriel, et al. \"Classifying the classifier: dissecting the weight space of neural networks.\" ECAI 2020."
            }
        },
        {
            "title": {
                "value": "Author Response (1/2)"
            },
            "comment": {
                "value": "We would like to thank the reviewer for their thoughtful comments and feedback.\n\n> I could not find evidence on how weights are mapped to the weight/location space. Please provide detailed explanation of how model weights are represented in the weight/location space, including any techniques used for dimensionality reduction or embedding if applicable.\n\n> line 104 \"location represented by model weights;\" - Why are we not talking specifically about the weights being in an embedding space (locations) - assuming this is the case? Otherwise, one would wonder how the weights can purely be taken from different size/architecture type models and mapped to the same space.\n\nThe location space is just the weight space itself, i.e. the location of a model is the parameter values of the model in the 7B-dimensional space. There is no dimensionality reduction or embedding.\n\nSince models only move towards the velocity direction, which is in turn impacted by the weight of other models, this is not a \"free\" search rather a constrained search of model compositions, so directly operating on the 7B model weight space is effective.\n\nIn this sense, weights from different size/architecture could not be mapped to the same space. That\u2019s why we have \u201cDifferent Model Architectures with Token Swarms\u201d and Figure 5 in Section 5, where we move the definition of the space to the token probabilities space, where token probability distributions are composed on-the-fly instead of weights when there are heterogeneous model architectures, inspired by proxy tuning [1]. Empirically we see that 4 Mistral-7B experts and 4 Gemma-7B experts could compose in the token probability space with the token swarms variant in Figure 5 and collectively become better.\n\nEmbedding space is definitely another very promising space to consider when composing heterogeneous models. We are potentially exploring a follow-up on model heterogeneity and would be happy to explore alternative space definitions.\n\n[1] Liu, Alisa, et al. \"Tuning language models by proxy.\" COLM 2024.\n\n> In line 110, there is a first mention of a utility function, but at this point, I think this should have been explained already. I would suggest to introduce and explain the utility function earlier in the paper, as it's a key concept for understanding the method. This would improve the paper's overall clarity and flow.\n\nWe did introduce the utility function earlier in line 98:\n\n\u201cIt also requires a \\emph{utility function} $f:\\mathbf{x} \\rightarrow \\mathcal{R}$, mapping each expert onto a scalar value that should be optimized for model adaptation.\u201d\n\nLet us know if you are suggesting something else!\n\n> line 154, I think it would be important to explain how to prevent experts that are best (globally) from being drawn to their own \"best\" initial location and how the random factor helps to explore. Please elaborate on how this issues is addressed, specifically detailing how the random factors in your method help prevent getting stuck in local optima and encourage exploration.\n\n> line 171 \"Since MODEL SWARMS explicitly encourage randomness and exploration,\" - how? This could be picked up briefly, here again, to circle back. \"repeating\" this here solidifies the idea\n\nThe global best expert is not drawn to their own \u201cbest initial location\u201d. So when an expert is the global best, $p_i-x_i=0$ since the personal best is its current location $p_i=x_i$, similarly $g-x_i=0$ since the global best is also just its current location $g=x_i$. So the personal/global best terms have no effect on the current global best expert.\n\nAbout the effect of randomness: let\u2019s assume there is one expert that is in a very high-utility location but not the highest. If there is no randomness, then the $\\phi_g(g-x_i)$ would be very large and all models are drawn towards it very hard, leading to them stuck in the local optima. However, if there is a randomness factor $r_g \\sim \\mathcal{U}(0,1)$ that discounts the term as $r_g\\phi_g(g-x_i)$ and $r_g$ happens to be very small, then it would allow the model to be less impacted by the global best term but more impacted by other terms to explore and move out of the local optima.\n\nWe add to line 171 \u201cthis exploration is made possible by randomness factors $r_v, r_p, r_g, r_w \\sim \\mathcal{U}(0,1)$, where the impact of personal/global bests are randomly discounted to favor exploration rather than overly quick convergence.\"\n\n> line 258 - it might make sense to bring in SOTA numbers from non-composition LLMs\n\nThe first two lines/baselines are non-composition LLMs. \u201cBest Single\u201d is the best-performing LLM expert in the pool decided by performance on the validation set. \u201cData Merge\u201d is having one LLM trained on all SFT data that produced the pool of experts.\n\n> List following line 77 Is this the same model pool? Or for each point in the list of different pools?\n\nAll evaluation types and tasks share the same model pool, described in Section 3."
            }
        },
        {
            "title": {
                "value": "Author Response"
            },
            "comment": {
                "value": "We would like to thank the reviewer for their thoughtful comments and feedback.\n\n> Can this approach extend to decision-making tasks? Decision-making often requires the learning of a return or value function, which might pose challenges for stability in the search process. It would be insightful to understand how MODEL SWARMS handles stability and reliability under these conditions.\n\nAbsolutely yes, as long as the \u201creturn or value function\u201d outputs a scalar value representing the utility of the decision. I wonder if the reviewer might have any pointers to decision making/planning data that could employed as evaluation for future work.\n\n> Would larger LLMs make the search more difficult as the parameter size increases?\n\nConceptually no, as the particles/experts in a Model Swarm could be of any size, and the methodology isn\u2019t impacted by size.\n\nComputationally the cost would certainly go up, linear to the amount of parameters, since the main computation cost comes from running model inference on a small validation set.\n\nEmpirically it would be interesting to see whether model size has an impact on effectiveness: we by default employed Gemma-7B and we now quickly evaluate Gemma-2B experts on NLGraph, we see that:\n\n|    NLGraph    |       dev      |      test      |\n|:-------------:|:--------------:|:--------------:|\n|  pre-swarm 2B |      0.315     |      0.330     |\n| post-swarm 2B | 0.425 (+34.9%) | 0.420 (+27.2%) |\n|  pre-swarm 7B |      0.540     |      0.535     |\n| post-swarm 7B |  0.730 (+37.0%)  | 0.672 (+25.6%) |\n\nThe improvement is consistently 20%-30% across 2B and 7B. We will try out models on a scale of 13B/70B when more compute is available."
            }
        },
        {
            "title": {
                "value": "Author Response"
            },
            "comment": {
                "value": "We would like to thank the reviewer for their thoughtful comments and feedback.\n\n> Why did you choose Particle Swarm Optimization (PSO) for your model instead of other evolutionary algorithms such as Ant Colony Optimization (ACO) or Genetic Algorithms? Could you discuss the benefits of using PSO compared to these other common evolutionary game theory (EGT) strategies in your context?\n\nLines 514-529 discuss why PSO is uniquely fitting for composing and adapting LLM experts. To recap:\n\nGenetic algorithm requires much more manual engineering. For example, several crossover operations and several mutation operations are required to produce ``offsprings\u2019\u2019 from existing entities, in this case, LLMs. While there are preliminary attempts at designing these operations, they rely on heuristics and manual engineering that are not scalable or flexible, and they are mostly applicable to automated prompt design only.\n\nFor Ant Colony, the problem needs to be transformed into the format of finding the shortest paths on a weighted network. It is unclear how composing LLM model parameters could become a graph problem and whether ACO is applicable in this case.\n\nThat said, we agree that evolutionary LLM design is a promising direction and there are many classic algorithms to further explore beyond Model Swarms, an important first step to this direction.\n\n> Evolutionary Game Theory (EGT) is often criticized for its lengthy and unstable search times, as well as the significant computational resources required. Could you provide details on the computational time associated with your design?\n\nFor stability, Table 7 shows the performance on dev/test sets where Model Swarms is repeated for multiple runs. Despite the randomness, Model Swarms consistently find expert models better than any baseline in 73% of the runs, indicating stable improvements.\n\nFor computational time, we refer the reviewer to lines 1160-1183 in the appendix. To recap, Model Swarms has linear complexity to the number of models and to the cost of one model inference, while it takes about 10-15 iterations for each run on average. Empirically, with 5 40GB GPUs you could run a Model Swarms search under an hour, with 2 GPUs you need about 3 hours. If that is still not fast enough, we propose further acceleration with dropout-k and dropout-n: we refer the reviewer to lines 483-488 and Figure 9."
            }
        },
        {
            "title": {
                "value": "Author Response (2/2)"
            },
            "comment": {
                "value": "> The method proposed is a variation on Particle Swarm Optimization and Genetic Algorithms, so the innovation seem limited to apply those methods to LLMs.\n\n> Can you explain what changes done compared to the PSO and GA algorithms are the most useful to achieve the results obtained?\n\nModel Swarms is not related to genetic algorithms (GA). It is related to Particle Swarm Optimization (PSO), another instance of evolutionary algorithms: PSO was proposed to solve classic optimization problems and is vastly different from Model Swarms. The key differences are:\n\n**1. Populate initial experts**. While PSO starting points could be randomly sampled from the weight space, it is simply impossible to randomly sample LLM weights due to the ultra-high dimensionality. Lines 136-140 describe Model Swarms\u2019 unique way of growing the initial set of experts as starting points.\n\n**2. Global worst**. LLMs often have the weight continuity/smoothness that classic optimization problems don\u2019t [1]: if you change a few parameters by 1e-3, the output would still be the same (but for non-convex non-continuous optimization problems, this is not guaranteed). Given this unique attribute, we track the global worst expert and push the swarm away from bad model checkpoints (lines 114-117, lines 122-126).\n\n**3. Restarting failed experts**. While evaluating a particle on classic optimization problems is lightweight, evaluating an LLM expert on an objective is not trivial. To make this feasible and efficient, we propose to restart failed experts, experts who are not becoming better in $c_r$ iterations, by reverting it back to their personal best. This encourages diverse experts to freely explore and will restart them to have another chance in case of failure.\n\nTable 5, Figure 9, and Table 6 demonstrate the uniqueness of Model Swarms and the contribution of these bullet points.\n\n[1] Eilertsen, Gabriel, et al. \"Classifying the classifier: dissecting the weight space of neural networks.\" ECAI 2020."
            }
        },
        {
            "title": {
                "value": "Author Response (1/2)"
            },
            "comment": {
                "value": "We would like to thank the reviewer for their thoughtful comments and feedback.\n\n> There are no examples in the paper of the kind of text the LLMs experts produce. There are detailed examples in the appendix but showing an example would have gone a long way in helping the readers understand the significance of the work.\n\n> The paper is not an easy read. The tables are dense, there are a lot of charts, but since no examples have been included, they feel too abstract.\n\nWe add examples of the LLM expert-generated texts to the main paper by appending a few lines in Table 4. Specifically:\n\n```\nQ: How can natural language processing (NLP) technology enhance the drafting and review processes for legal documents, ensuring precision and compliance?\n\nPre-swarm: Enhanced Drafting and Review Processes for Legal Documents Through NLP Natural language processing (NLP) is revolutionizing the legal industry by automating and improving various tasks, including document drafting and review. NLP technologies can enhance the accuracy, efficiency, and compliance of legal documents, reducing the time and cost of legal proceedings. NLP technology could generate many sentences, documents, and content that could be helpful to legal professionals.\n\nPost-swarm: Natural Language Processing (NLP) technology can significantly enhance the drafting and review processes for legal documents, ensuring precision and compliance. Here\u2019s how: 1. Extracting Key Information: NLP can analyze legal documents and extract relevant information like contracts, agreements, and regulations. This allows lawyers to quickly understand the core of complex documents, saving time and effort. 2. Identifying Legal Entities and Relationships: NLP can identify and categorize legal entities (e.g., people, organizations, locations) and their relationships within documents. This facilitates accurate document organization and analysis. 3. Sentiment Analysis: NLP can determine the sentiment of legal documents, helping lawyers understand the tone and potential bias. This can be valuable in contract negotiations or dispute resolution.\n```\n\nQuantitative evaluation for the above example shows that the post-swarm expert generates better (Gemini judge score of 5->8 out of 10) and more factual (Facts&Evidence score 0.478 -> 0.696) response in the legal AI domain. Another example:\n\n```\nQ. What are some distinct stylistic differences between the use of bass in jazz versus rock music?\n\nPre-swarm: Sure, here are some distinct stylistic differences between the use of bass in jazz versus rock music: The bass is often used to play the melody, rather than just the root notes. The bass is used to play chords, rather than just root notes. The bass is used to play a variety of rhythm patterns, including syncopation, swing, and waltz. The bass is used to play a variety of dynamic changes, including volume changes, pitch changes, and tempo changes.\n\nPost-swarm: While both genres utilize bass, their stylistic differences are evident in the instrument\u2019s role, playing techniques, and overall musical context.\nMelodical Focus: The bass often takes the lead, playing intricate melodies and improvisational lines. Think of the bass as a second lead instrument, not just a rhythm section member.\nHarmonic Sophistication: Jazz basslines often incorporate chromaticism, dissonance, and complex chord structures, adding harmonic depth and complexity.\nDynamic Range: Jazz bass players exhibit a wide dynamic range, playing with both power and finesse, often switching between the two within a single phrase.\nRhythm Section Synergy: The bass interacts with the drums and piano to create a dynamic rhythm section, emphasizing interplay and improvisation.\n```\n\nFor the above example, we again see Gemini judge score improvement from 5 to 7 out of 10 and Facts & Evidence score improvement from 0.444 to 0.722.\n\nBy appending a few lines in Table 4 to include these examples, we hope these expert generated texts would strengthen the main paper."
            }
        },
        {
            "summary": {
                "value": "The paper proposes a collaborative search algorithm that uses swarm intelligence to adapt LLMs.  The method proposed starts with different LLM experts and a utility function; collaboratively the LLMs experts optimize the utility function. Each expert is a particle in the swarm. It has a location that depends on the model weights, a velocity which is the direction where it should move next, the personal best found location, and the global best and worst location over the entire history.  The population is initialized using crossover, the velocity is updated as a weighted average of the current velocity, personal best, global best, and global worst.  Weights and locations are updated in steps. The length of the steps is reduced over time until convergence. The results are compared with baselines for different ways of doing the composition (trivial, static, dynamic) on multiple datasets. The examples shown in the paper work in multiple cases, i.e., for a single task, multi-tasks, reward models, and different human interests.  The results overall are good."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The method proposed is training free and can be used with a small number of examples.\n\nIt is interesting to see how the collaboration of week models can outperform strong models.\n\nThe experimental results presented are overall positive, showing the method is a promising approach over a large variety of domains."
            },
            "weaknesses": {
                "value": "There are no examples in the paper of the kind of text the LLMs experts produce. There are detailed examples in the appendix but showing an example would have gone a long way in helping the readers understand the significance of the work. \n\nThe method proposed is a variation on Particle Swarm Optimization and Genetic Algorithms, so the innovation seem limited to apply those methods to LLMs.\n\nThe paper is not an easy read. The tables are dense, there are a lot of charts, but since no examples have been included, they feel too abstract."
            },
            "questions": {
                "value": "Can you explain what changes done compared to the PSO and GA algorithms are the most useful to achieve the results obtained?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents MODEL SWARMS, a collaborative search algorithm that adapts various Large Language Models (LLMs) for multiple applications. This method employs the best checkpoints as guides to optimize utility functions for different objectives, utilizing the concept of evolutionary games. Extensive testing demonstrated that MODEL SWARMS enhances performance by up to 21.0% compared to twelve conventional model composition baselines in four adaptation scenarios. Notably, this model does not require fine-tuning or presuppose the existence of expert models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The concept is straightforward and the writing is clear and easy to comprehend.\n2. This model operates without the need for fine-tuning and does not depend on already established expert models.\n3. The performance appears to be quite strong relative to existing approaches."
            },
            "weaknesses": {
                "value": "1. Why did you choose Particle Swarm Optimization (PSO) for your model instead of other evolutionary algorithms such as Ant Colony Optimization (ACO) or Genetic Algorithms? Could you discuss the benefits of using PSO compared to these other common evolutionary game theory (EGT) strategies in your context?\n\n2. Evolutionary Game Theory (EGT) is often criticized for its lengthy and unstable search times, as well as the significant computational resources required. Could you provide details on the computational time associated with your design?"
            },
            "questions": {
                "value": "see Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces MODEL SWARMS, a collaborative search algorithm for adapting Large Language Models through swarm intelligence. The approach involves a pool of LLM experts that collectively optimize a utility function by adjusting in the weight space, guided by checkpoints from the most successful models. MODEL SWARMS offers tuning-free model adaptation and is designed to operate effectively even with minimal data (as few as 200 examples). The method is evaluated across various domains, showing improvements of up to 21.0% over 12 model composition baselines."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The proposed method is well-founded and practical, with wide applicability across diverse adaptation scenarios, from single-task to multi-task learning, as well as reward modeling and aligning with varied human interests. \n\nThis work is likely to inspire new directions in LLM research, providing valuable insights into adaptive model composition. \n\nThe paper is well-organized and clear, with a logical flow and thorough experimental design, making the findings compelling and easy to follow. \n\nThe experiments are comprehensive and detailed, offering a robust evaluation across various tasks and contexts."
            },
            "weaknesses": {
                "value": "No major faults were identified."
            },
            "questions": {
                "value": "Can this approach extend to decision-making tasks? Decision-making often requires the learning of a return or value function, which might pose challenges for stability in the search process. It would be insightful to understand how MODEL SWARMS handles stability and reliability under these conditions. \n\nWould larger LLMs make the search more difficult as the parameter size increases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores swarm intelligence to guide LLM weight updates of a set of expert LLMs. In this work, a given set of LLMs are distributed in \"weight-space\" with randomized velocity and location (inspired by particle swarms). The algorithm updates the LLM particles such that local and global (collective) locations are considered to find new optimal locations, that is, model weights. Optimal locations are evaluated by benchmarking across various tasks from literature, as well as model composition/ensembling methods as baselines."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "- Novel way of utilizing swarm intelligence (particles) to update weights of a given set of LLMs (experts)\n- Very well presented in terms of language, explanations, and figures, but also evidence of why and how this works\n- Extensive experimentation and very well-suited benchmarks/baselines\n- Especially interesting experiments on the emergence of new skills, \"Correctness Emergence\", as well as the diversity of LLM sets (\"Diversity Matters\")"
            },
            "weaknesses": {
                "value": "- I could not find evidence on how weights are mapped to the weight/location space. Please provide detailed explanation of how model weights are represented in the weight/location space, including any techniques used for dimensionality reduction or embedding if applicable.\n- In line 110, there is a first mention of a utility function, but at this point, I think this should have been explained already. I would suggest to introduce and explain the utility function earlier in the paper, as it's a key concept for understanding the method. This would improve the paper's overall clarity and flow.\n- line 154, I think it would be important to explain how to prevent experts that are best (globally) from being drawn to their own \"best\" initial location and how the random factor helps to explore. Please elaborate on how this issues is addressed, specifically detailing how the random factors in your method help prevent getting stuck in local optima and encourage exploration.\n- line 258 - it might make sense to bring in SOTA numbers from non-composition LLMs\n- line 318 - \"in LLM-as-a-judge\"\nHere, I think we need more details on the prompts, etc. as there is evidence that LLM as a judge evaluation methods can be gamed easily. Could you provide more detail about the prompt and evaluation process? Perhaps discussing how you mitigate potential biases or gaming of this evaluation method.\n- line 367\n\"Averaged across the four datasets, we found that only 10.4% of the ending-best particles also started as the best (#1), while surprisingly, the bottom half of the starting experts were able to rise to the top in 56.9% of the MODEL SWARMS searches.\"\nThis would be a good opportunity to further discuss the importance of the \"global\" values in the utility function, especially in the early iterations."
            },
            "questions": {
                "value": "- Paragraph following line 67 -  This should be explained at a high level at this point:\nWhat does velocity map to?\nWhat does \"best found location\" map to?\nWhy is it important to be in a \"best\" neighbourhood, and what does \"best\" refer to?\nI suggest providing a high-level explanation of these concepts early in the paper, as this would help readers better understand the method.\n- List following line 77\nIs this the same model pool? Or for each point in the list of different pools?\n- line 104\n\"location represented by model weights;\" - Why are we not talking specifically about the weights being in an embedding space (locations) -  assuming this is the case? Otherwise, one would wonder how the weights can purely be taken from different size/architecture type models and mapped to the same space. \n- line 171\n\"Since MODEL SWARMS explicitly encourage randomness and exploration,\" - how? This could be picked up briefly, here again, to circle back. \"repeating\" this here solidifies the idea"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}