{
    "id": "n0YCAMVh8b",
    "title": "Multiscale Training of Convolutional Neural Networks",
    "abstract": "Convolutional Neural Networks (CNNs) are the backbone of many deep learning methods, but optimizing them remains computationally expensive. To address this, we explore multiscale training frameworks and mathematically identify key challenges, particularly when dealing with noisy inputs. Our analysis reveals that in the presence of noise, the gradient of standard CNNs in multiscale training may fail to converge as the mesh-size approaches to $0$, undermining the optimization process. This insight drives the development of Mesh-Free Convolutions (MFCs), which are independent of input scale and avoid the pitfalls of traditional convolution kernels. We demonstrate that MFCs, with their robust gradient behavior, ensure convergence even with noisy inputs, enabling more efficient neural network optimization in multiscale settings. To validate their generality and effectiveness, we show that MFCs can seamlessly replace standard convolutional layers across diverse architectures and benchmarks, delivering substantial computational speedups without sacrificing performance.",
    "keywords": [
        "Multilevel Stochastic Gradient Descent",
        "Multiscale Training",
        "Mesh Free Convolutions"
    ],
    "primary_area": "optimization",
    "TLDR": "We propose a PDE-inspired multiscale framework to accelerate training of neural networks, and demonstrate it on several benchmarks.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=n0YCAMVh8b",
    "pdf_link": "https://openreview.net/pdf?id=n0YCAMVh8b",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces an approach to improving the computational efficiency of CNNs through a multiscale training framework.\nIt first presents Multiscale Stochastic Gradient Descent (Multiscale-SGD), which leverages the Multilevel Monte Carlo method to reduce computational costs by approximating gradients across resolutions.\nHowever, the authors identified that noisy or high-frequency inputs can lead to unbounded gradients across mesh resolutions, complicating the convergence process.\nTo address this, they propose Mesh-Free Convolutions (MFCs), which are independent of specific input scales and ensure consistent gradient convergence across resolutions, even with noisy inputs. This mesh-independence of MFCs overcomes the convergence limitations observed in standard CNNs during multiscale training."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper introduces an innovative approach to multiscale CNN training through Multiscale-SGD and Mesh-Free Convolutions (MFCs).\nAll the developments are backed by mathematically reasoning.\nAnd the whole paper is logically organized."
            },
            "weaknesses": {
                "value": "1. The experimental results are really limited, and you should also compare the performance with the fixed computational budget.\n2. The experimental comparison with existing multiscale or Fourier-based CNN methods is not presented\n3. The mathematical foundation for Mesh-Free Convolutions, particularly the differential operator theory, could be challenging for readers less familiar with this domain. Adding a visual explanation or intuitive analogies could make the theory more accessible.\n4. In the experiments, the network seems really shallow, could you train deeper networks, to see whether the methods works?"
            },
            "questions": {
                "value": "1. All the results show mixed performance bettween the regular SGD and the two Multiscale-SGD. If you could train longer for the multiscale-SGD, could you improve all the performance comparing to SGD?\n2. The MSE results are strange, with the two UNet have huge gap. What is the reason behind it?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Training convolutional neural networks (CNNs) on high-resolution images can be computationally demanding. One way to alleviate the computational burden is by including downsampled versions at different scales during training. This paper identifies shortcomings of standard CNNs in such multiscale approaches and introduces \u201cMesh-Free Convolutions\u201d (MFCs) to address these limitations. The paper presents two novel multiscale training algorithms, \u201cMultiscale-SGD\u201d and \u201cFull-Multiscale-SGD\u201d, which estimate high-resolution parameters using coarser-resolution samples, thus avoiding costly training on high-resolution data. For such multiscale training schemes, gradients of conventional CNNs for noisy data might diverge with decreasing resolution compared to \u201csmooth data\u201d. Inspired by differential operators, MFCs offer a resolution-independent generalisation of standard convolutions. The empirical validation demonstrates that these techniques can improve training time and ensure consistent gradient behaviour, as shown for standard UNet and ResNet architectures and image blur estimation, image denoising, and image super-resolution tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I believe this paper provides several important contributions and extends the existing literature in different significant ways:\n\n__S1.__  The authors propose new techniques for training CNNs within a multiscale resolution framework, which improve computational efficiency while maintaining test performance. This alone can be a valuable contribution, as shown by the \u201cnon-MFC\u201d empirical results in Tables 3, 4, and 5. \n\n__S2.__ As far as I can tell, MFCs are an original and promising alternative to traditional convolutional layers (though some clarifications may be necessary; see W2 below) that can alleviate some of the shortcomings of traditional CNNs in multiscale approaches. Given their potential application across various high-resolution tasks, this can be a significant contribution, too.\n\n__S3.__ The quality of the paper is high, with claims well-supported through mathematical derivations, proofs, and empirical evidence. In particular, the experimental results in Tables 3, 4, and 5 highlight that significant speed-ups might be feasible (as measured by #WU; however, see Q2 below).\n\nFor these reasons, I consider this a good paper."
            },
            "weaknesses": {
                "value": "Some clarity aspects could be addressed to strengthen the paper. In more detail, I currently see the following weaknesses:\n\n__W1.__ Section 3.1 on \u201cMesh-Free Convolutions\u201d is relatively dense and challenging to follow. For instance, some notations typically used with parabolic PDEs, like indices denoting partial derivatives, should be more clearly introduced. More importantly, the connection between $u$, $v$, $\\tilde{v}$, and $\\mathcal{C}$ is not very clear and should be made more explicit.\n\n__W2.__ Expanding on the previous point, the paper would benefit from a more intuitive explanation or illustration of the connection between standard convolutions and mesh-free convolutions. Currently, the connection is difficult to assess. In that regard, Figure 3 lacks clarity and might require additional explanation. \n\n__W3.__ _Computational considerations_: As the authors highlight in lines 334-338, the required Fast Fourier Transform dominates the overall computational cost. To provide the complete picture, additional consideration of running time and optimisation steps in the results in section 5 on \u201cExperimental Results\u201d would strengthen the contribution. This would allow a more comprehensive assessment of MFC's computational feasibility. \n\nMinor remarks:\n- Line 231: \u201c[\u2026] without compromising on its efficiently.\u201d should likely be \u201c[\u2026] on its _efficiency_.\u201d\n- Line 293: \u201c[\u2026] let $v = \\mathcal{C}(\\xi)u$ be the mesh-free convolution is parameterized by $\\xi$\u201c needs to be rewriten.\n- Line 296: Equation \u201c(17a)\u201d likely should just read \u201c(17)\u201d.\n\nMy rating is currently \u201cborderline accept\u201d due to the aspects raised in W1 to W3. However, I am willing to adjust my rating if some of these points and the questions below can be addressed."
            },
            "questions": {
                "value": "__Q1.__ To illustrate the difference between conventional CNNs and MFC, would it be possible to compare feature maps between these two approaches?\n\n__Q2.__ A more formal definition of the _work unit_ metric #WU might be beneficial. As I understand it, #WU measures how many evaluations of the highest resolution are required, with evaluations at lower resolutions being weighted by the corresponding \u201cdownsampling factor\u201d. Is this correct? If so, should this not lead to fractional values of #WU?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work begins with an error analysis of the the standard training procedure for convolutional networks when training at multiple resolutions, and (using some clever re-interpretations by varying mesh size and sample size) derive new multiscale training algorithms that require fewer network evaluations at the finest mesh size. They further present mesh-free convolutions that can be thought of as limits of progressively refined mesh-dependent convolutions. Finally, experimental results are presented to compare the training algorithms and network variants against deep learning standards like SGD and U-Nets/ResNets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The primary research questions in the paper are well-motivated.\n2) The technical analysis seems to be sound, and the authors are able to propose interesting, non-obvious training algorithms from them (i.e., this goes far beyond \"multiscale training is useful for learning convolutional kernels\"). I also found the further step of mesh-free convolutions to be very interesting from a theoretical perspective."
            },
            "weaknesses": {
                "value": "My major problems with the paper pertain to the experimental results, and fall into two main categories:\n\n1) The experiments done are on extremely small networks and datasets. There are very standard collections of computer vision experiments that could be used to show the benefits of the proposed multiscale-SGD training algorithms (see, for example, the benchmarks used for experiments in any landmark CV papers from the past few years, like CLIP or Segment Anything). I understand that compute access could be a factor here, but many of these datasets are small enough that access to even a single GPU should be sufficient to perform the necessary experiments.\n\n2) Unclear presentation of results. It's difficult to understand the relative compute requirements of different algorithms based on the work unit representation that is used in the experiments. It would be much clearer to either present the FLOPs required per iteration or backward pass, or best of all would be to simply show the amount of time taken to train the network in each case.\n\n3) Non-competitive baselines. In practice, multiscale training is implicitly accounted for in most deep learning training procedures via data augmentations in the form of varying-sized crops of images (see the widely-used RandomResizedCrop transform in torchvision). To me, this is the actual baseline to which we should compare to, but it is not mentioned or compared against at all in your work.\n\nBased on these, I find it difficult to be convinced by the presented experiments that the proposed multiscale algorithms would be better to use than standard SGD on usually-sized computer vision trianing tasks."
            },
            "questions": {
                "value": "1) Why are there no experiments on CV prediction tasks like classification, segmentation, object detection, etc.? Are the proposed algorithms not expected to work well in these contexts?\n\n2) Corresponding to the data augmentation comment above: It was not clear to me if you use data augmentations like RandomResizedCrop or similar in any of your experiments; do you? If not, would you be able to show what happens when these are added to the single-scale SGD training procedure?\n\n3) Corresponding to the comment on presentation of results comment above: could you add (even approximate) FLOPs computations or raw timings to your experiments?\n\n4) Are there situations in which it would be preferable to use the Multiscale-SGD algorithm that you proposed instead of the Full-Multiscale-SGD algorithm? If not, it would be better to focus more of the experiments on the Full-Multiscale algorithm, and leave the Multiscale algorithm to an ablation study."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a training framework designed to optimize CNNs more efficiently by employing multiscale training strategies. This approach targets the significant computational challenges that arise when training CNNs at high resolutions, especially when handling noisy input data. By introducing Multiscale-SGD and the MFCs, the framework reduces computational costs and ensures stable gradient behavior across varying scales."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe paper presents a good framework that integrates multiscale training with convolutional neural networks (CNNs).\n2.\tThe writing of this paper is easy to follow.\n3.\tThe experiments validate the effectiveness of the proposed methods, showcasing practical improvements of the proposed method.\n4.\tThis paper offers thorough mathematical analysis, including proofs and lemmas."
            },
            "weaknesses": {
                "value": "1.\tA significant limitation is the evaluation performed with shallow CNNs, all models containing fewer than four layers. Given that practical applications often leverage deeper architectures like ResNet with over 18 layers, it raises concerns about the scalability and real-world applicability of the proposed multiscale framework. Demonstrating results with deeper networks could help validate its utility for larger and more complex tasks.\n2.\tIn the context of modern architectures, transformers such as Vision Transformers (ViT) are becoming more prominent for various applications. The paper does not explore whether the proposed method could be adapted or extended to transformer-based architectures. This is particularly relevant as transformers may handle multiscale data differently from CNNs. Discussing or experimenting with how the proposed method might extend to these architectures would strengthen the paper's broader applicability\n3.\tWhile the paper presents experimental validation of the proposed method's effectiveness, the experiments are mostly focused on simpler tasks. The applicability of the method to more complex and challenging applications, like object detection, video understanding tasks, remains untested. \n4.\tWhile the paper claims significant computational advantages with the multiscale approach, it lacks a thorough comparative analysis of computational costs with existing training methods. For instance, how this method compares in terms of hardware resource use, memory consumption, or training speed against some techniques like mixed-precision training is not clearly outlined.\n5.\tThe paper could benefit from a deeper exploration and further discussion of how the training dynamics change with the proposed method compared to standard methods."
            },
            "questions": {
                "value": "See the above weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}