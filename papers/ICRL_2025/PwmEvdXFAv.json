{
    "id": "PwmEvdXFAv",
    "title": "An efficient algorithm for entropic optimal transport under martingale-type constraints",
    "abstract": "This work introduces novel computational methods for entropic optimal transport (OT) problems under martingale-type conditions.\nThe problems can map to a prevalent class of OT problems with structural constraints, encompassing the discrete martingale optimal transport (MOT) problem, as the (super-)martingale conditions are equivalent to row-wise (in-)equality constraints on the coupling matrix. Inspired by the recent empirical success of Sinkhorn-type algorithms, we propose an entropic formulation for the MOT problem and introduce Sinkhorn-type algorithms with sparse Newton iterations that utilize the (approximate) sparsity of the Hessian matrix of the dual objective. As exact martingale conditions are typically infeasible, we adopt entropic regularization to find an approximate constraint satisfied solution. We show that in practice the proposed algorithms enjoy both super-exponential convergence and robustness with controllable thresholds for total constraint violations.",
    "keywords": [
        "Optimal Transport",
        "Martingale Constraints"
    ],
    "primary_area": "optimization",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=PwmEvdXFAv",
    "pdf_link": "https://openreview.net/pdf?id=PwmEvdXFAv",
    "comments": [
        {
            "title": {
                "value": "Reply to the reviewer"
            },
            "comment": {
                "value": "The authors thank the reviewer for the reviewer's time. If the response addresses some of the reviewer's concerns, we kindly suggest that the reviewer consider increasing the score.\n\n### Comment 1\n*Entropic regularization leads to a dual with similar properties to other OT problems. The resulting algorithm is a standard algorithm in the OT literature, the Sinkhorn-type algorithm. It seems that this paper isn't very novel. It's too similar to the existing numerical considerations for OT problems. The martingale-type conditions did not require new mathematical analysis to come up with an algorithm.*\n\n**Response:**\n\nThe author would like to provide some clarifying remarks about the novelty. The first main novelty of this work is that the entropic regularization is based on an LP with approximate martingale condition. As the LP with exact martingale condition may not have any feasible solution, the approximate constraint satisfaction formulation in equation (4) is necessary. One of the main novelties of this work is to use the entropic LP formulation based on the LP in equation (4). While fairly natural, the authors think the proposed approach is the correct numerical treatment for entropic MOT. We can summarize this contribution by concluding that the authors identified the correct entropic formulation for MOT.\n\nSecondly, the APDAGD algorithm is quite strong and the design of an algorithm that outperforms APDAGD is a rather significant undertaking. The Sinkhorn-type algorithm is based on a novel analysis of the exact sparsity structure of the Hessian matrix, and the SNS algorithm is based on the approximate sparsity structure of the Hessian matrix, which are both novel observations of the newly proposed entropic formulation. The numerical performance is quite strong and the two algorithms' performance is much better than APDAGD. This design of a fast algorithm is a significant achievement for MOT and allows the computational MOT to catch up with the development of Sinkhorn in computational OT.\n\n### Comment 2\n*An observation is that in the dual problem (7), authors should explain how these variables connect with the variables in the primal problem (6). This may help the presentation of the paper.*\n\n**Response:**\n\nThe authors thank the reviewer for the suggestion. Following the suggestion, we added further remarks to the dual variables.\n\n\n### Comment 3\n\n*The title of the paper is \"An Efficient Algorithm for...\". However, the paper spends multiple pages on martingale-type conditions and related problems. The numerical consideration is mainly summarized from the middle of page 5 to the middle of page 6. It sounds like the main work is to figure out the dual objective function and realize it is a convex function. The algorithmic analysis is thin, and this seems to be an inconsistency by this reviewer.*\n\n**Response:**\n\nThe authors thank the reviewer for the remark and the authors agree that the numerical algorithm appears only at the second half of the main text. As mentioned in the reply to Comment 1, the entropic formulation for MOT is not established and a large portion of this manuscript goes into developing the best entropic formulation for MOT. Thus, figuring out what primal formulation to use is a quite important design choice for MOT.\nMoreover, the first half of the work also carries the important message that all row-wise constraints, such as certain kinds of fairness or diversity constraints, would be encoded by a martingale constraint. As the use of MOT in machine learning is a relatively nascent field, the writing also focuses on how fairness or diversity constraints manifest as MOT problems. \n\nIn terms of the composition of the paper, however, we would like to remark that Appendix A contains the implementation detail for the APDAGD algorithm, and Appendix D contains the implementation detail for super-martingale optimal transport problems. If one takes those two sections into consideration, the focus on the numerical algorithm would be quite heavy. The authors do agree with the reviewer that the analysis of this work is rather light. The algorithmic analysis is outside the scope of this work, but we would like to comment that the  APDAGD algorithm has a robust convergence guarantee, and the Sinkhorn-type algorithm significantly outperforms it in practice. Thus, we do anticipate follow-up work to provide good numerical performance guarantee to our approach."
            }
        },
        {
            "title": {
                "value": "Reply to the reviewer (Part 2)"
            },
            "comment": {
                "value": "### Comment 5\n*For the sparse Newton method, the author(s) mention the super-exponential convergence rate, but I do not find any theoretical guarantee. So is it only from empirical observation? Also, I doubt whether the sparsified $H$ is guaranteed to be invertible. If not, how do you address it?*\n\n**Response:**\nThe observation for super-exponential convergence is empirical. The sparsified $H$ follows a very specific truncation structure. Effectively, the sparsified Hessian is the Hessian matrix of a concave function plus a non-positive diagonal matrix. This ensures that the Hessian is negative semi-definite. So any slight regularization of the Hessian by the identity matrix will guarantee an invertible Hessian. We remark that the Hessian matrix regularization for convex optimization is well-established and falls under the name of the Levenberg\u2013Marquardt (LM) algorithm. For our case, we use GMRES for the linear system solving, which provides sufficient numerical stability that the algorithm performs well typically, and we have not found the need to apply LM. If we apply regularization by a small identity matrix as in the LM algorithm, it could also potentially improve numerical performance."
            }
        },
        {
            "title": {
                "value": "Reply to the reviewer (Part 1)"
            },
            "comment": {
                "value": "The authors thank the reviewer for the reviewer's time. If the response addresses some of the reviewer's concerns, we kindly suggest that the reviewer consider increasing the score.\n### Comment 1\n\n*One of my major concerns for this article is the necessity of using a Sinkhorn-type algorithm for solving the entropic MOT problem. The Sinkhorn algorithm is efficient partly because it has closed-form formulas for the two alternating minimization steps. However, in the entropic MOT problem, the author(s) show that the $g$ variables need to be updated using Newton's method. If this is the case, what is the benefit of using the alternating minimization method? We can simply use limited-memory quasi-Newton methods, e.g., L-BFGS, to solve problem (7): L-BFGS is well tested and practically useful for smooth unconstrained problems, the per-iteration cost is also $O(n^2)$, and it has the benefit of avoiding an inner loop for sub-problems.*\n\n**Response:**\n\nThe authors agree that the L-BFGS algorithm is worth considering. We have tried numerical experiments with L-BFGS, but the results were not quite desirable, and as a result we left the part out of the text for simplicity. To fully address the reviwer's question, we will add a section in the appendix showcasing the comparison between the Sinkhorn-type algorithm and L-BFGS before the end of the discussion session.\n\n\n### Comment 2\n\n*In the abstract, the author(s) state that \"As exact martingale conditions are typically infeasible, we adopt entropic regularization to find an approximate constraint satisfied solution\". However, I do not think the logic here is correct. The entropic regularization is not related to the feasibility of the solution, but is used to smooth linear programming (LP) problems for faster computation. The infeasibility issue is addressed by the relaxation of constraints given in equation (4).*\n\n**Response:**\n\nThe authors would like to clarify on this point, this work is based on an LP formulation which asks the solution to approximately satisfy the martingale constraint. This is because an LP formulation with exact constraint satisfaction will typically be infeasible in the sense that the feasibility set is empty. This LP formulation is in equation (4) in the main text. The infeasibility refers to the infeasibility of the LP, which is highly non-trivial. This infeasibility of exact constraint satisfaction is a core feature in MOT, and in contrast this infeasibility doesn't occur in OT.\n\nBased on the LP formulation, we provide an associated entropically regularized problem in equation (6). The motivation here is that the LP in (4) is computationally hard to solve. In contrast, the entropic regularization leads to practical algorithms with $O(n^2)$ iteration complexity.\n\n### Comment 3\n*It is good to have Theorem 1 for the approximation error, but is it a direct application of Corollary 9 of [1]? Because [1] provides analysis for general LP problems, and once you have transformed the problem into a standard form, the results should automatically hold.*\n\n**Response:**\n\nThe authors agree with the reviwer that the proof is straightforward. The entropic regularization formulation for MOT is a novel setting proposed in this work, and the theorem is included for completeness. \n\n\n### Comment 4\n*The sparse Newton algorithm is a good addition to the Sinkhorn-type algorithm, but I feel it largely overlaps with the previous work [2]. Is it simply an application of [2] to entropic MOT?*\n\n\n**Response:**\n\nThe fact that sparse Newton iteration can be done in $O(n^2)$ complexity comes from a highly non-trivial observation of the sparsity pattern of the Hessian matrix. While [2] claims the SNS algorithm would have $O(n^2)$ complexity for $O(1)$ constraints, we show in this work that special constraints structure would permit an $O(n)$ number of constraints to be allowed and one could still have an algorithm with $O(n^2)$ iteration complexity."
            }
        },
        {
            "comment": {
                "value": "The authors thank the reviewer for the reviewer's time. If the response addresses some of the reviewer's concerns, we kindly suggest that the reviewer consider increasing the score.\n\n### Comment 1\n\n\n*The paper lacks clarity in its structure. It is strongly recommended that the authors clearly state the main message at the beginning of each section and subsection. This would improve transitions and prevent readers from feeling confused.\nAlthough the topic is interesting, the absence of a theoretical convergence analysis for the proposed algorithm raises concerns about its suitability for publication in high-tier machine learning conferences.*\n\n\n**Response:**\n\n\nWe would greatly appreciate if the reviewer could provide further context on where the clarity can be improved. While the algorithmic convergence analysis is not included in this work, we would like to mention that the APDAGD algorithm has a robust convergence guarantee, and the Sinkhorn-type algorithm significantly outperforms it in practice, and so we do hope subsequent follow-up work can support the strong numerical findings rigorously.\n\n\n### Comment 2\n\n\n*While the authors claim that the proposed algorithm performs well in practice, they have only benchmarked it against APDAGD. Given the availability of other computational methods for solving MOT, such as those proposed by Guo (2019), a more comprehensive comparison with existing literature in the numerical experiments is needed to robustly demonstrate the algorithm\u2019s efficiency.*\n\n\n**Response:**\n\n\nThe entropic regularization formulation for MOT is novel, and for this setting the authors do not have benchmark algorithms other than general-purpose algorithms such as APDAGD. We mention that the MOT method proposed by Guo (2019) calls for the use of a general LP solver with an interior point method. We mention that the interior point method has an $O(n^3)$ per iteration complexity and is quite difficult to scale to large problems. In the numerical experiment section, we take the setting of $n = 800$, and the LP solver proposed by Guo (2019) would take very long to solve even in that relatively small problem size setting. One of the central aim of this work is to put forward a method that enjoys an $O(n^2)$ iteration complexity, and so our algorithm would not be directly comparable with Guo (2019), as they are in very different settings.\n\n\n### Question 1\n*How do the authors determine the switching criteria for transitioning to Newton's method, and what justifications support their choice?*\n\n\n\n**Response:**\n\nThe switching criteria is currently done by fixing a pre-selected integer for simplicity. One could use an adaptive method by checking for the magnitude of the gradient for the dual potential, and switching to the Newton iteration after the gradient is small. Having a small gradient will mean that the current dual variables will be close to the region where Newton's algorithm would enjoy super-exponential convergence. Typically, an earlier switch to the sparse Newton iteration would not be too disadvantageous, as any variational method will increase the dual potential and make the dual variables closer to the optimal point.\n\n\n### Question 2\n*To ensure a fair comparison, could the authors include the runtime of the warm initialization in the numerical experiments?*\n\n**Response:**\n\nThe runtime of the warm initialization is very mild. The warm initialization for the option pricing example and the assignment example both take about 4.5 seconds.\n\n\n### Question 3\n*Could the authors clarify the technical challenges involved in extending the Sinkhorn algorithm*\n\n**Response:**\n\nThe challenge is that the Sinkhorn's algorithm is done via matrix multiplication and matrix scaling, which are quite conventional numerical linear algebra routines. In contrast, this new setting calls for quite complicated linear system solving to perform the $g$ update step. Even though the Sinkhorn algorithm and the Sinkhorn-type algorithm both perform block-wise ascent methods, the Sinkhorn-type algorithm is more involved and has a longer runtime."
            }
        },
        {
            "title": {
                "value": "Reply to the reviewer"
            },
            "comment": {
                "value": "The authors thank the reviewer for the reviewer's time. If the response addresses some of the reviewer's concerns, we kindly suggest that the reviewer consider increasing the score.\n\n### Comment 1\n\n\n*My main concern is that I am not sure if the choice of an entropy regularization makes sense for the additional constraints. For the standard OT problem, this choice is justified as the dual problem can be solved by exact block coordinate ascent, but if one needs to employ the Newton's method, then, why should not one use e.g. a logarithmic barrier (instead of entropic regularization), which also has the self-concordance property?*\n\n\n\n**Response:**\n\nThe authors agree with the reviewer that a log barrier can be used. Following this choice of log barrier, the associated Newton's method would not have a sparse approximation, and the Newton step would be of cost $O(n^3)$ per iteration. In contrast, the proposed Sinkhorn-type algorithm has an $O(n^2)$ iteration complexity. Thus, it is quite likely that using the entropic barrier for the coupling while using the log barrier for the constraints might be less desirable than simply using the interior point method for the LP in equation (4).\n\n\n### Comment 2\n\n*I appreciate the provided theoretical results, but I think that it still does not show how much the complexity grows with dimensions. A general concern about Theorem 1 is that although it is formulated as an exponential decay, it really shows the requirement that the regularization parameter grows proportionally with the inverse of the duality gap. In practice, the gap can be extremely small for large problems, leading to extremely large regularization parameters. As a result, it is interesting to see how the speed of convergence scales with the regularization parameter (is it linear, for example, as in (Altschuler 2017)?). The theoretical converghence analysis of the algorithm seems to be lacking.*\n\n**Response:**\n\nThe authors agree with the reviewer and would like to remark the exponential convergence analysis depends on the optimality gap, and thus could be less effective for particular cases. There is also a slower rate of $O(1/\\eta)$ convergence, which would also be a good convergence result with a moderate regularization parameter. The authors do not have a concrete answer on the convergence guarantee for the proposed approach, but the numerical experiments we conduct indeed showcase good performance. We hypothesize that a thorough analysis would lead to a comparable guarantee as that of (Altschuler 2017). The manuscript also acknowledges this limitation in the conclusion section."
            }
        },
        {
            "summary": {
                "value": "This paper considers a linear program for the problem of optimal transport with martingale type constraints, which is in the form of standard Kantorocich relaxation with multiple additional linear constraints. Employing an entropic regularization, this work arrives at a dual (variational) framework, which is then solved by block coordinate ascend. A part of this procedure recovers the well-known Sinkhorn algorithm. However, an additional block appears due to the additional constraints, which is then maximized by the Newton's method. It is further shown that the underlying Hessian matrix is sparse, leading to less computations in the Newton steps."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper provides and discusses multiple examples that well justifies the underlying problem in various fields. In support of the choice of entropic regularization, the paper proves that the effect of this term vanishes at an exponential speed w.r.t the growth of the regularization parameter. The resulting algorithm is compared to a state-of-the-art first-orde method, which shows remarkable improvement in the speed of convergence."
            },
            "weaknesses": {
                "value": "I have few concerns related to the algorithmic choices and the theory that will explain tn the next part (questions). \n\nThe theoretical result is a streightforward generalization of the result in (Weed 2018), but still interesting.\n\nThe exaperiments certainly support that the algorithm is applicable to relatively large problems, but the setup of the experiments is still considered small in certain fields such as machine learning. However, this is a general limitation of the Kantorovich formulation, and is not necessarily a limitation for this paper."
            },
            "questions": {
                "value": "1- My main concern is that I am not sure if the choice of an entropy regularization makes sense for the additional constraints. For the standard OT problem, this choice is justified as the dual problem can be solved by exact block coordinate ascent, but if one needs to employ the Newton's method, then, why should not one use e.g. a logarithmic barrier (instead of entropic regularization), which also has the self-concordance property?\n\n2- I appreciate the provided theoretical results, but I think that it still does not show how much the complexity grows with dimensions. A general concern about Theorem 1 is that although it is formulated as an exponential decay, it really shows the requirement that the regularization parameter grows proportionally with the inverse of the duality gap. In practice, the gap can be extremely small for large problems, leading to extremely large regularization parameters. As a result, it is interesting to see how the speed of convergence scales with the regularization parameter (is it linear, for example, as in (Altschuler 2017)?). The theoretical converghence analysis of the algorithm seems to be lacking."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes Sinkhorn-type algorithms with sparse Newton iterations to solve entropic optimal transport under martingale-type constraints. Some numerical experiments have been shown to validate the efficiency of the proposed algorithms."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Focusing on optimal transport with martingale conditions is interesting, given its applications, such as model-free optimal pricing, as highlighted by the authors. Additionally, it would be valuable to explore extensions of Sinkhorn-type and other OT solvers to address this novel class of optimal transport problems."
            },
            "weaknesses": {
                "value": "1. The paper lacks clarity in its structure. It is strongly recommended that the authors clearly state the main message at the beginning of each section and subsection. This would improve transitions and prevent readers from feeling confused.\n\n2. Although the topic is interesting, the absence of a theoretical convergence analysis for the proposed algorithm raises concerns about its suitability for publication in high-tier machine learning conferences.\n\n3. While the authors claim that the proposed algorithm performs well in practice, they have only benchmarked it against APDAGD. Given the availability of other computational methods for solving MOT, such as those proposed by Guo (2019), a more comprehensive comparison with existing literature in the numerical experiments is needed to robustly demonstrate the algorithm\u2019s efficiency."
            },
            "questions": {
                "value": "1. How do the authors determine the switching criteria for transitioning to Newton's method, and what justifications support their choice?\n\n2. To ensure a fair comparison, could the authors include the runtime of the warm initialization in the numerical experiments?\n\n3. Could the authors clarify the technical challenges involved in extending the Sinkhorn algorithm"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this article, the author(s) consider the martingale optimal transport (MOT) problem, and develop a Sinkhorn-type algorithm for solving a relaxed version of MOT. A sparse Newton method is also used to accelerate the Sinkhorn-type algorithm for better convergence speed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The article is well organized, and the overall motivation and story is clear. The MOT problem considered is an extension to the well-known OT problem, and developing efficient algorithm for solving MOT is helpful."
            },
            "weaknesses": {
                "value": "1. One of my major concerns for this article is the necessity of using a Sinkhorn-type algorithm for solving the entropic MOT problem. The Sinkhorn algorithm is efficient partly because it has closed-form formulas for the two alternating minimization steps. However, in the entropic MOT problem, the author(s) show that the $g$ variables need to be updated using Newton's method. If this is the case, what is the benefit of using the alternating minimization method? We can simply use limited-memory quasi-Newton methods, e.g., L-BFGS, to solve problem (7): L-BFGS is well tested and practically useful for smooth unconstrained problems, the per-iteration cost is also $O(n^2)$, and it has the benefit of avoiding an inner loop for sub-problems.\n\n2. In the abstract, the author(s) state that \"As exact martingale conditions are typically infeasible, we adopt entropic regularization to find an approximate constraint satisfied solution\". However, I do not think the logic here is correct. The entropic regularization is not related to the feasibility of the solution, but is used to smooth linear programming (LP) problems for faster computation. The infeasibility issue is addressed by the relaxation of constraints given in equation (4).\n\n3. It is good to have Theorem 1 for the approximation error, but is it a direct application of Corollary 9 of [1]? Because [1] provides analysis for general LP problems, and once you have transformed the problem into a standard form, the results should automatically hold.\n\n4. The sparse Newton algorithm is a good addition to the Sinkhorn-type algorithm, but I feel it largely overlaps with the previous work [2]. Is it simply an application of [2] to entropic MOT?\n\n5. For the sparse Newton method, the author(s) mention the super-exponential convergence rate, but I do not find any theoretical guarantee. So is it only from empirical observation? Also, I doubt whether the sparsified $H$ is guaranteed to be invertible. If not, how do you address it?\n\n[1] Weed, J. (2018). An explicit analysis of the entropic penalty in linear programming. Conference On Learning Theory.\n\n[2] Tang, X., Shavlovsky, M., Rahmanian, H., Tardini, E., Thekumparampil, K. K., Xiao, T., & Ying, L. (2024). Accelerating Sinkhorn algorithm with sparse Newton iterations. International Conference on Learning Representations."
            },
            "questions": {
                "value": "See the \"Weaknesses\" section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work introduces novel computational methods for entropic optimal transport (OT) problems under martingale-type conditions. The OT problems under martingale-type conditions are discussed up to the middle of page 5. Entropic formulation starts in (6) on page 5. Theorem 1 gives an inequality that justifies the entropic regularization formulation. Then optimizing the dual problem is proposed in Section 4, which is consistent with existing approaches. Sinkhorn-type algorithm is proposed. Given the structure of the objective function in dual, the Sinkhorn-type algorithm is reasonable. The sparsity of Hessian is noticed. So, the resulting algorithm is fast."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The literature of numerically solving OT problem is well presented."
            },
            "weaknesses": {
                "value": "Entropic regularization leads to a dual with similar properties to other OT problems. The resulting algorithm is a standard algorithm in the OT literature, the Sinkhorn-type algorithm. It seems that this paper isn't very novel. It's too similar to the existing numerical considerations for OT problems. The martingale-type conditions did not require new mathematical analysis to come up with an algorithm. \n\nAn observation is that in the dual problem (7), authors should explain how these variables connect with the variables in the primal problem (6). This may help the presentation of the paper. \n\nThe title of the paper is \"An Efficient Algorithm for...\". However, the paper spends multiple pages on martingale-type conditions and related problems. The numerical consideration is mainly summarized from the middle of page 5 to the middle of page 6. It sounds like the main work is to figure out the dual objective function and realize it is a convex function. The algorithmic analysis is thin, and this seems to be an inconsistency by this reviewer."
            },
            "questions": {
                "value": "No."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}