{
    "id": "6HfNB34x9I",
    "title": "Learning with Real-time Improving Predictions in Online MDPs",
    "abstract": "In this paper, we introduce the Decoupling Optimistic Online Mirror Descent (DOOMD) algorithm, a novel online learning approach designed for episodic Markov Decision Processes with real-time improving predictions. Unlike conventional methods that employ a fixed policy throughout each episode, our approach allows for continuous updates of both predictions and policies within an episode. To achieve this, the DOOMD algorithm decomposes decision-making across states, enabling each state to execute an individual sub-algorithm that considers both immediate and long-term effects on future decisions. We theoretically establish a sub-linear regret bound for the algorithm, providing a guarantee on the worst-case performance.",
    "keywords": [
        "Online learning",
        "Markov decision process",
        "regret analysis",
        "predictions"
    ],
    "primary_area": "learning theory",
    "TLDR": "An online learning algorithm designed for episodic Markov Decision Processes with real-time improving predictions.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=6HfNB34x9I",
    "pdf_link": "https://openreview.net/pdf?id=6HfNB34x9I",
    "comments": [
        {
            "summary": {
                "value": "This paper studies online episodic MDP with time-varying cost functions and predictions. A novel algorithm, Decoupling Optimistic Online Mirror Descent (DOOMD), is proposed to update both predictions and policies throughout the episodes. A sublinear regret guarantee is also established to demonstrate the effectiveness of the proposed algorithm."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Involving real-time predictions in online MDP is an interesting idea since many real-world applications have certain predictions on the future costs.\n\n2. The paper is well-written. The algorithm procedures are well explained.\n\n3. The proposed algorithm can update its predictions and policies during the episode instead of at the end of each episode, which has some practical appeal."
            },
            "weaknesses": {
                "value": "1. One of my major concerns is about the assumptions. This paper considers a deterministic transition function but claims that it can be easily generalized to stochastic transitions. Can the authors provide more details on this generalization?\n\n2. The lack of simulation results is another major weakness of this paper. The authors should provide some numerical justifications of their algorithm, hopefully in both deterministic cases and stochastic cases.\n\n3. It is true that any episodic MDP can be transformed into a loop-free MDP. However, this comes at the cost of enlarging the state space. How does this transformation affect the regret bounds' dependence on dimensionality and episode length?"
            },
            "questions": {
                "value": "There are several other papers considering predictions in online learning and online control, such as [C1] [C2]. \n\nQ1: How does the prediction model compare with the prediction models considered in [C1] and [C2]? \n\nQ2: Besides, can the regret analysis in this paper be generalized to the prediction model in [C1] and [C2]? \n\n\n\n[C1] Li, Y., Chen, X. and Li, N., 2019. Online optimal control with linear dynamics and predictions: Algorithms and regret analysis. Advances in Neural Information Processing Systems, 32.\n\n[C2] Li, Y. and Li, N., 2020. Leveraging predictions in smoothed online convex optimization via gradient-based algorithms. Advances in Neural Information Processing Systems, 33, pp.14520-14531."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes an algorithm to solve episodic MDP with deterministic transitions by allowing the policies to update continuously within an episode. To this end, the paper builds on Optimistic mirror descent (OMD), which provides a prediction functionality via the so-called predictable sequences."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The model is novel and exciting; while the broad area of improving RL with predictions is not new, the methodology and the model are new.\nThe methodology nicely adopts the optimistic mirror descent technique for solving deterministic episodic MDP with clear and complete regret analysis."
            },
            "weaknesses": {
                "value": "1) I have significant concerns about the clarity of the content presentation and layout, e.g.,\nAlgorithm 2 is incomprehensible without looking at the appendix, which does not seem to be a good workaround around the page limit of the submission at the cost of clarity.\n\n2) To my understanding, in episodic MDPs, the learned policy is itself non-stationary, I.e., it depends on h. The policy takes into account how many times steps are remaining in the episode. While the setting is different here it is not clear what is the baseline case that the paper is trying to contrast. Can something easier/computationally faster be done when there are no predictions, what will be the regret then?\n\n3) The nature of predictions is not clearly defined in the introduction. The authors need to consider a comparison with a large body of work with online learning with predictions (which is not done) e.g. https://proceedings.mlr.press/v119/bhaskara20a/bhaskara20a.pdf (Online Learning with Imperfect Hints) and related papers cited and citing the linked paper.\n\n4)Lines 181-182 say the methodology can be generalized to stochastic transition. How?\n\n5)The model has unbounded robustness. Ideally, it is expected that the model should be robust when errors in the predictions are really high. What can be done to mitigate this?\n\n6)While the paper builds on the ideas of Optimistic mirror descent for predictable loss sequences, it does not explain the key technical insights that make the algorithm applicable to this setting. This is an important constituent of a well-written theory paper."
            },
            "questions": {
                "value": "Please refer to the above section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "none"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new framework that enables a more general and flexible interaction between the learner and the environment in online episodic MDPs. Different from traditional methods that use a fixed policy throughout each episode, this framework allows for updates to both predictions and policies within an episode.\n\nThe authors introduce the concept of cumulative cost, which considers both immediate costs and the long-term effects on future decisions. Building on this idea, they propose the Decoupling Optimistic Online Mirror Descent (DOOMD) algorithm.\n\nA \\sqrt{T} regret bound is established in this work."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper is straightforward and easy to follow.\n\nIt introduces a new framework that facilitates a more general and flexible interaction between the learner and the environment in online episodic MDPs."
            },
            "weaknesses": {
                "value": "This paper appears to consider only the deterministic transition case, though it mentions that the approach can be easily generalized to the stochastic case. Could you provide more details on how this generalization would work?\n\nThe paper lacks technical novelty, as many of the proofs are largely adapted from previous works such as Zimin & Neu (2013).\n\nIt seems that this paper assumes the agent has access to the cumulative cost, which is a stronger assumption. However, it\u2019s unclear if this assumption actually leads to an improved regret bound.\n\nRelated to the above, it's not clear how the regret bound of DOOMD compares to those in previous studies. Could you include further discussion on the sharpness of the results, e.g., in terms of factors like \u2223S\u2223 and \u2223A \u2223? Without stronger guarantees, it\u2019s hard to see the why allowing changing the policy within the episode is favorable, especially since it doesn\u2019t achieve a better regret bound."
            },
            "questions": {
                "value": "This paper addresses the transition to a from a nonlayered structure to a layered structure. However, the sharp bounds in layered Markov Decision Processes (MDPs) do not appear to be easily transferable to unlayered MDPs and vice versa. A straightforward conversion on a bound between these two settings could result in a more relaxed dependence on H.\n\nIt appears that the paper assumes the learner receives an updated cost prediction for state-action pairs across all subsequent layers. Can you provide a motivating example to justify this assumption in real-world scenarios, particularly when dealing with a stochastic transition function? Additional elaboration on this would be helpful.\n\nThe main text lacks self-containment. It would be beneficial to discuss some high-level concepts of Algorithms 3 to 5 within the main text."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduce the Decoupling Optimistic Online Mirror Descent (DOOMD) algorithm, a novel approach for episodic Markov Decision Processes that incorporates real-time updates in predictions. Unlike traditional methods with fixed policies per episode, DOOMD continuously adjusts both predictions and policies. By decomposing decision-making across states, each state executes a unique sub-algorithm that considers immediate and future decision impacts. This paper also establish a sub-linear regret bound for DOOMD, ensuring a worst-case performance guarantee."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- While computing different policies across episodes or steps is an established concept, this paper introduces a novel approach by computing an optimal policy based on distinct subspaces within the state space. Unlike prior decision-making algorithms in non-stationary or time-varying environments that focus on optimal policy computation over the time dimension, this work innovates by emphasizing optimal policy computation across the spatial dimension of the state space."
            },
            "weaknesses": {
                "value": "Thanks for the paper and the efforts. Please see the *questions* for the weakness."
            },
            "questions": {
                "value": "- The reviewer understands that the primary algorithm divides the state space into subspaces and computes policies independently for each. Could the authors clarify how, in practical terms, one might decide on these partitions?\n\n- On line 38: It appears that the route $1 \\to 2 \\to 3$ is more optimal than $1 \\to 2 \\to 4$ for Figure 1-(a). Please verify this.\n\n- On line 56: Could you please be specific on how the current paper's approaches can help the development of Large Language Models?\n\n- Please consider referring to the following papers that use predictions to dynamically update policies (or relate to optimal early stopping for policy updates):\n\n  - Lee, H., Jin, M., Lavaei, J., & Sojoudi, S. Pausing Policy Learning in Non-stationary Reinforcement Learning. Forty-first International Conference on Machine Learning.\n  - Pettet, G., Mukhopadhyay, A., & Dubey, A. (2022). Decision Making in Non-stationary Environments with Policy-Augmented Monte Carlo Tree Search. arXiv preprint arXiv:2202.13003.\n\n- Regarding the partition $\\mathcal{X} = \\bigcup_{l \\in \\mathcal{L}} \\mathcal{X}^l$, are $\\mathcal{X}^l, l \\in [L]$ disjoint sets?\n\n- Lemma 3.1 would benefit from further elaboration. Could the authors clarify the significance of Lemma 3.1 and its implications?\n\n- On line 190: This equation seems to need adjustment. Here, $M^l_t$ is defined as the prediction cost from $l$ to $L-1$, with the cost\u2019s input as $\\mathcal{U}$. Should there be a new notation, such as $c^l_t$, representing the actual cost function from $\\mathcal{U}^l_t \\to [0,1]$?\n\n- With this in mind, Assumption 3.2 appears somewhat straightforward, as $c_t(u)$ and $M^l_t$ account for different input lengths."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}