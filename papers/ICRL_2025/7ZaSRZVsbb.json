{
    "id": "7ZaSRZVsbb",
    "title": "Rethinking the Expressiveness of GNNs: A Computational Model Perspective",
    "abstract": "Graph Neural Networks (GNNs) are extensively employed in graph machine learning, with considerable research focusing on their expressiveness. Current studies often assess GNN expressiveness by comparing them to the Weisfeiler-Lehman (WL) tests or classical graph algorithms. However, we identify three key issues in existing analyses: (1) some studies use preprocessing to enhance expressiveness but overlook its computational costs; (2) some claim the anonymous WL test's limited power while enhancing expressiveness using non-anonymous features, creating a mismatch; and (3) some characterize message-passing GNNs (MPGNNs) with the CONGEST model but make unrealistic assumptions about computational resources, allowing $\\textsf{NP-Complete}$ problems to be solved in $O(m)$ depth. We contend that a well-defined computational model is urgently needed to serve as the foundation for discussions on GNN expressiveness. To address these issues, we introduce the Resource-Limited CONGEST (RL-CONGEST) model, incorporating optional preprocessing and postprocessing to form a framework for analyzing GNN expressiveness. Our framework sheds light on computational aspects, including the computational hardness of hash functions in the WL test and the role of virtual nodes in reducing network capacity. Additionally, we suggest that high-order GNNs correspond to first-order model-checking problems, offering new insights into their expressiveness.",
    "keywords": [
        "Graph Neural Networks",
        "Expressive Power",
        "Computational Model",
        "Weisfeiler-Lehman Test"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=7ZaSRZVsbb",
    "pdf_link": "https://openreview.net/pdf?id=7ZaSRZVsbb",
    "comments": [
        {
            "title": {
                "value": "Initial Response to Reviewer b62D (2/2)"
            },
            "comment": {
                "value": "**References**:\n\n[Pritchard, 2006] David Pritchard. An Optimal Distributed Edge-Biconnectivity Algorithm. arXiv 2006.\n\n[Loukas, 2020] Andreas Loukas. What Graph Neural Networks Cannot Learn: Depth vs Width. ICLR 2020.\n\n[Zhang et al., 2023] Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the Expressive Power of GNNs via Graph Biconnectivity. ICLR 2023.\n\n[Xu et al., 2019] Keyulu Xu*, Weihua Hu*, Jure Leskovec, Stefanie Jegelka. How Powerful are Graph Neural Networks? ICLR 2019.\n\n[Suomela, 2013] Jukka Suomela. Survey of Local Algorithms. ACM Computing Surveys (CSUR), 45(2):24, 2013.\n\n[den Berg et al., 2018] Rianne van den Berg, Thomas N Kipf, and Max Welling. Graph Convolutional Matrix Completion. KDD 2018.\n\n[You et al., 2021] Jiaxuan You, Jonathan M Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-aware Graph Neural Networks. AAAI 2021.\n\n[Abbound et al., 2021] Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The Surprising Power of Graph Neural Networks with Random Node Initialization. IJCAI 2021.\n\n[Sato et al., 2021] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random Features Strengthen Graph Neural Networks. SDM 2021."
            }
        },
        {
            "title": {
                "value": "Initial Response to Reviewer b62D (1/2)"
            },
            "comment": {
                "value": "Dear Reviewer b62D,\n\nThank you for taking the time to review our paper. We would like to address your concerns as follows:\n\n**W1**:\n\nOur primary goal is to reveal limitations in current analyses of GNNs' expressive power and to introduce a new analytical approach that addresses these issues, rather than to develop a specific GNN model with enhanced performance or expressiveness. Specifically, as demonstrated in Theorems 5-8, we leverage the RL-CONGEST framework to provide a more reasonable evaluation of GNNs' expressive power on simulating one iteration of the WL test. Additionally, in Section 5, we also propose open questions that may be investigated within the RL-CONGEST framework. It is important to note that our RL-CONGEST framework is designed to assess a model's expressive power in executing algorithmic tasks or achieving \"algorithmic alignment\", rather than to predict its quantitative performance on learning tasks such as node classification.\n\n**W2**:\n\nYes, the second half of your question, \"different features have varying degrees of power; some can help count more complex graph structures than others\", precisely reflects what we aim to convey. Under the non-anonymous setting, CONGEST and MPGNNs can exhibit greater expressiveness than the anonymous WL test. Our main argument in Section 3.2 is that while existing works claim their models' expressiveness advantage by proving they can perform tasks beyond the WL test's scope, this approach is questionable. Equating anonymous WL with MPGNNs, as previous works have done, is not entirely reasonable, and consequently, concluding that MPGNNs are weak because the WL test is weak is also debatable. In fact, MPGNNs can perform certain algorithms (such as solving edge biconnectivity in $O(D)$ rounds within the CONGEST model [Pritchard, 2006]).\n\nOur logical flow is as follows:\n1. Numerous studies claim that the vanilla WL test has limited expressive power --- a claim that we affirm, as discussed in Figure 2. However, the appropriateness of using the anonymous WL test to characterize MPGNNs is debatable, given that real-world graphs often contain rich features. Additionally, [Loukas, 2020] demonstrated that with unique IDs (and other assumptions), MPGNNs can perform a wide range of algorithmic tasks.\n2. To address the \"limited\" expressiveness of MPGNNs (stemming from the limitation of the vanilla WL test), some works incorporate additional features (e.g., [Loukas, 2020]) to enhance the expressiveness of their proposed models. Nonetheless, as outlined in (1), the suitability of the anonymous WL test as a characterization for MPGNNs is questionable. Consequently, the practice in some studies of demonstrating the advantage of their model's expressiveness by proving it can perform algorithmic tasks beyond the WL test's capabilities may not be entirely valid. A more reasonable approach would be to compare these models with MPGNNs under a non-anonymous setting (as suggested in [Loukas, 2020]). Further, evidence from [Loukas, 2020; Suomela, 2013; den Berg et al., 2018; You et al., 2021; Abbound et al., 2021; Sato et al., 2021] suggests that the non-anonymous setting can enhance model expressiveness, highlighting a mismatch in works that argue for a \"weak MPGNN\" yet use additional features that break the anonymous setting in the WL test to improve expressiveness.\n3. As you mentioned, \"different features have varying degrees of power; some can help count more complex graph structures than others\", our RL-CONGEST analysis framework can be applied in studies proposing new GNN variants that use additional features and claim the ability to perform certain algorithmic tasks, with the only requirement being a **clear specification of the preprocessing time** complexity of the features.\n\nAdditionally, points (1) and (2) highlight the need to reconsider the validity of comparing a proposed model's expressiveness directly with the vanilla WL test. We hope this discussion encourages the community to more accurately assess existing results on GNNs' expressiveness.\n\nThank you again for reviewing our paper, and we are looking forward to any further discussions with you."
            }
        },
        {
            "title": {
                "value": "Initial Response to Reviewer DTJH (3/3)"
            },
            "comment": {
                "value": "**Regarding Other Distributed Computing Models**:\n\nIndeed, we are aware of various distributed computing models, such as the CONGEST-CLIQUE, Coordinator, and Blackboard models. Some of these models can be considered special cases of the CONGEST model. For example, the CONGEST-CLIQUE model can be implemented by adding virtual edges to make the original graph a complete graph; the Coordinator model can be implemented by adding a virtual node connected to all other nodes. However, the LOCAL and CONGEST models are still the most widely mentioned in distributed computing books [Peleg 2000], courses [Hirvonen et al., 2020; Ghaffari, 2022], and conferences, so we chose to focus our discussion on these two. Additionally, some of our ideas are inspired by [Loukas, 2020], which explores the relationship between GNNs and these two models. Our framework generalizes their results, but it is based on the CONGEST model.\n\n\nThank you again for your detailed feedback. We hope our response clarifies our approach and addresses your concerns. We look forward to any further discussions.\n\n\n**References**:\n\n[Merrill et al., 2024] William Merrill, and Ashish Sabharwal. The Expressive Power of Transformers with Chain of Thought. ICLR 2024.\n\n[Li et al., 2024] Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of Thought Empowers Transformers to Solve Inherently Serial Problems. ICLR 2024.\n\n[Azizian et al., 2021] Waiss Azizian, and Marc Lelarge. Expressive Power of Invariant and Equivariant Graph Neural Networks. ICLR 2021.\n\n[Wang et al., 2022] Xiyuan Wang, and Muhan Zhang. How Powerful are Spectral Graph Neural Networks. ICML 2022.\n\n[Loukas, 2020] Andreas Loukas. What Graph Neural Networks Cannot Learn: Depth vs Width. ICLR 2020.\n\n[Pritchard, 2006] David Pritchard. An Optimal Distributed Edge-Biconnectivity Algorithm. arXiv 2006.\n\n[Xu et al., 2019] Keyulu Xu*, Weihua Hu*, Jure Leskovec, Stefanie Jegelka. How Powerful are Graph Neural Networks? ICLR 2019.\n\n[Zhang et al., 2023] Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the Expressive Power of GNNs via Graph Biconnectivity. ICLR 2023.\n\n[Suomela, 2013] Jukka Suomela. Survey of Local Algorithms. ACM Computing Surveys (CSUR), 45(2):24, 2013.\n\n[den Berg et al., 2018] Rianne van den Berg, Thomas N Kipf, and Max Welling. Graph Convolutional Matrix Completion. KDD 2018.\n\n[You et al., 2021] Jiaxuan You, Jonathan M Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-aware Graph Neural Networks. AAAI 2021.\n\n[Abbound et al., 2021] Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The Surprising Power of Graph Neural Networks with Random Node Initialization. IJCAI 2021.\n\n[Sato et al., 2021] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random Features Strengthen Graph Neural Networks. SDM 2021.\n\n[Peleg, 2000] Distributed Computing: A Locality-Sensitive Approach. 2000.\n\n[Hirvonen et al., 2020] Juho Hirvonen and Jukka Suomela. Distributed Algorithms (course). 2020. https://jukkasuomela.fi/da2020/\n\n[Ghaffari, 2022] Mohsen Ghaffari. Distributed Graph Algorithms (course). 2022. https://people.csail.mit.edu/ghaffari/DA22/Notes/DGA.pdf"
            }
        },
        {
            "title": {
                "value": "Initial Response to Reviewer DTJH (2/3)"
            },
            "comment": {
                "value": "**On Additional Features Enhancing Models by Breaking Anonymity**:\n\nOur framework permits nodes to access unique IDs, but this **does not imply that models must use them**. This flexible setting is compatible with various feature types, including pseudo-identifiers or molecular types, as you mentioned. This choice is motivated by our observation that existing works often equate MPGNNs' expressive power with the anonymous WL test, which we find to be a mismatch due to the questionable anonymous setting. In Section 3.2, we aim to point out that previous works' equating anonymous WL with MPGNNs is not entirely reasonable, and thus concluding that MPGNNs are weak because WL test is weak is also debatable. In fact, MPGNNs can perform certain algorithms (such as solving edge biconnectivity in $O(D)$ rounds within the CONGEST model [Pritchard, 2006], Lines 311-313).\n\nFor clarity, we summarize the logical flow of Section 3.2 as follows:\n1. Numerous studies following the seminal work GIN [Xu et al., 2019] claim that the vanilla WL test has limited expressive power --- a claim that is true, as shown in Figure 2. However, the appropriateness of using the **anonymous WL test to characterize MPGNNs is debatable**, given that real-world graphs frequently contain rich features. Additionally, [Loukas, 2020] demonstrated that with unique IDs (and other assumptions), MPGNNs can perform a wide range of algorithmic tasks.\n2. To address the \"limited\" expressiveness of MPGNNs (stemming from the WL test's limitations), some works incorporate additional features (e.g., [Zhang et al., 2023]) to increase their models' expressiveness. Nonetheless, as discussed in (1), the anonymous WL test may not be the appropriate characterization for MPGNNs. Consequently, some studies' approach of demonstrating their model's expressiveness advantage by proving it can perform tasks beyond the WL test's capabilities may not be entirely valid. A more reasonable comparison would use MPGNNs in a non-anonymous setting (as suggested in [Loukas, 2020]). Further, evidence from [Loukas, 2020; Suomela, 2013; den Berg et al., 2018; You et al., 2021; Abboud et al., 2021; Sato et al., 2021] shows that non-anonymous settings can enhance model expressiveness, highlighting a mismatch when studies argue for \"weak MPGNNs\" yet use features that break the WL test's anonymity to boost expressiveness.\n3. Our framework allows nodes to know their unique IDs, though this is **optional**. This flexibility is compatible with the use of features such as \"a few different atom types in molecular tasks\". Our RL-CONGEST analysis framework can apply to studies proposing new GNN variants that leverage additional features and claim the ability to perform specific algorithmic tasks, with the only requirement being a **clear specification of the preprocessing time complexity for these features**.\n\n**On \"Lacks Direct Applicability to Fixing or Ranking GNN Architectures\"**:\n\nOur RL-CONGEST analysis framework has practical applications, as illustrated through results like model checking. For example, we show that $k$-WL GNNs can perform PNF $C^k$ model checking --- a class of significant problems in theoretical computing --- while previous research aligned with WL tests, which are equivalent to the model equivalence problem. These results are discussed in detail in Section 4.3. However, please note that our paper's primary goal is to **highlight issues** in existing studies on GNN expressiveness and to propose a new analytical framework that **avoids these issues**. We do not aim to design a specific GNN model with improved performance or expressiveness or to provide guidance for such future work. Rather, we hope our framework will assist future research by helping to avoid issues discussed in Section 3 and encouraging a **re-evaluation of common assumptions** in GNN expressiveness studies.\n\n**On Subgraph GNNs**:\n\nThank you for providing references to additional models. We have incorporated these references into the paper and marked them in red (Lines 43-44)."
            }
        },
        {
            "title": {
                "value": "Initial Response to Reviewer DTJH (1/3)"
            },
            "comment": {
                "value": "Dear Reviewer DTJH,\n\nThank you for reviewing our paper. We are very grateful for your detailed feedback and appreciate the opportunity to address some misunderstandings that may have arisen in the \u201cWeaknesses\u201d section of your review.\n\n**Regarding Unlimited Computational Resources in the CONGEST Model**:\n\nWe respectfully disagree with the comment that \"just use a more restrictive computation class for the node updates\". Our goal is to introduce flexible constraints on the resources class $\\mathsf{C}$ to derive different independent results, as discussed in Lines 381-389. For instance, setting $\\mathsf{C} = \\mathsf{R}$ (the class of recursive languages decidable by Turing machines) and network width $w = O(1)$ transforms our RL-CONGEST framework into the CONGEST model. By setting $\\mathsf{C}$ to a class such as $\\mathsf{TC}^0$, which reflects the capabilities of MLPs, the resulting model would resemble \"real-world\" GNNs with MLPs as update functions. Alternatively, if node update functions used transformer-based LLM agents enhanced by Chain-of-Thought (CoT) reasoning, which are claimed to solve problems in $\\mathsf{P}$ exactly [Merrill et al., 2024; Li et al., 2024], we could set $\\mathsf{C} = \\mathsf{P}$ to derive new theoretical results based on this adjustment. We hope that our framework can inspire future research on graph agents, and have added it in red color in the revised PDF (Lines 384-387). As discussed in Lines 381-389, adjusting $\\mathsf{C}$ in different ways may yield diverse outcomes, making our RL-CONGEST framework a \"framework scheme\" or \"framework template\".\n\nWe respect your statement that \"is more defined by approximation quality of whatever computation it needs to perform\". We recognize the importance of the Universal Approximation Theorem (UAT) in machine learning and are aware of work addressing the approximation capabilities of GNNs, such as [Azizian et al., 2021; Wang et al., 2022]. However, as indicated by our paper's title, our work aligns with a different research path, focusing on a model's expressiveness through its capability to perform algorithmic tasks. For example, [Loukas, 2020] uses the CONGEST model to analyze MPGNNs' algorithmic abilities, while the Outstanding Paper at ICLR 2023 [Zhang et al., 2023] assesses GNNs' power to determine graph biconnectivity. These two lines of research --- expressiveness for algorithmic tasks versus approximation quality --- are largely orthogonal and develop independently. Additionally, discussions in the literature (e.g., Section 1.1 in [Loukas, 2020], which states that \"**Turing completeness is a strictly stronger property** than universal approximation\") suggest that Turing completeness is indeed a stronger property than universal approximation. Therefore, we believe that our focus on computability is sufficiently general and without loss of scope."
            }
        },
        {
            "title": {
                "value": "Initial Response to Reviewer YAM3 (3/3)"
            },
            "comment": {
                "value": "**References**:\n\n[Zhang et al., 2023] Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the Expressive Power of GNNs via Graph Biconnectivity. ICLR 2023.\n\n[Thiede et al., 2021] Erik H. Thiede, Wenda Zhou, Risi Kondor. Autobahn: Automorphism-based Graph Neural Nets. NeurIPS 2021.\n\n[Bouritsas et al., 2022] Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M. Bronstein. Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting. TPAMI, 2022.\n\n[Wollschlager et al., 2024] Tom Wollschlager, Niklas Kemper, Leon Hetzel, Johanna Sommer, and Stephan Gunnemann. Expressivity and Generalization: Fragment-biases for Molecular GNNs. ICML 2024.\n\n[Pritchard, 2006] David Pritchard. An Optimal Distributed Edge-Biconnectivity Algorithm. arXiv 2006.\n\n[Loukas, 2020] Andreas Loukas. What Graph Neural Networks Cannot Learn: Depth vs Width. ICLR 2020.\n\n[Suomela, 2013] Jukka Suomela. Survey of Local Algorithms. ACM Computing Surveys (CSUR), 45(2):24, 2013.\n\n[den Berg et al., 2018] Rianne van den Berg, Thomas N Kipf, and Max Welling. Graph Convolutional Matrix Completion. KDD 2018.\n\n[You et al., 2021] Jiaxuan You, Jonathan M Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-aware Graph Neural Networks. AAAI 2021.\n\n[Abbound et al., 2021] Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The Surprising Power of Graph Neural Networks with Random Node Initialization. IJCAI 2021.\n\n[Sato et al., 2021] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random Features Strengthen Graph Neural Networks. SDM 2021.\n\n[Merrill et al., 2024] William Merrill, and Ashish Sabharwal. The Expressive Power of Transformers with Chain of Thought. ICLR 2024.\n\n[Li et al., 2024] Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of Thought Empowers Transformers to Solve Inherently Serial Problems. ICLR 2024."
            }
        },
        {
            "title": {
                "value": "Initial Response to Reviewer YAM3 (2/3)"
            },
            "comment": {
                "value": "**W3**:\n\nYes, the CONGEST model can still serve as an upper bound for computational capacity. Our point is that selecting the unrestricted CONGEST model as the computational model for GNNs would yield impractical outcomes, such as $O(m)$-depth GNNs that could theoretically solve $\\mathsf{NP}$-$\\mathsf{complete}$ problems on connected graphs (we appreciate your feedback on Theorem 4 and have now corrected this condition in red color, Lines 323). In reality, we cannot expect a polynomial-sized neural network to solve $\\mathsf{NP}$-$\\mathsf{complete}$ problems without error (unless $\\mathsf{P} = \\mathsf{NP}$). This misalignment results from overly strong assumptions about nodes' computational power. Our intention is to introduce **flexible constraints** on the computational resources class $\\mathsf{C}$ to derive independent results, as is discussed in Lines 381-389. For instance, by setting $\\mathsf{C}$ to a class reflecting MLPs, such as $\\mathsf{TC}^0$, the resulting model would resemble \"real-world\" GNNs with MLPs as update functions. Alternatively, if nodes' update functions used transformer-based LLM agents enhanced by Chain-of-Thought (CoT) reasoning, which are claimed to solve problems in $\\mathsf{P}$ [Merrill et al., 2024; Li et al., 2024], we could set $\\mathsf{C} = \\mathsf{P}$ and derive new theoretical results based on this adjustment. We hope our framework can inspire future research on graph agents, and have added it in red color in the revised PDF (Lines 384-387). As discussed in Lines 381-389, adjusting $\\mathsf{C}$ in different ways may yield diverse outcomes. Thus, our RL-CONGEST framework serves as a \"framework scheme\" or \"framework template\".\n\n**W4**:\n\nAs noted in the first open problem in Section 5, deriving general resource-round tradeoffs for the RL-CONGEST model is challenging, and we leave this problem for future work. \n\nWe also believe your statement \"GNNs are usually implemented with fixed-size networks that run in constant time\" may not be accurate, as a node $v$'s aggregation function takes at least $\\Omega(d(v))$ time. Moreover, studies aligning GNNs' expressiveness with the WL test assume that MLPs can execute $\\mathsf{HASH}$ functions for node recoloring \u2014-- an assumption whose practicality is also debatable. Your question supports our idea that WL tests may not be as straightforward as prior studies suggest, which aligns with our findings in Theorem 5. Regarding concerns about our framework's practicality, **Theorems 5-8** illustrate our RL-CONGEST model's application in analyzing the unreasonableness of certain assumptions in previous studies. We invite you to kindly review these examples.\n\n**Q1**:\n\nPlease note that our paper aims to conduct a theoretical analysis that identifies issues in existing studies on the expressive power of GNNs and to propose a new framework that avoids these issues. We do not intend to design a specific GNN model with improved performance or expressiveness, nor to offer guidance for future work directed toward these goals.\n\n**Q2**:\n\nAs mentioned in our response to W3, we do not treat our entire framework, including the RL-CONGEST model with preprocessing and postprocessing, as a \"benchmark model\". Rather, it functions as a \"framework scheme\" or \"framework template\". We hope our framework will assist future research on GNN expressiveness by helping to **avoid issues discussed in Section 3** and **encouraging a re-evaluation** of the validity of common assumptions in the field.\n\n**Q3**:\n\nWe believe this point is addressed in Lines 381-389, and we reiterate it in our response to W3. Adjusting $\\mathsf{C}$ in different ways may lead to varied outcomes. For example, setting $\\mathsf{C} = \\mathsf{R}$  (recursive languages, which Turing machines can decide) and network width $w = O(1)$ turns our RL-CONGEST model into the CONGEST model. Thus, the RL-CONGEST model can be seen as a generalization of the standard CONGEST model, allowing flexible settings on the computational resource class $\\mathsf{C}$.\n\n**Q4**:\nAs discussed in our response to W3, there is no universally \"appropriate\" complexity class for all GNN researchers. Researchers focused on current MPGNNs with MLP-based update functions might set $\\mathsf{C} = \\mathsf{TC}^0$ or $\\mathsf{AC}^0$ to derive their theoretical results, while those interested in graph agents could set $\\mathsf{C} = \\mathsf{P}$. By setting $\\mathsf{C} = \\mathsf{R}$, our model also connects to CONGEST algorithms, so results proposed by Loukas [Loukas, 2020] are special cases within our RL-CONGEST framework.\n\nThank you again for your detailed feedback. We hope our response addresses your concerns and questions to some extent, and we look forward to further discussions with you."
            }
        },
        {
            "title": {
                "value": "Initial Response to Reviewer YAM3 (1/3)"
            },
            "comment": {
                "value": "Dear Reviewer YAM3,\n\nWe are very grateful for your detailed feedback and appreciate the opportunity to address some misunderstandings.\n\n**W1**:\n\nYes, we use two examples to demonstrate that preprocessing complexity is often underestimated in the literature. Please note that the GD-WL paper [Zhang et al., 2023] was awarded ***Outstanding Paper at ICLR 2023***. We chose this work as it is representative enough: its recognition by the community underscores that **even well-regarded papers can exhibit this \"underestimated preprocessing complexity\" issue**, making it a persuasive example to support our claim. However, we have also identified other examples, such as [Thiede et al., 2021, Bouritsas et al., 2022], which use hand-crafted features by recognizing subgraphs. The theoretical analysis also suggests that the proposed model achieves full expressiveness only when the subgraph is unrestricted, which is the same as [Wollschlager et al., 2024]. We have added them in Section 3.1 in red (Lines 212-216 in the revised PDF). Listing every example exhaustively is infeasible, so we have selected a recent example from ICML'24 [Wollschlager et al., 2024] and the notable ICLR'23 outstanding paper [Zhang et al., 2023] to substantiate our point in Section 3.1.\n\nAdditionally, we **respectfully disagree** with \"computational complexity might just not be the main focus\".\n+ First, for [Zhang et al., 2023] (and similar works), the authors show the model's expressiveness by assessing its capability in performing algorithmic tasks, making time complexity crucial in evaluating their results.\n+ Second, while the authors discuss preprocessing time complexity (as noted in Lines 223-230), their GD-WL framework requires $O(\\min\\\\{mn, n^{\\omega}\\\\})$ time to precompute all-pair resistance distances (RDs), though the target algorithmic task --- determining biconnectivity -- only requires $O(m)$ time. Additionally, as stated in our Theorem 2, RDs can **directly imply edge biconnectivity**; thus, the **message-passing phase is actually unnecessary for this task in GD-WL framework** when RDs are precomputed. We argue that **overlooking the comparison between preprocessing time and the task's time complexity** leads to questionable conclusions.\n+ Third, we also found that a CONGEST model proposed by [Pritchard, 2006] can solve the **edge biconnectivity problem in $O(D)$ rounds** (we add this in Lines 311-313 in red color). [Loukas, 2020] further suggests that the CONGEST model can handle many algorithms. These findings highlight that with unique IDs, MPGNNs might indeed solve the biconnectivity problem, supporting our view and **challenging studies that rely on WL tests** --- which they deem \"weak\" --- to define MPGNN expressiveness.\n\n**W2**:\n\nThank you for your suggestion on clarifying this mismatch. Your review aligns with our discussion in the paper. Our main argument is that while existing works claim the proposed models' expressiveness advantage by proving they can perform tasks beyond the WL test's scope, this approach is questionable. The previous works' equating anonymous WL with MPGNNs is not entirely reasonable, and thus concluding that MPGNNs are weak because WL test is weak is also debatable. In fact, MPGNNs can perform certain algorithms (such as solving edge biconnectivity in $O(D)$ rounds within the CONGEST model [Pritchard, 2006], Lines 311-313). The logical flow of Section 3.2 is as follows:\n1. The claim that the vanilla WL test has limited expressive power is true, as discussed in Figure 2. However, real-world graphs often contain rich features, and [Loukas, 2020] demonstrated that with unique IDs (and other assumptions), MPGNNs (Loukas used CONGEST to characterize) can perform a wide range of algorithmic tasks. Thus, using the anonymous WL test to characterize MPGNNs is debatable.\n2. To address MPGNNs' \"limited\" expressiveness (stemming from the vanilla WL test's limitations, as many works use the WL test to characterize GNNs), some studies, such as [Zhang et al., 2023], incorporate additional features to enhance model expressiveness. Nonetheless, as outlined in (1), using the anonymous WL test as a characterization of MPGNNs is questionable. Consequently, demonstrating a model's expressiveness by proving it can perform tasks beyond the WL test's capabilities may not be entirely valid.\n3. A more reasonable approach would be to compare these models to MPGNNs in a **non-anonymous setting**, as suggested in [Loukas, 2020]. Furthermore, evidence from [Suomela, 2013; den Berg et al., 2018; You et al., 2021; Abbound et al., 2021; Sato et al., 2021] indicates that the non-anonymous setting can enhance expressiveness, again highlighting the mismatch when works argue for \"weak MPGNNs\" but use additional features, breaking the WL test's anonymous setting to enhance expressiveness.\n\nWe have revised the introduction in Section 3.2 (Lines 253-258) and highlighted the changes in red to clarify our points more effectively."
            }
        },
        {
            "title": {
                "value": "Initial Response to Reviewer ftna"
            },
            "comment": {
                "value": "Dear Reviewer ftna,\n\nThank you for your time and effort in reviewing our paper. We would like to address your concerns and questions as follows:\n\n**W1, W2, and Q1**:\n\nThank you for these suggestions. Please note that our primary goal is to conduct a theoretical analysis to highlight issues in existing studies on the expressive power of GNNs and to propose a new analytical framework that avoids these issues. Our intention is not to design a specific GNN model with improved performance or expressiveness, nor to provide guidance for future work aimed at doing so.\n\n**Q2**:\n\nYes, we believe the answer is affirmative. As discussed in Section 3.1 (Lines 201-209 in the revised PDF) of our paper, many GNNs conform to the \"preprocessing-then-message-passing\" framework. From a practical standpoint, mainstream GNN libraries, such as PyG (torch-geometric), implement GNNs with a \"MessagePassing\" base class, meaning that models built with these libraries naturally align with this framework. High-order GNNs, subgraph GNNs, and GNNs with additional features can also be implemented in these libraries by first constructing the $k$-WL graphs, subgraphs, or graphs with additional features, followed by message-passing operations, thereby fitting into the \"preprocessing-then-message-passing\" framework. As a result, we believe our analytical framework applies to the analysis of most GNNs.\n\n**Q3**:\n\nYes, exactly. The resource limitation we consider reflects the constraints of real-world GNNs. Many GNNs use MLPs (or similar neural network models) as update functions, but these are far from Turing-complete, as required in condition (3) of Theorem 3 (Lines 308-310). For instance, we cannot expect an MLP of polynomial size to solve $\\mathsf{NP}$-$\\mathsf{complete}$ problems without error (unless $\\mathsf{P} = \\mathsf{NP}$). However, directly setting the resource limitation class $\\mathsf{C}$ as a class specifically reflecting MLPs (e.g., $\\mathsf{TC}^0$) would reduce flexibility, as future GNNs may adopt new architectures for update functions. Our framework remains adaptable by allowing adjustments to the class $\\mathsf{C}$. For a hypothetical example, if we implemented the nodes' update functions with transformer-based LLM agents enhanced by Chain-of-Thought (CoT), which are claimed to solve problems within $\\mathsf{P}$ **[Merrill et al., 2024; Li et al., 2024]**, we could set $\\mathsf{C} = \\mathsf{P}$ and derive new theoretical results based on this adjustment. We hope that our analysis framework can also inspire future work on graph agents, and have added it in red color in the revised PDF (Lines 384-387). This point is already discussed in Lines 381-389 (of the revised PDF), adjusting $\\mathsf{C}$ in different ways may lead to varied outcomes. In this way, our RL-CONGEST framework serves as a \"framework scheme\" or \"framework template\".\n\n**Q4**:\n\nAs noted in our response to W1, W2, and Q1, our focus is on identifying issues in the existing analysis of GNN expressiveness and introducing a new framework for this analysis, rather than providing a guideline for future work to enhance expressiveness. The idea is that once researchers propose a new GNN model, they can analyze its expressive power using our framework, rather than using ad-hoc methods, which may have limitations as discussed in Section 3.\n\n**Q5**:\n\nYes, we believe this is correct. We leave the exploration of specific tasks to future studies, and we also outline other open questions for further research in Section 5. \n\nThank you again for reviewing our paper. We hope this response clarifies our approach and addresses your questions. We look forward to any further discussions with you.\n\n**References**:\n\n**[Merrill et al., 2024]** William Merrill, and Ashish Sabharwal. The Expressive Power of Transformers with Chain of Thought. ICLR 2024.\n\n**[Li et al., 2024]** Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of Thought Empowers Transformers to Solve Inherently Serial Problems. ICLR 2024."
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new computational model\u2014the Resource Constrained CONGEST (RL-CONGEST) model\u2014designed to address the inconsistencies and irrationalities in the current analysis of GNNs' expressivity. The RL-CONGEST model forms a framework for analyzing the expressivity of GNNs by introducing resource constraints and optional pre-processing and post-processing stages. Through this framework, it can reveal computational issues, such as the difficulty of hash function computation in the WL test and the role of virtual nodes in reducing network capacity, thereby providing theoretical support for understanding and improving the expressivity of GNNs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper clearly identifies three key issues that are commonly overlooked in the current analysis of GNNs' expressivity, which represents a relatively novel perspective.\n\n2. The RL-CONGEST model proposed in this paper provides a theoretical framework for the expressivity of GNNs.\n\n3. The paper conducts an in-depth analysis of the computational complexity of the WL test, which is valuable for understanding the potential and limitations of GNNs and also demonstrates the paper's solid theoretical foundation."
            },
            "weaknesses": {
                "value": "1. Lack of Empirical Validation: The paper lacks empirical experiments to support the theoretical results.\n\n2. Lack of Guidance on Model Design: The paper does not clearly propose how to use the RL-CONGEST model to enhance the expressive power of GNNs. Although a theoretical framework is presented, there are no specific implementation details or design principles provided."
            },
            "questions": {
                "value": "1.Can you provide some empirical experiments to verify the correctness of the analysis results of the RL-CONGEST model?\n\n2.Is the RL-CONGEST model applicable to the analysis of all different types of GNNs and tasks on graphs?\n\n3.Do the computational resource limitations mentioned in the article reflect the constraints in the real world? Are these limitations applicable to all types of GNNs?\n\n4.Can you further provide design guidance on how to use this method to improve the model's expressive power?\n\n5.Since the article mentions analyzing the expressive power of GNNs under resource constraints, is the RL-CONGEST model applicable to learning tasks on large graphs that are also resource-constrained?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper examines the limitations of the theoretical expressiveness of GNNs and introduces a novel computational framework, RL-CONGEST, which factors out pre- and postprocessing and limits the computational power of nodes. The authors further analyze the WL-test within this framework and contribute some theoretical insights. RL-CONGEST, while positioned primarily for GNNs, also offers implications for understanding computational constraints in other computation models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The paper introduces RL-CONGEST, a new computational model that addresses aspects previously overlooked in the GNN literature, particularly computational constraints at the node level.\n* Some shortcomings in prior work are highlighted and critically analyzed, including preprocessing complexities and computational limits.\n* RL-CONGEST has potential standalone value beyond GNNs, as it provides a framework to study computational complexity and expressiveness that could benefit other areas."
            },
            "weaknesses": {
                "value": "* Section 3.1: The authors argue that preprocessing time complexity is often underestimated in the GNN literature, with Wollschl\u00e4ger et al. (2024) as an example. However, this appears to be an isolated case rather than a trend in the field. A more robust case for this claim could be made by referencing additional studies or a systematic analysis that demonstrates the prevalence of overlooked preprocessing complexities. Zhang et al. (2023), which the authors cite and analyze, actually discusses preprocessing time explicitly in the paper, which weakens the generality of this argument. While it is valuable to account for preprocessing, demonstrating that this issue extends across multiple papers would strengthen the point. Further, as most of these papers mainly focus on expressiveness, computational complexity might just not be the main focus.\n\n* Section 3.2: The \u201cmismatch\u201d claim between models with and without features lacks clear evidence. The advantage provided by features in model initialization is well-known, and the WL test is adaptable to both anonymous and pre-colored contexts. More detail and examples of specific instances where this mismatch has led to issues in the literature would clarify and strengthen the claim. The authors tend to write around what the mismatch actually is in this section and should clearly define it. \n\n* Section 3.3: The assertion that CONGEST is \u201cinappropriate\u201d for direct use is somewhat unconvincing, as it can still serve as an upper bound for computational capacity. While RL-CONGEST\u2019s constraint on node computation is a useful contribution, existing models are still relevant for the purpose of their analysis. Furthermore, Theorem 4 should explicitly assume a connected graph and the version stated in the paper is technically wrong. It is also worth noting that in many GNN studies, expressiveness rather than computational complexity is the focus, so adding computational constraints could shift the narrative and purpose of the study. If the authors are proposing RL-CONGEST as a practical standard for GNNs, specific examples and a discussion on which complexity classes should be used for GNNs would help contextualize it within the field. \n\n* Adding computational constraints to CONGEST is an interesting approach, but it becomes very detached from the application in GNNs. For example, the authors do not go into detail on what complexity classes we should allow for GNNs. One could make an argument that as GNNs are usually implemented with fixed size networks that run in constant time, the computational envelope should also be constant to yield the most realistic bounds. RL-CONGEST is interesting on its own, but how the computational constraints should be best put to use should be discussed in paper that claims to investigate the GNNs. The paper would benefit from more guidance on how GNN practitioners should employ RL-CONGEST, along with concrete examples of benefits. A more precise articulation of the expected impact or practical value this framework could offer would also strengthen the contribution.\n\nOverall, the paper makes several claims and only backs up some of them. In the end, it is not clear how the newly proposed model is supposed to be used in future work (should everybody just use their own complexity classes for the local computation, what benefit does this have?) and leaves the question on what impact this work can have. The authors should address this issue and formulate some clear benefits of their framework."
            },
            "questions": {
                "value": "* Could the authors clarify specific insights from the RL-CONGEST model that would be practically useful for GNN practitioners?\n* Do the authors envision RL-CONGEST serving as a new standard or benchmark model for GNN complexity analysis? If so, could they suggest specific complexity classes for GNN applications or examples that showcase RL-CONGEST\u2019s advantages?\n* Could you clarify your position on CONGEST's usefulness as an upper bound and discuss whether RL-CONGEST complements rather than replaces existing models?\n* Could you add a discussion on appropriate complexity classes for GNN analysis using RL-CONGEST? In that context, can you provide guidelines or a framework for GNN practitioners on how to effectively use RL-CONGEST in their research or applications?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors very correctly point out that the current theoretical analysis of GNNs is lacking in a few key ways (e.g. granularity and taking into account computational expense). To remedy that they propose using Resource-Limited CONGEST model, instead of usual CONGEST and relating WL-tests to model-checking problems that can prove a more granular expresivity testing."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "I agree with the authors that the theoretical expresivity analysis of GNNs is quite lacking. It makes a lot of sense to limit the computational power of the nodes (GNN update functions). As that is more realistic. The idea to use model-checking problems instead of WL to judge the theoretical power of GNNs is novel and I think quite promissing, as it allows for higher granularity. \n\nThis work also provides interesting motivation for why virtual nodes help, as they are a very common tool in practice. One of the first works to look at this theoretically to the best of my knwoledge.\n\nIt's generally well written and easy to follow."
            },
            "weaknesses": {
                "value": "Authors stress that \"unlimited computational resources of CONGEST\" is an issue and chose to just use a more restrictive computation class for the node updates. Ideally I'd like to see this being contrasted with the universal approximation theorem for MLPs. As the update function is usually an MLP it's power I'd say is more defined by approximation quality of whatever computation it needs to perform.\n\nIn the section \"Additional Features Empower Models by Breaking Anonymity?\" authors say that it's not good that some expressive GNNs might be breaking anonymous setting by using additional features. I would say that this is not a good way to look at this. In my opinion that the point of a good chunk of more expressive GNN research is precisely how to add pseudo-indentifiers to a graph with as few negative impacts (bad generalization). \n\nSpeaking about negative impacts of node identifiers, in the proposed computation model authors permit \"nodes to be aware of their own unique IDs\". This doesn't make much sense from ML perspective as generalization will be terrible if a stable ID assignment is not possible, and normally it is not possible to do so on general graphs. So for a paper arguing about making theoretical GNN analysis more realistic I think this is a noteable issue.\nAuthors do motivate this choice by saying that \"real-world graph datasets are rich in node features\". I'd argue that this is still very far away from node IDs, e.g. say if features are just a few different atom types in case of many molecular tasks. I'd like to see some data analysis showing the unique identifiability of nodes in multitude of real world datasets to convince me that this is the case.\n\nThe work also lacks direct applicability to fixing or ranking GNN architectures. Which would be the main benefit of the newly proposed GNN analysis. To make the paper complete I would like to see analysis/ranking of some few popular GNN architectures and hopefully showing that this translates to some real tasks, for example ones for which the assumptions, such as unique identifiability by node features, more or less hold.\n\nAlso, speaking about popular GNN architectures, authors skipped the two first subgraph GNN papers, when discussing subgraph GNNs (https://arxiv.org/abs/2110.00577 https://arxiv.org/abs/2111.06283)"
            },
            "questions": {
                "value": "Distributed computing has various computation models already, besides LOCAL and CONGEST. It would be nice if authors would dig a bit deeper in the distributed computing literature to see what alternatives already exist and if they would be more fitting than CONGEST. It's been a while since I looked at those myself, but for example https://arxiv.org/pdf/1202.1186 investigates a very restricted computational model, that should still be able to simulate a WL test (it was also used in some simpified GNNs https://arxiv.org/pdf/2205.13234). I'm sure that others exist as well."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors first explain the limitations and unrealistic assumptions of several current approaches in analyzing the expressive power of GNNs, including underestimated preprocessing time, anonymous WL tests with non-anonymous features, and unrealistic assumptions in the CONGEST model. Next, the authors propose the RL-CONGEST model to address these issues. Several results are derived: (1) GNNs require substantial width and depth to simulate the WL test; (2) virtual nodes can help reduce computation costs, although they do not improve theoretical expressive power; (3) the RL-CONGEST model can solve the PNF model-checking problem with \n$k$-WL graph transformation in $O(k^2)$ rounds."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-structured and nicely presented.\n2. The stated limitations of existing approaches make sense to me, and the examples are intuitive.\n3. The new results derived by the RL-CONGEST model are interesting."
            },
            "weaknesses": {
                "value": "My main concern is about the practical implication of the proposed model beyond what the author presented. \n1. One question is how we can use the RL-CONGEST model to effectively estimate and compare the representational power of different GNN variants or even predict their performance in real-world applications.\n2. The authors claim that the proposed framework can be used for analyses involving non-anonymous node features. I wonder how this framework can be leveraged to truly evaluate differences between various added features, such as SPD or resistance distance. In my view, although the broken symmetry introduced by these additional features is undoubtedly a source of improved expressivity, different features have varying degrees of power; some can help count more complex graph structures than others."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}