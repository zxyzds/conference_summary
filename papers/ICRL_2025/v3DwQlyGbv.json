{
    "id": "v3DwQlyGbv",
    "title": "Paramanu-Ganita: An Efficient Pre-trained Generative Mathematics Language Model with Chain-of-Thought Instruction Fine-Tuning",
    "abstract": "In this paper, we present PARAMANU -G ANITA, a 208 million-parameter novel\nAuto Regressive (AR) decoder based language model on mathematics. We per-\nformed pretraining from scratch on 31.5 billion tokens using a context size of 4096\non a mixed mathematical corpus consisting of mathematical web pages, mathe-\nmatics related source code such as AlgebraStack, mathematical textbooks, Chain-\nof-Thought (CoT) templatised mathematical StackOverflow question answers\npairs, and mathematical lecture notes in LATEX curated by us. We also trained a\nmath and code specialised BPE tokenizer. We proposed and performed Chain-of-\nThought instruction fine-tuning of Paramanu-Ganita on the MetaMathQA dataset.\nWe evaluate our model on GSM8K and MATH mathematical benchmarks, and on\nlogical deductive reasoning (LogiQA) and multiple choice high school and col-\nlege level math questions from SAT (AGIEVAL-SAT-Math), GRE/GMAT ques-\ntions (AGIEVAL-AQuA-RAT), college and high school level math questions from\nMMLU. Our model Paramanu-Ganita, despite being 34 times smaller than the\n7B LLMs, outperforms general LLMs by approximately 30% points, and even\nmath-specialised LLMs by 3-23% points in GSM8K test accuracy metric. On\nMATH benchmark, Paramanu-Ganita outperformed the various models by 6-8%\npoints. On other benchmarks such as LogiQA logical deductive reasoning bench-\nmark, mathematical high school level multi-choice questions (MMLU-math-high-\nschool), GRE-GMAT level quantitative questions (AGIEVAL-AQuA-RAT), SAT\nlevel math questions, Paramanu-Ganita was better than the others by about 1-4%\npoints. The large significant margin improvement in performance of our math\nmodel over the existing LLMs signifies that reasoning capabilities of language\nmodels are just not restricted to those with humongous number of parameters.\nParamanu-Ganita took only 170 hours of A100 training whereas large LLMs such\nas the math-specialised LLM, LLEMMA 7B, was trained for 23,000 A100 equiv-\nalent hours. Thus, our approach of pretraining powerful domain-specialised lan-\nguage models from scratch for domain adaptation is much more cost-effective and\nenvironmental friendly than performing continual training of LLMs.",
    "keywords": [
        "reasoning",
        "language models",
        "pretraining",
        "CoT fine-tuning",
        "AI4Math"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "An efficient language model for mathematics, which is pretrained from scratch on a custom corpus on 31.5 billion tokens; despite having only 208 million parameters, it outperforms several large and very large LLMs on standard benchmarks",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=v3DwQlyGbv",
    "pdf_link": "https://openreview.net/pdf?id=v3DwQlyGbv",
    "comments": [
        {
            "summary": {
                "value": "Authors present a LLM specializing in mathematics, called Paramanu-Ganita. It is quite smaller in size and exhibits interesting performance benefits in several math and logical datasets, compared some LLMs with bigger size. Authors also trained tokenizers from scratch, curate a new dataset for pretraining and show that Paramanu-Ganita outperforms several general-purpose LLMs and some domain-specialist LLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-written. I appreciate the background and the description of how the model is trained. The idea of targeting mathematics is important and building LLMs specializing in math (at least some part of it) is important. \n\nThe dataset is an important contribution, however I am not sure whether the authors plan to make it public."
            },
            "weaknesses": {
                "value": "I feel the paper explores an interesting direction, but there are some concerns:\n\n1. Firstly, GSM8k tests basic math word problem skills and given the model's GSM8k performance is pretty poor, I do not feel the model is ready yet. I think more experimentation is required. Also, how are Table 2 values computed? It seems the MetaMath paper reports GSM8K performance to be 82.3. Why is it 66.5 here? [1]\n\n2. What is mostly missing from the paper are proper motivations and justification as to what \"contributes\" or what is expected to contribute to the \"improved\" performance? \n\n - Looking at this from a different point of view, why did the authors not start with MetaMath, then say change the tokenizers or change the dataset? Then, slowly demonstrate how all the innovations are truly necessary. At the least such ablations would have showed the necessity of new models. \n\n - Secondly, given the model's performance is not so great, what are we gaining by spending so much training time and cost?\n\n3. One more important aspect is, what are the domains that the model targets? What are the grade levels? Is it the expectation that we will also do IMO problems starting from GSM8k? Or, are we targeting sub-disciplines algebra, pre-algebra, calculus etc.? I think this depth is also missing, so is related papers that investigate the need of such models [2].\n\n[1] https://openreview.net/forum?id=N8N0hgNDRt\n[2] MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning"
            },
            "questions": {
                "value": "Some minor and major questions:\n1. Abstract: Concrete examples would be better, such as which model did it beat despite being smaller etc..\n2.  L196: Please give examples, what happens for various ways of writing floats. How are the European and US/UK numbers treated 1,43 vs 1.43.  Mixed numbers and digits, other mathematical symbols.  To me, its not so clear from the writing.\n3. L210: The architecture description seems incomplete, given its a section. You have mentioned decoders elsewhere, but you should complete this here, mentioning how many layers of decoders (or ranges), how many dense layers, and some block diagrams, referred from the section. This is supposed to be the most important section.\n4. L249: Whats the perplexity of other models, especially mathematics specialist or science specialist ones? Can you show a table comparing them? Otherwise, the standalone numbers do not make sense to me."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces PARAMANU-GANITA, a 208 million parameter mathematics-focused language model trained from scratch. The authors demonstrate that effective mathematical reasoning capabilities can be achieved with smaller, more efficient models when trained specifically for the domain. This approach offers significant advantages in terms of computational costs and environmental impact while maintaining good performance."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Good empirical results despite smaller model size, demonstrating the effectiveness of their approach\n- Demonstrates that smaller, more efficient models can achieve good mathematical reasoning performance"
            },
            "weaknesses": {
                "value": "- The overall presentation of the paper still needs much improvement. The paper is not in ready-to-review or ready-to-submit status. The figures are pretty rough and unclear for what the authors want to express. For example, Figure 2 shows GPU Power Usage during pretraining of Paramanu-Ganita. But what conclusions do the authors want to make here? How does it illustrate the environment friendly nature of the model? For the figure 1, what does the blue line mean here?\n- Limited Ablation Studies. The paper doesn't analyze the relative importance of different components of their training data (web text vs. code vs. lecture notes). It is unclear why the authors want to utilize these data sources and why the data mixture should be adopted as it is in the paper.\n- Contamination issues. The model achieves good performance on GSM8K and MATH with 200M parameters. It is unclear whether there is data contamination issue."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The author presents a small decoder-based language model on mathematics called Paramanu-ganita. They trained this model from scratch using the existing public mathematical corpus and also performed CoT instruction finetuning on top of it. They also train their own tokenizer specialised in math and code. Despite their model only having 208 million parameters, it outperforms general LLMs by approximately 30% points, and even math-specialised LLMs by 3-23% points in  GSM8K, 6-8% on MATH. The 208 million parameter model outperformed  LLaMa-1 (33B, 13B, 7B), LLaMa-2 (7B, 13B), Falcon (40B, 7B), PaLM (62B, 8B), MPT (30B, 7B), Vicuna 13B, and math-specialised LLMs like Minerva 8B, LLEMMA-7B on GSM8K, MATH, AGIEVAL-AQuA-RAT benchmarks. They also showed the reduced time and computation requirement to train this model as compared to existing LLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. A novel decoder model, that is 34 times smaller than existing LLMs and can outperform them by a huge margin\n2. A detailed explanation of the training process required\n3. Detailed benchmarking on GSM8K, MATH and other datasets.\n4. Emphasis on the training time required and compared it to other existing LLMs, showing computation and environmental prowess in training an exclusive tiny model from scratch."
            },
            "weaknesses": {
                "value": "1. The paper uses Qwen-72B to label the corpus and use a score >= 0.6 for training the model, ensuring only a high-quality dataset is used. However, apart from this, the training process used is not novel. Specifically, there is no novelty in the model architecture or training paradigm used that can justify the complete novelty of the paper and also puts into question the improved performance of a 208 million parameter model over LLMs\n2. The paper does not touch upon, newer and difficult mathematical datasets such as MATHBENCH or JEEBENCH. These are some datasets that were released after training cutoff time for some models, ensuring they are not part of their training data. These datasets are also much more difficult as compared to gms8k. This will ensure that the proposed model is robust in solving difficult problems that it hasn't seen before.\n3. Will the checkpoint-filtered corpus used for training be publicly available?\n4. How does the model perform on out-of-distribution data points, this can be checked by first doing a sanity check of data memorization/contamination [1]. Performing simple algorithms 1 and 2 from the paper will ensure that the model has not seen the evaluation dataset, making the results more robust.\n5. The empirical analysis is missing from the paper. A thorough qualitative comparison of reasoning chains produced by Paramanu-Ganita versus other models on a few representative problems from the benchmark datasets. For example, what errors are made by existing LLMs vs. Paramanu-ganita and in which area does it improve?\n\nReference\n[1] Golchin, Shahriar, and Mihai Surdeanu. \"Time travel in llms: Tracing data contamination in large language models.\" arXiv preprint arXiv:2308.08493 (2023)."
            },
            "questions": {
                "value": "Address the weaknesses of the paper mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}