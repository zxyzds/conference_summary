{
    "id": "tZk3LnvVtK",
    "title": "Measuring Language Model Uncertainty With Internal Concepts",
    "abstract": "We study the problem of evaluating the predictive uncertainty of large language models (LLMs). \nWe assign an uncertainty measure to the correctness of an LLM using a form of entropy that applies to semantic objects (concepts).\nUnlike prior works, the notion of meaning used to define concepts is derived from the LLM, rather than from\nan external model. \nOur notion of conceptual equivalence draws from ideas in Formal Concept Analysis (FCA) and lattice/order theory, and can be used to estimate correctness in closed- and open-ended scenarios.\nOur method has a relative improvement of up to 4.8% on average across five benchmarks and up to 10.9% on mixtures of closed- and open-ended questions.",
    "keywords": [
        "uncertainty",
        "correctness",
        "large language models",
        "concepts"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=tZk3LnvVtK",
    "pdf_link": "https://openreview.net/pdf?id=tZk3LnvVtK",
    "comments": [
        {
            "summary": {
                "value": "The paper is well written and structured. \nIt deals with a way to evaluate the uncertainty of a Large Language Model\u2019s (LLM)  in terms of output meaning as corresponding the questions submitted to it. \nThe authors approach relies on \u201cinternal concepts \u201d i.e. semantic objects from the LLM itself and not derived from an external semantic model of the language thus avoiding introducing differing/contradicting semantics between the external model and the LLM.\nTheir approach is entropy-based."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Quantified uncertainty is meant to evaluate the LLM\u2019s responses and thus can help detect highly probable (semantically) odd responses (LLM hallucinations).\nThe approach tackles both open and close-ended queries and through what is termed \u201cconceptual uncertainty\u201d. It applies a query-dependent \u201ctruth-assignment likelihood\u201d function to estimate LLM answers \u201ctruthfulness\u201d.\nBased on the various LLM responses\u2019 \u201ctruthfulness\u201d values, an entropy-based  uncertainty measure is computed. The entropy measure introduced is called \u201cConceptual Entropy\u201d, only relies on non-all-false LLM responses truth  assignments, and accounts for mutually exclusive concepts and output length differences through length-normalized likelihoods.\nFinally uncertainty is computed as a function of answers semantic groups (of model outputs) and their likelihoods."
            },
            "weaknesses": {
                "value": "See questions' section"
            },
            "questions": {
                "value": "S: Statement. Q: Question.\n\nS1: In \u201c2 A GENERAL FRAMEWORK FOR LLM UNCERTAINTY\u201d, you state \u201c We use the term uncertainty as opposed to confidence as we view uncertainty as independent of the selected output to the query\u201d.  \nQ1.1: Can you further clarify ?\n\nS2: In \u201cCOMPONENTS OF LLM UNCERTAINTY\u201d, you state \u201cWe further decompose existing diversity measures into three key components\u201d. \nQ2.1: Are you referring to \u201c2) measuring some form of answer diversity\u201d form of uncertainty mentioned earlier in the paragraph? \nQ2.2: Isn\u2019t this in contradiction with  your statement in Q1? Or\nQ2.3 Do you consider that diversity of answers is independent from selected output?\n\nS3: In \u201c3.1 INTERNAL CONCEPTS\u201d, You state \u201cGiven a set of candidate answers A = {a1, . . . , an} \u2282 A\u2217 for a query q, we view the corresponding \u201cconcept structure\u201d as determined by a partial ordering where ai \u2264 aj indicates that if ai is a valid output for q, then aj is also a valid output.\u201d \n\tQ3.1: This is not clear to me. Doesn\u2019t this make all model candidate answers, whatever their place in the order, valid as a hypothesis? No order depth limit as deeper positions might mean very uncertain?\n\nS4: In \u201c4.1 SETUP-Metric\u201d, you state \u201crandomly selected correct question having a larger uncertainty score than a randomly selected incorrect question.\u201d.\n\tQ4.1 can you clarify \u201ccorrect/incorrect question?\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "To address limitations in existing uncertainty evaluation metrics\u2014such as dependence on external models lacking equivalent knowledge to the evaluated LLM, inability to handle open-ended questions, and failure to leverage hierarchical semantic structures for more effective grouping\u2014the authors propose a framework called Conceptual Uncertainty for assessing LLM uncertainty. Specifically, they introduce an internal concept structure to define the distribution over possible truth assignments to answers, which is then used to calculate the newly-introduced conceptual entropy as a measure of uncertainty. Extensive experiments demonstrate that this proposed metric outperforms baseline methods in handling both open-ended questions and those questions with long answers."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- A novel approach that effectively leverages the inherent hierarchical semantic structures and the internal knowledge of the evaluated LLM to improve the precision of uncertainty evaluation.\n- A clear presentation of the motivation and key challenges addressed by the framework.\n- Intuitive figures and examples that aid readers in understanding the newly introduced concepts.\n- Experiments conducted on both challenging cases, covering both closed- and open-ended questions, as well as questions with short and long answers."
            },
            "weaknesses": {
                "value": "- The clarity of the paper could be further enhanced, as noted in the questions below.\n- There appears to be an inconsistency between the algorithm and the main text; specifically, lines 5 and 6 in Algorithm 1 seem more aligned with standard uncertainty evaluation practices rather than fully reflecting the conceptual entropy measure introduced in the paper. If the algorithm is correct, it would be helpful to clarify the connections between the functions in the algorithm and the functions in the main text."
            },
            "questions": {
                "value": "- In line 67, Why can the connection between a and b be quantified by the likelihood ratio of strings of the form qbqa and qa? A more intuitive explanation would be helpful.\n- Why does Equation (1) hold? Could you provide a more intuitive rationale for it?\n- Some notations in Equation (2) are not formally defined, which may lead to reader confusion.\n- How the expected (internal) concepts are determined?\n- In line 319, you mention that 20% of the samples are used for parameter tuning. Which specific hyperparameters are tuned?\n- Have you evaluated the performance of the new metric on larger-scale LLMs, such as GPT-4 or others?\n- How many sample answers are generated to evaluate conceptual uncertainty for each question? How does the sample size impact the performance of the conceptual uncertainty metric?\n- In line 360, why is the question \u201cWhat is my favorite integer between X and Y, inclusive?\u201d considered a closed question?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The submission proposes a new approach to quantifying uncertainty of LLMs. The method, named Conceptual Uncertainty, performs a hierarchical grouping (via a lattice) of candidate answers on the basis of which is then derives a likelihood function, form which an entropy measure is derived. The resulting measure then provides a measure of uncertainty for the scenario. \n\nThe key novelty is the organisation of candidate answers in a lattice structure that captures more detailed relationships between answers than the partitioning into semantically equivalent answers generally considered by previous work. In particular, the lattice is induced by the partial order over answers that represents an answer being correct implying another answer being correct.\n\nThe method is then compared empirically to a range of methods from the literature on Mistral 7B, Falcon 7B, and Mamba 2.8B, with the proposed method of Conceptual Uncertainty performing best in most experiments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1) The combination of a likelihood function derived directly from the approximation of the partial ordering, as well as the variation on traditional entropy combine elegantly to an intuitively sensible measure of uncertainty. \n\nS2) The concept lattice is, to the best of my knowledge, an original contribution to the study of LLM uncertainty and conceptually seems to be a step forward from considering only equivalence classes.\n\nS3) In the empirical evaluation the method compares favourably against a wide range of methods from the literature. Notably experiments were performed with transformer-based and with state-space LLMs which is particularly relevant as the proposed approach is highly LLM dependent."
            },
            "weaknesses": {
                "value": "W1) To me, the presentation is confusing at times. While effort was made to provide ample examples, they seem under explained and often unclear. For example, as far as I understand the assignment likelihoods in Figure 1 seems unconnected to the lattice in the figure and overall the figure offers little insight at this point in the text without further explanation.\n\nW2) The authors explain that a key distinction of their approach to mot approaches in the literature is that they derive their concept structure from the LLM. This is done by approximately recovering the partial order over concepts via a specific ratio of LLM output probabilities. This then naturally carries with it its own uncertainty based on the LLM which is ignored in this analysis. See also Q2. \nFurthermore, I am not sure if it is not straightforward to adapt other methods to derive their respective concept structures (say semantic equivalence relations) via the LLM. This key distinction therefore seems somewhat weak to me."
            },
            "questions": {
                "value": "Q1) In order to estimate the uncertainty of answers for question q, the proposed method uses the ratio of LLM probabilities on question q itself to construct the lattice from which the uncertainty measure is then derived. This seems somewhat circular to me, and intuitively this approach would perform worse in situations with high uncertainty, as the approximated concept lattice would be less reliable. Could you clarify why this cyclic nature is not of greater concern?\n\n\nQ2) The method relies critically on the approximation of the partial ordering of concepts by I_q. Have you performed any systematic comparisons of the approximate partial orderings to the real partial orderings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper evaluates the predictive uncertainty of LLMs for tasks such as QA. It assigns an uncertainty using a form of entropy that applies to semantic concepts. A key feature is the acknowledgement of hierarchy of concepts.  A high-level framework is proposed, followed by some specific approaches in the framework that group generated answers, apply some weights, and estimate uncertainty scores for answers using a weighted sort of entropy. Some experiments are performed, revealing good performance on AUROC for some datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The authors propose a method that unifies some of the related work on semantic similarity and uncertainty quantification for LLMs. They propose some new ways to obtain clusters of answers and use those to compute uncertainty.\n\nAnother strength is that the results on some datasets seem promising. The choice of datasets is somewhat diverse."
            },
            "weaknesses": {
                "value": "In my view, the paper suffers from clarity related issues and seems too similar in many ways to previous work in the space. The paper goes out of its way to make the case that 1) the semantic understanding part of the work is internal rather than external, and 2) the work has theoretical basis as opposed to some prior work.\n\nRe: 1): I could not fully understand the distinction between prior work on external models vs. the claim about using the internal model. Wouldn\u2019t it be possible to use the same LLM for NLI, similar to Kuhn et al.? Why is this distinction important? I did not understand this from the paper, even though it came up several times.\n\nRe: 2): the authors claim some theoretical basis but most of the discussion does not seem directly pertinent to the approach. The work feels like an extension of prior efforts. In my view, the claims of the paper are inconsistent with the presentation. The claims of theoretical justification either need to be softened or better explained.\n\nAs should be clear from many of my comments, I could not fully understand important details in the paper. Further details about my concerns and some associated questions are provided in the next section. Even though I\u2019m familiar with much of the related work, I could not understand the new contributions. Perhaps the authors can clarify some aspects."
            },
            "questions": {
                "value": "Some questions and comments follow:\n\nThe abstract is too short. There are also some statements about % improvements that are unnecessary \u2013 I feel this is not suitable for situations where the datasets and baselines are not well known and standard.\n\nIn the second paragraph, the point about \u201cexternal model\u201d is too vague. This distinction b/w external and internal model was generally unclear to me and needs specification. I recommend adding some detail here. In general, there is a lot of repetition in the paper with details delayed to later, and when they do come later in Section 3, they are not clear enough for me.\n \nThe authors mention they focus on uncertainty (which is a function of a query) as opposed to confidence (which is a function of a query and generation). As I understand, the system computes a score U(q) for query q, for a model. How should one interpret this score? Does it represent the probability that at least one generation is correct for a query? Doesn\u2019t this depend on the number of generations?\n\nThe title of Section 2.1 is too broad \u2013 there is a lot of literature that does not follow this paradigm.\n\nLines 127-129: The authors mention an exception but there are many types of exceptions given the vast literature on the subject of uncertainty quantification for LLMs. I recommend rewriting these lines.\n\nWhat is the rationale behind the equation in line 215? Is it in the appendix and not in the main text?\n\nWhy is the NLI approach of Kuhn et al. for semantic relations between two answers not sufficient? Is that not a way to obtain I_q(a_i, a_j)?\n\nThere is an issue with the citation style in several places in the paper, like in line 242.\n\nWhy is there a min in equation 1?\n\nWhat are the other strategies referenced in line 263?\n\nIt seems like conceptual entropy is mentioned in some older work. What was in termed in this work? Why was the old term not used?\n\nRe: experiments: parameter tuning is done on 20% of what dataset? The full training dataset? And why is the test set so small, only 400 instances? Did the experiments repeat sub-samples for test set? Is the error mentioned anywhere?\n\nCan the authors clearly explain how the AUROC was computed? This is not clear enough from line 327 and nearby places. When was a question deemed to be correct? When at least 1 answer is correct from the multiple answers generated?\n\nHow are correct questions determined when the datasets are combined? I don\u2019t see this mentioned anywhere.\n\nDid the authors run ablations on the Rouge-L threshold for determining correctness of queries for some datasets?\n\nThere are many more references in this space \u2013 please search for a review paper and add some of these if possible."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to measure the predictive uncertainty of LLMs with concepts. This method defines semantics through the internal conceptual structure of the model rather than an external model, thereby assessing the correctness of open and closed questions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed method does not rely on external semantic models but instead measures uncertainty based on the internal conceptual structure of LLM itself. It avoids the bias of incorporating external model knowledge into LLM evaluations, making the uncertainty measurement more aligned.\n\nAdditionally, the method can adapt to both open-ended questions (\"List a city in California\") and closed questions (\"Which state is number 31?\u201d), which is very useful in many practical application scenarios. \n\nThrough experiments, the method shows an improvement, which demonstrates the effectiveness of conceptual uncertainty in hallucination detection."
            },
            "weaknesses": {
                "value": "The proposed method requires generating 10 to 20 candidate answers for each question to construct a conceptual structure, which has a high demand for computational or human annotation resources. This sampling amount will further increase, especially when dealing with open-ended questions.\n\nAlthough the proposed method reduces reliance on external models, it still depends on labeled datasets (for example, accurate matching of correct answers to questions). This places high demands on the accuracy and coverage of the dataset being constructed. In some fields, obtaining sufficient labeled data may be quite difficult. I believe that this may limit the generalizability of this method.\n\nThe author used fixed rules and structures to construct concepts and categorized answer groups by defining implicational relationships. However, this may seem limited when dealing with more complex semantic relationships. I suggest the authors consider representation engineering [1] to enhance their concept discovery.\n\n[1] https://arxiv.org/abs/2310.01405"
            },
            "questions": {
                "value": "**Q1**: From the perspective of practical usage, how to balance the trade-off between computational cost and performance improvement?\n\n**Q2**: Do you have plans to use self-supervised or representation techniques to automatically construct concepts in order to reduce annotation dependence?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}