{
    "id": "TtUh0TOlGX",
    "title": "Regularization by Texts for Latent Diffusion Inverse Solvers",
    "abstract": "The recent development of diffusion models has led to significant progress in solving inverse problems by leveraging these models as powerful generative priors. However, challenges persist due to the ill-posed nature of such problems, often arising from ambiguities in measurements or intrinsic system symmetries. To address this, we introduce a novel latent diffusion inverse solver, regularization by text (TReg), inspired by the human ability to resolve visual ambiguities through perceptual biases. TReg integrates textual descriptions of preconceptions about the solution during reverse diffusion sampling, dynamically reinforcing these descriptions through null-text optimization, which we refer to as adaptive negation. Our comprehensive experimental results demonstrate that TReg effectively mitigates ambiguity in inverse problems, improving both accuracy and efficiency.",
    "keywords": [
        "Inverse problem",
        "Text regularization",
        "Diffusion model"
    ],
    "primary_area": "generative models",
    "TLDR": "We propose text regularization for inverse problem solving.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=TtUh0TOlGX",
    "pdf_link": "https://openreview.net/pdf?id=TtUh0TOlGX",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a novel method for solving inverse problems using diffusion models, with a focus on reducing ambiguity in the solutions.\nThe authors propose a latent diffusion inverse solver that incorporates regularization by texts (TReg), which applies textual descriptions of the expected solution during the reverse sampling phase. \nDuring training, this text guidance is dynamically adjusted through null-text optimization for adaptive negation.\n\nOverall, I think this is a good paper."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* this paper introduces an explicit regularization term during training via text-driven regularization (TReg)\n* the proposed TReg effectively addresses the challenge of ambiguity in inverse problems\n* this paper further introduces an adaptive negation to dynamically adjust the influence of textual guidance\n* the paper is well written and easy to follow, extensive in main text and supplementary demonstrate the effectiveness of the proposed method for diffusion inverse solvers"
            },
            "weaknesses": {
                "value": "1. lack of overall results over whole dataset. all the experiments results are shown in visualizations or subset results of specific classes.\n2. i notice in both table 1 and table 2, PSNR scores are higher without adaptive negation, do authors have any analysis or intuition about this results?\n3. (this might not be a weakness, just naturally curious) in the experiments, the text regularization is only tested with class name, how about the results using some natural captions (such as generated with BLIP or GPT)?"
            },
            "questions": {
                "value": "please refer to weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces TReg (Regularization by Text), a method for solving inverse problems in image processing using latent diffusion models by conditioning on text.  TReg leverages user provided textual descriptions to guide the image reconstruction process.\n\nPrior diffusion-based inverse solvers often struggle with ambiguity, as multiple different images can produce the same degraded output. TReg addresses this by incorporating textual prompts as a form of regularization. The authors frame the problem as a text-conditioned latent optimization, where the goal is to find a solution that is both consistent with the measurements and aligned with the provided text description.\n\nThe authors introduce the following:\n* A text based regularization term that encourages the reconstructed image's latent representation to be close to a text-conditioned denoised estimate. This helps guide the sampling trajectory towards solutions that are semantically aligned with the text.\n* To further refine the textual alignment, this method uses a novel \"null-text optimization\" technique. This dynamically adjusts the influence of the textual guidance during the reverse diffusion sampling process, suppressing unintended signal components.\n\n\nThe authors also discuss limitations, particularly the challenge of finding suitable text prompts for some challenging inverse problems where the problem is extremely ill posed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Overall the paper is well written, provides clear motivation, and the authors perform reasonably comprehensive experiments. The authors do a good job in their logical flow and provide pretty clear explanations of the proposed method and experimental setup. Figures look nice.\n\nIn terms of novelty, they authors introduce a reasonably novel approach to solving inverse problems by incorporating text-based regularization into latent diffusion models.  This addresses a significant limitation of existing solvers, mostly their struggle with ambiguity, and mitigates the need for task specific training. \n\nThe proposed method is technically sound, with a clear mathematical formulation and well-defined optimization procedures.  The integration of LDPS further enhances the method's capabilities."
            },
            "weaknesses": {
                "value": "* `Line 071` In my view the connection to the human brain is quite weak, not sure why the authors put this in the paper...\n* I would like to see additional, none cherry picked outputs. In my experience using diffusion models to recover images can often fail in very strange ways. I am specifically interested in image inpainting results, which nearly always result in boundary artifacts.\n* I would like to see experiments in the presence of measurement noise, for example with JPEG compression artifacts after gaussian blurring."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work uses diffusion models with text guidance in order to solve inverse problems. Inverse problems (e.g., in-painting or deblurring) have been recently tackled using diffusion models, however, their ill posedness nature makes resolving some ambiguities still challenging. The authors propose to leverage a user bias via a text prompt in order to guide the diffusion process towards a solution that is aligned with the description provided. The proposed framework uses a latent diffusion model with classifier-free guidance as a core framework with two innovations compared to previous works: (1) a term that induces alignment between the latent being optimized and that conditioned through the text prompt, and (2) a dynamic null-text adaptation. (1) is the distance between the latent embeddings that are being optimized and those obtained with the text used as guidance (in a classifier free guidance algorithm). (2) The null-text (which usually is the representation provided without any guidance) is updated dynamically in order to be aligned with the CLIP embeddings of the text provided by the user, incentivizing in this way the embeddings to continue to learn new concepts by moving the null-text closer and closer to the text description. \n\nThe authors show results on super resolution, deblurring and Fourier Phase Retrieval problems and compares with some baselines such as DDRM, PGDM, PSLD, and PnP."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper tackles an interesting problems with many potential applications. \n\nWhile the individual ingredients are not completely new, even in the context of inverse problems (the use of prompts and the idea of solving the problem using alternate direction method have been explored by Chung et al in \u201cPROMPT-TUNING LATENT DIFFUSION MODELS FOR INVERSE PROBLEMS\u201d; the optimization of the null-text has been explored by Mokady et all in \u201cNull-text Inversion for Editing Real Images using Guided Diffusion Models\u201d) the overall framework and the way these ingredients are used is novel and seems to lead to compelling results."
            },
            "weaknesses": {
                "value": "The main weaknesses of this work are in the lack of quantitative evaluations on full datasets, lack of some important baselines and the clarity of the writing could be improved. See below for more detailed comments.\n\nEvaluation: all tasks presented are obtained on specific classes rather than being averaged across full datasets as it is done in other the state of the art works (e.g., P2L). Specifically:\n- Ambiguity reduction: it is unclear if the quantitative evaluation in Figure 3 is an average over a dataset or simply the results of the image shown (bear). It is also unclear from which source this image comes from. Maybe it is ok since this is an interesting experiment but not the core? In any case more clarity is needed.\n- Accuracy of obtained solution: use true class as c. Results in  Table 1 and Table 2 show quantitative measurements (great) for two tasks (super resolution and de-blurring) but only on two classes \u201cIce-cream\u201d and \u201cfired rice\u201d rather than averaging across full datasets.\n- Accuracy of obtained solution: use different class as c. Results shown in Table 3 are for only two classes using two different C labels (fried rice to spaghetti, and Ice Cream to Macaroons). It is understandable to show selected classes in Figures but the quantitative evaluation should be reported on a larger datasets (with mean and standard deviations).\n- Results on non-linear inverse problems. These results are only qualitative shown on 4 different images (Figure 6).\n\nThis work shares a lot of similarities with P2L (Chung et al.) so I was expecting P2L to be among the baselines. In the intro when talking about P2L the authors say \u201cHowever, this method primarily focuses on data consistency and lacks robust alignment with textual prompts.\u201d It would be great to support this statement with quantitative results.\n\nFor the alternate directions method there should be some additional information about convergence. Being an iterative method one would expect to see some analysis about it but I did not find any information. \n\nThe work of Mokady et al also shares the idea of adaptive negation (but with different application and implementation) so for Inpainint experiments (which are suitable for Mokady) this would also be a useful baseline.\n\nWriting: The paper is not alway easy to follow. The readability of the manuscript would benefit from some re-writing. For example \n- Background session could be summarized and moved to the appendix since these are all known concepts.\n- The additional space gained should be used to provide more details about the proposed method. Here are some examples of things that could be clarified but there are more throughout the text:\n    - Probably the most important: authors should mention that the proposed update is scheduled to be performed every so many iterations  end not always. This is somewhat hidden in the \u201crange\u201d argument of algorithm 1 but it becomes clear only reading the appendix. I strongly encourage the authors to bring this discussion into the main paper.\n    - Consider moving the inpaiting results (with comparison with Mokady) in the main paper.\n    - In Eq (13) and (14) there is the term A(x). Given that A is an \u201cimaging system\u201d but it is not clear the role it plays in practice when solving a specific problem. I would recommend the authors to provide some information about the role of the forward operator when discussing section 3.1.\n    - \u201cHere, \u03b6,\u03b3 are empirically chosen to satisfy\u00af\u03b1t\u22121 = \u03b6/(\u03b6+ \u03b3).\u201d Please provide an explanation why satisfying this equation is important.\n    - \u2026\n- Since Figure 1 is even before the abstract the reader should be able to understand it without reading anything but the caption and looking at the figure itself. Consider increasing the details in the caption or moving the figure in a place where the reader has sufficient information to understand all of the examples.\n- I find the abstract lacking important details. It should encourage a reader to continue the reading but in order to do so the more details  need to be presented. Here some potential improvements:\n    - ``ambiguities in measurements or intrinsic system symmetries.\u2019\u2019 consider adding a concrete example of these problems.\n    - define what adaptive negation is  or avoid adding a non-defined term.\n    - ``Our comprehensive experimental results\u2019\u2019 consider telling the reader which are the most important quantitive results so that it encourages the reader to continue.\n- In the experimental section consider breaking down the baselines for each experimental task. Currently they are presented all at once at the beginning irrespective of the tasks and I find this to be too much information to process and too far from where it is useful.\n- The authors used the prompt \u201cA photography of a \u2026.\u201d. This might not be grammatically correct. Photography is the art of creating photos or the process of creating photos. A better prompt would  be \u201cA photo of a\u2026\u201d or \u201cA photograph of a \u2026\u201d. I would be curious to know if this changes the results.\n- Tweedie\u2019s formula should be cited.\n- From (10) to (13) the terms are swapped. Please consider maintaining consistent order.\n- While the fact that the work uses a pre-trained VAE is mentioned in the main paper, I think the fact that also the diffusion model is pre-trained is not explicitly mentioned before the appendix. Consider clarifying this aspect in the main paper.\n- The following sentences are unclear, consider rephrasing them: \n    - \u201cWe also prepare a situation that different class label is given as a text description. Here, we should carefully set proper text prompt for measurement to avoid ignoring the provided guidance\u201d. \n    - \u201cFor the case that given text description is differnet from the original classes, we use \u201cspaghetti\" and \u201cmacarons\", respectively.\u201d. There is a typo (differnet) but beyond that it is unclear what this sentence means.\n    - \u201cFor data consistency methods, we utilize the measurement itself \u201c. What is the measurement?\n    - \u201cTReg leads to a unique solution corresponding to the given text description\u201d. Maybe \u201cTReg leads to similar solutions.\u201d? Possibly one could say \u201csemantically unique and aligned with the text\u201d?\n    - \u201c This discrepancy is clearly observed in pixel-level uncertainty in Figure 3(c). \u201c I suppose this is the variance computed across the 10 reconstruction? Consider mentioning explicitly how uncertainty is computed."
            },
            "questions": {
                "value": "Main questions (answering these questions could affect the recommendation)\n\n- Could the authors provide information about convergence of the alternate direction method? \n- Could the authors provide quantitive evaluation averaged across full datasets (as done in state of art works, e.g., P2L) and include relevant baselines, as mentioned above P2L should be included. Mokady would also be a good comparison for inpainting results. \n\nOther questions (this are more curiosity driven questions, it is unlucky the answers here will change the recommendation)\n- In figure 2 authors show that thanks to the adaptive negation strategy the generated image is less noisy. It is unclear however why this would be the case. Could it be that without adaptive negation one could reach a similar results with more iterations?\n- Due to (18) does it mean that the VAE needs to have the same dimensionality of CLIP embeddings?\n- \u201cThe proposed solver without adaptive negation tends to achieve higher PSNR than with adaptive negation, since it obtains blurry images\u201d. Could the Structural similarity index measure (SSIM) be a better metric in this case?\n- For the phase retrieval experiments: the error in the LDPS reconstruction seem to go beyond the ambiguity of symmetries\u2026 Do the authors have any intuition about why the quality of LDPS reconstruction is so low compared to TReg?\n- How sensitive is this frameworks to slight variation of the text? This would be an interesting discussion of the paper. The authors use \u201cA photography of \u2026\u201d how much do results change if instead they used \u201cA photo of xxx\u201d or simply \u201cxxx\u201d?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}