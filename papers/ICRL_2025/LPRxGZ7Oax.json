{
    "id": "LPRxGZ7Oax",
    "title": "Complementary Label Learning with Positive Label Guessing and Negative Label Enhancement",
    "abstract": "Complementary label learning (CLL) is a weakly supervised learning paradigm that constructs a multi-class classifier only with complementary labels, specifying classes that the instance does not belong to. We reformulate CLL as an inverse problem that infers the full label information from the output space information. To be specific, we propose to split the inverse problem into two subtasks: positive label guessing (PLG) and negative label enhancement (NLE), collectively called PLNL. Specifically, we use well-designed criteria for evaluating the confidence of the model output, accordingly divide the training instances into three categories: highly-confident, moderately-confident and under-confident. For highly-confident instances, we perform PLG to assign them pseudo labels for supervised training. For moderately-confident and under-confident instances, we perform NLE by enhancing their negative label set with different levels and train them with the augmented negative labels iteratively. In addition, we unify PLG and NLE into a consistent framework, in which we can view all the pseudo-labeling-based methods from the perspective of negative label recovery. We prove that the error rates of both PLG and NLE are upper bounded, and based on that we can construct a classifier consistent with that learned by clean full labels. Extensive experiments demonstrate the superiority of PLNL over the state-of-the-art CLL methods, e.g., on STL-10, we increase the classification accuracy from 34.96% to 55.25%. The code has been submitted to supplementary material.",
    "keywords": [
        "Complementary label learning",
        "negative label enhancement",
        "weakly supervised learning"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We propose a novel complementary label learning method with excellent classification performance and theoretical guarantee.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=LPRxGZ7Oax",
    "pdf_link": "https://openreview.net/pdf?id=LPRxGZ7Oax",
    "comments": [
        {
            "summary": {
                "value": "In this paper, the authors address the complementary label learning problem by performing positive label guessing (PLG) and negative label enhancement (NLE). The authors prove the generalization error bound and the error rates bounds. Extensive experiments demonstrate the superiority of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThe authors provide theoretical generalization error bound for their proposed method.\n2.\tThe results of the experiments verify the effectiveness of the method.\n3.\tThis paper is well-organized and easy to understand."
            },
            "weaknesses": {
                "value": "1.\tAccording to Eq.(21), the upper bound of the PLG error rate ranges from 0 to 1, which is too broad and meaningless. And it is not easy to estimate the interval of the error bound in Eq.(22). It would be more meaningful if the authors could show that the upper bounds for these error rates are small numbers close to zero.\n2.\tMost datasets used in Tables 1 and 2 are relatively easy, it would be better if data set such as 20 Newsgroups is included for comparison.\n3.\tSome of the experimental results need further explanation, please refer to the questions below."
            },
            "questions": {
                "value": "1.\tIn Figure 2(b), the PLG precision decreases as the training progresses, while the NLE precision remains relatively stable, please explain this.\n2.\tWhy do you report the method POCR in Table 4?\n3.\tPlease give the detailed derivation of Eq.(30), especially the first inequality.\n4.\tThere are some typos, e.g., in Eq.(32), \u201cB\u201d is missing, in line 134, \u201cshow\u201d should be \u201cshown\u201d, in line 169, \u201dp\u201d should be \u201cf\u201d, in line 216, \u201care\u201d should be \u201cis\u201d, and in Figures 2(b) and 2(c), \u201cPrecison\u201d should be \u201cPrecision\u201d."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new Complementary Label Learning (CLL) method called PLNL (Positive Label Guessing and Negative Label Enhancement), which solves the CLL problem by decomposing the inverse problem into two subtasks, positive label guessing (PLG) and negative label enhancement (NLE). Specifically, PLNL classifies training instances into three categories based on the confidence evaluation of the model output: high confidence, medium confidence, and low confidence, and performs PLG and NNE on them respectively. This paper also proposes a unified framework that considers PLG and NLE as the process of negative label recovery, and theoretically proves that the error rates of PLG and NLE have upper bounds, thus enabling the construction of a model consistent with classifiers learned using clean and complete labels."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.This paper provides a novel solution for the CLL field by transforming the CLL problem into an inverse problem that outputs spatial information and decomposing it into two subtasks, PLG and NLE.\n2.This paper also provides a theoretical upper bound proof of error rate, enhancing the credibility and effectiveness of the method."
            },
            "weaknesses": {
                "value": "1. The performance of PLNL may be sensitive to parameter selection, such as k-NN parameter k and confidence threshold \u03bb, which requires users to make detailed parameter adjustments when using it.\n2.Although PLNL has performed well in experiments, its generalization ability on different datasets and tasks of different complexities still needs further validation.\n3.The k-NN computation involved in PLNL may have high computational costs on large-scale datasets, which may limit its application in resource-constrained environments."
            },
            "questions": {
                "value": "Please see the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces PLNL, a new method for complementary label learning that leverages positive label guessing and negative label enhancement based on a confidence-based instances selection module. Through a unified framework and theoretical analysis, it demonstrates the effectiveness of PLNL, achieving state-of-the-art results in CLL."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper is written clear and easy to understand.\n\n2. This paper proposes a new method PLNL for CLL by PLG and NLE.\n\n3. Extensive experiments demonstrate the effectiveness of PLNL over the SOTA CLL methods."
            },
            "weaknesses": {
                "value": "The contribution of this paper is incremental and using a commonly used confidence based instance selection strategy. Moreover,  only one SOTA method (published in 2023 or 2024) is used and it is too weak to validate the effectiveness of the proposed method."
            },
            "questions": {
                "value": "1. In the ablation study, it appears that the weak-strong data augmentation strategy is more effective than the proposed confidence-based instance selection strategy.\n\n2. The settings outlined in Table 1 are lacking experimental results for the CIFAR100 dataset.\n\n3.  The number of SOTA methods employed in the experiments is insufficient: only one SOTA method (published in 2023 or 2024) is used, which is inadequate and too weak to validate the effectiveness of the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper solved the complementary label learning problem by dividing the training instances into three categories: highly-confident, moderately-confident and under-confident. Then, it performed positive label guessing (PLG) and negative label enhancement (NLE) according to the categories of the instances. In addition, it unified PLG and NLE into a consistent framework. It also studied the bounds of the error rates of both PLG and NLE."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The study of generalization error bound is useful for algorithm evaluation.\n2. The improvement in empirical study is significant."
            },
            "weaknesses": {
                "value": "1. There is no performance reported under biased complementary label learning settings.\n2. There is no performance reported under manually annotated datasets, such as CLCIFAR10\uff0cCLCIFAR20. In addition, the results on these manually annotated data indicates the performance under instance-depedent CLL.\n3. The semi-supervised learning methods used in this paper is not the SoTA ones.\n4. There is no deviation reported in Table 3 and Table 4. It is suggested to add standard deviations or confidence intervals for the results, which would help readers better assess the statistical significance and reliability of the reported performance.\n5. The study of the separation of moderately-confident and under-confident is not reported in the ablation. It will be helpful for evaluate the idea by comparing the performance when treating these two categories the same versus separately."
            },
            "questions": {
                "value": "1. Please consider the point 1 and 2 in the weaknesses.\n2. How to evaluate the contribution of the separation of moderately-confident and under-confident instances?\n3. Assumption 1 should be reconsidered and carefully checked. It assume that the variety of negative labels is large enough in the KNN instances. However, I think this assumption is too strict.\n4. Is there any difference between the supervised and complementary learning losses for the high-confident instances? Maybe there are different from the loss gradient perspective.\n5. The negative label set size in eq.(15) is fixed at $\\tau_i$. However, when the top-$\\tau_i$ and the complementary label is largely overlapped in the multiple complementary label setting, the benefit from the complementary label sharing mechanism is largely weakened."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to decompose the CLL problem into multi-class classification problems. The one is to infer the positive label by a positive label guessing method, and another is to infer the negative labels by enhancing the negative labels of the moderately-confident and under-confident instances. Besides, this paper provides the upper bounds for the error rate of positive label guessing method and negative label enhancement method, and the effectiveness of the proposal is demonstrated by several experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper proposes a new method for CLL, which divides the CLL into two subtasks: PLG and NLE to deal with the high-confident and low-confident instances separately. The authors also verify the effectiveness of the proposal by several experiments and theorems."
            },
            "weaknesses": {
                "value": "1. This paper attempts to learn a multi-label classifier from CL. However, the difference between CL and partial labels is not clarified. Initially, Ishida proposed complementary label learning, and noted that complementary label y can be regarded as an extreme case of partial labels given to all K \u22121 classes other than class y. However, in lines 48 and 121 of this paper, the authors claim that CL can be multiple classes that the instance does not belong to, which seems to fundamentally eliminate the difference between partial label and CL.\n2. This paper does not well-motivate the proposed method, so that its advantages over existing methods are not clear. It is also not clear what conditions make the proposed method work or fail. Besides, given the strong relevance of this paper and partial label learning, it is not yet clear what advantages the algorithms in this paper have over existing partial label learning algorithms, either motivationally or experimentally.\n3. Theorem 2 and Theorem 3 do not give valuable information. For example, the theorem presented in Equation (21) seems to say one thing that a probability value does not exceed 1. Furthermore, the author's discussion of Theorems 2 and 3 is very shallow; they just tell readers that there is an upper bound on the error rate, which is meaningless because any error rate has an upper bound of 1."
            },
            "questions": {
                "value": "1. What is the difference between partial label learning and the problem addressed in this paper?\n2. What are the advantages of the proposed algorithm over other existing algorithms (including CLL and partial label learning)?\n3. What are the conditions for the proposed algorithm?\n4. What do Theorem 2 and Theorem 3 reveal?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}