{
    "id": "GDZeeCZ3MM",
    "title": "IMPLICIT VARIATIONAL REJECTION SAMPLING",
    "abstract": "Variational Inference (VI) is a cornerstone technique in Bayesian machine learning, employed to approximate complex posterior distributions. However, traditional VI methods often rely on mean-field assumptions, which may inadequately capture the true posterior's complexity. To address this limitation, recent advancements have utilized neural networks to model implicit distributions, thereby offering increased flexibility. Despite this, the practical constraints of neural network architectures can still result in inaccuracies in posterior approximations. In this work, we introduce a novel method called Implicit Variational Rejection Sampling (IVRS), which integrates implicit distributions with rejection sampling to enhance the approximation of the posterior distribution. Our method employs neural networks to construct implicit proposal distributions and utilizes rejection sampling with a meticulously designed acceptance probability function. A discriminator network is employed to estimate the density ratio between the implicit proposal and the true posterior, thereby refining the approximation. We propose the Implicit Resampling Evidence Lower Bound (IR-ELBO) as a metric to characterize the quality of the resampled distribution, enabling the derivation of a tighter variational lower bound. Experimental results demonstrate that our method outperforms traditional variational inference techniques in terms of both accuracy and efficiency, leading to significant improvements in inference performance. This work not only showcases the effective combination of implicit distributions and rejection sampling but also offers a novel perspective and methodology for advancing variational inference.",
    "keywords": [
        "Varaitional inference",
        "reject sampling",
        "implict distribution"
    ],
    "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)",
    "TLDR": "We introduce a novel method called Implicit Variational Rejection Sampling (IVRS), which integrates implicit distributions with rejection sampling to enhance the approximation of the posterior distribution",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=GDZeeCZ3MM",
    "pdf_link": "https://openreview.net/pdf?id=GDZeeCZ3MM",
    "comments": [
        {
            "title": {
                "value": "Authors' Rebuttal"
            },
            "comment": {
                "value": "Thank you for your valuable feedback. We have carefully considered your comments, and here are our responses to each of your points:\n\n1. **On the Overstatement of Contribution:**\n   We appreciate the reviewer pointing this out and agree that the application of rejection sampling to an unnormalized posterior is a standard textbook approach. In the revised version, we will add more references to classical textbooks to downplay this point. However, we still believe that the introduction of this method is important, as it provides an effective solution to the problem of evaluating implicit variational distributions. We will adjust the language in the revised version to better clarify this.\n\n2. **On the Neural Network and Computational Requirements:**\n   We have already mentioned the increased computational demand when setting $M $ through cross-validation in the limitations section. In the revised version, we will emphasize more clearly the issue of training an additional neural network, particularly the computational overhead involved. This will help readers gain a clearer understanding of this limitation.\n\n3. **On the Empirical Hand-Tuning of Cross-Validation:**\n   We will further clarify that cross-validation is an empirical method and that the selection of $M$ is an experimental adjustment. This approach is based on iterative refinement through experimentation. In the revised version, we will make this more explicit to ensure the reproducibility and clarity of the method.\n\n4. **On the Setting of $ M $ and the Rejection Rate:**\n   On page 5, we have already demonstrated the properties of the resampling distribution $ r_{\\theta, \\phi}(z|x) $ and shown that it provides a better approximation compared to the implicit proposal distribution. We also pointed out that there is a trade-off: if the rejection rate is too high, it can hinder the model\u2019s ability to efficiently obtain samples. Therefore, an appropriate choice of $ M $ is crucial to ensure that the rejection rate remains at a practically suitable value. Based on our experiments, a high rejection rate will primarily affect sampling efficiency and will not typically lead to worse performance compared to plain variational inference. In the revised version, we will make this clearer and emphasize that high rejection rates generally do not cause a catastrophic failure of the method, but rather affect its efficiency.\n\nThank you again for the thorough review of our work. We will incorporate these suggestions to further improve the clarity and quality of the paper."
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel approach that leverages neural networks to construct implicit proposal distributions, incorporating rejection sampling for improved efficiency. \nAdditionally, it employs a discriminator network to estimate the density ratio between the implicit proposal and target distributions. Building on this, the paper proposes a refined Implicit Resampling Evidence Lower Bound (IR-ELBO) to enhance accuracy. \nThe proposed method is evaluated against existing variational inference (VI) techniques through a series of experiments, demonstrating its effectiveness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-written, providing a clear presentation of the background, related work, major challenges, and the strategies employed to address each challenge.\n2. The use of rejection sampling to enhance the accuracy of implicit variational inference is straightforward yet effective.\n3. By improving the accuracy of implicit variational inference, the paper enables the construction of a tighter Evidence Lower Bound (ELBO)."
            },
            "weaknesses": {
                "value": "1. Variational inference is typically employed to avoid direct sampling from complex target distributions, thus enhancing sampling efficiency. \nHowever, the algorithm proposed in this paper requires an additional discriminator network to estimate the acceptance probability and density ratio. \nAdditionally, there is a manually tuned parameter, $M$, which must be selected via cross-validation. \nIn high-dimensional or large-scale settings, this approach could become computationally intensive due to the overhead of training the discriminator network and optimizing the hyperparameter $M$.\n\n2. Variational inference is generally favored over sampling methods like MCMC for high-dimensional posterior distributions due to its efficiency. \nHowever, rejection sampling faces significant challenges in high-dimensional settings because of the curse of dimensionality. I am therefore skeptical about this algorithm's performance and scalability in high-dimensional scenarios."
            },
            "questions": {
                "value": "1. Rejection sampling may face challenges in high-dimensional settings. \nCould you discuss whether your method maintains robustness under these conditions and support this claim with experiments conducted in high-dimensional scenarios?\n2. Additionally, could you compare the computational efficiency of your method with other approaches when applied to large-scale datasets or high-dimensional cases?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this submission a lower bound IR-ELBO is derived by considering a family of implicit variational distributions obtained via rejection sampling. The method is tested on Bayesian NN learning, toy examples and applied to VAEs, tested on MNIST where it compared favorably against the considered baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is well-written and there are some recent works on variational rejection sampling, thus the submission should be relevant to the ML community. The method is evaluated in quite varied experimental settings which is good (although the VAE results on MNIST are not close to the SOTA [5]; to be clear, this was not a claim made by the authors). Furthermore, the algorithmic descriptions are useful for understanding the proposed methods and I also appreciate the transparency of the limitation provided in Sec. 6."
            },
            "weaknesses": {
                "value": "To start with, I want to be clear that I am not an expert on implicit variational inference, which may be reflected in some of my concerns below.\n\nI was expecting to see a distinction between the proposed method and the rejection sampling approach in [1] and the one in [2].\n\nIn [1] they also learn an acceptance function which \"can be interpreted as estimating a (rescaled) density ratio between the aggregate posterior and the proposal\". I think the principle is different between this submission and [1] since [1] reformulates the prior to be a resampled distribution, but this should be made clear in the submission.\n\nIn [2] their Equation (2) is identical to your Eq. (13), which you state is different to \"traditional implicit variational inference approaches\". Could you please expand on how your formulation of $r_{\\theta, \\phi}$ is different from the one in [2]? Furthermore, Eq. (14) is the same lower bound as the one used in Eq. (4) in [2] and in Eq. (5) in [3] which should be stated---as it reads now, Eq. (14) appears to be a contribution of the submission.\n\nRegarding the IR-ELBO, I am a bit confused about the exact formulation of the IR-ELBO: below Eq. (16) it states \"Substituting the lower bound for log $Z_{\\theta, \\phi}(x)$ from Equation (16) into Equation (15) yields the final loss function, which we call the IR-ELBO\" which to me implies that Eq. (16) is not IR-ELBO, but a term in the IR-ELBO, while in row 4 in Algorithm 2 it says that the IR-ELBO is Eq. (16)?\n\nI suspect that my concerns can be resolved by the authors, at which point I will consider raising my rating.\n\n[1] https://arxiv.org/pdf/1810.11428\n\n[2] https://proceedings.mlr.press/v238/jankowiak24a/jankowiak24a.pdf\n\n[3] https://arxiv.org/pdf/1804.01712v1"
            },
            "questions": {
                "value": "Do I understand it correctly that the IR-ELBO is a looser bound on the marginal log-likelihood than the one in Eq. (14)? To me it seems a bit counter intuitive as the implicit distribution setting \"allows for more flexible posterior approximations\". Do you have an intuition to why the bound is looser (if this is indeed the case)?\n\nIs Eq. (4) really correct? Typically $f_\\phi(x)$ denotes the amortized mapping from data to variational parameters, $\\phi$ (is it the same here?). Here I read Eq. (4) as the function (the neural net which outputs $z$ after taking also the standard normal sample) is sampled from the variational distribution. Maybe this is correct, but the formulation looks a bit awkward.\n\nCould IWRS be used for mixtures of variational distributions? I.e., would it make sense to have mixtures of resampled distributions to leverage the strong results from [4, 5]?\n\nWould it be possible to apply the importance weighted ELBO [6] to Eq. (16) to make it tighter?\n\n[4] https://proceedings.mlr.press/v202/kviman23a.html\n\n[5] https://arxiv.org/pdf/2406.07083\n\n[6] https://arxiv.org/abs/1509.00519"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes \"Implicit Variational Rejection Sampling\" (IVRS), an approach combining implicit variational inference with rejection sampling to approximate complex posterior distributions more accurately. An implicit variational approximation is learned using prior-contrastive adversarial variational inference (density ration estimation $T(z,x)=p(z)/q(z|x)$). IVRS refines the posterior approximation using rejection sampling using an accept-reject routine with an acceptance probability function of form  $a(z,x)=\\sigma(T(z,x) + p(x|z) -M)$. The authors derive a new, tighter, lower-bound evidence called IR-ELBO (which is analogous to R-ELBO for explicit models). The method is tested on a set of toy examples, Bayesian Neural Network (BNN) benchmarks, and a Variational Autoencoder (VAE) on MNIST. The authors claim that IVRS outperforms traditional VI in terms of accuracy and efficiency."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. **Well written**: The paper is overall well written and structured, with clear explanations of the proposed method and its theoretical underpinnings. The paper is grounded in solid theoretical foundations, combining implicit variational inference with rejection sampling.\n2. **Experiments BNN** : The proposed method is evaluated on various datasets, and the method is compared to other methods (for implicit VI), demonstrating its effectiveness in approximating complex posterior distributions for BNN (Table 2). The reported results of 81.78 nats on MNIST seem good to me (but not state-of-the-art)."
            },
            "weaknesses": {
                "value": "1. **Novelty**: The idea of using rejection sampling to refine variational distributions was already introduced by Grover et al. 2018 for explicit models (as the authors acknowledge); the extension to implicit models is new to my knowledge (although very related).\n\n2. **Toy evaluation (5.1)**:  The performance for toy data is not very convincing (at least as visualized in Fig. 1). This might also be due to the non-optimal visualization/KDE artifacts (what is the color gradient on the contours? Shouldn't the approximations be perfect in these simple cases, especially using rejection sampling?). Can you provide a more interpretable metric, such as two-sample tests or actual statistical distances (C2ST,  Wasserstein distance...) to the target? (in Table 1, in addition to NLL) \n\n3. **VAE evaluation (5.3)**: To my understanding Table 3, contains metrics from literature (which might have different hyper parameterizations/training routines)? Furthermore, it \"compares\" against methods that are almost ten years old, with the most recent one from 2018. Picking out some examples from more recent work [1,2] that achieve even better nats on MNIST (79.09, or 76.93 with data augmentation). This should be discussed in more detail (any reason why this is not included?). Furthermore, only MNIST is evaluated, which is rather simple; the paper would benefit from more datasets (e.g., CIFAR or ImageNet, which also have clearer leaderboards on bits/dim). Overall I hence think that the current manuscript does not provide enough evidence to support the authors claim that the \"method outperforms traditional variational inference techniques in terms of both accuracy and efficiency.\"\n\n\n[1] Consistency Regularization for Variational Auto-Encoders, Samarth Sinha et. al. 2022.\n\n[2] Efficient-VDVAE: Less is more, Louay Hazami et.al. 2022."
            },
            "questions": {
                "value": "- Can the authors address my major concerns raised in the weakness section?\n- I would expect the rejection rate to be rather high initially (i.e. depending on the initializations of q, choice of M and T). For particularly bad initializations, the rejection sampling could just get stuck in the while loop. Do the authors truncate the rejection sampling algorithms after a maximum number of iterations to avoid this problem?\n- The paper would benefit from a more elaborate evaluation to support the claim by the authors that the \"method outperforms traditional variational inference techniques in terms of both accuracy and efficiency\". This can be done by comparing against more recent baselines and/or more complex datasets (with clear performance results).\n\nOverall, I tend to reject the paper in its current form. The paper is well written, and the methodology is sound, but the novelty is limited. In addition, the experimental evaluation is not very convincing or rather limited (the toy data and the VAE evaluation)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose to combine implicit variational inference with rejection sampling to improve inference quality. To overcome the limitation that the implicit variational distribution cannot be evaluated, they train a discriminator to learn the density ratio. The authors apply their method to multiple toy examples and benchmarks."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "(1) Presentation\n\nThe paper is well-presented and clearly written.\n\n(2) Empirical evaluation\n\nThe paper demonstrates the utility of the method on multiple low-dimensional toy datasets which provide a nice intuition for the method and clearly demonstrate that it improves inference quality. In addition, the authors also provide comprehensive evaluation on two large scale examples (BNNs and VAEs)."
            },
            "weaknesses": {
                "value": "(1) Overstatement of contribution\n\nThe authors point out that (page 3 middle) one of the contributions of their method is to apply rejection sampling also when the posterior is unnormalized. I don\u2019t think this should be phrased as a contribution as it is a standard textbook application of rejection sampling. Please de-emphasize this point.\n\n(2) Lack of clearer limitations\n\nWhile the authors provide a limitations section, this section is quite minimal. I think the authors should comment on the following points that are currently not addressed explicitly (or only in other parts of the paper): (A) The need to train an additional neural network, (B) The increased compute requirement to set M with cross validation.\n\n(3) Lack of explanations on how to set M\n\nThe authors state that they set M with cross validation, but later claim that it requires empirical hand-tuning. Which one is it?"
            },
            "questions": {
                "value": "I would appreciate a discussion of the behavior of the algorithm if M is not set correctly\u2014will the proposed method just perform sub-optimally or will it fail catastrophically (and potentially even worsen performance of plain VI)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}