{
    "id": "ZGyapLnDb7",
    "title": "Balancing Gradient Frequencies Facilitates Inductive Inference in Algorithmic Reasoning",
    "abstract": "Inductive inference, or extrapolation of general rules from finite instances, is understood to be the foundation of human intelligence. Unfortunately, Deep Neural Networks (DNNs) struggle with inductive inference and thus fail to learn even the simplest algorithms in Algorithmic Reasoning (AR). Existing research efforts on AR with DNNs are limited to those on the architectural design for DNNs. In this study, we investigate the influence of optimization techniques on AR performance. Through toy experiments designed to understand an optimizer's susceptibility to shortcuts in AR, we reveal that Adam, the naive choice of optimization, is easily fooled by spurious correlations. To overcome this shortcoming of Adam, we propose a novel optimizer that avoids spurious correlations by balancing gradients of low- and high-frequencies (BGF). We present extensive experiments and analyses to demonstrate the broad and multifaceted advantages of BGF across various architectures and AR tasks. In particular, BGF expands the AR capability of all explored DNN models and even shows the potential to enable learning of tasks that they previously failed at. The observed success of BGF in climbing the Chomsky hierarchy underscores the importance of optimization for developing advanced artificial intelligence with DNNs.",
    "keywords": [
        "Algorithmic Reasoning",
        "Inductive Inference",
        "Spurious Correlation",
        "Out-of-Distribution Generalization",
        "Optimization"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "Introduce gradient frequency balancing to facilitate inductive inference in Algorithmic Reasoning.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ZGyapLnDb7",
    "pdf_link": "https://openreview.net/pdf?id=ZGyapLnDb7",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a new optimization method called BGF (Balancing Gradient Frequencies), aimed at improving length generalization in  Algorithmic Reasoning (AR) through the mitigation of shortcut learning. Building on the experimental findings regarding Adam's vulnerability to spurious correlations in AR tasks, the authors propose that BGF mitigates these correlations by balancing high- and low-frequency gradients. This approach encourages the model to learn general rules rather than relying solely on patterns present in the training data. Extensive experiments are conducted, demonstrating the effectiveness of BGF across various DNN architectures and tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Extensive experiments and analyses;\n\nA novel approach to study the effect of optimization on generalization in AR tasks;\n\nThe results on multiple DNN architectures and tasks show that BGF effectively improves generalization on test data."
            },
            "weaknesses": {
                "value": "The task description in the main text lacks clarity. There are too many task-related descriptions in the appendix\u2014such as the \"Modular Arithmetic,\" \"Solve Equation,\" and \"Bucket Sort\" tasks\u2014that interrupt the flow of the main text and affect its readability."
            },
            "questions": {
                "value": "Since the low-pass filter is implemented using a moving average filter with a window size of \\lambda, what is the basis for selecting the window size \\lambda? How does \\lambda value affect the results?\n\nIn Table 3 [Right], why is the result of gradient filtering with an EMA lower than that with a queue in the LSTM model?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents Balancing Gradient Frequencies (BGF), an optimizer that enhances inductive inference and OOD generalization in deep neural networks for algorithmic reasoning tasks. Unlike traditional optimizers like Adam, which can learn spurious correlations, BGF balances low- and high-frequency gradient components to avoid shortcut learning and improve OOD generalization. Experiments across various DNN architectures and AR tasks demonstrate BGF\u2019s superior performance and faster convergence."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The method tackles improving OOD AR tasks by proposing a new optimizer, which is novel and interesting to me.\n\n2. The empirical studies at the beginning of Section 3 are interesting and well-designed, effectively illustrating the susceptibility of standard optimizers like Adam to shortcut learning. \n\n3. Extensive experiments show that BGF consistently outperforms Adam across multiple architectures and tasks."
            },
            "weaknesses": {
                "value": "1. The paper lacks formal theoretical analysis explaining why BGF outperforms traditional optimizers. While the empirical results are strong, a theoretical understanding of BGF\u2019s benefits would enhance the contribution.\n\n2. The study primarily compares BGF with Adam, with fewer comparisons to more recent optimizers designed to improve generalization. Including a wider range of baseline optimizers would strengthen the evaluation."
            },
            "questions": {
                "value": "Please refer to the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This study examines optimization\u2019s impact on AR performance, showing through toy experiments that Adam, a commonly used optimizer, is susceptible to spurious correlations that hinder out-of-distribution (OOD) generalization. To address this, the authors introduce a novel optimization method, Balancing Gradient Frequencies (BGF), designed to reduce shortcut learning by balancing low- and high-frequency gradients, thus promoting stronger inductive inference. Extensive experiments across varied AR tasks and DNN architectures reveal that BGF significantly improves accuracy, accelerates convergence, and smoothens the loss landscape, enabling DNNs to tackle tasks previously deemed unmanageable."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThrough extensive experiments, the authors demonstrate that balancing gradient frequencies directly impacts the model\u2019s ability to generalize beyond the training distribution. This insight is particularly valuable, as it highlights the frequency characteristics of gradients as a crucial factor in overcoming shortcut learning, a perspective that has been relatively unexplored in AR.\n2.\tThe study documents a phenomenon termed \"train-test splitting,\" in which DNNs achieve training accuracy early on but require additional steps to attain OOD test accuracy. This splitting behavior resembles the grokking phenomenon and is linked to changes in gradient frequency patterns. The discovery emphasizes the importance of low-frequency gradient components for OOD generalization, offering a new understanding of model behavior during training.\n3.\tBGF\u2019s architecture-agnostic design is evidenced by improved performance across various DNN types, including RNNs, LSTMs, and Transformer models, on multiple AR tasks. This adaptability underscores BGF\u2019s potential for broad application, marking it as a significant innovation in optimization for generalization across neural network architectures."
            },
            "weaknesses": {
                "value": "1. Although the manuscript compares BGF with optimizers like Adam, SAM, SWAD, and LPF-SGD, incorporating more recent advancements in optimization techniques that are known to enhance generalization would further strengthen the case for BGF's superiority.\n\n2. Conduct a hyperparameter sensitivity analysis focusing on \u03bb and other gradient-balancing factors (e.g., \u03b1 and \u03b2). This experiment would provide insights into how these parameters influence BGF's ability to generalize, especially under varying data conditions.\n\n3. To deepen insights into how BGF affects gradient dynamics, examining gradient frequencies across different layers of DNN architectures is recommended. This analysis could reveal whether specific layers benefit more from low-frequency gradients, enabling further fine-tuning of BGF for optimal performance in various model architectures."
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new optimizer for Algorithmic reasoning tasks to avoid/mitigate shortcut learning for better OOD generalizability. Especially the work looks into the frequency domain of the gradients. The work provides toy experiments and synthesized spurious correlations in Algorithmic reasoning tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "\u2022\tIf the claim is correct, the proposed optimizer would be simple and easy to implement, as it would be just a frequency (low/high) pass filter."
            },
            "weaknesses": {
                "value": "\u2022\tMost of all, the definition of shortcuts is the most serious problem that I have with this paper. I cannot agree with the authors\u2019 definition of \u201cshortcuts\u201d which is stated as \u201cnon-generalizing hypotheses are termed \u201cshortcuts.\u201d Shortcut learning happens when a model can minimize the training loss just by looking at simpler features and giving up learning more complex core features. That means if such non-generalizing features are complex enough or more complex than core features, shortcut learning may not happen. That is why the name is \u201c\u201cshort\u201dcut\u201d learning. That is why \u201cnon-generalizing hypotheses\u201d cannot be just termed shortcuts.\n\n\u2022\tAlso, the sentence over lines 191-193 \u201cShortcuts can be interpreted as learning non-generalizing correlations between inputs and outputs that are valid in the training distribution but do not hold in the target distribution;\u201d is not fully true. It depends on the target distribution. If the target distribution is still in distribution, such correlations will still be valid, but such correlations just won\u2019t be visible.\n\n\u2022\tThis paper only considered two simple forms of synthetic spurious correlations on \u201ctoy\u201d experiments. Algorithmic reasoning tasks are a good start and are used to examine the viability of an idea to apply to real-world tasks. It is unclear how this paper\u2019s outcome can be useful for real-world tasks, vision and language. I am not confident to believe the claims stated in this paper as a general phenomenon. Also, nobody knows if something will go different in such Algorithmic reasoning tasks and conventional vision and language tasks?\n\n\u2022\tFor experiments with spurious correlations in Fig 2, the authors only experimented with spurious correlations of which portion is over 80% (when P >= 0.8). However, when the proportion is just that high the spurious correlation is trivial and straightforward."
            },
            "questions": {
                "value": "-\tThe following paper seems to have a good overlap with this submission:\n\u201cFrequency Shortcut Learning in Neural Networks,\u201d Shunxin Wang, Raymond Veldhuis, Christoph Brune, and Nicola Strisciuglio, NeurIPS workshop on Distribution Shifts, 2022.\nHence, the novelty of this submission may be hurt.\n\n-\tFor other points, please refer to the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}