{
    "id": "zP8HygcAMY",
    "title": "Can LLMs Evaluate Complex Attribution in QA? Automatic Benchmarking Using Knowledge Graphs",
    "abstract": "The attribution of question answering (QA), which is to get evidences for supporting the generated answer, has attracted wide research attention. The current methods for automatically evaluating the attribution, typically relying on Large Language Models (LLMs), are still inadequate, particularly in recognizing subtle differences between attributions, and in measuring complex attribution reasoning. Existing benchmarks, which are primarily based on manual annotations, suffer from limited evaluation settings with incomplete and coarse attribution categories and reasoning scenarios, hindering the evaluation and advancement of attribution evaluators. To address this gap, we introduce Complex Attributed Question Answering (CAQA), a large-scale benchmark automatically generated using Knowledge Graphs (KGs), containing more comprehensive attribution categories and complex attribution reasoning scenarios. Our experiments with two specifically developed evaluators and nine LLM evaluators reveal that they struggle in identifying negative attribution categories and handling complex attribution reasoning in both zero-shot and few-shot settings, but mostly perform relatively well in the fine-tuning setting. Moreover, all evaluators perform inadequately in fine-grained attribution identification scenarios. The experiments also demonstrate that CAQA is consistent with human annotations, and is promising for selecting and developing more effective attribution evaluators in QA.",
    "keywords": [
        "Large Language Model",
        "Attributed Question Answering",
        "Knowledge Graph"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zP8HygcAMY",
    "pdf_link": "https://openreview.net/pdf?id=zP8HygcAMY",
    "comments": [
        {
            "summary": {
                "value": "The paper presents CAQA (Complex Attributed Question Answering), a large-scale automatically generated benchmark designed to assess the attribution capabilities of QA systems, particularly Large Language Models (LLMs). CAQA leverages Knowledge Graphs (KGs) to create comprehensive attribution categories and to handle complex reasoning scenarios. The benchmark distinguishes between supportive, partially supportive, contradictory, and irrelevant evidence types and introduces reasoning complexity through different forms of evidence combination (e.g., union, intersection, concatenation)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- CAQA uses KGs to generate complex QA benchmarks automatically, enabling scalability and minimizing manual annotation effort.\n- Different reasoning complexities are considered, highlighting LLMs' capabilities in handling logical relationships between facts.\n- The benchmark includes fine-grained attribution categories."
            },
            "weaknesses": {
                "value": "- The task setting seems very similar to NLI to me, more discussions are needed.\n- Lack of a few details about the human annotation process.\n- The distribution of the complexity is biased."
            },
            "questions": {
                "value": "- How do you verify the quality of converted natural language style questions?\n- What is the inter-agreement score of human annotations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Complex Attributed Question Answering (CAQA), a large-scale benchmark designed to evaluate complex attributions in question answering (QA). CAQA is automatically generated using knowledge graphs (KGs), includes a broader range of attribution categories along with intricate attribution reasoning scenarios, and is also aligned with human annotations. Experiments with two specifically developed evaluators and nine large language model (LLM) evaluators reveal that these models struggle to identify negative attribution categories and handle complex attribution reasoning in both zero-shot and few-shot settings but mostly perform relatively well in the fine-tuning setting."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper introduces CAQA, a large-scale benchmark for evaluating complex attributions in QA.\n2. The CAQA dataset contains various new definitions (e.g., fine-grained attribute categories and attribution complexities), and the data construction process is automatic, considerate, and comprehensive.\n3. This paper contains comprehensive experiments. In addition to model performance on CAQA, it also includes fine-grained analysis, human consistency, and out-of-distribution data."
            },
            "weaknesses": {
                "value": "1. This paper only considers GPT-3.5 and GPT-4 as closed-source LLMs, and some open-source LLMs used may be outdated (e.g., Mistral-7B has revolutionized various versions). Adding more diverse and latest models in experiments would have greater contributions and help to discover which LLMs perform best on this challenging task.\n2. There is a lack of comparisons with human performance on (a subset) of the dataset, which would better illustrate the performance gap and the challenge of the dataset.\n3. While the contribution of the paper centers on a new challenging benchmark, it would be much helpful if the authors can provide an error analysis, which will direct newcomers in future research."
            },
            "questions": {
                "value": "See \"Weaknesses\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces the dataset Complex Attributed Question Answering (CAQA), containing answers to questions with associated source attributions, where the attributions may or may not support the answer. The non-support attributions are divided into 3 labeled categories: Partially supported, Contradictory Irrelevant.\n\nThey evaluate how well different LLMs can classify Q+A+source into these 4 categories, finding that in many cases they struggle to do well, especially on distinguishing the non-supportive categories.\n\nThe CAQA dataset is constructed from existing KGQA datasets (GrailQA and WebQuestionsSP), making use of the associated knowledge graph to produce different types of non-supportive evidence (and using GPT-3.5 to turn KG triples into natural language sentences). The resulting dataset is quite big (137k train, 24k test), allowing for fine-tuning experiments as well. The fine-tuned models do very well in distribution, and they also do limited out-of-distribution evaluation on a subset of ALCE further annotated with these non-supportive categories, showing promising results there as well."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The dataset is relevant for the important topic of answers with attributions from LLMs. Being able to carefully validate whether an answer actually follows from the sources is an important skill, and this dataset aims at helping with this.\n\nThe paper is well written, clearly describing the approach.\n\nThe use of the KG to create various incorrect attributions, together with using LLM to rewrite at text, seems quite effective.\n\nThe paper provides access to the full dataset for exploration which is truly helpful in assessing it.\n\nThe methods are tested on a more realistic, OOD, dataset."
            },
            "weaknesses": {
                "value": "While breaking down the non-supportive cases into three subcategories can be helpful for understanding limitations, the boundary between them can be quite unclear. Also the prompt for the non-GPT models doesn't go into great detail (beyond some examples) on what each category means. For instance, the \"contradictory\" evidence is often for actual true facts, so they're not actually contradiction, it's just the \"wrong\" evidence.\n\nE.g., the answer \"The person who founded the United States Coast Guard also founded the United States Department of the Treasury.\" is presented as being contradicted by the source \"Alexander Hamilton is the founder of the United States Coast Guard and the Montgomery County Sheriff's Office, which is a government agency.\", but this isn't really a contradiction, it's more like missing evidence. A true contradiction should lead you to think the answer is actually false, if you trust the source.\n\nThe \"Partial support\" category also can be quite subjective, as in the case of \"The 2011 Estoril Open tournament event competition is the Men's Singles.\" being partially supported by \"The 2011 Estoril Open had a men's singles competition.\"  (what's missing is apparently that \"2011 Estoril Open\" was \"tournament event competition\", but that's pretty much implied by the fact that they had a men's single compeition).\n\nBecause of this, it might also be useful to report the most important \"supported\" vs \"not supported\" scores.\n\nAnother concern is the simplicity of the dataset, with simple QA assertions attributed by short source sentences. How does good performance on CAQA transfer to more realistic settings. And can it be used to train better source attribution models as well? There is some exploration of this with the OOD ALCE dataset, but the effect (e.g., between Vicuna-13B and Vicuna-13B-finetuned) isn't as impactful as one might have hoped."
            },
            "questions": {
                "value": "Some discussion on the ambiguity (and actual errors) in the labeled categories would be useful (e.g., human annotator agreements on a sample). \n\nAlso would be good to discuss the lack of context consideration which is usually very important in real usage (e.g., the example in the paper \"Who plays Fruma Sarah in Fiddler on the Roof\" depends on which version of Fiddler on the Roof is being referenced)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This manuscript aims to bridge the gap in attribution evaluators' use of knowledge graphs by providing detailed categories. This study's experiment offers multiple configurations in zero-shot, few-shot, and fine-tuned contexts, demonstrating that the fine-tuning process can significantly improve performance. This benchmark aims to address the shortcomings of existing attribution evaluators, which face challenges with intricate attribution categories and sophisticated reasoning processes."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The use of KGs for automatic construction of the benchmark is novel, making the process scalable and adaptable.\nThe research tests various models and demonstrates the needs of fine-tuning for achieving robust performance.\nThe benchmark shows high consistency with human evaluations, supporting its credibility as an effective tool for future developments in QA systems.\nThe choice to test multiple LLMs, including state-of-the-art models like GPT-4 and LLaMA variants, provides a robust analysis of performance across different model scales and settings.\nThe inclusion of a wide range of attribution types and complexity levels sets a high standard for evaluating QA systems."
            },
            "weaknesses": {
                "value": "The benchmark is tailored for KG-based QA tasks, which may not reflect the challenges present in more diverse, open-domain QA systems. \nThe reliance on GPT models for generating natural language representations from KGs may introduce subtle biases.\nThe rationale behind choosing specific complexity types (e.g., concatenation, intersection) could be expanded with examples illustrating real-world implications of these complexities in QA."
            },
            "questions": {
                "value": "1. How might CAQA be adapted to handle dynamic or temporal data in QA tasks?\n2, What specific types of biases were identified or considered when using LLM-generated prompts?\n3. Are there plans to include more diverse logical operations, such as negation, in future iterations of CAQA?\n4. Could the inclusion of human-in-the-loop evaluations further enhance the quality of the generated benchmark data?\n5, Extend the benchmark's applicability by including examples from various knowledge domains (e.g., medical, legal) to test the robustness of attributions in specialized contexts.\n6. The reliance on GPT models for generating natural language representations from KGs may introduce subtle biases. Addressing how these biases are minimized or discussing potential implications would strengthen the manuscript.\n7. Discussing how CAQA could be adapted for such tasks would add value especially how to address the challenges in more diverse, such as open-domain QA systems. ."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}