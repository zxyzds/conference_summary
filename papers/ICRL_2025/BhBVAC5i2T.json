{
    "id": "BhBVAC5i2T",
    "title": "Meta-Referential Games to Learn Compositional Learning Behaviours",
    "abstract": "Human beings use compositionality to generalise from past to novel experiences, assuming that past experiences can be decomposed into fundamental atomic components that can be recombined in novel ways. \nWe frame this as the ability to learn to generalise compositionally, and refer to behaviours making use of this ability as compositional learning behaviours (CLBs). \nLearning CLBs requires the resolution of a binding problem (BP). \nWhile it is another feat of intelligence that human beings perform with ease, it is not the case for artificial agents. \nThus, in order to build artificial agents able to collaborate with human beings, we develop a novel benchmark to investigate agents\u2019 abilities to exhibit CLBs by solving a domain-agnostic version of the BP. \nTaking inspiration from the Emergent Communication, we propose a meta-learning extension of referential games, entitled Meta-Referential Games, to support our benchmark, the Symbolic Behaviour Benchmark (S2B). \nBaseline results and error analysis show that the S2B is a compelling challenge that we hope will spur the research community to develop more capable artificial agents.",
    "keywords": [
        "referential game",
        "language grounding",
        "compositionality",
        "systematicity",
        "few-shot learning",
        "meta-learning",
        "reinforcement learning",
        "language emergence",
        "symbolic behaviours",
        "benchmark"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We present a novel framework of Meta-Referential Games upon which we built a novel benchmark, the Symbolic Behaviour Benchmark, and use it in an initial study to evaluate AIs' abilities to learn compositional learning behaviours.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=BhBVAC5i2T",
    "pdf_link": "https://openreview.net/pdf?id=BhBVAC5i2T",
    "comments": [
        {
            "summary": {
                "value": "This paper introduce a novel benchmark aiming to assess the ability of artificial agents to meta-learn behaviors that leverage the compositional nature of their sensory inputs. In this benchmark, two collaborative agent strive to meta-learn to solve referential games. In each episode, the two agents first execute a series of referential games that take in samples endowed with a specific compositional distribution. Then, they test their ability to generalize to novel samples from the same distribution. The global objective is to meta-learn this task for any compositional distribution of inputs. Experiments support that current methods largely fail at this task."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This benchmark introduces a challenging and important problem for current methods. Experiments support the claim of the paper."
            },
            "weaknesses": {
                "value": "It is hard to assess the novelty as it is stated in the paper that there is a previous version of the benchmark (published ?). The paper is hard-to-follow and often confusing."
            },
            "questions": {
                "value": "The paper could likely be simplified:\n\nThe paper introduces a lot of concepts, the abstract refers to compositionality, binding problem, emergent communication, human-agent collaboration and meta-referential game. Also few-shot learning in the introduction.\n- Human-agent collaboration is only used as a high-level motivation of the work, so its reference in the middle of the abstract and line 52 mostly confuse the reader with works including humans in the loop.\n- Emergent Communication: In the abstract, it is unclear what \"Emergent Communication\" refers to. Is it a framework ?\n- The binding problem (BP): The relation between CLB and binding problem is unclear in the abstract, not very clear in the introduction, and  get clearer in Greff et al. (2020). Most of the statements related to BP are unclear: that an \"inherent BP\" must be solved be agents to exhibit CLB. \"Solving the BP instantiated in such a context, i.e. re-using previously-acquired information in ways that serve the current situation\" is done by all learning artificial agents. What does it mean to \"instantiate a BP\" ? Do any representations of any latent factors instantiate a BP ?\nI overall don't understand why this paper needs to talk about BP. This paper is about meta-learning behaviors that leverage the compositional nature of their inputs. \n\nSome critical concepts are unclearly defined: systematicity, ZSCT (l 174), symbolic space (l195), EoA (l379), posdis (l163) and bosdis (l163). It would help if compositionality measures were briefly explained (posdis, bosdis). \n\nI do not understand how the object-centric variant of the representation (for the listener) is built. That should be clarified in Section 3.\n\nMinor points:\n- generalise -> generalize\n- l391: axises -> axis\n- l87: What is \"a semantic domain that can be probed and queried\"\n- Figure 2 and Figure 4 are hard to read: the fontsize is too small and the bold text is hard to read.\n- l256: partitionaing\n- l334: what does it mean do bridge the gap between two conditions \"Hill-RSC and Chaa-RSC\".\n- l349: What is the core memory module ?\n- The beginning of 4.2 about meta-RG is very clear and could probably go to Section 3.\n- l285-286 : It is confusing as the described set of stimuli used is misaligned with what is described in Section 2. The speaker does not receive the same set of stimuli in both.\n- Section 4.2.1: Is it using the rule-based speaker agent ? Is it normal there is no 4.2.2 ? Then 4.3. is using the Posdis-speaker agent ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel benchmark, the \"Symbolic Behaviour Benchmark\", for evaluating compositional learning behaviors. The study introduces Meta-Referential Games (Meta-RGs), a meta-learning extension of referential games, to test agents' ability to solve a binding problem that is crucial for learning CLBs. This benchmark emphasizes symbolic receptivity and constructivity, encouraging agents to develop compositional generalization skills while interacting with each other."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Relevance: I think this paper tries to address an important problem in that it proposes a benchmark in which succesful behavior means that agents learned to generalize compositionally, instead of only having learned to generalize compositionally. The problem of compositionality and compositional generalization is of general interest to the community. Thus, this benchmark might be generally useful.\n\nNovelty: The introduction of S2B and Meta-RGs adds depth to the compositionality field by pushing beyond mere combinatorial generalization (CG) to a meta-learning context where agents adapt to unseen symbolic structures. The Symbolic Continuous Stimulus (SCS) representation is an innovative method to instantiate a BP, ensuring agents must infer structures over multiple observations, aligning with the real-world learning constraints in open-ended contexts.\n\nAnalyses: The experiments establish state-of-the-art limitations effectively, showing that both MARL agents and LLMs struggle with this benchmark, thus illustrating its difficulty and relevance. The experiments are clear and I like that they always come with a hypothesis followed by the results.\n\nIntroduction: The first few sections, i.e. overview of the problems of systematicity/compositionality, lingustic compositionality, and compositionality are helpful."
            },
            "weaknesses": {
                "value": "Accessibility: The meta-learning setup, combined with the specialized SCS representation, might limit accessibility and reproducibility. The SCS's construction, particularly the Gaussian kernel setup, could be further detailed, I didn't quite get what was going on there. The writing is generally quite verbose and I had really some difficulties in following along. That there are many abbreviations throughout doesn't really help here either. Some of the figures are very small an complicated to read. This makes it again hard to follow.\n\nScope: The focus on RL and MARL agents is suitable, but extending evaluations to to other models could have been fun. I would have really liked to see something on further multi-modal models here. Many of the evaluations are currently in the simplest form, i.e. the basic form of the proposed game, the most common agents playing them, as well as LLMs without any modifications to the standard prompts. This makes it a little unclear what exactly makes the difference in agents' inabilities to do these games well.\n\nCLB definition: While the paper defines CLBs distinct from CBs, it would benefit from clearer operationalization criteria to guide comparisons. Are there any performance metrics beyond linguistic compositionality and RG accuracy?\n\nLLM behavior: The below-chance LLM performance is interesting but could be further analyzed. \n\nFit: It felt a little like this paper would fit better to a more targeted conference but I can be convinced."
            },
            "questions": {
                "value": "What do we learn from this? Essentially most of the results just point to an inability of different agents to learn to generalize compositionally. Is the idea that the community should now focus on getting these agents to be better on the benchmark? How would the authors imagine such progress?\nI'm not quite sure but what is meant by domain-agnostic BP?\nPerhaps adding short summaries to every section about what the main message is could help?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a benchmark called S2B that can evaluate the ability of artificial intelligence agents in combinatorial learning behaviors (CLBs). S2B uses Meta-Referential Games as the basic framework and uses the SCS method to represent stimuli. This paper uses S2B to test the CLBs of multi-agent reinforcement learning models and LLM."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper introduces the S2B benchmark, designed to evaluate the combinatorial learning behaviors (CLBs) of AI models.\n- It proposes the SCS method for representing stimuli in a domain-independent manner, avoiding reliance on specific modalities like visual, verbal, or auditory information.\n- Meta-Referential Games are presented as the primary framework within the S2B benchmark, aiming to assess agents' capabilities in symbolic learning and combinatorial learning behaviors (CLBs)."
            },
            "weaknesses": {
                "value": "- Insufficient validation of domain-agnostic BP. While the S2B benchmark and meta-referential game frameworks intend to construct domain-agnostic BP, there is a lack of sufficient experimental data to validate their applicability in various domains or applications. Whether this benchmark and framework can be extended to different fields such as vision and language still needs to be further verified.\n- Terminology and lack of concrete examples: The paper contains a large number of terms (such as CB, CLB, BP, support stage, query stage, etc.), although their concepts are mentioned in the article, the concepts are relatively vague and lack simple examples to help readers understand. It may not be intuitive for readers who are new to these concepts."
            },
            "questions": {
                "value": "- In Figure 2, it is not clear what Latent Stimuli is.\n- In line 267, the standard deviation sampling interval of the Gaussian distribution is not explained.\n- In 4.1 and 4.2 section, there are lacks of details and examples: the representation and meaning of the symbol combination, the specific process of the experiment are not explained in detail, which confuses some readers when understanding the experimental design. While the interaction between multiple agents is mentioned, no specific examples are provided to illustrate how the messaging and identification tasks are performed, which can lead to barriers to understanding."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the Symbolic Behaviour Benchmark (S2B), a meta-learning benchmark designed to test agents\u2019 abilities to generalize compositionally within single episodes through Compositional Learning Behaviours (CLBs). S2B utilizes Meta-Referential Games, an extension of referential games embedding a Binding Problem (BP), challenging agents to dynamically bind and recombine information from limited examples. Baseline results on multi-agent reinforcement learning (MARL) and large language models (LLMs) suggest S2B presents a challenging benchmark for current AI capabilities."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Originality: The benchmark provides a unique approach to evaluating compositionality in a few-shot learning context, moving beyond static generalization and introducing a dynamic, episode-based test.\n- Quality: The benchmark and supporting experiments are rigorously defined, leveraging established methods like referential games and meta-learning frameworks to create a novel testing ground for CLBs.\n- Significance: S2B addresses an important challenge in AI\u2014evaluating compositional learning in dynamic environments\u2014which could spur new research into agent architectures and learning strategies."
            },
            "weaknesses": {
                "value": "- Presentation: The structure of the meta-RG could be clarified by using a single detailed example to illustrate an episode from beginning to end, making the compositional requirements more apparent.\n- Experiments: Evaluating LLMs within this benchmark may not be entirely fair, as the task setup deviates from natural language processing, which LLMs are primarily designed for. The benchmark\u2019s symbolic structure lacks the natural language context that LLMs are optimized to process, raising concerns about using LLMs without modifications to better align with symbolic input.\n- Human Baseline: The lack of a human experiment or baseline raises questions about the difficulty of the benchmark for agents versus human performance. Introducing human testing on S2B could provide additional insights, highlighting any inherent complexity in CLB learning and enabling better evaluation of AI performance relative to human compositional understanding."
            },
            "questions": {
                "value": "- Could the authors clarify how the vocabulary permutation scheme is implemented and whether it impacts the agents\u2019 learning or communication strategies in any unintended ways?\n- How sensitive are the experimental results to variations in the number of symbolic dimensions (Ndim) or other hyperparameters of the SCS representation? Additional experiments on this could offer more insight into the scalability of the benchmark.\n- Were any human experiments conducted to provide a baseline for performance on S2B? Including human data could offer a valuable perspective on the complexity of the benchmark."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}