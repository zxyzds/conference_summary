{
    "id": "DD11okKg13",
    "title": "Exploring the Effectiveness of Object-Centric Representations in Visual Question Answering: Comparative Insights with Foundation Models",
    "abstract": "Object-centric (OC) representations, which represent the state of a visual scene by modeling it as a composition of objects, have the potential to be used in various downstream tasks to achieve systematic compositional generalization and facilitate reasoning. However, these claims have not been thoroughly analyzed yet.\nRecently, foundation models have demonstrated unparalleled capabilities across diverse domains from language to computer vision, marking them as a potential cornerstone of future research for a multitude of computational tasks.\nIn this paper, we conduct an extensive empirical study on representation learning for downstream Visual Question Answering (VQA), which requires an accurate compositional understanding of the scene. \nWe thoroughly investigate the benefits and trade-offs of OC models and alternative approaches including large pre-trained foundation models on both synthetic and real-world data, and demonstrate a viable way to achieve the best of both worlds. \nThe extensiveness of our study, encompassing over 600 downstream VQA models and 15 different types of upstream representations, also provides several additional insights that we believe will be of interest to the community at large.",
    "keywords": [
        "Object-centric Learning",
        "Foundation Models"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "We evaluate and clarify the benefits and trade-offs of object-centric models compared to alternative approaches including foundation models, and analyze the benefits of object-centric bias",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=DD11okKg13",
    "pdf_link": "https://openreview.net/pdf?id=DD11okKg13",
    "comments": [
        {
            "comment": {
                "value": "Thank you for your review and your positive assessment of our work. We answer your questions and concerns below:\n\n### Weaknesses and Questions\n\n> As stated in the paper, the datasets would have included more real-world data.\n\nWe kindly refer you to our general response to all reviewers, where we explain that we are incorporating the GQA dataset as an additional real-world dataset into our study and will share the results as soon as they are ready.\n\n> Some analysis of the application of the findings is lacking.\n\n> Could you give some more insights on the application of the findings in the paper?\n\nWe would like to highlight that the role of OC models in downstream tasks has been partially underexplored and our empirical study tries to cover this gap in the literature. We have summarised some of the takeaways of our study and their contribution to the community below:\n\n* We\u2019ve empirically clarified the trade-offs of foundation models vs OC models and found out that foundation models will perform the same or better than standard OC models on complex or real-world data without any training or hyperparameter tuning. However, they require much more downstream compute and they have less explicit representations compared to OC models. However, applying the OC bias on top of a foundation model (DINOSAUR) will benefit us with the best of both worlds, leveraging the general vision capabilities of foundation models and thus, showing strong performance on hard synthetic and real-world datasets while having explicit object representations. The only downside is that the OC bias should be trained on the downstream dataset. Therefore, depending on the settings at hand, one can select the proper type of model based on the insights of our study.\n\n* For evaluation, the focus of the OC community has been mainly on unsupervised object discovery tasks and segmentation metrics while downstream evaluation of representations has often been overlooked. We\u2019ve empirically shown that representations that perform well in downstream tasks do not necessarily excel in segmentation tasks, and that segmentation accuracy (ARI) or reconstruction error (MSE) are not reliable predictors of downstream performance. We hope this highlights the importance of incorporating downstream evaluation as a key aspect of model assessment in the OC community.\n\n> I'm wondering if the conclusions would be applied in other domains out of VQA.\n\nWe would like to emphasize that VQA involves a diverse range of questions that require understanding and reasoning about the visual scene. Demonstrating that the methods work effectively on this task suggests that the results can be extended to other tasks that require similar levels of understanding and reasoning.\n\n> What makes me curious is what the performance would be after fine-tuning the foundation models on the specific VQA tasks, since other models have been fitted on the data. I suppose this would make it more fair for the comparison of different models and may inspire more insights.\n\nThat\u2019s a good point. In our initial set of experiments, we analyzed the effect of fine-tuning DINOv2 on the VQA task, and as expected, we observed that the downstream performance improved but the gain was not significant and came with a substantial computational burden. This made the comparison with OC models less fair in terms of computational requirements. Thus, we decided not to pursue fine-tuning further."
            }
        },
        {
            "comment": {
                "value": "> I am not sure how these takeaway messages could contribute to the community. \n\nWe would like to highlight that the role of OC models in downstream tasks has been partially underexplored and our empirical study tries to cover this gap in the literature. We have summarised some of the takeaways of our study and their contribution to the community below:\n\n* We\u2019ve empirically clarified the trade-offs of foundation models vs OC models and found out that foundation models will perform the same or better than standard OC models on complex or real-world data without any training or hyperparameter tuning. However, they require much more downstream compute and they have less explicit representations compared to OC models. However, applying the OC bias on top of a foundation model (DINOSAUR) will benefit us with the best of both worlds, leveraging general vision capabilities of foundation models and thus, showing strong performance on hard synthetic and real-world datasets while having explicit object representations. The main downside is that the OC bias should be trained on the downstream dataset. Therefore, depending on the settings at hand, one can select the proper type of model based on these insights.\n\n* For evaluation, the focus of the OC community has been mainly on unsupervised object discovery tasks and segmentation metrics while downstream evaluation of representations has often been overlooked. We\u2019ve empirically shown that representations that perform well in downstream tasks do not necessarily excel in segmentation tasks, and that segmentation accuracy (ARI) or reconstruction error (MSE) are not reliable predictors of downstream performance. We hope this highlights the importance of incorporating downstream evaluation as a key aspect of model assessment in the OC community.\n\n> Some conclusions are already known such as : ' Consistency of the Results Across Question Types.'\n\nTo the best of our knowledge, this conclusion has not been definitively demonstrated for OC models. We agree that the finding might not be surprising, but it\u2019s not entirely obvious either. Specifically, given that the representations in our study are learned in very different ways, it could also be plausible that certain representations would perform better on specific question types, while others might excel on different types. This possibility makes the consistency of results across question types relevant. Even though it\u2019s a relatively minor insight, we felt it was important to include, as we believe it adds value to the overall understanding and provides helpful documentation.\n\nWe are happy to answer any further questions you may have."
            }
        },
        {
            "comment": {
                "value": "Thank you for your review of our work and the detailed feedback. We answer your questions and concerns below:\n\n### Weaknesses\n\n> The question that this paper wants to answer is: whether OC, fixed-region or global representation is better for VQA. To answer this question, the authors need to provide a model with the same architecture, model size and pre-training data. The only variant in the model is the feature representation way, i.e., OC, fixed-region or global. However, I didn't find this experiment in the paper. Maybe I missed it. Please point it out and highlight it in the rebuttal.\n\nThank you for pointing this out. We completely agree that the fairest way to compare these different representations would be to maintain the same factors of variation such as model architecture, size, and pre-training data while varying only the representation type (OC, fixed-region, or global). However, implementing such a controlled experiment would require substantial engineering effort and computational resources to ensure all factors except the representation type remain consistent, and everything works as expected. In our study, we instead took a best-effort approach by utilizing existing models with minimal modifications, which allowed us to assess the effectiveness of different representations without setting up the entire experiment from scratch. While we acknowledge this as a limitation, we have made efforts to minimize the impact of other factors, particularly by comparing a foundation model with and without the OC inductive bias (DINOv2 and DINOSAURv2), ensuring as fair a comparison as possible within our available resources.\n\n> Synthetic data such as CLEVR and CLEVRTex are not challenging enough since the objects can be easily detected (and even hard-coded).\n\nWe agree that solely using these synthetic datasets might not be enough for our study and therefore, we have included VQA-v2 to confirm our results in real-world settings. We have selected these datasets mainly because they are some of the traditional synthetic datasets that have been popular in the OC community in the past years.\nFurthermore, we kindly refer you to our general response to all reviewers, where we explain that we are incorporating the GQA dataset as an additional real-world dataset into our study and will share the results as soon as they are ready.\n\n> Many symbolic methods have solved these datasets with 100% accuracy.\n\nCan you please elaborate on which methods have achieved the perfect performance, and on which tasks?\n\nNonetheless, we would like to highlight that this is not directly relevant to our study. The fact that some models can achieve perfect accuracy indicates that those models are effective at solving all the specific challenges presented by the dataset. However, for our study, the objective is not to solely push the boundaries of performance but to investigate how different model representations, including OC representations, behave on the VQA tasks defined on these datasets. In fact, our goal is not to solve the VQA task and achieve state-of-the-art results but to analyze the benefits and trade-offs of OC representations compared to other types of representations, with VQA serving as the downstream task for this evaluation. Therefore, as long as we observe a difference in the performance of models on these datasets and not all of the models achieve perfect accuracy, they would be suitable datasets for this goal."
            }
        },
        {
            "comment": {
                "value": "### Questions\n\n> Minor: I wonder how is the \"640\" computed on Line 315? And the \"640 models\" is slightly misleading, as they are actually different are experimental configurations rather than distinct model architectures.\n\nWe apologize for the confusion. We will clarify this in the final version of the paper. This number corresponds to the total number of VQA downstream models we trained for the study. Given the number of datasets (4) and the number of different split sizes of Multi-dSprites (4; 40k, 80k, 160k, and 320k images. So 7 different datasets in total), number of models (15), number of seeds (1 for pre-trained models, 3 for other models), and the number of downstream model sizes (3; T-2, T-5, and T-15), the total number of downstream models can be calculated as:\n\n* For Foundation models, we will have: 6 * 7 * 1 * 3 (#foundation models * #datasets * #seeds * #downstream_models) = 126\n* For other models on all datasets excluding VQA-v2: 9 * 6 * 3 * 3 (#models * #datasets excluding VQA-v2 * #seeds * #downstream_models) = 486\n* And for other models on VQA-v2:  3 * 1 * 3 * 3 (#models * #datasets * #seeds * #downstream_models) = 27\n\nAdditionally, for each dataset, we have one baseline with only the information about the questions, which will add 7 more downstream models to our study. Therefore, in total, we\u2019ve trained 646 downstream models.\n\n\n[1] Seitzer, Maximilian, et al. \"Bridging the gap to real-world object-centric learning.\" arXiv preprint arXiv:2209.14860 (2022).\n\n[2] Yoon, J., Wu, Y. F., Bae, H., & Ahn, S.. \u201cAn investigation into pre-training object-centric representations for reinforcement learning.\u201d arXiv preprint arXiv:2302.04419 (2023).\n\n\nWe are happy to answer any further questions you may have."
            }
        },
        {
            "comment": {
                "value": "Thank you for your review and your positive assessment of our work. We answer your questions and concerns below:\n\n### Weaknesses\n\n> Some findings align with existing intuitions. For example, it is not suprising that large foundation models perform comparably to specialized OC models, and that their combination yields better performance. While valuable, these conclusion is intuitive and somehow obvious to the maching learning field.\n\nThank you for your comment. We would like to highlight the following points: \n\n1. We appreciate the reviewer\u2019s observation that some findings align with existing intuitions. However, we emphasize that scientific progress relies on rigorous validation, even for results that might seem obvious. While it may seem intuitive that large foundation models perform comparably to specialized OC models and that their combination further improves the performance, such outcomes have not been systematically shown on downstream reasoning tasks. Additionally, not everyone in the field shares the same intuition\u2014some may expect the combination to perform worse and view the OC inductive bias as a potential bottleneck. By empirically testing these assumptions, we believe our work provides a strong basis for future research and offers valuable insights for the community.\n2. We would like to highlight that despite their improvement in recent years, OC models still lag behind other types of models and foundation models in several tasks such as unsupervised segmentation in real-world settings [1]. Furthermore, the comparison between OC representations and other types of representations is still partially underexplored. The closest work that addresses this is [2] which evaluates OC representations in specific RL tasks that are significantly simpler than our VQA tasks. Therefore, it\u2019s hard to predict how OC representations perform on harder tasks that require reasoning such as VQA without conducting a proper study to analyze it.\n3. We would like to clarify that our focus is not solely on performance, but also on other important aspects such as the explicitness of representations and computational requirements. While performance is a key consideration, it is not the only relevant metric, and its significance depends on the specific setting and objectives at hand. Measuring other factors alongside performance is crucial for gaining a more comprehensive understanding of model behavior and evaluating their practical applicability in different contexts.\n\n> The correlation between property/attribute accuracy and overall VQA performance may be due to the characteristics or preference of the selected 4 datasets, rather than a fundamental relationship. This correlation might not generalize well to relational or reasoning or logical questions, potentially make the conclusion less convincing.\n\nWe agree that there is a possibility that this correlation is due to the properties of the datasets. However, we would like to clarify that many generated questions for synthetic datasets do in fact involve relational reasoning. For example:\n* \u201cIs the shape of the small object that is on the right side of the medium cube the same as the large thing that is behind the medium block?\u201d\n* \u201cHow many other things are there of the same shape as the large cyan thing?\u201d\n\n> While there are many real-world QA datasets like VCR, GQA, DAQUAR which would capture more general data intrinsics of QA question, only one real-world dataset (VQA-v2) was used in this work. The three synthetic datasets is quantitatively extensive, but might exhibit construction biases or preferences and not capture the full complexity of real-world scenarios. Focusing on more diverse real-world datasets would strengthen the conclusions\n\nWe kindly refer you to our general response to all reviewers, where we explain that we are incorporating the GQA dataset as an additional real-world dataset into our study and will share the results as soon as they are ready."
            }
        },
        {
            "comment": {
                "value": "Thank you for your review of our work and the detailed feedback. We answer your questions and concerns below:\n\n### Weaknesses and Questions\n\n> While the study includes VQA-v2, it primarily relies on synthetic datasets. The evaluation would be more compelling if it included additional established real-world VQA datasets such as GQA or CRIC. Each image in the GQA and CRIC datasets is associated with a scene graph describing the image's objects, attributes, and relations. Moreover, questions in these datasets involve multiple reasoning skills, spatial understanding, and multi-step inference, making them ideal for analyzing object-centric models.\n\n> The majority of claims and findings are based on synthetic datasets, which feature simpler scenes with clear object-background separation. This raises concerns about the generalizability of the findings to more complex real-world scenarios. It would be great if the author can provide additional analysis on the challenging datasets such as GQA and CRIC\n\n> How would the proposed approach perform on more challenging real-world datasets like GQA or CRIC?\n\n> How would the findings about the benefits of object-centric representations translate to more challenging scenarios in real datasets?\n\nWe kindly refer you to our general response to all reviewers, where we explain that we are incorporating the GQA dataset as an additional real-world dataset into our study and will share the results as soon as they are ready.\n\n> The downstream architecture is limited to BERT-style transformer encoders. The authors should explore decoder-based transformer architectures, which have become increasingly popular and achieved more favorable results than BERT-style encoders on VQA tasks in recent research.\n\n> Why did the authors choose to focus on transformer encoder architectures for downstream models? Would incorporating decoder-based architectures potentially lead to different conclusions?\n\nIn our work, we frame VQA as a classification task and adopt an encoder-decoder architecture with a transformer encoder to process the input and a classification head applied to the [CLS] token. A transformer encoder is well-suited for this setup because its bidirectional attention mechanism enables it to integrate and contextualize information from both the vision and language representations into the [CLS] token, which is specifically designed to represent the aggregated information required for classification tasks. However, if we were to shift to a token generation approach for the VQA task, using a transformer decoder could indeed improve the performance. On the other hand, handling VQA as a classification task helps avoid the potential influence of language variability, open-ended outputs, and other confounding factors. Despite its simplicity, this approach still aligns with our goal and effectively captures what we aim to measure: the models\u2019 understanding of the visual scene and their relational reasoning abilities, rather than merely solving the VQA task. Moreover, it avoids the added complexity of language generation, ensuring that the results remain clearer and easier to interpret.\n\n> While the authors claim they evaluated 640 downstream models, the paper lacks sufficient detail and analysis regarding these experiments. They should provide comprehensive information about these models in the paper\n\n> Could the authors provide more detailed analysis of experiments with 640 downstream models?\n\nWe apologize for the confusion. This number corresponds to the total number of VQA downstream models we trained for the study. Given the number of datasets (4) and the number of different split sizes of Multi-dSprites (4; 40k, 80k, 160k, and 320k images. So 7 different datasets in total), number of models (15), number of seeds (1 for pre-trained models, 3 for other models), and the number of downstream model sizes (3; T-2, T-5, and T-15), the total number of downstream models can be calculated as:\n\n* For Foundation models, we will have: 6 * 7 * 1 * 3 (#foundation models * #datasets * #seeds * #downstream_models) = 126\n* For other models on all datasets excluding VQA-v2: 9 * 6 * 3 * 3 (#models * #datasets excluding VQA-v2 * #seeds * #downstream_models) = 486\n* And for other models on VQA-v2:  3 * 1 * 3 * 3 (#models * #datasets * #seeds * #downstream_models) = 27\n\nAdditionally, for each dataset, we have one baseline with only the information about the questions, which will add 7 more downstream models to our study. Therefore, in total, we\u2019ve trained 646 downstream models.\n\nRegarding the request for a \u201cmore detailed analysis of experiments,\u201d we\u2019d be happy to provide this and will address any specific aspects you\u2019d like clarified. As long as it does not involve new experiments with significant runtime, we should be able to include the results before the discussion deadline.\n\n\n\nWe are happy to answer any further questions you may have."
            }
        },
        {
            "title": {
                "value": "General Response to All Reviewers"
            },
            "comment": {
                "value": "We thank all reviewers for their helpful feedback. We are happy to see that the reviewers find our paper well-written (feeJ), with important insights for the community (7see, feeJ) supported by extensive experimentation (rPSW, 7see, HU9k, feeJ).\n\nCommon concerns were raised regarding the need for experiments on additional real-world datasets. We appreciate this valuable feedback and fully acknowledge its importance. To address this, we have started integrating the GQA dataset into our framework. Given the effort and computational resources needed, we expect to have the initial results ready a few days before the discussion deadline. We will post an update here and hope the reviewers will consider this in their assessment.\n\nIn the meantime, we will address your other concerns and questions in our responses to each review."
            }
        },
        {
            "summary": {
                "value": "This paper presents a comprehensive empirical study comparing object-centric (OC) representations with foundation models on Visual Question Answering tasks. Through extensive experiments involving 15 different upstream models, the authors demonstrate that combining object-centric bias with foundation models can achieve strong performance while reducing computational costs compared to using foundation models alone."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Extensive empirical evaluation across different upstream and downstream models and datasets\n- Experimental results thoroughly support and validate each insight presented"
            },
            "weaknesses": {
                "value": "- While the study includes VQA-v2, it primarily relies on synthetic datasets. The evaluation would be more compelling if it included additional established real-world VQA datasets such as GQA or CRIC. Each image in the GQA and CRIC datasets is associated with a scene graph describing the image's objects, attributes, and relations. Moreover, questions in these datasets involve multiple reasoning skills, spatial understanding, and multi-step inference, making them ideal for analyzing object-centric models.\n- The majority of claims and findings are based on synthetic datasets, which feature simpler scenes with clear object-background separation. This raises concerns about the generalizability of the findings to more complex real-world scenarios. It would be great if the author can provide additional analysis on the challenging datasets such as GQA and CRIC\n- The downstream architecture is limited to BERT-style transformer encoders. The authors should explore decoder-based transformer architectures, which have become increasingly popular and achieved more favorable results than BERT-style encoders on VQA tasks in recent research.\n- While the authors claim they evaluated 640 downstream models, the paper lacks sufficient detail and analysis regarding these experiments. They should provide comprehensive information about these models in the paper"
            },
            "questions": {
                "value": "- How would the proposed approach perform on more challenging real-world datasets like GQA or CRIC? \n- How would the findings about the benefits of object-centric representations translate to more challenging scenarios in real datasets?\n- Why did the authors choose to focus on transformer encoder architectures for downstream models? Would incorporating decoder-based architectures potentially lead to different conclusions?\n- Could the authors provide more detailed analysis of experiments with 640 downstream models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a comprehensive empirical study of Object-Centric (OC) representations in VQA task. The study evaluates 15 different upstream models across three synthetic datasets and one real-world dataset. The authors draw conclusions on the comparison foundation models and OC models, the relation of VQA performance and intermediate tasks or metrics, the training data efficiency and so on."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I appreciate the authors' great efforts (and also their GPU cluster's) of this empirical study. The paper shows empirical rigor through its extensive experimental design, with multiple trials across various models and datasets. The experiments brings credibility to the conclusions, while the conclusions themselves could provide valuable insights for the field."
            },
            "weaknesses": {
                "value": "- Some findings align with existing intuitions. For example, it is not suprising that large foundation models perform comparably to specialized OC models, and that their combination yields better performance. While valuable, these conclusion is intuitive and somehow obvious to the maching learning field.\n- The correlation between property/attribute accuracy and overall VQA performance may be due to the characteristics or preference of the selected 4 datasets, rather than a fundamental relationship. This correlation might not generalize well to relational or reasoning or logical questions, potentially make the conclusion less convincing.\n- While there are many real-world QA datasets like VCR, GQA, DAQUAR which would capture more general data intrinsics of QA question, only one real-world dataset (VQA-v2) was used in this work. The three synthetic datasets is quantitatively extensive, but might exhibit construction biases or preferences and not capture the full complexity of real-world scenarios. Focusing on more diverse real-world datasets would strengthen the conclusions"
            },
            "questions": {
                "value": "Minor: I wonder how is the \"640\" computed on Line 315? And the \"640 models\" is slightly misleading, as they are actually different are experimental configurations rather than distinct model architectures."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This is an experimental paper that examines how object-centric (OC) representations, which treat scenes as compositions of objects, compare with large pre-trained foundation models in Visual Question Answering (VQA) tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Using object features to represent images to solve VQA has become quite popular since the bottom-up attention (from Peter Anderson et al.). However, there have been many debates about whether the object-level feature makes it work or whether it is just because the image representation model used in BUTD was better trained (with more data). This paper tried to solve this problem by running many experiments."
            },
            "weaknesses": {
                "value": "1. The question that this paper wants to answer is: whether OC, fixed-region or global representation is better for VQA. To answer this question, the authors need to provide a model with the same architecture, model size and pre-training data. The only variant in the model is the feature representation way, i.e., OC, fixed-region or global. However, I didn't find this experiment in the paper. Maybe I missed it. Please point it out and highlight it in the rebuttal.\n\n2. Synthetic data such as CLEVR and CLEVRTex are not challenging enough since the objects can be easily detected (and even hard-coded). Many symbolic methods have solved these datasets with 100% accuracy.\n\n3. I am not sure how these takeaway messages could contribute to the community. Some conclusions are already known such as : ' Consistency of the Results Across Question Types.'"
            },
            "questions": {
                "value": "See weakness, I would like to see the responses to the above three weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores the effectiveness of object-centric (OC) representations in the Visual Question Answering (VQA) task and compares them with traditional large-scale pretrained foundation models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The experimental design is rigorous, considering the impact of multiple factors on the task.\n2. The writing is fluent, and the conclusions are clear.\n3. The finding that object-centric representations are effective in VQA tasks is insightful for the community."
            },
            "weaknesses": {
                "value": "1. As stated in the paper, the datasets would have included more real-world data.\n2. Some analysis of the application of the findings is lacking."
            },
            "questions": {
                "value": "1. What makes me curious is what the performance would be after fine-tuning the foundation models on the specific VQA tasks, since other models have been fitted on the data. I suppose this would make it more fair for the comparison of different models and may inspire more insights.\n2. Could you give some more insights on the application of the findings in the paper? I'm wondering if the conclusions would be applied in other domains out of VQA."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}