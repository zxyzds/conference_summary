{
    "id": "vQ1y086Kn2",
    "title": "UnrealCV Zoo: Enriching Photo-realistic Virtual Worlds for Embodied AI Agents",
    "abstract": "The embodied artificial intelligence agents should be capable of sensing, reasoning, planning, and acting in complex open worlds, which are unstructured, high-dynamic, and uncertain. To apply agents in the real world, the realism of the simulated worlds is important for training and evaluating the built agents. This paper introduces UnrealZoo, a rich collection of photo-realistic 3D environments that mimic the complexity and variability of the real world based on Unreal Engine. For embodied AI, we provide a diverse array of playable entities in the environments and a suite of tools, based on UnrealCV, for data collection, reinforcement learning, and evaluation. In the experiments, we benchmark the agent on visual navigation and tracking, two fundamental tasks for embodied vision agents, in complex open worlds. The results provide valuable insights into the strengths of enriching the diversity of the training environments and the challenges to current embodied vision agents in the open worlds, e.g., the latency in the closed-loop control to interact with the dynamic objects, reason the accordance of the spatial structure in the complex scenes.",
    "keywords": [
        "Virtual worlds; Embodied AI; Embodied Tracking and Navigation; Visual RL;"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "A collection of photo-realistic 3D environments for benchmarking embodied AI agents.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vQ1y086Kn2",
    "pdf_link": "https://openreview.net/pdf?id=vQ1y086Kn2",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces UnrealZoo, a collection of 100 3D environments based on top of the UnrealCV engine. This collection of environments is novel since it spans a variety of scales, agent types, has a rich navigation system and is multi agent."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Originality: The paper demonstrates originality by contributing large scale diverse environments that can't be found for training RL agents today. Performance of RL agents on such environments from different playable entities hasn't been studied in detail before and having such environments would allow for that.\nQuality: Inclusion of images, algorithmic benchmarks, and comparison tables make the paper comprehensive \nClarity: Paper is well written and clear. Details make it clear that the environment will be easy for the community to leverage for training and benchmarking.\nSignificance: With the integration of OpenAI Gym and ability to use this without expertise in Unreal, this work could allow for significant downstream RL benchmarking and explorations that could lead to interesting insights."
            },
            "weaknesses": {
                "value": "While the types of environments included in UnrealZoo are diverse, there is little diversity in the types of interactions and actions the agents can have in these environments. This limits how complicated these environments can be and how much novelty these environments will drive in terms of RL algorithms explored for learning and becoming experts. \n\nIt is a little hard to understand what the \"ceiling\" for each task is in terms of performance. How hard are the given tasks for current RL algorithms? How long does it take for them to learn and become experts? \n\nInclusion of results for only DowntownWest feels lacking. Would like to see what performance looks like for other environments too."
            },
            "questions": {
                "value": "1. How hard are the given tasks for current RL algorithms? With some limited compute, is it possible for RL algorithms to achieve the same performance as a human expert today? These environments can drive downstream RL innovation by just simply being impossible to become experts in today.\n\n2. How long does it take for an RL agent to become an expert in these environments? Another interesting problem to explore in RL is becoming an expert fast. If it takes unreasonably long to become an expert in an environment, then these environments become interesting to share with the community."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed UnrealZoo, a photorealistic and large-scale environment for embodied AI agents. In UnrealZoo, agents can perform much more complicated actions than just traditional navigation, such as jumping and climbing. Further, agents can control vehicles, humanoids, animals, etc, allowing experimentation with different embodiments.\n\nThe authors propose and instantiate various tasks in their proposed simulator. They evaluate both VLMs and RL-trained agents on a subset of their proposed tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The visuals from the proposed simulator look great.\n\nThere are a variety of task and environments. Environments also have a wide variety of scales.\n\nThe authors improved the performance of the rendering pipeline.\n\nThe environment supports multiple agents."
            },
            "weaknesses": {
                "value": "My biggest concern with this paper is that the proposed tasks and simulator lack a defined direction.\n\nThere has been a considerable amount of interest centered around indoor navigation with the goal of sim2real transfer. Indoor navigation has been the subject of focus due to real robots that can serve as a deployment target existing, albeit it far from perfect hardware. The reviewer is unaware of any existing hardware platforms that would be sensible deployment targets.\n\nThere is also considerable interest in environments to evaluate new reinforcement learning methods and algorithms. While there are multiple criteria for these environments, one key one is speed -- the environment itself must be very performant as the ability to quickly iterate on new ideas is key. UnrealZoo is unfortunately very slow by these standards.\n\nWhile I do find the proposed simulator, tasks, and environments interesting, I am concerned about the value of the contribution."
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces UnrealZoo, a collection of photorealistic environments based on the Unreal game engine. These environments serve as a platform for training and analyzing embodied agents in highly realistic virtual environments. Moreover, unlike previous works that limit to small scenarios (e.g., a kitchen or a room), UnrealZoo provides vast virtual environments with carefully designed assets that resemble the challenges that agents operating in the real world would face. UnrealZoo is based on UnrealCV, which authors have modified to suit the needs of embodied agent research, improving its usability and performance (FPS). Experiments demonstrate the usability of UnrealZoo for embodied agent research, showcasing applications such as visual navigation, social tracking, and more. Results highlight the importance of diversity in the training environments for offline RL methods, how these fail to generalize across embodiments, and show the limitations of VLM models in highly complex realistic environments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper introduces realistic, highly complex open-world scenarios that resemble real-life scenarios embodied agents would face in the real world. UnrealZoo fills the gap between open-world environments that are far from the complexity of real-life scenarios (e.g., MineDojo, NetHack Learning Environment, or Craftax) and realistic environments that are usually far from the vast scenarios typically encountered in real life (e.g., \nThreeDWorld or Habitat 3).\n\n- UnrealZoo provides 100 scenes of very different natures and many playable entities (humans, animals, cars, robots, etc.). Although realistic virtual environments exist for embodied AI research, these often focus on some specific domain (e.g., CARLA and autonomous driving). The variety of the UnrealZoo environments makes the proposed framework a very versatile tool for embodied AI research.\n\n- Although tools exist for creating environments in the Unreal game engine and libraries for computer vision (UnrealCV), the authors modify UnrealCV to suit the needs of embodied agent research. Moreover, they make some modifications to the agent-environment communication and the rendering pipeline improving the performance (frames per second) of the environments significantly.  Finally, the authors provide an easy-to-use but versatile API based on OpenAI Gym."
            },
            "weaknesses": {
                "value": "Although I think that the main contribution of this work (UnrealZoo) could be of great relevance to the embodied AI field, I have two major concerns that I strongly believe should be addressed before:\n\n**Concern 1:**  In lines 185-187 authors state that the environments are sourced (and paid) from the Unreal Engine Marketplace (now renamed as Fab, see https://www.fab.com/), and in lines 236-238 they mention: *\"[...] we will package the projects and release binaries for the community.\"*. However, the standard license of Fab states that (verbatim from section 5.a of https://www.fab.com/eula): *\"Under a Standard License, you may not Distribute Content on a standalone basis to third parties [...]\"*. Please **make sure** that distributing the binaries as mentioned in the paper is legal under the terms in which the assets were purchased. \n\nMoreover, if the binary distribution of the environments is legal, the fact that this has to be shipped in a binary package greatly limits the open-source nature of the project. For example, if a contributor or developer wants to modify existing UnrealZoo environments, they would be greatly limited by the binary format of the Unreal environment. I'm open to discussion and willing to read the responses of the authors in this regard. \n\n**Concern 2:**  I strongly believe that the overall presentation and writing quality of the paper should be improved. Examples:\n\n- Lines 123 to 142 discuss previous literature on virtual environments. This text uses the \"virtual environments\" term which is very generic, but only discusses environments based on realistic simulators. Please modify the text to explicitly refer to **only** realistic simulator-based environments, or include the vast literature on environments that don't simulate the real world (e.g., NetHack, MineDojo, MineRL, Craftax, Atari, VizDoom,...).   \n\n- Missing details in the experimentation (Section 4). The authors do not provide any detail (or reference) on the specific methods used for the experiments. For example, the online and offline RL methods are unknown, there is no mention of the NN architectures employed. Moreover, encourage the authors to include the training curves of the methods (at least in the appendix). Currently, there is no evidence of the level of convergence of the methods. Furthermore, there is no information on how the human benchmark was collected: how many humans have been used? Were the humans informed on the task to solve or only had access to reward values? Were the humans experienced in video games?   \n\n- In L402 authors define the \"Average Accumulated Reward\" metric. I believe the authors refer to the average episodic return typically employed in RL. If this is the case, I think that common terms are much preferable instead of introducing new terminology.\n\n- Section in the appendix should be listed with letters, not with numbers. For example, the first section of the appendix should not be \"7 Data\", but \"A Data\". I believe this could be fixed by including the \"\\appendix\" command in LaTeX. Moreover, the appendix sections include many of the minor issues from the main text (some are pointed out below). Please consider also proofreading the appendix sections. \n\n- The references section should be carefully checked and improved. These are some of the issues, but please check all the references:\n    + Some papers referenced as preprints have been published in conferences or journals. For example, the CARLA paper was published in CoRL 2017 but is listed as an arXiv preprint.\n    + Incorrect capitalization in many titles. Example: CARLA in Dosovitskiy et al. 2017 should be all uppercase. \n    + Some names have been abbreviated, for example: in Gaidon et al. 2016. The same reference is missing the full name of the conference. \n    + Gupta et al. 2017 and many other references have inconsistent use of capitalization.\n    +  Some references include the acronym of the conference and others don't.\n    + Inconsistent format of the proceedings name, for example: some include the year, some don't.\n    + Some references are included as @article when should be @inproceedings. Example: Yuan et al. 2020. \n\n**Minor issues:**\n\nI found many minor writing and presentation issues in the paper, in the following I list some of them. The issues are individually minor, but many minor minor issues add a major issue. Please consider an exhaustive revision of the full paper.  \n\n- When listing the contributions (last part of the intro) the numbers jump from 1) to 3), missing 2).\n- The caption of Table 1 is missing the final dot. Moreover, the employed icons should be explained in detail somewhere in the main text or the appendix.\n-  L197: \"[...]Project Website [...]\" should be lowercase.\n- L309: \"To be user-friendly [...]\"; L311: \"In this way, the beginner can easily use and customize [...]\", reformulate to maintain formalism. \n- L349: \"visual Navigation tasks [...]\" fix capitalization.\n- L350 and L351 have missing whitespace before and after parenthesis.\n- L368: \"In Table. 4.1\". Extra dot after Table and there's no Table 4.1, I believe the authors refer to Table 3.\n- Table 3 does not define what the numbers on the table represent. I believe these are EL and SR, but these should be explicitly mentioned.\n- L413: missing whitespace before parenthesis.\n- L414: extra dot inserted after \"Figure\".\n- L428: \"[...] are provided in the appendix.\",  please specify the appendix section.\n- L431: extra dot and whitespace inserted after \"Figure\". \n\nI want to emphasize that I believe that the contribution of the work is relevant to the field, thus, I'm willing to update my score if the authors address the mentioned concerns and issues."
            },
            "questions": {
                "value": "- **Q1:** The paper includes experiments on the cross-embodiment generalization capabilities of some offline RL, however, it does not motivate cross-embodiment generalization per se, which I find non-trivial. What is the motivation behind cross-embodiment? Why is cross-embodiment generalization an interesting capability for embodied agents? Is there any practical or real-life scenario where cross-embodiment generalization is relevant?  \n\n- **Q2:** What happens when the agent's implementation can not handle the control frequency specified by the environment? Are the actions repeated or no action is taken (something like no-op in some RL environments)? Is it possible to freeze the environment to wait for the action of the agent?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a photo-realistic simulation environment for embodied training and a scene dataset of 100 diverse scenes.\nThe simulator is based on UnrealCV develops interfaces for RL training, and supports multi-agent.\nThe simulator highlights its simulation speed and diversity of scenes and entities. Necessary experiments are conducted to show its usability."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The environment is photo-relaisitic. \n- Scene quality an is good. Entities are diverse. Simulation efficiency is improved.\n- There are a lot of experiments."
            },
            "weaknesses": {
                "value": "- There are only 100 scenes and they are not scalable. \n- There is no comparison with more recent related works like [1][2].\n- The motivation is not clear in introduction. What's the disadvantages of previous simulators? How does UNREALZOO settle them?\n- The interaction between agents and the world is not discussed, which is important for embodied training.\n- There are no language-modal input/communication among agents. The multi-agent feature is not highlighted.\n\n\n[1] Cheng, Zhili et al. \u201cLEGENT: Open Platform for Embodied Agents.\u201d ArXiv abs/2404.18243 (2024): n. pag.\n\n[2] Yang, J., Ding, R., Brown, E., Qi, X., & Xie, S. (2024). V-irl: Grounding virtual intelligence in real life. arXiv preprint arXiv:2402.03310."
            },
            "questions": {
                "value": "- About the choice of base engine, is UE the only engine that can render photo-realisitic images?\n- There is a lot of Agent body mentioned in table 1. Are they driven by pre-defined policy? What are their action spaces?\n- Cannot find Table 3.3 in line 308 and Table 4.1 in line 368. `GPT4-o `in table 5 should be `GPT-4o`.\n- Why GPT-4o performs so poorly? Do you do the error breakdown analysis? Can you use other information like position or third-person view to increase the success rate?\n- In social tracking experiment, as the training set grows, the growth of SR is intuitive. Can you conduct the experiment on same scale training (compared to the less diverse data) to give a fair evaluation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There may be copyright problem of using scenes in the market."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}