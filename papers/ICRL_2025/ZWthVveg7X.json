{
    "id": "ZWthVveg7X",
    "title": "Enhanced multi-task learning of imputation and prediction via feature relationship graph learning",
    "abstract": "Missing values present significant challenges in machine learning, often degrading predictive performance. Traditional and deep learning imputation methods often overlook the relationships between features and their connections to downstream tasks. To address these gaps, we propose PIG (multi-task learning of Prediction and Imputation via feature-relationship Graph learning), a model that integrates imputation and prediction by leveraging feature interdependencies. \nPIG utilizes a graph-based approach to capture intricate feature relationships, thereby enhancing the accuracy of both imputation and downstream tasks. Our strategic training process begins with pre-training for both tasks, ensuring the model learns effective representations. This is followed by fine-tuning the entire model to further optimize imputation and downstream tasks simultaneously.\nWe evaluated our method using nine benchmark datasets, three for regression and six for classification.\nOur method showed superior imputation and prediction performance across nine datasets, achieving an average rank of 1.33 for both imputation and regression tasks and 1.83 for imputation and 1.17 for classification tasks. Additionally, in sensitivity analysis with respect to missing rates, our method demonstrated its robustness, especially in predictive performance, compared to other methods that showed significant degradation.",
    "keywords": [
        "Missing values",
        "imputation",
        "feature selection",
        "graph nueral network",
        "multi-task learning"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ZWthVveg7X",
    "pdf_link": "https://openreview.net/pdf?id=ZWthVveg7X",
    "comments": [
        {
            "summary": {
                "value": "This paper addresses the limitations of existing missing value imputation methods, which have largely overlooked the relationship between features and downstream tasks, by proposing a novel approach called PIG. PIG integrates imputation and prediction tasks to improve overall performance.\n\nFirst, using only non-missing values, it models feature interdependencies through Graph Construction (i.e., Graph Learning Layer; GLL). This GLL is then trained by comparing it to prior relationships derived from a decision tree algorithm by predicting a column using other columns. Additionally, a prediction layer (PL) is trained, completing the pre-training stage.\n\nIn the next phase, inputs with missing values are initially imputed using a simple method (Soft-Impute). The model then fine-tunes the GLL, Graph layer with residual connections (GRIL), and the PL, with the PL tailored to each specific downstream task, such as classification or regression."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The proposed method effectively integrates imputation with downstream analysis, offering a unified framework that addresses both imputation and prediction tasks simultaneously.\n\n* It demonstrates broad applicability by handling both numerical and categorical variables and by supporting both regression and classification tasks.\n\n* Extensive experiments showcase the model's superior performance, underscoring its effectiveness across various scenarios."
            },
            "weaknesses": {
                "value": "* Limited technical novelty: Although new components are introduced, they are derived from general techniques, such as standard graph construction (GLL) and a basic graph layer with residual connections (GRIL).\n\n* The model has a relatively complex training structure due to the two-phase training, initial imputation, and prior feature relationship modeling, which likely results in higher overall complexity compared to existing models.\n\n* If feature interdependency is learned through the model, it should not only differ from prior relationships obtained via a Decision Tree but also capture meaningful relationships that the prior approach may have missed. It is essential to demonstrate the qualitative advantages of the model beyond quantitative results through additional experiments."
            },
            "questions": {
                "value": "* Could an analysis of the overall complexity of the training process be added?\n\n* Can the effectiveness of pre-training for GLL and PL be evaluated separately to determine which has a greater impact? In the ablation study, the effect of pre-training only appears in the downstream task. Is pre-training for GLL truly necessary?\n\n* Can you demonstrate experimentally that the model captures meaningful feature relationships beyond the performance improvement?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a model called PIG (Prediction and Imputation via feature-relationship Graph learning) aimed at improving both imputation and predictive tasks in datasets with missing values. Traditional imputation methods often ignore feature interdependencies, affecting downstream predictive accuracy. \nTo address this, PIG uses a graph-based multi-task learning approach that explicitly captures feature relationships. The model incorporates a Graph Learning Layer (GLL) to identify dependencies among features, which a Graph Residual Imputation Layer (GRIL) then uses to perform more accurate imputation by focusing on relevant features. Following this, the Prediction Layer (PL) leverages the imputed data for classification or regression tasks. Through a carefully designed training process involving pre-training and fine-tuning, PIG achieves improved accuracy and robustness, outperforming baseline models across various benchmarks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper considers feature interdependence in handling missing data and designs a multi-task loss function. It employs two distinct modules\u2014the Graph Learning Layer and the Graph Residual Imputation Layer\u2014to identify interdependence relationships and use these relationships for imputation."
            },
            "weaknesses": {
                "value": "The paper lacks novelty, as the algorithm design is highly similar to MIRACLE, with both methods employing a loss function for multi-task learning according to feature interdependence. The main difference lies in the addition of multi-task learning on graphs through GNN (GLL and GRIL), but this innovation does not stand out significantly. Furthermore, there is no theoretical explanation to support the advantages of the proposed method in learning interdependence."
            },
            "questions": {
                "value": "1.\tRegarding the lack of a full bar plot in the paper, could you provide a more detailed visualization?\n2.\tThe datasets shown in the main text are primarily low-dimensional. How does the model perform on higher-dimensional datasets? If possible, please provide results.\n3.\tHow does your approach handle different missing mechanisms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors assert that previous works did not take downstream tasks into account when imputing missing values and were vulnerable to irrelevant features, which could lead to overfitting.\nTo address this, they propose a multi-task learning approach called Prediction and Imputation via feature relationship Graph learning (PIG). This model leverages both imputation and downstream task losses to optimize performance, guided by both tasks.\nAdditionally, this model learns an adjacency matrix to capture relationships between features and incorporates this relational information for both imputation and the downstream task.\nThrough extensive experiments on both regression and classification datasets, the authors demonstrate the effectiveness of PIG."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed model achieves state-of-the-art performance with a simple and effective approach.\n- The authors propose a novel approach to learning relationships between features, utilizing prior knowledge derived from feature importance in a decision tree.\n- The authors conducted extensive experiments across various datasets and compared the model with multiple appropriate baseline methods to demonstrate the effectiveness of PIG.\n- The paper is written to be easily understood."
            },
            "weaknesses": {
                "value": "1) The analysis of individual components is insufficient.\n- To assess the contributions of each loss and examine the synergy between imputation and downstream task loss, the authors should conduct ablation studies for each loss.\n- In particular, a detailed analysis of the contribution of the prior adjacency is needed to determine whether the performance of PIG is due to this beneficial prior information or the effective relational learning capability of GLL. For a deeper understanding, the authors could compare performance when using only relationships derived from prior knowledge.\n\n2) Concerns remain regarding the fairness of the comparison.\n- I conjecture that constructing a complement matrix with other imputation tools and using it as a starting point could create an ensemble effect. Since other baseline methods could also leverage this complement matrix as a starting point, it should be incorporated into the baseline models for a fair comparison."
            },
            "questions": {
                "value": "In the experimental section (lines 344\u2013346), the authors state that 'MTL versions of existing models are implemented by modifying each method to learn imputation and prediction simultaneously as in PIG'. \nAre they implying a two-step training strategy where the model is initially trained on the downstream task without masking, focusing only on optimizing the embedded part and predictor, and then subsequently updating all components together?\nIf not, I suggest using the same MTL scheme to ensure a fair comparison between baseline methods.\n\nI will update my score if these concerns are all addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}