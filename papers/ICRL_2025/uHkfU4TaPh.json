{
    "id": "uHkfU4TaPh",
    "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs",
    "abstract": "Efficiently managing the KV cache in Large Language Models (LLMs) is a critical challenge for long-context processing tasks such as retrieval-augmented generation (RAG), long text summarization, and multi-document analysis. Extending the context length substantially increases the KV cache size, leading to excessive memory consumption. Existing KV cache compression methods enforce a fixed pattern, neglecting task-specific characteristics, which hampers the effective retention of essential information while discarding less important tokens. In this paper, we introduce a novel Task-Aware KV cache mechanism that dynamically adjusts the KV cache size across different layers based on the characteristics of the tasks. Our approach builds on the significant observation of distinct activation patterns across layers in various tasks, which highlights the need for adaptive strategies tailored to each task's unique demands. Based on this insight, we propose DynamicKV, a method that dynamically optimizes token retention by adjusting the number of tokens retained at each layer, adapting to the specific task. DynamicKV establishes global and per-layer maximum KV cache budgets, temporarily retaining the maximum budget for the current layer, and periodically updating the KV cache sizes of all preceding layers during inference. Our method demonstrates exceptional performance on the LongBench dataset, retaining only 1.7\\% of the KV cache while preserving 90\\%, 87\\%, 78\\%, and 83\\% of the original accuracy for LlaMA-3-8B-Instruct, Mistral-7B-Instruct-v0.2, Qwen2-7B-Instruct, and InternLM-2.5-7B-Chat-1M, respectively. When the retained KV cache size is increased to 6.9\\%, the performance becomes nearly indistinguishable from that without any KV cache compression. Notably, even under extreme compression (0.9\\%), DynamicKV surpasses state-of-the-art (SOTA) methods by 11\\% in the Needle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be released to the public.",
    "keywords": [
        "Large Language Model",
        "Efficient Inference",
        "Key-Value Cache Compression"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We introduce a task-aware adaptive KV cache compression method, which enables Large Language Models to compress KV cache extremely during inference while maintaining high performance.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=uHkfU4TaPh",
    "pdf_link": "https://openreview.net/pdf?id=uHkfU4TaPh",
    "comments": [
        {
            "summary": {
                "value": "This work argues dynamically allocating kv cache budget across different layers to can result in better efficiency-performance trade off in long-context handling."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "a. The argument of dynamic cache budget across different layers is sound and consistent with the findings of many recent literature.   \nb. Comprehensive model and baseline coverage for longbench evaluation."
            },
            "weaknesses": {
                "value": "a. Doesn't seem to support FlashAttention by the look of Eq 1.   \nb. No proper efficiency evaluation.   \nc. Dataset-wise, LongBench is too short to be utilized as the only long context evaluation. Please consider adding coverage of infinitybench and ruler, with a more long-context table model like llama 3.1/3.2.   \nd. I am interested in comparing DynamicKV with some newer head-based methods, such as Ada-KV and MInference.   \ne. The main argument of the DynamicKV is different tasks might prefer a different budget distribution for cache, but it looks like there is no significant distinction in Figure 2. Also, these four tasks are far from properly covering the diverse types of tasks a LLM will face. I'd like to see more variants. This request can be partially filled with (c) above, and some short context tasks can help, too.    \n\nI generally like this task-aware idea since works like Gear, InfLLM and many layer pruning works do seem to suggest different kv cache compression strategies can result in different performance across task types. However, I think the evaluation and practicality of this work can use a more comprehensive justification."
            },
            "questions": {
                "value": "a. What is the budget for Figure 4? It looks different to PyramidKV's needle results if it is 128.   \nb. Continuing on the diversity of tasks, I am not sure how DynamicKV will handle a change in task type during a multi-round conversation, or under batch inference where different requests might prefer different distribution patterns."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Efficient KV cache management is crucial for long-context tasks in LLMs, as extending context length increases memory demands. Current compression methods lack adaptability to specific tasks, leading to information loss. This paper presents *DynamicKV*, a task-aware mechanism that adjusts cache size per layer to optimize token retention based on task needs. Tested on the LongBench dataset, DynamicKV achieves high accuracy even with significant cache reduction."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The experiment is solid\n- The problem of finding more efficient architectures for transformers is relevant and not saturated"
            },
            "weaknesses": {
                "value": "- The result of Needle in a Haystack is only on one model, maybe more models are better\n- For the Needle in a Haystack task, it might be better to test longer with a model that can support a longer context window like InternLM-2.5-7B-Chat-1M or Llama-3-8B-Instruct-Gradient-1048k"
            },
            "questions": {
                "value": "- How's your performance on longer context length and on different models?\n- How's your performance on RULER?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper looks into the problem of accelerating LLM inference under long contexts. The key insights is that different tasks may need to allocate KV cache differently across layers. The paper then proposes a task-aware adaptive KV cache compression method, called DynamicKV. Evaluation on Longbench shows that the proposed method it is able to retain comparable accuracy with 6.9% tokens."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper tackles an important and timely problem.\n- Interesting observation that different tasks exhibit different KV cache patterns. \n- Promising accuracy results under high compression ratio."
            },
            "weaknesses": {
                "value": "- The technical novelty is limited. Dynamic layer-wise KV cache allocation has been explored before, such as https://arxiv.org/pdf/2406.13035 and https://arxiv.org/pdf/2405.14366. It would be better if the authors can discuss and compare the proposed method with existing ones. Also, many of the optimizations proposed by this work are from prior studies, e.g., pooling from SnapKV and layer-wise allocation from pyramidKV. \n\n- Significantly increased tuning cost. The method is not easy to use, it requires setting hyperparameters such as tw, ws, r-max per layer. For large LLMs that have close to hundreds of layers, this significantly increases the tuning overhead in addition to the fact that the method is task-specific, which means different tasks may need different tuning of those parameters. There are also no ablation studies on the sensitivity of the parameter choice.  \n\n- Missing comparison with related work on layer-wise adaptive KV. \n\n- No measured system efficiency improvements. As a LLM inference optimization study, there is no real latency, throughput, and memory results reported in the paper. And it is unclear how the proposed method works with system optimizations such as FlashAttention, which reduces the primary bottleneck in long context inference, e.g., the attention map."
            },
            "questions": {
                "value": "The key formula, equation 2, does not seem to have any task-specific element in it, e.g., how to decide which task should use which allocation policy. It seems the algorithm introduces adaptive KV cache but leaves the real hard problem, e.g., how to figure out the optimal strategy for each task to users or to tuning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents DynamicKV, a KV cache compression approach that dynamically adjusts the KV cache during the prefilling stage. It analyzes the distribution of the most important tokens across layers and task-aware distribution difference. It utilizes attention weights of recent tokens and all other tokens and updates the cache size of past layers every certain intervals. Experiments are conducted on LongBench and Needle-in-a-haystack across four LLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The authors analyze the token retention of different kinds of tasks across layers.\n- Compared to compression with fix pattern, the proposed method can achieve higher compression ratios."
            },
            "weaknesses": {
                "value": "- The paper claims a task-aware approach, but the observation regarding token retention trends across different tasks appears to be quite consistent.\n- The paper relies on scalar notation for its expressions, which does not accurately convey the potential vector or matrix operations intended.\n- While the prefilling stage does incorporate a dynamic update mechanism for the cache every few layers, the budget determined remains static throughout the decoding phase.\n- There is a concern about how the proposed allocation mechanism would work with batched inference. Different samples within the same batch may have varying attention patterns, which could lead to different budget requirements."
            },
            "questions": {
                "value": "- It is unclear where the boundary lies between task-aware and instance-aware adaptations. It is possible that different datasets within the same task category exhibit varying characteristics, which could account for the differing results observed in the Figure 2. Could you provide a token retention analysis for each dataset to clarify this distinction?\n- There seems to be a misunderstanding in the interpretation of $A_l$ in Equation 3 and Algorithm 1. It should be a four-dimensional tensor with size `(batch_size, num_heads, win_len, seq_len)`. When applying TopK, $A_l$ should first be summed across the third dimension.\n- Please provide a comparison of DynamicKV's performance against baselines in terms of efficiency metrics such as first token latency and memory consumption."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenge of managing the huge KV cache in LLMs for long-context tasks. Current KV cache compression methods use a fixed approach, often ignoring task-specific needs, which limits the retention of essential information. The authors introduce DynamicKV, a task-aware mechanism that adjusts KV cache sizes dynamically across layers based on task requirements. DynamicKV sets global and per-layer cache budgets, updating them periodically to optimize memory usage while retaining critical tokens."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The motivation is clear\n\nThe presentation is great. It is easy to read and follow\n\nThe empirical analysis is relatively comprehensive"
            },
            "weaknesses": {
                "value": "The main weakness of this paper is there is no discusson on the benefits of dynamic KV.\n\n- Did DynamicKV reduce the peak memory? Improve throughput? Reduce TTFT? Also, different budgets in different layers create load imbalance in pipeline parallelism, which is commonly used in LLM serving [1].\n\n- Does this method compatible with commonly used serving framework and efficient techniques? Like PagedAttention [2] and quantization [3]\n\n- What is the overhead of the Algo. 1 compared to the normal prefill phase?\n\n- Is this framework compatible with Flashattention [4]? In long context scenario, FA is a must-use. In FA, you cannot explicitly obtain the full  attention score, as it is fused in FA\n\nAlso, please clearly state your setting of Needle test. Needle test is very sensitive to the prompt given to the model.\n\n\n[1] Orca: A Distributed Serving System for Transformer-Based Generative Models\n\n[2] Efficient Memory Management for Large Language Model Serving with PagedAttention\n\n[3] Gear: an efficient kv cache compression recipe for near-lossless generative inference of llm\n\n[4] FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"
            },
            "questions": {
                "value": "N/A"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}