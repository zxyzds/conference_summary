{
    "id": "8Ds99sdp3U",
    "title": "A Semantic Data Augmentation driven Nested Adapter for Video Moment Retrieval",
    "abstract": "Existing transformer-based video-moment retrieval models achieve sub-optimal\nperformance when using the pretrain-finetuning learning paradigm \u2013 a pretrained\nmultimodal encoder is finetuned using the target training data. While current work\nhas explored different model architectures and training paradigms to explore this\nproblem, the problem of data dilemma has been under addressed. Specifically,\nthere exists high diversity of how semantic is captured in textual query and the\ntraining dataset only consist of limited moment-query pairs for the highly diverse moments. This work addresses this problem with a novel nested adaptor\nand a LLM-driven semantic data generation pipeline. First, a LLM-driven data\naugmentation generates queries that are semantically similar to the ground truth,\nwhich enrich the semantic boundary captured by textual query. We empirically\nanalyze the effectiveness of data augmentation, and proposed a simple yet effective quality measure to retain high quality samples. Second, we propose a novel\nnested adapter that utilises both augmented queries and human annotated queries\nfor model coarse-tuning and fine-tuning, respectively. By combining semantic\nperturbation with domain adaptation, our approach addresses the variability in\nvideo content while capturing nuanced features more effectively. Experimental\nresults on various baseline models show the efficacy of our proposed approach.",
    "keywords": [
        "Moment Retrieval",
        "Highlight Detection",
        "Adapter",
        "Data Augmentation"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=8Ds99sdp3U",
    "pdf_link": "https://openreview.net/pdf?id=8Ds99sdp3U",
    "comments": [
        {
            "summary": {
                "value": "The manuscript proposes a data augmentation process driven by a Large Language Model (LLM) to enrich the semantic boundaries of textual query capture by generating queries semantically similar to real queries. Experimental results on several baseline models show the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The manuscript proposes a simple quality metric for retaining high-quality data-enhanced samples.\n2. A new nested adapter is proposed, which utilizes augmented and manually annotated queries for coarse and fine-tuning the model, respectively. \n3. The proposed data augmentation approach is model-independent and can be applied to different architectures, enhancing its adaptability"
            },
            "weaknesses": {
                "value": "1. Explainability of Data Augmentation: while the manuscript proposes an LLM-driven approach to data enhancement, it may not adequately explain the selection criteria for the enhanced data and the specific impact on model performance. For example, it is too \nsimplistic to do research only on the number and proportion of generated queries. Second, for queries that may have semantic annotation errors, does this practice of generating approximate semantic sentences amplify the errors and degrade performance, and how robust is the model, although the model allows for the presence of a certain amount of noise. \n2. Figure content error: In Figure 1, the \u2018Text Encoder\u2019 and \u2018Image Encoder\u2019 of the CLIP feature extractor are incorrectly drawn. This visual error may mislead the reader and negatively affect the accuracy and professionalism of the article. \n3. Irregularity in the title of the figure: the title of Figure 1 repeats the phrase \u2018Figure 1: Figure 1\u2019, which is unnecessary and should be simplified to \u2018Figure 1\u2019. Figure 4 has no full stop."
            },
            "questions": {
                "value": "My question has been shown in the weaknesses above. In addition, are there any clear conclusions from Figures 3 and 4?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel approach to enhance video-moment retrieval models that traditionally struggle with the pretrain-finetuning paradigm due to the data dilemma. The LLM-driven data augmentation enriches the semantic boundary, while the nested adapter leverages both augmented and human-annotated queries for effective model training. The approach addresses variability in video content and captures nuanced features more effectively."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "A. The text of this article is clearly written and easy to understand.\nB. This paper provides a detailed description of the key benefits of using LLMs for query augmentation."
            },
            "weaknesses": {
                "value": "A. This paper mentions \"domain gap\" in Line 58, but the corresponding example does not adequately demonstrate this gap; a more detailed explanation should be provided.\nB. This paper discusses the differences in data format and task requirements for LLMs in MR in the related work section, but it fails to address the relationship between MLLMs and MR.\nC. In Line 211, this paper mentions the use of GPT-3.5-turbo and GPT-4-o mini, but it does not explain why these two LLMs were chosen, nor does it discuss more diverse evaluation methods to assess the effectiveness of query generation.\nD. The comparative methods in this paper are insufficient; additional baselines should be included for comparison.\nE. This paper does not provide information on the number of trainable parameters and the speed of operation for the proposed method, raising concerns about its feasibility in practical applications.\nF. There is a lack of visual results, such as visualizations of data augmentation and retrieval results.\nG. The meaning of the bolded data in Table 2 is unclear; for instance, \"65.6\" in \"CG DETR\" for R1@0.5 is bolded, but \"65.7\" is not (GPT-3.5-turbo, 8x Aug).\nH. This paper should provide an explanation for the values of R1@0.7 and mAP (None, None) in \"CG DETR\" in Table 2, particularly why the performance declined after data augmentation was applied."
            },
            "questions": {
                "value": "see the above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper targets the video moment retrieval (MR) task. In this paper, authors try to use the CLIP network to handle the task. LLMs are used for query augmentations, which can improve the query diversity. Two linear layers are used in the nested adapter module. T-SNE visualization and ablation study are designed to analyze the model performance. Some experiments based on some MR baselines are conducted."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The presented method is model-agnostic. It can be used in some baselines (e.g., Moment DETR, QD DETR, and CG DETR) to improve their performance.\n\nLLMs are utilized in the designed method to generate various queries for data augmentation. Moreover, a filter is adopted to select the generated queries. Different LLMs are used to evaluate the model performance.\n\nThe t-SNE visualization can help reader understand the feature distribution.\n\nAuthors provide some details about the model implementation in Section 4.2."
            },
            "weaknesses": {
                "value": "Figure 1 is wrong. The text encoder and the image encoder should swap positions. The video input contains multiple clips, not images. Why the image encoder can be used to obtain the clip features?\n\nWhy authors only consider the R@1 results and ignore the R@5 results?\n\nWhy not present the retrieval visualization results and the e\ufb03ciency results? They are very important for the video moment retrieval task.\n\nThe number of baseline methods are not enough. Only three methods (Moment DeTR (Lei et al., 2021), CG DETR (Moon et al., 2023a) and QD DETR (Moon et al., 2023b)) are used in the experiment section. Authors should compare more baseline (e.g., MH-DETR (Xu et al., 2023)).\n\nThe nested adapter is not novel since it only contains two linear layers. The linear layer is very common in many networks.\n\nThe caption of Figure 1 needs to be revised since there are two \u201cFigure 1:\u201d.\n\nIn Section 3.1, why the query corresponds to two different symbols in Line 189 and Line 191?\n\nFigures 2-4 are not clear (especially lines) and the texts are two small. Authors should polish them carefully.\n\nSome grammatical errors, such as \u201cWhile current work has explored different model architectures\u201d and \u201cthe training dataset only consist of \u2026\u201d in Abstract.\n\nSome punctuation marks are incorrectly used, such as \u2019Very Good\u2019 in Line 405 and \u201c(see Fig: 2)\u201d in Line 269."
            },
            "questions": {
                "value": "Please address the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a nested adaptor and a LLM-driven semantic data generation pipeline to improve video moment retrieval. The pipeline generates semantically similar queries to enrich query diversity, while the nested adapter uses both augmented and human-annotated queries for coarse-tuning and fine-tuning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. Using LLM-generated captions to enhance video moment retrieval is reasonable.\n2. Experiments demonstrate some effectiveness."
            },
            "weaknesses": {
                "value": "1. The writing and figures require improvement.\n2. The contribution is limited, with minimal improvements. Using LLMs to enhance textual semantics has been explored with more substantial gains (e.g. [1-2]). The nested adaptor is also trivial.\n\n   [1]  ChatVTG: Video Temporal Grounding via Chat with Video Dialogue Large Language Models.\n\n   [2]  Context-Enhanced Video Moment Retrieval with Large Language Models.\n\n3. The experiments are insufficient, lacking results on popular datasets like Charades-STA and ActivityNet Captions.\n4. Minor errors, such as \"Figure 1: Figure 1:\"."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}