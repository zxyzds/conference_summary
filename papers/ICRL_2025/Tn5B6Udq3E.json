{
    "id": "Tn5B6Udq3E",
    "title": "Language Models, Grade-School Math, and the Hidden Reasoning Process",
    "abstract": "Recent advances in language models have demonstrated their capability to solve mathematical reasoning problems, achieving near-perfect accuracy on grade-school level math benchmarks like GSM8K. In this paper, we formally study how language models solve these problems. We design a series of controlled experiments to address several fundamental questions: (1) Can language models truly develop reasoning skills, or do they simply memorize templates? (2) What is the model's hidden (mental) reasoning process? (3) Do models solve math questions using skills similar to or different from humans? (4) Do models trained on GSM8K-like datasets develop reasoning skills beyond those necessary for solving GSM8K problems? (5) What mental process causes models to make reasoning mistakes? (6) How large or deep must a model be to effectively solve GSM8K-level math questions?\n\nOur study uncovers many hidden mechanisms by which language models solve mathematical questions, providing insights that extend beyond current understandings of LLMs.",
    "keywords": [
        "linear probing",
        "language model",
        "grade math problems",
        "logic following",
        "reasoning"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "Probing reveals that LLMs develop some \"level-2\" reasoning skill beyond Humans on solving grade-school level math problems",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Tn5B6Udq3E",
    "pdf_link": "https://openreview.net/pdf?id=Tn5B6Udq3E",
    "comments": [
        {
            "summary": {
                "value": "This paper attempts to measure the reasoning ability of models by training on a synthetic dataset based on GSM8k. Among other things, they show that training from scratch on this synthetic dataset of grade school math has some signs of generalization to harder problems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Measuring reasoning via training an LLM from scratch is an interesting direction. There are lots of interesting results and experiments in the paper."
            },
            "weaknesses": {
                "value": "I think the writing can be improved somewhat. Additionally, I think there are some questions about whether the findings are broadly applicable given a single synthetic eval on GSM8k. In some sense, LLMs are useful because they generalize widely beyond a small set of very specific problems."
            },
            "questions": {
                "value": "Q: My big question is whether this is a proper use of generalization, especially with LLMs, which are useful primarily because they generalize to very different kinds of data. \"Generalization\" from a synthetic dataset to a similar synthetic dataset is not nearly as ideal as generalization to a very different kind of dataset."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper dives deep into how language models solve grade-school math problems. The authors first design an elaborated data generation pipeline to synthesize a large amount of grade-school math problems with a specific form of Chain-of-Thoughts. Then the authors design a probing method to analyse how the language models solve the problems, and acquire some interesting conclusions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* This paper carries out a substantial experiment (including large-scale data synthesis, fine-tuning, and probing) to systematically research the reasoning ability of LLMs in grade-school math problems.\n* To understand how the metal process of language models, the paper proposes a few **probing tasks** which align with human problem-solving strategies. Such probing tasks might be insightful to other works.\n* Multiple interesting conclusions are found in this paper."
            },
            "weaknesses": {
                "value": "* **Limitation of specific CoT form**: This paper trains the GPT-2 model using synthetic data with a specific form of CoT. The fune-tuning and probing experimental conclusions are based on such CoT. However, the CoT that is now used to train the LLMs on grade school math problems deviates considerably from this form. This limits the generalizability of the paper's conclusions.\n* **Lack of analysis on data quantity**: Although this paper synthesizes a large amount of data to train a language model, some studies have shown that more training data is not always better for language models. I expect to see a curve or table of [model performance - the quantity of training data]."
            },
            "questions": {
                "value": "1. **Lack of analysis on data quantity**: See the second point in Weaknesses.\n2. **About the backward thinking process**: In line 447-453 of this paper, the authors define the \u201cbackward thinking process\u201d as \u201cbecause I want to compute X, but X depends on Y and Y depends on Z, so let me compute Z first\u201d. Moreover, they conclude that this backward thinking process can be autonomously learned through language modeling with abundant data. **I can't understand how the author came to this conclusion**. I cannot understand how the authors arrive at this conclusion."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work conducts controlled experiments to probe several fundamental questions about math reasoning capability of language models, the experimental results show that language models can solve math problems like humans and conduct backward thinking process."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This work introduces a framework to generate a set of diverse GSM problems that focus on \u201clogical reasoning\u201d aspect.\n- This work finds that \u201cthe model has learned to plan ahead, identifying necessary\nparameters before starting to generate the solution\u201d."
            },
            "weaknesses": {
                "value": "- The small model size of GPT2-small (100M-200M) may effect the generalization of the experimental conclusion. It would be appreciated if conducted experiments on LLMs with billion parameters.\n- The conclusions drawn from experiments on synthetic data may not be applicable to real-world scenarios with large data volumes."
            },
            "questions": {
                "value": "Since IGSM only has four categorizations, which may result in poor diversity in the description of quantitative relationships. Could this affect the generalization of the experimental conclusions of this paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper investigates the capability of language models (the authors used the GPT-2 model for their experiments), to solve grade-school math problems to explore whether models genuinely develop reasoning skills beyond template memorization. They address several key questions: Can LMs generalize reasoning skills? Do they replicate human-like problem-solving methods? Are their errors predictable, and does model architecture (depth versus width) influence reasoning abilities?\n\nIt is important to control the dataset to explore these questions. For this, the authors generate a synthetic dataset with high template diversity, reducing contamination and ensuring complex, logical dependencies in problems. Results indicate that the model can indeed generalize reasoning processes and even learns dependencies and plans solutions. The paper explores the models internal states through probing to understand how the model is solving the problem. \n\nOverall, the paper is highly experimental with the goal to identify the factors that enable a model to develop reasoning skills through systematically controlled experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The biggest strength of the paper is the controlled experiments that the authors conducted. Training a GPT-2 model from scratch with their own synthetic data can give them full control over the experiments. Also, generating their own data can prevent contamination of the training test data, which might be one of the problems in the other benchmark data. \n\nThe authors also introduce the probing tasks to explore the internal states of the model, which gives some idea of the reasoning process of the model. In particular, the V-probing approach is interesting because it can tell whether pre-training or fine-tuning was responsible for the probing signal. \n\nFinally, the paper presents many interesting results, including the depth vs. width of the models, the generalizability of the model to unknown tasks of greater depth, models learning backward reasoning processes like humans, and so on. These results (if they can be scaled) may allow better design of language models in the future and may help to understand the role of synthetic data in learning a concept."
            },
            "weaknesses": {
                "value": "The weakness of the paper also lies in the main choice of experiments: the creation of the dataset and the choice of the model. \n- Regarding the dataset, the use of synthetic data may not fully capture the complexity and nuances of real-world mathematical problems. This limitation could affect the generalizability of the results to practical applications. It would be important to mix and match the training dataset with real-world data or test the methodology on some real-world data such as GSM8K.\n- Regarding the models, although it is difficult to test the methodology on multiple models, the model performance scales well with depth is kind of a known fact and with more layers, the performance improves creating a question that if scaling leads to memorization?\n\nThe use of math word problems is a good choice to control the experiments, but the question is if the results are limited to a structured reasoning problem like math word problems? This is important to know in order to understand the limitations of the approach. \n\nThe analysis of the model errors was omitted from the paper, saying that the authors are writing another paper because it is not good for the current paper. It would be good to see the error analysis. At least a summarized version needs to be presented. \n\nFinally, since the training of the model was done by the authors, it would be interesting to see in which training iterations the model learned certain skills. This can easily be discussed in the paper, but I found it missing. \n\n[MINOR] The use of words like AGI while experimenting with synthetic datasets and the GPT-2 model is not correct and can be avoided."
            },
            "questions": {
                "value": "It would be interesting to see some results on real-world datasets like GSM8K to see how much the models can generalize to real-world problems when trained on synthetic datasets.\n\nIs it possible to test the models on similar synthetic datasets like mathworld (https://arxiv.org/abs/2306.04347)?\n\nModel errors and how LLMs can fix them (Result 6) needs to be discussed in the paper and not left saying its there in another paper.\n\nThe results of the backward thinking process are missing from the paper and saying that the models learned it without any discussion is not correct. It needs to be added to the paper. \n\nFinally, it would be interesting to see an analysis of when a model learned a particular reasoning skill during training. And can it be learned in a continuous training fashion?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work focuses on evaluating the grade-school level mathematical reasoning of neural language models (GPT models).\nThe authors set out to explore several critical questions surrounding the actual abilities of LMs to reason mathematically.\nTo make clear and controllable experiments, the authors introduces a synthetic dataset iGSM.\nUsing iGSM to train and test GPT-2, the authors argue that the model can truly understand and reason through problems rather than just memorizing instances.\nThe examination of the causes behind reasoning mistakes made by LMs is another highlight, shedding light on the limitations and potential areas for improvement."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Overall, the exploration of the model's hidden reasoning process provides valuable insights into the black-box nature of neural language models.\n2. This work introduces a new synthetic dataset iGSM which could facilitate the training and evaluation of contemporary large language models.\n3. This work provides many interesting findings. It provides a valuable addition to better understanding the reasoning capabilities emerged in large language models."
            },
            "weaknesses": {
                "value": "My major concern is about the generalizability of the conclusions drawn in this work.\nIt is unclear whether the experimental findings on formal language reasoning can be generalized to natural language reasoning.\nThe formal language patterns in iGSM are too much limited and inflexible compared with natural language expressions, although it may contain a large number of templates if omitting the primitives in the expressions.\n\nFor instance, for the math operation *multiplication*, there are many different ways to express using natural language, e.g., \"there are three children in the house and each child eats two eggs for breakfast\".\nBut in iGSM, it seems that *multiplication* is always mapped from \"times\" in the input expression.\nI think for grade-school level mathematical reasoning, one major capability is to map the flexible natural expressions to calculations.\nTherefore, I think reasoning on iGSM could be much easier than reasoning on natural language questions."
            },
            "questions": {
                "value": "1. Can you provide some evidence to support the generalizability of your conclusions, even some of them?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}