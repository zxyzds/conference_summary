{
    "id": "ykt6I21YQZ",
    "title": "Ensemble Kalman Diffusion Guidance: A Derivative-free Method for Inverse Problems",
    "abstract": "When solving inverse problems, it is increasingly popular to use pre-trained diffusion models as plug-and-play priors. This framework can accommodate different forward models without re-training while preserving the generative capability of diffusion models.  Despite their success in many imaging inverse problems, most existing methods rely on privileged information such as derivative, pseudo-inverse, or full knowledge about the forward model. This reliance poses a substantial limitation that restricts their use in a wide range of problems where such information is unavailable, such as many scientific applications. To address this, we propose Ensemble Kalman Diffusion Guidance (EnKG) for diffusion models, a derivative-free approach that can solve inverse problems by only accessing forward model evaluations and a pre-trained diffusion model. We study the empirical effectiveness of our method across various inverse problems, including scientific settings such as inferring fluid flows and astronomical objects, which are highly non-linear inverse problems that often only permit black-box access to the forward model.",
    "keywords": [
        "inverse problem",
        "diffusion model",
        "derivative-free"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ykt6I21YQZ",
    "pdf_link": "https://openreview.net/pdf?id=ykt6I21YQZ",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a new approach to solve inverse problems using a derivative-free optimization method based on the Ensemble Kalman Filter. The core idea is to approximate the data-fidelity term gradient with a statistical\nlinearization from the ensemble Kalman methods. The method is applied to three  types of inverse problems: computational imaging problems, the Navier-Stokes equation, and the black-hole imaging problem. The method is compared other derivative-free baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Interesting approach to solve inverse problems.\n- Derivative-free approaches can be useful in many cases and have received much less attention than gradient-based methods."
            },
            "weaknesses": {
                "value": "- Positioning relative to the state-of-the-art is not clear, in particular with respect to the proposed framework, which seems to be a variant of the existing ones (see **Q2**).\n- The evaluation is not completely satisfactory (see **Q3, Q4**)."
            },
            "questions": {
                "value": "- **Q1:** Use of a `pretrained diffusion model`  is mentionned. I can see this type of models for images, but for complex data with varying domain of definition, like in many scientific applications, the diffusion model needs to be retrained for each type of data no? IT is not completely clear how reusable these models are outside computational imaging.\n\n- **Q2:** The prediction-correction scheme strongly relate to the usual Plug-and-Play methods classicaly used in inverse problem resolution. The proposed scheme is related to the Forward-Backward Splitting (FBS) method, which is typically used in the DiffPIR [B]. From what I understand, equation (9) in this paper corresponds to equation (13) in [A]. Could you elaborate on the main difference of between the proposed method and the DiffPIR framework? (except the derivation free aspect). Other missing references are [B], a survey on using diffusion models for inverse problems, and [C], which also consider diffusion prior for inverse problems, with gradient based method but applied to the Black-hole imaging problem. Adding comparison with their method would be very useful.\n\n- **Q3:** To make is possible to evaluate how good the results are compared to methods that are able to access the gradient of the forward operator, it is necessary to add a few methods that have access to the forward operator's gradients. Indeed, even though ODE solvers are not always natively differentiable, there are more and more works that consider making them differentiable, for instance using `jax` for Navier stokes using pseudo-spectral solver  [here](https://github.com/google/jax-cfd). The interesting question is: should we spend some time making them differentiable or do we not gain much by doing so. Therefor, quantifying how much is lost on simple cases as the ones presented here is necessary to make the case of derivative free methods. In particular, adding the results of the DPS method and the DiffPIR method would be very useful at least for the imaging task. Note that these models are both implemeted in the [`deepinv`](https://deepinv.github.io) library (see [here](https://deepinv.github.io/deepinv/deepinv.sampling.html)). Also, adding the DPS base line and PnP-DM form [C] for black hole imaging would better illustrate how much we loose by note considering the gradient of this differentiable operator. Adding them for Navier-Stokes would also be very interesting but probably more challenging.\n\n- **Q4**: The value of $J$ and $Q$ in the experiments are not reported. Could the author provide them? From equation (16), I understand that we need to compute the forward operator $J$ or $Q$ times at each iteration. From the NAvier stokes experiment, assuming that the procedure is run for 1k steps, I guess $J = 295$ and $Q=2048$? How were these value chosen? What are their impact on the results? Also, would having access to the gradient mean going roughly 100 times faster than EnKG? (The computational cost of computing the gradient through autodiff is approximately x2/3 times the cost of evaluating the forward operator). Note that the metrics chosen (# of evaluation of Fwd/DM, Seq) are not clear. Better definition of what they represent mean would be useful. Adding the total runtime of the method would help a lot to assess the computational cost.\n\n- **Q5:** In the black-hole experiment, how many simulation are used to train the diffusion model?\n\n### Minor comments, nitpicks and typos\n\n- Missing ref: \n- l.066: \"One more challenging\" -> \"On\"\n- l.069: \"More computationally efficient\" -> than what?\n- l.078: \"often Gaussian\" -> They are almost never Gaussian, but they are modeled as such and this gives reasonable results.\n- l.141: \"and Nelder-Mead simplex methods\" -> Extra `,`.\n- l.198: The drop of the subscript $x$ for the gradient is confusing\n- Eq (7) -> The notation $\\Delta x_i$ is not defined. As it is not used anywhere else, I would recomment using $\\|x_{i+1} - x_i\\|$ instead.\n- Eq (12) -> $x_i'$ should have a super script $(j)$. It is also not completely consistent for $x_{i+1}$.\n- l.302: \"instead\"\n- l.316: `our approach outperforms the standard strong DPS baseline` -> The DPS result are not present in the table, so I think they are missing?\n- l.335: What is the `EDM` framework?\n\n\n- l.847: The change from  \"l\" to \"j\" should be made explicit as it is not immediately clear and can appear as a typo. \n\n### References\n\n[A] : Daras, Giannis, et al. [A survey on diffusion models for inverse problems.](https://arxiv.org/pdf/2410.00083) arXiv preprint, 2024.  \n[B] : Zhu, Yuanzhi, et al. [Denoising diffusion models for plug-and-play image restoration.](https://arxiv.org/pdf/2305.08995) Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.  \n[C] : Wu, Zihui, et al. [Principled Probabilistic Imaging using Diffusion Models as Plug-and-Play Priors.](https://arxiv.org/pdf/2405.18782) arXiv preprint, 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents Ensemble Kalman Diffusion Guidance (EnKG), a novel derivative-free method for solving inverse problems using pre-trained diffusion models. Traditional approaches often require detailed knowledge of the forward model, such as derivatives, which limits their applicability. EnKG overcomes this by relying solely on black-box evaluations of the forward model and a pre-trained diffusion model.\nKey contributions are twofold: 1) EnKG operates solely with black-box access to forward model evaluations and a pre-trained diffusion model, making it particularly useful in scenarios where derivative information is inaccessible. 2) The authors introduce a prediction-correction (PC) framework that utilizes the empirical covariance matrix of ensemble particles during the correction step. This innovation allows EnKG to effectively bypass reliance on gradients, enhancing its applicability in non-linear inverse problems.\nThe paper demonstrates the effectiveness of EnKG across various inverse problems, including applications in fluid dynamics. These examples highlight the method's capability to handle complex, non-linear scenarios that are common in scientific research.\nIn summary, this work expands the toolkit for addressing inverse problems in machine learning by introducing a flexible and robust approach that maintains the generative power of diffusion models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1)\tEnKG operates without gradients, needing only black-box access to forward models, making it highly applicable to complex inverse problems with unknown or undefined derivatives. The proposed PC framework generalizes existing methods, enabling adaptability across various inverse problems without retraining.\n2)\tThe current work demonstrates strong performance, notably in complex tasks like the Navier-Stokes equation, outperforming gradient-based solutions\n3)\tProvides deeper understanding and new interpretations of diffusion-based approaches, contributing to the field of inverse problem-solving."
            },
            "weaknesses": {
                "value": "1) The method may face challenges when scaling to very large models or high-dimensional data, as ensemble-based approaches can become computationally expensive. A further insight along this line would be useful. Also, it relies on pre-trained diffusion models, which might limit effectiveness if high-quality models are not available for certain tasks.\n2) The empirical validation focuses on specific problem sets; broader testing across diverse applications would strengthen the generalizability claim. \n3) A further analysis on the algorithm complexity would be beneficial as the combination of the prediction and correction steps might introduce additional computational and implementation complexity due to ensemble covariance estimation."
            },
            "questions": {
                "value": "1) How does EnKG perform when applied to larger-scale problems or high-dimensional data? Are there specific limitations to its computational efficiency?\n2) Is there any dependence on the performance and the pertrained model? How sensitive is the method to the quality and type of pre-trained diffusion model used? What are the implications if a suitable model is not available?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Diffusion models have been used to address inverse problems, with numerous diffusion-based solvers that avoid retraining existing diffusion models. These approaches typically rely on pseudo-inverses or derivatives tied to the forward model. This paper introduces a novel diffusion-based inverse solver designed for cases where the forward model is unknown.\n\nThe authors propose Ensemble Kalman Diffusion Guidance (EnKG), a derivative-free method that utilizes only forward model evaluations along with a pre-trained diffusion model.\nIn the proposed method, guidance term is computed as follows:\n1. Particles are initialized to compute covariance in following steps.\n2. During the diffusion trajectory, the particles are pushed by ODE solver.\n3. Then, synthesized samples are applied to the forward model and compute covariance of them.\n4. Diffusion trajectory is updated by the formula given the EnKG.\nThe proposed method replace the derivative of the forward model by computation of covariance and ODE solving.\n\nThe empirical results demonstrate the effectiveness of EnKG across diverse inverse problems, including cases where the measurement operator is treated as a black box. Specifically, the method is applied to image inversion problems with explicit forward models, Navier-Stokes inverse problems where the forward model is computed by solving PDEs, and black hole imaging, where the forward model is a black box."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1.\tNovel Approach: The paper introduces statistical linearization within ensemble Kalman methods to diffusion-based inverse problems, a novel concept in this context.\n2.\tInnovative Guidance Term Formulation: The authors present a unique formulation for the guidance term, with a clever trick that replaces the derivative of the forward model with covariance from forward evaluations.\n3.\tComprehensive Validation: The effectiveness of EnKG is demonstrated across three different scenarios: (1) cases where the forward model is known and differentiable, (2) cases where the forward model is known but differentiating it is impractical (e.g., PDE-based models), and (3) cases where the forward model is a black box, with observations as the only available information."
            },
            "weaknesses": {
                "value": "1. Limited Discussion of Related Work: The paper lacks depth in interpreting and explaining related works.\n- The motivation behind the weighting matrix $w_i C_{xx}^{(i)}$ is unclear. Could you provide further explanation on the intuition and reasoning behind the choice of weighting matrix?\n- Although the Kalman method is a core component, the paper does not provide a thorough explanation of its role and mechanics in this context. Specifically, which parts of the method are directly applied from existing literature and which represent novel contributions of this paper? For example, the introduction of weighting matrix, the derivation using local linearity of the operator in the proof, and the convergence claim. Related to above questions, please clarify the intuition and motivation provided by the literature versus that introduced by the authors.\n2. Overstated Contribution: The claimed contributions seem somewhat overstated. The concept of Predictor Corrector interpretation in guidance-based methods is not entirely new."
            },
            "questions": {
                "value": "- Line 316: Is the DPS baseline included in the table?\n- Black-Hole Imaging Problem: In the black-hole imaging problem, how is $G(\\phi(x_i, t_i))$ computed if $G$ is unknown and $\\phi(x_i, t_i)$ differs from the observed data? Could you provide a step-by-step explanation of how the black-box forward model is handled in the case? Additionally, please clarify if any assumptions are made about $G$ or generated samples in this scenario.\n- Proof of Lemma 1: The proof shows a monotonic decrease in $\\text{tr}(C_{xx}^{(i)})$. Why does this quantity converge to zero? From another perspective, a vanishing covariance implies the trajectories converge to a single point. Does this implication contradict the ill-posed nature of inverse problems, which typically have many possible solutions?\n\nPossible Errata\n- Lines 225 and 284: Remove  \\Gamma .\n- Line 731: Remove the extraneous \u2018(\u2018.\n- Line 847: Replace with $\\approx$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors address inverse problems using diffusion models as priors within a restrictive setup where the forward model is accessible only point-wise.\nThey employ a Predictor-Correct framework, where in the prediction stage, samples are drawn from the prior using the ODE describing the diffusion model.\nIn the correction stage, a MAP problem involving the forward model at the current diffusion step is solved through gradient-based updates.\nUnlike previous approaches, the authors approximate the intractable term in this MAP formulation by evaluating it on samples generated via the ODE.\nSince only point-wise access to the forward model is available, the authors estimate the gradient through statistical linearization and ensemble Kalman methods.\nThis approach maintains a set of particles and uses them, along with their centroids, to approximate the gradient.\nThe authors validate their algorithm on three different inverse problems."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Introduction of an algorithm that solves inverse problems with diffusion models prior that only requires point-wise access to the forward model"
            },
            "weaknesses": {
                "value": "**Methodological concerns**\n\n- The authors\u2019 motivation for their \"Derivative-free correction step\" in Lines 246\u2013263. The matrix $C_{xx}$ appears without adequate justification, and in Equation (12), the authors invert $C_{xx}$ even though it is a singular matrix, as the number of samples used to compute it is less than the dimensionality of the problem. Consequently, $C_{xx}$ cannot be treated as a preconditioning matrix in this context.\n- In the appendix, the authors base their proofs on Equation (20), which is introduced without sufficient explanation. This equation appears to correspond to the ensemble update, which assumes an estimation of the gradient, the very objective of the lemmas and propositions that follow. This circular reasoning raises concerns about the validity of the proofs.\n\n\n**Technical concerns**\n\n- In Lines 125\u2013127 (paragraph following Equation (4)), the authors suggest that the guidance term depends on the noise scheduler $\\dot{\\sigma} \\sigma$. However, this dependence result from their specific formulation. As verification, the authors can review Equation (5) in [1] or write DPS's algorithm in terms of the score. The guidance term is not scaled by the noise scheduler.\n- The statement in Lines 194\u2013195 is misplaced. Specifically, $\\log \\hat{p}(y | x_{i+1})$ is a composition of the simulated ODE and the forward model, making it highly non-convex. Therefore, the hypothesis of convexity is unrealistic. Besides, this term varies at each diffusion step, as it is composed with the ODE at different time steps, which diverges from the requirements outlined in [2], Chapter 4. Hence, the iterative updates may not converge to a true MAP estimate within this setups.\n\n\n**Errors and clarifications**\n\n- Equations (12)\u2013(14) lack clarity. The variable $x_{i+1}'$ is undefined, and while the argmin is specified with respect to $x_{i+1}^{(j)}$, this variable does not appear in the equations.\n- In Lines 897\u2013903, the gradients of $p$ are missing a logarithmic term.\n- The first part of Assumption (3) regarding $C_{xx}$ is redundant. Since $C_{xx}$ is a positive semi-definite matrix, its trace is non-negative, being the sum of positive eigenvalues. Therefore, if the trace of $C_{xx}$ is zero, $C_{xx}$ must be the zero matrix.\n- In Lines 316\u2013317, \"DPG\" should replace \"DPS,\" as DPS is not included in the experiments.\n- The term \"guidance\" is repeated twice in Line 52.\n\n\n---\n\n.. [1] Chung, Hyungjin, et al. \"Diffusion posterior sampling for general noisy inverse problems.\" arXiv preprint arXiv:2209.14687 (2022).\n\n.. [2] Parikh, Neal, and Stephen Boyd. \"Proximal algorithms.\" Foundations and trends\u00ae in Optimization 1.3 (2014): 127-239."
            },
            "questions": {
                "value": "- Plot (b) in Figure 3 is difficult to interpret. Could you clarify what the values 0.2, 0.4, ..., 0.8 represent? Additionally, what do \"Seq # DM\" and \"Seq # DM grad\" refer to in this context?\n- Plot (c) reports the runtime of the proposed algorithm, with an average of  approximately 140 minutes. This is a considerable computational cost (about 2 hours) for solving one inverse problem.\n    * How does this runtime compare to other algorithms (aside from EKI) ?\n    * Can you comment on the practical applicability of the method given this runtime?\n    * Considering the high computational cost, how does the algorithm perform relative to methods that fine-tune or train smaller network components for the guidance term, as seen in [1, 2, 3, 4]?\n\n\n---\n.. [1] Black, Kevin, et al. \"Training diffusion models with reinforcement learning.\" arXiv preprint arXiv:2305.13301 (2023).\n\n.. [2] Uehara, M., Zhao, Y., Black, K., Hajiramezanali, E., Scalia, G., Diamant, N. L., ... & Levine, S. (2024). Fine-tuning of continuous-time diffusion models as entropy-regularized control. arXiv preprint arXiv:2402.15194.\n\n.. [3] Fan, Ying, et al. \"Reinforcement learning for fine-tuning text-to-image diffusion models.\" Advances in Neural Information Processing Systems 36 (2024).\n\n.. [4] Denker, Alexander, et al. \"DEFT: Efficient Finetuning of Conditional Diffusion Models by Learning the Generalised $ h $-transform.\" arXiv preprint arXiv:2406.01781 (2024)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}