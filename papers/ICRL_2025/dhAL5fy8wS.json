{
    "id": "dhAL5fy8wS",
    "title": "Data Selection via Optimal Control for Language Models",
    "abstract": "This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. \nWe formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics.\nBased on these theoretical results, we introduce PMP-based Data Selection (PDS), a framework that approximates optimal data selection by solving the PMP conditions. \nIn our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on a wide range of downstream tasks across various model sizes.\nMoreover, the benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws.\nPDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which mitigates the quick exhaustion of available web-crawled corpora. We will open-source our code, models, and data.",
    "keywords": [
        "Pre-training Language Models",
        "Data Selection",
        "Optimal Control"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "This paper introduces a framework to select high-quality pre-training data via optimal control.",
    "creation_date": "2024-09-20",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=dhAL5fy8wS",
    "pdf_link": "https://openreview.net/pdf?id=dhAL5fy8wS",
    "comments": [
        {
            "comment": {
                "value": "Thank you for your excellent work.\n\nI have a question regarding Algorithm 1. It appears that the vanilla trajectory $\\theta_{t+1}=\\theta_t - \\eta \\nabla L(\\theta_t, \\gamma)$ is treated as the solution to the motion equation defined in Eq.(4). While, if the outer loop is executed only once, and $\\gamma$ is initialized as $[\\dots, \\frac{1}{\\vert \\mathcal{D}\\vert}, \\dots]$, Would this approximation result in a significant gap compared to the optimal trajectory?\n\nAdditionally, the results are quite interesting: in Table 1, the conventional method outperforms all baselines except the proposed method. it seems that modifications of these baselines compared to the conventional one do not lead to improved performance?"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a new data selection methodology inspired by control theory. The authors design a control problem that corresponds to training dynamics of a data-reweighted objective where the control action is the reweighting, the dynamics are gradient descent or Adam dynamics on the parameters, and the objective is minimal training error of the trajectory induced by such dynamics.  This is similar to some works in the learned optimizer literature but the optimizer parameters being optimized correspond to data selection.  \n\nOn a small proxy LM, the algorithm applies a fixed-point iteration (which is shown to be projected gradient descent) in order to induce Pontryagin's Maximum Principle for this control problem and to produce a dataset of quality scores (example weights).  A model is then learned to label examples, which can be used in a data filtering process for a much larger training run.  The authors evaluate this methodology on 160M to 1.7B parameter models with some initial promising results in comparison to baseline data selection approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "As far as i can tell, this approach is novel, in that similar methods have not been applied to the data selection setting. This is a theoretically motivated approach that attempts to optimize the training trajectory directly.  The idea to learn a labeler to translate the ideas from leaned optimizers into a data filtering mechanism is very clever.\n\nThe paper is very well written in my opinion--the general algorithm pipeline is quite complicated, but the main technical/algorithmic ideas were well motivated and clear.\n\nThe algorithm seems to perform well against baselines, although the compute requirements are likely higher (though maybe not prohibitively so as can be seen in Table 4)."
            },
            "weaknesses": {
                "value": "I am very skeptical of the scaling laws presented in this work.  There does not appear to be enough data to fit such a complicated model, and I do not think these results should be included nor trusted.\n\nWhile the empirics seem to be better than baselines, there seems to be fairly marginal improvements across the board and without very careful hyperparameter sweeps on the downstream optimizer, there is reason to be skeptical of the empirics."
            },
            "questions": {
                "value": "Seems A single step of the inner loop gets most of the gains, why do you think this is?  Given this is much cheaper is this a more feasible algorithm.\n\nDoes the proxy model and the scorer models complexity need to scale as we scale up the final pretrained model we want to use this with?\n\nLine 883, what is B and what is beta? \n\nTypo:\nLine 68: \"(e.g., 125M paramaters)\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a data selection method (called PDS) for language model pretraining based on optimal control principles. Specifically, the PDS method aims to compute a data score for re-weighting data, such that the trained model (via a fixed procedure) achieves the best downstream test error. In practice, this is done using small-scale proxies. Additionally, a model is fitted to predict scores for general data. One important benefit of the proposed method is its offline nature, enabling an effortless plug-in to existing language model training pipelines. The effectiveness of the proposed method is verified across multiple benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well-written, and the experimental setup is clearly explained.\n- The PDS method is well-motivated by control theory and demonstrated to be effective in practice.\n- The offline nature of the proposed method makes it very easy to use in existing language model training pipeline.  \n- Multiple thoughtful ideas are proposed to accelerate the implementation of PDS."
            },
            "weaknesses": {
                "value": "I didn\u2019t observe significant weaknesses in the work. If I had to mention one, it would be the lack of large-scale verification of the method's effectiveness. This limits the strength of some claims (e.g., fitting scaling laws and extrapolating results). However, this is understandable given the substantial computational resources required for such large-scale experiments.\n\nOverall, I think the paper demonstrates the promise of the proposed method, and I am inclined to support its acceptance."
            },
            "questions": {
                "value": "Theorem 2.1 seems related to the KKT conditions of the optimization problem in equation (3). Could you briefly discuss the connection between Theorem 2.1 and the KKT conditions? Of course, this is not a weakness of the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper suggests a method for data selection based on applying optimal control theory to compute the importance of individual data points. Crucially, this importance is computed in the context of end-to-end training, to optimize the AUC of downstream performance - this is in contrast to previous works that don\u2019t look at training trajectory. Several optimizations are used to approximate the PMP solution. Moreover, the data selection is done offline so there is just a constant cost."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "Extensive ablations (optimizing Algorithm 1, choice of downstream task) are performed to support the empirical claims and many of the choices made.\nThe tackled problem is very relevant and the approach is innovative, using optimal control to account for the complete training trajectory.\nThe empirical results are very promising on the small-to-medium scale experiments that they are run on. A scaling law is also computed to try to support generalization to larger models."
            },
            "weaknesses": {
                "value": "It would be good if Appendix G3, choice of $J(\\theta)$ was covered in the main body (regarding the influence of the downstream data on performance) since that is an important driver of the performance.\nBetter explain Figure 1: in (b), do you have the same amount of data being used by your method and Redpajama\u2019s to train? Do all points correspond to the same model size in (a)? If not, how do you vary these?\nIt would be useful to mention at line 89 that J is not computed on the same downstream tasks that you measure one (as you mentioned at line 307) since this distinction is crucial and not made clear until line 307.\nIt seems like line 100, defining $\\gamma$\u2019s makes an implicit assumption about points having a notion of independent importance, whereas one could envision cases where a point\u2019s importance is contingent on other points being in the dataset. It would be useful to add a discussion on this - as stated, it looks like this is a fact, not a choice.\nIt would be useful to add a discussion on why it makes sense to need Gumbel sampling - after all, one could expect $\\gamma$\u2019s to already have accounted for the need of diversity."
            },
            "questions": {
                "value": "Why did you decide at line 259 to use the average of the output hidden states rather than the last one? Intuitively this gives more weight to how the datapoint starts (since the first activations at the first few positions affect both the hidden states at these positions as well as later on).\nIs it computationally feasible to constrain the $\\ell_0$-norm of $\\gamma$? If so, do you expect to get the same data points as you would if you took the biggest ones by the current $\\gamma$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes a novel method of data selection for training machine learning models, or LLMs in particular.\nIt starts by theoretically formulating data selection as an optimal control problem, \nwhere the weights of training samples are optimized in order to minimize the total area under the downstream loss curve during training with gradient descent on the weighted data.\nBased on this, a practical and efficient implementation is developed: \nfirst solve the optimal control problem on a small proxy dataset (approximately, via bi-level optimization),\nand then train a data scorer (by fitting the obtained scores from the first step),\nwhich can then be applied for training any LLM with other datasets.\nExperiments show improvements over prior data selection algorithms,\nin terms of test losses and scores on some commonly used benchmarks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The formulation of data selection as optimal control, and the theoretical connection between two seemingly unrelated fields, are interesting.\n\n- The presentation is good overall. Ideas are clearly explained."
            },
            "weaknesses": {
                "value": "- According to the theory of this work, in particular Eq. (6), the optimal value of $\\gamma$ (the weights for training samples) is an one-hot vector, \nwhich leads to the surprising conclusion that training an LLM with a single sample is theoretically optimal.\nThe authors also mention this in Line 235. \nMy major concern is that this seems to reveal a fundamental caveat of the proposed methodology.\nAlso, with many approximations in deriving the practical implementation (Section 2.3), \nit becomes less and less clear how much the empirical advantages of the resulting algorithm are actually relevant to the theory in Sections 2.1 and 2.2.\n\n\n- Using the \"scaling law\" for extrapolating empirical results with small-scale models to 175B or 405B models (Table 3) doesn't feel like science. \nIt is not obvious why scaling laws for conventionally trained LLMs should be applicable to PDS-trained LLMs.\nIn addition, the goodness of fit for calculating the scaling law's coefficients is not reported, which further makes the extrapolation less convincing.\n\n\n- Regarding the empirical results, the proposed method is compared with prior data selection algorithms only in Tables 1 and 2, for 160M and 470M models that are relatively small-scale.\nMoreover, many scores in these results are close to random choice (e.g., 25% accuracy for MMLU in Table 2), \nhence it is not clear what the minor differences between the scores of different methods really imply."
            },
            "questions": {
                "value": "- Two questions about the theory:\n(a) Could you provide some intuition about why theoretically a single training sample is optimal, according to Eq. (6)? \nWhat's missing in this theoretical analysis?\n(b) In Eq. (22) where the method of Lagrange multiplier is applied, it seems that it misses a $\\max_{\\mu}$ term. Is this correct? If so, does the remaining analysis still hold, or need corrections as well?\nI'm willing to raise my rating if these two questions can be resolved.\n\n\n- Line 883, \"B\" and \"\\beta\" seems like typos; should be \"C\" and \"c\" respectively?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a formulation of turning the data selection to the AUC optimization problem. More importantly, this paper derives the optimal solution for the data selection problem and proposes an efficient implementation of the proposed method. The proposed methods are empirically verified in diverse settings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This work is sufficiently solid. It covers the fundamental problem formulation, which introduces a new optimization problem. Also, it provides the theoretical optimization solution to the proposed problem, which has not been studied before. From this perspective, this paper has made sufficiently novel contributions.  Moreover, this work designs an efficient implementation to avoid additional computation overhead. These methods are empirically verified with different experiments setting, demonstrating the effectiveness of this method across different settings. Due to its theoretical contributions and solid experiments, I give a clear accept (score 10) to this paper."
            },
            "weaknesses": {
                "value": "I didn't identify any major weaknesses of this paper."
            },
            "questions": {
                "value": "It is really interesting to see the formulation of optimization problem (3) in the data selection problem. Has this method been used in other tasks such as computer vision, diffusion model, or simple MNIST dataset classification? It seems to be a very general approach that can be used in many senarios. \n\nIn Line 60, the PMP derives the necessary condition for optimal data selection. Does it mean that it is not sufficient to guarantee the data selection is optimal? \n\nIt seems that the PDS method always achieves better performance. Is there any negative results where the PDS method fails? Do we have any senarios where we should avoid using this method for the data selection?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 10
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}