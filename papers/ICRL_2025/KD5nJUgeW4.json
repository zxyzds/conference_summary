{
    "id": "KD5nJUgeW4",
    "title": "Solving Multiplayer Partially Observable Stochastic Games by Divergence-Regularized Discounted Aggregation",
    "abstract": "This paper presents Divergence-Regularized Discounted Aggregation (DRDA), a multi-round learning system for solving partially observable stochastic games (POSGs), which unify normal-form games (NFGs), extensive-form games (EFGs), and Markov games (MGs). In each single round, DRDA can be viewed as a discounted variant of Follow the Regularized Leader (FTRL) under a general value function for POSGs concerning imperfect information and an infinite horizon. While previous studies on this FTRL variant have demonstrated its last-iterate convergence towards quantal response equilibrium (QRE) in NFGs, this paper extends the theoretical results to POSGs by defining a generalized Nash distribution (GND), which extends the QRE concept of Nash distribution in NFGs through divergence regularization. The linear last-iterate convergence of single-round DRDA to its rest point is proved under a general assumption of hypomonotonicity. When the rest point is unique, it induces the unique GND, which has a bounded deviation with respect to Nash equilibrium (NE). Under multiple learning rounds, DRDA keeps replacing the base policy for divergence regularization with the policy at the rest point in the previous round. It is further proved that the limit point of multi-round DRDA must be an exact NE rather than a QRE under the unique rest point assumption. In experiments, the last iterates of multi-round DRDA converge to NE at a near-exponential rate in NFGs, outperforming existing baselines including moving-magnet magnetic mirror descent (MMD) in multiplayer EFGs. In an infinite-horizon MG, DRDA significantly outperforms the applicable algorithms based on best-response computations.",
    "keywords": [
        "partially observable stochastic game",
        "Nash distribution",
        "divergence regularization",
        "hypomonotonicity",
        "last-iterate convergence",
        "Nash equilibrium"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=KD5nJUgeW4",
    "pdf_link": "https://openreview.net/pdf?id=KD5nJUgeW4",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces a new variant of multi-round discounted FTRL (called DRDA) which works in general (n-player, general-sum, partially observable, simultaneous or sequential action) games. The paper shows that the new algorithm converges (last-iterate) to a generalized Nash distribution (a Nash equilibrium of a regularized version of the game). Under an assumption (unique rest point), multi-round DRDA is proved to converge to an NE."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "General games are hard. Any work that attempts to learn and prove convergence in this setting is important to the community.\n\nLast-iterate convergence is valuable, especially compared to average-iterate algorithms that are common in the field.\n\nExperimental results look impressive."
            },
            "weaknesses": {
                "value": "No discussion on the assumptions.\n\nTabular.\n\nSingle experiment seed runs."
            },
            "questions": {
                "value": "Q1: Can you say anything about what equilibria is being selected by DRDA compared to other methods?\n\nQ2: How realistic is the local hypomonotonicity assumption?  I don\u2019t think this is addressed in the text.\n\nQ3: How realistic is the single rest point assumption? Is the single rest point a feature of the game, learning dynamics, or both? Do the games in the  experiments section have this property? What happens if they do not have this property?\n\nQ4: The experiments look like single seed runs. Did you run a parameter sweep?\n\nMinor:\n\n* Line 16: \u201cGeneral value function\u201d. I was unsure what this meant at first read through. A Non-zero sum value function?\n* Line 39: Go is more naturally formulated as an EFG. Is there a more natural example for MGs?\n* Line 69: If I am reading this right, \u201cNash distribution\u201d is distinct from \u201cNash equilibrium\u201d?\n* Line 109: Is the set of all joint observations ever used? It seems like a strange quantity.\n* Line 169/184/203/\u2026: Please use inline math styling.\n* Line 169: I am curious if using sum over player convs (rather than max) is important? I realize both would be zero at equilibrium. \n* Line 183: Should a have a superscript: a^i? Because it is player i\u2019s action, rather than a joint action?\n* Line 225: Is this not more like a q-value? Why use v rather than q for the notation?\n* Line 299: Does a GND always exist? It is not clear to me since any perturbation away from pi_base will result in: u^i(pi^i, pi^01_*) - u^i(pi_*) <= epsilon * (some negative value)\n\nScore:\n\nI am willing to raise the score after discussion with co-reviewers. Although I like the results, my main concern is how novel the contributions are compared to previous works. I would also like to be reassured that the method routinely converges -- I do not have a good intuition of how strong of an assumption single-resting point is. Furthermore, any answers to the questions I have asked will also likely improve my understanding and appreciation of the work."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new algorithm for finding a Nash equilibrium in partially observable stochastic games (POSGs), which include setting with an infinite horizon. \nThe algorithm, abbreviated DRDA, is inspired by previous work on finding epsilon-regularized Nash equilibria, which have limitations of a finite horizon and may not always have convergence guarantees in a multi-player setting.\nThey generalize the epsilon-regularized Nash equilibria to include the discounting factor of infinite-horizon POSGs, to form what they call a generalized Nash distribution (GND).\nDRDA can be formulated as an ODE, which is solved repeatedly in multiple rounds to converge to an NE in the limit.\n\nWhile the theory for convergence requires an assumption that is not clear if it can be satisfied in all games (rest point of DRDA is unique => GND has bounded deviation from NE) and the theory of multi-round DRDA converge is not well established, experiments show that the DRDA converges to a high precision in small games, at a near-exponential rate.\nMoreover, they provide linear convergence bounds under some hypomonotonicity assumptions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Paper is well written, given the high complexity of the topic.\n- They show linear convergence if epsilon > lambda, the hypomonotonicity constant. I find the hypomonotonicity intriguing. I think it's likely to  see future follow-ups on this work."
            },
            "weaknesses": {
                "value": "- The theorems 1, 2 and 4 look similar to the results found in [Sokota] and [Perolat], extended to the POSGs. I did not investigate the exact differences, but these links deserve more highlights. Connection to MMD is written about in the Appendix, but as the update equations are written in two different notations I found it hard to follow in a limited time.\n- Connected to previous the previous point, in experiments they compare with moving-magnet version of MMD, as L431 \"MMD is close to DRDA\", which they also abbreviate as MMD, while this version is substantially different from the non-moving-magnet base case. (Perhaps MMM would be a better abbreviation.) This makes me concerned that the real innovation of this paper is showing linear convergence under the hypomonotonicity assumption. \n- I would enjoy more thorough discussion of the experiment results, see also the questions.\n- Appendix E: The Euler method used for single-round DRDA -- SDRDA -- has a learning rate whose schedule is not mentioned in the paper and its relation to the rest point convergence in Theorem 2 is not clear. Also, in experiments they used MDRDA, which uses the Euler-based SDRDA. At first, based on the main paper text, I was under the impression that the discretization of Eq (6) is used for DRDA in the experiments, but the actual algorithm used is different. What is considered as an iteration exactly? One step of the Euler algorithm? It would be helpful to correctly label the algorithm in experiments as MDRDA."
            },
            "questions": {
                "value": "- How does multi-round DRDA depend on the gap of single-round DRDA from its rest point? If I understand correctly, this is unknown?\n- How do you explain the \"spikes\" in Figure 1/2 at the beginning of each new round?\n- In Figure 2, 3-player Kuhn, the \"levels\" (ignoring the spikes) are not monotonic for DRDA. I find it surprising. How is it possible it converges overall? Is there a measurable quantity that is monotonic? (perhaps regret? or gap from regularized eq?)\n- Finding NE outside of constant-sum games is PPAD-hard as I'm sure the authors are well aware of. Perhaps it is hard to remove the uniqueness assumption because of that?\n- Can you please run the algorithm on typical POSG games like the tiger, and compare to methods like HSVI?\n- Can you please compare the algorithm also with Smoothed Predictive Regret Matching+ (with restarts)? \n- How much tuning did the experiments require? The discussions is missing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Divergence-Regularized Discounted Aggregation (DRDA), a multi-round learning method designed for partially observable stochastic games. DRDA builds on a discounted version of Follow the Regularized Leader, adapting it for POSGs to address imperfect information and infinite horizons. The authors expand on previous work showing last-iterate convergence to quantal response equilibrium by defining a generalized Nash distribution, which extends the QRE concept to POSGs via divergence regularization."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper builds on prior work by extending last-iterate convergence from NFGs and EFGs to POSGs, with theoretical proofs, broadening the method's applicability.\n2. It introduces the concept of a generalized Nash distribution, enhancing the QRE framework through divergence regularization.\n3. Convergence proofs are provided for both single-round and multi-round DRDA, strengthening the method's theoretical foundation.\n4. Experimental results demonstrate that DRDA outperforms existing methods across various benchmarks."
            },
            "weaknesses": {
                "value": "See Questions."
            },
            "questions": {
                "value": "1. The paper uses a more general POSG setting compared to traditional EFGs. What are the core challenges unique to this choice, and how do they affect algorithm design?\n2. Multi-round DRDA resembles a variant of Perturb-based methods from previous work [1]. What advantages does DRDA offer over these established approaches?\n3. Can prior methods like R-Nad [2] or APMD [1] be directly applied to POSGs, or are there specific limitations in these algorithms that DRDA addresses?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The submission studies a regularized variant of FTRL for solving for equilibria. The submission derives continuous-time limit point results for convergence to Nash equilibria in general-sum settings and compares experimentally to a variety of baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Converging to Nash in general-sum settings is a very difficult problem. Let me also say that I did not attempt to go through the submission's math. I evaluated the submission's soundness, presentation, and contribution as \"fair\" largely because I lack expertise to judge the novelty or correctness of the technical contribution. Similarly, I evaluated the submission as a 6 as I feel I lack the expertise to verify its value. Should other reviewers possess greater expertise I will defer to them, though I do feel confident saying that the writing is unclear in important places and the experimental results are weak. I'm also a bit concerned about the tension between the complexity results for computing equilibria in general-sum games and the claims made in the submission."
            },
            "weaknesses": {
                "value": "> solving partially observable stochastic games (POSGs), which unify normal-form games (NFGs), extensive-form games (EFGs),\nand Markov games (MGs).\n\nThis isn't correct. In contrast to EFGs, POSGs can neither express untimeable games nor imperfect-recall games.\n\n> While a wide range of game-theoretic learning dynamics, including no-regret dynamics and best response dynamics, were primarily analyzed in static normal-form games (NFG), many real-world games are dynamic and thus require to be solved under a different game representation\n\nInconsistent tense. Also, the reason they require a different representation is because treating them as NFGs is too expensive, not because they involve state.\n\n>  imperfect information game\n\nRequires hyphen.\n\n>  For example, the perfect information game of Go can be formulated as a Markov game (MG) by ignoring the action\nof the waiting player. The imperfect information game of Texas Hold\u2019em is commonly formulated as\nan extensive-form game (EFG). In view of this requirement,\n\nThese sentences don't flow correctly.\n\n> As an extension of MGs, partially observable stochastic games (POSGs) introduces imperfect information and is capable of unifying NFGs, EFGs, and MGs.\n\nAgain, not true re EFGs.\n\n> forizon\n\n> The existence of the rest point of single-round DRDA is thus proved.\n\nIt's not clear to me why this is a non-trivial result.\n\n> Theorem 1. In a POSG, every GND \u20d7\u03c0\u2217 under \u03f5 > 0 induces a rest point (\u20d7v(\u20d7\u03c0\u2217), \u20d7\u03c0\u2217) in single-round\nDRDA, and every GND \u20d7\u03c0\u2217 under \u03f5 = 0 (i.e., Nash equilibrium) induces a DA rest point (\u20d7v(\u20d7\u03c0\u2217), \u20d7\u03c0\u2217).\n\nThis also seems kind of trivial to me.\n\n---\n\nOther weaknesses:\n- The results are continuous time, not discrete time.\n- The games studied in the experiments seem quite limited."
            },
            "questions": {
                "value": "> On the other hand, \n\nOn the other hand compared to what?\n\n> For general-sum games, the last iterate of a \u201cdiscounted\u201d variant of FTRL, first examined in Leslie & Collins (2005), is proved to converge to the solution concept of Nash distribution (Coucheney et al., 2015; Gao & Pavel, 2021), a specific form of quantal\nresponse equilibrium (QRE) (McKelvey & Palfrey, 1995)) defined in NFGs.\n\nWhat's the catch here? Does this require exponential time for small quantal shocks?\n\n>  Therefore, the hypomonotonicity assumption used in our convergence analysis should not be regarded as a strong\nassumption.\n\nIs this claim based on experimental results for Kuhn poker? That would strike me as far from comprehensive enough to make such a sweeping claim.\n\n> These underestimates could reflect the order of magnitude of the true global hypomonotonicity value\n\u03bb, which must be an upper bound for the local hypomonotonicity required in Theorem 2.\n\nWhy does it matter to provide an underestimate of an upper bound?\n\nHow did you tune the parameters of the baselines for the experiments?\n\nDid you use Q-values or counterfactual values for R-NaD and MMD for the experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}