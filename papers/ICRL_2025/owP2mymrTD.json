{
    "id": "owP2mymrTD",
    "title": "Facilitating Multi-turn Function Calling for LLMs via Compositional Instruction Tuning",
    "abstract": "Large Language Models (LLMs) have exhibited significant potential in performing diverse tasks, including the ability to call functions or use external tools to enhance their performance. While current research on function calling by LLMs primarily focuses on single-turn interactions, this paper addresses the overlooked necessity for LLMs to engage in multi-turn function calling\u2014critical for handling compositional, real-world queries that require planning with functions but not only use functions. To facilitate this, we introduce an approach, BUTTON, which generates synthetic compositional instruction tuning data via bottom-up instruction construction and top-down trajectory generation. In the bottom-up phase, we generate simple atomic tasks based on real-world scenarios and build compositional tasks using heuristic strategies based on atomic tasks. Corresponding functions are then developed for these compositional tasks. The top-down phase features a multi-agent environment where interactions among simulated humans, assistants, and tools are utilized to gather multi-turn function calling trajectories. This approach ensures task compositionality and allows for effective function and trajectory generation by examining atomic tasks within compositional tasks. We produce a dataset BUTTONInstruct comprising 8k data points and demonstrate its effectiveness through extensive experiments across various LLMs.",
    "keywords": [
        "large language model",
        "function calling",
        "instruction tuning",
        "synthetic data"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We introduce a \"bottom-up then top-down\" method to enhance LLMs' multi-turn function calling by generating synthetic compositional instruction data.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=owP2mymrTD",
    "pdf_link": "https://openreview.net/pdf?id=owP2mymrTD",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a new approach, BUTTON, to generate synthetic compositional instruction tuning data, which could further serve as instruction tuning data for LLMs. Compared to existing methods, this approach can engage in multi-turn function calling, which enables planning with functions in handling real-world compositional tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThis paper first addresses the problem of planning with functions but not only use them when handling compositional tasks. The originality sounds good. \n2.\tThis paper is well structured with good clarity.\n3.\tThe experiment results are mostly convincing and significant compared to existing methods."
            },
            "weaknesses": {
                "value": "1.\tThe authors state that they differ from existing papers that \u2018use functions\u2019 by \u2018plan with functions\u2019. They are encouraged to compare with these methods. A simple way can be conducting chain-of-thought methods to decompose a compositional tasks, and then use these reference methods in each step. Can the performance still be significant enough?\n2.\tMore experiment designs could make it more convincing. How is the performance when the task composition length, i.e. the amount of sub-tasks in one task increases? See also Question 1."
            },
            "questions": {
                "value": "1.\tDoes the designed heuristic strategy for compositional task construction only contain 2 or 3 sub-tasks? For example, can the sequential composition heuristic allow for an arbitrary length of sub-tasks? How is the performance according to different length of sub-tasks?\n2.\tIt is a confusing on how function generation is conducted in Bottom-Up. The authors state that they allow for the construction of more realistic tasks, and generate functions that are likely to be called in these tasks. Is there any limitation or alignment on the function call generation? Since the function tools are specified and given in GTA and Tool-Query for evaluation, it seems that the Bottom-Up should be limited in using these given functions, rather construct new ones. Is there a conflict?\n3.\tThe authors are encouraged to provide how prompts are designed for ablation study, i.e., how to call the single direct generation steps using one prompt. \n4.\tIn Introduction, the bullet-in points in the last paragraph of how to solve challenges do not tackle with the challenges very well. The authors may better summarize how \u201ccompatibility\u201d in challenge 2 and \u201cwithout human supervision\u201d in challenge 3 are tackled.\n5.\tTypo and grammar check, such as \u2018we first extract a series of real-world scenarios from existing datasets that using external tools.' in Scenario Collection in Section 2.1."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper discussed an approach, BUTTON, which generates synthetic compositional instruction tuning data via bottom-up instruction construction and top-down trajectory generation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The research topic itself has a wide range of practical use cases. \n2. The model training approach is innovative and offers a new perspective on training large language models to make multi-turn function calls. \n3. The explanation of the methodology is clear and detailed. The authors trained multiple popular large language models to compare performance and validated accuracy on two benchmark datasets, providing a more comprehensive analysis."
            },
            "weaknesses": {
                "value": "In the paper, the author mentioned about \u201cmulti-turn function calling\u201d,  \u201cfunction calling\u201d and \u201cinvoke functions\u201d many times. Although the solution purposed could potentially solve the function calls with other additional infrastructure setups, it could be a bit misleading to the readers since the pipeline is still mocking the function call with simulated responses.\n\nIn the section 3.1 experiments setup, the author evaluated the pipeline with two different benchmark datasets. This part could be better structured."
            },
            "questions": {
                "value": "1.  It might be better to explain \u201cfunction calling\u201d a bit ahead in the abstract part to avoid confusion as the pipeline is not essentially invoking a function.\n2. I would recommend to move the \u201crelated work\u201d after the introduction to give the readers more background before introducing the methodology.\n3. I would recommend to start section 3.1 experiments with a brief introduction and split them into two subsections under which you can explain the description and evaluation metrics with some bullet points."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors found that when large language models are called external functions, there is another important but neglected problem: how should the model plan and use these functions when dealing with complex tasks that require multiple rounds of dialogue?\n\nTo solve this problem, they proposed a method called **BUTTON**. This method is divided into two steps:\n\n- The first step is to build training data from the bottom up. They first designed some simple basic tasks and then combined them into more complex ones. Then, they developed corresponding functional interfaces for these tasks.\n\n- The second step is to generate dialogues from the top down. They built a multi-agent environment to allow simulated users, assistants, and tools to interact and generate data for multiple rounds of dialogue.\n\nIn this way, they generated 8,000 high-quality training data. Experimental results show that models trained with this data perform better when dealing with complex tasks.\n\nThis research is efficient because tasks in real life often require multiple steps to complete, and models must learn to plan what functions to use and when to use them."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "**Originality**\n\nCurrently, large prediction models are very popular, and most researchers are studying how to make the model give higher-quality human-like text in dialogue. The authors found two difficulties: 1. The instructions of multi-round dialogues are too complex to be recognized by the model, and 2. There are problems with the compatibility of instructions and functions.\n\n**Quality**\n\nThe author proposed a new method, the BUTTON method, which has a clear technical route and includes two stages: 1) bottom-up instruction construction and 2) top-down trajectory generation. Two professional benchmarks (GTA and Tool-Query) were used to evaluate performance during the experiment. The scale of 8,000 data points is medium-sized in current similar studies, but it includes multiple roles such as system, user, assistant, tool, etc., which enriches the interaction scenario. The experimental results compare several currently very popular large language models, which is convincing.\n\n**Clarity**\n\nThe paragraphs of the article are very clear and easy to understand. The technical terms are also explained.\n\n**Significance**\n\nThis study solves the data acquisition problem through synthetic data and function call mechanism, which not only reduces the development cost but also expands the practical scenarios of LLM, which is of great significance in promoting the implementation of AI technology in practical applications."
            },
            "weaknesses": {
                "value": "The sample size of the benchmark GTA is small, with only 229 queries, and the benchmark Tool-Query has only 60 tasks, which may not fully cover the actual application scenarios. At the same time, the benchmark may have a subjective bias because the manually written queries may carry the subjective judgment of the writer, and the difficulty classification criteria may not be objective enough. Suggestions can be combined with questionnaires for evaluation, such as collecting feedback from actual users and obtaining more real scenario requirements through questionnaires."
            },
            "questions": {
                "value": "How the 8,000 data points mentioned in the experiment were selected was not mentioned, which lacks the rigor of experimental design. It is recommended that the author describe the data mining process in detail. In addition, the specific distribution of these data for different roles is not explained. It is better to provide a chart to show it."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces BUTTON, a generation pipeline to improve multi-turn function calling for LLMs through compositional instruction tuning, to handle complex real-world tasks requiring sequential or parallel function calls.  BUTTON uses a combination of bottom-up task creation and top-down trajectory generation in a multi-agent environment. This pipeline also produces BUTTONInstruct, a dataset with 8,000 multi-turn function call trajectories, and demonstrates improved model performance across various benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper offers a scalable solution for creating synthetic multi-turn datasets, enhancing LLM capabilities in function calling. This is an important feature for AI agents in many applications.\n\n2. The BUTTON pipeline is innovative in combining both bottom-up and top-down data generation processes to address the need for getting complex, multi-turn interactions data  from real-world.\n\n3. The performance seems good. After finetuned with the generated dataset, all models get improved in terms of accuracy."
            },
            "weaknesses": {
                "value": "1. While the BUTTON pipeline effectively generates compositional tasks and interaction trajectories, the paper lacks a rigorous quality control process. For example, it does not sufficiently address how to ensure that two atomic tasks are logically compatible for creating a meaningful complex task. Furthermore, the paper mentions filtering compositional tasks by \u201cchecking whether each one can be completed by its atomic sub-tasks.\u201d However, it remains unclear whether this condition alone is adequate to ensure that the composed tasks are realistic and applicable to real-world scenarios.\n\n2. The generated function calls are conceptual, lacking actual implementations, which may limit their practical usability. Without concrete implementations, there is a risk that these generated function calls might not be feasible in real-world applications. Additionally, it\u2019s unclear whether the pipeline checks for duplicated function calls or if previously generated functions can be reused in later processes, both of which could enhance efficiency and coherence in the generated dataset."
            },
            "questions": {
                "value": "1. The pipeline effectively generates parallel-then-sequential compositions, but how does BUTTON determine which tasks are best suited for parallel execution? Are there specific criteria or heuristics that guide the system in identifying parallelizable tasks?\n\n2. What is the seed data to generate the atomic tasks?\n\n3. How many atomic tasks can be used to generate a composed task at most? Does it affect the data complexity and fine-tuned performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}