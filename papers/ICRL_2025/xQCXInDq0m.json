{
    "id": "xQCXInDq0m",
    "title": "CoS: Enhancing Personalization and Mitigating Bias with Context Steering",
    "abstract": "To deliver high-quality, personalized responses, large language models (LLMs) must effectively incorporate \\textit{context} \u2014 personal, demographic, and cultural information specific to an end-user. For example, asking the model to explain Newton's second law with the context \\textit{``I am a toddler''} should produce a response different from when the context is \\textit{``I am a physics professor''}. However, leveraging the context in practice is a nuanced and challenging task, and is often dependent on the specific situation or user base. The model must strike a balance between providing specific, personalized responses and maintaining general applicability. Current solutions, such as prompt-engineering and fine-tuning require collection of contextually appropriate responses as examples, making them time-consuming and less flexible to use across different contexts. In this work, we introduce \\textbf{Context Steering (CoS)} \u2014a simple, training-free decoding approach that amplifies the influence of the \\textit{context} in next token predictions. CoS computes contextual influence by comparing the output probabilities from two LLM forward passes: one that includes the context and one that does not. By linearly scaling the contextual influence, CoS allows practitioners to flexibly control the degree of personalization for different use cases. We show that CoS can be applied to autoregressive LLMs, and demonstrates strong performance in personalized recommendations. Additionally, we show that CoS can function as a Bayesian Generative model to infer and quantify correlations between open-ended texts, broadening its potential applications.",
    "keywords": [
        "personalization",
        "context",
        "large language model",
        "inference",
        "controllable generation"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We propose Context Steering (CoS), an inference-time technique that enables generating outputs more relevant to the user-provided contexts, leading to better personalization and various applications for large language models.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=xQCXInDq0m",
    "pdf_link": "https://openreview.net/pdf?id=xQCXInDq0m",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces Context Steering (CoS), a method for controlling the influence of context in Large Language Model generated text. The key idea behind CoS is to quantify the impact of context by comparing the output probabilities of the LLM with and without the given context. This key parameter lambda allows CoS to adjust the level of contextual influence on the generated text.  \n\nThe paper demonstrates the effectiveness of CoS in various applications. One application is generating personalized recommendations, where CoS can tailor the LLM's output to specific user preferences. Another application is inferring relationships between open-ended texts, which can be used for tasks like classification and quantification of implied statements."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "-The paper is well written and easy to read.\n\n-The proposed approach to achieve personalization is simple, novel, and training-free, applicable to various LLMs. \n\n-Extensive experiments demonstrate strong performance in personalized recommendations, identification of implicit intents and quantification of extent of \u201cpersonalization\u201d.  \n\n-The experimental analysis is comprehensive."
            },
            "weaknesses": {
                "value": "-Focused primarily on a single context. The paper primarily focuses on scenarios with a single, dominant context. However, real-world situations often involve multiple, potentially conflicting contexts. For example, in the movie case, the user might be interested in comedy movies, science fiction but also movies with great storytelling. \n\n-Limited Discussion on Computational Complexity: While the authors mention that CoS requires twice the amount of compute compared to a vanilla forward pass, they do not provide a detailed analysis of its computational complexity. A more in-depth analysis of how the computational cost scales with input length, context size, and lambda values would be beneficial.  \n\n-Limited discussion on the impact of CoS on other tasks such as reasoning and creativity."
            },
            "questions": {
                "value": "-How can CoS be extended to handle multiple contexts with varying levels of influence? How would the method resolve potential conflicts between different contexts?\n\n-Can you provide a more comprehensive analysis of the computational complexity of CoS? How does the computational cost vary with different parameters and input characteristics (input length, context size, and lambda values)? \n\n-How does the CoS approach affect the LLM's ability for other tasks, e.g., reasoning and creativity? It might be worth some discussion here.\n\n-The appendix seems missing from the manuscript. Is it accidentally omitted?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Context Steering (CoS), a method to personalize large language model (LLM) outputs at inference time. This is done by providing the user's characteristics and preference as context, and adjusting the influence of provided context using a contextual influence function. The influence of this function on the token probabilities can be adjusted, to control how personalized the output is to the given context. Applications of CoS include personalized recommendations involving topics such as movies, travel, cooking, etc. Besides this, the paper also introduces a Bayesian inference model by inverting the CoS probability model. This is used for classifying and quantifying implicit hate speech. Further applications of this Bayesian model include identifying tones in open-ended statements and online content moderation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. CoS is a simple method of personalizing LLM outputs to context, without requiring fine-tuning, or prompt tuning. The method saves on the cost and effort needed for training or prompt-tuning, while being effective in the tests carried out by the authors.\n2. The framework can be used directly across many personalization contexts. Fine-tuning or prompt-tuning would require re-tuning for each new context.\n3. The experiments show promise, and include human evaluations, GPT4 evaluations, and comparisons with baseline models, across various personalization contexts and implicit hate settings."
            },
            "weaknesses": {
                "value": "1. Limited contexts: While CoS is effective for single, straightforward contexts (e.g., \"I like {genre}\"), user preferences are often more complex, involving various (possibly conflicting) likes and dislikes.  It would be interesting to see the method's performance under more sophisticated and detailed contexts.\n2. The baseline experiments in Figure 4 are unclear to me. How are various values of lambda used in the case of in-context learning and multi-turn QA? Also, could the supposedly worse performance of ICL be fixed via prompt tuning?"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the CoS method for controlling the personalization of LLM generation results during inference. CoS operates by calculating the difference between LLM outputs with and without personalized context, and subsequently incorporating this difference into the original outputs with a weight parameter, lambda, to adjust the level of personalization. A higher lambda corresponds to a greater degree of personalization. The core idea shares similarities with existing counterfactual methods; however, applying it to control personalization is novel. Besides proposing CoS, the paper presents a method for inferring lambda in reverse from a given generation result, aiding in the identification of implicit intents, such as the 'degree of hate in statements.'"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1. Controlling the level of personalization by using the difference between LLM outputs with and without personalized context appears reasonable and straightforward, with the entire process completed at inference time.\nS2. The approach of inferring implicit intents from a given generation result is interesting.\nS3. A variety of experiments are presented."
            },
            "weaknesses": {
                "value": "W1. The experimental evaluation appears insufficiently convincing. It would be beneficial to include more evaluations with objective metrics. For instance, incorporating experiments conducted on established benchmarks for LLM personalization [1] and recommendation [2] would strengthen the analysis.\n\nW2. Some experiments and their results are difficult to follow, such as those related to movie recommendations and hate identification. In the recommendation experiments, it is unclear how the baselines\u2014multi-turn Q&A and in-context learning\u2014are compared under different lambda values. Moreover, the results indicate a higher win rate for these baselines. How do these outcomes demonstrate the proposed method's advantages? For the hate identification experiments, the results are not presented in a clear manner.\n\nW3. The method's effectiveness seems dependent on the LLM\u2019s existing ability to generate personalized responses for a given context. This suggests that the approach amplifies current personalization rather than fundamentally enhancing it. For example, if an LLM's personalization is flawed, the method cannot correct it. This limitation indicates that the approach may not serve as a replacement for traditional tuning-based methods.\n\nW4. The advantages of this method over prompt-based approaches (e.g., the multi-turn Q&A baseline) or in-context learning are not clearly outlined.\n\nW5. Table 2 does not include results for lambda=0. Providing these results would offer a more comprehensive view of the evaluation.\n\n[1] LaMP: When Large Language Models Meet Personalization.\n[2] BARS: Towards Open Benchmarking for Recommender Systems."
            },
            "questions": {
                "value": "The main concerns have been outlined under Weaknesses. Below are some additional questions:\n\nQ1. When adding the difference to the original model's predictions, how do you ensure that the generated results remain fluent, coherent, and meaningful?\n\nQ2. Could you provide an example illustrating how to compute lambda and the degree of hate using equations (4) and (5)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}