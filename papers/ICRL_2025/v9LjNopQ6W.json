{
    "id": "v9LjNopQ6W",
    "title": "Do Not Mimic My Voice: Teacher-Guided Unlearning for Zero-Shot Text-to-Speech",
    "abstract": "The rapid advancement of Zero-Shot Text-to-Speech (ZS-TTS) technology has enabled high-fidelity voice synthesis from minimal audio cues, raising significant privacy and ethical concerns. In particular, the ability to replicate an individual\u2019s voice without consent poses risks, highlighting the need for machine unlearning techniques to protect voice privacy. In this paper, we introduce the first machine unlearning framework for ZS-TTS, Teacher-Guided Unlearning (TGU), designed to ensure that the model forgets designated speaker identities while retaining its ability to generate accurate speech for other speakers. Unlike conventional unlearning methods, TGU leverages randomness to prevent consistent replication of forget speakers' voices, ensuring unlearned identities remain untraceable. Additionally, we propose a new evaluation metric, speaker-Zero Retrain Forgetting (spk-ZRF), which measures the model\u2019s effectiveness in preventing the reproduction of forgotten voices. The experiments conducted on the state-of-the-art model demonstrate that TGU prevents the model from replicating forget speakers' voices while maintaining high quality for other speakers. The demo is available at https://speechunlearn.github.io/",
    "keywords": [
        "zero-shot tts",
        "machine unlearning",
        "voice privacy"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "Enhancing voice privacy through machine unlearning in zero-shot text-to-speech",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=v9LjNopQ6W",
    "pdf_link": "https://openreview.net/pdf?id=v9LjNopQ6W",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces TGU (Teacher-Guided Unlearning), a novel mechanism that enhances zero-shot text-to-speech (TTS) systems to address ethical and privacy concerns. To effectively assess the speaker randomness of the 'forget prompts' used within the system, the authors have developed a new metric named spk-ZRF. The experimental findings presented in the study validate the effectiveness of the proposed TGU framework, showcasing its potential to significantly improve the safety and privacy aspects of TTS technologies."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is rooted in a meaningful motivation. The advancement of zero-shot TTS systems raises ethical and privacy concerns and highlights potential misuse for fraudulent activities. This relevance to real-world issues underscores the importance of the research.\n2. The introduction of the novel metric spk-ZRF for evaluating speaker randomness in 'forget prompts' is a commendable aspect of the paper. This new metric contributes to the field by providing a quantifiable measure to assess the efficacy of the proposed unlearning mechanism.\n3. The clarity and quality of the figures presented in the document significantly aid in understanding the complex concepts and methodologies described, making the results and processes more accessible and comprehensible to the audience."
            },
            "weaknesses": {
                "value": "1. The \"fine-tune\" process of TGU is based on given forgotten prompts, which naturally raises a question regarding how this framework performs with other prompts from the same forgotten speaker. Moreover, it is unclear how the framework handles prompts from other retained speakers who have similar timbres with a forgotten one. The paper currently lacks a discussion on these aspects, which are crucial for comprehensively evaluating the robustness and versatility of the proposed TGU framework.\n2. The TGU mechanism resembles a distillation process, suggesting another potential baseline worth exploring. The approach could involve using the zero-shot TTS model to generate audio with the retained speaker style using text from the forgotten set, then employing the generated audio prompt to potentially address the issues mentioned in L234-236 about speaker confusion and privacy constraints.\n3. There are inaccuracies in the bold results presented in Table 1; the WER-F and WER-R recorded for the TGU approach are higher than those of the Exact Unlearning and Fine Tuning methods."
            },
            "questions": {
                "value": "See the above weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new problem to address for the zero-shot TTS model, which is to unlearn an existing voice. The authors provide a simple solution, which is to fine-tune the model on the original training set along with a newly defined target generated by the original teacher model without the target speaker as the reference. As a result, the fine-tuned model keeps the original performance on other speakers while generating random speaker identities for the selected speakers whose voices are supposed to be removed."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "* **Originality**: This work is the first in the field to define voice unlearning for zero-shot TTS (ZS-TTS) and propose a simple solution with synthetic data to address it. It has also proposed a new metric, spk-ZRF, to examine the degree of reconstructability of the target speaker that is supposed to be unlearned. \n\n* **Quality**: The paper has compared several baselines with various metrics and demonstrated the effectiveness of this proposed method. It also has a nice visualization to showcase the effects of various methods for achieving this goal. \n\n* **Clarity**: The presentation of the paper is fairly clear, with all necessary symbols defined, which made it not difficult to follow.\n\n* **Significance**: It is the first paper in ZS-TTS to address the voice unlearning problem with a new metric to account for the reconstructability of the target speaker that can have a significant influence on future works in this field."
            },
            "weaknesses": {
                "value": "The major weakness of this work is its use case is unclear. The problem being solved is not well-motivated, especially in a non-zero-shot setting. Under what condition would the user (in this case, the model trainer or machine learning engineer) re-train the model with all the training data to remove some voices? What benefits does this method provide? It is not obvious to come up with a practical use case for the proposed method where the entire training data is needed to fine-tuned the model for 145k steps (more than a quarter of the 500k steps of the base model training), and the specific speaker that needs to be forgotten has to come with at least 5 minutes of their training audio. In fact, some recent work [1] in image generation has provided more interesting methods for zero-shot unlearning, where only a single image is needed for the model to stop generating the target facial identity. \n\nSince this method requires the entire training data of the original model, 5 minutes of audio for the forget speaker, and more than a quarter of training iterations of the original model, the actual significance of this work is rather limited. For example, in the case voice unlearning is to be used by a cloud service provide, if a zero-shot TTS service provider wants to prevent the model from cloning certain voices, they can easily use a speaker verification model to check whether the provided prompt speaker collides with a database of speaker embeddings whose voices are not supposed to be used and stop providing the service if the provided voice is in the forbidden speaker database. On the other hand, if it is for an open-source model, it is also possible to fine-tune the model on some other dataset for the model to regain the ability to clone the forgotten speaker's voice. From the paper, it is unclear how much data is needed to recover the forgotten speaker's voice as the paper does not show it. The significance of this work could be higher if the proposed method requires an enormous amount of data for the unlearned model to regain its ability to reproduce the voice. However, since no experiment has been conducted, it is unknown whether this work would benefit the open-source community either. \n\nDue to these reasons, the significance of this paper is limited in its current state. It is suggested that the authors provide more motivations and practical use cases of the proposed method and the initial problems of unlearning certain voices to begin with, as it is a new problem proposed by this paper, and so far, the problem does not seem to be very meaningful, and the proposed solution makes it even less effective in practice. \n\n\n[1] Seo, J., Lee, S. H., Lee, T. Y., Moon, S., & Park, G. M. (2024). Generative Unlearning for Any Identity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9151-9161)."
            },
            "questions": {
                "value": "1. What is the intended use case of this unlearning scheme? \n\n2. How much data is needed to recover the unlearned speech in both the non-zero-shot setting (where the forget speaker is in the training set) and the zero-shot setting (where the forget speaker is not in the training set)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "There is no information about the demographics, compensation, or criteria for hiring human subjects for the subjective evaluation. Please add this information to the appendix. Also, please indicate whether you have obtained any IRB approval for these evaluations."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel Teacher-Guided Unlearning (TGU) framework, which allows models to forget specific speaker identities while retaining the ability to synthesize speech for other speakers. This is particularly relevant given the potential misuse of ZS-TTS systems that can replicate voices without consent. The proposed method is built on top of VoiceBox (Le et al. 2024, from Meta) which has reached the SOTA as a ZS-TTS model."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The key strengths of the paper include:\nPrivacy Concerns: The rapid development of ZS-TTS raises ethical issues, particularly the unauthorized replication of individuals' voices, necessitating effective machine unlearning techniques to protect voice privacy.\nTGU Framework: Proposed TGU is the first machine unlearning framework specifically designed for ZS-TTS. It utilizes a pre-trained teacher model to guide the generation of speaker-randomized outputs, effectively helping the model to forget specific speaker identities while maintaining performance for others.\nRandomness in Outputs: Unlike traditional unlearning methods, TGU incorporates randomness in voice styles when the model encounters prompts related to forgotten speakers, which helps neutralize the model's responses to these prompts.\nEvaluation Metrics: The paper introduces a new evaluation metric, speaker-Zero Retrain Forgetting (spk-ZRF), to measure the effectiveness of the unlearning process. The results indicate that TGU not only limits the replication of forgotten voices but also preserves the quality of speech generation for remaining speakers."
            },
            "weaknesses": {
                "value": "Evaluation Metrics: The introduction of spk-ZRF is a valuable contribution, as it provides a quantitative measure of the unlearning effectiveness. However, the paper could benefit from a more detailed explanation of how this metric compares to existing metrics in the literature.\n\nRandomness Implementation: The paper emphasizes the importance of randomness in unlearning, yet it does not sufficiently address potential trade-offs between randomness and speech quality. The balance between generating random outputs for forget speakers while maintaining high fidelity for others needs further exploration.\n\nComplexity of Implementation: The introduction of randomness may complicate the training process and could lead to inconsistent performance across different applications. A clearer discussion on how to balance randomness with quality would be beneficial.\n\nLimited Scope of Forgetting: The focus on only preventing replication of specific voices may overlook broader implications, such as how unlearning affects overall model performance or its ability to generalize across different tasks. A more holistic approach could provide deeper insights into the trade-offs involved.\n\nDataset size: Used Dataset size is relatively small, may not representative of practical scenarios."
            },
            "questions": {
                "value": "1. Perform a computational analysis detailing computational costs, training time requirements, comparison of computational overhead with\nbaseline approaches, and inference time and resource requirements."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an unlearning framework for zero-shot text-to-speech (TTS) to prevent the model from replicating the speech characteristics of specific speakers.\n1. The authors appear to misunderstand the concept of zero-shot TTS. In Figure 1, it seems that the model functions when the speaker is included in the training set. However, in true zero-shot TTS, speakers should not be present in the training data. This discrepancy undermines the authors\u2019 claim that the framework is suitable for zero-shot TTS.\n2. For practical relevance, it would be beneficial to demonstrate the results across multiple models, preferably open-source ones. However, the authors only tested their framework on Voicebox, which is not open-source.\n3. The authors acknowledge that model performance degrades significantly as the number of \u201cforgotten\u201d speakers increases, which raises concerns about the practicality of this approach."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "It seems the authors are first to work on preventing voice clone using machine unlearning."
            },
            "weaknesses": {
                "value": "1.The models do not work well when forgot speakers number increase. 2. The only model they used are not open-sourced."
            },
            "questions": {
                "value": "Have the authors try their approach on open sourced models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}