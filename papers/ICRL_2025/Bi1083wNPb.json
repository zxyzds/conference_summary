{
    "id": "Bi1083wNPb",
    "title": "Equivariant Graph Self-Attention Transformer for Learning Higher-Order Interactions in 3D Molecular Structures",
    "abstract": "Despite their considerable success in multiple fields, studying 3D molecular structures of varying sizes presents a significant challenge in machine learning, particularly in drug discovery, as existing methods often struggle to accurately capture complex geometric relationships and tend to be less effective at generalizing across diverse molecular environments. To address these limitations, we propose a novel Equivariant Graph Self-Attention  Transformer, namely EG-SAT, which effectively leverages both geometric and relational features of molecular data while maintaining equivariance under Euclidean transformations. This approach enables the model to capture molecular geometry through higher-order representations, enhancing its ability to understand intricate spatial relationships and atomic interactions. By effectively modeling the radial and angular distributions of neighboring atoms within a specified cutoff distance using Atom-Centered Symmetry Functions (ACSFs), EG-SAT leads to a more nuanced and comprehensive understanding of molecular interactions. We validate our model on the QM9 and MD17 datasets, demonstrating that EG-SAT achieves state-of-the-art performance in predicting most quantum mechanical properties, thus showcasing its effectiveness and robustness in this domain.",
    "keywords": [
        "Graph Self-Attention",
        "GNNs",
        "3D Molecular Structures"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "This paper presents the Equivariant Graph Self-Attention Transformer (EG-SAT), a novel model that leverages self-attention mechanisms to accurately capture complex geometric relationships and higher-order interactions in 3D molecular structures.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Bi1083wNPb",
    "pdf_link": "https://openreview.net/pdf?id=Bi1083wNPb",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes EG-SAT, an equivariant graph self-attention transformer for modeling 3D molecular structures, introducing Attention-based Atom-Centered Symmetry Functions (AACSFs) to capture higher-order geometric interactions. The authors showed the performance of proposed model on QM9 and MD17 datasets. While the paper presents some interesting ideas around combining attention mechanisms with ACSFs, there are several critical limitations that need to be addressed."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The integration of attention mechanisms with ACSFs is interesting. \n2. the framework is applicable to multiple molecular property prediction tasks\n3. the paper discusses the theoretical foundation of symmetry and group representation in detail"
            },
            "weaknesses": {
                "value": "1. The paper omits several recent works in molecular property prediction, making the comparisons less relevant.\n2. The authors didn't conduct ablation studies to evaluate the contribution of different components in the framework. \n3. There is no computational efficiency analysis, what's more, the claims of improved scalability are unsupported by any experiments.\n4. The motivation for incorporating angular information lacks clear examples where angular information provides benefits.\n5. There's no proof that the attention mechanism preserves chemical validity"
            },
            "questions": {
                "value": "1. Can the authors provide experiment for the claimed scalability improvements over traditional ACSFs?\n2. How does the computational complexity scale with the number of atoms and chemical elements compared to existing methods?\n3. What is the memory footprint of the attention mechanism for larger molecular systems?\n4. Can the authors provide ablation studies showing the specific benefits of angular information integration?\n5. How sensitive is the model to hyperparameter choices, particularly the attention and gating parameters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes EG-SAT (Equivariant Graph Self-Attention Transformer), a novel approach for learning 3D molecular structures. The key contribution is the introduction of Attention-based Atom-Centered Symmetry Functions (AACSFs) that integrate both radial and angular information while maintaining roto-translational invariance. The model improves upon traditional ACSFs by incorporating element-specific attention mechanisms and addresses scalability challenges through attention-based mechanisms. The authors validate their approach on QM9 and MD17 datasets, demonstrating competitive performance in predicting quantum mechanical properties and molecular forces."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper attempts to address the scalability limitations of traditional ACSFs through attention mechanisms, providing a potentially interesting direction for future research in this area.\n\n2. The mathematical formulation of the model's equivariance properties is presented in a structured manner with supporting proofs in the appendix, making the theoretical aspects accessible.\n\n3. The implementation details are documented clearly with hyperparameters and architectural specifications, which aids in potential reproduction of the results."
            },
            "weaknesses": {
                "value": "1. Incomplete baselines for all the datasets presented in the paper. For QM9 dataset, authors didn't include recent works such as Spherenet [1], Equiformer(V2) [2,3], LEFTNet [4], SaVeNet [5], and Geoformer [6] to name few. Although the authors cited Equiformer, they didn't compare the results on QM9.\n\n2. The baselines are on all small molecular tasks on QM9 and MD17 datasets. Therefore, limiting the applicability of the proposed methods.\n\n3. Complexity Analysis: While the authors claim linear complexity with respect to the number of edges, this seems inconsistent with the use of angular information ($\\beta_{ijk}$) which typically involves triplet interactions. The current analysis doesn't adequately justify how the model maintains linear complexity despite considering all possible triplets.\n\n4. Empirical Validation of Efficiency Claims: Despite emphasizing computational efficiency and suitability for high-throughput screening, the paper lacks empirical evidence comparing computational costs with baseline methods.\n\n\n[1] Liu, Y.\u00a0_et al._\u00a0(2022) \u2018Spherical Message Passing for 3D Molecular Graphs\u2019, in\u00a0_International Conference on Learning Representations_. Available at: https://openreview.net/forum?id=givsRXsOt9r.\n\n[2] Liao, Y.-L. and Smidt, T. (2022) \u2018Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs\u2019. Available at: https://openreview.net/forum?id=_efamP7PSjg.\n\n[3] Liao, Y.-L.\u00a0et al.\u00a0(2024) \u2018EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations\u2019, in\u00a0The Twelfth International Conference on Learning Representations. Available at: https://openreview.net/forum?id=mCOBKZmrzD.\n\n[4] Du, W.\u00a0et al.\u00a0(2023) \u2018A new perspective on building efficient and expressive 3D equivariant graph neural networks\u2019, in\u00a0Thirty-seventh Conference on Neural Information Processing Systems. Available at: https://openreview.net/forum?id=hWPNYWkYPN.\n\n[5] Aykent, S. and Xia, T. (2023) \u2018SaVeNet: A Scalable Vector Network for Enhanced Molecular Representation Learning\u2019, in\u00a0Thirty-seventh Conference on Neural Information Processing Systems. Available at: https://openreview.net/forum?id=0OImBCFsdf.\n\n[6] Wang, Y.\u00a0_et al._\u00a0(2023) \u2018Geometric Transformer with Interatomic Positional Encoding\u2019, in\u00a0_Thirty-seventh Conference on Neural Information Processing Systems_. Available at: https://openreview.net/forum?id=9o6KQrklrE."
            },
            "questions": {
                "value": "1. As mentioned in W1 authors didn't include recent baselines, some of which even cited in the work. Therefore, authors shown to be aware of those works but why are they decided not to include in the baselines?\n\n2. Authors discussed their computational complexity and mentioned \"high-throughput screening\" however there is no experiment to support authors claim on the proposed methods' computational complexity compared to the baseline methods.\n\n3. Given that the proposed method utilizing a angular information with $\\beta_{ijk}$, how does the complexity remains linear to the number of edges? The clarifications are needed for this since when we consider all possible triplets, the complexity is $n^3$ with respect to the number of nodes $n$.\n\n4. Could the authors provide additional experiments on larger molecular systems or different types of chemical structures to demonstrate the method's generalizability beyond small molecules?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to improve computational efficiency while preserving equivariance under Euclidean transformations for 3D molecules of varying sizes. By introducing the ACSFs, the authors propose the Equivariant Graph Self-Attention Transformer (EG-SAT), which leverages both geometric and relational features while maintaining roto-translational invariance. The theoretical analysis and time complexity are presented. Experimental results on the QM9 and MD17 datasets demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper addresses a significant research question and proposes a method to solve it.\n- The paper employs a self-attention Transformer to achieve equivariance while reducing computational costs.\n- Comprehensive background information is provided, making the paper accessible and easy to follow.\n- Experiments on the QM9 and MD17 datasets demonstrate the effectiveness of the proposed method."
            },
            "weaknesses": {
                "value": "- The specific research problem addressed by the paper is unclear. While it provides comprehensive background information on molecular learning, symmetry, invariance, irreps, and ACSFs, the research problem is not clearly defined. In Sec. 4.1, the authors highlight the limitations of ACSFs but do so without detailed discussion or analysis to clarify the issue.\n- The paper contains excessive information that may obscure its primary focus. For instance, the authors introduce irreps, but there is limited mention of its relevance within the method or analysis.\n- Compared to ACSFs, the proposed EG-SAT still faces challenges in computational complexity. While the paper provides a complexity analysis for the proposed method, it lacks a direct comparison with ACSFs. Equations 7 and 8 do not offer computational savings relative to Equations 4 and 5.\n- Overall, the content does not sufficiently support the contribution claims in Sec. 1. The paper lacks novelty and significant contributions."
            },
            "questions": {
                "value": "- The work primarily addresses the limitations of ACSFs. However, why are ACSFs not included in the comparison?\n- What is the computational complexity of ACSFs?\n- Can visualizations or case studies be provided to demonstrate that the method achieves equivariance?\n- What criteria are used to select baselines? SchNet and DimeNet are invariant networks, so why are they chosen? Why do the comparison methods differ for QM9 and MD17? More recently proposed methods may also warrant comparison.\n- What is meant by the gating mechanism in the paper, and what improvement does it offer over ACSFs?\n- What challenges are encountered when integrating the self-attention Transformer, and what improvements does this integration provide?\n- What are the limitations of the current work, and what steps could improve efficiency?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This manuscript describes a novel machine learning model called the Equivariant Graph Self-Attention Transformer (EG-SAT), which is designed to overcome challenges in capturing geometric and relational structures of molecules. By using Atom-Centered Symmetry Functions (ACSFs), EG-SAT captures molecular geometry through higher-order representations, modeling both the radial and angular distributions of neighboring atoms within a certain cutoff distance. This allows the model to gain a nuanced understanding of molecular interactions, which benefits the geometric information preservation.\n\nIn the experiment, the model is validated on the QM9 and MD17 datasets. Compared with multiple methods, EG-SAT shows state-of-the-art performance, especially in predicting quantum mechanical properties."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This paper is well-organized with clear writting."
            },
            "weaknesses": {
                "value": "Lack of Novelty:\n1. Equivariance Claim: The proposed method, Equivariant Graph Self-Attention Transformer (EG-SAT), is supposed to be equivariant, which the authors aim to achieve by encoding the geometric information of 3D graphs in Euclidean space. However, this is a commonly used approach seen in models such as SchNet, DimeNet, and SphereNet, which are not only equivariant but also invariant. Therefore, the claim of \"equivariance\" is insufficient as a primary contribution of this method. Given that the model is invariant, introducing the concept of irreducible representations (irreps) in Section 3 is unnecessary. The irreps of SO(3) are typically not used in invariant graph neural networks (GNNs).\n\n2. Graph Transformer: The model is based on the graph transformer architecture, which is also widely used in the field. This, again, is not sufficient to be considered a significant contribution of the work.\n\n3. Scalability of ACSFs: The authors claim to address scalability issues in Atom-Centered Symmetry Functions (ACSFs) using attention-based mechanisms through GRU blocks to approximate interactions between atoms. However, attention mechanisms for interaction have already been considered within the graph transformer framework. The authors should further explain the necessity of introducing this specific mechanism in their approach.\n\nInsufficient Experiments:\n\n1.Outdated Comparisons: The experiments compare the proposed method with multiple other approaches. However, many of these comparison methods are outdated. The authors should benchmark their model against more recent methods such as SphereNet and Molformer. Moreover, since the method is theoretically invariant, there is little need to compare it with numerous equivariant methods."
            },
            "questions": {
                "value": "Suggestions:\n1. Improve the novelty of the method.\n2. Make more comparison with newer invariant methods to demonstrate the effectiveness and robustness.\n3. Adjust the framework of the manuscript, reduce the length of background and related work, and strengthen the correlation with the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Previous geometric graph neural networks generally exhibit poor scalability when dealing with large molecular structure data. To address this issue, this paper proposes improvements to the traditional atom-centered symmetry functions by incorporating self-attention mechanisms to integrate both angular and radial information. This approach enhances the scalability of the graph neural network while preserving rotational and translational invariance."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper introduces the related work and background knowledge of equivariant graph neural networks in a detailed and clear manner, allowing readers to quickly establish relevant domain knowledge."
            },
            "weaknesses": {
                "value": "1. The writing and formatting of the paper are somewhat rough. The size of Figure 1 and the font of Figure 2 both require adjustments. The caption of the table should be placed above the table. Additionally, Section 4 contains only a portion of the content, yet it is labeled with a subsection title '4.1,' which seems redundant. The first sentence of the abstract uses 'their' without a clear referent, among other issues. These problems make the article difficult to read and do not meet the standard of a top conference paper.\n2. This paper resembles a review of equivariant graph neural networks, and its actual contributions are not aligned with what is claimed in the introduction. The article devotes a significant amount of space to background knowledge and related work, only presenting the proposed method towards the end of page 7. Given the structure, it would be more appropriate to submit this as a review paper.\n3. The experimental performance of the proposed method is not promising, and the baseline methods used for comparison are somewhat outdated, mostly from before 2021. Several well-known methods in the field, such as Equiformer[1], are missing from the comparison. As a result, the experiments do not effectively demonstrate the validity of the proposed method.\n4. There seems to be an error in Equation 5, where $d_{ij}$ should be $d_{jk}$. \n5. The innovation of the proposed method is rather limited. Introducing the attention mechanism into graph neural networks is not particularly novel, and I am curious about the differences and connections between the proposed method and existing approaches like GAT[2].\n\n[1] Equiformer: Equivariant graph attention transformer for 3d atomistic graphs.\n\n[2] Graph attention networks."
            },
            "questions": {
                "value": "Please refer to Weaknesses 5."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}