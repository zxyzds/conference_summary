{
    "id": "cPjBTj1Qf5",
    "title": "Counterfactual fairness prediction:  Consistent estimation with generative models and theoretical guarantees",
    "abstract": "Fairness in predictions is of direct importance in practice due to legal, ethical, and societal reasons. This is often accomplished through counterfactual fairness, which ensures that the prediction for an individual is the same as that in a counterfactual world under a different sensitive attribute. However, achieving counterfactual fairness is challenging as counterfactuals are unobservable, and, because of that, existing baselines for counterfactual fairness do not have theoretical guarantees. In this paper, we propose a novel counterfactual fairness predictor for making predictions under counterfactual fairness. Here, we follow the standard counterfactual fairness setting and directly learn the counterfactual distribution of the descendants of the sensitive attribute via tailored neural networks, which we then use to enforce fair predictions through a novel counterfactual mediator regularization. Unique to our work is that we provide theoretical guarantees that our method is effective in ensuring the notion of counterfactual fairness. We further compare the performance across various datasets, where our method achieves state-of-the-art performance.",
    "keywords": [
        "Counterfactual",
        "Causal inference",
        "Counterfactual fairness",
        "Generative models"
    ],
    "primary_area": "causal reasoning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cPjBTj1Qf5",
    "pdf_link": "https://openreview.net/pdf?id=cPjBTj1Qf5",
    "comments": [
        {
            "summary": {
                "value": "The authors introduce a framework called the Generative Counterfactual Fairness Network (GCFN), which is designed to produce predictions that are counterfactually fair. Their approach involves a two-step process using Generative Adversarial Networks (GANs) to learn the distribution of counterfactuals of sensitive attribute mediators. This method is unique in that it directly models counterfactual mediators without requiring latent variable inference, a step that previous methods used but lacked guarantees of fairness."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Counterfactual fairness is crucial since it directly addresses biases related to sensitive attributes. Counterfactual fairness ensures fairness at an individual level.\n\n2. The paper is well-written and easy to follow.\n3. The proposed method is evaluated through different settings on multiple datasets."
            },
            "weaknesses": {
                "value": "I have multiple concerns regarding the theory, objective and experiments. Please see questions below."
            },
            "questions": {
                "value": "1. The objective requires training multiple GANs, which may not be feasible for lots of real-world tasks.\n\n2. Lemma 1: The cited Corollary 3 (Melnychuk et al., 2023) is valid only for real-valued random variables. If the mediator is assumed to be scalar, this assumption should be explicitly stated. Moreover, assuming real-valued mediators is restrictive.\n\n3. The performance heavily depends on the quality of the generated counterfactual mediator. How is this ensured in your objective? While the objective includes both adversarial loss and reconstruction loss, it is unclear how this setup could effectively control the quality of the generated counterfactual mediators. For example, if the generated counterfactual mediator $\\tilde{M}_{a\u2019}$ is distributionally aligned with the factual data $M$ given $A=a'$, how can a discriminator distinguish between them using your objective? If the discriminator is unable to differentiate between them, how can we assert that the generated mediators are counterfactual rather than factual samples conditioned on an alternate sensitive attribute value? In such a case, the method would likely address group fairness rather than counterfactual fairness.\n\n4. Why not just use some generative model to generate counterfactuals and train a classifier on top of it? Is generating the mediator M easier or more accurate compared to directly generating the counterfactual $X$?\n\n5. In Appendix D, why does equation 18 hold? What does $\\\\{M, G(X,0,M)_1\\\\}$ mean? And why $\\\\{M, G(X,0,M)_1\\\\}=\\tilde{G}$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenge of achieving counterfactual fairness by proposing a novel approach that learns the counterfactual distribution of the descendants of sensitive attributes using specifically designed neural networks. Fair predictions are enforced through a counterfactual mediator regularization, which aims to minimize the influence of sensitive attributes. Experimental results demonstrate the effectiveness of this architecture."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The identifiability of counterfactual fairness is an important problem.\n\n2. Experimental results showcase the effectiveness of the proposed architecture."
            },
            "weaknesses": {
                "value": "1. The paper assumes that the function  $f_M$  is a bijective generation mechanism, which is a strong assumption, particularly in real-world datasets where such conditions are rarely met. While this assumption holds in synthetic datasets, it limits the method\u2019s applicability in fairness contexts that generally involve real-world data. Therefore, the claim that this is the first method for counterfactual fair predictions with theoretical guarantees might be overstated given the challenges with real data. The experiment on real data demonstrates the anticipated outcomes that the proposed GAN architecture can achieve. However, there appears to be a disconnect between the empirical application and the theoretical results; the practical implementation and theoretical guarantees seem to function as separate aspects without a relationship.\n\n2. Lemma 1 seems to be a straightforward extension of results from previous studies (e.g., Lemma B.2 in Nasr-Esfahany et al., 2023, and Corollary 3 in Melnychuk et al., 2023) to GANs. It\u2019s plausible that similar results could be obtained for other neural models with continuously differentiable functions, raising the question of whether the paper\u2019s theoretical contributions are specific to GANs or could be generalized to other architectures.\n\n3. Although the reconstruction loss is employed to ensure the similarity between generated factual mediators and observed factual mediators, there is no explicit assurance regarding the accuracy of generated counterfactuals. This limitation raises concerns about the robustness of counterfactual predictions produced by the model.\n\n4. Lemma 2 establishes an upper bound on the effect of the sensitive attribute on the target variable by focusing on the performance of counterfactual mediator generation and regularization. However, this upper bound does not imply the model\u2019s effectiveness in achieving counterfactual fairness. It also leaves open questions regarding the extent to which the proposed method truly guarantees counterfactual fairness in real-world applications."
            },
            "questions": {
                "value": "Please refer to the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to address counterfactual fairness by proposing a two-step solution: (1) training a Generative Adversarial Network (GAN) to estimate counterfactual mediators and (2) training a predictor with regularization based on the estimated counterfactual mediator. Under certain assumptions, they demonstrate that their method can guarantee counterfactual fairness. They validate their method on semi-synthetic and real-world datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper proposes theoretical results on counterfactual fairness. While I didn\u2019t examine all the proofs in detail, their result seems solid.\n2. The writing is clear, and the paper is easy to follow.\n3. The empirical study is generally convincing."
            },
            "weaknesses": {
                "value": "**Theoretical contribution in comparison to existing works**\n\nWhile I acknowledge the theoretical contribution of this paper, I believe the authors overemphasize their contribution relative to existing baselines. For example, methods in [1][2][3][4] all provide methods that satisfy counterfactual fairness under certain conditions. \n1. Could the authors clarify the differences between these methods in terms of theoretical guarantees? With regard to [1], I don\u2019t believe the primary difference lies in the need for knowledge of the ground-truth SCM, as stated in the paper. They only need to know which variables are the descendants of the sensitive attributes, which is also necessary for the method proposed in this paper. Other papers are not discussed in the current draft.\n2. The authors should discuss the significance of their unique contribution more carefully. In some sections, they describe their approach as \u201cthe first neural method with theoretical guarantees,\u201d while in others, this claim is omitted (e.g., Line 17, Line 48). This inconsistency could be misleading to readers.\n\n\n\n\n**Causal Model**\n\nCausal-based fairness notions rely heavily on the chosen causal model. Hence, it is important to clearly present the motivation behind the selected causal model. I agree that the causal model considered in this paper is general. However, could the authors provide additional insight into this model, perhaps by offering a real-world example to illustrate the role of each variable?\n\n\n**Presentation**\n\nHere are a few places where I find the presentation to be unclear or confusing\n1. Line 154 - Does the theoretical result in this paper require invertibility? If so, this should be explicitly stated alongside other assumptions, as this is a fairly strong assumption.\n2. In the original Standard Fairness Model, the $X$ is considered as a hidden confounder. A similar $U$ is considered in some other existing works such as [1][2]. Why can we assume access to $X$ in this paper?\n3. Line 282 - Could the authors clarify why this is described as \u201cnon-trivial\u201d and explain how it might be advantageous (if at all) compared to existing methods that require access to counterfactuals?\n\nI am willing to adjust my score if the authors could address my concerns above. \n\n[1] Kusner, M. J., Loftus, J., Russell, C., & Silva, R. (2017). Counterfactual fairness. Advances in neural information processing systems, 30.\n\n[2] Wang, Y., Sridhar, D., & Blei, D. (2023). Adjusting Machine Learning Decisions for Equal Opportunity and Counterfactual Fairness. Transactions on Machine Learning Research.\n\n[3] Zuo, Z., Khalili, M., & Zhang, X. (2023). Counterfactually fair representation. Advances in Neural Information Processing Systems, 36, 12124-12140.\n\n[4] Zhou, Z., Liu, T., Bai, R., Gao, J., Kocaoglu, M., & Inouye, D. I. (2024). Counterfactual Fairness by Combining Factual and Counterfactual Predictions. arXiv preprint arXiv:2409.01977."
            },
            "questions": {
                "value": "Two short questions\n1. Could there be an arrow from $A$ to $Y$ in Figure1?\n3. Line 206 - Could the authors clarify more carefully why avoiding the abduction-action-prediction framework reduces the possibility of estimation errors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposed a new method to produce counterfactually fair predictions. Specifically, the authors designed a GAN-like structure (GCFN) to learn the counterfactual mediator distribution when the structural causal model (SCM) satisfies certain assumptions. Then the learned mediator together with the features is used to learn the predictor. Furthermore, the paper provided some theoretical guarantees of the method. Lemma 1 claims that GCFN is able to learn the counterfactual mediator distribution only with the factual mediator values, while Lemma 2 provides a fairness guarantee. Experiments on synthetic and real datasets verify the effectiveness of the method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The topic is well-motivated: learning counterfactually fair predictors is important, while VAE-based methods may produce biased estimations of latent variables.\n\n2. Using a GAN-like structure naturally makes sense to me, and the causal structure in Figure 1 is not too restricted in my opinion.\n\n3. The theoretical analysis provides guarantee of the proposed algorithm."
            },
            "weaknesses": {
                "value": "1.  The conditions of Lemma 1 are restrictive. Moreover, Lemma 1 seems to say we can always identify the distribution of the counterfactual mediator. Does this mean if we directly learn $P(M|X=x, A=a)$ we can learn the distribution of $M_{a'}$ and we do not necessarily need GAN? After all, the GAN can only be trained using factual data, so it approximates the factual mediator distribution. \n\n2. It would be beneficial to include some examples to illustrate: (i) how VAE-based methods get a biased estimation of latent variables; (ii) What the mediators are in some practical example causal graphs.\n\n3. The paper does not compare the proposed method with the counterfactually fair representation method [Zuo et al., 2023]. Basically, learning the counterfactual mediator also serves the function of preserving more information of the features, so it is worthwhile comparing the 2 methods.\n\n4. It may help to include some proof intuition in the main paper."
            },
            "questions": {
                "value": "1. Could you elaborate more on Lemma 1 (see weakness 1)?\n\n2.  Line 338:  how can we empirically measure the quality of \"generated counterfactual mediator\", i.e., $\\|M_{A'} - \\hat{M}_{A'}\\|^2_2$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}