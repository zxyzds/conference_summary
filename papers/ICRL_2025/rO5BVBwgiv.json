{
    "id": "rO5BVBwgiv",
    "title": "Retrieval-augmented Encoders for Extreme Multi-label Text Classification",
    "abstract": "Extreme multi-label classification (XMC) seeks to find relevant labels from an extremely large label collection for a given text input. To tackle such a vast label space, current state-of-the-art methods fall into two categories. The one-versus-all (OVA) method uses learnable label embeddings for each label, excelling at memorization (i.e., capturing detailed training signals for accurate head label prediction). In contrast, the dual-encoder (DE) model maps input and label text into a shared embedding space for better generalization (i.e., the capability of predicting tail labels with limited training data), but may fall short at memorization. To achieve generalization and memorization, existing XMC methods often combine DE and OVA models, which involves complex training pipelines. Inspired by the success of retrieval-augmented language models, we propose the Retrieval-augmented Encoders for XMC (RAE-XMC), a novel framework that equips a DE model with retrieval-augmented capability for efficient memorization without additional trainable parameter. During training, RAE-XMC is optimized by the contrastive loss over a knowledge memory that consists of both input instances and labels. During inference, given a test input, RAE-XMC retrieves the top-$K$ keys from the knowledge memory, and aggregates the corresponding values as the prediction scores. We showcase the effectiveness and efficiency of RAE-XMC on four public LF-XMC benchmarks. RAE-XMC not only advances the state-of-the-art (SOTA) method DEXML, but also achieves more than 10x speedup on the largest  LF-mazonTitles-1.3M dataset under the same 8 A100 GPUs training environments. Our experiment code is available in the Supplementary Material.",
    "keywords": [
        "Extreme Multi-label Text Classification",
        "Dual-encoders",
        "Retrieval-Augmented"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "This paper presents RAE-XMC, a novel retrieval-augmented method that effectively enhances the memorization capabilities of dual-encoders for extreme multi-label classification.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=rO5BVBwgiv",
    "pdf_link": "https://openreview.net/pdf?id=rO5BVBwgiv",
    "comments": [
        {
            "summary": {
                "value": "This paper studies the Extreme multi-label classification problem via proposing the Retrieval-augmented Encoders for XMC (RAE-XMC) framework. This framework equips a DE model with retrieval-augmented capability for efficient memorization. The empirical study confirms the effectiveness and efficiency of four public benchmarks. In addition, the authors shows the presented approach achieves significant speedup on the largest dataset."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This study provides a comprehensive overview and in-depth summarization of the various existing approaches that have been developed for extreme multi-label classification. A thorough analysis is conducted on the advantages and disadvantages of each type of approaches. method. This ensures that readers gain a robust understanding of the current models available.\n\n2. The concept of introducing the retrieval-augmented method is interesting. This presents interesting possibilities for improving the performance of the extreme multi-label classification tasks."
            },
            "weaknesses": {
                "value": "1. My primary concern regarding this study is on the aspect of novelty. The concept of incorporating retrieval-augmented knowledge certainly has the potential to provide valuable background information that can enhance classification performance. However, aside from this innovative idea, the overall design of the model remains quite conventional and adheres to traditional methodologies, which may limit its effectiveness. \n\n2. There is a lack of detailed information regarding the implementation of this study. Furthermore, it could be beneficial to explore the potential of utilizing various other language models or, ideally, open-source large language models to assess the feasibility of integrating the proposed model with those advanced technologies. This exploration could offer insights into how to enhance the performance and capabilities of the existing framework."
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The Retrieval-augmented Encoders for XMC (RAE-XMC) framework enhances dual-encoder (DE) models with retrieval capabilities, improving memorization without adding extra trainable parameters. RAE-XMC uses contrastive loss over a knowledge memory of input instances and labels during training. For inference, it retrieves top-K keys from this memory and aggregates their values for prediction scores. Demonstrated on four public LF-XMC benchmarks, RAE-XMC surpasses the state-of-the-art method DEXML, achieving over 10x speedup on the LF-AmazonTitles-1.3M dataset using the same 8 A100 GPUs training setup."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The idea of using retrieval augmentation with XMC is very interesting.\n2. Storing existing dataset samples in memory is a nice trick. \n3. Training seems to be very efficient. Also, the authors have performed extensive experimentation under many settings."
            },
            "weaknesses": {
                "value": "1. OAK is a recent method which also uses memory for XMC tasks. How does RAE-XMC compare with OAK?\n2. PSP metrics seem to be very commonly used across XMC literature. Can you please report PSP also?\n3. Table 1: Does TT include memory construction time also? If not, it will be nice to include that also.\n4. Improvements are somewhat weak in Table 1. On LF-AmazonTitles-131K, P@1 is not best for RAE-XMC. On LF-WikiSeeAlso-320K it looks like RAE-XMC is not stat sig better than NGAME. Also, on LF-AmazonTitles-1.3M, RAE-XMC is not stat sig better than DEXML. \n5. Since memory needs to be stored as well, how do these methods compare with respect to their RAM requirements?\n6. From a novelty perspective, the method looks like Approximate KNN using encoders (trained with std methods taken from DEXML and NGAME). Although it is being called called retrieval augmented, there is actually no augmentation here at all. It is more of KNN rather than retrieval augmentation.\n7. Case studies: It would be nice to see top b retrieved samples for some sample, and show why those help to improve accuracy of the proposed method compared to DEXML. \n8. Also some error analysis would be nice to do, especially on samples where RAE-XMC was wrong but DEXML was correct. What percent of errors are because of wrong top b neighbors?\n9. Can lambda be learned/tuned/computed per sample?\n10. Line 192: what is f?"
            },
            "questions": {
                "value": "Please see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Paper is about generalization and memorization trade off in extreme classification (XC) and give good quality background and information about recent related works. Proposed approach is scalable and shows state of the art results in XC."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "-\tVery well written, I could follow each and every section\n-\tTraining is scalable and architecture does not add to the training memory"
            },
            "weaknesses": {
                "value": "-\tDoes not compare with State-of-the-art XC methods like OAK which works in retriever augmented encoders\n-\tResults are not reported on short text titles datasets, also available on the XC repository. I would suggest authors to report numbers on titles datasets as they are closer to real world tasks.\n-\tOAK uses auxiliary information, why was this auxiliary information not considered in knowledge memory"
            },
            "questions": {
                "value": "Along with answers to the weakness sections, please address the following questions:\n\n- **The approach is not novel**, many papers in the past have use label centroids (Parabel, Astec, XR-Transformer etc) as well as knn on the training documents (Astec, FasterXML). Since authors cited all of them, Kindly comment on how proposed approach is different for these methods. As of now it looks like authors added KNN over DEXML.\n\n- Since authors main argument is about memorization, authors should also report accuracy of encode on training set. Reason to ask this is, is if accuracy is sufficiently high what is the need of adding knn?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes the in-domain retrieval-augmented framework RAE-XMC for extreme multi-label text classification. RAE-XMC uses a clean contrastive learning function to train and uses a knowledge memory to inference. During inference, the test text retrieves top-b nearest neighbors in the knowledge memory to construct the predicted label."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The presentation is good.\n2. The method is sound.\n3. The experimental results are good. In addition, the method converges fastly."
            },
            "weaknesses": {
                "value": "1. The retrieval-augmented framework for extreme multi-label text classification is not novel. Su et al. (2022) use a similar retrieval-augmented framework for multi-label text classification. The difference is that this paper uses a cleaner contrastive learning loss and incorporates label representations in the memory.\n\n[1] Contrastive learning-enhanced nearest neighbor mechanism for multilabel text classification"
            },
            "questions": {
                "value": "1. It seems that the method has a higher inference overhead. Can you show the inference overhead compared to the OVA and DE methods?\n2. Do the methods rely on label descriptions?\n3. The improvement mainly comes from the head class in Table 2. Is there some explanation for this? It looks like it's caused by the imbalance in the memory.\n4. As shown in Lines 317-323, the method also seems applicable to small-scale multi-label text classifications."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}