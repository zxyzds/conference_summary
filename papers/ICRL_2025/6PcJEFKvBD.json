{
    "id": "6PcJEFKvBD",
    "title": "offline_rl_ope: A Python package for off-policy evaluation of offline RL models with real world data",
    "abstract": "offline_rl_ope is a fully unit tested and runtime type checked Python package for performing off-policy evaluation of offline RL models. offline_rl_ope has been designed for OPE workflows using real world data by: naturally handling uneven trajectory lengths; including novel convergence metrics which do not rely on OPE estimator ground truths; and providing a compute and data efficient API which can be integrated with many offline RL frameworks. This paper motivates and describes the core API design and functionality to enable ease of use and extension. The implementations of OPE methods have been benchmarked against existing implementations to ensure consistency and reproducibility. The offline_rl_ope source code can be found on GitHub at: REDACTED.",
    "keywords": [
        "Offline RL",
        "OPE",
        "Python",
        "PyTorch"
    ],
    "primary_area": "infrastructure, software libraries, hardware, systems, etc.",
    "TLDR": "Python package for performing off-policy evaluation of offline RL models, focused on real world applications.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=6PcJEFKvBD",
    "pdf_link": "https://openreview.net/pdf?id=6PcJEFKvBD",
    "comments": [
        {
            "title": {
                "value": "Question responses"
            },
            "comment": {
                "value": "Thank you for your considered response and for taking the time to read our paper. We have responded to each of your questions below. We also completely take on board that the paper needs to be written more clearly. \n\n**What metrics are novel to this paper? Can the authors please state this clearly?**\n\nWe would like to emphasise that the core contribution is:\n- A production ready Python package for performing offline OPE considering only assumptions that are valid in real world use cases;\n- The VWP and WeightStd form only one part of this contribution and were specifically designed since existing packages (i.e., ScopeRL) focus only on metrics requiring an oracle i.e., not \"realworld\".\n\nThe metrics that are novel in this paper are valid weight proportion. In particular, the use of VWP in conjunction with an analysis of the standard deviation of weights to assess the validity of the IS estimates. The only metrics currently used to evaluate IS based OPE estimates for offline RL models are ESS. In particular, we are not proposing ESS here, in fact we propose it should not be used. As demonstrated by the example in the appendix of this paper (copied below), ESS is suboptimal for evaluating IS OPE estimators. \n\nConsider two evaluation policies $\\pi_{e_{1}}$ and $\\pi_{e_{2}}$. Let $w_{1} = \\{w_{1,i} = c_{+}: i \\mod 2 = 1 \\forall i \\in 1, ..., n\\}\\cup\\{w_{1,i} = c_{++}: i \\mod 2 = 0 \\forall i \\in 1, ..., n\\}$ define the set of importance sample weights for $n$ trajectories associated with evaluation policy $\\pi_{e_{1}}$. Let $w_{2} = \\{w_{1,i} = c_{+}: i \\mod 2 = 1 \\forall i \\in 1, ..., n\\}\\cup\\{w_{1,i} = c_{+}': i \\mod 2 = 0 \\forall i \\in 1, ..., n\\}$ define the set of importance sample weights for $n$ trajectories associated with evaluation policy $\\pi_{e_{2}}$. Additionally let $c_{++} = c_{+} + \\epsilon$ and $c_{+} = (c_{+}' + \\epsilon)^{-1}$.\\\\\n\nIn words, policy $\\pi_{e_{1}}$ and $\\pi_{e_{2}}$ deviate to equal extents from $\\pi_{\\beta}$, the difference being $\\pi_{e_{2}}$ is symmetric. Let $\\textrm{ESS}$ be defined as per equation 7 then the metric is defined by the value of $\\textrm{cv}(w)^2$. For $\\pi_{e_{1}}$ and $\\pi_{e_{2}}$ this equals:\n\\begin{align*}\n\\textrm{cv}(w_{1})^2 & = \\Bigg(\\frac{\\sqrt{\\frac{n}{4n-1}\\epsilon^{2}}}{c_{+}+\\frac{1}{2}\\epsilon}\\Bigg)^{2}\\\\\n\\textrm{cv}(w_{2})^2 & = \\Bigg(\\frac{\\sqrt{\\frac{n}{n-1}(\\frac{1}{2}c_{+}+\\frac{1}{n\\epsilon})^{2}}}{2(n\\epsilon)^{-1}}\\Bigg)^{2}\n\\end{align*}\nAnd therefore, as $c_{+} \\rightarrow \\infty$, $\\textrm{cv}(w_{1})^2 \\rightarrow 0$ and $\\textrm{cv}(w_{2})^2 \\rightarrow \\infty$. Following from this, as $c_{+} \\rightarrow \\infty$, $\\textrm{ESS}(w_{1}) \\rightarrow m$ whilst $\\textrm{ESS}(w_{2}) \\rightarrow 0$. However, regardless of the value of $c_{+}$, both policies $\\pi_{e_{1}}$ and $\\pi_{e_{2}}$ should be defined equally in terms of the \"(potentially) reduced information content of a dataset given an evaluation policy\".\\\\\n\nSince writing, it has come to our attention that variance metrics are used in contextual bandit IS estimators however, as demonstrated by  the results of section 5, solely relying on variance can give a false impression of performance when the evaluation policy concentrates far away from the behviour policy.\n\n**How do these implementations differ?** \n- offline_rl_ope can **only** handle stochastic continuous action spaces and compares the density functions of evaluation and behaviour policies (thereby preventing measure 0 evaluations i.e. $P(X=x)$ is measure 0 for continuous polices by p(x) is not where P is the cdf and p is the pdf);\n- In contrast, Scope RL assumes deterministic continuous action spaces and calculates IS estimates through kernel smoothing.\n\nIt is the ambition that in the future, offline_rl_ope will include the option to perform kernel smoothing for deterministic continuous policies however, in the current release this is not available. That being said, the two methods give different policy evaluations and thus it was felt that comparing the two did not make sense since the aim of the comparison was to evaluate implementations.\n\n**Why can they only compare the ranking of continuous actions? Why do the spearman ranking correlation coefficients seem low in some instances?**\n\nRankings were compared since OPE estimation is known to have high MSE see for example, Empirical Study of Off-Policy Policy Evaluation for Reinforcement Learning, Voloshin et al 2019 who also perform ranking/relative MSE. We would be more than happy to include the MSE results however, given the aim was to evaluate the implementations by observing expected trends (due to the lack of continuous baseline), ranking was chosen.\n\n**Relevance to Longchao, et al 2024**\nThank you for pointing this reference - we do agree that the experiments are similar here. However, Longchao, et al 2024 seem to propose a novel method for off-policy selection where as the experiments presented in our paper are only to show the efficacy of implementations."
            }
        },
        {
            "title": {
                "value": "More specific feedback"
            },
            "comment": {
                "value": "Hi, I appreciate you reading the paper however, please could I request some more specific feedback regarding:\n- Why is it incomplete? What were you unconvinced by? \n- What conclusions did we leave open?\n- In what way was it poorly organised? Which parts did you not understand?\n\nThanks"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a python package for off-policy evaluation methods. It has integrated common methods such as IS, WIS, PD, WPD, etc. The package supports multiple metrics and portable APIs for most of the classic methods. The paper also compares with a similar work **Scope-RL**, and shows the advantages of this work. Some details of the implementation of the existing work are discussed in the paper, which provides readers with necessary background information on the relevant techniques."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper proposes a useful Python package for the off-policy evaluation methods in the RL domain, which can be helpful for an easy-to-use toolbox if one wants to implement an evaluation method quickly. \n- The paper discussed some technique details of existing work, making readers out of the domain clearer on how the authors provide unified implementations. \n- The author also provides a flowchart on how the package is designed and how each module is connected with each other."
            },
            "weaknesses": {
                "value": "- The paper is obviously written in a rush, with multiple unclear expressions and roughly created tables. For example, the caption text is not aligned between lines in Figure 1, the missing caption in Figure 2. Lack of explanations for equations  - see details in the questions section.\n- Based on the content of the paper, it is not clear the significant contributions made by this work, while the unified framework for multiple off-policy evaluators is appreciated, it seems like the calibration of performance of the implemented methods is not mentioned, but it is crucial for a standard comparisons and potential users to care about. \n\n> E.g1., authors can consider the use of Mean Absolute Error (MAE) to calculate the error between the OPE estimates and the ground truth for each method, while lower MAE indicates that the method is better calibrated.\n\n> E.g2., another suggestion is the calibration curve: authors can consider generating the calibration visualization plots by showing the estimated returns (OPE results) against actual returns. In this test, a well-calibrated model should ideally fall on the diagonal line. This can provide better insights into how trustworthy this work's implementation is.\n- The presentation in the paper is too simple and not informative. \nE.g., it is unclear how many times the experiments have been done for the result report, from the table 6-7-8, are the values reported in the table average values or the experiment results from one execution? A more convincing way of conveying the results is by: mean \u00b1 std for a method's stable performance. Similarly for the content in the Figure 2."
            },
            "questions": {
                "value": "1. The format for the caption in Figure 1 seems not aligned, it is suggested to adjust for better presentation.\n2. Figure 2 is not completed. The caption content is not described at all.  \n3. In line 794, what does `??` refers to? \n4. In the section 4, line 380, there are some grammar issues in the writing for this paragraph, e.g., `been unit tested however....`, there lacks a comma before the 'however', and the content is actually no contrast in the content of this sentence.  \n5. Since this is a benchmark paper, it is important to know whether the author has calibrated the performance of the implemented IS methods and DR estimators? Are the performance of these methods aligned with the original research papers? \n6. The equations are not fully numbered, for example, the equation about the `self-normalized weights` is not labeled, besides, it lacks explanations for this equation, and notation $\\epsilon$. \n7. In table 5, what does the difference mean? I.e., the difference between oracle IS vs implemented version in offline-rl-ope? or between Scope-RL vs offline-rl-ope (it seems like the two columns are duplicated, there is no further discussion regarding the table)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper describes a novel software library called offline_rl_ope to make real-world off-policy evaluation of RL policies easier. In particular, the python package (and paper) focus on Importance Sampling-based OPE methods (in contrast to fitted Q evaluation), as IS-based methods are missing a canonical implementation. The paper describes the problem of OPE, motivates the API design and functionality, and discusses common metrics for OPE.  Finally, the authors perform benchmarks against other existing software to ensure correctness. \n\nOverall, I think the paper is interesting and potentially useful to the community. I have some questions about novelty, and what are the claimed contributions. Some of the results are difficult to evaluate. Perhaps the authors can help shed light on key questions that will better inform my decision. \n\nI recommend rejecting the paper in its current form, as I do not believe it holds up to ICLR standard."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper proposes a software library with a good API. The abstractions utilized by the API are organized around important calculations in the OPE problem setup. The high-level code interface makes it easy for non-experts to evaluate their policies with simple python code.\n\nThe proposed library fills gaps in existing work (ie., Scope RL). In particular, facilitating evaluation of policies with uneven trajectories is a particularly useful feature.\n\nThe experimental cross-validation of the offline_rl_ope implementation and existing implementations is very strong and suggests the quality of the algorithms."
            },
            "weaknesses": {
                "value": "The overall contribution (while useful) seems small as other libraries do exist. Scope RL has a number of useful features, while d3rlpy has implemented FQE. I am not saying the proposed work has zero novelty; only that there is meaningful prior art in the space. \n\n\n\n\nThe claimed contributions are not entirely clear. I understand that the software package is new and how it compares to previous work. However, some of the contributions regarding metrics are not clear. For example, the VWP and WeightStd metrics are presented and experimentally validated. This would suggest that they are novel to the paper. Additionally, the authors state, \u201cthe metric \u201dVWP\u201d (valid weight proportion) is proposed.\u201d This leads me to believe that they are proposed here, but this verbiage is somewhat ambiguous as this also follows a discussion of ESS, which is not new. The major difficulty is that this contribution is stated neither in the abstract nor in the introduction, which casts doubt on the conclusion that they are novel to this paper.\n\nThe experimental validation of continuous action space in Section 4.2 is difficult to understand. The authors state, \u201coffline rl ope and Scope-RL differed significantly in their approach and as such, could not be compared against one another.\u201d Consequently, the authors only compare the relative ranking of the OPE outputs and compute the spearman correlation coefficient. While ranking is useful, I cannot assess the absolute quality of the OPE outputs under this condition, which casts doubt on the quality. Additionally, the authors state \u201cestimators implemented in offline rl ope were able to accurately rank the performance of policies against the ground truth performance.\u201d Some of the ranking statistics in Table 6 are fairly low (i.e., 0.3-0.5); I\u2019m not sure the preceding statement is entirely true given this result. \n\nThe paper does not seem entirely complete. There are some typos and presentation issues. One of the most glaring problems is the empty caption in Figure 2. This error makes it seem like the paper was hastily written. Other typos:\n- Line 427 \u201cintegrogate\u201d should be interrogate\n- Line 462: \u201ceffected\u201d \u2192 \u201caffected\u201d"
            },
            "questions": {
                "value": "What metrics are novel to this paper? Can the authors please state this clearly?\nWhy can they only compare the ranking of continuous actions? How do these implementations differ? Why do the spearman ranking correlation coefficients seem low in some instances?\n\n\nOther suggestions:\n\nThe experiments in Section 5 reminded me of recent work on probabilistic policy ranking, which could potentially be incorporated into future releases:\n\nDa, Longchao, et al. \"Probabilistic Offline Policy Ranking with Approximate Bayesian Computation.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 18. 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce a Python package for offline policy evaluation (OPE)\nand discuss a number of improvements it offers such as handling uneven trajectory length, including novel metrics, providing effective API.\nThey also report experimental results for reproducibility check and some performance statistics."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "It provides an extensive explanation of a new software package for OPE."
            },
            "weaknesses": {
                "value": "The paper is incomplete, poorly organized and inconclusive.\nIt is not ready for publication."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}