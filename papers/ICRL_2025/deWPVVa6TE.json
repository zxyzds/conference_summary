{
    "id": "deWPVVa6TE",
    "title": "FedL2G: Learning to Guide Local Training in Heterogeneous Federated Learning",
    "abstract": "Data and model heterogeneity are two core issues in Heterogeneous Federated Learning (HtFL). In scenarios with heterogeneous model architectures, aggregating model parameters becomes infeasible, leading to the use of prototypes (i.e., class representative feature vectors) for aggregation and guidance. However, they still experience a mismatch between the extra guiding objective and the client's original local objective when aligned with global prototypes. Thus, we propose a Federated Learning-to-Guide (FedL2G) method that adaptively learns to guide local training in a federated manner and ensures the extra guidance is beneficial to clients\u2019 original tasks. With theoretical guarantees, FedL2G efficiently implements the learning-to-guide process using only first-order derivatives w.r.t. model parameters and achieves a non-convex convergence rate of $\\mathcal{O}(1/T)$. We conduct extensive experiments on two data heterogeneity and six model heterogeneity settings using 14 heterogeneous model architectures (e.g., CNNs and ViTs) to demonstrate FedL2G\u2019s superior performance compared to six counterparts.",
    "keywords": [
        "Heterogeneous Federated Learning",
        "Model Heterogeneity"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We propose a Federated Learning-to-Guide (FedL2G) method that adaptively learns to guide local training in a federated manner and ensures the extra guidance is beneficial to clients\u2019 original tasks.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=deWPVVa6TE",
    "pdf_link": "https://openreview.net/pdf?id=deWPVVa6TE",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces the FedL2G method, designed to address challenges in Heterogeneous Federated Learning that arise from data and model heterogeneity. The proposed method focuses on optimizing the training process by ensuring that additional guiding objectives introduced during the federated learning process are beneficial and align with clients' original local objectives."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow.\n2. The paper is well-motivated that Heterogeneous Federated Learning is quite realistic.\n3. The authors provide rigorous convergence analysis."
            },
            "weaknesses": {
                "value": "1. The paper may overlook some important related methodologies, such as FedPCL[1] and FPL[2], which also address the central issue highlighted by the authors: the deviation of aggregated global prototypes from client-specific feature vectors due to data heterogeneity. Specifically, FPL tackles similar challenges, making its comparison with FedL2G pertinent for a comprehensive evaluation.\n2. The proposed method introduces complexity by requiring an additional small quiz set and a warm-up period, which could complicate implementation. The quiz set, while beneficial for validating model updates, may also introduce an unfair advantage over other baselines that do not use this approach.\n3. It would be beneficial for the authors to include more recent federated learning methodologies like FedPCL[1] and FPL[2] in the experimental comparisons. The current baseline methods, while foundational, may not represent the state-of-the-art, limiting the robustness of the comparative analysis presented.\n4. Including experiments on the DominNet dataset could enhance the applicability and robustness of the FedL2G method across diverse scenarios. \n\n[1] Tan, Y., Long, G., Ma, J., Liu, L., Zhou, T., Jiang, J.: Federated learning from pre-trained models: A contrastive learning approach. Advances in Neural Information Processing Systems 35, 19332\u201319344 (2022)\n\n[2] Huang, W., Ye, M., Shi, Z., Li, H., Du, B.: Rethinking federated learning with domain shift: A prototype view. In: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 16312\u201316322. IEEE (2023)"
            },
            "questions": {
                "value": "Please see the weakness above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper under review studies heterogeneous federated learning (HtFL) and aims to address the mismatch between local and global learning objectives. By learning a set of local guiding vectors during model training, the local loss is prioritized when minimizing the global loss. The guiding vectors are compact in size and are updated based on feedback from clients\u2019 local quiz sets. Therefore, the proposed FedL2G method is claimed to be lightweight, efficient, and adaptable."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Sections 1 and 2 clearly introduce the research problem, relevant literature, and contributions, making the paper easy to follow.\n\n2. The mismatch between model personalization and generalization is identified as a primary research problem in the field of HtFL. \n\n3. Guiding vectors, which are compact in size, are communicated between clients, thereby reducing communication overhead compared to direct model sharing.\n\n4. Relevant benchmarks and theoretical analysis are included to support the paper\u2019s contributions."
            },
            "weaknesses": {
                "value": "1. Feature Extraction Consistency: The consistency of feature extraction is not discussed in detail. With heterogeneous models and quiz sets across different clients, distributed feature extraction may produce varying representations (i.e., guiding vectors) for the same class. How can the averaging of vectors in line 13 of Algorithm 1 be effective in this context? Are there any explanations of feature extraction consistency when local models and private datasets are heterogeneous? \n\n2. Warm-Up Period in Large-Scale Distributed Learning: In practical settings, a warm-up period may not be feasible in large-scale distributed learning scenarios. Does the warm-up phase require all clients to join the system for at least 50 rounds, as indicated on line 236? An ablation experiment should be added to help readers understand why the warm-up is necessary or to quantify the accuracy loss if it is omitted. \n\n3. Data Heterogeneity Settings: The data heterogeneity settings may be overly strict. Using a Dirichlet distribution with control parameters like 0.1 and 0.01 makes the dataset highly skewed. The authors should explore a range of settings, from 0.01 to 1, to show how data heterogeneity impacts the performance of FedL2G. \n\n4. Quiz Set Requirement: The proposed approach assumes that each client can maintain a quiz set, which implies that clients have sufficient training data. The authors should clarify a practical scenario to support this assumption and ensure that the settings are realistic."
            },
            "questions": {
                "value": "1. How can the averaging of vectors in line 13 of Algorithm 1 be ensured to be effective? Are there any explanations regarding feature extraction consistency when local models and private datasets are heterogeneous? The authors should provide quantitative analysis to assess how the consistency of feature extraction impacts the performance of the guiding vectors, for example, measuring cosine similarity between guiding vectors from different clients for the same classes, or examining how variance in feature representations correlates with model performance.\n\n2. Does the warm-up phase require all clients to join the system for at least 50 rounds, as indicated on line 236? An ablation experiment should be included to help readers understand the necessity of the warm-up phase or to quantify the accuracy loss if it is omitted. The review suggests the authors compare performance with different warm-up durations (e.g., 0, 25, 50, 100 rounds) and analyze how it affects convergence speed and final accuracy across various client participation scenarios.\n\n3. Will the proposed method outperform benchmarks in both homogeneous and heterogeneous data settings? The authors should explore a range of heterogeneity settings, with a verity of Dirichlet parameters, to demonstrate how data heterogeneity impacts the performance of FedL2G. The reviewer suggests adopting specific Dirichlet parameter values to test (e.g., \u03b1 = 0.01, 0.1, 0.5, 1, 10) and suggests key metrics to report for each setting, such as accuracy, convergence speed, and communication efficiency.\n\n4. What real-world scenario allows for sufficient quiz sets and warm-up phases? The authors should clearly state a practical scenario to justify the feasibility of these settings. The reviewer suggests that the authors may provide specific examples of real-world applications where these requirements could be met, or discuss potential modifications to the method for scenarios with limited data or stricter time constraints."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics concerns are raised for this paper."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "To address the objective mismatch in Heterogeneous Federated Learning, this paper proposes a Federated Learning-to-Guide method that adaptively learns to guide local training in a federated manner and ensures the extra guidance is beneficial to clients\u2019 original tasks. The technique efficiently implements the learning-to-guide process using only first-order derivatives and achieves a non-convex convergence rate of $\\mathcal{O}(1/T)$. The authors provide empirical validations on the theoretical results as well."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The proposed method is well-motivated, the paper shows that existing methods suffer from the objective mismatch issue, and show how to fix it.\n\nThe empirical results show that Federated Learning-to-Guide method is better than other lightweight HtFL methods, as expected, and the authors conduct ablations that show the influence of the server learning rate and visualizations of data distributions."
            },
            "weaknesses": {
                "value": "The analysis only covers FedOpt with SGD as the optimizer. Still, recent work [1] shows a different combination (Nesterov-accelerated SGD as the outer optimizer and AdamW as the inner optimizer) is much better for practical performance.\n\nIn the theorems that are presented, summarizing the main insights of these theorems may be needed since currently they are just written as long paragraphs.\n\nIn experiments, the least partial client participation ratio is set as 0.5. In more realistic settings, the participation ratio is lower with more clients.\n\n[1].\tArthur Douillard, Qixuan Feng, Andrei A. Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro, Marc'Aurelio Ranzato, Arthur Szlam, Jiajun Shen. DiLoCo: Distributed Low-Communication Training of Language Models. arXiv:2311.08105."
            },
            "questions": {
                "value": "See in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on two classic challenges in Federated Learning (FL): data heterogeneity and model heterogeneity, both of which are crucial for deploying FL in real-world scenarios. In model-heterogeneous settings, aggregating model parameters is not feasible, making the aggregation of class prototypes a common approach. However, the authors observe that the simple aggregation of locally uploaded class prototypes into a global prototype fails to effectively guide the local training process. Thus, the authors propose a Federated Learning-to-Guide (FedL2G) method, which adaptively learns to guide local training in a federated manner and ensures that the additional guidance is beneficial to clients\u2019 original tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The paper tackles two critical challenges for deploying Federated Learning: data heterogeneity and model heterogeneity, which are essential for real-world FL deployment.\n2) The authors identify that simple aggregation of class prototypes can even harm local updates, and therefore propose a learnable approach to generate global prototypes that effectively guide local training processes.\n3) The paper provides a convergence proof, validating the theoretical feasibility of the proposed method.\n4) Extensive experiments are conducted to demonstrate the effectiveness of FedL2G."
            },
            "weaknesses": {
                "value": "1) The meaning conveyed by Figure 1 is unclear. It lacks sufficient explanation, making it difficult to understand the challenges the paper aims to address based on the illustration.\n2) Figure 2 has poor readability, and the lines are overly complicated and cluttered, making it hard to capture the key points. I recommend adding some descriptions to aid in understanding.\n3) Although Figure 1 somewhat points out the challenges to be solved, the paper lacks sufficient motivation to validate this phenomenon.\n4) I am concerned about whether uploading local class prototypes or features of local data samples may leak local data privacy since these data represent local statistical information.\n5) There is a misuse of symbols. In Equation 1, the denominator uses \"n\" to represent the overall data, which may be misunderstood as \"n\" representing a specific device, especially since there are a total of N clients.\n6) There is a lack of qualitative or quantitative analysis to prove that the proposed method effectively reduces the cross-entropy loss on users' local data.\n7) Equation 6 aggregates gradients uploaded by all clients in a simple manner, which I believe may further introduce bias. This is because the information uploaded by strong clients (with stronger feature extraction capabilities and more data) should be more valuable than that from weaker clients."
            },
            "questions": {
                "value": "1) For the prototypes uploaded by different clients, can dynamic aggregation be performed based on the performance of each client? Strong clients often have richer information, so the initial global prototype could be aggregated based on their information to guide the training of weaker clients. As weaker clients produce more robust prototypes, they can also upload their local class prototypes to aggregate a new global prototype.\n2) For other issues, see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}