{
    "id": "IXFCPqFHMQ",
    "title": "SceneFunctioner: Tailoring Large Language Model for Function-Oriented Interactive Scene Synthesis",
    "abstract": "With the Large Language Model (LLM) skyrocketing in recent years, an increasing body of research has focused on leveraging these models for 3D scene synthesis. However, most existing works do not emphasize homeowner's functional preferences, often resulting in scenes that are logically arranged but fall short of serving practical functions. To address this gap, we introduce SceneFunctioner, an interactive scene synthesis framework that tailors the LLM to prioritize functional requirements. The framework is interactive, enabling users to select functions and room shapes. SceneFunctioner first distributes these selected functions into separate areas called zones and determines the furniture for each zone. It then organizes the furniture into groups before arranging them within their respective zones to complete the scene design. Quantitative analyses and user studies showcase our framework\u2019s state-of-the-art performance in terms of both design quality and functional consistency with the user input.",
    "keywords": [
        "Scene Synthesis",
        "Multi-Function Design",
        "Large Language Model",
        "User Interaction"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "We introduce SceneFunctioner, a three-step interactive framework that tailors the LLM to assist users in designing multi-functional scenes.",
    "creation_date": "2024-09-15",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=IXFCPqFHMQ",
    "pdf_link": "https://openreview.net/pdf?id=IXFCPqFHMQ",
    "comments": [
        {
            "comment": {
                "value": "**8.Is there any limitations regarding the number of different zones? E.g. could SceneFunctioner easily be extended to also cover outdoor spaces?**\n\nConsidering the computational complexity of postprocessing in step 1, we currently limit the number of zones to 10. For SceneFunctioner as it currently is, it struggles to handle a large number of zones. However, we believe that with modifications in configurations and approaches, our coarse-to-fine framework can be extended to address object layouts in outdoor spaces. For example, when placing the vegetation or artificial elements, we can consider determining \u2018zones\u2019 based on design objectives, followed by grouping and arranging them. We have to clarify that our solution may be inspiring for outdoor scene generation, but such problems could not be easily \u2018covered\u2019 due to a bunch of factors (e.g., terrain, sunlight, temperature).\n\n**9.The authors may want to also consider the following work as part of their related work discussion: R. Ma, A. Gadi Patil, M. Fisher, M. Li, S. Pirk, B.-S. Hua, S.-K. Yeung, X. Tong, L. Guibas, H. Zhang, Language-Driven Synthesis of 3D Scenes from Scene Databases, ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia), 2018**\n\nThank you for pointing out this missing reference. After going through this paper, we think it is worth discussing as a related work (which will be addressed in the revision file)."
            }
        },
        {
            "comment": {
                "value": "We thank you for your valuable comments on our work. Here we provide our point-by-point reply. \n\nWe look forward to your response and appreciate it if you could share your ideas.\n\n**1.I appreciate the conducted user study, however the validation of the approach appears to be more on the lightweight side.**\n\nWe understand the concern about the comparison experiments, but we believe they are appropriately done. Evaluation of scene quality (functionality, aesthetics, etc.) is inherently subjective and lacks reliable quantitative metrics. Without reference images and other data, computing the collision rates of objects and the CLIP score was what we could do. We also supplemented three criteria (function, practicality, and aesthetics), evaluating them with both a GPT evaluator and human participants. \n\n**2.While the use of zones seems to generate interesting results, it is unclear if the zones can be easily adapted for more complex concepts and layouts (e.g. mixed-use spaces).**\n\nThe motivation for proposing \u2018zones\u2019 is handling more complex concepts/layouts and multi-use spaces, since it helps to decompose the concepts and functions, lightens the burden for the LLM, and enhances understanding of the task. It is still possible that \u2018zones\u2019 could fail in extremely large and complex spaces, but we believe they have the generalizability to most interior scenarios in real life. If you have more insights or questions, please discuss them with us.\n\n**3.The current approach does not consider the aesthetics of the generated furniture layouts (which has also been highlighted as limitation).**\n\nThe current framework has some aesthetical limitations as noted in the paper. However, we have instructed the LLM to consider aesthetics (e.g., balance among objects and coherence in layout) through prompting in each step, and our experiments have demonstrated that the results are aesthetically appealing.\n\n**4.How does SceneFunctioner handle function conflicts in multi-functional zones?**\n\nWe appreciate that you have raised an interesting point. Currently, we would say that no mechanism is implemented to explicitly solve functional conflicts, but we expect the first step of our framework to never provide such cases. A \u2018zone\u2019, as described in our paper, should ensure consistency of the functions within it. The LLM is explicitly informed of this and carefully instructed to coordinate the functions. Still, we acknowledge that understanding and handling relationships among functions are worth studying, rendering it for our future work. \n\n**5.How would an LLM perform better for irregular room layouts (e.g. how would it need to be trained to achieve better performance)?**\n\nIn our paper, the LLM is instructed to decide the zone sizes while the framework automatically places them within the room. For better performance in irregular spaces, the LLM could be provided with a \u2018reference\u2019 rectangular division plan. For example, an L-shaped room could be divided into two or three rectangles. Giving the references as prompts can reduce the possibilities of zone collision errors, but may likely lead to zone sizes that are similar to the division plan (which is undesirable). Therefore, currently we only provide these prompts when encountering continuous failures. \n\nAnother solution is to dispatch an LLM agent that focuses only on room division when determining the zones. However, overlooking whether the functions can fit in these zones may eventually lead to worse results.\n\n**6.How would this framework adapt to advancements in LLM capabilities (e.g. what features would be required to improve upon the current results)?**\n\nThis is another interesting perspective. We would expect major advancements and adjustments in our framework when provided with more capable LLMs: (1) Ideally, the LLM could directly suggest the zone positions and shapes (could be irregular) in the first step so no postprocessing is required. The functions will also be allocated more appropriately. (2) The second and the third steps can possibly be combined if the LLM can handle all furniture relations and layouts in a single shot. More complicated relations and layouts may also be possible. (3) More factors (e.g., lighting, windows and doors, and human interactions) may be more easily included into the framework as long as they are appropriately prompted for the LLM to understand them.\n\n**7.How adaptable is SceneFunctioner to user feedback after an initial scene is generated (if a user wants minor adjustments, would the framework require a complete re-synthesis)?**\n\nCurrently, our framework would need a complete re-synthesis for adjustments. For minor adjustments, the user may want to directly interact with the generated scene using our 3D scene platform. Of course, we appreciate the idea of enabling adjustment features and will investigate a stepwise and context-based \u2018editable\u2019 framework in the future.\n\n**(due to the character limit, we will add another comment)**"
            }
        },
        {
            "comment": {
                "value": "We thank you for your valuable comments on our work. Here we provide our point-by-point reply to your questions. We will post the additional results in the revision file when ready. \n\nWe look forward to your response and appreciate it if you could share your ideas.\n\n**1.In L231-232, the author mentions appending sample inputs and outputs to the context. How are these samples selected/formed? LayoutGPT selects the closest samples for in-context learning using room dimensions & type for generating random layouts of specific room types. However, synthesizing scenes based on room functions is a more specific task than generating random plausible layouts. Therefore, for a fair comparison, I suggest providing LayoutGPT with in-context samples that are closest in terms of room functions. \nSample inputs/outputs to our work are manually designed.**\n\nRegarding the suggestion on LayoutGPT, we appreciate your valuable insight and agree that function-oriented examples could be fairer. However, the challenge lies in defining the \u2018distance\u2019 of functions. As far as we know, embedding and representation of functions in interior 3D scenes is still an unsettled problem (and this paper is investigating it). The results may vary significantly with different standards for deciding the examples. Additionally, sample inputs also need to be modified to suggest the functions. We are actively working on this and may post the results to the revision file. We also look forward to your response if you have any ideas on it.\n\n**2.The author mentions that the wrong results produced by LLMs increase the retries and the overall generation time. I am curious about the quantitative analysis of the feedback mechanism. How much time do the retries take when compared to the actual generation time? How many retries are required on average per-stage?**\n\nThank you for your interest. We will soon list these retry-related statistics in the revision file.\n\n**3.While the paper presents intriguing qualitative results, it would be interesting to see how the method performs on larger rooms (dimensions more than 5 meters) or smaller rooms, different room types (such as bedrooms) to better evaluate the method's generalizibility.**\n\nThank you for your interest. Our framework can be generalized to such configurations, and we will soon post the results to the revision file."
            }
        },
        {
            "comment": {
                "value": "We thank you for your valuable comments on our work. Here we provide our point-by-point reply. We understand your concerns, but we respectfully disagree with you on some points, **especially concerning your misunderstanding of our framework in #1 and #2.**\n\nWe look forward to your response.\n\n**1.The only contribution is a stepwise human-in-the-loop prompting technique to use an LLM (GPT-4o) for scene synthesis. Such a framework does not necessarily address the issue of generating functionally plausible and usable 3D scenes. While there are some merits to the study in the paper in terms of helping LLMs adapt to 3D scene generation tasks, the overall impact and utility is limited.**\n\nWe have to emphasize that our framework is not a human-in-the-loop technique. The user acts before the generation starts, and can interact with the scene or restart after the generation finishes. The postprocessing and feedback mechanisms are automatically performed. We will soon provide more technical details on these steps in the revision file, and we think that these mechanisms also contribute to the method. Also, we think the proposed \u2018zone\u2019, as the bridge for abstract representations (function) and spatial representation (layout), is also inspiring for the research field of scene representation. \n\nRegarding the utility, the experiments demonstrated our framework generally outperforms LayoutGPT and I-Design and achieves decent quality. The user study suggested that our framework saves much time for interior design. Overall, we believe that our framework appropriately addresses generating functionally plausible and usable scenes.\n\n**2.In addition, the experiments presented in the paper are performed on a small dataset (500 scenes) that is manually generated for this framework.**\n\nThe \u2018manually generated\u2019 is another misunderstanding, since all scenes for all three methods are automatically generated, with random-sampled function-specified prompts. We also believe that a sample number of 500 is sufficient to show our framework\u2019s relative performance. \n\n**3.Did the users have a pool of scene functions to choose from? If not, the presented approach is going to see high input-entropy. How does the LLM handle that? If yes, how many total number of scene functions are provided to the users to choose from? I do see in Line 366 which mentions that each scene is configured with randomly selected 6 functions. From how many available functions are these chosen from?**\n\nYes, there is a pool containing 16 functions to choose from.\n\n**4.Were there any quantitative metrics used to verify/validate the correctness of scenes generated at each stage? For example, zone allocation accuracy for stage 1, furniture grouping coherence for stage 2, and placement accuracy for stage 3. Such a quantitative evaluation for every stage can potentially provide more insights into the framework on where things are better/worse.**\n\nThanks for your valuable insight. Although we cannot quantify whether a step output is \u2018proper\u2019 or \u2018good\u2019, we can judge whether it \u2018succeeds\u2019 or results in \u2018violation\u2019, as it is what the postprocessing mechanism does. We will soon provide a revision file supplementing these statistics.\n\n**5.Finetuned LLMs have shown to provide good generalization capability. Current framework uses the GPT-4o LLM as is. It would be interesting to see the results when the LLM is finetuned for scene synthesis task and then used to synthesize a new scene. I was actually hoping to see some sort of finetuning, followed by an contextual analysis. That would have been more interesting.**\n\nThank you for your suggestion. Due to our limited time and budget, we found it difficult to collect (construct and label) sufficient data for effective finetuning but instead focused on improving the prompting and sample inputs/outputs. Nevertheless, we agree that finetuning is a promising approach in our problem setting that may further enhance the generation quality and efficiency.\n\n**6.The paper uses the words \u201cscene\u201d and \u201croom\u201d interchangeably. Is the method proposed in paper for individual room synthesis or scene-level (entire house) synthesis? Please adhere to one convention: either use scene or room everywhere. Better yet, good to provide a clarifying statement in the Intro and/or methodology section about this.**\n\nOur paper targets individual room synthesis and both \u2018scene\u2019 and \u2018room\u2019 refer to this. We will clarify such terms and avoid ambiguity."
            }
        },
        {
            "comment": {
                "value": "We thank you for your valuable comments on our work. Here we provide our point-by-point reply. We understand your concerns, but we respectfully disagree with you on some points.  \n\n**Your reviews #3 and #5 contradict each other, where \u2018a niche problem\u2019 seems controversial to \u2018extensively studied\u2019. We are confused about the reviews.**\n\nWe look forward to your response.\n\n**1.This paper reads more like a technical report of a user application and prompt engineering rather than addressing fundamental research problems.**\n\nOur paper addresses a research problem of \u2018tailoring user preferences to scene representation and generation\u2019. It may not be \u2018fundamental\u2019 as you suggest, but we consider it a valuable topic for deepening the understanding of representation for 3D scenes. Also, our contributions are not limited to prompt engineering. We believe our coarse-to-fine framework and the proposed \u2018zone\u2019 representation provide new insights into 3D scene representation and related research.\n\n**2.The proposed solution also seems tentative, as demonstrated by several limitations**\n\nWe acknowledge that our solution requires improvements. However, some limitations mentioned are not critical or can be addressed by expanding our framework:\n\nLimited zone shapes: Flexible shapes can be supported since we can simply ask the LLM to support them. However, as noted in the paper, the LLM\u2019s performance may significantly reduce. Thus, we find it more reliable to support flexible zone layouts rather than the shapes themselves.\n\nGeneration inefficiencies: As long as there are LLM errors, extra efforts are needed. For example, I-Design[1] relies on a \u2018layout corrector\u2019 to fix such errors. We have implemented postprocessing mechanisms to handle them and minimize their impact on the generation time. Also, the average time for generating each scene is less than 30 seconds, which is acceptable for live user interaction. For comparison, I-Design requires about two minutes to generate each scene.\n\nFloor plan structures: These factors can be easily addressed by expanding each step to include them.\n\n[1] \u00c7elen, A., Han, G., Schindler, K., Van Gool, L., Armeni, I., Obukhov, A., & Wang, X. (2024). I-design: Personalized llm interior designer. arXiv preprint arXiv:2404.02838.\n\n**3.The paper tackles a niche problem that may not resonate with a broad ICLR audience.**\n\nOur research falls into general topics like \u2018applications of 3D scenes\u2019 and \u2018scene representation and understanding\u2019, which are appropriate topics for ICLR. The research problem is worth studying (as you also suggest in #5 that extensive prior research exists in this area), regardless of whether it is \u2018broad\u2019. \n\n**4.The use of GPT, a closed-source model, means the solutions are less reliable and more tentative due to the lack of understanding and access to the model\u2019s workings.**\n\nFirst, our research does not emphasize the mechanisms of LLMs. Instead, we emphasize how LLM understands and yields 3D scenes. Second, we do not think that a close-source model means the method is less reliable. This statement lacks factual evidence and theoretical basis. According to our experiments and existing research, the result scenes are reliable. \n\nPlease refer to our experimental sections. \n\n**5.The spatial understanding of LLMs has been extensively studied in prior research, and this paper does not provide new insights that could inspire or motivate future work in this area.**\n\nHow we address the functions of the scene distinguish our work from prior research. We have proposed using \u2018zones\u2019 as a bridge for abstract representations (function) and spatial representation (layout), which is novel. Additionally, we believe the coarse-to-fine framework provides instructive inspiration in addressing more scene representation and generation problems. \n\nPlease refer to our \u2018related works\u2019 section and our supplementary video."
            }
        },
        {
            "summary": {
                "value": "The paper introduces SceneFunctioner, an interactive framework that leverages GPT-4o for function-oriented 3D indoor scene synthesis, addressing the gap in user-specific, practical room designs. Unlike existing methods, it focuses on distributing user-selected functions into distinct zones and organizing furniture to align with these functional requirements while preventing common issues like object collisions and logical inconsistencies. The framework follows a structured three-step process\u2014deciding zones, forming furniture groups, and arranging them within zones\u2014supplemented by robust postprocessing and feedback mechanisms. The main contributions include a structured approach to function-oriented scene synthesis, the introduction of zones to manage complexity, and iterative verification steps to ensure collision-free, practical designs that align with user inputs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The strengths of the paper include its approach to integrating function-oriented design in 3D scene synthesis, ensuring that generated layouts align with user-specified functional needs. It introduces a multi-step process involving zone-based division and iterative postprocessing checks to prevent furniture collisions and logical inconsistencies. The framework\u2019s use of GPT-4o demonstrates strong LLM capabilities, effectively balancing user input with practical design outcomes. Additionally, its feedback loop ensures error correction and refinement, resulting in high-quality, customized scene synthesis with enhanced user interaction and reduced design time."
            },
            "weaknesses": {
                "value": "This paper reads more like a technical report of a user application and prompt engineering rather than addressing fundamental research problems.\n\nThe proposed solution also seems tentative, as demonstrated by several limitations:\n- Limited zone shapes: Restricted to rectangular zones and room shapes, reducing flexibility for complex layouts.\n- Zone border inconsistencies: Does not account for relationships between adjacent zones, potentially causing pathway issues.\n- Generation inefficiencies: Requires multiple retries due to LLM errors, increasing overall generation time.\n- Not considering floor plan structures: Overlooks the placement of doors and windows, which can significantly affect the usability of the generated scenes.\n\nThe paper tackles a niche problem that may not resonate with a broad ICLR audience.\n\nThe use of GPT, a closed-source model, means the solutions are less reliable and more tentative due to the lack of understanding and access to the model\u2019s workings.\n\nThe spatial understanding of LLMs has been extensively studied in prior research, and this paper does not provide new insights that could inspire or motivate future work in this area."
            },
            "questions": {
                "value": "NA"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Given a list of user-specified scene functions and a room shape, the paper proposes an\ninteractive framework based on LLMs to synthesize a scene that adheres to the given scene\nfunctionalities.\n\nThe main gaps identified in the literature that motivates this work are: (a) scene generation methods that are functionality-unaware, focusing only on object relations, and (b) LLMs requiring contextual information to generate functionally-aware scenes.\n\nThis work presents an interactive, human-in-the-loop scene generation method, with user-specified functionalities and room shapes. The proposed method can be divided into three stages: (1) dividing a scene into \u201czones\u201d based on user selected functionalities, (2) organize furniture for each \u201czone\u201d into groups, and (3) placing furniture at the right orientation at an appropriate location. In every stage, there exists a verification and feedback mechanism to address potential issues arising out of LLM errors. This feedback mechanism from the humans makes the approach more reliable despite using LLMs. In terms of quantitative evaluations, four different measures are recorded. They are: (1) Generation support: a binary indicator that tells whether the framework allows support for irregular shape and user control, (2) Percentage of invalid objects that are out of bound or collide with other objects, (3) CLIP-score that measures how well the generated scene aligns with given input, and (4) Overall scene quality measured in terms of functionality, practicality and aesthetics\njudged by GPT. For comparison, the paper looks at LayoutGPT (NeurIPS 2023) and I-Design (arXiv 2024), both of which do not incorporate human feedback.\n\nTo summarize again, the input is a set of user-specified scene functionalities, scene shape. The output is a scene populated with furniture. The dataset used in ObjaVerse. The LLM chosen here is GPT-4o.  There is no learning mechanism involved (i.e., no training  step involved) as the paper proposes a better way to prompt LLM by dividing the task of scene synthesis into aforementioned three steps, instead of a single one."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "* Well-written paper\n\n* Dividing the scene synthesis task into three stages to make things relatively easy for LLM is an interesting way to stabilize LLMs for complex tasks in 3D\n\n* The second user study shows that the proposed framework is beneficial to the designers."
            },
            "weaknesses": {
                "value": "* The only contribution is a stepwise human-in-the-loop prompting technique to use an LLM (GPT-4o)\nfor scene synthesis. Such a framework does not necessarily address the issue of generating functionally plausible and usable 3D scenes. While there are some merits to the study in the paper in terms of helping LLMs adapt to 3D scene generation tasks,  the overall impact and utility is limited.\n\n* In addition, the experiments presented in the paper are performed on a small dataset (500 scenes) that is manually generated for this framework.\n\nOverall, the paper proposes a step-wise LLM prompting strategy for scene synthesis. The idea is\ninteresting but lacks technical contribution."
            },
            "questions": {
                "value": "* Did the users have a pool of scene functions to choose from? If not, the presented approach is going to see high input-entropy. How does the LLM handle that? If yes, how many total number of scene functions are provided to the users to choose from?  I do see in Line 366 which mentions that each scene is configured with randomly selected 6 functions. From how many available functions are these chosen from?\n\n* Were there any quantitative metrics used to verify/validate the correctness of scenes generated at each *stage*? For example, zone allocation accuracy for stage 1, furniture grouping coherence for stage 2, and placement accuracy for stage 3. Such a quantitative evaluation for every stage can potentially provide more insights into the framework on where things are better/worse.\n\n* Finetuned LLMs have shown to provide good generalization capability. Current framework uses the GPT-4o LLM as is. It would be interesting to see the results when the LLM is finetuned for scene synthesis task and then used to synthesize a new scene. I was actually hoping to see some sort of finetuning, foloowed by an contextual analysis. That would have been more interesting.\n\n* The paper uses the words \u201cscene\u201d and \u201croom\u201d interchangeably. Is the method proposed in paper for individual room synthesis or scene-level (entire house) synthesis? Please adhere to one convention: either use scene or room everywhere. Better yet, good to provide a clarifying statement in the Intro and/or methodology section about this."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces SceneFunctioner, a function-oriented scene synthesis framework that allows users to decide on room functions and room shape. The three-stage framework employs LLMs at each stage, which first divides the scene into zones containing the furniture for one or more functions. The second stage further divides the furniture within each zone into groups and constructs a graph for relations within each group. The last step places the groups within each zone to complete the layout. Quantitative results and user studies prove the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "a) The overall idea of coarse-to-fine scene synthesis strategy is interesting and novel. b) Scene synthesis based on room functions yields plausible and realistic object arrangements. c) The interactive scene synthesis framework allows users to design customized spaces with ease. d) The visual results look good."
            },
            "weaknesses": {
                "value": "a) In L231-232, the author mentions appending sample inputs and outputs to the context. How are these samples selected/formed? LayoutGPT selects the closest samples for in-context learning using room dimensions & type for generating random layouts of specific room types. However, synthesizing scenes based on room functions is a more specific task than generating random plausible layouts. Therefore, for a fair comparison, I suggest providing LayoutGPT with in-context samples that are closest in terms of room functions. b) The author mentions that the wrong results produced by LLMs increase the retries and the overall generation time. I am curious about the quantitative analysis of the feedback mechanism. How much time do the retries take when compared to the actual generation time? How many retries are required on average per-stage? c) While the paper presents intriguing qualitative results, it would be interesting to see how the method performs on larger rooms (dimensions more than 5 meters) or smaller rooms, different room types (such as bedrooms) to better evaluate the method's generalizibility."
            },
            "questions": {
                "value": "All of my questions are listed in the weaknesses section, and I may adjust the rating if they are well addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel approach for the interactive synthesis of 3D scenes, called SceneFunctioner. The framework allows users to sketch rooms and to define functional zones. Given the user input, an LLM is used to generate furniture layouts for each zone based on a three-step generation process (zone creation, local furniture grouping, and final arrangement). A feedback mechanism is used to manage potential LLM errors that may occur during the generation process (e.g. incorrect formatting, object collision, etc,). The paper discusses quantitative results and provides user studies that show that SceneFunctioner outperforms previous approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The three-step generation process (zoning, local grouping, and final arrangement) and the introduced feedback mechanisms for the LLMs are novel and interesting.\n- The approach focuses on room functions, which is not addressed by other  approaches. \n- The conducted user studies seem reasonable and validate the effectiveness of SceneFunctioner. \n- The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- I appreciate the conducted user study, however the validation of the approach appears to be more on the lightweight side. \n- While the use of zones seems to generate interesting results, it is unclear if the zones can be easily adapted for more complex concepts and layouts (e.g. mixed-use spaces). \n- The current approach does not consider the aesthetics of the generated furniture layouts (which has also been highlighted as limitation)."
            },
            "questions": {
                "value": "- How does SceneFunctioner handle function conflicts in multi-functional zones?\n- How would an LLM perform better for irregular room layouts (e.g. how would it need to be trained to achieve better performance)? \n- How would this framework adapt to advancements in LLM capabilities (e.g. what features would be required to improve upon the current results)? \n- How adaptable is SceneFunctioner to user feedback after an initial scene is generated (if a user wants minor adjustments, would the framework require a complete re-synthesis)?\n- Is there any limitations regarding the number of different zones? E.g. could SceneFunctioner easily be extended to also cover outdoor spaces?\n- The authors may want to also consider the following work as part of their related work discussion: R. Ma, A. Gadi Patil, M. Fisher, M. Li, S. Pirk, B.-S. Hua, S.-K. Yeung, X. Tong, L. Guibas, H. Zhang, Language-Driven Synthesis of 3D Scenes from Scene Databases, ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia), 2018"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No concerns."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}