{
    "id": "torbeUlslS",
    "title": "Rethinking Multiple-Instance Learning From Feature Space to Probability Space",
    "abstract": "Multiple-instance learning (MIL) was initially proposed to identify key instances within a set (bag) of instances when only one bag-level label is provided. Current deep MIL models mostly solve multi-instance aggregation problem in feature space. Nevertheless, with the increasing complexity of data, this paradigm faces significant risks in representation learning stage, which could lead to algorithm degradation in deep MIL models. We verify that the degradation issue stems from the persistent drift of instances in feature space during representation learning. In this paper, we propose a novel Probability-Space MIL network (PSMIL) as a countermeasure. In PSMIL, a self-training alignment strategy is introduced in probability space to solve the drift problem in feature space, and the alignment target objective is proven mathematically optimal. Furthermore, we reveal that the widely-used attention-based pooling mechanism in current deep MIL models is easily affected by the perturbation in feature space and further introduce an alternative called probability-space attention pooling. It effectively captures the key instance in each bag from feature space to probability space, and further eliminates the impact of selection drift in the pooling stage. To summarize, PSMIL seeks to solve a MIL problem in probability space rather than feature space. We also introduce new comprehensive benchmarks to evaluate the representation quality for MIL models. Experimental results illustrate that our method could potentially achieve performance close to supervised learning level in complex tasks (gap within 5\\%). The incremental alignment could also bring more than 19\\% accuracy improvements for current existing mainstream models in challenging scenarios. For existing bag-level classification benchmarks, our method also achieves competitive performance to the state-of-the-art deep MIL models. $\\textit{Our code and benchmarks will be released to the community.}$",
    "keywords": [
        "Multiple-Instance Learning\uff1bRepresentation Learning\uff1bSemi-supervised Learning; Weakly-supervised Learning;"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "Transition MIL model to align and aggregate instance features in probability space to avoid representation degradation",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=torbeUlslS",
    "pdf_link": "https://openreview.net/pdf?id=torbeUlslS",
    "comments": [
        {
            "title": {
                "value": "Application Experiment Implementation Suggestion Needed"
            },
            "comment": {
                "value": "As suggested, an additional real-world large-scale application could now be considered to further strengthen our theory. After investigation, we find that the data in real-world only containing bag-level tasks are often much smaller, while an application in VAD with instance-level evaluation task may contain 500+GB raw data (UCF-Crime), and an application in WSI with instance-level evaluation task may contain 700+GB raw data(CAMELYON). We're still trying to download the CAMELYON16 dataset right now, but the scale limits our progress in re-implement our method, thus for us it may be unpractical to provide the re-implement results in several days. We welcome any further specific advise on the dataset selection if practical here to strengthen our theory."
            }
        },
        {
            "comment": {
                "value": "[R]Question-1: We adopt the format of presenting the formula first, followed by the explanation of symbol definitions. Formulas 2 and 5 are further clarified as follows:\n\nEq 2: The derivation process can be found in Appendix A.1. Instance-level recombination is considered the core part of this derivation. Appendix A.1 contains a detailed layer-by-layer differentiation process, along with recombination  and the final attribute proof.\n\nEq 5: As an alignment enhancement strategy, the method prototype originates from $L_{REG}$ in [1]. Such similar strategy is proven effective in Table 3 of [1]. Compared to [1], we consider using the conjunction of results in the set as the optimal label. As a discrete problem, our inference process is actually obtained through taking the difference. Similar to Eq.2, we assume a negative definite result in derivation, attempt to rearrange the terms based on this, and find suitable constraints. A possible operation that can effectively satisfy these constraints in log space are conjunctions.\n\n[R]Question-2: As you can see, this paper focuses on problem identification and theoretical analysis. Basically this work is theoretical right now. Considering the significance of this issue and the oversimplification of benchmarks in current MIL, we have approached it not as an application-oriented paper but rather from the perspective of providing new benchmarks. The problem description, theoretical analysis, and solution design, together with creating these benchmarks, involve quite a time-consuming workload, but we believe this is meaningful to the field. In our problem introduction, we have touched upon application areas of ABMIL such as VAD and WSI. However, these datasets are relatively large-scale and they are not our area of expertise; they are merely used to demonstrate the commonality of ABMIL application issues. As noted in our paper, we mention the additional training burden introduced by feature learning and data augmentation, and as pointed out in Appendix A.3, the issue that current MIL models can only process one bag per training step. These issues might require higher performance, GPU memory, and more suitable model architecture. However, this is a separate issue not aligned with our paper's original intent. Therefore, we present it as potential limitations, as shown in Appendix A.3. Nonetheless, addressing these application issues is indeed a practical necessity. After thorough discussion, we will decide on the option of application experiments. Even though this was not our original intent, we might aim to provide these experimental results before the deadline; otherwise, they will only be available in the journal version. This pending concern will be further clarified before ddl.\n\non FMNIST compared with DTFDMIL: For DTFDMIL, the authors designed original attention pooling to be executed twice sequentially, making it somewhat similar to a simplified transformer structure. By dividing a bag into several sub-bags, the first layer of attention pooling is applied within the sub-bags, followed by another round of attention pooling between the sub-bags in the second layer. Basically DTFDMIL is a  reapplication and extension of the original ABMIL, with the original version of attention pooling still being their core component. When we consider PSMIL, it's designed to  solve the issue of drift in instance representation learning when complex data inputs. For FMNIST, the difficulty is lowest across the 4 datasets. With the regularity of the data styles and the simplicity of the feature extractor, the feature perbutation that cause selection drift is muchly alleviated. This case does not sufficiently demonstrate the feature/selection drift issue we investigate, therefore newest ABMILs may achieve slightly better performance than PSMIL. \n\n[R]Question-3: We appreciate the concerns regarding PSAtt. We will supplement this with the ablation results.\n\nLet us reiterate here: on simple data, replacing original attention solely with PSAtt does not result in significant performance improvement; their performance is basically similar. PSAtt, when applied independently on existing benchmarks (Section 4.4), demonstrates performance similar to the traditional ABMILs family. In lines 521-526 and 808-813 of the paper, we also mention the characteristics of PSAtt compared to traditional Attention and suggest that both original attention and PSAtt can be used for simple data.\n\nPSAtt is designed to address the issue of selection drift resulted by feature drift in complex learning tasks, as shown in Section 4.3 and line 796. PSAtt is more suitable for complex data, and when used in conjunction with PSAli, it can achieve the best results.\n\n[R]Question-4: We agree with your point. And we'll present more ablations as suggested. In general, the influence of hyper-parameter deserve to be claimed explicitly.\n\n[R]Question-5: Incorporated in R-2, see above.\n\n[R]Question-6: Incorporated in R-1, see above."
            }
        },
        {
            "comment": {
                "value": "[R]Question-1: As you can see, this paper focuses on problem identification and theoretical analysis. Considering the significance of this issue and the oversimplification of benchmarks in current MIL, we have approached it not as an application-oriented paper but rather from the perspective of providing new benchmarks. The problem description, theoretical analysis, and solution design, together with creating these benchmarks, involve quite a time-consuming workload, but we believe this is meaningful to the field.\nIn our problem introduction, we have touched upon application areas of ABMIL such as VAD and WSI. However, these datasets are relatively large-scale and they are not our area of expertise; they are merely used to demonstrate the commonality of ABMIL application issues.\nAs noted in our paper, we mention the additional training burden introduced by feature learning and data augmentation, and as pointed out in Appendix A.3, the issue that current MIL models can only process one bag per training step. These issues might require higher performance, GPU memory, and more suitable model designs. However, this is a separate issue not aligned with our paper's original intent.\nTherefore, we present it as potential limitations, as shown in Appendix A.3. Nonetheless, addressing these application issues is indeed a practical necessity. Other reviewers have also mentioned validation in VAD, and after thorough discussion, we will decide on the option of application experiments. Even though this was not our original intent, we might aim to provide these experimental results before the deadline; otherwise, they will only be available in the journal version. This pending concern will be further clarified before ddl.\n\n[R]Question-2: The design  of PSMIL originates from addressing the selection drift in original attention pooling caused by dramatic feature variation. We collectively refer to popular models like DSMIL and DTFDMIL as ABMILs because, in our view, they belong to the same family.\n\nSpecifically, DSMIL modifies ABMIL into a pooling scheme that runs original attention pooling and max pooling (a conservative strategy that simply selects the instance with the highest score) in parallel. The instance obtained from the max pooling branch is used to compute a set of weights by taking the inner product with each instance. These weights are averaged with the weights from the attention pooling branch to serve as the overall weights.\nIn practice, we found that the stability and performance do not necessarily surpass the original attention pooling. The paper was proposed on WSI data, with another major contribution being the introduction of SimCLR, which brought performance improvements.\n\nFor DTFDMIL, the authors designed original attention pooling to be executed twice sequentially, making it somewhat similar to a simplified transformer structure. By dividing a bag into several sub-bags, the first layer of attention pooling is applied within the sub-bags, followed by another round of attention pooling between the sub-bags in the second layer.\n\nOverall, these two models along with other mainstream methods mentioned in the paper, are basically reapplications and extensions of the original attention pooling, with the original version of attention pooling still being their core component. Therefore, in the paper, we classify them as ABMILs. We provide a theoretical analysis of the attention mechanism and clearly identifies the critical flaws that current feature space attention mechanisms bring, with PSMIL as solution.While as the experiment shows, ABMILs face similar issues in complex case. This is the essential difference between PSMIL and others.\n\n [R]Question-3: We agree with your point, as this issue may indeed be a potential limitation. We have mentioned the additional burden in the appendix and experiment section of our paper, which you may have already noticed. Our intent in this theoretically-focused paper is to provide a potential solution strategy for critical issues, offering forward-looking support for integrating feature learning into the MIL framework. Therefore, we did not present complexity analysis as an ablation study within the paper, but rather expect to address it through appropriate engineering or optimization in different scale future applications.\n\nOverall, under the same conditions, the final version, which involves data augmentation and feature learning, takes approximately five times longer (1h15min for one round of training with CIFAR data) than just fine-tuning the classification head (15min, without feature learning and data augmentation). PSMIL  takes  longer on the same data than ABMIL (1h40min and 1h15min, respectively). If a specific method for complexity evaluation is needed, please let us know for further clarification.\n\n [R]Question-4: We agree with your point. And we'll present more ablations as suggested. In general, the influence of hyper-parameter\ndeserve to be claimed explicitly."
            }
        },
        {
            "comment": {
                "value": "We appreciate your recognition of our theoretical contributions and the valuable insights.\n\n[R] Weakness-1: Thank you for providing this citation. According to the model diagram, it appears to be a direct adaptation of PiCO in the multi-instance learning field. It's notable that the paper chooses to compute class prototypes to directly estimate instance labels and incorporates them as instance losses in training, opting for average pooling instead of the widely used attention mechanism. This is a meaningful paper because the authors attempt to showcase advantages at the instance level by adapting PiCO. While we acknowledge the influence of PiCO, the influence to us is primarily conceptual, as it validates the effectiveness of data augmentation for weakly supervised learning tasks. Our main distinction lies in our formulation of the working principles of attention mechanisms and why they can lead to selection bias in end-to-end training. In our approach, class prototypes are used to provide an estimate during the pooling stage to bypass the shortcomings of the attention mechanism we proposed, which is why class prototypes are not highlighted as our main contribution. In other words, probability space pooling can be achieved without introducing class prototypes; it is merely one option. Furthermore, in [1], average pooling is a conservative strategy, thus there is no issue of selection bias. Therefore, the starting points of the two approaches are significantly different. Our paper is essentially theoretical, focusing on revealing both the working principles and critical drawbacks of the widely used feature space attention mechanisms in multi-instance learning, and propose a method to solve the deadly issue. Our data augmentation methods and alignment strategies also differ in implementation, which will be further clarified in the Response-3 below.\n\n[R] Weakness-2: Based on our theoretical analysis in Section 3.1, we predict that improving the quality of pre-trained features cannot address the drift problem during training when using attention mechanisms. However, we are willing to address this concern if further experiments are required. Please specify the experimental conditions and settings explicitly, and we will strive to provide more ablation studies.\n\n[R] Weakness-3: We are familiar with traditional contrastive learning, which may involve SimCLR (self-supervised) and SupCon[2] (supervised contrastive). In fact, we are using a variant of the latter, which originates from [3]. Such pseudo-label contrastive augmentation strategy has been validated in [3] to be significantly effective for the ambiguous problem. Additionally, unlike [3], we provide a multi-instance version and attempt to find the optimal labels generated by the augmented set as the optimization target, further providing mathematical proof.\n\nThe articles mentioned above will be further cited in our paper.\n\n[1] Rethinking Multiple Instance Learning for Whole Slide Image Classification: A Good Instance Classifier is All You Need. TCSVT 2024\n\n[2] Supervised contrastive learning. NeurIPS 2020\n\n[3] Adaptive integration of partial label learning and negative learning for enhanced noisy label learning. AAAI 2024"
            }
        },
        {
            "summary": {
                "value": "This work observes that the core issues leading to performance degradation in MIL with Attention-based methods stem from selection shift and feature shift. To address these problems, it introduces prototype learning and feature alignment."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This work identifies the core issues leading to failures in Attention-based MIL approaches and proposes targeted solutions. It also provides clear theoretical proofs and achieves superior performance."
            },
            "weaknesses": {
                "value": "The article employs two core technologies to address the aforementioned issues; however, there are aspects of these technologies that require further clarification. Firstly, introducing prototypes to resolve selection bias is a technique that has been mentioned in \"Rethinking Multiple Instance Learning for Whole Slide Image Classification: A Good Instance Classifier is All You Need.\" Secondly, feature alignment appears to be a specific application of contrastive learning (either bringing enhanced image features closer or aligning probability spaces). If this process were placed before the overall model training (e.g., as a pre-training step), would it achieve similar effects?\nAdditionally, it is necessary to highlight the differences between this approach and traditional contrastive learning methods."
            },
            "questions": {
                "value": "The authors need to supplement their work by addressing several issues mentioned in the aforementioned weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the issue of drift in instance representation learning within multiple instance learning (MIL) models by introducing Probability-Space Multiple Instance Learning (PSMIL). In PSMIL, the authors present two main strategies: probability-space attention pooling and a probability-space alignment objective. Experimental results show that the proposed approach improves performance on challenging MIL tasks, achieving results closer to supervised learning standards, while maintaining competitive performance on bag-level classification benchmarks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+ The approach of addressing drift issue in instance representation learning through pseudo-label inference-based probability-space alignment appears novel to me.\n+ The authors have provided enough visualizations (e.g., Figure 2) along with the ablation study in Table 3 to justify the effectiveness of the proposed components PSAtt, and PSAli.\n+ The strong performance of existing techniques like DTFDMIL is well-explained in the context of the FMNIST dataset."
            },
            "weaknesses": {
                "value": "+ The authors should ensure all symbols used in the equations are clearly defined and explained. For example, in Equation 5, it is unclear how ${p^{^\\rightarrow}_s}(x^\\prime)$i s derived. Similarly, in Equation 2, the steps from layerwise differentiation to decomposition and recombination require further clarification.\n+ My main concern is the evaluation, as PS-MIL is tested only on synthetic datasets without including any challenging, real-world datasets. Furthermore,  in Table 2, PSMIL performs significantly lower on the FMNIST dataset compared to DTFDMIL, which requires further indepth justification.\n+ In the ablation study (Table 3), the standalone contribution of PSAtt, independent of PSAli, is unclear. Including results for PSAtt alone would help clarify its individual impact on overall performance.\n+ The impact of hyperparameters, such as $\\lambda$ in Equation 9, is not explored. A detailed study on the effect of  $\\lambda$ would offer insights into the relative importance of $L_{\\text{bag}}$ and $L_{\\text{ins}}$ on performance.\n+ Video anomaly detection is a key application of the MIL approach [1, 2, 3], providing challenging, real-world datasets. The authors should demonstrate the performance of their method on these datasets in comparison with established baselines [1, 2, 3]. This would strengthen the paper, as only simpler synthetic datasets are currently considered, which do not fully represent real-world MIL challenges.\n+ The process of pseudo-label inference in Equation 5 is unclear. Providing additional explanation beyond the equations would aid readers in understanding how pseudo-labels are inferred.\n\n**References**\n1. Wu et al. \"VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection\". AAAI2024. \n2. Tian et al. \u201cWeakly-supervised Video Anomaly Detection with Robust Temporal Feature Magnitude Learning\u201d. ICCV2021\n3. Sultani et al. \u201cReal-world Anomaly Detection in Surveillance Videos\u201d. CVPR2018"
            },
            "questions": {
                "value": "Please refer to Weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The author contends that deep MIL models suffer from a drift in the representation of instances during training. This drift, which occurs in the feature space, negatively impacts the model's performance. To address this issue, the paper proposes the Probability-Space MIL network. This network introduces probability-space attention pooling and a probability-space alignment objective. By transforming instances from the feature space to the probability space using class prototypes, the network mitigates the effects of feature drift and consequently enhances the overall performance of the MIL model."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1-\tThe problem is described clearly.\n\n2-\tThe investigated problem is important."
            },
            "weaknesses": {
                "value": "1-\tLimited downstream evaluation. \n\n2-\tThe PSMIL approach seems similar to other approaches that use additional attention pooling, such as DSMIL [1] and DTFDMIL [2].\n\n3-\t The paper does not discuss the computational complexity and scalability of the proposed approach in detail, which could be a concern for large-scale applications.\n\n4-\tLimited Ablation Studies: While the paper includes some ablation studies, more extensive ablations could strengthen the claims.\n\n[1] Bin Li, Yin Li, and KevinWEliceiri. Dual-stream multiple instance learning network for whole slide image classification with self-supervised contrastive learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 14318\u201314328, 2021.\n\n[2] Hongrun Zhang, Yanda Meng, Yitian Zhao, Yihong Qiao, Xiaoyun Yang, Sarah E Coupland, and Yalin Zheng. Dtfd-mil: Double-tier feature distillation multiple instance learning for histopathology whole slide image classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 18802\u201318812, 2022."
            },
            "questions": {
                "value": "1- Use another evaluation method, such as localization on WSI datasets, and compare it with the previous approaches. \n\n2-Illustrate the key differences between the proposed approach, DSMIL [1] and DTFDMIL [2].\n\n3-The approach uses additional probability space with the class prototype. Thus, it is worth comparing the proposed approach's scalability and complexity against the previous approaches. \n\n4-We encourage the authors to do more ablation studies for the approach and see how that will impact the performance of the proposed approach. For example, manipulate the hyper-parameter in the equation (11)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}