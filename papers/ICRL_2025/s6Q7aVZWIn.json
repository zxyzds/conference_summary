{
    "id": "s6Q7aVZWIn",
    "title": "Targeted Low-rank Refinement: Enhancing Sparse Neural Networks with Precision",
    "abstract": "Pruning is a widely used technique for compressing large neural networks that eliminate weights that have minimal impact on the model's performance. Current pruning methods, exemplified by magnitude pruning, assign an importance score to each weight based on its magnitude and remove weights with scores below a certain threshold. Nonetheless, these methods often create a gap between the original dense and the pruned sparse model, potentially impairing performance. Especially when the sparsity ratio is high, the gap becomes more pronounced. To mitigate this issue, we introduce to bridge the gap left by pruning by utilizing a low-rank approximation of the difference between the dense and sparse matrices. Our method specifically entails the iterative refinement of the sparse weight matrix, augmented by a low-rank adjustment. This technique captures and retains the essential information often lost during pruning, thereby improving the performance of the pruned model. Furthermore, we offer a comprehensive theoretical analysis of our approach, emphasizing its convergence properties and establishing a solid basis for its efficacy. Experimental results on LLaMa models validate its effectiveness on large language models across various pruning techniques and sparsity levels. Our method shows significant improvements: at 50\\% sparsity, it reduces perplexity by 53.9\\% compared to conventional magnitude pruning on LLaMa-7B.Furthermore, to achieve a specific performance target, our approach enables an 8.6\\% reduction in model parameters while maintaining a sparsity ratio of about 50\\%.",
    "keywords": [
        "Model Compression",
        "Low-Rank Refinement",
        "Model Pruning"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We propose an iterative method for refining pruned neural network weights, aiming to improve model performance while maintaining sparsity",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=s6Q7aVZWIn",
    "pdf_link": "https://openreview.net/pdf?id=s6Q7aVZWIn",
    "comments": [
        {
            "summary": {
                "value": "This paper presents an approach to approximate a dense full matrix by combining a sparse matrix with a low-rank approximation of the residual error. It introduces an iterative algorithm that progressively refines the sparse weight matrix while incorporating a low-rank approximation, backed by theoretical analysis to demonstrate the convergence and effectiveness of the method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1): The theoretical analysis is solid and contributes a valuable foundation for understanding the method\u2019s validity.\n\n2): The presentation is clear and well-organized."
            },
            "weaknesses": {
                "value": "1): The technical novelty is limited, as similar decompositions for mitigating compression error during fine-tuning have been explored in large language models, including methods like LQ-LoRA  [A1], LoQT[A2], LoftQ [A3], and etc.\n\n2): The method operates in the weight space. It would be interesting to explore decomposition in the gradient space instead, similar to techniques like LoRAPrune [A4] and GaLore [A5]. \n\n3): The experimental results need improvement, particularly in demonstrating practical significance\u2014a common issue in post-training structured pruning methods. For example, with a 13B model at 2:4 sparsity, the average accuracy drops from 40.4 to 31.2, which limits practical impact.\n\n4): There are no comparisons with prior extensive structured pruning literature.\n\n\nReference: \n\n[A1]: LQ-LORA: LOW-RANK PLUS QUANTIZED MATRIX DECOMPOSITION FOR EFFICIENT LANGUAGE MODEL FINETUNING, ICLR 2024 \n\n[A2]: LoQT: Low Rank Adapters for Quantized Training, ICML 2024 workshop\n\n[A3]: Loftq: Lora-fine-tuning-aware quantization for large language models, ICLR 2024\n\n[a4]: LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning, ACL 2024 findings\n\n[A5]: GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection, ICML 2024"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents Targeted Low-Rank Refinement for enhancing the performance of sparse neural networks. By augmenting pruned models with low-rank adjustments, it bridges the gap between dense and sparse networks. They provide theoretical analysis and experiments on LLAMA to verify the efficacy of the approach."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The idea of introducing low-rank adaptor to estimate the deviation between dense and sparse model is interesting.\n\n- Writing is generally good and easy to follow."
            },
            "weaknesses": {
                "value": "- Experiments are not satisfactory. At first, the title is targeting on sparse neural network, while the experiments only target on LLMs. The author needs to add more CNN experiments to make the scope properly aligned. Secondly, they only report perplexity of LLM, which is not sufficient. They need to report more standard benchmark metrics for cross validation.\n\n- Motivation of using low-rank matrices is not clear. In particular, why the difference between sparse and dense blocks are low-rank?"
            },
            "questions": {
                "value": "See the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new method to improve pruned neural networks by using a low-rank approximation to capture essential information lost during pruning, bridging the performance gap between dense and sparse models. Unlike traditional pruning, this approach iteratively refines the sparse weight matrix with a low-rank adjustment, enhancing model accuracy, especially at high sparsity levels."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "+ The proposed low-rankness plus sparsity is reasonable for compressing LLMs since we cannot train large models. \n+ The low-rank part and sparse part can reduce the approximation error between the compressed model and the original model, thereby improving the compressed accuracy.\n+ The paper provides multiple baselines and approximation strategies, as well as theoretical proofs."
            },
            "weaknesses": {
                "value": "- The novelty is not sufficient for this venue. The combination of low-rankness and sparsity is an old topic that has been explored for many years [R1, R2]. Applying the well-established approximation techniques to decompose/compress the large matrices in LLMs has little technical contribution. Besides, compressing DNN models using low-rank and sparse decomposition has already been well explored in [R3]. This paper just scales it to larger models and matrices.\n- The accuracy comparison with respect to sparsity level is not fair. With the additional low-rank part, it is obvious that the accuracy would get improved. However, the authors do not show the storage cost of additional low-rank part.\n- Except for 4:8 and 2:4 sparsity, other random sparsity is not meaningful, which would not get practical acceleration and will be even slower in the GPU execution.\n- The authors have referred to [R4-R6]. I have not seen significant differences among them.\n- The theoretical error bound is not tight and cannot guarantee anything in the practical assessment.  \n\n\n[R1] Sparse and Low-Rank Matrix Decompositions, Forty-Seventh Annual Allerton Conference, 2009.\n\n[R2] Godec: Randomized low-rank & sparse matrix decomposition in noisy case. ICML 2011.\n\n[R3] On compressing deep models by low rank and sparse decomposition, CVPR 2017.\n\n[R4] Slope: Double-pruned sparse plus lazy low-rank adapter pretraining of llms\n\n[R5] LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation\n\n[R6] OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition"
            },
            "questions": {
                "value": "Please refer to the Weakness. Additionally, what are the practical model sizes in the experimental results? What are the practical speedups?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates methods to refine sparse LLMs without re-training. The authors propose using SVD to model the differences between the original large model and its sparse version with a low-rank matrix. They further develop an iterative updating strategy to optimize this low-rank matrix without altering the sparsity level of the model. Experiments on sparse models generated by techniques like magnitude and Wanda demonstrate the effectiveness of the proposed approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. **Well-Written**: The paper is well-structured and written, presenting its methodology and proofs in a clear and rigorous manner.\n2. **Efficient Methodology**: The proposed method is efficient, showing promising results in refining sparse models without re-training."
            },
            "weaknesses": {
                "value": "1. **Limited Experimental Scope:**\n   - The experiments are primarily conducted on models up to 13 billion parameters. Larger models, such as those with 30 billion and 70 billion parameters, were not explored.\n   - The method is only tested on sparse models generated by magnitude and Wanda. It lacks evaluation on sparse models processed by SparseGPT. Additionally, there is no downstream task results for Wanda, which limits a comprehensive understanding of its effectiveness.\n\n2. **Lack of Comparative Analysis:**\n   - The authors did not compare their method against similar types of method that refines sparse LLMs without re-training, such as DsNot[1], in terms of both time using and performance metrics. \n\n3. **Methodological Concerns:**\n   - While the proposed approach shows innovation, there is concern about whether it can outperform traditional fine-tuning methods like LoRA. It might also be beneficial to investigate if further fine-tuning the low-rank parameters, estimated by the proposed method, with LoRA can lead to even more exciting results.\n\n[1] Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs. In ICLR, 2024."
            },
            "questions": {
                "value": "Please see the weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}