{
    "id": "mn61GWpEiK",
    "title": "HeteGraph-Mamba: Heterogeneous Graph Learning via Selective State Space Model",
    "abstract": "We propose a heterogeneous graph mamba network (HGMN) as the first exploration in leveraging the selective state space models (SSSMs) for heterogeneous graph learning. Compared with the literature, our HGMN overcomes two major challenges: (i) capturing long-range dependencies among heterogeneous nodes and (ii) adapting SSSMs to heterogeneous graph data. Our key contribution is a general graph architecture that can solve heterogeneous nodes in real-world scenarios, followed an efficient flow. Methodologically, we introduce a two-level efficient tokenization approach that first captures long-range dependencies within identical node types, and subsequently across all node types. Empirically, we conduct comparisons between our framework and 19 state-of-the-art methods on the heterogeneous benchmarks. The extensive comparisons demonstrate that our framework outperforms other methods in both the accuracy and efficiency dimensions.",
    "keywords": [
        "graph neural network",
        "state space model"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "a heterogeneous graph mamba network (HGMN) as the first exploration in leveraging the powerful selective state space models (SSSMs) for heterogeneous graph learning.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=mn61GWpEiK",
    "pdf_link": "https://openreview.net/pdf?id=mn61GWpEiK",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a graph mamba network as a graph model to learn node representations, aiming to incorporate long-range dependencies for heterogeneous graphs. It adopts subgraphs containing multiple meta-path instances to be tokens of all nodes. Aggregate embeddings of meta-path instances for node $v$ within one meta-path and across multiple meta paths using attention mechanism to get the node embedding and then uses the updated method in MAMBA to update node embeddings within one node type first and then across node types to model the long-range dependency. The experimental results show marginal improvement."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. it is good to adopt MAMBA for graph data.\n2. Comprehensive baseline models are used.\n3. The proposed model is relatively fast empirically."
            },
            "weaknesses": {
                "value": "1. The related work lacks targeting the problem that the paper wants to solve. i.e., if you claim the proposed model can do long-range dependency, so your related work should discuss the work that is related to this challenge. \n2. The paper has typos and doesn't provide explanations for some symbols, i.e., $\\mathcal{N}_s^M$, and `spacial` at line 172.\n3. Additionally, the experiments are unable to be reproducible. it is unclear how metapaths are built. Since the improvement is marginal, it would be better to show the variance of the performance. This paper lacks an ablation study for the components in the model. sin\n4. My major concern is that the local information of a node might gradually disappear during the inner and outer updates since the order of these two processes is only determined by the degree."
            },
            "questions": {
                "value": "1. How do you get the subgraph for all nodes? Does it include every node next to it?\n2. Also please address the fourth point in weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper applies Mamba to heterogeneous graph learning tasks. The authors provide a full recipe to build the model, which includes tokenization, heterogeneity alignment, inner and outer ordering, and inner and outer updating steps. The six-step recipe help convert the heterogeneous graph to sequence, capture long-range dependency among various types of nodes as well as reduce computation burden. The empirical results demonstrate that the proposed HGMN enjoys higher accuracy and more efficiency than exsiting transformer-based GNNs on multiple heterogeneous graph benchmarks"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed recipe to implement the HGMN is highy scalable and take both the long-range homogeneity and heterogeneity into take into consideration."
            },
            "weaknesses": {
                "value": "1. The motivation is too weak. Since Mamba claims to handle long-range context better\nand operates in linear time complexity, the paper applies Mamba to heterogeneous graph\nlearning tasks, expecting superior results. This point lacks persuasiveness.\n2. Novelty is very limited. The paper shares many similarities with  Graph Mamba\uff08KDD 2024\uff09.\n\n2.1 For instance, in the Tokenization section, both fundamentally encode subgraphs to\nrepresent nodes, with the difference being that Graph Mamba uses a random walk\napproach to sample subgraphs, while this paper uses metapath instances.\n\n2.2Additionally, the language style in the Tokenization section of the article is very similar\nto that of Graph Mamba. \n\n2.3 In this paper, I think the only difference between it and Graph Mamba lies in the inner\nand outer ordering methods. However, these ordering methods lack persuasiveness.\n*Why does the inner ordering rely on the number of metapaths as its basis?\n*Why can\u2019t the number of node degree be used as a basis during inner ordering?\n*Shouldn\u2019t the importance of different metapaths be considered?\n\n3. Some important detail  is missing. For example, in the Metapath Instance Aggregation\nsection, the implementation of the instance encoder is not explained. How is beta\nimplemented in Equation 9?\n\n4. The experiments are not enough. The ablation experiments are not\nanalyzed and are not comprehensive, for example, there is a lack of ablation on the\nTOKENIZATION and HETEROGENEITY ALIGNMENT sections. There is also a lack of\nanalysis on key parameters in the method.\n\n\nReference:*Graph Mamba: Behrouz, Ali, and Farnoosh Hashemi. \"Graph mamba: Towards learning on\ngraphs with state space models.\" Proceedings of the 30th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining. 2024."
            },
            "questions": {
                "value": "1. How can you prove that your tokenization is both effective and efficient\uff1f\n2. For Equation 6, is it reliable to map vectors of different types of nodes to the same latent\nspace using just a single linear layer? If this approach is successful, does it suggest that\nthe node feature vectors xi constructed in this paper already exist in a larger latent space,\nand that a simple linear layer is used for dimension reduction? Could this operation\ninterfere with subsequently capturing homogenous and heterogeneous dependencies\nwithin the scope?\n3. In the ordering process, if multiple nodes have the same degree or the same number of\nrelevant metapaths, how should they be handled?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents HeteGraph-Mamba (HGMN), a novel heterogeneous graph learning framework that uses Selective State Space Models (SSSMs) to address the limitations of existing graph neural networks. HGMN introduces a six-step process, including graph tokenization and context-aware node ordering, to improve computational efficiency and capture long-range dependencies across heterogeneous nodes. Experimental results indicate that HGMN achieves higher accuracy and efficiency than other state-of-the-art models on several benchmark datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1\uff1aThe integration of SSSMs in heterogeneous graph learning is innovative, addressing scalability and long-range dependency challenges not fully tackled by traditional Transformer-based models. HGMN\u2019s tokenization and ordering mechanisms for heterogeneous nodes offer a fresh approach to sequence modeling in graphs.\nS2\uff1aThe six-step processing flow in HGMN separately addresses homogeneity and heterogeneity dependencies, enhancing the model's expressive capability. Additionally, the intra-type and inter-type node ordering improves the accuracy of context capture.\nS3\uff1aThe paper provides a detailed overview of each component of the HGMN framework, making it easy to follow the flow and understand the contributions of each design choice.\nS4\uff1aHGMN\u2019s performance improvements over 19 baseline models on accuracy and efficiency metrics demonstrate the potential impact of this approach in the heterogeneous graph learning domain."
            },
            "weaknesses": {
                "value": "W1. The motivation for using Mamba on graphs is unclear. Mamba is faster than Transformer only when modeling sequences longer than 2k.\nW2. Although the paper demonstrates the superiority of the experimental results, it lacks sufficient theoretical analysis of the proposed method, especially in the theoretical basis for improving the efficiency and accuracy of SSSMs. It is recommended to add more mathematical analysis or complexity proof.\nW3. The previous experiments only verified the effect of HGMN in the node classification task. The applicability and performance of other graph tasks (such as edge prediction and graph classification) have not been tested, which affects its wide applicability.\nW4. The model hyperparameters are not explained enough, and the analysis of the functions of each module is also lacking. It is recommended to increase the analysis of hyperparameters and add ablation experiments."
            },
            "questions": {
                "value": "Q1: Why was Mamba chosen for graphs? Mamba shows improved efficiency over Transformers when modeling sequences longer than 2K tokens, based on current knowledge. Providing a theoretical analysis to support this choice in graph contexts would add significant value to the paper.\nQ2: Are all six processing steps necessary? A more detailed ablation study on each step would better clarify the contributions and necessity of each component in the model.\nQ3: Performance on tasks beyond node classification: Can the framework achieve similar high performance on other tasks beyond node classification?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces HGMN, the first to apply selective state space models (SSSMs) to heterogeneous graph learning. HGMN captures long-range dependencies and adapts SSSMs for heterogeneous data. It outperforms 19 state-of-the-art methods in both accuracy and efficiency."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper's description of the model is relatively clear. The language is fluent."
            },
            "weaknesses": {
                "value": "1. The problem statement is unclear. The introduction does not effectively clarify whether the focus is on learning heterogeneous graph representations or capturing long-range dependencies. I suggest the authors clearly delineate the specific focus and differentiate between these two aspects in the introduction.\n\n2. Lack of sufficient motivation. The paper introduces challenges related to selective state space models (SSSMs) but fails to explain the necessity of integrating SSSMs. I recommend the authors elaborate on the limitations of existing approaches and provide examples where SSSMs would be particularly beneficial to enhance the motivation.\n\n3. The experimental results are not convincing:\n\n(1) The paper mentions two recent baselines\u2014LMSPS (Li et al., 2024a) and Graph-Mamba (Wang et al., 2024)\u2014but LMSPS is not included in the experiments. The authors should explain why LMSPS was omitted and discuss its relevance for comparison.\n\n(2) Graph-Mamba focuses on long-range graph sequence modeling rather than heterogeneous graph representation learning. The authors should justify the appropriateness of using it as a baseline.\n\n(3) The paper claims that HGMN performs well on large heterogeneous graphs, yet the four datasets used are all standard datasets, not large-scale ones. I suggest the authors consider using specific large-scale heterogeneous graph datasets to better support their claims regarding HGMN's performance."
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}