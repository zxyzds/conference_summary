{
    "id": "bKQJzuBSRJ",
    "title": "NegMerge: Consensual Weight Negation for Strong Machine Unlearning",
    "abstract": "Machine unlearning aims to selectively remove specific knowledge from a model. Current methods, such as task arithmetic, rely on fine-tuning models on the forget set, generating a task vector, and subtracting it from the original model. However, we argue the effectiveness of this approach is highly sensitive to hyperparameter selection, necessitating careful validation to identify the best model among many fine-tuned candidates. In this paper, we propose a novel method that leverages all given fine-tuned models rather than selecting a single one. By constructing task vectors from models trained with varied hyperparameters and merging only the components of the task vectors with consistent signs, we perform unlearning by negating the merged task vector from the original model. Given that existing methods also utilize multiple fine-tuned models, our approach delivers more effective unlearning without incurring additional computational costs. We demonstrate the effectiveness of our method on both vision-language models and standard image classification models, showing improved unlearning performance with minimal degradation on the retain set, outperforming state-of-the-art techniques.",
    "keywords": [
        "Machine Unlearning",
        "Image Classification",
        "Model Merging"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "NegMerge is a novel machine unlearning method that computes task vectors from multiple models, combining only elements with consistent signs to effectively induce forgetting while preserving the model's retained knowledge.",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=bKQJzuBSRJ",
    "pdf_link": "https://openreview.net/pdf?id=bKQJzuBSRJ",
    "comments": [
        {
            "summary": {
                "value": "This paper focuses on the machine unlearning problem, which aims to selectively remove knowledge from a pre-trained model without retraining from scratch. Instead of subtracting a single task vector from a pre-trained model, this paper proposed to use multiple task vectors with different hyperparameters. Multiple task vectors are merged by averaging elements with the same signs, while the remaining elements with different signs are replaced with zeros. Experimental results showed the effectiveness of merging multiple models during the unlearning process. More importantly, the elements with the same signs have more impact than the elements with conflict signs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* This paper tackles an important and relatively new topic of machine unlearning. It would be very impactful to remove specific data slices from a pre-trained model, without retraining from scratch due to the training cost. \n* This paper proposed an intuitively simple idea to merge multiple fine-tuned task vectors into a single one. Empirical results showed the effectiveness. \n* Not only the classification results are reported, but also the Membership Inference Attack (MIA) metric is used to assess privacy protection. This could be more useful in practice than just checking classification results."
            },
            "weaknesses": {
                "value": "* The core idea of using task vector negation for machine unlearning was proposed in (Ilharco et al., 2022). The idea proposed in this paper is a bit incremental, though coming up with this simple idea could still be non-trivial. It would be more convincing to augment this paper with more in depth discussion and analysis of why the proposed method could work.\n* Evaluation of a few design choices are missing, please refer to the following questions section for more details."
            },
            "questions": {
                "value": "* Could you provide stats about how much ratio of the merged task vector becomes ratio? Especially when tuning the number of task vectors (e.g. 5, 10, 20, 30), would that have a significant impact on the zero ratio of the merged vector?\n* How do you decide on the hyperparameters? For example, augmentation variants are used for CLIP finetuning but weight decay / epochs / label smoothing are used for standard image classifications. Is it sensitive to choose the hyperparameters to create a set of task vectors? What're the recommendations for a practitioner to create variants?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper extends the existing work of machine unlearning by generating a task vector based on a forget set for finetuning and subtracting it from the original model. The new proposal gives an approach to merge finetuned candidates by merging only the components of the task vectors with consistent signs. \nOne advantage of the approach is computational efficiency in the merging step. \nThe model both shows improvements on unlearning tasks and maintains performance on the retain set."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper has sufficient numerical results to support the claim for advantages of the proposal on several tasks, including better unlearning, less degradation on the retain set, and computation efficiency. These experiments show concrete evidence, especially the appendix. \nFor \u201crelationship with TIES-merging,\u201d it is stated that \u201celements with inconsistent signs across task vectors are more closely related to the retain set than the forget set.\u201d This is an essential claim for this paper, which is indicated by aggregated metrics of two methods and visualization of model activation during inference."
            },
            "weaknesses": {
                "value": "The evaluation set of forget and retain do not have clear separability and they may have overlap - ImageNet still requires knowledge relevant to the eight tasks of fine-grained datasets and the CIFAR-10 is sampled randomly. Though the higher accuracy for the current retain set can still be a positive evidence, some eval sets completely irrelevant to the forget set may be more strong."
            },
            "questions": {
                "value": "The paper shows the feasibility of the theory. My personal intuition is that the most promising application of machine unlearning is safety and privacy. However, the experiments focus on some functional recognition and, thus, have a retain eval set with overlap with the forget set. Is there any reason for not having related experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method for machine unlearning in deep learning models. Unlike existing methods that rely on a single fine-tuned model's task vector, NegMerge combines task vectors from multiple fine-tuned models trained with different hyperparameters, keeping only elements with consistent signs. This approach enables robust forgetting of the forget set while minimizing the impact on the retain set."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Easy to follow\n* Simple yet effective performance : intuitive approach \n* Experiments with different archihtectures (e.g CLIP and ResNet) which are commonly used."
            },
            "weaknesses": {
                "value": "* Lack of technical contribution \n: They introduce practical approach but still lacks the theoretical depth. The paper may need for more analysis why sign-consistency aligns with forget set or exploring alternative merging method. \n* Reliance on empirical evidence\n: As mentioned in Section 4.3, the paper relies on empirical results regarding performance on the forget set and retain set. The experimental results show that while the method achieves effective unlearning on the forget set, it compromises performance on the retain set, showing degradation in preserving non-targeted information compared to baselines. Without further analysis, it remains unclear whether these findings generalize across diverse datasets and model architectures."
            },
            "questions": {
                "value": "* As mentioned in the paper, only the final layer of CLIP's text encoder remains frozen during fine-tuning. Given that the CLIP model\u2019s image-text alignment might still influence the unlearning process, I wonder if unfreezing the final layer would provide any additional benefits. Was this approach tested, or was it determined that freezing the final layer would not significantly impact the results? If so, what was the reasoning behind this choice?\n\n* The method relies on multiple fine-tuned models, which can increase memory costs due to the need of several sets of weights. Compared to baselines, could you compare how computationally efficient this approach is and whether this efficiency is associated with performance improvements across the models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The article introduces a model aggregation method for unlearning, which relies on task arithmetic. It uses multiple sets of hyperparameters to generate several task vectors for a single forget task, then merges them. During this merge process, parameters with sign conflicts are discarded, as they show low correlation with the forget set. Experiments demonstrate that this method is more effective in unlearning compared to other model merge techniques."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "*   The paper proposes aggregating multiple models in unlearning, which is more advantageous than using a single model.\n*   The method is simple and effective."
            },
            "weaknesses": {
                "value": "*   The paper lacks an explanation of why the proposed method specifically targets unlearning. From the description, it seems more like an optimization method for task vectors.\n\n*   Naturally, this raises further questions about how this model merge method performs on other tasks, such as multi-task adding and task analogies in task arithmetic.\n\n*   The comparison of computational complexity is puzzling. The paper discusses the complexity of the validation process, but the training process's complexity is of greater concern. Moreover, for vanilla task arithmetic, multiple models are not used, so the (O(mn)) notation is misleading."
            },
            "questions": {
                "value": "*   The paper uses an averaging method to obtain the final task vector. Has there been any exploration of other methods, such as taking the maximum (similar to MagMax) or minimum values?\n*   Why was RandAugment used for the CLIP model instead of varying hyperparameters?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}