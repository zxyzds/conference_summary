{
    "id": "GG80jy9KI5",
    "title": "Strong denoising of financial time-series",
    "abstract": "In this paper we introduce a method for improving the signal to noise ratio of financial data. The approach relies on combining a target variable with different context variables and using auto-encoders (AEs) to learn reconstructions of the combined inputs. The idea is to seek agreement among multiple AEs which are trained on related but different inputs for which they are forced to find common ground. The training process is set up as a conversation where models take turns at producing a prediction (speaking) or reconciling own predictions with the output of the other AE (listening), until an agreement is reached. This leads to \"mutual regularization\" among the AEs. Unlike standard regularization which relies on including a complexity penalty into the loss function, the proposed method uses the partner network to detect and amend the lack of generality in the data representation. As only true regularities can be agreed upon by the AEs, the replication of noise is costly and will therefore be avoided.",
    "keywords": [
        "mutual learning",
        "regularization"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "a regularization scheme for auto-encoders where two networks compare related but different input data",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=GG80jy9KI5",
    "pdf_link": "https://openreview.net/pdf?id=GG80jy9KI5",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a method for learning denoised representations of financial time series. They use distinct autoencoders (AEs) that mutually regularize each other\u2019s learned representations through a collaborative process. To learn these representations, the authors propose training AEs to reconstruct distinct input features corresponding to the same underlying time series (e.g., 10-year treasury yield and cyclically adjusted price/earnings ratio), where the decoder of one AE takes as input the \u201ctranslated\u201d hidden states of the other AE. This translation is done via an attention layer that associates the two hidden states together via high-similarity. The authors validate their approach with  empirical results showing that their method effectively identifies profitable trading strategies based on denoised relationships between market returns and various macroeconomic indicators."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Novel Approach to Denoising**   \nThe paper introduces a novel concept of using a \"conversational\" framework with two distinct AEs and translation layers for mutual regularization. This approach differs from traditional methods and offers a potentially powerful way to filter out noise by seeking agreement between independently trained AEs\n\n**Intriguing Application to Trading Strategy Discovery**   \nThe application of this denoising technique to discover trading strategies is a compelling demonstration of its potential. The idea of identifying \"typical\" patterns in denoised context variables to predict future market movements is interesting and could have practical implications.\n\n**Focus on Practical Relevance**   \nThe paper acknowledges the challenge of the low signal-to-noise ratio in financial data and attempts to address it with a method that aims to extract real market inefficiencies for profitable trading. This focus on practical relevance and potential impact on financial markets is a strength"
            },
            "weaknesses": {
                "value": "**Limited evaluation**  \nThe empirical results are based on a specific set of context variables and a limited time period. It's unclear how well this method generalizes to other markets, asset classes, or timeframes. More extensive experiments and robustness checks are needed to assess the generalizability of the findings. \n* The paper relies heavily on the profitability of the discovered trading strategies as evidence of successful denoising. While this is an interesting application, a more direct and quantitative evaluation of the denoising performance itself is necessary. Comparing the denoised outputs to a benchmark or using metrics specific to time-series denoising would strengthen the claims\n\n**Lack of baselines / comparison to related works**  \nSimilarly, I was unable to track the progress of this work versus prior works discussed in the related work. Did the authors run their evaluation on similar denoising methods?\n\n**Lack of clarity / justification in the method**  \nIt was hard for me to really understand the method and its justifications. While the paper describes the architecture of the AEs and the training process, the implementation and function of the \"translator\" remain unclear. A more detailed explanation of how this component works, including the choice of kernel function and the impact of its non-trainable nature, would be appreciated. After multiple readings, it was still unclear to me how exactly the two autoencoders could learn regularized representations. \n* For example, if AE1 and AE2 were both trained with a \"self-reconstruction\" term MSE_self and \"cross-reconstruction\" term MSE_cross, then the presented intuition would have made sense, i.e.,:\n  * MSE_self = (AE1(x1) - x1)^2 + (AE2(x2) - x2)^2  \n  * MSE_cross = (Decoder1(z2) - x1)^2 +  (Decoder2(z1) - x2)^2)  \n  * AE1(x) = Encoder1(Decoder1(x))  (same for AE2)\n  * Encoder1(x1) = z1 (same for Encoder 2)     \n* But I could not tell if such a setup was being used, and the motivation behind instead using an attention mechanism felt unclear to me.\n\nSimilarly, how is y learned?"
            },
            "questions": {
                "value": "**Method Clarity**  \nCan the authors clarify where $\\bar{z}_1$ and $\\bar{z}_2$ come from? I was confused by this because their definition was deferred in their initial introduction (L170), but this was not defined in the next section. \n\nHow exactly does the attention mechanism with key-value pairs work? What is the rationale behind using the specific kernel function in equation (7)? Why the RBF kernel instead of the usual exponential over dot products? Why is the translator non-trainable?\n* Suggestions: To improve clarity, I'd include:\n  * a step-by-step explanation of the attention mechanism and its implementation.\n  * Justification for the choice of kernel function and any hyperparameters involved.\n  * Clarification on why the translator is non-trainable and how this affects its performance.\n  * A redrawn visual diagram to illustrate exact mapping between x1, x2, z1, z2, and y\n\n\n**Experiments**  \nIs there a way you can quantify the effect of denoising? I wasn't quite sure how to interpret the visuals in Figure 5 \n\n\n\n**Presentation nits**  \nWhat are the x and y-axis in figure 2 correspond to? x-axis is training steps? As nit, would be helpful to label in figure."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel method for enhancing the signal-to-noise ratio in financial time-series data through a unique application of auto-encoders (AEs). Unlike traditional methods, which often rely on regularization by complexity penalties, this approach uses a dual-AE system where two AEs iteratively refine their reconstructions through a conversational process, leading to mutual regularization. By enforcing agreement between the networks, only genuine data regularities are captured, reducing noise. The authors validate the approach through experiments showing the effectiveness of this denoising technique in creating interpretable signals, benefiting systematic trading strategies."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper introduces an innovative regularization mechanism, where two AEs mutually regularize each other\u2019s representations. This approach is novel and avoids the drawbacks of traditional regularization methods.\n\nThe methodology is detailed, with a clear description of the interaction between the AEs and their translation layers. Additionally, empirical results on financial time-series demonstrate how the denoised signals can be applied to real-world trading scenarios.\n\nThe paper is generally well-written, especially in the sections explaining the conversational setup between the AEs and how mutual agreement results in effective noise reduction.\n\nThe approach could significantly impact finance by improving time-series predictability and revealing hidden data patterns, contributing to more robust trading strategies. The mutual regularization approach could also have broader applications beyond finance, potentially in any domain involving high-noise data."
            },
            "weaknesses": {
                "value": "The dual-AE setup may introduce additional **computational overhead**, especially with high-dimensional data. The paper does not discuss scalability or computational trade-offs.\n\nThe paper lacks **quantitative metrics** to objectively evaluate the performance of the denoising method. Only qualitative results are presented. Additionally, there is no **comparison with other state-of-the-art** denoising techniques, such as those based on wavelet-thresholding or stacked autoencoders. Including these would provide a clearer understanding of how the proposed method stands relative to existing approaches in terms of accuracy and denoising effectiveness.\n\nThe authors do not provide the **code** implementing the method presented in the paper, making it difficult to reproduce the results."
            },
            "questions": {
                "value": "Could the authors provide a complexity analysis or benchmarks comparing the proposed dual-AE approach with single AE models? Understanding the trade-offs in terms of training time or computational resources would be valuable.\n\nThe model employs several parameters (e.g., layer configurations, mutual regularization strength). Could the authors discuss the sensitivity of the model\u2019s performance to these parameters? It would be helpful to know which parameters have the most impact on denoising effectiveness.\nTo strengthen the evaluation, it would be beneficial to include quantitative metrics to objectively assess the model\u2019s denoising effectiveness. \n\nAdditionally, comparing the proposed approach with other established denoising techniques (e.g., wavelet thresholding, stacked autoencoders) would contextualize its performance and help highlight any unique strengths."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses a method for modeling time-dependent signal data, such as financial time series. The method employs two autoencoders with different network architectures, assuming they learn different features. The emprical results are demonstrated mainly on S&P500 data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "This work presents a new method to tackle the problem of time series prediction."
            },
            "weaknesses": {
                "value": "- I find some of the components of this paper not well-defined. Terms \"listen\" and \"speak\" are fine to use as long as they are well defined. I understand the intuition but cant verify if what I understand is correct.\n\n- Some components of this paper are not well-defined. Terms like \"listen\" and \"speak\" are acceptable as long as they are clearly defined. While I understand the intuition, I cannot verify if my understanding is correct. \n\n- There is a lack of quantitative results (I am not familiar with financial benchmarks). The authors present strategies obtained by the model in Figure 7, which, from my perspective, seem to mostly predict following the present (i.e., the prediction is a one-day shift from the ground truth). This observation might be incorrect, but I cannot verify it. Therefore, I am missing a few metrics that indicate the model's quality, such as the profit over 1 or 5 years compared to simply staying invested in the S&P 500.\n\n- The paper includes a lot of details that are obvious at this point, such as reparameterization trick derivations.\n\n- I see no reason to use this method exclusively on financial data. While financial data is a challenging domain, it would be beneficial to see this method applied to more popular modalities or benchmarks.\n\n- There is no comparison to prior works.\n\nI could not find any code attached to this work. I believe that providing code that reproduces the paper's results adds significant value to any research work."
            },
            "questions": {
                "value": "What is the difficulty of training such method? It seems like the models may collapse."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a  method using autoencoders to improve the signal-to-noise ratio in financial time-series data. The approach involves using two AEs that collaborate through a \"conversation,\" alternating between producing predictions and reconciling differences to enhance data representation. The goal is to extract meaningful features and denoise financial data, which has a  low signal-to-noise ratio. Experiments suggest that this  regularization approach supports finding market regularities and helps the development of new trading strategies."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Originality**: The paper introduces a method for enhancing financial data representations by using a regularization between two auto-encoders, hereby I think the conversational aspect is novel.\n\n**Clarity**: The methodology, especially the \"speaking and listening\" phases of the auto-encoders, is well explained\n\n**Significance**: In general i think the work could have significant implications for more robust trading strategies. The proposed approach could lead to new insights in financial forecasting by enhancing the clarity of underlying market patterns."
            },
            "weaknesses": {
                "value": "My background is not in the context of financial time-series, however I really struggled understanding what the authors try to describe with their provided figures, e.g., what is a strat and mkt? In the following i outline some areas which might allow for improvements\n\n**Figure Quality**: \n- In general, the figures in the manuscript need a significant revision\n- Figure 2 uses hand-drawn letters, which is not appropriate for a venue like ICLR.\n- Axis labels are overlapping, and there is no legend for the blue, red, or black series in Figure 6.\n- Figure 4 does not provide substantial value; a table would suffice. Additionally, the use of color in the layer names is not explained, making it misleading.\n- Figure 3 lacks axis labels, which makes interpretation difficult.\n\n**Reproducibility**: The absence of a code or detailed procedural steps limits the reproducibility of the experiments. Providing a link to code or pseudocode would help mitigate this issue.\n\n**Experimental Scope**: The experimental dataset used (around 2000 samples) is relatively small, especially in the context of financial data. Broader testing with larger datasets is needed to substantiate the claims made in the paper. Additionally, I think qualitative evaluations alone are insufficient in this case. A more thorough quantitative evaluation is required. Showing improvement over older/other denoising methods (e.g., traditional filters or more advanced nn approaches) would provide stronger support for the novelty and effectiveness of the proposed method.\n\nMethod Justification: The design choices are not well-supported by ablation studies or references to related literature. It would be helpful to mention similar concepts like Siamese Networks (e.g., Dong et al.) and mutual information alignment by Lee et al.\n\nRelated Work: Basing on the pure number of references and the amount of demand in financial forecasting (as the authors motivated), i do not believe the related work is nowhere near exhaustive.\n\n### References:\n\nLee et al., Soft Contrastive Learning for Time Series (ICLR 2024)\n\nDong et al., TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling (2024)"
            },
            "questions": {
                "value": "1. Could you clarify why you think the test on this relatively small dataset provides any signifiance?\u00a0\n\n2. How would the proposed method perform against similar approaches? Are there particular benefits that could be highlighted more explicitly?\n\n3. Is there a plan to release the code for reproducibility?\n\n4. What exactly do we see in Figure 3? Validation Loss?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}