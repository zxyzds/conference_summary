{
    "id": "ewBe3QpCyY",
    "title": "Dataset Condensation with Sharpness-Aware Trajectory Matching",
    "abstract": "Dataset condensation aims to synthesise datasets with a few representative samples that can effectively represent the original datasets. This enables efficient training and produces models with performance close to those trained on the original sets. Most existing dataset condensation methods conduct dataset learning under the bilevel (inner and outer loop) based optimisation. However, due to its notoriously complicated loss landscape and expensive time-space complexity, the preceding methods either develop advanced training protocols so that the learned datasets generalise to unseen tasks or reduce the inner loop learning cost increasing proportionally to the unrolling steps. This phenomenon deteriorates when the datasets are learned via matching the trajectories of networks trained on the real and synthetic datasets with a long horizon inner loop. To address these issues, we introduce Sharpness-Aware Trajectory Matching (SATM), which enhances the generalisation capability of learned synthetic datasets by minimising sharpness in the outer loop of bilevel optimisation. Moreover, our approach is coupled with an efficient hypergradient approximation that is mathematically well-supported and straightforward to implement along with controllable computational overhead. Empirical evaluations of SATM demonstrate its effectiveness across various applications, including standard in-domain benchmarks and out-of-domain settings. Moreover, its easy-to-implement properties afford flexibility, allowing it to integrate with other advanced sharpness-aware minimisers. We will release our code on GitHub.",
    "keywords": [
        "dataset condensation",
        "meta-learning"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ewBe3QpCyY",
    "pdf_link": "https://openreview.net/pdf?id=ewBe3QpCyY",
    "comments": [
        {
            "summary": {
                "value": "This paper enhances the generalization capability of dataset condensation by minimizing sharpness in the outer loop of bilevel optimization. Specifically, it introduces Sharpness-Aware Trajectory Matching (SATM), a variant of trajectory matching. SATM jointly minimizes the sharpness and the distance between training trajectories with a tailored loss landscape smoothing strategy. This paper also introduces some techniques to tackle the problems of using SATM, such as the computational overhead, redundancy of the (hyper) gradient calculation, and hyperparameter tuning.\nAccording to the experiments, it achieves the best results in the conventional dataset condensation setting and also shows impressive results in the OOD setting, demonstrating great generalization capability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Generalization capability is a good research topic in dataset condensation, and a few papers study it.\n2. The overall writing and presentation are satisfactory. \n3. SATM and some techniques to tackle the problems when using SATM are novel."
            },
            "weaknesses": {
                "value": "Missing comparisons with SOTA methods, such as DATM, RDED, and CUDD.\nTowards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching\nOn the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm\nCurriculum Dataset Distillation"
            },
            "questions": {
                "value": "How about the results on ResNet-101 and ImageNet-1K."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Sharpness-Aware Trajectory Matching (SATM) as an approach to improve dataset condensation. The goal is to synthesize representative samples from large datasets, reducing both computational costs and training time. SATM enhances generalization by minimizing the sharpness in the outer loop of bilevel optimization and uses efficient hypergradient approximation techniques to control memory and computational costs. The method outperforms existing condensation approaches across in-domain and out-of-domain benchmarks, particularly when used in conjunction with sharpness-aware minimization methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. SATM\u2019s use of sharpness-aware trajectory matching addresses generalization issues in previous dataset condensation methods and introduces a theoretically sound approach to managing computational complexity.\n\n2. The proposed hypergradient approximation strategies reduce computational overhead, making SATM adaptable and efficient.\n3. SATM demonstrates robust performance gains over other state-of-the-art methods across various benchmarks and settings.\n4. SATM is compatible with other sharpness-aware optimizers, making it adaptable to a wide range of machine-learning tasks."
            },
            "weaknesses": {
                "value": "1. **Proposition 3.1**: Could the authors clarify whether $\\alpha J$ is greater than or less than 1? If $\\alpha J > 1$, the inequality appears incorrect since the norm cannot be bounded by a negative value on the right-hand side. If $\\alpha J < 1$, a smaller $\\iota$ would yield a tighter bound. Therefore, a discussion on the trade-off between the number of truncated steps and performance is needed. An ablation study on this trade-off would add valuable insights.\n\n2. **Theorem 3.2**: $\\theta$ should be treated as a vector, so $\\Delta \\theta_\\tau$ is also a vector, not a scalar. Additionally, the bound on $||\\Delta \\theta_\\tau||$ makes more sense to me, and the upper bound of $\\Delta \\theta_\\tau$ does not necessarily imply that $\\theta_\\tau$ is close to $\\hat{\\theta}_\\tau$. \n\n3. **Sharpness-Aware Minimization**: Sharpness-aware minimization is known to impose a significant computational burden. The authors argue that minimizing sharpness enhances generalization, but could they elaborate on why sharpness minimization is particularly beneficial in the context of dataset condensation? Further insights on this aspect would strengthen the motivation behind using sharpness-aware methods here.\n\n4. **Formatting Issues**: Ensure there is a space between \"min\" and $\\mathcal{L}$; additionally, Figure 1 currently displays an incomplete label for \"training iteration.\""
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This article mainly builds on the MTT method, which aims to make a network mimic the training patterns of real data by ensuring that the paths (or parameter trajectories) created with synthetic data align with those from real data. The improvement here is adding a \"sharpness-smoothing\" strategy with Bayesian Optimization (BO) to approximate sharpness, plus a few other gradient tweaks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This article does a solid job of exploring optimization theory, with most of the formula derivations being accurate. The experimental setup is also fairly thorough."
            },
            "weaknesses": {
                "value": "This article seems like a patchwork of various innovative points. The main concern is that the baseline experimental data is identical to that in the reference paper, Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation. While it's understandable for the data to be similar, it raises the question: was the author involved in the previous paper? If so, please clarify this connection."
            },
            "questions": {
                "value": "1. The innovation in the method isnt major. It mostly combines BO with MTT. There's no new data compression method, just some changes to the optimization steps.\n2. The optimization tweaks are pretty basic too, mostly sticking with classic gradient truncation.\n3. For improving data compression, the focus should be on synthesizing effective datasets to cut down the need for lots of data, not on cutting the cost of loop optimization.\n4.Also, for experiments with DC, DM, MMT, etc., the results are the same as those in the paper \"Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation.\" Please list specific settings like the learning rate."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a new approach of the Dataset Distillation method with Sharpness-Aware Minimisation (SAM)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The theoretical analysis and proofs are sufficient. The authors detailed the approach of Matching Training Trajectory (MTT) with SAM. The proof of the theorem is provided and technically sound. \n\nThe manuscript is well-structured."
            },
            "weaknesses": {
                "value": "Though the proposed approach is technically sound, my main concern is that the experiments can\u2019t support the claims of the proposed model. Specifically,\n\n1. The proposed method claims to reduce the computation overhead and the time complexity of the MTT. However, the method doesn\u2019t provide a computational cost comparison with the current methods.\n\n2. The proposed method claims that the memory cost is also reduced. Given that the baseline Tesla [1] mentioned in the manuscript already reduces the memory cost so that the distillation of ImageNet-1K becomes available, evaluating the effectiveness of the proposed method on ImageNet would be beneficial. \n\n[1] Justin Cui, Ruochen Wang, Si Si, Cho-Jui Hsieh. \u201cScaling Up Dataset Distillation to ImageNet-1K with Constant Memory,\u201d NeurIPS 2022."
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes to improve the performance of trajectory-matching based dataset distillation method by using sharpness-aware minimiser (SAM) to flatten the trajectory used for matching.\nAlso, to mitigate the additional budget introduced by SAM, several tricks including truncate unrolling hypergradient and trajectory resuing are proposed."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Good writing, easy to follow.\n2. The author gives comprehensive math proof of how to efficiently integrate SAM into trajectory matching-based distillation method, improving this paper's soundness."
            },
            "weaknesses": {
                "value": "1. Missing comparison with SOTA methods such as DATM [1] in Table 2.\n\n2. Only one ablation study (on sharpness-aware optimization method) is conducted. Authors should also conduct ablation studies on the proposed trajectory resuing and hyper-gradient truncating methods to prove their effectiveness.\n\n3. Poor performance. Compared with FTD [2], which also proposes to use SAM, the performance improvement brought by SATM becomes marginal as IPC increases (less than 0.2% on CIFAR, IPC50). Is this method only effective in low IPC cases?\n\n4. The distillation cost is only calculated in theory. Comparisons of the distillation cost in practice should be included.\n\n5. The evaluation setting is not introduced clearly, whether the zca whitening, EMA, and DSA augment are used?\n\n[1] Towards lossless dataset distillation via difficulty-aligned trajectory matching, ICLR 2024.\n\n[2] Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation, CVPR 2023."
            },
            "questions": {
                "value": "Please see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}