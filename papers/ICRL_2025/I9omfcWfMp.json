{
    "id": "I9omfcWfMp",
    "title": "Is Graph Convolution Always Beneficial For Every Feature?",
    "abstract": "Graph Neural Networks (GNNs) have demonstrated strong capabilities in processing structured data. While traditional GNNs typically treat each feature dimension equally during graph convolution, we raise an important question: \\textit{Is the graph convolution operation always beneficial for each feature dimension?} If the answer is no, the convolution operation on certain feature dimensions can lead to even worse performance than the convolution-free model. In prior studies, to assess the effects of graph convolution on features, people proposed metrics based on feature homophily to measure feature consistency with the graph topology. However, these metrics have shown unsatisfactory alignment with GNN performance and have not been effectively employed to guide feature selection in GNNs. To address these limitations, we introduce a novel metric, Topological Feature Informativeness (TFI), to distinguish between GNN-favored and GNN-disfavored features, where its effectiveness is validated through both theoretical analysis and empirical observations. Building on TFI, we propose a simple yet effective method, Graph Feature Selection (GFS), which processes GNN-favored and GNN-disfavored features using GNNs and non-GNN models, respectively. This approach maximizes the extraction of useful topological information from each feature. Extensive experiments show that applying GFS to $8$ baseline and state-of-the-art (SOTA) GNN architectures across $10$ datasets yields a significant performance boost in $83.75\\%$ ($67$ out of $80$) of the cases. Additionally, we demonstrate that GFS\u2019s improvements are robust to hyperparameter tuning, highlighting its potential as a universal method for enhancing various GNN architectures.",
    "keywords": [
        "Graph Neural Networks",
        "Graph Homophily",
        "Topological Feature Selection"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "We propose a new metric to identify GNN-favored and GNN-disfavored features and use topological feature selection to fuse these features into GNNs, which significantly improves GNNs performance without hyper-parameter tuning.",
    "creation_date": "2024-09-20",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=I9omfcWfMp",
    "pdf_link": "https://openreview.net/pdf?id=I9omfcWfMp",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates the problem of feature selection for graph Convolution Networks (GCNs). It begins by introducing TFI, a metric designed to guide the selection of relevant features. Following that, it introduces GFS, a plug-in method that distinguishes between features that benefit graph convolution and those that do not contribute positively or may even have a negative impact. Then, the two sets of features are processed separately using GCN and Multi-Layer Perceptrons (MLP), respectively. Evaluations on node classification tasks demonstrate the effectiveness of the proposed TFI and GFS."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1 This paper is well-organized and easy to follow. \n\n2 The figures regarding the design motivation and the proposed framework (Figures 1 and 3) are clear.\n\n3 The theoretical proofs are detailed."
            },
            "weaknesses": {
                "value": "1 The proposed feature selection metric TFI, which leverages mutual information between features and labels, is not novel. This is a conventional approach for feature selection, as outlined in [1]. Although the TFI utilizes the features derived from the neighborhood average (AX), the approach is incremental. \n\n2 This paper lacks a comparative analysis of classic feature selection methods, such as [2]. \n\n3 The GPS exhibits limited robustness to the selection of the hyperparameter r.\n\n4 It is unclear how the high-pass filters of FAGCN and ACMGNN, as presented in Table 1, align with the TFI metric with low-pass AX.\n\n5 A significant concern is the applicability of the proposed GFS. The paper asserts that TFI is computed on training nodes. Thus, I am concerned that the unusual dataset division of 50/25/25 for training/validation/testing employed in this paper is crucial for the effectiveness of GFS. The question then becomes: how would the results be affected if a public splitting, such as 20 per class for training in GCN, SGC, GAT, and APPNP, were applied?\n\n[1] Feature Selection: A Data Perspective. ACM Comput. Surv. 50(6).\n\n[2] Multi-label Feature Selection via Global Relevance and Redundancy Optimization. IJCAI 2020."
            },
            "questions": {
                "value": "See Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a new method to identify features that are favored and disfavored by GNNs. It uses topological feature selection to integrate these features into GNNs, leading to significant improvements in their performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is well-motivated and presents a novel approach to distinguish between GNN-favored and GNN-disfavored features, treating them separately to learn with different methods.\n2. The paper is clearly presented and includes solid theoretical guarantees, making it easy to follow.\n3. The authors provide sufficient empirical analysis, including ablation studies and comparisons with state-of-the-art methods."
            },
            "weaknesses": {
                "value": "The evaluation method is not fair enough. The performance results in Table 1 should be presented with only the difference of with or without GPS, while keeping the model architecture the same (e.g., number of layers and hidden dimension)."
            },
            "questions": {
                "value": "1. As shown in Figure 2, the performance gap for the Roman dataset between GCN and MLP in regions with low TFI is minimal. How is the improvement over GCN in Figure 4 so significant regardless of $r$?\n2. Since features are updated in each GNN/MLP layer, is the TFI computed at each layer to get (dis)favored features, or is it only computed at the beginning? This point does not seem clearly explained in the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "GNNs have strong graph learning capabilities. However, GNNs are not effective at learning all graph data features. In previous work, many metrics based on graph topology and features have been proposed to describe feature homogeneity in order to assess the effectiveness of GNNs. However, there has been no metric that can directly and effectively guide which features GNNs should learn. To fill this gap, this paper proposes an evaluation metric, Topological Feature Informative (TFI), to compute the learnability of features for GNNs. Subsequently, a dual-channel embedding architecture combining GNNs and MLPs is used to embed these features separately."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The writing of the paper is clear and easy to understand, with a well-defined theme.\n2. The paper provides a theoretical foundation for using TFI to guide feature selection through simple proofs.\n3. The experimental results on node classification tasks are impressively good."
            },
            "weaknesses": {
                "value": "The paper does not provide any statistical analysis of the selected features, which raises some questions."
            },
            "questions": {
                "value": "1. How is the decomposition of node features in the graph executed? \n2. What impact does the dimension of the initial node features have?\n3. Can this method only be applied to node features, or can it also be used on edge features, and what would be the effect?\n4. Is it possible to measure what a \"good heterophily\" feature looks like? Can other metrics be used for supplementary description, such as Label Homophily, Feature Homophily, Mutual Information, and so on?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores the varying impact of graph convolution across different feature dimensions in GNNs. It introduces a novel metric called Topological Feature Informativeness (TFI), designed to identify features that are either favored or disfavored by GNNs. The authors propose a Graph Feature Selection (GFS) method that leverages TFI to process GNN-favored features with GNNs and GNN-disfavored features with MLPs, enhancing overall performance. The experimental results across multiple GNN architectures and datasets demonstrate that GFS improves accuracy with minimal computational overhead. The work emphasizes the importance of feature-aware processing in GNNs, showing that graph convolution is not uniformly beneficial for all feature dimensions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1: The concept of distinguishing between GNN-favored and GNN-disfavored features is interesting and novel. \n\nS2: The paper is well-written and structured, easy for readers to follow the methodology and findings. \n\nS3:  The experimental results consistently demonstrate the effectiveness of the proposed GFS trick."
            },
            "weaknesses": {
                "value": "W1: As far as I am concerned, the theoretical analysis of the proposed strategy is incomplete. The paper does not clearly explain the specific conditions under which distinguishing between GNN-favored and GNN-disfavored features is most effective. \n\nW2: The proposed TFI metric, while beneficial, appears similar to existing mutual information-based methods. The authors need to provide more clarification on how TFI uniquely contributes to feature selection in graph learning, emphasizing its distinct advantages over prior approaches.\n\nW3: Although the experimental results demonstrate strong performance of the GFS-augmented models, I remain somewhat unconvinced. To strengthen the validity of the findings, it would be helpful to include experiments on synthetic datasets, such as those generated by the SBM model with varying levels of heterophily and homophily. Such experiments could offer more reliable insights into the potential advantages of the proposed methodology."
            },
            "questions": {
                "value": "Q1: I am still unclear about how TFI fundamentally differs from other mutual information-based metrics for feature selection in graph learning. Could the authors provide a clearer explanation of its unique contributions and advantages?\n\nQ2: How does the proposed method handle extremely sparse features, which are common in real-world graphs? Are there specific challenges or limitations in this context?\n\nQ3: It appears that the proposed method offers limited advantages on homophilous graphs compared to its performance on heterophilous graphs. Could the authors provide insights to clarify this discrepancy and explain why the method may be less effective in homophilous settings?\n\nQ4: I am curious about the robustness of the proposed method when applied to noisy graphs, where certain features may contain varying levels of noise. Could the authors provide insights or results on its performance in these scenarios?\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}