{
    "id": "vgMAtJONKX",
    "title": "Towards Accurate Validation in Deep Clustering through Unified Embedding Learning",
    "abstract": "Deep clustering integrates deep neural networks into the clustering process, simultaneously learning embedding spaces and cluster assignments. However, significant challenges remain in evaluating and comparing the performance of different deep clustering algorithms\u2014or even different training runs of the same algorithm. First, evaluating the clustering results from different models in the same high-dimensional input space is impractical due to the curse of dimensionality. Second, comparing the clustering results of different models in their respective learned embedding spaces introduces discrepancies, as existing validation measures are designed for comparisons within the same feature space. To address these issues, we propose a novel evaluation framework that learns a unified embedding space. This approach aligns different embedding spaces into a common space, enabling accurate comparison of clustering results across different models and training runs. Extensive experiments demonstrate the effectiveness of our framework, showing improved consistency and reliability in evaluating deep clustering performance.",
    "keywords": [
        "Internal validation measures",
        "Deep clustering",
        "Clustering evaluation",
        "Unified embedding learning"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vgMAtJONKX",
    "pdf_link": "https://openreview.net/pdf?id=vgMAtJONKX",
    "comments": [
        {
            "summary": {
                "value": "This paper proposed a model that searches the common representations from multiple learned representations of different methods via clustering. Additionally, this architecture can serve as an evaluation metric for comparing various clustering methods. The paper is well-organized and easy to follow. However, I have some concerns. The techniques used in this paper, including all modules and evaluation metrics, do not appear novel."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The whole paper is easy to follow and well-organized.\n\n2. The motivation is clear."
            },
            "weaknesses": {
                "value": "1. The model itself lacks originality; the unified similarity matrix learning module appears to be derived from [1], and the unified embedding space learning module closely resembles IDEC [2].\n\n2. Equation (4) means that $U$ should more closely approximate $S^{(m)}$ as their Euclidean distance decreases. But all $S^{(m)}$ is learned during the optimization process, relying on the unreliable metric to decide their optimization trends, does this point make sense? It could cause performance to depend heavily on how to initialize the weight $w$.\n\n3. Lacking clear evaluation details. The paper does not specify which variables were used to calculate the NMI and ACC scores.\n\n4. Why do results from all spaces sometimes outperform those from the unified space, while in other cases, the unified space outperforms all spaces? Please analyze this point clearly.\n\n5. The t-SNE visualization comparing the unified embedding with the coupled embeddings should be included.\n\n**References:**\n\n[1] Feiping Nie, Jing Li, Xuelong Li, et al. Self-weighted multiview clustering with multiple graphs.\n \n[2] Guo X, Gao L, Liu X, et al. Improved deep embedded clustering with local structure preservation. IJCAI. 2017, 17: 1753-1759."
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new deep clustering evaluation framework, which aims to solve the problem that different deep clustering algorithms are difficult to compare and evaluate in high-dimensional space. Experimental results show that this method outperforms traditional methods in terms of accuracy and consistency of internal evaluation, which helps to more reliably evaluate clustering performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. By unifying the embedding spaces of different models into a common space, the evaluation bias caused by different algorithms or parameters can be reduced, making the evaluation results more consistent.\n\n2. Through experimental verification, the method in the paper shows higher reliability when using internal evaluation indicators (such as Silhouette score, Calinski-Harabasz index, etc.), and is highly correlated with external evaluation indicators (such as clustering accuracy).\n\n3. Compared with traditional embedding methods that require frequent parameter adjustment, the main steps of the unified embedding space method do not rely on specific parameters, are simple to operate and easy to promote."
            },
            "weaknesses": {
                "value": "1) The font of the text in the figure should be consistent with the font of the text;"
            },
            "questions": {
                "value": "1) this work relies on Euclidean distance as a similarity metric. In some deep clustering tasks, other distance metrics (such as cosine similarity) may perform better. Can your evaluation framework maintain consistent results under different similarity metrics?\n\n2) the aothurs focus on preserving the local structure of the data to improve clustering accuracy. However, on some datasets, preserving the global structure may be equally important. In the process of generating the unified embedding space, have you considered balancing the impact of local and global structures? Does this method have limitations on datasets with particularly complex data distribution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a novel internal cluster quality measure for deep clustering algorithms. The key idea is to learn a unified embedding space that algins different embedding spaces learned by deep clustering models into a common space. The unified embedding is then used to compare the different clusterings with commonly used internal cluster evaluation methods, like the silhouette score."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "**Originality**\n- The idea of combining multiple embeddings learned from deep clustering methods to achieve a unified embedding to compare different clustering solutions is interesting.\n\n**Quality**\n- Evaluation across a wide range of diverse data sets and three different internal cluster evaluation methods provides good evidence for their proposed evaluation procedure.\n\n**Clarity**\n- The method description and Figure 1 illustrate the method clearly\n\n**Significance**\n- Internal cluster quality measures are of high significance for the deep clustering community. I would even say that it is one of the most pressing issues that holds back the application of deep clustering algorithms in practice. Currently, almost all deep clustering methods need to be tuned with access to ground truth labels, which is fine for method development, but is not a realistic use case for clustering in practice. Therefore, the presented  work is of high significance to the deep clustering community."
            },
            "weaknesses": {
                "value": "**Originality**\n- Existing work (Figure 4 in Lowe et al, 2024) provides already a large-scale analysis of internal cluster measures (silhouette score) for clustering methods in embedding spaces. Their work shows that there is a strong correlation between the AMI (Adjusted Mutual Information) and the silhouette score computed in the UMAP reduced embedding space. This work should be discussed in the related work section so that it is clear, why the proposed method is necessary and a simple UMAP reduction for each embedding would not work.\n\n**Quality**\n- The selection of DEPICT and JULE for evaluation experiments is not well motivated. There are many more \u201cfoundational\u201d deep clustering methods that are widely used and have inspired many follow-ups, e.g., DEC (Xie et al, 2016), IDEC (Guo et al, 2017), DCN (Yang et al, 2017). Further, only autoencoder-based methods are compared and no recent contrastive methods, like Contrastive Clustering (Li et al, 2021), SCAN (Van Gansbeke et al, 2020) or SeCu (Qi 2023). I understand that it is not feasible to compare with every deep clustering method there is, but the selection of methods in your experiment section should be clearly motivated. For example, take one or two methods from each deep clustering family, like k-means based, hierarchical clustering based, density based\u2026 and with different representation learning objectives, like autoencoder and contrastive learning.\n\n**Significance**\n- My concern with the proposed method is that it might not be very useful in practice, as it requires multiple embedded spaces that need to be learned first with deep clustering methods. This makes it quite expensive to compare clustering solutions.\n\n\n**References**\n\nXie, J., Girshick, R. and Farhadi, A., 2016, June. Unsupervised deep embedding for clustering analysis. In International conference on machine learning (pp. 478-487). PMLR.\n\nYang, B., Fu, X., Sidiropoulos, N.D. and Hong, M., 2017, July. Towards k-means-friendly spaces: Simultaneous deep learning and clustering. In international conference on machine learning (pp. 3861-3870). PMLR.\n\nGuo, X., Gao, L., Liu, X. and Yin, J., 2017, August. Improved deep embedded clustering with local structure preservation. In Ijcai (Vol. 17, pp. 1753-1759).\n\nLi, Y., Hu, P., Liu, Z., Peng, D., Zhou, J.T. and Peng, X., 2021, May. Contrastive clustering. In Proceedings of the AAAI conference on artificial intelligence (Vol. 35, No. 10, pp. 8547-8555).\n\nVan Gansbeke, W., Vandenhende, S., Georgoulis, S., Proesmans, M. and Van Gool, L., 2020, August. Scan: Learning to classify images without labels. In European conference on computer vision (pp. 268-285). Cham: Springer International Publishing.\n\nQian, Q., 2023. Stable cluster discrimination for deep clustering. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 16645-16654).\n\nLowe, S. C., Haurum, J. B., Oore, S., Moeslund, T. B., & Taylor, G. W. (2024). An Empirical Study into Clustering of Unseen Datasets with Self-Supervised Encoders. arXiv preprint arXiv:2406.02465."
            },
            "questions": {
                "value": "- Are the internal cluster measures in the comparison representations (\u201call spaces\u201d, \u201ccoupled spaces\u201d and \u201craw space\u201d) computed in a t-SNE reduced space or in the higher dimensional representation space? \n\n- How many embedding spaces are needed to learn a sufficiently representative \u201cunified embedding\u201d?\n\n- Please justify the selection of JULE and DEPICT for your main experiments. If possible, add further deep clustering methods to your evaluation. See discussed weakness.\n\n- Please explain how your approach relates to the results in Lowe et al. (2024). I would like to see a clear motivation of why your method is needed and a simpler baseline like UMAP reduced embeddings does not work. See also the corresponding discussed weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses challenges in evaluating deep clustering methods, particularly discrepancies in comparing clustering results across different models due to varying learned embedding spaces. The authors propose a novel evaluation framework that introduces a unified embedding space for more accurate comparisons. This unified space aligns embeddings from multiple clustering results into a consistent representation, making internal validation measures more reliable and reducing inconsistencies. The proposed approach is empirically validated across several datasets, demonstrating improved accuracy in ranking clustering results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The authors provide thorough theoretical analyses of the limitations of traditional clustering evaluation approaches. They highlight the pitfalls of using internal validation measures in high-dimensional input spaces due to the curse of dimensionality and demonstrate the inconsistencies that arise when using these measures on coupled embedding spaces generated by different clustering models.\n- The proposed method is evaluated extensively across several benchmark datasets, including MNIST, COIL, UMist, and others. The empirical results consistently show that the unified embedding framework outperforms traditional approaches (i.e., raw space, coupled space, and averaging across all spaces) in terms of rank correlation with external validation metrics."
            },
            "weaknesses": {
                "value": "- The proposed approach resembles multi-view learning methods, particularly in S1 where a fusion weight and unified similarity matrix are learned, and S2 where a low-dimensional multi-view fused embedding is developed. This raises the question: Could most multi-view learning methods achieve similar unified spaces? If so, what differentiates the proposed method from existing multi-view techniques?\n- The quality of the unified embedding space may directly impact the framework\u2019s ability to compare clustering models. If the unified space is not well-learned, how would this influence the reliability of the evaluations?\n- The framework requires several optimization steps, such as learning the unified similarity matrix and the unified embedding space, which may be challenging for large datasets. S1, in particular, might not scale well for massive datasets. How does the proposed approach address these scalability concerns? The authors\u2019 claim that datasets of more than 10,000 samples represent a sufficiently large scale is not convincing\u2014evaluation on larger datasets (e.g., the complete MNIST dataset) is strongly recommended.\n- Comparisons are limited to only two clustering methods. To fully demonstrate the robustness of the evaluation approach, at least three different clustering models should be included."
            },
            "questions": {
                "value": "- How does the proposed method differ from standard multi-view learning methods, particularly those that also learn a unified embedding space by combining multiple views? Would it be possible to benchmark against a few of these existing multi-view learning methods (e.g., Completer, cvpr'21) to clarify the distinctions?\n- Are there any existing clustering evaluation frameworks that could be used as baselines for comparison to better highlight the strengths of the proposed approach?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}