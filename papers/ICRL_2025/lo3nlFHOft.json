{
    "id": "lo3nlFHOft",
    "title": "From Promise to Practice: Realizing High-performance Decentralized Training",
    "abstract": "Decentralized training of deep neural networks has attracted significant attention for its theoretically superior scalability over synchronous data-parallel methods like All-Reduce. However, realizing this potential in multi-node training is challenging due to the complex design space that involves communication topologies, computation patterns, and optimization algorithms. This paper identifies three key factors that can lead to speedups over All-Reduce training and constructs a runtime model to determine when, how, and to what degree decentralization can yield shorter per-iteration runtimes. Furthermore, to support the decentralized training of transformer-based models, we study a decentralized Adam algorithm that allows for overlapping communications and computations, prove its convergence, and propose an accumulation technique to mitigate the high variance caused by small local batch sizes. We deploy the proposed approach in clusters with up to 64 GPUs and demonstrate its practicality and advantages in both runtime and generalization performance under a fixed iteration budget.",
    "keywords": [
        "distributed training",
        "data parallelism",
        "decentralized optimization"
    ],
    "primary_area": "infrastructure, software libraries, hardware, systems, etc.",
    "TLDR": "A system-level design for practical decentralized training in multi-GPU clusters",
    "creation_date": "2024-09-20",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=lo3nlFHOft",
    "pdf_link": "https://openreview.net/pdf?id=lo3nlFHOft",
    "comments": [
        {
            "summary": {
                "value": "The paper investigates decentralized training of DNNs. This used to be a fairly hot topic a few years ago (e.g. Lian et al 2017 and follow-up work), but has to some extent cooled down recently as models and workloads have changed, and model-parallelism has become more standard. \nThe paper focuses on the classic data-parallel setting; its contributions are as follows: \n\n- An analytical model to understand what are the conditions under which decentralized training can bring gains.\n\n- A decentralized version of Adam, which decouples computation from communication and provides convergence guarantees under fairly standard assumptions (for Adam) \n\n- An implementation of the algorithm and its technical evaluation. This is done fairly thoroughly, on systems with up to 64 GPUs, showing the method\u2019s potential."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Strengths:\n\n1. new perspectives on an \u201cold\u201d problem by today\u2019s standards (decentralized training), from the point of view of modeling and adaptive optimization\n\n2. analytical results are a plus\n\n3. the experiments are fairly thorough"
            },
            "weaknesses": {
                "value": "Weaknesses: \n\n1. unfortunately the paper seems to be completely missed some important related work in the area, which makes it very hard to position the paper properly in terms of its contribution \n\n2. more broadly, the experimental results are in a system parameter range that has been rendered somewhat obsolete by current-day systems, which are able to e.g. train ImageNet in minutes on a single node https://github.com/libffcv/ffcv"
            },
            "questions": {
                "value": "Detailed comments and questions:\n\nQ1. The paper seems to ignore work on the decentralized setting since late 2020 to early 2021. Here are some of the many references that are missed:\n\n- There is a lot of nice work by Anastasia Koloskova and co-authors on analyzing Gossip variants of SGD, many of which are missed:\n\n1. Koloskova, Anastasia, et al. \"A unified theory of decentralized sgd with changing topology and local updates.\" International Conference on Machine Learning. PMLR, 2020.\n\n2. Koloskova, Anastasiia, Tao Lin, and Sebastian U. Stich. \"An improved analysis of gradient tracking for decentralized machine learning.\" Advances in Neural Information Processing Systems 34 (2021): 11422-11435. \n\nSee also:\n\nZhang, Jiaqi, and Keyou You. \"Fully asynchronous distributed optimization with linear convergence in directed networks.\" arXiv preprint arXiv:1901.08215 (2019).\n\n\"SQuARM-SGD: Communication-Efficient Momentum SGD for Decentralized Optimization\" by Navjot Singh et al. (2020)\n\nIn addition: \n\n- Nadiradze, Giorgi, et al. \"Asynchronous decentralized SGD with quantized and local updates.\" Advances in Neural Information Processing Systems 34 (2021): 6829-6842.\n\nThis paper seems to be trying to do something similar to what is done here\u2013decoupling communication from computation\u2013but in some sense goes further since it also supports quantization and completely non-blocking reads. Even the experimental setup is very similar to the one presented in this paper, so I am surprised to see that there is no mention of this work whatsoever. \n\n- Similar work: \n Li, Shigang, et al. \"Breaking (global) barriers in parallel stochastic optimization with wait-avoiding group averaging.\" IEEE Transactions on Parallel and Distributed Systems 32.7 (2020): 1725-1739.\n\nI would therefore recommend that the authors go over related work more thoroughly, and provide more complete positioning of their work relative to prior art. I don\u2019t think the paper can be accepted in the absence of this. \n\nQ2: Are the loss improvements quoted in L488 (2.841 vs 2.846) actually statistically significant?\n\nQ3: Is your setup really faster than just setting up large-batch SGD using e.g. FFCV on a single 4-GPU system?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the gap between decentralized training of DNN and data parallel (DP) methods. \nIn particularly, although decentralized training is theoretically superior to DP, certain network topologies make the use of this technique challenging. \nThe present propose to quantify such gap through the use of a \"runtime model\".\nMoreover, the author introduce a decentralized version of Adam particularly suited to the training of transformer based models.\nExtensive numerical experiments illustrate the speedup investigation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This paper proposes to study an important, impactful problem (practical gap between DP and decentralized training)\n- the accuracy of the model (ie how it captures real time speed discrepancies) is well illustrated\n- the numerical experiments are extensive and thorough. There is a clear focus on transformer based architectures, making it relevant to challenges posed by modern LLMs.\n- the version of decentralized Adam optimizer is thoroughly compared to other decentralized adam based methods\n- the papers lists 3 factors that could explain the aforementioned discrepancy  1) overlapping communication and computation 2) heterogeneous communications costs 3) sensitivity to varying computation times. For each factor, an extensive investigation is proposed supported by numerical experiments."
            },
            "weaknesses": {
                "value": "- the explanation/details of the runtime model in Appendix A.5 could be stated more clearly \n(the reviewers would like to respectfully suggest an explanation similar in spirit to the way Algorithm 1 is stated, although this is relatively minor)\n- there is no clear link to the reviewer between the decentralized adam optimizer and the runtime model (please correct me if missed anything)."
            },
            "questions": {
                "value": "The reviewer is curious about the intuition behind the runtime model: were the assumptions behind the model proposed previously by some other work or is this coming solely from the author?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores decentralized training of deep neural networks (DNNs) as an alternative to synchronous data-parallel methods like All-Reduce, which are known for their scalability issues. The authors identify three key factors that can lead to speedups in decentralized training: overlapping communication and computation, respecting heterogeneous communication costs, and reducing sensitivity to varying computation times. They propose a runtime model to optimize decentralized training configurations and design a decentralized variant of the Adam optimizer, called DAdam, which supports overlapping communications and computations. Additionally, they introduce an accumulation technique to mitigate the high variance caused by small local batch sizes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors provide a runtime model that quantifies key environmental parameters and estimates potential speedups, offering valuable insights into the conditions under which decentralized training is advantageous.\n2. There is theoretical analysis to support the proposed algorithm.\n3. The experiment consists of 64 GPUs, which is a suitable scale.\n4. There is detailed hyper-parameter settings for reproducibility."
            },
            "weaknesses": {
                "value": "1. Some important related works in MLsys [1,2] are missed, in which the dynamic gossip communication topologies are also discussed, and the communication compression is utilized.\n2. The novelty of the DAdam is limited. There is no improvements on the Adam itself. The proposed method can be seen as a system optimization instead of the optimizer itself. However, the systematic optimization including the overlapping between communication and computation is largely used in many ML systems [3,4].\n3. While the paper presents extensive experiments, it lacks detailed ablation studies to isolate the impact of individual components of the proposed approach.\n4. The tested topologies are limited.\n5. For training GPT-2 on OpenWebText, it is unclear whether the final convergence of DAdam can match the All-Reduce Adam when training with more iterations. The convergence curves of training on OpenWebText are not provided.\n6. There lacks enough details of how the decentralized training is implemented built upon PyTorch.\n\n\n[1] GossipFL: A Decentralized Federated Learning Framework With Sparsified and Adaptive Communication. In TPDS 2022.\n[2] Communication-efficient decentralized learning with sparsification and adaptive peer selection. In ICDCS 2020.\n[3] Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models. In ASPLOS 2022.\n[4] Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning. In ASPLOS 2024."
            },
            "questions": {
                "value": "Please refer to the weaknesses. \n\nIt would be better to provide training convergence curves of using DAdam and the All-Reduce Adam on OpenWebText. \n\nAlso, it is important to illustrate how the decentralized training is implemented, as the Pytorch and DeepSpeed have conducted many system optimizations on the All-Reduce, there should be some convincing illustrations to show that the implemented decentralized training can outperform the All-Reduce. Specifically, how the communication is launched, using torchrun or MPIRUN or something else? how the synchronization between different nodes is implemented, is there a distributed.barrier()? how the overlapping between communication and computation is implemented? How do you manage the communication topology? The source code with a document may be very helpful.\n\nIf these questions are addressed, I'd like to increase my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}