{
    "id": "o0BEth0Po3",
    "title": "ExID: Offline RL with Intuitive Expert Insights in Limited-Data Settings",
    "abstract": "With the ability to learn from static datasets, Offline Reinforcement Learning (RL) emerges as a compelling avenue for real-world applications. However, state-of-the-art offline RL algorithms perform sub-optimally when confronted with limited data confined to specific regions within the state space. The performance degradation is attributed to the inability of offline RL algorithms to learn appropriate actions for rare or unseen observations. This paper proposes a novel domain knowledge-based regularization technique and adaptively refines the initial domain knowledge to considerably boost performance in limited data with partially omitted states. The key insight is that the regularization term mitigates erroneous actions for sparse samples and unobserved states covered by domain knowledge. Empirical evaluations on standard offline RL datasets demonstrate a substantial average performance increase compared to ensemble of domain knowledge and existing offline RL algorithms operating on limited data.",
    "keywords": [
        "Offline Reinforcement Learning",
        "Knowledge distillation"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "The paper introduces a novel domain knowledge-based regularization technique to enhance offline Reinforcement Learning (RL) performance in scenarios with limited data and partially omitted states.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=o0BEth0Po3",
    "pdf_link": "https://openreview.net/pdf?id=o0BEth0Po3",
    "comments": [
        {
            "summary": {
                "value": "This paper aims to incorporate the domain knowledge in the sparse states where the offline data is not explored much. \nFirstly, the teacher policy $\\pi_t^w$ is trained by using the domain knowledge in the heuristic manner. And then, the offline policy is trained using a proposed critic-regularization term to enhance performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The novel concept: It is not easy to incorporate some general domain knowledge to train the policy. They try to put somehow symbolic control in the discrete action offline RL.\nMethod\n\n- The critic regularization term: To use domain knowledge in the offline RL setting, the proposed regularization term is intuitive and proper."
            },
            "weaknesses": {
                "value": "- The method \u2018Training Teacher\u2019 is somehow naive, and Eq. (2) is not clear. (Actually, I don\u2019t understance what it is)\n\n- Limited domain: It is not applicable in the continuous action space.\n\n- The hyperparameter $\\lambda$, the coefficient constant for critic regularization. There is no exact way to decide the proper value of $\\lambda$.\n\n- Updating the Teacher policy: The explanation for updating the teacher policy, especially after line 226, becomes difficult to follow.\n\n- Writing should be enhanced."
            },
            "questions": {
                "value": "In the regularization term, are there any gradient stop or .detach() (in torch) in Q-network? \nLine 128, The definition of target network parameter $\\theta'$ is missing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents ExID, an offline RL method that enhances policy learning in limited-data settings by using a domain-knowledge-based regularization technique, significantly improving performance over traditional offline RL approaches across diverse datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Innovative to incorporate the domain knowledge. \n2. real-world application in sales promotion dataset and simglucose dataset."
            },
            "weaknesses": {
                "value": "1. Training the teacher network highly requires domain knowledge. \n2. The method is heuristic and plug-and-play in different algorithms. \n3. The simulation tasks are simple."
            },
            "questions": {
                "value": "1. Can this method be applied to a continuous environment? If yes, should the authors try difficult simulated tasks, such as d4rl? \n2. Can this method be applied to different base algorithms, e.g., BEAR, IQL."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this work, the authors propose a new transfer offline RL method ExID, which transfer the learned policy of the source domain into the target domain with a expert system and limit data of target domain."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This work forces on a new and important setting. This setting is make sence because the less the data is, the more the risk is.\n- This work proposes a simple but effect architecture, which update the target network and the teacher at the same time. This architecture let the method can be used with expert systems with different qualities.\n- The final result is good especially when the target data is noisy."
            },
            "weaknesses": {
                "value": "- The presentation of this work is limit. For example, in Section 3, there is \"(s,a,s')\\in B, then s'\\in B\". What exactly the structure of the buffer is? Also, the Eq. 2 is hard to understand at the first time. \n- The environments used in this work are all simple and visible. Can we propose a useful expert system for more complex environments, such as halfcheetah?"
            },
            "questions": {
                "value": "- This work leverages a uncertainty based method to update the teacher network. Is this a stable standard? How many times the teacher network is updated in the learning process?\n- How many feasible s of  Proposition 4.3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper work on incorporating domain knowledge into offline RL to improve OOD performance.\nThe method appears to be based on DQN, but incorporates a \"teacher\" policy network which is used to regularize the argmax Q-function policy."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The use of domain knowledge in RL is worthy of study.\n\nThe results seem to indicate some empirical advantage of the method over baselines (although not consistently so)."
            },
            "weaknesses": {
                "value": "**Unconvincing motivation and contributions**\nThis seems to be an avenue of work that has received very limited attention, and I would appreciate a more thorough motivation for both the problem setting and the approach proposed.\n\nThe environments studied seem pretty simple, and I'm not convinced by the baselines.  \nI'm skeptical that this approach can scale, as it is based on strong domain knowledge. \nThe practical utility on simple environments is also unclear -- can the authors make a more clear and compelling case of when and why they expect this approach to yield practical value?\nI would also expect more discussion of how well this approach can scale to more complicated environments, and what sort of future work might help answer that question.\n\n**Other presentation issues**\n- I found the presentation of the algorithm confusing and presentation to be weak overall.\n- should use citep\n- Definition 4.1 is confusing.\n-- I think the notation in the first condition (bullet point) is used incorrectly.\n-- The 2nd bullet point seems redundant (being guaranteed to hold if the first condition holds).\n\n\n**Inadequate treatment of model-based RL**\nThe work needs a more thorough comparison with model-based offline RL methods.\nCurrently, there is only one experiment reported (in insufficient detail) in the appendix.\nI would expect these baselines to be run for all of the environments and reported in main text.\nAs such, I'm not convinced the authors have made a fair effort to tune and compare their methods with baselines. \nFurthermore, the point of comparison is MOPO, a method from 2020 -- are there more modern methods which ought to be considered?\n\nThe submission also states that for such methods \"performance highly depends on the accuracy of the learned dynamics.\"  But this claim is not supported.  These methods are meant to keep policies from seeking out states where the learning dynamics are inaccurate, and thus, we might expect performance to degrade gracefully in the presence of inaccurate dynamics.  On what basis are the authors claiming otherwise?"
            },
            "questions": {
                "value": "Are there more modern offline RL methods which ought to be considered?\n\nCan your approach be viewed as a form of actor-critic?  Would an actor-critic method be more performant?\n\nWhat are some alternative methods for \n\nDoes D specify a complete policy (I.e. does it have support on the entire state space)?  If so, I think the notation should be pi_D."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an offline RL algorithm, ExID, that can leverage domain knowledge-based heuristic rules to enhance small-sample performance. The proposed algorithm can be seen as combining conservative Q-learning (CQL) and Uncertainty Weighted Actor-Critic (UWAC), and adds a CQL-style domain knowledge-based regularize, i.e., push up Q-values for actions that comply with the domain knowledge and push down Q-values for policy-generated actions. For detailed, comments please refer to strengths and weaknesses."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Leveraging domain knowledge to enhance sample efficiency in offline RL is meaningful.\n- The idea of modeling domain knowledge as tree-based heuristic rules and incorporating them using CQL-style value regularization is interesting. \n- The paper is easy to read. The results show good performance for tasks that have domain knowledge-based rules."
            },
            "weaknesses": {
                "value": "- The literature review is insufficient, missing many recent offline RL works that focus on sample efficiency and OOD generalization.\n- The proposed method has very stringent requirements for the domain knowledge input. Although we can design domain knowledge trees for simple tasks, it is not practical for complex tasks. Typically, in many real-world tasks, we might only be able to write very few heuristic rules for a task, which can only sparsely cover the state-action space. This severely limits the applicability of the proposed method for a wide range of problems. In the experiments, the authors are also only able to evaluate their method in very simple task environments. \n- When training the teacher policy $\\pi_t^w$, a random action is used if a state is not covered by the domain knowledge rules. This can be very problematic, as it will inevitably lead to sub-optimality when regularizing the learned policy. Moreover, if only a small set of domain knowledge rules is given (common in most practical problems), then it will cause the teacher policy to learn lots of random behavior, which could damage policy learning.\n- The algorithm is essentially a combination of CQL and UWAC, with an additional value regularization that penalizes the value for actions that are not consistent with the domain knowledge. Since CQL is already very conservative and does not generalize well. Adding additional conservative regularization will further distort the value function, which could impact the optimality of the learned value.\n- Propositions 4.2 and 4.3 are meaningless and have huge theory-practice gaps. The proposed method combines both CQL and the uncertainty-based method, which makes it almost impossible to draw any reliable theoretical conclusion on the learned value function and the policy. In the proof of Proposition 4.2, the underlying analytical tools used work for any policy, the only thing related to the proposed method is the **assumptions** $\\rho_{\\hat{\\pi}}=\\rho_{\\pi_{u}}-\\Delta_i$ and $\\rho_{\\hat{\\pi}}=\\rho_{\\pi_{u}}-\\Delta_o$, with $\\Delta_i$ and $\\Delta_o$ assumed to be positive, as well as $Q^*(s,\\hat{\\pi}(s))-Q^*(s,\\pi_u(s)) \\approx 0$ for in distribution actions. First, these assumptions have no guarantees from your algorithm. Second, to prove something, you simply cannot introduce some arbitrary assumptions to achieve your purpose. Proposition 4.3 is also trivial and not very meaningful. It only relates to the optimal value function $Q^*$, rather than the learned value function $Q^{\\theta}$. You can only learn $Q^{\\theta}$, and the proposition provides no information to tell if the teacher policy indeed helps to improve the learned $Q^{\\theta}$.\n- There are also some inconsistencies in the symbols used in the paper. For example, the teacher policy is expressed as $\\pi_t^w$ in the text, but written as $\\pi_w$ in Figure 2. The action value function is written as $Q_s^{\\theta}$ in the text, but written as $Q_{\\theta}$ in Figure 2."
            },
            "questions": {
                "value": "- How can the method be used if only limited domain knowledge rules are provided?\n- Is it possible to not use random action for cases that are uncovered by domain knowledge rules?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}