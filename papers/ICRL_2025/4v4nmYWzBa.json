{
    "id": "4v4nmYWzBa",
    "title": "REVISITING MULTI-PERMUTATION EQUIVARIANCE THROUGH THE LENS OF IRREDUCIBLE REPRESENTATIONS",
    "abstract": "This paper explores the characterization of equivariant linear layers for representations of permutations and related groups. Unlike traditional approaches,\nwhich address these problems using parameter-sharing, we consider an alternative\nmethodology based on irreducible representations and Schur\u2019s lemma. Using this\nmethodology, we obtain an alternative derivation for existing models like DeepSets,\n2-IGN graph equivariant networks, and Deep Weight Space (DWS) networks. The\nderivation for DWS networks is significantly simpler than that of previous results.\nNext, we extend our approach to unaligned symmetric sets, where equivariance\nto the wreath product of groups is required. Previous works have addressed this\nproblem in a rather restrictive setting, in which almost all wreath equivariant layers\nare Siamese. In contrast, we give a full characterization of layers in this case and\nshow that there is a vast number of additional non-Siamese layers in some settings.\nWe also show empirically that these additional non-Siamese layers can improve\nperformance in tasks like graph anomaly detection, weight space alignment, and\nlearning Wasserstein distances.",
    "keywords": [
        "deep weight spaces",
        "permutation equivariance",
        "irredicible representations."
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "Characterization of all permutation equivariant linear mappings using irreducibles approach on several use-cases.",
    "creation_date": "2024-09-22",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4v4nmYWzBa",
    "pdf_link": "https://openreview.net/pdf?id=4v4nmYWzBa",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces an alternative approach for characterizing equivariant linear layers in neural networks that process permutation and related group representations. The paper derives a simpler method for obtaining existing models such as DeepSets, 2-IGN, and Deep Weight Space networks, based on irreducible representations and Schur\u2019s lemma. The proposed framework also considers unaligned symmetric sets, that build upon equivariance to the wreath product of groups."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper introduces a fresh perspective on equivariant layer characterization by applying irreducible representations and Schur\u2019s lemma to obtain simplified derivations of established models, such as DeepSets, 2-IGN, and Deep Weight Space (DWS) networks.\n\n2. The theoretical foundations are well-developed. The work provides a complete characterization of equivariant layers in the context of unaligned symmetric sets, which is an interesting theoretical contribution."
            },
            "weaknesses": {
                "value": "1. The presentation and flow of the paper could be improved. The claims and results are challenging to follow, which may limit the broader audience\u2019s ability to appreciate the work.\n\n2. The paper\u2019s contributions lack clarity. The paper offers an irreducible-based derivation for existing results and characterizes equivariant functions on unaligned symmetric elements, but the impact and relevance of these contributions remain unclear. It is not evident how these results benefit the design of novel architectures or enhance our understanding of current ones. This limits the significance of the work and may fall short of ICLR\u2019s standards.\n\n3. The empirical evaluation is limited, and the results are not compelling. Using synthetic data for anomaly detection does not sufficiently demonstrate the method\u2019s practical applicability, as the task is relatively unchallenging and does not show the strengths of the proposed approach."
            },
            "questions": {
                "value": "Please see Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies equivariant linear layers for representations of permutations and related groups from a novel irreducible representations perspective. The authors provide an alternative derivation for models including DeepSets, 2-IGN, and Deep Weight Space (DWS) networks. The theory is then extended to unaligned symmetric sets, showing that there is a vast number of additional non-Siamese layers in certain settings. Experiments show that additional non-Siamese layers improve the performance in tasks like graph anomaly detection, weight space alignment, and learning Wasserstein distances."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper offers the irreducible representations perspective for deriving classical models like DeepSets, 2-IGN and DWS networks. Some derivations are simpler than the original ones. The writing is clear and easy to follow. I check with the details and they are sound."
            },
            "weaknesses": {
                "value": "* While the new derivations align with original methods, the resulting models are not new. The concept of ``irreducible representation'' is also well studied, so the contribution of the paper lies mainly in bridging two topics, which is interesting but natural. In particular for equivariant graph layers, the authors only provide derivations for 2-IGN. As admitted in the limitation section, the paper does not involve higher-order $k$-IGN. The author should explain whether their method is broadly applicable for these networks based on tensor representations, or need case-by-case derivations.\n\n* Although this is a theoretical paper, the experiments could be improved. More baselines and more real-world tasks are strongly encouraged."
            },
            "questions": {
                "value": "* Can the method be generalized to higher-order $k$-IGN in a principled manner? Can you briefly describe the claim that ``using irreducibles could lead to new equivariant models with intermediate irreducible features of lower dimensions''?\n\n* Can you conduct more experiments on real-world and large-scale datasets, and include more baseline? In addition, can you intuitively explain why non-Siamese layers help in these tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel methodology for characterizing equivariant linear layers for permutation representations, utilizing classical results from representation theory. Specifically, it provides an alternative characterization of equivariant linear layers for DeepSets, $2$-IGNs, and DWSNets, as well as the first comprehensive characterization of equivariant linear layers for unaligned symmetric elements. Importantly, the authors identify novel non-Siamese layers and empirically assess their impact."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Clear presentation and notation, supported by rigorous proofs.\n- The methodology is both valuable and simple, with potential to generalize beyond the examples presented.\n- A novel and complete characterization of representations for unaligned symmetric elements."
            },
            "weaknesses": {
                "value": "- Lacks discussion on extending the approach to groups and representations beyond the few presented.\n- In particular, an appropriate discussion on characterizing the more expressive layers of $k$-IGNs for $k>2$ is missing."
            },
            "questions": {
                "value": "1. I find the methodology presented in L135-155 valuable to the research community due to its generalizability beyond the provided examples, most of which are already characterized. For this reason, would it be possible to add a *brief* discussion on the generalization of this methodology to strengthen the impact of this contribution and broaden its relevance to a wider community? See the following for more specific questions.\n2. Computing a basis compatible with the irreducible representation decomposition can be challenging. Does this difficulty limit the methodology\u2019s generalization? Are there similar technical challenges for characterizing $k$-IGN layers for $k > 2$?\n3. Can this methodology be applied to other groups beyond $S_n$ and wreath products? If so, could you briefly provide a few examples?\n4. Representations of the symmetric group are relevant in machine learning and its irreducible representation are absolutely irreducible. In contrast, other relevant groups, such as finite cyclic groups, have real irreducible representations that are not absolutely irreducible. Could the framework presented here be extend to these cases? What potential challenges do you envision in extending to non-absolutely irreducible representations?\n5. Could you elaborate on the future directions for $k$-IGNs presented in the conclusions (L537-539)?\n\n**Minor Issues (No Impact on Recommendation):**\n- L073: I recommend specifying \"$2$-IGNs\" for transparency.\n- L183: Is the presentation of $P_\\tau$ unnecessary?\n- L340: The wreath product of groups is introduced but not defined in detail; as this operation is uncommon in machine learning literature, additional explanation would benefit Section 5. Also, consider demonstrating that equation 7 forms a linear representation of this group, perhaps in the appendix.\n- L420: Typo, \u201cis prove\u201d.\n- L379 and L1030: I cannot understand why $\\mathcal{V}^k$ is an irreducible representation of $\\mathcal{G}^k$; is it instead irreducible for $\\mathcal{G} \\wr S_n$?\n- L1040: The closing curly bracket is missing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper considers the problem of constructing linear equivariant layers for groups acting (linearly) on input and output spaces. Specifically, it proposes to exploit the decomposition into irreducible group representations and then appealing to Schur\u2019s Lemma, which reduces the problem to choosing coefficients for pairs of isomorphic representations. Several specific instances are analyzed, such as permutation groups in the context of graph neural networks, groups acting on weights of deep networks, and wreath products acting on products of representations."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "-The paper is exceptionally well written. The language is clear and concise, the sections are structured, and the mathematical formalism/notation is elegant. \n\n-The problem considered is a fundamental one in machine learning literature. Constructing (linear) equivariant maps lies at the heart of geometric deep learning, which has been successful in several applications. \n\n-The proposed solution is general, as it applies, in principle, to any input/output group representation. Several existing frameworks are phrased under the same paradigm, contributing with structure and clarity to the geometric deep learning literature."
            },
            "weaknesses": {
                "value": "I believe that the proposed approach via Schur\u2019s Lemma comes with disadvantages. To begin with, using Schur\u2019s Lemma to construct equivariant linear maps is not novel in the geometric deep learning community. It is a rather well-known technique \u2013 see, for example, Behboodi et al., Section 3.2. This is a major concern, since Schur\u2019s Lemma represents a core point of this work; the other contributions amount to rephrasings of known frameworks from the literature under the lenses of Schur\u2019s Lemma. Moreover, Schur\u2019s Lemma has some restrictions. First, it requires the decomposition into irreducible representations to be known a priori, which is not always the case. Such decomposition is challenging to compute algorithmically for general groups and representations.  Second, Schur\u2019s Lemma applies naively only to complex representations (i.e., over $\\mathbb{C}$). As the authors mention, this is not an issue for permutation groups (appendix B), but it can be for other groups. It is still possible to apply Schur\u2019s Lemma to arbitrary real representations of arbitrary groups, but this involves subtleties \u2013 see Behboodi et al., Section 8. \n\nI also find the experimental section rather weak. The experiments reported only consider ideal equivariant tasks, i.e., scenarios where the ground-truth function is equivariant. The experimental results show that adding equivariant layers to the network improves (generalization) performance, as compared to non-equivariant architectures. This is not surprising, since in these cases the inductive bias given by equivariance aligns perfectly with the structure of the task. In typical real-world scenarios (e.g., image classification), the (highly-noisy) ground-truth function is instead not exactly equivariant, or it is not equivariant on all the input data. In my opinion, it would be more informative and less trivial to test the models on these types of real-world tasks. The equivariance bias is often still beneficial in terms of generalization \u2013 as works in geometric deep learning have extensively shown \u2013 but empirical investigations are required to assess this carefully. \n\nMinor typos: \n\n-The paragraph title on line 86 is not capitalized, while the one on line 100 is. \n\n-The tables in section 6 exceed the margins of the paper.\n\n\nBehboodi et al., A PAC-Bayesian Generalization Bound for Equivariant Networks, NeurIPS 2022."
            },
            "questions": {
                "value": "I would like the authors to comment on the above points regarding novelty and significance of experiments. \n\nMy current opinion is that the work is exceptionally well-written, and bears several contributions to the geometric deep learning literature. However, I am concerned with the novelty and significance, as outlined above. Still, I am leaning towards accepting the paper, but would like to hear from the authors about my points of criticism."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}