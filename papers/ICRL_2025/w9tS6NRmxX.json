{
    "id": "w9tS6NRmxX",
    "title": "Few-shot In-context Preference Learning using Large Language Models",
    "abstract": "Designing reward functions is a core component of reinforcement learning but can be challenging for truly complex behavior. Reinforcement Learning from Human Feedback (RLHF) has been used to alleviate this challenge by replacing a hand-coded reward function with a reward function learned from preferences. However, it can be exceedingly inefficient to learn these rewards as they are often learned tabula rasa. We investigate whether Large Language Models (LLMs) can reduce this query inefficiency by converting an iterative series of human preferences into code representing the rewards. We propose In-Context Preference Learning (ICPL), a method that uses the grounding of an LLM to accelerate learning reward functions from preferences. ICPL takes the environment context and task description, synthesizes a set of reward functions, and then repeatedly updates the reward functions using human feedback over videos of the resultant policies over a small number of trials. Using synthetic preferences, we demonstrate that ICPL is orders of magnitude more efficient than RLHF and is even competitive with methods that use ground-truth reward functions instead of preferences. Finally, we perform a series of human preference-learning trials and observe that ICPL extends beyond synthetic settings and can work effectively with humans-in-the-loop.",
    "keywords": [
        "Large Language Models",
        "Preference-based RL",
        "Reinforcement Learning from Human Feedback",
        "Reward Design"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "We propose In-Context Preference Learning, a method that address the challenges of reward design in reinforcement learning through the integration of large language models and human preferences.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=w9tS6NRmxX",
    "pdf_link": "https://openreview.net/pdf?id=w9tS6NRmxX",
    "comments": [
        {
            "comment": {
                "value": "**Q4: Why is it necessary to use pair-wise human feedback (a good example and a bad example) if RTS is available? Why not just use all the reward functions with their RTS as prompt (maybe together with other information like reward trace, differences, etc) to generate reward functions**\n\nOur aim is **NOT** to demonstrate that human preferences or preference-based signals are inherently superior to sparse rewards but rather to address tasks where sparse rewards are unavailable. We include experiments on tasks with accessible RTS purely for rapid, quantitative evaluation, given the challenge of comparing methods on tasks without RTS. In our method, RTS serves solely as a proxy for human feedback in selecting reward functions, and the RTS value is not visible in the prompts used for learning.\n\n\n**Q5: Could you please explain the counter-intuitive results in Table 2? It seems the more prompt components you remove (from w/o RT, to w/o RTD, to w/o RTDB), the better performance it gets (w/o RT wins 2 tasks, w/o RTD wins 3 tasks, and w/o RTDB wins 4 tasks), but adding all the components back, i.e., ICPL(Ours), it wins all the tasks.**\n\nThe reasons may be twofold: 1. If the prompt lacks sufficient information, reducing the number of modules\u2014or in other words, shortening the prompt\u2014can help the LLM better identify useful information; 2. LLMs themselves exhibit large variance.\n\nWe hope our response has addressed the reviewer\u2019s concerns and **kindly request a re-evaluation** of our paper, as there may have been a **misunderstanding of the research goal** in the previous review. Should there be any further questions, we would be more than happy to provide additional clarifications."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for highlighting that our work enhances the interoperability and capacity of reward design. The main reason for the reviewer to give a rejection decision is uncertainty as to why we need to involve human feedback in this paper. This appears to stem from a **misunderstanding of the problem we aim to tackle**. We would like to first clarify the scope of our research:\n\n**Our focus is on tasks where humans cannot define any clear reward functions, including both dense and sparse rewards.** In these cases, learning a reward model from human feedback becomes a viable solution. Specifically, humans provide preferences on trajectories without the need to explicitly write out a reward function. In other words, our work is positioned within the realm of preference-based learning methods, where **human feedback serves as a protocol for deriving rewards**. The main contribution of our paper is to **significantly enhance the efficiency of human involvement in preference-based learning by leveraging large language models (LLMs).**\n\nWe will address the reviewer\u2019s detailed questions in the following sections.\n\n**Q1: ICPL involves human labor, but does not show any significant gain over Eureka, which doesn't require any human feedback.**\n\nAs mentioned earlier, involving human feedback is essential for tackling tasks where humans cannot design any clear reward function. EUREKA, however, still relies on human-designed sparse rewards as fitness scores for evolving reward functions. Therefore, EUREKA is **NOT** a true baseline for our work. We have revised the paper to make this clear. We include it solely to demonstrate an approximate upper bound of performance assuming sparse rewards are available. Our goal is **NOT** to outperform EUREKA, but to show that our approach, which relies only on human preferences, can achieve **surprisingly comparable** results.\n\n**Q2: For challenging tasks, true human feedback does not work better than proxy human feedback. This undermines the necessity of involving humans.**\n\nIt is true that real human feedback, which includes noise, can not outperform proxy human feedback, which is noise-free. However, this does not undermine the necessity of involving humans in our tasks. **We believe the reviewer\u2019s point is based on the assumption that sparse rewards are accessible. However, our work focuses on tasks where sparse rewards do not exist.**\n\nFirst, there might also be a misunderstanding for the reviewer regarding the use of proxy human feedback in our experiments. Tasks without any reward or task metric make it difficult to assess method performance, and conducting real human experiments is time-consuming and not easily scalable. Using proxy human feedback is a standard practice for evaluating preference-based learning methods as It allows for rapid and quantitative comparisons across different methods. In this paper, we use sparse rewards as a proxy for perfect human preferences, but this way is actually not feasible for tasks without sparse rewards. Therefore, real human feedback is necessary in these cases. \n\nBesides, the proxy human feedback is a simulated perfect human without any noise and may not fully capture the challenges humans may face in providing preferences,  we further conduct real human feedback experiments to demonstrate the true effectiveness of our approach. In other words, it is expected that true human feedback doesn\u2019t work better than proxy human feedback, the point we want to show is that our method can still work with true human feedback.\n\nFinally, we want to clarify that our goal is **NOT** to show that human preference or preference-style signals are inherently better than directly using sparse rewards. Rather, we are addressing tasks that lack sparse rewards by incorporating human feedback to guide the learning process.\n\n\n**Q3: For challenging tasks, like humanoid jump task, ICPL does not have any solid comparisons with other baselines.**\n\nEUREKA's reliance on sparse rewards makes it unusable as a baseline in tasks where no explicit reward function is defined, as is the case here: we do not have a sparse reward for this task. Thus, a direct comparison with EUREKA is not feasible. Besides, Table 1 shows that PrefPPO, a preference-based learning baseline, struggles to learn effective behaviors in more challenging tasks, even after 15,000 human queries. Given these limitations, it would be impractical to conduct real human experiments on complex tasks like HumanoidJump based on this performance evaluation."
            }
        },
        {
            "comment": {
                "value": "**Q7: Why did you not use PEBBLE as a baseline given that you made use of BPref? Also, did you consider any other baselines as there are more recent works [1,2,3] (To name a few)? It would be great if you discuss how you determined which baseline to use and if you considered any others.**\n\nThank you for the suggestion. We selected the most commonly used preference-based learning method as our baseline, but we are happy to incorporate additional baselines. In the revised version, we will report the performance of PEBBLE along with two methods from the mentioned works that have open-sourced code, once we complete the experiments.\n\nThank you again for the valuable feedback. We hope our response has addressed your concerns. If you have any further questions, we would be happy to provide additional information."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for highlighting the substantial reduction in the number of human queries, and for noting that the evaluation of our method was conducted using both synthetic data and real humans. We also appreciate the advice on making our paper clearer and more comprehensive. We have revised the paper accordingly, with changes highlighted in red, based on the reviewer\u2019s suggestions. Below are the detailed responses to the reviewer\u2019s questions.\n\n**Q1: As with all works that use LLMs to generate reward functions from human feedback I question how well it will perform with more complex tasks which is one of the big reason for using human feedback.**\n\nIt is a reasonable concern that our method may not scale to more complex tasks. However, we would like to clarify two points: 1) we do not claim that our approach will definitively scale to more complex tasks, but rather provide evidence suggesting that it might, and 2) the tasks we evaluate are already sufficiently complex that humans find it challenging to define reward functions for some of them (as demonstrated in EUREKA).\n\n**Q2: The synthetic experiment uses completely noiseless preferences while the standard in these kinds of control environments are typical a noise of let's say 10%. What is the rationale for using noiseless preferences and what would be the effect of noisy preferences for your method?**\n\nWe use noiseless preferences initially to verify whether our method can effectively work solely with preference signals, allowing us to assess the upper bound of expected performance for all preference-based learning methods. Additionally, this setup enables a comparison with EUREKA, which uses noiseless sparse rewards as fitness scores to enhance reward functions. Subsequently, we conduct real human experiments, incorporating true noisy preferences. As shown in Table 3, noisy preferences (ICPL-real) perform comparably or slightly lower than noiseless preferences (ICPL-proxy) across all five tasks, yet still surpass the performance without any preferences (OpenLoop) in 3 out of 5 tasks.\n\n**Q3: While the authors uses B-Pref from Kimin et al for some reason they use only the PPO version even tho the repository is more associated with PEBBLE the SAC version. Why is SAC not used as well?**\n\nWe adopt PPO as IsaacGym tasks are trained by PPO, which is well-suited to high-parallel, GPU-based simulators. Using different training algorithms, such as PPO and SAC, could result in varying basic performance even with the same human-designed reward function, making it difficult to ensure a fair comparison. Furthermore, this way we can ensure that each algorithm has a well-tuned inner loop in which we train the RL agents.\n\n**Q4:  It would be nice with some more information about the experiment like demographic data as well as discussing the limitation of a smaller sample size when it comes to generalizability.**\n\nThank you for the suggestion. We have revised the paper to incorporate this information. The participants in our study consisted of six individuals aged 19 to 30, including two women and four men, with educational levels ranging from undergraduate students to graduate students and one postdoc. We acknowledge that the small sample size may limit the generalizability of our findings, as it may not capture the full range of human preferences. Expanding the sample size and diversity is a crucial direction for future work to improve the robustness and applicability of our results.\n\n**Q5: It seems like Eureka has very similar performance to ICPL, what would you say is the benefit of your method compared to Eureka**\n\nOur focus is on tasks where humans cannot define any clear reward functions, including both dense and sparse rewards. EUREKA, however, still relies on human-designed sparse rewards as fitness scores for evolving reward functions. Therefore, EUREKA is not a true baseline for our work. We include it solely to demonstrate the upper bound of performance assuming sparse rewards are available. **Our goal is not to outperform EUREKA, but to show that our approach, which relies only on human preferences, can achieve comparable results. We have revised the paper to make this clear.**\n\n\n**Q6: To make the related work more complete, there is another paper using LLMs with preferences.**\n\nThanks for pointing that and we have revised the paper to add it in the related work section."
            }
        },
        {
            "comment": {
                "value": "We sincerely appreciate the reviewer for acknowledging our hard work on the real human study and for recognizing that the idea is generally interesting and compelling. We value this positive feedback, which motivates us to further improve our work. Below, we provide detailed responses to the reviewer\u2019s questions:\n\n**Q1: comparison with EUREKA**\n\nWe would like to clarify the scope of our research: **our focus is on tasks where humans cannot define any clear reward functions\u2014whether dense or sparse.** In these cases, learning a reward model from human feedback becomes a viable solution, as humans provide preferences on trajectories without explicitly defining a reward function. Our work, therefore, falls within the domain of preference-based learning, where human feedback serves as the protocol for deriving rewards.\n\n**EUREKA and ICPL work under different assumptions.** EUREKA\u2019s primary goal is to test whether LLMs can produce better reward functions than humans by leveraging human-designed sparse rewards as fitness scores to evolve reward functions. In contrast, ICPL is designed for tasks even without available sparse rewards and leverages LLM grounding to accelerate learning reward functions directly from human preferences. For this reason, **EUREKA is NOT a true baseline for our work, as it cannot address tasks lacking sparse rewards.** We include it only to illustrate an approximate performance upper bound assuming sparse rewards were available. Our goal is not to outperform EUREKA but to show that ICPL, which relies solely on human preferences, can achieve comparable results. **We have revised the paper to make this clear.**\n\nIn the EUREKA appendix, an additional experiment uses human text feedback to describe behaviors and improve the initial human-designed reward function. In contrast, our approach relies solely on preferences\u2014yielding higher human-involvement efficiency\u2014and does not rely on an initial human-designed reward function. Note also that in our proxy human preference experiments, EUREKA\u2019s results are based on sparse rewards, not human text feedback.\n\n **Q2: it's stated in the intro and conclusion that ICPL surpasses RLHF in efficiency, but RLHF is not mentioned anywhere in the experiments.**\n\nIn this context, RLHF (reinforcement learning from human feedback) refers to the baseline method PrefPPO. PrefPPO, which learns a reward model from human preferences and subsequently uses this learned reward model to train an RL policy, is a preference-based learning method, i.e., a subset of RLHF.\n\n**Q3: Based on 5 iterations, I'm not sure that you can make the claim that it will monotonically improve much past that point. Did authors try past 5 (10, 20).**\n\nThank you for pointing that out. We have not tested additional iterations, so we have revised the paper to remove this claim. Our intention is simply to highlight the method\u2019s effectiveness in refining reward functions over time.\n\n**Q4: One sort of undiscussed thing here is that, requiring new models to be trained every iteration does mean that loop is pretty slow. Was 5 iterations chosen for that reason (so it wouldn't take multiple days). This should be maybe discussed as a weakness. E.g. for human studies or using humans, doesn't that mean the humans need to wait hours or else get new humans to provide feedback for every iteration?**\n\nThanks for the suggestion. The overall time-consuming of one experiment is a limitation of our method. We have revised the limitation part to discuss this. This could be resolved by continually training an RL agent under non-stationary reward functions which could make for good future work.\n\n**Q5: Why GPT-4o for the human experiment only?**\n\nMidway through the paper writing process, we found GPT-4o is cheaper. However,  we had already completed most of the proxy human experiments using GPT-4, so we only switched to GPT-4o for the real human experiments.\n\nWe hope our response has addressed the reviewer\u2019s concerns. Should there be any additional questions, we would be glad to offer further clarifications."
            }
        },
        {
            "comment": {
                "value": "**Q5: On page 1, I was confused by the phrase tasks are distinct from the training data. What does this mean**\n\nAn LLM's ability to zero-shot generate correct reward functions may stem from either the task being relatively simple or the model\u2019s exposure to similar data during training, allowing it to partially memorize the task. However, if a task differs significantly from the data the model has seen in its training set, the LLM lacks the necessary knowledge and will struggle to generate accurate reward functions in a zero-shot manner. The initial low performance in Figure 2 and Figure 3 shows that the reward function is likely not memorized and that ICPL is capable of enhancing performance through the iterative incorporation of preferences.\n\n**Q6: How do you actually generate the 6 reward function candidates? Do you randomly sample from the LLM? If so, how?**\n\nThe prompt sent to the LLM includes the task description, environment observations, necessary variables, and guidelines for writing reward functions. The LLM is then asked to generate six reward functions. Each generated function is parsed for its reward signature and then tested for executability. For any reward functions that fail, we regenerate replacements until we obtain six executable reward functions in total.\n\nWe hope our response has addressed the reviewer\u2019s concerns, and **we kindly request a re-evaluation of our paper**. If there are any further questions, we are more than happy to provide additional clarifications."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for pointing out that we made reasonable efforts to optimize the baseline methods. We also appreciate the feedback on the strong comparison against PrefPPO. Below, we provide detailed responses to the reviewer\u2019s questions:\n\n**Q1: ICPL in comparison with EUREKA and human preference comparison with sparse reward**\n\nThe reviewer\u2019s question regarding the comparison between ICPL and EUREKA, as well as the comparison of human preferences with sparse rewards, suggests **there may be a misunderstanding of the paper's scope. Our focus is on tasks where humans cannot define any clear reward signal, including both sparse and shaped rewards for RL training.** Preference-based learning offers a solution by learning a reward model based on human preferences across different trajectories. For tasks where a clear sparse reward can be defined, human preference is not necessary, and a sparse reward is a more conventional learning signal. Therefore, comparing human preference to sparse reward is NOT the goal of our research. Instead, our objective is to explore how to effectively use human preferences to address tasks that lack clear reward signals.\n\nEUREKA, while also using LLMs to generate reward functions, still relies on sparse rewards as a fitness score for evolving reward functions. Thus, **EUREKA is NOT a direct baseline for our work. It serves as an approximate oracle method to demonstrate the upper bound of performance. We have revised the paper to make this clear.** Our aim is not to outperform EUREKA, but to show that our approach, which relies solely on human preferences, can achieve results that are **comparable**. \n\nLastly, we want to clarify our use of tasks with sparse rewards in the proxy human preference section. This approach is standard practice for evaluating preference-based learning methods, as it allows for a rapid and quantitative comparison across different methods. Tasks without any reward or task metric make it difficult to assess method performance, and conducting real human experiments can be time-consuming and not easily scalable. Using sparse rewards as a proxy for human preferences to automatically run experiments is a practical and effective evaluation strategy.\n\n**Q2: I\u2019m not convinced that this success generalizes to the domain of LLM-generated reward functions.**\n\nWe do not fully understand the point being made here and would appreciate clarification if possible! Our paper is a demonstration that a specific type of preference based-learning does in fact work for LLM-generated reward functions.\n\n**Q3: \u201c however the authors only offer one case-study as evidence of the efficacy of human preference data in this domain.**\n\nFirst, our goal is **NOT** to demonstrate that human preferences or preference-based signals are inherently better than directly using sparse rewards. Instead, we focus on addressing tasks that lack clear reward signals by incorporating human feedback to guide the learning process.\n\nAdditionally, we conducted **real human experiments** on **six** tasks, including five IsaacGym tasks and one humanoid jump task. In IsaacGym experiments, humans were unaware of the true sparse reward, which allows these tasks to also serve as test cases. One key reason for adopting the five IsaacGym tasks is that they can provide quantitative results by using human-designed sparse rewards as task metrics, enabling us to present evidence of ICPL\u2019s efficacy in this domain.\n\n**Q4: Evaluation Metric to be very confusing. I wasn\u2019t sure what an \u201cenvironment instance\u201d was. I also didn\u2019t understand which set of task metric values was used to compute the maximum for the RTS.**\n\nWe appreciate the reviewer for raising the issue of clarity, and we have revised the paper accordingly. The term 'environment instance' refers to the parallel environments used during RL training. Specifically, we collect data from 10 parallel environments, meaning 10 environment instances. The task metric is the average sparse reward across these parallel environments that return sparse rewards. In each iteration of ICPL, we have six RL training runs. For each RL run, the RTS is the maximum task metric value sampled at fixed intervals. TS represents the maximum of the 30 RTS sets (6 RL runs \u00d7 5 iterations)."
            }
        },
        {
            "title": {
                "value": "Overall response to all reviewers"
            },
            "comment": {
                "value": "We sincerely thank the reviewers for their valuable feedback, which has significantly enhanced the quality of the paper. We have made revisions based on their suggestions to improve the clarity and rigor of our work. However, we want to draw the attention of all reviewers to a misunderstanding that is consistent across reviews: **a common criticism that our method does not outperform EUREKA**. Our method should ***NOT*** be expected to beat EUREKA as EUREKA **has access to the actual reward instead of just preferences**. We only included EUREKA as an approximate upper bound on the expected performance ICPL could achieve. Rather than being a negative (i.e. we failed to outperform EUREKA), it is instead surprising that our method is able to match it. **We have revised the paper to make this clear.**\n\nAlso we want to clarify that our goal is **NOT** to show that human preference or preference-style signals are inherently better than directly using rewards. Rather, we are addressing tasks that lack any rewards by incorporating human feedback to guide the learning process. This has also been revised to be clearer in the paper."
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel framework called In-Context Preference Learning (ICPL), which automatically generates dense reward functions by utilizing an LLM capable of querying humans for preference data. The authors find that their method greatly outperforms one baseline, PrefPPO, with respect to sample efficiency (PrefPPO requires far more human preference queries) and task performance. The authors also find performance comparable to that of Eureka, a baseline that also utilizes an LLM for generating dense reward functions but relies upon access to ground-truth sparse reward data rather than human preference data. The authors argue, since ICPL does not require access to a ground-truth sparse reward function, it has a clear advantage for tasks that are less well-defined or require human intuition. Additionally, they argue that training with human preferences will enable greater model alignment."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "I appreciated the fact that this paper took steps to optimize their baseline methods within reason. For instance, for Eureka, the authors continued generating candidate reward functions until the LLM had generated 6 executable ones (to make things fair for comparison against their own method).\n\nThe comparison against PrefPPO was strong."
            },
            "weaknesses": {
                "value": "According to table 1, ICPL performance seems no better than that of Eureka. Furthermore, substituting in the values from table 3, ICPL performance with real human preference queries does not exceed Eureka\u2019s performance on any task except Ant. Since ICPL does not outperform Eureka, ICPL\u2019s benefit relies upon the ease of obtaining human preference queries in comparison with a ground-truth sparse reward function. I\u2019m not convinced that this benefit is significant.\n\nOne argument, from the introduction, is an appeal to the success of preference-based training in other domains. I\u2019m not convinced that this success generalizes to the domain of LLM-generated reward functions. \n\nThe other core argument in favor of preference-based training is that human insight\u2014expressed through preference queries\u2014can better align agent behavior with human preferences. The authors motivate this through their custom HumanoidJump task, wherein the task is \u201cto make humanoid jump like a real human.\u201d They argue that this is a domain in which designing a sparse reward function would be difficult due to the nuances/subjectivity of mathematically defining jumping \u201clike a real human.\u201d In my mind, the paper largely hinges on this argument, however the authors only offer one case-study as evidence of the efficacy of human preference data in this domain.\n\nI could be convinced otherwise, but I think there would need to be a more thorough analysis of human preferences in comparison with sparse reward functions in order to be certain.\n\nAlso, I found section 5.3.2: Evaluation Metric to be very confusing. I wasn\u2019t sure what an \u201cenvironment instance\u201d was. I also didn\u2019t understand which set of task metric values was used to compute the maximum for the RTS."
            },
            "questions": {
                "value": "On page 1, I was confused by the phrase \u201ctasks are distinct from the training data.\u201d What does this mean?\nAre there any other reasons to account for why human preference data might be preferable to sparse reward functions?\nHow do you actually generate the 6 reward function candidates? Do you randomly sample from the LLM? If so, how?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces ICPL, a method for iteratively improving reward functions for RL problems with human feedback. The method has LLMs generate reward functions specified by code, trains and executes these rewards, and then ranks the final trajectories with human feedback to then update the reward functions again."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea is generally really interesting and compelling. The idea of having LLMs generate an initial reward function and then iteratively repeat it is really interesting.\n\nThe human study was really compelling and thought out. It's really good that this was actually tried and not just assumed it would work with real human feedback.\n\nPaper really well presented, ideas presented very clearly. Motivation clear and compelling."
            },
            "weaknesses": {
                "value": "I am struggling to figure out what the compelling advantage is of this method over the baseline Eureka. As far as I understood reading the paper, Eureka operated from the same set of assumptions about the environment as did ICPL. And in the non-human experiment performed very similarly. In the related work it says that EUREKA requires humans to give feedback in text, whereas ICPL only requires ranked preferences. During the description in 5.2 it also says that sparse rewards are used to select the best candidate reward function. Does that mean that this is additional assumptions EUREKA needs. There was also not a comparison to EUREKA in the human study. Was that because it would not work without these other assumptions? I think it's possible I'm just misunderstanding here, so if authors could clarify this point it would really help me understand the paper and potentially improve my rating.\n\nIt's stated in the intro and conclusion that ICPL surpasses RLHF is efficiency, but RLHF is not mentioned anywhere in the experiments. Is this an experimental finding of the paper, or are authors just saying based on known findings about the efficiency of RLHF. Could a direct comparison be made in the first (non-human) experiments since you don't need actual humans and can thus potentially run more. More clarity on this point would really help.\n\nBased on 5 iterations, I'm not sure that you can make the claim that it will monotonically improve much past that point. Did authors try past 5 (10, 20).\n\nOne sort of undiscussed thing here is that, requiring new models to be trained every iteration does mean that loop is pretty slow. Was 5 iterations chosen for that reason (so it wouldn't take multiple days). This should be maybe discussed as a weakness. E.g. for human studies or using humans, doesn't that mean the humans need to wait hours or else get new humans to provide feedback for every iteration?"
            },
            "questions": {
                "value": "Please clarify the points mentioned above, that would really help me make a better decision about the paper. In particular explaining why this method would be better in some way that EUREKA\n(Either because ICPL doesn't require some assumption made by EUREKA or it's better in some other way).\n\nMinor:\nWhy GPT-4o for the human experiment only? I'm not sure how much it matters actually, but found it curious."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes In-Context Preference Learning (ICPL), a method using LLMs for more efficient preference-based reinforcement learning. ICPL uses LLMs, such as GPT-4, to synthesize reward functions based on environment context and task descriptions. These generated functions are refined through human feedback iteratively. The approach shows significant efficiency gains, requiring fewer preference queries compared to traditional RLHF and achieving comparable or superior performance to methods using ground-truth reward functions. The experiments validate ICPL's performance across various simulated and real human-in-the-loop RL tasks, showcasing robustness in complex, subjective environments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Demonstrates a substantial reduction in the number of human queries needed for preference-based learning which is sorely needed since human-in-the-loop approaches should ideally just require a handful of preferences.\n\n* It's appreciated that the evaluations of the method is done both in synthetic data and with real humans.\n\n* The paper is well written and the provided method is explained well. Even tho generating reward functions from LLMs is not novel, the way the iteratively make use of human preferences to update their prompt is."
            },
            "weaknesses": {
                "value": "* As with all works that uses LLMs to generate reward functions from human feedback I question how well it will perform with more complex tasks which is one of the big reason for using human feedback.\n\n* The synthetic experiment uses completely noiseless preferences while the standard in these kind of control environments are typical a noise of let's say 10%. What is the rationale for using noiseless preferences and what would be the effect of noisy preferences for your method? \n\n* While the authors uses B-Pref from Kimin et al for some reason they use only the PPO version even tho the repository is more associated with PEBBLE the SAC version. Why is SAC not used as well?\n\n* 6 participants are very low for a study with humans. Still, it is better than some papers that run their method with just the authors feedback. It would be nice with some more information about the experiment like demographic data as well as discussing the limitation of a smaller sample size when it comes to generalizability.\n\nMinor things:\n* You introduce the same abbreviation on multiple occassions.\n\n* To make the related work more complete, there is another paper using LLMs with preferences.\n1. Holk, S., Marta, D., & Leite, I. (2024, March). PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning. In Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction (pp. 259-268)."
            },
            "questions": {
                "value": "* What was the demographic data for the human provided feedback? \n\n* It seems like Eureka has very similar performance to ICPL, what would you say is the benefit of your method compared to Eureka? Eureka seems to have some constraints but it would be nice to show in experimentation or come up with a scenario where it would fail.\n\n* It would be good to justify the length of the paper. For example, what sections do you believe require the additional space? You are of course free to use all the space but the readability of the paper could improve by making it more crisp.\n\n* Why did you not use PEBBLE as a basline given that you made use of BPref? Also, did you consider any other baselines as there are more recent works [1,2,3] (To name a few)? It would be great if you discuss how you determined which baseline to use and if you considered any others.\n1. Kim, C., Park, J., Shin, J., Lee, H., Abbeel, P., & Lee, K. (2023). Preference transformer: Modeling human preferences using transformers for rl. arXiv preprint arXiv:2303.00957.\n2. Park, J., Seo, Y., Shin, J., Lee, H., Abbeel, P., & Lee, K. (2022). SURF: Semi-supervised reward learning with data augmentation for feedback-efficient preference-based reinforcement learning. arXiv preprint arXiv:2203.10050.\n3. Marta, D., Holk, S., Pek, C., Tumova, J., & Leite, I. (2023, October). VARIQuery: VAE Segment-Based Active Learning for Query Selection in Preference-Based Reinforcement Learning. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 7878-7885). IEEE.\n\nI am more than willing to up the score given reasonable answers to these points."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a reward design method. It uses LLMs to generate reward functions to calculate the reward, and the prompt of the LLM is learned through human feedback of the policy rollouts and other historical information in the loop.\n\nIt replaces the implicit reward model in traditional RLHF with an LLM and its output reward function. This enhances the interoperability and capacity of the reward design."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "It replaces the implicit reward model in traditional RLHF with an LLM and its output reward function. This enhances the interoperability and capacity of the reward design."
            },
            "weaknesses": {
                "value": "(1) ICPL involves human labor, but does not show any significant gain over Eureka, which doesn't require any human feedback.\n\n(3) For challenging tasks, true human feedback does not work better than proxy human feedback. This undermines the necessity of involving humans.\n\n(2) For challenging tasks, like humanoid jump task, ICPL does not have any solid comparisons with other baselines."
            },
            "questions": {
                "value": "(1) Why is it necessary to use pair-wise human feedback (a good example and a bad example) if RTS is available? Why not just use all the reward functions with their RTS as prompt (maybe together with other information like reward trace, differences, etc) to generate reward functions?\n\n(2) Could you please explain the counter-intuitive results in Table 2? It seems the more prompt components you remove (from w/o RT, to w/o RTD, to w/o RTDB), the better performance it gets (w/o RT wins 2 tasks, w/o RTD wins 3 tasks, and w/o RTDB wins 4 tasks), but adding all the components back, i.e., ICPL(Ours), it wins all the tasks."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}