{
    "id": "VVO3ApdMUE",
    "title": "Transformer Encoder Satisfiability: Complexity and Impact on Formal Reasoning",
    "abstract": "We analyse the complexity of the satisfiability problem, or similarly feasibility problem, (trSAT) for transformer encoders (TE), which naturally occurs in formal verification or interpretation, collectively referred to as formal reasoning. We find that trSAT is undecidable when considering TE as they are commonly studied in the expressiveness community. Furthermore, we identify practical scenarios where trSAT is decidable and establish corresponding complexity bounds. Beyond trivial cases, we find that quantized TE, those restricted by fixed-width arithmetic, lead to the decidability of trSAT due to their limited attention capabilities. However, the problem remains difficult, as we establish scenarios where trSAT is NEXPTIME-hard and others where it is solvable in NEXPTIME for quantized TE. To complement our complexity results, we place our findings and their implications in the broader context of formal reasoning.",
    "keywords": [
        "transformer",
        "formal reasoning",
        "complexity"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=VVO3ApdMUE",
    "pdf_link": "https://openreview.net/pdf?id=VVO3ApdMUE",
    "comments": [
        {
            "title": {
                "value": "comment on assessment of our style of presentation"
            },
            "comment": {
                "value": "The initial reviews presented contradicting perspectives on the style of presentation, some describing it as too technical, others as not technical \nenough. We understand each of these viewpoints. They may arise naturally given that the paper's topic lies at the intersection of machine learning and \ntheoretical computer science, respectively computational complexity. To address the broader audience present at ICLR, presumably mostly at home in the machine learning area, we decided to present our results in three stages:\n\n- *Overview*: Initially, we present an overview of the core results, accompanied by informal explanations of their implications. This part should be \naccessible to the entire audience.  \n- *Proof sketches and ideas*: Next, we offer for all core results a description of the proof strategies, including the underlying ideas and tools, as well as sketches of the arguments. This is designed to help readers gain an initial understanding of the proofs and an insight into their validity.\n- *Technical appendix*: Finally, we provide complete technical proofs for all statements, constrained by a strict page limit, with major sections placed in the appendix. Naturally, this requires some background knowledge regarding the topics of the paper.\n\nThe overview is Section 3, sketches are Sections 4 and 5 and technical parts are in the appendix."
            }
        },
        {
            "title": {
                "value": "NP-completeness does not generically extend downwards"
            },
            "comment": {
                "value": "> For theorem 3 page 8: distinction un / bin is ok. Simplification could look like: BTRSATun[T ] is NP-complete. simple and effective.\n\nBut unfortunately not correct for arbitrary $T$. The sentence would be true if it only stated \"BTRSATun[T] is in NP\". For NP-hardness to hold as well, $T$ needs to be \"strong enough\" to capture the inherent difficulty of NP-hard problems. There are many (trivial) classes $T$ of transformers for which BTRSATun[T] is *not* NP-complete, for instance transformers that realise constant functions.\n\n> The additional \"if Tudec \u2286 T then BTRSATun[T ] is NP-complete\" brings absolutely no additional information AFAIK.\n\nYes, it does. The condition \"$Tudec \\subseteq T\" is a syntactic (and therefore automatically checkable) property that guarantees NP-hardness."
            }
        },
        {
            "comment": {
                "value": "Dear Authors,\n\nThanks a lot for the answer.\n\nSome more details.\n> While the periodicity combined with the finite input alphabet limits the number of possible distinct vectors following the positional embedding, fixed-width arithmetic is essential for bounding intermediate computations of a TE. \n\n\nI believe you do not need to reason at the level of intermediate computations. Different use of Dirichlet lemma should provide decidability without finite alphabet (with possibly higher complexity than what you obtain with finite input alphabet).\n\n\n>we do not have a formal proof for this setting, but this is mostly due to the fact that fixed-width arithmetic generally leads to periodic embeddings\n\n\nI do not understand the logical fundation of such a sentence. The fact that periodicity is often used should not prevent you from making a proof without periodicity. \n\n\nAgain, looking at things at a high level, a decidability proof seems reasonable with only fixed width arithmetic (but certainly with higher complexity than NEXPTIME-time).\n\n\n> we recognise that this might still lead to some misunderstanding\n\n\nYou just cannot represent things this way (unless you have stronger results).\nThis would be highly non-standard.\n\n\n> This is due to the distinct arguments involved\n\n\nPlease, enlighten me to the argument involved in the proof of Theorem 2. \nAs I see it, it is a simple: the encoding of the proof of Theorem 1 needs only logarithmic number bits, without (almost) any changes.\n\n\n\n> we would appreciate it if you can provide further clarification on this point of criticism,\n\nI was refering mainly to the statement of Theorem 3 (page 8), and to the explicit simplification of the statement of th1 and th2 into 1 single easier to parse statement.\n\nStatements of other Theorems are more reasonable, I rekon that I overgeneralized from these 3 theorems to all the theorems statements.\n\nFor theorem 3 page 8: distinction un / bin is ok.\nSimplification could look like:\nBTRSATun[T ] is NP-complete.\nsimple and effective.\n\nThe additional \"if Tudec \u2286 T then BTRSATun[T ] is NP-complete\" brings absolutely no additional information AFAIK.\n\nSo far, the authors did not bring strong additional information. The main concern remains about the big gap in the results, which I do not see much justification for. My current assessment remains, that is, interesting paper, but there is at least 1 missing result to make it ready for publication."
            }
        },
        {
            "title": {
                "value": "comments on weaknesses - part 2"
            },
            "comment": {
                "value": "> The paper\u2019s conclusions are not fundamentally surprising given the recent line on work on the expressiveness of Transformers.\n\nWhile we understand your perspective, we respectfully offer a deviation in our opinion. Although, as indicated by results like [1], one might anticipate undecidable cases (Theorem 1), our research further extends these findings even into log-precision settings (Theorem 2). Additionally, our analysis of the effects of quantisation and periodic embeddings (Theorems 4 and 5) presents insights that are not immediately derived from the current advancements in comprehending the expressive power of transformer encoders.\n\n\n> It would be more impactful if the work shows interesting classes of Transformers/properties that can be verified in polynomial time.\n\nWe concur that demonstrating such results would indeed be profoundly impactful. However, considering the substantial expressive capabilities of transformer encoders, it is improbable that such results exist without either trivialising the class of transformers or the class of properties being addressed. For instance, even for feedforward neural networks common verfication problems are NP-hard [2]. \nAs a result, these expectations may be somewhat ambitious.\n\nNonetheless, we concur that pursuing more tractable scenarios is desirable. In this regard, we perceive our work as a foundational endeavour, laying a groundwork for future research aimed at identifying tractable contexts for formal reasoning. Most of all: our work presents principal limitations on the possibility to find tractable classes of transformers/properties.\n\n\n--- \n\n[1] Jorge P\u00e9rez, Pablo Barcel\u00f3, Javier Marinkovic:\nAttention is Turing-Complete. J. Mach. Learn. Res. 22: 75:1-75:35 (2021)\n\n[2] Marco S\u00e4lzer, Martin Lange: Reachability is NP-Complete Even for the Simplest Neural Networks. RP 2021: 149-164"
            }
        },
        {
            "title": {
                "value": "comments on weaknesses - part 1"
            },
            "comment": {
                "value": "We are grateful for your time in reviewing our paper. \nIn response to the weaknesses you highlighted, we will address each one step by step, aiming for a productive and engaging discussion.\n\n### Weaknesses\n\n> The naming in the paper can cause much confusion. SAT typically refers to the Boolean Satisfiability problem in computational complexity, and using SAT to also refer to Transformer Encoder Satisfiability can be confusing to many readers, especially the claim that \u201cSAT is undecidable\u201d in the abstract.\n\nWe understand the potential for confusion in terminology. To address this, we have consistently renamed the problem as TrSAT throughout the paper in the revised version to distinguish it clearly from the Boolean satisfiability problem. Additionally, we have rephrased the introductory sentence in the appendix. We appreciate your suggestion!\n\n> The paper tries to make arguments that connects the satisfiability theorems proven in the paper to \u201cformal reasoning\u201d (i.e, model verification and interpretation), which I believe is not sufficiently justified. [...] \n\nAs you rightly noted, we have devoted Section 3.1 (line 177-) to highlight this connection stating examples regarding robustness of interpretation problems. There we provide two examples of a connection. In the following following we give a third, more general one:\n\nA prominent problem in the category of formal verification is the reachability problem (Reach), which can be described as follows: given a transformer $T$, a set of valid inputs $X$, and a set of valid outputs $Y$, decide whether there exists an input $w \\in X$ such that $T(w) \\in Y$. The TrSAT problem can be considered to be a fundamental form of Reach, where the set of valid inputs $X = \\Sigma^*$, with $\\Sigma$ representing the alphabet of $T$, and the set of valid outputs is $Y = \\{1\\}$. \nThe basic nature of TrSAT implies that Theorems 1 and 2 apply to all variants of Reach, where we can define $X$ and $Y$ as mentioned above. We believe this represents a level of expressiveness in input and output specifications which is general. Similarly, upper bounds such as those in Theorems 4 and 5 are valid for all variants of Reach, provided that the complexity of determining membership in $X$ or $Y$ does not overshadow the complexity introduced by the kinds of transformers considered.\n\nNonetheless, you highlight an important aspect: research centred on specific reasoning problems must elucidate the connections to SAT. \nWe believe this task is straightforward for those engaged in such areas due to the inherent simplicity of SAT. The two examples in the paper \nand the one given above highlight this. Apart from the general concepts already discussed both above and in our paper, we see limited value in defining further, more specific connections in this work. \nUltimately, the aim of this paper is to lay down foundational baselines, which we believe should be framed as broadly as possible.\n\n>  \u201cFormal Reasoning\u201d also has specific meanings referring to reasoning over formal systems with well-defined inference rules and axioms. The \u201cformal reasoning\u201d in this paper can be directly stated as \u201cverification and interpretation\u201d. \n\nWe have introduced the phrase \"formal reasoning\" to have a single term to collectively refer to problems, procedures, and similar concepts involved in \"formal verification and formal interpretation.\" We explained this in lines 44\u201345. That is all there is to it, but we remain open for other suggestions.\n\nNonetheless, to avoid any misunderstandings we have included an additional sentence in the beginning of the appendix of the revised version of the paper to clarify the term as early as possible."
            }
        },
        {
            "title": {
                "value": "answer to questions"
            },
            "comment": {
                "value": "We appreciate you taking the time to review our paper! We are particularly grateful for your precise articulation of the contributions our paper makes, and for acknowledging our efforts to ensure its accessibility to a broader audience.\n\nBelow, we provide succinct responses to each of your inquiries, and we look forward to engaging in a productive dialogue!\n\n\n### Questions \n> Do any of the proven results change if satisfiability is defined as exceeding a given threshold instead of just being equal to 1? This could be a more realistic condition for acceptance.\n\nIn summary, the answer is: no. In more detail:\nIn our lower-bound proofs, we can similarly construct feedforward neural networks (FNN) to represent the output function in those proofs, such that they output a value of $\\geq c$ for arbitrary (rational) thresholds $c$ if and only if the desired property is satisfied, instead of precisely equalling $1$.\nIn the case of upper-bound proofs, the distinction is negligible, as determining whether a numerical value equals $1$ or is greater than or equal to some threshold $c$ involves a similar level of complexity.\n\nIn addition, we agree with your suggestion to emphasise this point in our paper. Consequently, we have included a clarifying note as a footnote following the SAT definition. Thank you!\n\n> Do the authors have an intuition on to what extent their results could generalize to soft-attention?\n\nThe upper bounds we established also apply to TE with softmax as normalisations. Thus, the interesting part here are lower bounds.\n\nWe do not have a definitive answer for the lower bounds. It seems that the constructive approaches we employed to prove lower bounds might not work with softmax, due to its less discrete nature. For instance, we frequently leveraged specific encodings to ensure that hardmax could uniquely focus on particular positions. This is by definition impossible with softmax, as long as we assume unbounded precision. Consequently, we believe that alternative types of arguments would be necessary when considering softmax in lower bounds.\n\n> It was not clear to me if the definition of SAT on transformers is new, or has been proposed before.\n\nTo the best of our knowledge, this problem has not been explored before in the context of transformers, at least in this specific manner. However, there is similar work for other models, such as Graph Neural Networks (GNN) and FNN; see references [1] and [2] for more details, but note that the terminology sometimes differs.\n\n> This perhaps a bit of a broad/vague question, but seeing the many undecidability and hardness results proven in the paper, do the authors think that there is hope for formal verification techniques on transformers? Or should the verification efforts focus on more tractable and less expressive classes of models than the transformer?\n\nWe appreciate the question and agree with you that, from a practitioner's perspective, this is the primary concern within this area of research. At present, our results, especially the lower bounds, suggest that it is advisable to concentrate on less expressive models when aiming for sound and complete verification or interpretation. For those focused on transformers, the best approach right now may be to employ, for example, non-complete methods.\n\nHowever, this holds only for the time being. Our findings suggest that there are potential strategies to enhance the possibilities of formal reasoning, such as focusing on quantised models or significantly restricting the context length of interest.\nAdditionally, a detailed examination of our lower bound proofs provides insights into the properties that may contribute to the generally high complexity of the problem. For example, using less powerful positional encodings or normalisation functions could yield more tractable settings. \n\nNevertheless, the question remains whether such transformers will prove to be competitive and what the precise complexity of problems like SAT will be for these models. Ultimately, obtaining a better outcome than NP-completeness is improbable in non-trivial cases, as evidenced by similar challenges found in FNN [2]. It is plausible that the middle ground will consist of PSPACE-completeness, akin to certain non-trivial GNN models [1], which some consider a generalised form of transformers.\n\n> Some small nitpicks \n\nThanks for pointing these out. We fixed both in the revised version of the paper.\n\n---\n[1] Michael Benedikt, Chia-Hsuan Lu, Boris Motik, Tony Tan:\nDecidability of Graph Neural Networks via Logical Characterizations. ICALP 2024: 127:1-127:20\n\n[2] Marco S\u00e4lzer, Martin Lange:\nReachability is NP-Complete Even for the Simplest Neural Networks. RP 2021: 149-164"
            }
        },
        {
            "title": {
                "value": "answer to questions + weaknesses - part 2"
            },
            "comment": {
                "value": "> The number of Theorems is also inflatted. E.g. theorem 2 should just be a note at the end of theorem 1\n\nWe acknowledge the connection between Theorems 1 and 2, as well as Theorems 4 and 5. However, we believe it is a matter of preference, and deem it reasonable to treat them separately. This is due to the distinct arguments involved, each of which is considerably detailed, as highlighted in the technical appendix. \n\n\n> The authors need 2 restrictions together to get decidability (see 4.), and the proof of undecidability needs both restriction lifted. So what happens if only one of this restriction? ...\n\nThis is addressed in our response to your second question. However, if you feel there are additional aspects that require clarification, we would be happy to elaborate on this further."
            }
        },
        {
            "title": {
                "value": "answer to questions + weaknesses - part 1"
            },
            "comment": {
                "value": "Thank you for dedicating your time to providing a review of our paper. We hope this serves as a sound foundation for a productive discussion.\n\nWe shall first address your direct questions, following which we will comment on the weaknesses you have highlighted.\n\n### Questions\n\n> Where is the fix-precision used in the proof of lemma 3? Is it necessary?\n\nFirst, a brief response: \nRefer to line 1067, where it's noted that $V$ represents the finite set of all numerical values expressible within fixed-width arithmetic. \nAfterwards, we use the bound $|V|$ in line 1087.\n\nNow, to provide an intuitive explanation: \nWhile the periodicity combined with the finite input alphabet limits the number of possible distinct vectors following the positional embedding, fixed-width arithmetic is essential for bounding intermediate computations of a TE. For instance, the hardmax or softmax normalisations used in the considered classes of TE exhibit averaging behaviour, meaning their outputs depend on the length of input words. \nThis length is unbounded and, thus, there is a possibly infinite number of different outputs.\nHowever, since only finitely many values are representable (see set $V$), we observe an \"internal periodicity\" concerning normalisations based on the pigeon hole principle. The same intuition applies for pooling functions. In summary, this is the idea behind the bound used in line 1087 and why the fixed-width arithmetic assumption is essential in the proof of Lemma 3.\n\n> Do you have proof of decidability for fix-precision (without periodic embeddings)?\n\nThank you for raising this question! No, we do not have a formal proof for this setting, but this is mostly due to the fact that fixed-width arithmetic generally leads to periodic embeddings:\n\nLet's consider positional embeddings that encode positional information in the form $1, 2, 3, \\ldots$. Such embeddings are evidently periodic in fixed-width arithmetic, using wrap-around to handle overflow, as it results in $1,2, \\dotsc, n, 1, 2, \\dotsc, n, \\dotsc$ and so on. If overflow is handled using saturation, these positional embeddings result $1, 2, 3, \\ldots, n, n, \\ldots$, comprising a finite prefix followed by a suffix with periodicity of $1$, for which the arguments are the same as we used. \n\nWe acknowledge that alternative embeddings could be considered, which (presumably) do not witness periodic behaviour in fixed-width arithmetic. For instance, enlisting the digits of pi as positional information. However, we believe that such considerations are somewhat contrived and detract from the fundamental objectives of our paper.\n\n> In all cases, you cannot draw Sat[T^fix] on the Nexptime ball (in figure 2), as there is no proof it is in Nexptime.\n\nThank you for pointing that out. This was not the intention we wished to communicate in the illustration. Originally, we aimed to clarify this in the figure's description with the statement: \"The NEXPTIME-hardness result of $\\text{Sat}[\\mathcal{T}^\\text{FIX}]$ is depicted by placing it precisely on the upper boundary between NEXPTIME and all decidable problems.\" \n\nNevertheless, we recognise that this might still lead to some misunderstanding. Thus, we have updated the figure and the clarifying sentence in the description in the revised version which hopefully resolves any ambiguities.\n\n### Weaknesses\n\n> The paper could be written in a more reader-friendly way, it is very technical.\n\nThank you for considering the style of presentation we have adopted. This paper aims to enhance readability in three significant ways: Firstly, in Section 3, we provide an informal overview of our results and their implications, which should be accessible to those without specialised knowledge of the topic. Secondly, Sections 4 and 5 offer proof sketches for all our core results, allowing readers with more expertise to grasp the essence of our arguments. Lastly, the technical appendix contains the full detailed proofs, enabling the complete verification of all our findings.\n\nWe believe this method effectively presents our specialised results to a wider audience. However, we remain open to any suggestions you might have!\n\n> Statements of the theorems are unnecessarily complicated.\n\nIf you find the time, we would appreciate it if you can provide further clarification on this point of criticism, as it is not our intention to come across as overly complex. Most of our results are conveyed in a single sentence, such as \"Problem X is undecidable.\" or \"Problem Y is in NEXPTIME.\" While this does require a basic understanding of computational complexity theory, we believe it is the most succinct manner to present these findings."
            }
        },
        {
            "title": {
                "value": "answer to questions  - part 3 + weaknesses pointed out"
            },
            "comment": {
                "value": "> Pg. 8. It seems that the proof sketch of The 3 does not take into account the representation of w. Please comment on this.\n\nWe believe you are referring to the fact that $|w|$ measures only the length of a word, as opposed to its representation size, which should also account \nfor $|\\Sigma|$. You are indeed correct in this observation. However, the size of a TE $|T|$ does depend on $|\\Sigma|$, hence this poses no issue for the \nlower bounds demonstrated in Theorem 3. \nNevertheless, we agree with your point and have included a clarifying sentence in the revised version in the first paragraph of the proof sketch of Theorem \n3 (page 8).\n\n> Other comments\n\nThank you for pointing out these typos. We fixed them in the revised version!\n\n\n### Weaknesses\n\nRegarding the structure and presentation of the paper: we see the point. But other reviewers have commented on the presentation in an opposite way. We \nwill write a short reply on this issue to all reviews as a general comment."
            }
        },
        {
            "title": {
                "value": "answer to questions - part 2"
            },
            "comment": {
                "value": "> Pg. 6. L. 306. The name \u201coctant\u201d tiling problem does not seem to be standard. It is not mentioned as is in the provided reference. Also it is not clear \nwhy it is necessary to distinguish between \u201ctiling\u201d problem\" and \u201ctiling word\u201d problem. The definitions in the appendix do not make this clear. Could you \nexplain it?\n\nOctant tiling is a variant of the well-known quadrant-tiling (named in [Rob71]) instance in the class of domino tiling problems, where the tiling is \nrestricted to a certain octant of the plane, also known as a quadrangle. It is true that there does not seem to be one citable reference for \nthe undecidability of the octant tiling problem, but this is also due to the fact that it is easily seen to be undecidable by observing that the \nstandard reduction from the halting problem for Turing Machines to the quadrant tiling problem yields an octant tiling (because after n steps a TM can \nhave modified or visited at most the first n tape cells). The octant tiling problem is often used in the literature for undecidability proofs, for \ninstance in [BGD+10,BDG+14,B22,B24,HMS08]. \n\nIn our undecidability proofs, the \"octant tiling word problem\" serves as an intermediate problem between octant tiling and the satisfiability problem for \ntransformer encoders. The \"octant word tiling problem\" is used to translate the source problem in the reduction, formulated as a constraint problem \nover a two-dimensional geometry, into a constraint problem over a one-dimensional geometry, i.e. as a problem of finding a linear word structure. This is\nall there is to it.\n\n> Pg. 8. L. 401. It reads \u201cone can reasonably assume the size of a syntactic representation of T to be polynomial on |T|\u201d. It is not clear to me that this \nassumption is reasonable. Please provide arguments.\n\nTo ensure that Theorems 3 and 4 are still broadly applicable, we have defined classes of transformer encoders (TE) without rigidly fixing every \nsingle parameter such as the precise combinations of scorings and similar aspects. However, for obtaining complexity bounds it is essential to establish the \nmeasure of representation size for transformers. The size measure |T| used in the paper takes only the most significant parts of an encoder transformer into \naccount, namely alphabet size, depth, width, and dimensionality. This ensures that Theorems 3 and 4 are applicable to transformer models that may vary in \ncertain aspects, for as long as the other parameters do not exceed this polynomial bound. It is, of course, possible to devise transformers that do not meet \nthe assumption in question, for example by use of non-standard normalisation functions that require large programs or very large constants for their \ncomputation or representation. To the best of our knowledge, the transformers that are widely considered in the literature and in practice do all meet this \nproperty of being representable in space that is polynomial in the measure |T|. This is why we call this assumption reasonable.\n\n> Pg. 8. L. 402. The so-called polynomial evaluation property is not discussed in Section 3 nor elsewhere in the paper. Why is it reasonable?\n\nThank you for pointing this out; it appears to be an oversight on our part. The polynomial evaluation property is deemed reasonable as it aligns with how \nencoders process their output. The outlined procedure is as follows: for each layer $l_i \\leq L$, the output sequence $w^i = x_1^i \\dotsb x_{|w|}^i$ is \nderived from $w^{i-1}$ by pairing each $x_h^{i-1}$ and $x_k^{i-1}$ with $h, k \\leq |w|$. This pairing occurs within each attention head \n$\\mathit{att}_{i,j}$ where $j \\leq H$. Given that $|T|$ is dependent on both $L$ and $H$, this computation is polynomial in terms of $|T| + |w|$.\nWe added a clarifying paragraph after Theorem 2 in Section 3!\n\n---\n[Rob71] R. Robinson. \"Undecidability and Nonperiodicity for Tilings of the Plane\", Inventiones Mathematicae, 12(3), 1971 pp. 177\u2013209\n\n[B22] L. Bartholdi, \"Monadic second-order logic and the domino problem on self-similar graphs\", Groups Geom. Dyn. 16 (2022), pages 1423\u20131459\n\n[B24] B. Bednarczyk, \"Exploring Non-Regular Extensions of Propositional Dynamic Logic with Description-Logics Features\", Log. Methods Comput. Sci. 20(2) (2024)\n\n[BDG+10] D. Bresolin, D. Della Monica, V. Goranko, A. Montanari, G. Sciavicco, \"Undecidability of the Logic of Overlap Relation over Discrete Linear \nOrderings\", Electronic Notes in Theoretical Computer Science, vol. 262, 2010, pages 65-81\n\n[BDG+14] D. Bresolin, D. Della Monica, V. Goranko, A. Montanari, G. Sciavicco, \"The dark side of interval temporal logic: marking the undecidability \nborder\", Ann. Math. Artif. Intell. 71(1-3): 41-83 (2014)\n\n[HMS08] I. M. Hodkinson, A. Montanari, G. Sciavicco, \"Non-finite Axiomatizability and Undecidability of Interval Temporal Logics with C, D, and T\", CSL \n2008:308-322"
            }
        },
        {
            "title": {
                "value": "answer to questions - part 1"
            },
            "comment": {
                "value": "Firstly, we would like to express our gratitude for your comprehensive review! We appreciate the considerable time you devoted to raising those questions. \n\nWe are looking forward to an engaging discussion. To make it efficient, we have paraphrased the main questions below.\nHowever, the numerous questions you stated make it necessary to spread our answer across several comments.\n\n--- \n\n### Questions \n\n> Pg. 2. L 54-54.  \"I disagree with this comment. Programming languages are Turing Complete, still [...]. Please comment.\"\n\nWe agree. It is certainly worth delving into research on formal reasoning for encoder-decoder transformers. The comment to which you refer was intended to \nclarify our decision to focus on encoder-only models, as this is the first work of its kind. We aim to establish fundamental results without imposing \nunnecessary restrictions. Commencing with full encoder-decoder capabilities could potentially require artificial constraints on models and settings to \nachieve decidability, due to their already known high expressive power (Per\u00e9z et al. 2021). Thus, we selected encoder-only models for this study, but never \nintended to suggest that encoder-decoder transformers lack interest.\n\n>Pg. 2. L 66-69. \"What do you mean by \u201cformal interpretation\u201d?\"\n\nThe term \"formal\" indicates our focus on methods that are both sound and complete. We elaborated on this in the paragraph beginning on line 38. \"Formal \ninterpretation\" refers to problems, procedures, and related topics concerning interpretability properties of neural network-based models. For a reference, \nplease see Marques-Silva & Ignatiev (2022), where the authors employ the term \"formal XAI\". We discussed this in the paragraph beginning on line 190.\n\n> Pg. 3. L 169-174. \"What part of the TC proof of Per\u00e9z et al. [..] T_udec could possibly be decidable?\n\nWe view the principal contribution of this work as establishing a foundational framework for comprehending the computational complexity of formal reasoning \nwithin transformers. The class T_udec under consideration should be regarded as the so far tightest \"threshold\" concerning expressive power, above which \nformal reasoning of transformer encoders is impossible/undecidable. \nIn the study by Per\u00e9z et al. (2021), the authors concentrated on encoder-decoder transformers, whereas our research is dedicated solely to encoder-only \ntransformers. While some of our architectural choices for T_udec were inspired by their work, primarily to establish the aforementioned threshold, the \nmodels we investigate are of a different type.\n\n> Pg. 4 \"Could it be possible that softmax rather than hardmax change the decidability result?\"\n\nWe understand this question as you referring to the class $T_\\circ^{\\text{fix}}$ or the result of Theorem 4. The class \n$T_\\circ^{\\text{fix}}$ encompasses transformer encoders (TE) that can incorporate either softmax or hardmax normalisations. Consequently, the \nupper bound on decidability also applies to classes of TE analogous to $T_\\circ^{\\text{fix}}$ that exclusively employ hardmax, which means that \nthe presence of softmax is not the reason.\n\n> Pg. 5. L 251. The 3. \"Clearly, bounded satisfiability is decidable [...] The important result here is the complexity bound.\"\n\nWe agree with your suggestion. This informal restatement of Theorem 3 would indeed benefit from the inclusion of the upper bounds specified in the formal \ntheorem. We have addressed this in the revised version by stating NEXPTIME- and NP-completeness.\n\n> Pg. 5. L. 265. \"In what sense fixed-width arithmetic has a similar effect to bounding the input length? Besides, why not considering then T^FIX only?\"\n\nThe sentence you are referring to was meant to convey that both, bounding the input length or considering fixed-width arithmetic, lead to decidability with \nan NEXPTIME upper bound.\nThe relevance of the bounded input length setting lies in its practicality as a natural constraint, such as when context sizes are limited.\n\n> Pg. 6. L. 302. Why is it the case that T_udec is the weakest class in terms of expressivity?\n\nWhile much remains unknown, providing a comprehensive hierarchy of classes for transformer encoders is challenging. This difficulty arises from the \nnumerous parameters involved, such as variations in encoding, normalisations, and scorings, among others.\nThe statement you are referring to was meant to introduce T_udec as the class for which we currently know, based on the results of our paper, that it is \nthe weakest leading to the undecidability of SAT. However, you are correct: the way that that sentence is currently phrased could suggest a stronger claim \nthan intended. We have addressed this in the revised version."
            }
        },
        {
            "summary": {
                "value": "The paper proves several hardness results on the satisfiability of encoder-only Transformers. These results demonstrates the hardness of preforming formal verification of satisfiability over Transformers Encoders, which is undecidable for unbounded-length, log-precision Transformer Encoders, NEXPTIME-Hard for bounded-length inputs and bounded precision."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Overall, the theoretical contributions of this work can be impactful as formalizations of impossibility results of general formal verification over Transformers. The theoretical contribution of implementing a tiling system within Transformer Encoders can also be useful for further theoretical work, whereas prior works on expressiveness on Transformer Encoders mostly focused on upper bounds with circuit complexity. As such, I recommend for Acceptance (assuming the authors make appropriate clarifications as mentioned below)."
            },
            "weaknesses": {
                "value": "The paper tries to make arguments that connects the satisfiability theorems proven in the paper to \u201cformal reasoning\u201d (i.e, model verification and interpretation), which I believe is not sufficiently justified. In the section 3.1 \u201cSatisfiability as a baseline formal reasoning problem\u201d the author makes 2 examples: robustness verification and formal interpretation. However, both examples require the input to satisfy certain properties and decides satisfiability on the set of inputs with the given properties. It is unclear whether the how hardness results still hold when input space is constrained as in the given examples. As such, this connection between the theorems proven in the paper and \u201cformal reasoning\u201d should be accurately characterized.\n\u00a0\nThe naming in the paper can cause much confusion. SAT typically refers to the Boolean Satisfiability problem in computational complexity, and using SAT to also refer to Transformer Encoder Satisfiability can be confusing to many readers, especially the claim that \u201cSAT is undecidable\u201d in the abstract. It is recommended to use a different acronym for the specific problem. Similarly, \u201cFormal Reasoning\u201d also has specific meanings referring to reasoning over formal systems with well-defined inference rules and axioms. The \u201cformal reasoning\u201d in this paper can be directly stated as \u201cverification and interpretation\u201d.\n\u00a0\nAt the current state, the paper\u2019s conclusions are not fundamentally surprising given the recent line on work on the expressiveness of Transformers (although, as mentioned, formalizing such statements and providing a concrete construction is a sufficient contribution). From a practicality perspective, both NP-Hard and NEXPTIME-hard are both infeasible, and it would be more impactful if the work shows interesting classes of Transformers/properties that can be verified in polynomial time."
            },
            "questions": {
                "value": "Please address the concerns raised in the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies the decidability of the emptiness problem of the language recognized by a transformer encoder classifier,  that is, whether there exists a string w in the input domain for which the given transformer will return true. It also studies the complexity of such problem in contexts where it is found to be decidable."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The main interest of the paper is to try to define bounds on the complexity of decidable classes of problems related to languages recognized by transformes."
            },
            "weaknesses": {
                "value": "The paper itself is quite a high-level description of a list of results. The necessary definitions, such as the tiling problem, and other key aspects of the proofs are in the appendix or completely missing. The absence of important definitions makes the paper difficult to follow.\n\nBesides, the structure of the paper with a preview of the results does not enhance readability but quite the contrary. It would be much readable if the results were shown, proved and explained once, providing reasonably detailed proofs that put forward the important issues (while the much technical details are put in the appendix)."
            },
            "questions": {
                "value": "Pg. 2. L 54-54. I disagree with this comment. Programming languages are Turing Complete, still formal reasoning and verification has been a very active, productive and necessary field of computer science. Nevertheless, it is worth knowing\tthe boundaries of decidability and complexity to figure out methods to cope with that.  Please comment.\n\nPg. 2. L 66-69. What do you mean by \u201cformal interpretation\u201d? The term is not common in the field of formal verification and you do not define it here nor provide references to definitions or related work. An example is provided later in Pg. 4, Sec. 3, which reduces to a verification problem. There is missing related work regarding formal methods and tools for extracting automata-based models from different kinds of neural classifiers (including transformers) and language models that should be cited here. Such methods can accomplish verification and generate explanations. Good sources of references for this matter are the Proceedings of the International Conference on Grammatical Inference 2023 and the last two LearnAut workshops. That line of relevant work is not referenced by the cited papers.\n\nPg. 3. L 169-174. What part of the TC proof of P\u00e9rez et al made you believe that languages recognized by the class T_udec could possibly be decidable?\n\nPg. 4. Could it be possible that softmax rather than hardmax change the decidability result?\n\nPg. 5. L 251. The 3. Clearly, bounded satisfiability is decidable if T(w) is computable because you can enumerate all words up to length n. The important result here is the complexity bound. Also, the way this theorem is written  here is different from Sec. 5. You should rewrite this.\n\nPg. 5. L. 265. In what sense fixed-width arithmetic has a similar effect to bounding the input length? Besides, why not considering then T^FIX only?\n\nPg. 6. L. 302. Why is it the case that T_udec is the weakest class in terms of expressivity? \n\nPg. 6. L. 306. The name \u201coctant\u201d tiling problem does not seem to be standard. It is not mentioned as is in the provided reference. Also it is not clear why it is necessary to distinguish between \u201ctiling\u201d problem and \u201ctiling word\u201d problem. The definitions in the appendix do not make this clear. Could you explain it?\n\nPg. 8. L. 401. It reads \u201cone can reasonably assume the size of a syntactic representation of T to be polynomial on |T|\u201d. It is not clear to me that this assumption is reasonable. Please provide arguments.\n\nPg. 8. L. 402. The so-called polynomial evaluation property is not discussed in Section 3 nor elsewhere in the paper. Why is it reasonable?\n\nPg. 8. It seems that the proof sketch of The 3 does not take into account the representation of w. Please comment on this.\n\nOther comments\n\nPg. 3. L. 138. k should be L since i is a layer, and layers go from 1 to L.\n\nPg. 3. L. 157. different rational number\u201ds\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This technical paper tackles the decidability of satisfiability for Transform encoders (TE), namely, does there exists an input such that the output of the TE is a given constant, say 1. This problem is a generic problem which can encode pattern recognition, etc. The results by the authors are as follows:\n1) in general, the satisfiability problem is undecidable. The proof is by reduction to a tilling system, or equivalently to halting problem - a correct input of the TE leading to output 1 corresponds one to one to a (correct) halting unfolding of a run of the machine. The encoding requires a number of bits for each neuron computation logarithmic in the size of the input, and also that the embeddings are *not* periodic.\n2) If the input is bounded by size n, then the problem is NP-complete(n) (leading to a usual NExptime-complete complexity if n is written in binary), the upper bound being trivial and the lower bound being the same reduction than for undecidability.\n3) Alternatively, if the input is unbounded but the number of bits for each neuron computation is bounded by a number of bits part of the input, then the problem is also NEXPTIME-hard using the exact same encoding, although no upper bound, is provided in this case, not even decidability.\n4) The most interesting decidability result is that if the number of bits for each neuron computation is bounded and the embedding periodic, with both the period and number of bits are part of the input (in unary), then the problem is decidable in NEXPTIME (but no lower bound is provided)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper explores the complexity of Transform encoders. Understanding the theoretical complexity of this hot topic is very timely.\n\n2. The proofs seem solid. 2 constructions are non-trivial (one lower bound encoding (1) and one decidability & (upper bound) complexity proof (4), the other being direct application of the first construction)."
            },
            "weaknesses": {
                "value": "1. The paper could be written in a more reader-friendly way, it is very technical. Statements of the theorems are unnecessarily complicated. The number of Theorems is also inflatted. E.g. theorem 2 should just be a note at the end of theorem 1: \"undecidable in general, even when restricted to log-precision transformers.\" At the end of the day, there are 2 main results in this paper (1 and 4 listed above).\n\n2. The biggest weakness is that the complexity landscape has a serious gap. The authors need 2 restrictions together to get decidability (see 4.), and the proof of undecidability needs both restriction lifted. So what happens if only one of this restriction? Even more problematic, the authors focus heavily on one of these restrictions (bounded-precision, which is arguably a very reasonable restriction to consider), leaving the other (periodicity, a much stronger restriction) as a technical factor that can be easily overlooked by an inattentive reader (for instance, the notation is using a small '_o'). The proof in appendix C reveals that its actually periodicity that is the main driver for decidability, and fix-precision only seems to be accessory to simplify the proof, and may be useful for the Nexptime upper bound complexity."
            },
            "questions": {
                "value": "1. Where is the fix-precision used in the proof of lemma 3? Is it necessary?\n \n2. Do you have proof of decidability for fix-precision (*without* periodic embeddings)?\n\nIn all cases, you cannot draw Sat[T^fix] on the Nexptime ball (in figure 2), as there is no proof it is *in* Nexptime."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies the complexity of deciding whether a transformer accepts a given input sequence. This problem of satisfiability (SAT) is studied on encoder-only transformers. Encoder-decoder transformers are not considered, as they are Turing complete and hence the SAT problem is immediately undecidable. Firstly, the SAT problem is proven to be undecidable for hardmax encoder-only transformers using a reduction to tiling problems, even when the transformer has log precision. Secondly, decidable restrictions are achieved using bounded length sequences or fixed precision in combination with periodic embedding results. However, in these cases, the SAT problem still remains NEXPTIME."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Although the content is very theoretical, the authors take care to first give a general overview of the results (Section 3), after which proof sketches are given (Sections 4 and 5) and more detailed proofs follow in the appendices. This presentation kept the paper relatively digestible, despite the fact I\u2019m not very familiar with this subject matter. From a theoretic viewpoint, I found the results interesting. The practical usefulness seems somewhat limited to me, but as the authors note the presented SAT problem is foundational in relation to safety and verification of model properties and this work provides a useful start in its study."
            },
            "weaknesses": {
                "value": "As I am not really familiar with the computational complexity study of neural networks, I do not feel very confident to judge the significance of the findings. However, some of the assumptions seem to limit the practical usefulness of the results.\n-  As the authors mention themselves, the distinctions between the considered transformer models fall away when a bounded word length is assumed. Given that transformers almost always have a (fixed) context window, such a bounded length assumption seems quite reasonable to me, and the results for unbounded length seem less relevant.\n- The $\\mathcal{T}_{udec}$ class for transformers (and its further restrictions) uses hard-attention instead of soft-attention. Previous works have indicated that soft-attention can achieve significantly different results compared to hard-attention (Strobl et al. 2024).\n- The decidability results of e.g. Theorem 3 are based on naive enumeration, which is not realistic except for very short word lengths."
            },
            "questions": {
                "value": "- Do any of the proven results change if satisfiability is defined as exceeding a given threshold instead of just being equal to 1? This could be a more realistic condition for acceptance.\n\n- Do the authors have an intuition on to what extent their results could generalize to soft-attention?\n\n- It was not clear to me if the definition of SAT on transformers is new, or has been proposed before.\n\n- This perhaps a bit of a broad/vague question, but seeing the many undecidability and hardness results proven in the paper, do the authors think that there is hope for formal verification techniques on transformers? Or should the verification efforts focus on more tractable and less expressive classes of models than the transformer?\n\n*Some small nitpicks*\n\n- Brackets for citations are not used properly. The paper consistently cites transformers Vaswani et al. (2018) instead of transformers (Vaswani et al., 2018).\n\n- The font in Figure 2 is too small for me to be readable without zooming."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}