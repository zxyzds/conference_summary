{
    "id": "cyPMEXdqQ2",
    "title": "Overcoming Lower-Level Constraints in Bilevel Optimization: A Novel Approach with Regularized Gap Functions",
    "abstract": "Constrained bilevel optimization tackles nested structures present in constrained learning tasks like constrained meta-learning, adversarial learning, and distributed bilevel optimization. \nHowever, existing bilevel optimization methods mostly are typically restricted to specific constraint settings, such as linear lower-level constraints. \nIn this work, we overcome this limitation and develop a new single-loop, Hessian-free constrained bilevel algorithm capable of handling more general lower-level constraints. \nWe achieve this by employing a doubly regularized gap function tailored to the constrained lower-level problem, transforming constrained bilevel optimization into an equivalent single-level optimization problem with a single smooth constraint. \nWe rigorously establish the non-asymptotic convergence analysis of the proposed algorithm under the convexity of lower-level problem, avoiding the need for strong convexity assumptions on the lower-level objective or coupling convexity assumptions on lower-level constraints found in existing literature. \nAdditionally, the generality of our method allows for its extension to bilevel optimization with minimax lower-level problem. \nWe evaluate the effectiveness and efficiency of our algorithm on various synthetic problems, typical hyperparameter learning tasks, and generative adversarial network.",
    "keywords": [
        "bilevel optimization",
        "constrained optimization",
        "gap function",
        "single-loop",
        "Hessian-free",
        "convergence analysis"
    ],
    "primary_area": "optimization",
    "TLDR": "We introduce a novel single-loop, Hessian-free constrained bilevel algorithm capable of handling broader lower-level constraints by using a doubly regularized gap function.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cyPMEXdqQ2",
    "pdf_link": "https://openreview.net/pdf?id=cyPMEXdqQ2",
    "comments": [
        {
            "summary": {
                "value": "This paper studies a bilevel problem where lower-level problem is convex with coupled constraints. To avoid joint projection onto the coupled constraint set which requires coupling convexity and is costly, this paper introduces a doubly regularized gap function to convert lower-level problem into a smooth constraint. This constrained problem is then solved by penalty method. Convergence analysis is provided and numerical results validate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This work tackles an important bilevel problem which involves coupled constraints at the lower level.\n2. The design of the doubly regularized gap function is novel and effective. \n3. The numerical studies are comprehensive and thorough."
            },
            "weaknesses": {
                "value": "This paper addresses a relaxation of the original bilevel problem through a penalty reformulation and truncation approach. For the penalty reformulation, it is shown that by selecting a sufficiently large penalty constant $c$, the penalty problem becomes an $\\epsilon$-approximate solution to the original problem. At the same time, the truncation parameter $r$ also appears to need a large value to ensure equivalence with the original problem. But in the final convergence theorem, $r$ is chosen as a constant, independent of the target error $\\epsilon$."
            },
            "questions": {
                "value": "In Proposition 3.1, it is assumed that the optimal solution $(x^*,y^*,z^*)$ to (6) exists with finite $z^*$. How can this be guaranteed? Additionally, how should $r$ be chosen in practice? Setting $r$ too large may hinder algorithm convergence, while choosing $r$ too small could exclude the optimal point $z^*$, which is unknown. \n\nAdditionally, several concurrent related works are missing: [1]--[2]. \n\n[1] A Primal-Dual-Assisted Penalty Approach to Bilevel Optimization with Coupled Constraints. L Jiang, et. al. arXiv:2406.10148. \n\n[2] First-Order Methods for Linearly Constrained Bilevel Optimization. G Kornowski, et.al. \tarXiv:2406.12771.\n\nIn Table 3, the term \"Required accuracy\" could be replaced with a word like \"Time\" to clarify the values reported. As it stands, it is unclear whether the numbers in this table represent accuracy or computation time."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a method for transforming constrained bilevel optimization problems with coupled lower-level constraints into a single-level problem with a smooth constraint, employing a doubly regularized gap function. Based on this reformulation, the authors propose a first-order single-loop penalty-based algorithm called BiC-GAFFA. Theoretically, they establish non-asymptotic convergence results under general assumptions. Additionally, they investigate the extension of the proposed algorithm to bilevel optimization problems featuring minimax lower-level problems, a topic that has received limited attention in the existing literature. Extensive experiments are conducted to validate the practicality, robustness and efficiency of BiC-GAFFA, encompassing multiple complex problem models within the learning problems."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.The paper introduces a smooth constrained reformulation for bilevel optimization with a constrained lower-level problem. This reformulation avoids the use of any implicit functions associated with the lower-level problem, which are typically known to necessitate additional iterations or inexact solutions.\n\n2. The authors present the first-order single-loop algorithm BiC-GAFFA, which exhibits overall complexity that is lower than that of many existing first-order algorithms requiring double-loop iterations. Notably, it does not rely on the assumption of full convexity of the lower-level constraints.\n\n3. When updating the proximal variable $\\lambda$ associated with the multipliers $z$, there is no need to compute the gradient of the gap function. Instead, the update can be effectively executed solely by applying the first-order optimality conditions of the maximization problem.\n\n4. BiC-GAFFA possesses a broader range of potential applications, making it suitable for solving bilevel optimization problems where the lower-level problem is a minimax problem. Existing bilevel optimization algorithms are unable to achieve this simultaneously."
            },
            "weaknesses": {
                "value": "1. From Section 6.2 and A.2.2., we observe that when dealing with nonsmooth problems, such as sparse group lasso and SVM, it is necessary to reformulate them into a smooth form for effective resolution. It may impact the applicability of the algorithms.\n\n2. By introducing the gap function, the authors have designed a single-loop algorithm. However, the dimensionality of the introduced variables remains the same as that of the original problem, resulting in an increased iteration scale. Compared to LV-HBA proposed in [1], there is no significant improvement in mathematical representation or convergence results.\n\n3. The necessity to control multiple iteration step sizes within the algorithm significantly affects the convergence theory and empirical performance, making them highly dependent on the adjustment of these step size parameters.\n\n[1] Wei Yao, Chengming Yu, Shangzhi Zeng, and Jin Zhang. Constrained bi-level optimization: Proximal Lagrangian value function approach and Hessian-free algorithm. In ICLR, 2024."
            },
            "questions": {
                "value": "1. The convergence results for extending the proposed algorithm to bilevel optimization problems with minimax lower-level problems are lacking, which leads to that Section 5 feels somewhat abrupt overall.\n\n2. In the experimental settings, we observe that the step sizes $\\alpha_k$ and $\\eta_k$ are fixed or vary in a specific form. Are the step sizes related to the specific formulations of different problems?\n\nSome more detailed questions:\n\n1. Can the conclusion of Lemma 2.1 be stated as \"$\\mathcal{G}_\\gamma(x,y,z)=0$ if and only if $y\\in S(x)$ and $z\\in \\mathcal{M}(x, y)$\"?\n\n2. In line 845, the loss function in lower-level problem should be $\\mathcal{L}_{tr}$.\n\n3. In line 1007, can you provide the specific mathematical format of the functions $\\mathcal{L}_{gen/det}$"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel algorithm for constrained bilevel optimization (BiO) using a regularized gap function to transform bilevel problems with lower-level (LL) constraints into a single-level optimization problem with a smooth inequality constraint. The authors propose a first-order, single-loop algorithm, BiC-GAFFA, that operates without requiring Hessian evaluations or projections onto the LL constraint set, offering both theoretical convergence results and empirical validation on synthetic and real-world tasks, such as hyperparameter optimization and generative adversarial networks (GANs). The paper also presents an extension of the method to minimax lower level problems, which broadens its applicability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The introduction of the doubly regularized gap function is a novel (as far as I am aware in the context of bilevel optimization) solution to deal with constraints at the lower level in bilevel optimization. This regularization allows the transformation of the bilevel problem into a smooth optimization problem, which simplifies the numerical implementation and analysis. \n\n2. The proposed BiC-GAFFA algorithm avoids costly second-order computations, which is usually the case for methods based on constrained optimization reformulation. The single-loop nature of the algorithm is also computationally efficient, as supported by the non-asymptotic convergence analysis. \n\n3. The extension to minimax inner level problems provides flexibility in dealing with a broader class of problems beyond the traditional settings."
            },
            "weaknesses": {
                "value": "1. Some theoretical assumptions are insufficiently justified (i.e. very weak statement), particularly in Remark 2.5, where it\u2019s stated that the existence of a multiplier in Theorem 2.3 \u201ccan be guaranteed\u201d under certain constraints qualification conditions like MFCQ. Is the assumption in fact guaranteed when the constraints qualification conditions are satisfied? Or does those conditions directly imply that the assumption is satisfied? Please clarify. \n\n2. The update steps in BiC-GAFFA are quite similar to existing constrained optimization reformulation methods, such as primal-dual [1] and dynamic barrier [2] methods. While these approaches have been applied successfully in other constrained settings, the paper does not explain why they are inadequate for the additional lower-level constraint here, suggesting a need for BiC-GAFFA. A discussion on this distinction would help clarify the necessity of more complex methods such as the proposed BiC-GAFFA. \n\n3. The algorithm requires additional sequences (e.g.,  $\\theta_k$  and  $\\lambda_k$), adding to the computational and memory load, which may challenge its feasibility for large-scale applications. \n\n4. The experimental results focus on toy numerical problems and small-scale real-world problems. While these results are encouraging, a broader evaluation on large-scale or diverse domains would better demonstrate BiC-GAFFA\u2019s practical applicability and scalability. \n\nReference: \n\n[1] Sow et al. A Primal-Dual Approach to Bilevel Optimization with Multiple Inner Minima. \n\n[2] Ye et al. BOME! Bilevel Optimization Made Easy: A Simple First-Order Approach."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "n/a"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work studies bilevel problems where the LL problem is constrained. \n\nThe key contributions are:\n\n1. The equivalence between main problem and the smoothed problem (6) under convexity.\n\n2. The relation between (6) and the sequence of problems in Thm 3.3\n\n3. A gradient based algorithm with nonasymptotic grad-norm rate for an instance of problem 3.3\n\nStrength:\n\n1.The paper is technically sound. \n\n2.The comparison against known results are clear, and hence the main contributions are clear.\n\n3. I like the discussions on reformulations and the experiments.\n\nWeakness:\nI hope the authors could address my concerns below.\n\n1. When and why would constraints in the LL be necessary? \n\na. GAN would not require constraints.\n\nb. SVM has uncontrained versions.\n\nc. Hyperparameter tuning / data selection can be solved with more naive approach without bilevel problems.\n\n2. There are more than one way to smoothen/regularize the lower level so that the limit is the original problem.\n\nFor example: one can add positive coefficients c1, c2, so that the bilevel is now F +  c1 f + c2 max(g, 0) . Then by sending c1, c2/c1 to infinity, one could also get an approximate solution. Why would one formulation better than another?\n\n\nMinors: \n1. The notation N_Y(y) and other similar ones are not defined."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "See summary"
            },
            "weaknesses": {
                "value": "See summary"
            },
            "questions": {
                "value": "See summary"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}