{
    "id": "DF5TVzpTW0",
    "title": "Detecting and Perturbing Privacy-Sensitive Neurons to Defend Embedding Inversion Attacks",
    "abstract": "This paper introduces Defense through Perturbing Privacy Neurons (DPPN), a novel approach to protect text embeddings against inversion attacks. Unlike ex- isting methods that add noise to all embedding dimensions for general protection, DPPN identifies and perturbs only a small portion of privacy-sensitive neurons. We present a differentiable neuron mask learning framework to detect these neu- rons and a neuron-suppressing perturbation function for targeted noise injection. Experiments across six datasets show DPPN achieves superior privacy-utility trade- offs. Compared to baseline methods, DPPN reduces more privacy leakage by 5-78% while improving downstream task performance by 14-40%. Tests on real- world sensitive datasets demonstrate DPPN\u2019s effectiveness in mitigating sensitive information leakage to 17%, while baseline methods reduce it only to 43%.",
    "keywords": [
        "Text embedding",
        "Defense Inversion Attack"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=DF5TVzpTW0",
    "pdf_link": "https://openreview.net/pdf?id=DF5TVzpTW0",
    "comments": [
        {
            "summary": {
                "value": "This paper focuses on defense strategies against embedding inversion attacks, a type of privacy attack where attackers attempt to reconstruct original sensitive data from its embedding representation. Existing defense methods commonly add noise uniformly across all embedding features. However, this approach is limited in maintaining model performance and is limited in effectively protecting privacy since, ideally, more noise should be directed towards privacy-sensitive features.\n\nTo address these issues, the authors first assume and validate that embeddings are composed of both privacy-sensitive and privacy-invariant features. Then, they propose an optimization problem in which a differentiable mask is optimized to isolate privacy-sensitive information within learned embeddings. The optimized mask becomes a tool to detect privacy-sensitive features, and by adding noise to these features, the authors achieve defense against embedding inversion attacks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The method is technically sound, successfully enhances benign accuracy in downstream tasks, and prevents privacy leakage.\n\n2. The hypothesis validated in this paper, that features can be divided into privacy-sensitive and privacy-invariant categories, is quite interesting. Building on this, the idea of using a mask to separate these features is also very novel."
            },
            "weaknesses": {
                "value": "1. The paper lacks a formal privacy guarantee. For instance, in methods like LapMech, the authors provide a proof to demonstrate the effectiveness of privacy protection, but this paper lacks such a discussion. Such a drawback raises doubts about the trustworthiness of the proposed method, especially if attackers know the defense mechanism.\n\n2. This defense method requires two datasets, one containing privacy-sensitive data and one without, which necessitates labeling what information as private information. This adds a labeling burden in real-world datasets, as additional annotation is required."
            },
            "questions": {
                "value": "1. Is it possible to formulate privacy realized by this framework?\n\n2. It is recommended that the authors consider adaptive attack scenarios to demonstrate that this defense method remains effective even if the privacy protection scheme is exposed to adversaries."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces DPPN (Defense through Perturbing Privacy Neurons), a novel method that protects text embeddings by selectively identifying and perturbing privacy-sensitive neurons. The experiments show the effectiveness of the method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper has a clear format, highlighting important keywords throughout the paper.\n2. The paper explains the methodology in details."
            },
            "weaknesses": {
                "value": "1. Is LapMech a variant of DP? What is the performance comparison with standard DP? The paper can add a new section on this.\n2. What is \"Downstream\" in utility metric? The paper could explain more about how they measure the utility of the method.\n3. How do you explain the results in Table 7? Table 7 gives more results of the method than Table 1, and It seems that DPPN does not outperform other defense methods in some datasets.\n4. The paper can include some other defense methods in their comparison. Experiments with two baselines are not very convincing.\n5. In Sec 4.1, they mention that \"Vec2text serves as our primary attack model in subsequent experiments.\" So it seems that only one attack model is evaluated in the whole paper, making the defense performance not convincing again. Are there any results for other attack models?\n6. Only Table 3 shows some results for attack models. However, only privacy metrics are presented. How about the utility metrics?\n7. The paper could reorganize some experimental results based on some comments above, making the experiment section better."
            },
            "questions": {
                "value": "Please see those in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents DPPN, a method for defending text embeddings against inversion attacks by selectively perturbing privacy-sensitive neurons. It demonstrates strengths in improving privacy-utility tradeoffs and shows robustness across different datasets and embedding models. However, it also has weaknesses, including the lack of theoretical privacy guarantees and potential challenges in generalizing to new data. Overall, DPPN is a promising approach for privacy protection in text embeddings, but further research is needed to address its limitations and ensure broader applicability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Strengths:\n\n1. Targeted Defense: DPPN focuses on identifying and perturbing only a subset of privacy-sensitive neurons, which is a more efficient approach compared to perturbing all dimensions of the embedding.\n\n2. Improved Privacy-Utility Tradeoff: The paper demonstrates that DPPN achieves a superior balance between privacy protection and maintaining the utility of the embeddings for downstream tasks.\n\n3. Effective Against Real-World Threats: DPPN shows significant effectiveness in mitigating sensitive information leakage on real-world datasets, such as medical records and financial data."
            },
            "weaknesses": {
                "value": "### 1. Personal Information Matching Issue\n\nThe DPPN method may not explicitly detail how to accurately match and process personal information. Ensuring accuracy and security, while protecting privacy, requires careful handling. This involves selecting and optimizing exact and fuzzy matching algorithms to provide accurate results without compromising privacy. The method uses a differentiable neuron mask learning framework to detect privacy neurons related to sensitive information. By assessing the importance of each embedding dimension, the top \\( k \\) dimensions with the highest importance are selected as privacy neurons. Visualizing the proportion of protected private data is necessary. The effectiveness of DPPN relies on accurately identifying and selecting neurons related to privacy information. Inaccuracies in this process may lead to sensitive information leakage.\n\n### 2. Lack of Theoretical Guarantees\n\nDPPN does not provide theoretical guarantees like Differential Privacy (DP), meaning it cannot quantify the degree of privacy protection or ensure the statistical insignificance of including or excluding a single data point. Adapting DPPN's targeted perturbation method to meet DP standards is challenging, as DP requires perturbing all published data for strong privacy guarantees, whereas DPPN only perturbs a subset of dimensions.\n\n### 3. Challenges in Multilingual and Multicultural Backgrounds\n\nExpressions of personal information vary across languages and cultural backgrounds. It is essential to discuss whether the DPPN method can adapt to and effectively handle these differences.\n\n### 4. Real-Time Performance and Computational Cost\n\nThe real-time performance and computational cost of DPPN in practical applications are unclear. This is an important consideration for systems that need to process large volumes of data in real-time. The interpretability of the DPPN method is relatively low, potentially limiting its use in scenarios requiring high model interpretability, such as medical diagnosis.\n\n### 5. Embedding Methods\n\nThe authors use embedding methods like GTR-base, Sentence-T5, and SBERT. It is suggested that traditional methods such as GloVe and Word2Vec be discussed and experimentally analyzed.\n\n### 6. Presentation Issues\n\nTable 9 is too large. Using \\resizebox \\textwidth is not recommended. Table 8: Statistics of datasets. The caption is below the table; the authors should unify the format. Authors can add more related work in the context:\n- Private Language Models via Truncated Laplacian Mechanism EMNLP 2024\n- Differentially Private Language Models Benefit from Public Pre-training PrivateNLP 2020\n\nThe explanations for Figures 1 and 3 are not detailed enough, especially regarding the meaning of arrows and labels. It is recommended that the authors clarify these in the legend."
            },
            "questions": {
                "value": "1. What are the benefits of neuron mask learning and direct matching for replacing PII compared to constructing a series of regular expressions to match and transform PII in the corpus?\n2. What are the benefits or advantages of the author's method compared to traditional DP-based methods?\n3. Could the author fully describe the challenges faced in this research?\n4. The author would be greatly appreciated if the code could be made open source and contributed to the community.\n5. Six datasets were used in the experiments, but only two datasets' results are shown in the main text. Could the author explain in more detail the reasons for choosing these two datasets?\n6. The results in Table 1 show the performance of different methods, but there is a lack of in-depth analysis of these results. It is suggested to add a discussion of the results, explaining why DPPN performs better in these cases."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a defense method DPPN to resist against embedding inversion attacks. In contrast to previous methods that add noise on all embedding dimensions, it recognize privacy neurons that contains more sensitive information and only perturbs them. Therefore, it achieve an excellent privacy-utility trade-off. Extensive experiments show that the defense method can protect private information while maintaining the origin semantic information."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality: The introduction of privacy neurons and targeted perturbation is innovative, departing from conventional methods that apply noise to all dimensions.\n\nQuality: The conducted experiments are comprehensive. The study evaluates DPPN across six datasets, multiple embedding models, and various attack models, showcasing robust and thorough experimental design.\n\nClarity: The paper follows a clear structure, with a logical flow from problem motivation to solution, experiments, and results.\n\nSignificance: The ability to reduce privacy leakage without sacrificing utility makes DPPN relevant for real-world applications where maintaining both privacy and accuracy is critical."
            },
            "weaknesses": {
                "value": "+ The paper does not provide sufficient detail on how parameters $\\xi$ and $\\eta$ in formulas 3 and 5 are selected during the experiments. Please provide additional information about the selection process. This clarification would help readers understand their impact and ensure reproducibility.\n+ The concepts of $D^+$ and $D^-$ are introduced on line 170, but their explanations are deferred until line 204. This gap may confuse readers. It would be clearer if a brief definition were provided when these concepts are first mentioned, or if the detailed explanation were moved closer to the first appearance.\n+ In Fig. 2, the paper introduces the concept of sensitivity for privacy neurons. What if privacy neurons were selected based on their top dimension-wise sensitivity? The paper lacks a study on it."
            },
            "questions": {
                "value": "+ The paper uses a fixed top-k selection method for privacy neurons. What if we choose them based on a threshold of $m$, such as 0.5?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}