{
    "id": "zM92zziRtQ",
    "title": "TLCM: Training- efficient Latent Consistency Model for Image Generation with 2-8 Steps",
    "abstract": "Distilling latent diffusion models (LDMs) into ones that are fast to sample from is attracting growing research interest. However, the majority of existing methods face two critical challenges: \n1) They need to perform long-time learning with a huge volume of real data. \n2) They routinely lead to quality degradation for generation, especially in text-image alignment. \n\nThis paper proposes the novel Training-efficient Latent Consistency Model (TLCM) to overcome these challenges.  \nOur method first fast accelerate LDMs via data-free multistep latent consistency distillation (MLCD), then  data-free latent consistency distillation  is proposed to guarantee the inter-segment consistency in MLCD at low cost.  \nFurthermore, we introduce bags of techniques to enhance TLCM's performance at rare-step inference without any real data, e.g., distribution matching, adversarial learning, and preference learning. \nTLCM demonstrates a high level of flexibility by allowing for adjustment of sampling steps within the range of 2 to 8 while still producing competitive outputs compared to full-step approaches.\nAs its name suggests, TLCM excels in training efficiency in terms of both computational resources and data utilization.\nNotably, TLCM operates without reliance on a training dataset but instead employs synthetic data for the teacher itself during distillation. With just 70 training hours on an A100 GPU, a 3-step TLCM distilled from SDXL achieves an impressive CLIP Score of 33.68 and an Aesthetic Score of 5.97 on the MSCOCO-2017 5K benchmark, surpassing various accelerated models and even outperforming the teacher model in human preference metrics. \nWe also demonstrate the versatility of TLCMs in applications including controllable generation, image style transfer, and Chinese-to-image generation.",
    "keywords": [
        "latent diffusion model",
        "consistency model",
        "acceleration"
    ],
    "primary_area": "generative models",
    "TLDR": "We propose a novel Training-efficient Latent Consistency Model (TLCM) to tackle the challenges of expensive cost and the performance drop when sampling with few steps in large distilled latent diffusion models.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zM92zziRtQ",
    "pdf_link": "https://openreview.net/pdf?id=zM92zziRtQ",
    "comments": [
        {
            "summary": {
                "value": "The authors propose TLCM (Training-efficient Latent Consistency Model) for accelerating text-to-image latent diffusion models in a data-free manner with a small amount of training time. The key innovation is a two-stage distillation process. A data-free multistep latent consistency distillation (MLCD) is proposed to accelerate the base model, followed by another improved data-free latent consistency distillation to ensure global consistency. The authors enhance the model's performance through several techniques including latent LPIPS for perceptual consistency, multi-dimensional preference learning, distribution matching, and adversarial learning. Their empirical results show that TLCM can generate high-quality images in just 2-8 inference steps."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "-\tThe authors propose a two-stage distillation scheme that progressively distills the model to a few-step regime with reduced training costs.\n-\tThe proposed method achieves superior results with SDXL on COCO-5k generation."
            },
            "weaknesses": {
                "value": "-\tThe submission is basically a technical report on achieving the SOTA few-step image generation performances with data-free multi-step consistency distillation, which is a successful practical combination of [1] and MCM. The second distillation stage also uses a series of SOTA techniques including ADD, DMD, and preference tuning. There are very limited scientific insights or technical innovations.\n-\tThe current presentation is poor and needs substantial improvement. For instance, there is extensive and unnecessary usage of acronyms, making the submission so confusing and hard to read.\n-\tLimited empirical evaluation. The method is only validated on SDXL. How does it perform on other diffusion models with different architectures, e.g., Pixart?\n\nReferences\n\n1.\tKohler, Jonas, et al. \"Imagine flash: Accelerating emu diffusion models with backward distillation.\" arXiv preprint arXiv:2405.05224 (2024)."
            },
            "questions": {
                "value": "As in weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a direct combination of Multistep Consistency Models [1] and Latent Consistency Models [2], resulting in a Multistep Latent Consistency Model.\n\nAdvantages:\n1. While straightforward, the idea shows some novelty\n2. The writing is acceptable, though the innovation isn't immediately apparent\n\nDisadvantages:\n1. The experimental results appear counterintuitive: contrary to normal expectations where more sampling steps yield better results, this paper shows deteriorating FID scores with increased sampling steps\n2. The paper lacks directness - the core innovations are hard to identify, as the authors seem to attempt highlighting multiple contribution points\n3. The generated images predominantly show a cyberpunk style, lacking photorealism and overall quality\n\nTechnical Issues:\n1. Equation 1 appears incorrect - contains an extra x_t term\n   Should be: dx_t = f(x_t, t)dt + g(t)dw_t\n\n2. Results Inconsistencies:\n   - Tables 1 and 3 show FID increasing with more sampling steps, contradicting common understanding\n   - Ablation study shows puzzling results: combining all techniques significantly worsens FID scores\n   - Equations 11-13 look unnecessary, just traditional confrontational losses.\n\nWriting Concerns:\nThe paper lacks focus on core contributions. It is hard for me to distinguish core contributions.\n\nReferences:\n[1] Heek et al., \"Multistep consistency models,\" 2024\n[2] Luo et al., \"Latent consistency models,\" 2023\n[3] Song et al., \"Consistency models,\" 2023"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Advantages:\n1. While straightforward, the idea shows some novelty\n2. The writing is acceptable, though the innovation isn't immediately apparent"
            },
            "weaknesses": {
                "value": "Disadvantages:\n1. The experimental results appear counterintuitive: contrary to normal expectations where more sampling steps yield better results, this paper shows deteriorating FID scores with increased sampling steps\n2. The paper lacks directness - the core innovations are hard to identify, as the authors seem to attempt highlighting multiple contribution points\n3. The generated images predominantly show a cyberpunk style, lacking photorealism and overall quality"
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Existing methods for distilling the pretrained knowledge of Diffusion Models have several drawbacks; expensive training time, necessity of large scale real data, and image generation quality given a few steps to generate an image. To this end, the authors propose a data-free distillation pipeline named as Training Efficient Latent Consistency Model (TLCM). Briefly, the initial parameter of TLCM is obtained by the proposed first process, and the second process boosts global consistency of TLCM. The experiment results show that TLCM has benefits over the baselines for the short training time and the data-free mechanism."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Interesting idea to use synthetic data for distillation.\n- Reasonable motivations."
            },
            "weaknesses": {
                "value": "- Overview figure is very complex to understand. My suggestion is that it would be better if the right figure is decomposed into two parts; section 4.2 and section 4.3\n- Weak explanations on the experiment settings.\n    - What is the meaning of each measure? \n    - Is higher better or lower better? \n    - How many samples are used? \n    - What was the input to generate the samples to measure?\n    - It sounds more reasonable if the proposed methods contain only IS or FID like [1]. The aim of the work is to distill the knowledge of the pretrained models and reduce the required sampling steps to some extent. Shouldn\u2019t it enough If the generated images by TLCM  (2-8 steps) are as realistic as DDIM? Please provide justifications why the other metrics are needed in detail.  \n- Weak interpretations of the experiment results.\n    - (L364-L365) The proposed methods use synthetic data to distill the pretrained knowledge while the baselines are using real data. How does the proposed method show better performance than the baselines? It sounds more reasonable if the baseline is a sort of upper bound.\n    - FID of TLCM (2 steps) is better than FID of TLCM (8 steps). It also shows the pattern that the # of steps and FID are inversely proportional, which is not reasonable. \n    - Prioritize what information is important for each table and emphasize those numbers.\n\n- Too many engineering techniques are applied for improving the marginal performance. They dilute the main point of the paper.. \n\nOverall, the proposed methods are interesting, but the results (FID) are not convincing enough.\n\n[1] PROGRESSIVE DISTILLATION FOR FAST SAMPLING OF DIFFUSION MODELS, ICLR\u201922"
            },
            "questions": {
                "value": "(L144-145) This is a vague statement. What is the meaning of the effectiveness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "- The authors propose a multi-step distribution matching diffusion model to generate samples within few steps. By dividing the whole iterations into several predefined steps, the consistency model can focus on matching distribution in narrower range.\n- Adversarial loss, latent-LPIPS loss, and human preference loss have also been adopted to boost the performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Explanation about existing methods have been given enough. It is easy to follow and the application of each component sounds reasonable.\n- Resulting images with only 4 steps look quite impressive.\n- Necessary ablation studies to make this method solid have been provided."
            },
            "weaknesses": {
                "value": "- Even though the results are quite impressive, this method is a combination of Multistep Consistency Models(MCM), Distribution Matching Distillation(DMD), and DiffusionGAN. It is hard to ignore novelty issue and this would be the main reason of my decision.\n- Too many notations made me really hard to follow this paper. It would be much better if some expressions can be trimmed.\n\nMinor issue\n- The caption of the 3rd row in Fig.3 should be modified."
            },
            "questions": {
                "value": "- After the emergence of rectified flow transformer, I think these kinds of distillation methods should be re-assessed. Apart from the non-predictable nature of the original diffusion trajectory, we now can learn the straightened flow of latent space and the distillation must be a lot easier. Even in the paper of SD3, it says that if the model capacity is given enough, reducing timesteps into a small number sacrifices little performance. This is not a critical comment about this paper and I would recommend the authors to think about this aspect."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a method for latent diffusion model distillation. The core idea of this paper is mainly inspired by latent consistency distillation. The authors implemented some improvements based on the previous success of diffusion distillation, such as sparse timesteps. To prevent the cost of collecting real-world data, the proposed method is trained on generated data from the teacher model instead. A bag of independent techniques such as adversarial training are further included to help the model match the SoTA performance. The evaluations are performed on SDXL."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper presents the method in a very intuitive way with very informative equations and figures. The overall writing quality of this paper is good."
            },
            "weaknesses": {
                "value": "- Novelty\n\nThis paper presents insufficient novelties. The entire method seems to be an ensemble of diffusion distillation techniques already proven effective. For example, this paper advocates the advantage of relying on no training data and uses synthesized data instead. However, this has already been proven useful in SD3-turbo [1], and learning from generated data is by no means a new idea under the context of model distillation. \n\n- Empirical evaluations\n\nThis paper is presented with insufficient evaluations. SDXL, which is a relatively old model, is the only one evaluated in the paper. And there are some important comparisons missing such as SD3-turbo, DMD, and instaflow. \n\nThe authors claim, 'We only report FID for reference and do not analyze it since FID on COCO is not reliable to evaluate text-to-image models.' I agree that FID is not the best way for diffusion evaluation. However, the values do reveal something about the proposed methods. I noticed that the FID values increase with the number of steps for the proposed method. In my understanding, this is a result of learning distillation from generated data instead of real data, as the modeled distribution deviates further from the real-world data distribution. \n\n- Supports to claims \n\nI find some claims in the paper require further support. For example, the authors claimed 'They (other distillation methods) need to perform long-time learning with a huge volume of real data.' I find it very hard to agree with this. According to my experience, methods such as LCM can be trained very fast with a small portion of training data. The authors are strongly encouraged to fill in the 'TH' value for LCM in Table 1 for a direct comparison. For the rest of the metrics, I don't see how the proposed method clearly outperforms the rest, except for IR, which is a score that the model is explicitly optimized for. \n\n\n[1] Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation, arxiv."
            },
            "questions": {
                "value": "1. Does the 70GPU-hour training time include the time spent on data generation? The overall cost of data generation is needed to justify L241 'with cheap cost in data-free manner.'"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}