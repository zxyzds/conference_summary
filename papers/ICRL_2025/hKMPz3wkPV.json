{
    "id": "hKMPz3wkPV",
    "title": "Towards a formal theory of compositionality",
    "abstract": "Compositionality is believed to be fundamental to intelligence. In humans, it underlies the structure of thought, language, and higher-level reasoning. In AI, it enables a powerful form of out-of-distribution generalization, in which a model systematically adapts to novel combinations of known concepts. However, while we have strong intuitions about what compositionality is, there currently exists no formal definition for it that is measurable and mathematical. Here, we propose such a definition, which we call representational compositionality. The definition is conceptually simple, quantitative, and grounded in algorithmic information theory. Intuitively, representational compositionality states that a compositional representation is both expressive and describable as a simple function of discrete parts. We validate our definition on both real and synthetic data, and show how it unifies disparate intuitions from across the literature in both AI and cognitive science. We also show that representational compositionality, while theoretically intractable, can be readily estimated using standard deep learning tools. Our definition has the potential to inspire the design of novel, theoretically-driven models that better capture the mechanisms of higher-level human thought.",
    "keywords": [
        "compositionality",
        "complexity",
        "deep learning",
        "representation",
        "generalization"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "We present a formal, mathematically-precise definition of compositionality. We argue that the theory accounts for and extends our intuitions about compositionality, and can inspire novel methods for learning compositional representations.",
    "creation_date": "2024-09-14",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=hKMPz3wkPV",
    "pdf_link": "https://openreview.net/pdf?id=hKMPz3wkPV",
    "comments": [
        {
            "title": {
                "value": "We define compositionality of representations, rather than functions"
            },
            "comment": {
                "value": "Thank you for the comment and for your interest in our work! Compositionality of functions is indeed a very important topic in machine learning, and we agree that it can and has been nicely formalized using ideas from compositional generalization. However, what we attempt to define here is the compositionality of *representations*, rather than *functions*. For instance, a representation might be the latent activity in some intermediate layer of a neural network, or some brain region. There is no current definition for the compositionality of representation, although we have intuitions about them (for instance, \"disentangled\" representations are a paradigmatic examples). In this paper, we are attempting to formalize and generalize these intuitions.\n\nNote that in our reply to other reviewers and in our updated paper, we plan to clarify the relationship between representational compositionality (as we define it) and compositional generalization, and indeed this will relate to the \"function compositionality\" of the function processing the representation. In particular, we hypothesize that increased representational compositionality puts fewer demands on the structure (or compositionality) of the downstream function required for compositional generalization. As we will explain in those other responses, however, representational compositionality also relates to other important topics apart from compositional generalization (e.g., generative models in latent space, multi-task adaptation, etc.). Crucially, we believe that *both* of these notions of compositionality (of representation and of function) are crucial to understand and formalize for machine learning and cognitive science."
            }
        },
        {
            "title": {
                "value": "Previous technical definitions and quantification of compositionality"
            },
            "comment": {
                "value": "We thank the authors for this interesting submission. While the authors propose an interesting definition of compositionality based on representations and Kolmogorov complexity (the main novel contribution of this paper), we would like to bring to their attention previous definitions of compositional functions such as the one discussed in Pagin & Westerst\u00e5hl (2010), which has been recently built upon to provide a quantification of compositionality  (Ram et al., 2024). The generality of their definition is demonstrated by applying it to various off-the-shelf sequence processing models such as recurrent, convolutional and transformer based neural networks. Their notion of compositional complexity is then theoretically tied to the expressivity and systematic generalization of any compositional model.\n\nA new definition is always interesting and can provide a new perspective, but it is also useful to understand how they compare and contrast to existing mathematical definitions.\n\nP. Pagin, and D. Westerst\u00e5hl. \"Compositionality I: Definitions and variants.\" Philosophy Compass 5.3 (2010): 250-264. https://doi.org/10.1111/j.1747-9991.2009.00228.x\n\nP. Ram, T. Klinger, and A. G. Gray. \"What makes Models Compositional? A Theoretical View.\" IJCAI 2024. https://www.ijcai.org/proceedings/2024/0533.pdf"
            }
        },
        {
            "summary": {
                "value": "The authors present a definition for representational compositionality, aiming to turn an old and only vaguely defined concept into a precise mathematical definition. They ground their definition in Kolmogorov complexity and demonstrate that for synthetic datasets, this quantity conforms well with intuitive expectations of how representational complexity should vary with different hyperparameters. They then consider two empirical representations and use an approximation to Kolmogorov complexity to compute their compositionality."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I think this is a generally well-written and thought-provoking paper. Compositionality and compositional generalization is an important topic across a number of different fields and I agree that a better formal understanding of what we mean by it is important. The authors present a conceptual framework that is well thought out and clearly presented and explain the intuitions behind it well --- I really appreciated the clarity of explanation especially given the highly conceptual and theoretical nature of the work. The empirical results demonstrate that this could be a useful metric and provide interesting use cases to think about.\n\nA few other points I liked about the paper:\n- I liked that the authors explicitly spelled out what bearing their definition has on various related concepts in the field.\n- I appreciated the point of having a useful quantitative point of reference (i.e. compositionality of 1 indicating a complete lack of compositionality).\n- I also thought the example of the grammar representations was a compelling argument for this definition."
            },
            "weaknesses": {
                "value": "I think the manuscript in its current form has two points that weaken its case for the proposed definition. One concerns discussing how this definition connects to compositional generalization and one concerns further demonstrating its practical usefulness and providing a concrete way for follow-up work to use this definition.\n\n**Relationship to systematic generalization**\n\nAs the authors note in their introduction, compositionality is a highly relevant topic in part because it can enable systematic out-of-distribution generalization. This provides a complementary approach towards this question, i.e. compositional representations are representations that enable systematic generalization. There are a number of prior works who have laid out formal or conceptual frameworks relating different representations, architectures, and inductive biases to systematic/compositional generalization (albeit sometimes under different terms) [e.g. 1-4; as well as some papers that the authors already cite, e.g. Ren et al., 2023] and it would be useful to discuss at least some of them in terms of how they relate to this approach, e.g.: Do they get at the same concept of compositionality or do they address a different problem? How specific would you predict your definition to be, i.e. will neural networks that generalize well have highly compositional representations according to your definition?\n\nI note that the authors already discuss systematicity and generalization in p. 252-262, but so far this discussion seems to be more focused on generalization of the functional embedding rather than the entire network of which $Z$ may be a part (or the input).\n\n**Practical usefulness**\n\nI found the authors suggested framework in Appendix B very intriguing and I think having such a concrete method for estimating compositionality could further increase the impact of the authors' proposed definition. I also think empirically evaluating the relationship between representational compositionality and generalization would be really interesting (e.g. across different network architectures that yield better or worse generalization, is there a correlation between compositionality and generalization?). I think adding this kind of practical method or evaluation would further increase the impact of this paper. I also understand, though, that addressing this may not be realistic during the revision period and I think the paper is still interesting and valuable without this --- as the authors note, the primary contribution of this paper is of theoretical/conceptual nature.\n\n1. Hupkes, Dieuwke, et al. \"Compositionality decomposed: How do neural networks generalise?.\" Journal of Artificial Intelligence Research 67 (2020): 757-795.\n2. Jarvis, Devon, et al. \"On the specialization of neural modules.\" arXiv preprint arXiv:2409.14981 (2024).\n3. Abbe, Emmanuel, et al. \"Generalization on the unseen, logic reasoning and degree curriculum.\" International Conference on Machine Learning. PMLR, 2023.\n4. Lippl, Samuel, and Kim Stachenfeld. \"When does compositional structure yield compositional generalization? A kernel theory.\" arXiv preprint arXiv:2405.16391 (2024)."
            },
            "questions": {
                "value": "- See weaknesses.\n- To what extent are the changes in topological similarity due to different factors in the lookup tables because that implicitly changes the embedding dimensionality of each individual symbol. For smaller dimensionality (e.g. due to longer sentences), there will be more variance in how correlated different symbols are with each other, leading to lower correlation, whereas for larger dimensionality, most of them will be approximately orthogonal, leading to higher correlation, correct?\n- Could you apply the prequential coding method to the synthetic examples in section 4.1, to further validate it?\n- Could you specify the numerical values on the y axis in Fig. 2?\n\nMinor comments:\n- L. 151: superfluous \u201ca\u201d\n- L. 178: \u201cwho\u2019s\u201d -> \u201cwhose\u201d\n- L. 374 \u201crather\u201d -> \u201crather than\u201d\n- L. 400: \u201cwho\u2019s\u201d -> \u201cwhose\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a formal definition of representational compositionality based on Kolmogorov complexity from the perspective of compression. The paper verifies that the definition aligns with human intuition on some examples in synthetic and real datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe motivation of this paper is greatly appreciated. It focuses on proposing a new formal definition of compositionality, which is sometimes a vague notion in previous literature. The definition based on compression is also novel.\n\n2.\tSome parts of the definition match well with intuition. For example, if the function $f$ to construct the representation $Z$ has no structure and is very complex to the extent of a look-up table, then intuitively this representation is not compositional. Accordingly, the proposed metric also shows low compositionally.\n\n3.\tThe proposed metric shows the extent to which the representation is compositional, rather than a binary indicator."
            },
            "weaknesses": {
                "value": "1. My biggest concern for this paper is its presentation. Although the notion of compositionality is abstract in nature, I think the presentation in Section 2 and 3 could be clearer if properly accompanied by concrete examples. I was a bit struggled with the abstract words like \u201crepresentations\u201d \u201cconstituents\u201d \u201csemantics\u201d \u201csentences\u201d \u201clanguage\u201d \u201csystematicity\u201d \u201cmodularity.\u201d They prevented me from reading through the whole section smoothly.\n\nI find the example in Appendix D useful for understanding the proposed definition, and I encourage the authors to move some of them into the main text. On the other hand, however, these examples are still not concrete enough. For example, Example 5 says \u201c$f$ is modular.\u201d Then, what is a modular function? It will look much better if the authors can write out *the specific formula* of the function (maybe in a toy setting) and use *a figure to illustrate* how this function structure contributes to high compositionality.\n\n2.\tThe assumption that the representation $Z$ follows a normal distribution seems arbitrary. Is there previous work to provide theoretical support for this assumption? Or is there any empirical evidence?"
            },
            "questions": {
                "value": "1.\tIn Line 270, what is the definition of *modularity*?  Is modularity a specific form of *systematicity* in Line 253? If so, I think it is not appropriate to put these two terms in parallel.\n\n2.\tIn the experiment on lookup table representations, are the sentence length and vocabulary size correlate with each other? For example, if vocabulary size is very large, then the sentence length is likely to be 1.\n\n3.\tAre there any practical implications of this theoretical definition of representational compositionality. In the conclusion, it says that the compositionality can be used to score tokenization schemes. Could you elaborate on this?\n\nI\u2019m also curious about whether this definition of representational compositionality can be readily applied to vision tasks. There have been some advancements in forcing CNNs to learn compositional parts [c1,c2], and object-centric learning [c3]. I wonder if the definition can be further validated under these settings.\n\n4.\tSome recent works studies the compositional generalization in object-centric learning, e.g., the reference [c4]. Although it focuses on a more concrete setting rather than a general definition as in this paper, it provides insight into how we can prove compositional generalization in a formal way. Could you briefly discuss how the definition of compositionality in this paper relates to or differs from that in [c4], and why viewing compositionality from the view of compression is \u201cbetter\u201d?\n\n[c1] Zhang et al. Interpretable Convolutional Neural Networks. CVPR 2018.\n\n[c2] Interpretable Compositional Convolutional Neural Networks. IJCAI 2021.\n\n[c3] Locatello et al. Object-Centric Learning with Slot Attention. NeurIPS 2020.\n\n[c4] Wiedemer et al. Provable Compositional Generalization for Object-Centric Learning. ICLR 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In machine learning applications, we're often interested in understanding whether\na model has learned to solve a problem using a compositional computation, or\nin evaluating whether it has discovered the latent compositional structure underlying\nsome domain of interest. This paper describes a procedure for answering these questions quantitatively. It\nprovides tools the compositionality of an\narbitrary collection of vector-valued datapoints $z$, using algorithmic\ninformation theory as a tool.\n\nFrom a technical perspective: we first posit that data results from a generative\nprocess of the following form:\n\n$$ \\textrm{words } w \\sim p(w) $$\n$$\\textrm{statistics } (\\mu, \\sigma) = f(w) $$\n$$\\textrm{true data } z \\sim N(\\mu, \\sigma) $$\n\nNext, we optimize $p$ and $f$ to minimize the overall Kolmogorov complexity:\n\n$$K(Z) = K(p) + K(f) + \\sum_i \\log p(w_i) + \\log p(z_i | w_i)$$\n\nwhere\n\n$$p(z_i | w_i) = N(z_i ; f(w_i))$$\n\nFinally, we measure the compositionality of the representation system as:\n\n$$C(Z) = K(Z) / K(Z | W) = K(Z) / (K(f) + \\log p(z_i | w_i))$$\n\nfor $p$ and $f$ as chosen above. Intuitively, a representation $z_i$ is \"more\ncompositional\" if it is easy to reconstruct given a simple \"parts-based\"\nrepresentation $w_i$.\n\nAt a high level: I really like what this paper is trying to do---this is an important topic and I'm \nvery excited about the idea\nof trying to better ground intuition about compositionality in terms of\nalgorithmic information theory. However, I think there are a few major issues\nwith the formulation presented in the current paper, and I don't think this work\nis quite ready to publish in its current form.\n\n## RELATION TO EXISTING WORK\n\nThe paper claims that nobody has attempted to offer a quantitative, graded\nmeasure of compositionality targeted at modern representation learning\napplications. This is not quite right: people were thinking about similar\nissues as part of the general late-2010s multi-agent communication fad, and I\nwould encourage the authors to check out https://arxiv.org/abs/1902.07181 and\nfollow-ups for some earlier work trying to answer related questions. The\ntechnical approach proposed in the current paper is substantially different from this past work---in\nparticular, it successfully infers latent compositional descriptions of\ndata---but many of the experiments are very similar and it\nmight be informative to discuss some relationships to\nrepresentation-reconstruction techniques in addition to the existing discussion of topological similarity.\n\n## DEGREES OF FREEDOM\n\nMy primary technical concern with the paper is the following: there's some\nsymmetry between $p$ and $f$ in the optimization problem described above, and as a\nconsequence different minima will give very different answers to the question of\nwhether a given representation system is compositional. Here's an argument that\nI think captures the essence of the issue (we could be more precise about some\nconstant factors, but it wouldn't change the overall conclusion).\n\nSuppose I have a set of $N$ random bitstrings of length $M$. This set is\nincompressible, so no matter how I set $p$ and $f$ I'm going to have $K(Z) = MN$.\n\n[For simplicity, let's also assume that $p(z ; f(w))$ isn't Gaussian, but instead a\nsequence of $M$ Bernoulli distributions whose parameters are given by $f(w)$. The\nchoice of Gaussians is described as arbitrary in the paper, and it wouldn't be\ntoo much more work to rephrase the argument below with that parameterization.]\n\nNow consider two different schemes for setting $p$ and $f$.\n\n**Scheme 1**: $p(z)$ is a deterministic distribution that places its mass on a single\nword $w$, and $f(w)$ outputs the all-0.5s vector (so both functions are simple, and $K(p)$ and $K(f)$ are both\nnegligible). Then $K(W | p) = 0$ (it's deterministic), and $K(Z | W, f) =\n-\\sum_{i=1}^N \\sum_{j=1}^M \\log p(z_{ij}) = -\\sum_{ij} \\log 1/2 = MN$ (it's the\nlog-probability of a sequence of fair coin flips). So $K(Z) = MN$ as expected, and\n$C(Z) = 1$ (since all terms but K(Z | W, f) are negligible or zero).\n\nScheme 2: Each word $w$ is just a copy of its associated bitstring $z$; $p(w)$ places a\nuniform distribution over all length-$M$ bitstrings, and $f(z)$ is the identity\nfunction (so a 0 in a bitstring gets turned into a Bernoulli that always outputs\n0). $K(p)$ and $K(f)$ are negligible as before. But now $K(W | p) = -\\sum_{i=1}^N\n\\log p(w_i) = -\\sum_{i=1}^N M \\log 1/2 = MN$, while $K(Z | W, f) = 0$ (since all\ndecoding is deterministic). $K(Z) = MN$ as before, but $C(Z)$ is **arbitrarily\nlarge** as both terms in the denominator are negligible relative to MN.\n\nSo in fact this definition of compositionality allows us to conclude that incompressible noise is either highly compositional, or highly non-compositional.\n\n## CONCEPTUAL CONSIDERATIONS\n\nTaking a step back: there's quite a lot of work---in fields ranging from formal\nsemantics to category theory---that offers a precise mathematical treatment of\ncompositionality (just not one that's graded or approximate in the way this\npaper and the above work is aiming at). One of the big differences between the\nexisting literature and current paper is that compositionality is usually\nthought of as a property of *interpretation functions*, not *sets of\nrepresentations*: if we have a space of \"sentences\" and \"meanings\", each\nequipped with some algebraic structure, then a mapping from sentences\nto meanings is compositional if it is a homomorphism with respect to this\nstructure (see e.g.  https://plato.stanford.edu/entries/montague-semantics/).\n\nThis is closer to the paper's Section 3.1, which fixes the encoding scheme W and\nits prior, and just tries to optimize the function f that translates ws into zs.\nBut in the experiments using this definition, $K(Z)$ is fixed to a constant, so\nwe're really just estimating something like $1/K(Z|W)$, which is just a measure of\nhow *predictable* $z$s are from $w$s, without saying anything about the structure\nthat relates them.\n\nAnd I think this gets at the fundamental conceptual issue with the current\nversion of the paper---that the definitions of $C$ and $C^L$ don't actually say\nanything about *parts*, or putting them together. They're just measuring a\npredictability relationship between $z$s and $w$s, and we can make systems look more\nor less \"compositional\" under these definitions by moving the cost of that\nprediction operation into p or f. The brief mention that \"structure-preserving\nmaps have low Kolmogorov complexity\" does all the work of relating these\noperations to compositionality as usually understood, such that we should maybe\nthink of the paper as arguing that low Kolmogorov complexity is necessary and\nsufficient for high compositionality (which I don't think is what was intended, and\nin any case orthogonal to the current experiments and technical presentation).\n\n## NEXT STEPS\n\nSo what would it look like to have a definition of compositionality grounded in\nalgorithmic information theory? I think the answer has to focus on mappings\nrather than representations---fixing both $Z$ and $W$ (as in 3.1, and existing work\non the subject). Then plausibly we could introduce a new latent variable that\ndoesn't describe an encoding of the data, but the structure of the mapping.\nMaybe something like:\n\n- Given w, compute $f(w) = w_1, ..., w_n$\n- translate each $w_i$ into $g(w_i) = z_i$\n- combine all $z_i$ into $h(z_1, ..., z_n) = z$\n\nThen a system is compositional if this decomposition is *useful* for\nimplementing a low-complexity mapping, which you could characterize by e.g.\nfinding the most aggressive decomposition that remains close to the\nunconstrained complexity $K(Z|W)$. This is of course all speculative! The main\nthing I want to communicate is that, while there are issues with the current\nproposal, I think the high-level idea is really promising, and could probably be\nmade to work if re-oriented around a definition of compositionality as a\nproperty of maps rather than sets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Very interesting problem formulation and approach"
            },
            "weaknesses": {
                "value": "- Fundamental identifiability questions make current problem formulation non-meaningful\n- Unclear connection to other formal theories of compositionality"
            },
            "questions": {
                "value": "- Did I miss something in the argument above?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a novel metric to measure representational compositionality from the perspective of algorithmic information theory. Specifically, by estimating the Kolmogorov complexity of the representations (Z) and the corresponding message matrix (W), the paper defined compositionality as the ratio of K(Z) to K(Z|W). Compared with earlier colloquial definitions of compositionality, the proposed metric is well-founded on algorithmic information theory. It has the potential to inspire a more detailed analysis of different practical systems. Compared with existing metrics like topological similarity, the proposed metric aligns better with our intuitions of compositional generalization, and also has the potential to be extended to more general scenarios. The paper also showcased how to estimate such a metric using standard deep-learning tools. The experimental results all support the analysis well. To the best of my knowledge, this is the first paper that formally defined compositionality using Kolmogorov complexity and the authors did a good job of practically estimating this metric in different scenarios. I hence suggest an acceptance."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well-written and easy to follow.\n- The discussion of limitations of colloquial compositionality highlights the necessity of a formal definition.\n- Reaching the definition of compositionality step-by-step makes it easier to understand how different components of the final metric are formulated.\n- The experiments are conducted at different levels (i.e., synthetic representations, emergent communication, and natural language)."
            },
            "weaknesses": {
                "value": "- The practical experiments can be more practical. It might be helpful to consider some LLMs. For example, if we can obtain the representations of some LLMs, how would the compositionality of these representations evolve during finetuning?\n- Although the authors discussed it in the conclusion part, more discussions on how the proposed metric can inspire the new algorithm design can make the paper stronger. For example,  are there particular machine learning tasks or model architectures where you believe optimizing for this compositionality metric could lead to improvements?\n- The role played by W and Z might be a bit confusing. It might be more helpful to draw a system diagram combining representation learning (i.e., how Z is estimated) and the sentence W together. IIUC, if we call our high-dim input X, consider an unsupervised representation learning as an example, our system should be X\u2192Z\u2192X\u2019. We then have the hidden representation of input signals. After that, when we evaluate how good Z is, we introduce the messages W and f to evaluate the corresponding Kolmogorov complexity."
            },
            "questions": {
                "value": "- For the first line of equation (1), if $p_w, W, f$ are all empty strings, then the right-hand side is still minimized. The equation is degenerated to be K(Z)=K(Z). Is that the case?\n- Is there any particular reason to define C(Z) as the ratio of two K terms? What about the K(Z)-K(Z|W)? What\u2019s the difference between these two options?\n- It might be more helpful to provide a more formal definition of topological similarity. It appears in Figures 2 and 3. But defining them in the main context might be helpful.\n- Still about topsim. This metric is notoriously known for its high complexity because we must calculate the pair-wise distance of all the examples. What about the complexity of the proposed method?\n- Figure 3 caption: iterated learning is an inductive bias \u2026 I guess it should be \u201citerated learning amplifies the inductive bias of model\u2019s learning that more compositional mappings are learned faster\u201d?\n- In section 4.2, the last paragraph, what is the \u201cnormal language\u201d system trained without iterated learning?\n- Results in section 4.3 demonstrate that different languages have different compositionality. Are there any results in linguistics that can verify this claim? Furthermore, the experiments show that Japanese has a negative topsim, is there any intuitive way to understand what this means?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}