{
    "id": "NhqKHHK4Nk",
    "title": "Boosting Recovery in Transformer-Based Symbolic Regression",
    "abstract": "The traditional objective in regression is generalization. That is, learning a function from training data that performs well beyond the training data. Symbolic regression adds another objective, namely, interpretability of the regressor.  \nIn the context of regression, interpretability means that the representation of the regressor facilitates insights into mechanisms that underlie the functional dependence. State-of-the-art symbolic regressors provide such insights. However, the state of the art predominantly incurs high costs at inference time. The recently proposed transformer-based end-to-end approach is orders of magnitude faster at inference time. \nIt does, however, not achieve state-of-the-art performance in terms of interpretability, which is typically measured by the ability to recover ground truth formulas from samples. Here, we show that the recovery performance of the end-to-end approach can be boosted by carefully selecting the training data. We construct a synthetic dataset from first principles and demonstrate that the capacity to recover ground truth formulas is proportional to the available computational resources.",
    "keywords": [
        "symbolic regression",
        "interpretability",
        "transformer",
        "recovery"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We show that the recovery performance of the end-to-end symbolic regression approach can be improved by carefully selecting the training data.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=NhqKHHK4Nk",
    "pdf_link": "https://openreview.net/pdf?id=NhqKHHK4Nk",
    "comments": [
        {
            "title": {
                "value": "Rebuttal"
            },
            "comment": {
                "value": "Thank you for your review. Before we address your questions, we would like to comment on the weaknesses mentioned. \n\n**Eval. on complex problems** \n\nThe recovery performance of SOTA symbolic regressors degrades with growing function complexity, which is typically measured by the size of a minimal expression tree. A lower bound on the complexity is two times the number of input variables. The E2E symbolic regressor is no exception to this rule. We have added a plot to the supplement that shows this phenomenon for various SOTA symbolic regressors. \n \n**Limitations of the standardization approach**\n\nThe standardization practically affects only the shift and the scale of the output. In practice, those effects can be dealt with by optionally fitting a constant to the generated expression.\n\n**Unclear advantages over E2E**  \n\nThe performance of the E2E approach depends on both the architecture and the training data generation. We keep the architecture of the original E2E approach, and only differ in the data generation method. Our novel data generation method empirically shows a drastically improved recovery.\n\n### **Questions and answers**\n\n**Do minimal RMPs have equivalent expr. trees?**\n \nYes, for each RMP there exists an equivalent expression tree. In RMPs, however, subexpressions can be reused, which makes for more compact representations. Therefore, we can express the same expression more succinctly. Once the minimal RMPs have been identified, it does not make a difference, whether we train on the expression trees or RMPs.\n\n**For Figure 4, could you provide:**\n- **R\u00b2 accuracy performance SRBench**\n\nThe R2 accuracy performance on SRBench ground truth problems is already provided in the supplement in Figure 6. However, in the context of symbolic regression, we consider the accuracy-complexity trade-off as shown in Figure 7 of the supplement more important than a pure accuracy measure such as R2, because low complexity leads to better interpretability. \n\n- **Fitting acc. (R\u00b2) relative to E2E**\n\nThe accuracy-complexity tradeoff for E2E and E2E-RMP is shown in Figure 7 (left) of the supplement. E2E-RMP is only slightly worse in terms of accuracy (R2) compared to E2E, but the generated expressions are far less complex and more interpretable.  \n\n**In Figure 5, why does recovery performance drop with context length?** \n\nWe see a recovery decrease with increasing input length, larger models decrease much less (Table 5 supplement). We conjecture that increasing the input length while keeping the parameter size fixed increases the required computational complexity for the model to make an accurate prediction. \n\n**How time-consuming is the data generation process?**\n\nEnumerating and checking RMPs with a maximum length of five lines and a maximum number of five input variables took approx. one hour on an Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz.  \n\n**Could E2E generate the same number of equivalent groups given equal time?**\n\nEven if the standard E2E sampling process would cover the same number of equivalence classes given the same time, the problem still would be that these classes are scattered over many complexity levels and therefore miss many low complexity equivalence classes, representing interpretable functions.\n\n**Have you explored reducing equation length?**\n\nSystematically enumerating succinct expressions, which in a sense means small equation length, is exactly what we do in our method. The other important contribution is the standardization of input and output values. Note that in the noise-free setting, recovery implies perfect accuracy. Therefore, one cannot improve recovery at the cost of reduced accuracy.\n\n**Would it be possible to share the code?**\n\nWe plan to make the code publicly available on GitHub. \n\n**Could you explain**\n- **Why mRMPs per dimension decreases from D=3 to D=5?**\n\nThis is implied by the combinatorics of the problem.  When the number of dimensions increases, then more binary operators (addition or multiplication) are needed to generate valid RMPs. Every additional input requires one more operator to be a binary operator, which reduces the number RMP lines, where the operator can be chosen freely (unary or binary). \n\n- **Is the constant range reasonable for practical problems?**\n \nThe model does not predict constants but placeholders. Therefore, it does not need to generalize to out of range constants. Placeholders are replaced by actual constants by a fitting procedure. During training, the constant range can be set by the user to any interval $[a,b]$.\n\n- **Examples of the recovered formulas** \n\nWe added the recovered formulas that result from generalization to the supplement. \n\n- **\"implicitly\" vs \"explicitly\"?**\n\nExplicitly means enumerating all small expressions, whereas implicitly means steering a search towards small expressions by a search policy.\n\n- **In Line 200-203, why is $c_3\\in\\mathbb{R}$?**\n \nHere, we scale the vector $x\\in\\mathbb{R}^D$ by the scalar $c_3\\in\\mathbb{R}$."
            }
        },
        {
            "title": {
                "value": "Rebuttal"
            },
            "comment": {
                "value": "Thank you for your review. We welcome the opportunity to clarify the novelty of our approach before addressing your specific questions and comments.\n\n**Novelty.**  In contrast to search-based approaches to symbolic regression, the transformer-approach needs training data. That is, the performance of the latter approach depends on the architecture as well as on the training data. The focus in previous work was mostly on the architecture. You are completely right, we keep the architecture of Kamienny 2022. However, we propose a novel scheme for generating the training data. Previous sampling approaches, for instance D\u2019Ascoli 2022, have two major shortcomings: 1. They are not able to systematically cover expressions with succinct representations and by that miss many important candidate functions that therefore can mostly not be recovered by the symbolic regressor, and 2. they are more dependent on the sampling domain, that is, they do not generalize beyond that domain. This becomes apparent in a poor recovery rate. We address the first shortcoming by generating *minimal* register machine programs for our training expressions, and the second shortcoming by standardizing the input as well as the output of the functions used in training.\n\n\n\n### **Questions and answers** \n\n\n**l.123 The random sampling method for expression trees was introduced in Lample & Charton 2020 (Deep learning for symbolic mathematics)** \n\nThank you, we have corrected this.\n\n **l.147 Kamienny 2022 clearly shows the benefit of estimating constants at inference, and fine-tuning them using BFGS, over predicting a \"skeleton\" with a special token replacing the constant. You seem to be using the latter, why?** \n\nKamienny 2022 have to fit many constants, because they do not use standardization on inputs *and* outputs, and thus have to rescale the outputs for prediction, which includes fitting many constants. Since we standardize the inputs and outputs, we avoid the rescaling and have expressions with fewer constants. Using skeletons in this scenario has the advantage that a beam search can cover a more diverse set of candidate functions.  \n\n**l. 191 Charton 2021 is not about symbolic regression, maybe cite D'Ascoli 2023 instead, who observe the overfitting**\n\nThank you, we have changed this.\n\n**l. 310 the three token representation for floats was introduced in Charton 2021**\n\nThank you, we have changed this. \n\n\n**l. 324: \"As usual, this number is rounded to the next power of two, which here is 64.\" why round the output sequence length to a power of two in an encoder-decoder architecture?** \n\nHere, we were referring to the decoder block size during batch training. Rounding the output sequence length to a power of two in an encoder-decoder architecture *during training* is computationally advantageous in terms of memory access, leading to faster attention computation. To avoid confusion, we have changed this in the paper.\n\n**l. 330 \"The RMP tokens are embedded into d_emb-dimensional space using a standard embedding layer. The embeddings are then fed into a standard transformer decoder stack.\" Are you feeding the desired output as output of the decoder? This is not clear.** \n\nSince we employ the same architecture as Kamienny 2022, the RMP tokens are first embedded and then fed into the decoder *during training*. At inference, the decoder is, of course, inferred in a step-by-step next token prediction manner. \n\n\n**l. 333 \"total context sizes of 256, 512, and 1024, respectively\": context size makes no sense in a decoder-only architecture.** \n\nFor the training, in the ablations, we were referring to the sum of the encoder block size (192, 448, or 960 tokens) and the decoder block size (64 tokens) as context length. In the paper, we have corrected this confusing notation, and now refer to the encoder block size as input length. \n\n\n**l. 339 \"Models are trained until the loss on the validation set is saturated.\" Can you explain what you mean by \"saturated loss\"?**\n\nWe optimize the cross-entropy loss over the training set. Every $k$ steps, we compute the loss on a validation set. If the validation loss decreases, then we store the decreased validation loss. If the validation loss does not decrease for $k\\cdot n$ steps, then we consider it to be saturated and stop the training. \n\n\n**l. 347 the R2 score certainly predates La Cava, it is usually attributed to Pearson.**\n\nWe did not mean to attribute the R2 score to La Cava, but only to mention that it is used in their benchmark. We have clarified this."
            }
        },
        {
            "title": {
                "value": "Rebuttal"
            },
            "comment": {
                "value": "Thank you for your review. Here, we answer your question. \n\n**How does this method achieve fine-grained control of registers to complete a series of complex computational tasks?**\nThe model generates the register machine programs sequentially, token by token. The registers are named $a_i$, while the inputs are named $x_i$ and the constants are named $c_i$. In order to control the registers in a series/sequence of computations, the model can reuse existing results from previous registers by referring to the register name."
            }
        },
        {
            "summary": {
                "value": "This paper aims to improve the recovery performance of transformer-based symbolic regression methods through more systematic training data generation. The authors represent equations as register machine programs (RMPs) and propose finding minimal RMPs with standardized input/output variables. The approach shows improved recovery rates on simpler equations from benchmarks like Feynman equations while maintaining the fast inference advantage of transformer-based approaches."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Addresses an important limitation in transformer-based symbolic regression methods by focusing on their poor recovery performance\n- The proposed minimal RMP generation approach shows promise in reducing expression complexity\n- Demonstrates meaningful improvement in recovery rate over E2E methods for simpler Feynman problems while maintaining fast inference times"
            },
            "weaknesses": {
                "value": "- Limited evaluation on complex problems - the approach's effectiveness seems primarily demonstrated on simpler equations\n- The standardization approach, while helpful for training, may limit the model's ability to handle complex nonlinear relations with varying constant ranges\n- The empirical advantages over E2E approach are not clearly demonstrated through comprehensive metrics (e.g., R\u00b2 accuracy, black-box problems), especially given that E2E's data generation could potentially be adjusted to achieve similar results"
            },
            "questions": {
                "value": "* Do minimal RMPs have equivalent expression trees? Can you comment on potential performance if the model was trained on expression tree versions of the same minimal RMP datasets?\n\n* Figure 4 presents recovery rate results. Could you provide:\n   - R\u00b2 accuracy performance on SRBench problems (both ground truth and black-box functions)\n   - Fitting accuracy (R\u00b2) comparison across different dimensions relative to E2E\n\n* In Figure 5, why does recovery performance drop with context length? Shouldn't more observations improve the performance?\n\n* Given that data generation includes RMP enumeration for finding minimal RMPs, how time-consuming is the data generation process?\n\n* You report that in 10K equations, E2E contains around 6.4K equivalence groups, and E2E prioritizes fast data generation. Could E2E generate the same number of equivalent groups given equal time?\n\n* One drawback of E2E data noted in the paper is its tendency toward complex forms, especially compared to low-dimensional Feynman equations. However, E2E's data generation can be controlled through hyperparameters (minimum/maximum number of unary and binary operators). Have you explored if reducing equation length in E2E could lead to simpler equations and better recovery, even if it comes at the cost of lower accuracy?\n\n* Would it be possible for the authors to share the code for data generation and model weights?\n\n* Could you explain:\n   - Why mRMPs per dimension decreases from D=3 to D=5 in Appendix Figure 3?\n   - Is the constant range [-10, 10] reasonable for practical problems, given E2E uses [-100, 100]? Can the model generalize to out-of-range constants?\n   - Could you provide examples of the 17% of recovered formulas that result from generalization (Lines 409-412)?\n   - What does \"implicitly\" vs \"explicitly\" mean in Lines 077-079?\n   - In Line 200-203, why is c\u2083 \u2208 \u211d? Shouldn't it be c\u2083 \u2208 \u211d\u1d30?\n\n[Note: There appears to be a typo in Line 285 where \"RPM\" should be \"RMP\"]"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a a structured way to  select the training data and construct a synthetic dataset from first principles, improving the interpretability and efficiency of symbolic regression using transformer models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The article is clearly written and its structure is well-organized\n\n2. The experimental validation in the article is sufficient.\n\n3. The method proposed in the article is practical and demonstrates good effectiveness and innovation.\n    What\u2019s most valuable is that the method integrates well with the hardware."
            },
            "weaknesses": {
                "value": "The paper highlights the transformer model\u2019s tendency to memorize training data, as 51% of the recovered formulas are from the training set. This method is somewhat too direct, which may lead to overfitting of the model."
            },
            "questions": {
                "value": "How does this method achieve fine-grained control of registers to complete a series of complex computational tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the recovery performance limitation in transformer symbolic regression models by introducing a novel data generation approach using register machine programs (RMPs). After pre-training previous transformer SR models with the same architecture on this new data, the authors demonstrate significant improvement in recovery rates while maintaining fast inference times."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Addresses a valid and current challenge in transformer-based symbolic regression methods\n* Takes a novel approach by focusing on data generation improvements\n* Shows great results in the improvement of recovery rates from the new data generation setting"
            },
            "weaknesses": {
                "value": "**Major Concerns:**\n\n* Overlook of literature: For example, [1] is the pioneering work in transformer SR which also similarly follows skeleton-based training (having placeholder parameter for constants in symbolic expressions).\n\n* While the paper focuses on recovery rate as the interpretability metric, evaluation on semantic symbolic correctness metrics such as out-of-domain generalization and extrapolation would also be helpful as they are more flexible than recovery rate and can consider other possibilities such as mathematical approximations or equivalence.\n\n* The paper's approach of normalizing outputs in addition to inputs may fundamentally alter the underlying function behavior and the mapping to the corresponding symbolic function. For example, If we normalize the output, the functions with same skeleton but different constant values may collapse which might lead to negligible impact of some symbolic terms. I do think that normalization might help transformer to have better memorization mostly for simple problems like Feynman. How do authors make sure that the correspondence between symbolic expressions and data observations are following original data behavior after normalization, particularly for more complex expressions?\n\n* The main novelty is in data generation for transformer SR model training, specifically the representation of expressions as register machine programs (RMPs). Not enough evidence is provided justifying RMP over expressions as sequence (prefix notation). Additional experiments are needed:\n  1. Ablation for performance with different data generation components \n  2. Comparison with [1] which also generates expression skeletons with placeholder parameters. [1] has shown a better recovery rate than (kamienny et al., 2022) due to its simpler data generation setting and focus on lower-dimensional problems. I would be interested to see the comparison of your method with [1] on Feynman problems with d_max = 3.\n\n\n* Concern on the reported results in Figure 4: \n  1. I don't understand why robustness to noise improves this much compared to other baselines. There's no detail from authors about adding noise to the new training data. Why this happen?\n\n  2. Limited comparison with recent SR models like uDSR [2], TPSR [3] and PySR [4]\n\n*  It's not clear what are the main features in new data generation setting that lead to this recovery boost? Ablation analysis is needed on the data generation components. For example, RPM vs expression prefix notation, target scaling, RMP verification steps, etc.\n\n\n**Minor Comments:**\n* Introduction writing cold be improved. There should be more focus on the contributions of the work than motivation or examples for symbolic regression. Figures 1-2 could move to appendix.\n\n* What beam size / inference sampling size were used for E2E and E2E-RMP results?\n\n---\n\n[1] Biggio et al., Neural Symbolic Regression that Scales, 2021\n\n[2] Landajuela et al., A Unified Framework for Deep Symbolic Regression, 2022\n\n[3] Shojaee et al., Transformer-based Planning for Symbolic Regression, 2023\n\n[4] Cranmer et al., Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl, 2023"
            },
            "questions": {
                "value": "provided above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new generation method for end to end symbolic regression with transformers, replacing the tree representation commonly used to sample functions by Register Machine Programs (RMP), a representation equivalent to the directed acyclic graphs that appear when one removes common subexpressions in trees. \n\nA training set is generated by enumerating all RMP for a given input dimension, up to a small number of instructions (ie operations). Then, the model proposed in Kamienny et al. (2022) is trained on this dataset, and tested on two small external test sets: Feynmann and Strogatz, for a total of 130 examples. \n\nOn this test set, the authors observe that the recovery rate of their new models is better than previous end to end transformer-based approaches."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea of adapting the training set of symbolic regression models, by selecting \"more desirable\", simpler, functions, is promising."
            },
            "weaknesses": {
                "value": "Novelty is extremely limited. The paper uses the architectures and generation techniques from D'Ascoli 2022 and Kamienny 2022, and the constant estimation methods from Biggio 2021. The main new element is the introduction of RMP, but its advantage over the expression trees used in previous works is not clear. \n\nIn previous works, expression trees are randomly generated, and a random tree usually has no common subexpressions (i.e. the corresponding DAG is the tree). As a result, the RMP introduces in this paper are most of the time equivalent to the trees they are supposed to replace. Besides, the RMP seems to result in longer sequences than enumerated trees. For instance, the expression $-sin(x_0)+0.3sin(x_0-x_1)$, given as an example in section 3.1, can be represented as a tree with no common subexpressions, and tokenized as 10 tokens. The corresponding RMP uses 31 tokens. What is the benefit?\n\nThe paper is difficult to read, it contains a number of incorrect and controversial statements (see below and questions), and does not clearly describe the generation techniques used, and the architecture ablation performed.  For example, figure 4 suggests data generation includes noise, this is not mentioned in sections 2.2 and 2.4. Section 3.1 describes the architecture used as an encoder-decoder model (as Kamienny), but line 332 states \"Therefore, a training example with 192, 448, and 960 data points and 64 RMP tokens results in total context sizes of 256, 512, and 1024, respectively.\" which evokes a decoder-only (GPT-like) architecture, where input and output are concatenated. This notion of \"context length\" is repeated in the ablations. \n\nThe evaluation is very limited. The model is evaluated on 130 examples only. Figure 5 suggests that model performance (recovery) on its train and test set is around 15%, but it is 40% on the Feynman dataset. This may be due to the fact the most functions in the Feynman set are extremely simple. Besides the authors acknowledge that more than half of the Feynman test functions are already in the train set. This data contamination weakens the claims made in the paper."
            },
            "questions": {
                "value": "* l.123 The random sampling method for expression trees was introduced in Lample & Charton 2020 (Deep learning for symbolic mathematics)\n* l.147 Kamienny 2022 clearly shows the benefit of estimating constants at inference, and fine-tuning them using BFGS, over predicting a \"skeleton\" with a special token replacing the constant. You seem to be using the latter, why?\n* l. 191 Charton 2021 is not about symbolic regression, maybe cite D'Ascoli 2023 instead, who observe the overfitting\n* l. 310 the three token representation for floats was introduced in Charton 2021\n* l. 324: \"As usual, this number is rounded to the next power of two, which here is 64.\" why round the output sequence length to a power of two in an encoder-decoder architecture? \n* l. 330  \"The RMP tokens are embedded into demb-dimensional space using a standard embedding layer. The\nembeddings are then fed into a standard transformer decoder stack.\" Are you feeding the desired output as output of the decoder? This is not clear.\n* l. 333 \"total context sizes of 256, 512, and 1024, respectively\": context size makes no sense in a decoder-only architecture\n* l. 339  \"Models are trained until the loss on the validation set is saturated.\" Can you explain what you mean by \"saturated loss\"?\n* l. 347 the R2 score certainly predates La Cava, it is usually attributed to Pearson."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}