{
    "id": "epbXCD1Ifk",
    "title": "Offline Reinforcement Learning With Combinatorial Action Spaces",
    "abstract": "Reinforcement learning problems often involve large action spaces arising from the simultaneous execution of multiple sub-actions, resulting in combinatorial action spaces. Learning in combinatorial action spaces is difficult due to the exponential growth in action space size with the number of sub-actions and the dependencies among these sub-actions. In offline settings, this challenge is compounded by limited and suboptimal data. Current methods for offline learning in combinatorial spaces simplify the problem by assuming sub-action independence. We propose Branch Value Estimation (BVE), which effectively captures sub-action dependencies and scales to large combinatorial spaces by learning to evaluate only a small subset of actions at each timestep. Our experiments show that BVE outperforms state-of-the-art methods across a range of action space sizes.",
    "keywords": [
        "reinforcement learning",
        "offline reinforcement learning",
        "combinatorial action space"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=epbXCD1Ifk",
    "pdf_link": "https://openreview.net/pdf?id=epbXCD1Ifk",
    "comments": [
        {
            "summary": {
                "value": "This paper tackles the problem of offline RL in combinatorial action spaces, where actions are formed by combinations of sub-actions, leading to a rapid increase in possible actions. Such action spaces challenge standard offline RL methods, particularly in handling dependencies among sub-actions and limited data availability.\n\nThe authors propose a novel method, Branch Value Estimation (BVE), to address these issues by structuring the action space as a tree to capture sub-action dependencies effectively. Their approach evaluates only a subset of possible actions, making the process more computationally efficient. BVE introduces a behavior-regularized temporal difference (TD) loss designed to reduce overestimation bias in such combinatorial settings, leading to more stable value estimates.\n\nIn their experiments, BVE consistently outperforms established baselines, including methods like Factored Action Spaces and Implicit Q-Learning, demonstrating superior performance and stability across a variety of high-dimensional environments. The results suggest that BVE is particularly effective in scenarios where standard methods fail to handle sub-action dependencies within combinatorial action spaces efficiently.\n\nThis paper\u2019s contributions include (1) introducing the BVE method with a novel value estimation strategy for structured action spaces, (2) providing theoretical insights into the impact of sub-action dependencies on offline RL performance, and (3) an extensive evaluation demonstrating BVE\u2019s effectiveness in complex, high-dimensional environments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Originality. The paper introduces a novel way to represent combinatorial action spaces using trees. The branch value estimation technique presents a valuable solution to action selection in large spaces. The combination of beam search with RL is innovative and hadn't been applied this way before. The approach to handling sub-action dependencies through tree traversal is original. The method provides a new angle on addressing overestimation bias in offline RL.\n\n2. Quality. The empirical results demonstrate clear performance advantages over existing approaches. The ablation studies effectively isolate the contribution of each component. The method successfully scales to large action spaces of up to 4 million actions. The consistent performance across different environment sizes shows robustness. The experimental design directly addresses the key claims.\n\n3. Clarity. Complex concepts are explained through effective visualizations and examples. The algorithm descriptions are precise and well-detailed with clear pseudocode. The paper follows a logical progression that builds understanding. The experimental setup and results are presented transparently. The technical content strikes a good balance between depth and accessibility.\n\n4. Significance. The paper tackles a fundamental challenge in applying RL to real-world problems with combinatorial action spaces. The computational efficiency improvements make previously intractable problems manageable. The method has potential applications in important domains like healthcare and robotics. The approach opens new directions for handling complex action spaces in RL.\n\n5. Technical contribution. The tree-based representation effectively reduces the search space while maintaining performance. The branch value estimation technique successfully handles dependencies between sub-actions. The beam search integration provides an efficient solution for policy extraction."
            },
            "weaknesses": {
                "value": "1. Limited comparative evaluation: The paper compares against only two baselines (FAS and IQL), which leaves questions about relative performance against other approaches. Particularly relevant is the recent parallel work on factorized action spaces (https://openreview.net/forum?id=STwxyUfpNV) which offers additional baselines and testing environments. \n\n2. Theoretical analysis: While the empirical results are promising, the paper would benefit from theoretical analysis of convergence properties and formal bounds on beam search approximation error. Understanding the conditions under which BVE is guaranteed to perform well would help practitioners apply the method with confidence. This theoretical foundation would also help characterize the relationship between approximation quality and final policy performance.\n\n3. Technical details: The paper could better explain several important implementation aspects, particularly the interaction between depth penalty and beam search during action selection. A more detailed analysis of computational overhead would help understand practical scaling properties, while clearer explanation of terminal state handling would aid reproducibility. The trade-offs between computational cost and performance deserve more thorough quantification.\n\n4. Empirical analysis: The method's robustness to different data distributions could be more thoroughly investigated to understand its reliability in varied conditions. The ablation studies could provide deeper insights by more systematically exploring beam width's impact on performance. Additionally, a more detailed analysis of how performance scales with increasing numbers of sub-actions would help understand the method's limitations.\n\n5. Scope of claims: The paper makes several broad claims about BVE's capabilities that could benefit from more nuanced presentation. While the claim about scaling to large action spaces is well supported empirically, the claim about \"effectively capturing sub-action dependencies\" needs stronger evidence from more diverse types of dependencies. The paper states BVE \"outperforms state-of-the-art methods\" but this is only demonstrated against two baselines. Similarly, while empirical results suggest reduced overestimation bias, the mechanism and conditions for this reduction could be better explained with theoretical analysis."
            },
            "questions": {
                "value": "Theory:\n- Could you outline what theoretical guarantees might be possible for the tree traversal algorithm's convergence properties?\n- How would you characterize the relationship between beam search approximation and policy optimality in your method?\n- Could you elaborate on the specific conditions under which BVE would be expected to perform optimally?\n\nComparative evaluation and claims:\n- Your paper addresses similar challenges as \"An Investigation of Offline Reinforcement Learning in Factorisable Action Spaces\" (https://openreview.net/forum?id=STwxyUfpNV) which relies on a value function decomposition. Have you considered comparing against their adaptations of BCQ, CQL, and IQL?\n- Could you evaluate BVE on the DeepMind Control Suite environments with discretised actions (or other complex environments with strongly dependent sub-actions) to validate the broad claim about outperforming state-of-the-art methods?\n- What evidence supports the claim about \"effectively capturing sub-action dependencies\" beyond the current experiments?\n\nTechnical implementation and scaling:\n- Could you explain how the depth penalty and beam search interact during action selection?\n-  What is the computational overhead of tree construction and traversal compared to baselines?\n- How are terminal states handled in the tree structure?\n\nEmpirical analysis:\n- How does the method's performance scale with increasing numbers of sub-actions?\n- Could you characterize the method's robustness to different data distributions?\n- Could you provide more detailed analysis of how beam width affects the trade-off between performance and computational cost?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses the challenge of combinatorial action spaces in offline reinforcement learning by proposing a method called Branch Value Estimation (BVE). BVE utilizes a tree structure to effectively prune the action space, organizing actions into a hierarchy where each node in the tree represents a sub-action conditioned on the values from its parent node. This structure, to some extent, captures dependencies among sub-actions and also reduces the number of actions evaluated at each timestep. As the tree is traversed, BVE estimates the highest achievable Q-value at each branch, allowing for the selection of the optimal action. The effectiveness of BVE is demonstrated through comparisons with state-of-the-art offline RL algorithms across various action space sizes, showing improved performance in environments with up to over 4 million possible actions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper explains its main idea clear.\n2. The paper targets an important problem.\n3. The application of tree-structured action search seems new."
            },
            "weaknesses": {
                "value": "The critique identifies several weaknesses that should be addressed:\n\nClarity on Action Space Reduction: The paper states that BVE reduces the effective action space by organizing it into a tree structure, unlike traditional RL methods which often misidentify the optimal next action a^\u2032\\hat{a}\u2019a^\u2032 in equation 4. However, it remains unclear whether the overestimated actions are effectively excluded by this method. The tree traversal procedure appears optimistic about the estimated action values. It is not evident from the paper which design choice specifically helps to prevent overestimation in an offline setting, as the behavior cloning itself contributes to this effect. If the authors want to claim the tree-based method can mitigate overestimation, either theoretical or empirical evidence should be provided. \n\nApplication to Online RL: Given that the tree structure concept for managing large action spaces could also be beneficial in online RL settings\u2014which are generally less challenging than offline settings\u2014it\u2019s surprising that the authors chose not to begin with online RL.\n\nBroader Applicability and Contributions: The potential for applying the tree-structure approach in other offline RL methods raises questions about the broader significance of the work. If applicable, the contribution could be more substantial.\n\nAblation Study and Its Implications: The message of the ablation study is ambiguous. Figure 1 only displays two deltas, with Figure (b) showing a difference only when alpha=0, while Figure (c) suggests that DQN performs poorly even in small action spaces, raising questions about its suitability as a competitor. Additionally, it\u2019s unclear whether the DQN is used in an offline learning setting, which would be unusual.\n\nJustification for Design Choices: Equation (4) appears abruptly, with no justification for discounting ||a' - \\hat{a}\u2018||. The paper should clarify why this design was chosen.\n\nAddressing Dual Challenges: The authors attempt to address both the challenges of large discrete action spaces and offline learning. This dual focus can cause confusion regarding the contributions and effects of the design choices. Two primary questions should be addressed:\nScalability: As the action space increases, can this approach maintain high computational and sample efficiency compared to other baselines? This should be the first point to verify given that the proposed idea mainly addresses scalability. Learning curves in the form of performance v.s. computation time/number of samples are expected. Overestimation in Offline Settings: How does this approach avoid the overestimation problem in offline settings, or what offline RL algorithms can be integrated if it does not?\n\nEmpirical Results and Parameter Selection: The empirical results in Figures 8 and 9 do not show statistically significant differences, raising concerns about the effectiveness. The selection process for IQL and FAS hyperparameters is also not clearly described.\n\nOmission of Related Work: The paper fails to cite several highly relevant works, including:\n\u201cConjugate Markov Decision Processes\u201d by Philip Thomas et al., which deals with extremely large action spaces.\n\u201cReinforcement Learning with Function-Valued Action Spaces for Partial Differential Equation Control\u201d by Yangchen Pan et al., applicable to high-dimensional continuous control and potentially adaptable for discrete settings.\n\u201cDeep Reinforcement Learning in Large Discrete Action Spaces\u201d by Gabriel Dulac-Arnold et al.\n\u201cConditionally optimistic exploration for cooperative deep multi-agent reinforcement learning\u201d by Xutong Zhao et al., which utilizes a tree-structured action for exploration purposes in a multi-agent, centralized setting. Note that, although it is a multi-agent setting, the subagents' actions are concatenated to a vector for execution, which can be thought of as one agent with high dimensional discrete actions."
            },
            "questions": {
                "value": "see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the challenges of reinforcement learning in combinatorial action spaces, which arise from executing multiple sub-actions simultaneously. The exponential growth in action space size and the interdependencies among sub-actions complicate learning, particularly in offline settings with limited and suboptimal data. Current methods often simplify this by assuming independence among sub-actions. The authors propose Branch Value Estimation (BVE), a method that effectively captures these dependencies and scales to large combinatorial spaces by evaluating only a small subset of actions at each timestep. Experimental results demonstrate that BVE outperforms baseline approaches across various action space sizes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The motivation of this paper is clear: the issue of combinatorial action spaces in offline reinforcement learning has indeed been underexplored, making it worthy of further attention. The experiments presented are also quite persuasive and effectively demonstrate the advantages of BVE."
            },
            "weaknesses": {
                "value": "1. The method introduction in this paper lacks clarity, particularly regarding the process of constructing the tree and the Q-learning based on the tree, making it difficult to understand the underlying design principles. For example, in the tree shown in Figure 1, are the elements in the third and fourth layers fixed, or can their positions be swapped? Additionally, the output of \\( f(s,a) \\) includes \\( v \\), but based on the example in Figure 3, the dimensions of \\( v \\) corresponding to different layers of the tree are different. So what is the dimensionality of the output of \\( f(s,a) \\)? Is it fixed or variable? I haven't found definitive answers to these questions in the paper.\n\n2. The paper claims that the proposed method can eliminate the assumption of action space independence from previous works; however, it does not explicitly explain why BVE can effectively consider the interdependencies among different sub-actions, merely presenting the process without clearly outlining its advantages.  So I recommend the author to provide a more explicit explanation or analysis of how BVE captures sub-action dependencies.\n\n3. I understand that the design concept may be similar to using a binary search method to find the maximum value, thereby obtaining the maximum among exponentially many action combinations with logarithmic operations. I'm not sure if my understanding is accurate, so I suggest the authors further develop the convergence theory of Q-learning under this search paradigm, which would enhance the paper.\n\n4. Although the paper considers the problem within an offline RL framework, it only uses an optimization objective with a behavior cloning regularization term without further addressing out-of-distribution (OOD) or distribution shift issues. This makes the offline RL setting seem unnecessary here, so the authors might consider examining the method's performance in an online setting. Additionally, I recommend the authors refer to the following paper, which, while focused on multi-agent reinforcement learning, also addresses the OOD problem in combinatorial action spaces in offline settings using a counterfactual Q-learning design, which could be relevant to this paper's problem setup.\n\n5. Regarding the experimental section, I find the current experimental setup somewhat artificial and lacking in realistic tasks. I suggest the authors conduct experiments in more meaningful real-world scenarios.\n\n[1] Counterfactual Conservative Q Learning for Offline Multi-agent Reinforcement Learning"
            },
            "questions": {
                "value": "The same as the questions in Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}