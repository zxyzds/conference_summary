{
    "id": "5IWJBStfU7",
    "title": "Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?",
    "abstract": "As AI systems are increasingly deployed in high-stakes real-world applications, ensuring their interpretability has become critical. Mechanistic Interpretability (MI) is a promising approach that aims to reverse-engineer neural networks to extract simple, human-understandable algorithms embedded in the neural structure that explain the model\u2019s behavior. \nIn this work, we investigate a fundamental concern with concrete formalizations of MI: do current criteria guarantee the identifiability of the explanation? We borrow the concept of identifiability from statistics to express the intuition that an explanation should be unique, meaning that the criteria for selecting explanations should not allow for multiple, incompatible solutions.\n\nWe identify two broad strategies to produce MI explanations: (i) \"where-then-what\", which first identifies a subset of the network (a circuit) that replicates the model's behavior before deriving its interpretation, and (ii) \"what-then-where\", which begins with candidate explanatory algorithms and searches in the activation subspaces of the neural model where the candidate algorithm may be implemented, relying on notions of causal alignment between the states of the candidate algorithm and the neural network. We systematically test the identifiability of both strategies using simple tasks (learning Boolean functions) and multi-layer perceptrons that are small enough to allow for the complete enumeration of candidate explanations. Our experiments reveal that current criteria suffer from identifiability issues at every stage: multiple circuits can replicate model behavior, multiple interpretations can exist for a circuit, several algorithms can be causally aligned with the neural network, and each algorithm can be aligned to multiple, different, subspaces of the neural network.\n\nThese findings suggest that current criteria are too permissive and need refinement to ensure identifiability. \nWe discuss the generalization of our results to larger models and potential fixes based on stricter criteria. Our work aims to contribute constructively to the ongoing effort to develop rigorous formalizations of MI's assumptions.",
    "keywords": [
        "AI interpretability",
        "mechanistic interpretability",
        "causal consistency",
        "explanatory algorithms",
        "circuits"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=5IWJBStfU7",
    "pdf_link": "https://openreview.net/pdf?id=5IWJBStfU7",
    "comments": [
        {
            "summary": {
                "value": "A summary of mechanistic interpretability is provided. Two approaches are proposed that attempt to interpret a \u201csimpler\u201d algorithm that emulates the behavior of a trained neural network in terms of a circuit. One approach focuses on modelling the behavior of the full network before finding a subset of related nodes, while the second approach focuses on finding an \u201cimportant\u201d sub-network of the full network, whose behavior is then interpreted. Both approaches are showcased in simple toy examples."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The manuscript is clearly written, with a good explanation of the context the work is placed in. The toy examples provided are illustrative. The overarching conclusion is that uniqueness of an explanation should generally not be expected within the context of mechanistic interpretability, and while a similar analysis cannot be conducted on large-scale models, it is likely that the same behavior could be expected. This insight is important in many practical applications where network interpretations are required (post-training) by a practitioner."
            },
            "weaknesses": {
                "value": "The issue of uniqueness of an explanation is addressed in the context of mechanistic interpretability. However, the \u201cincompatibility\u201d of different explanations is not substantially addressed. A more formal framework in which incompatibility can be \u201cmeasured\u201d would be very interesting, along with analyzing questions on differentiating between equivalence classes of explanations."
            },
            "questions": {
                "value": "If the training set is drawn from a distribution with certain biases, there may be correlations that essentially encourage multiple \u201cconflicting\u201d interpretations of a network. Can we resolve some of the issues that arise by putting conditions on the training distributions?\n\nWhat would (be expected to) happen if, in a simple toy example, an experiment was repeated with a perfect training error, with or without overfitting? Would we see a qualitatively different distribution of explanations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the \\emph{identifiability} of mechanistic explanations from interventional evaluations on toy neural networks. The authors find clear non-identifiability at multiple stages of the interpretability pipeline: multiple interpretations can exist for a circuit, multiple circuits can generate the same behavior, each algorithm can be aligned to multiple activation subspaces, etc.\nThis non-identifiability persists regardless of whether MI explanations are generated first by localizing a subset of the network, then deriving an interpretation, or first generating a candidate algorithm and trying to then find an activation subspace corresponding to that algorithm."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "A rare case of compelling and relevant deconfusion experiments which doesn\u2019t devolve into over-claims. The authors do well in the discussion highlighting that identifiability may not be needed for all applications of mech interp, and that the non-identifiability observed here is on toy-models, so might not extend to larger models trained on multiple tasks.\n\nThe \u201cwhat-then-where\u201d strategy implemented here appears to solely utilize an approach based on Distributed Alignment Search (DAS), which makes the results about non-identifiability quite relevant to recent discussion:\n\nMakelov, A., Lange, G., & Nanda, N. (2023). Is this the subspace you are looking for? An interpretability illusion for subspace activation patching. arXiv. https://arxiv.org/abs/2311.17030\n\nWu, Z., Geiger, A., Huang, J., Arora, A., Icard, T., Potts, C., & Goodman, N. D. (2024). A reply to Makelov et al. (2023)'s \"Interpretability Illusion\" arguments. arXiv. https://arxiv.org/abs/2401.12631"
            },
            "weaknesses": {
                "value": "Primary weakness: The most novel contribution of this paper are its experimental results, which don\u2019t have enough description: Appendix B only contains aggregated results about the number of circuits and average interpretations per circuit found, but e.g. lacks examples of said circuits for qualitative validation. This significantly undercuts my ability to validate the correctness of the experiments.\n\nIn addition, the authors acknowledge appropriately that identifiability may not be necessary if the goal of MI is merely to steer a model. However, much MI work is driven by a desire to simply further scientific understanding of language models. What types of scientific inquiries require computational identifiability, and which do not? The paper could be strengthened by further discussing how much identifiability matters if the goal is scientific understanding, rather than just the model steering mentioned in lines 520-527."
            },
            "questions": {
                "value": "1. Could you elaborate on the meaning of \u201cincompatible\u201d in line 355? The paper would benefit from a clear example of two incompatible explanations, ideally in the main body.\n\n2. Please include in the appendix random examples of some of the circuits found so that they can be qualitatively assessed by readers.\n\n3.Could you comment on how these results relate to the discussion of Makelov et al. (2023) and Wu et al. (2024), cited above in section \u201cStrengths\u201d?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors investigate the potential issue of identifiability in mechanistic interpretability through experiemnts on small MLPs where (isolated) circuits are ennumerated and assessed fairly exahustively. They find identifiability is an issue at all levels: in the number of subcircuits functionally aligned with the full network, in the number of algorithms consistent with the behavior, and in the mappings between algorithms and circuits. This problem gets worse as architecture size increases, and training on a greater number of tasks only mitigates this issue to some extent."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Well written\n* Well referenced\n* Adresses an issue of interest to the interpretability community\n* Provides exhaustive experiments with well-understood ground truth.\n* Investigates the effects of various variables (architecture size, number of tasks, noise) on the identifiability problem.\n\nThis kind of study is very much needed."
            },
            "weaknesses": {
                "value": "I first summarize the most important points and then elaborate.\n\n* It is unclear whether the conclusions about identifiability problems can be stated generally or only in particular for circuits as isolated objects (circuit discovery through zero-ablation), which might mischaracterize the functioning of the network as a whole.\n* The possible improvements advertised in the abstract/introduction are rather only sketched in section 5.1.\n\n\nIs the large number of circuits/possible explanations due to looking for circuits in isolation (e.g., via zero ablation) rather than working in context with the rest of the network (e.g., via activation patching)?\n\nLine 320 describes the circuit isolation procedure. This is equivalent to zero-ablation and the criterion is equivalent to the definition of suficcient circuit.\nHow would identifiability look like if we chose to define circuits as they function in the context of the full network? See for example the definition of circuit via activation patching in Adolfi et al., 2024.\n\nIsn\u2019t it possible that many of the isolated circuits discovered through zero-ablation are mischaracterizations of the in-context functioning of the circuits as they are embedded in the full network?\n\n\nLine 080: \u201ca model\u2019s behavior should have a single, well-defined explanation\u201d. There is no citation here and it is unclear where this intuition is coming from, what is it\u2019s theoretical support, etc. To offer a counter-intuition: consider a circuit that is sufficient on its own to mimic the behavior of the full network over some input domain; such a circuit need not be unique. Trivially, the circuit plus additional neurons to form the full network is another such circuit. But there is no contradiction in intuiting that multiple such circuits of different sizes, with partial or no overlap exist in the network and, in principle, offer alternative (perhaps incompatible?) \u2018explanations\u2019 (see Adolfi et al. 2024 for theoretical analyses).\n\nOn Line 091: the authors mention \u201cthe identifiability properties of current MI criteria\u201d. The criteria of interest that define circuits leave open the possibility that these circuits are not unique. So the definition of these circuits does not preclude non-identifiability unless the uniqueness property is trivially appended to the definition. This leads one to suppose that uniqueness under the typical definition of circuits is a property left to be determined empirically. It could, in principle, be motivated theoretically, but I see nothing in that direction here. Is it possible to provide some theoretical motivation for uniqueness that is not trivially stipulated but justified from first principles?\n\nIf a network implements the same functionality for different inputs through different circuits and algorithms, does this really make mechanistic interpretation hopeless? (i.e., in this case, is only a functional explanation capable of unifying all the existing \u2018explanations\u2019?). It would be useful to have any assumptions about satisfactory explanations made explicit in the manuscript.\n\nLine 489: \u201cthe challenge lies in defining criteria that distinguish valid explanations from misleading ones.\u201d\nIt seems to me that, conceptually, identifiability does not pose a problem for distinguishing misleading from valid explanations. The problem arises only if an explanation is presented as unique or valid for a full input domain when this is not so. This issue might warrant some clarification.\n\nLine 490: \u201cAccording to MI, the explanatory algorithm should be unique, meaning multiple competing explanations should not exist.\u201d But this statement is made without citation. This assumption seems ill-founded to begin with, for the reasons mentioned above. Where does the criterion come from?"
            },
            "questions": {
                "value": "Minor comments, questions, and suggestions:\n\nLine 403 states that \u201clarger architecture\u2026could also lead to greater overparameterization\u201d. This could benefit from elaboration; in particular, how larger architecture could lead to a reduction in the number of valid abstractions.\n\nOn Line 065, \u201cGiven the near impossibility of exhaustively searching all possible algorithms across all subsets of a neural network\u201d, I might suggest to reframe this not as impossibility but as intractability, infeasibility or implausibility. Certain interpretability queries might have large search spaces that could nevertheless be searched efficiently. The relevant property is the complexity of the interpretability query, not merely the size of the search space. For computational complexity analyses of circuit discovery, see Adolfi et al 2024.\n\nOn Line 067, the authors state \u201cresearchers have developed approximation methods with different assumptions and trade-offs\u201d. It seems to me that the circuit discovery methods that are typically developed are heuristics for circuit finding, not approximation algorithms with any proven guarantees. In any, case it would be useful if the authors can distinguish between these two categories in their descriptions.\n\nCitation to Van Rooij on Line 046 does not seem to fit with the corresponding sentence, as that paper does not deal at all with interpretability, as opposed to Lindsay, 2024, which is indeed an approapriate citation. For examples of studying the fundamental properties of (inner) interpretability queries see Adolfi et al., 2024.\n\nSection 2.1 mentions interpretability work on transformer models but only in language. An example from vision transformers can be found in Vilas et al. 2023.\n\nPlease clarify the notation in Definition 4.\n\nLine 229 makes an implicit statement about computational complexity but provides no citation. See Adolfi et al. 2024 for relevant complexity analyses. This is also relevant to the statement on Line 257. Here it would also be useful to clarify how uniform random sampling \u201capproximates\u201d the desired measure, as this seems non-obvious. Perhaps the authors mean random sampling is a heuristic with unknown properties?\n\nLine 494 states that current MI methods can only approximate their targets because exhaustive enumeration is impossible for large models. This is technically incorrect, as even for some problems with exponential serach spaces, efficient search algorithms that find optimal solutions are possible. The relevant notion is the computational complexity of the interpretabililty queries, not simply the size of their search space (see Adolfi et al., 2024).\n\nSection 2.1 describes a parallel between AI interpretability and neuroscience. A framework that draws from lessons grounded in this parallel is described in Vilas et al. 2024. This framework provides a nice embedding for the what-where distinction, corresponding to the algorithmic and implementational levels, respectively.\n\nThe problem of identifiability interacts in interesting ways with the computational complexity of circuit finding. Adolfi et al. 2024 analyses circuit queries that are relevant to the authors\u2019 points on identifiability. See, for instance, counting problems which ask for the number of circuits in a train neural network that have a certain property (e.g., they are sufficient for a behavior). Furthermore, if the number of sufficient circuits is typically large, heuristics for otherwise intractable problems (e.g., sufficient circuit) could seemingly find their targets in a feasible amount of time. In this scenario, non-identifiability is an important catch."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "- The objective of the paper is to answer the question, do current criteria in mechanistic interpretability (MI) guarantee the identifiability of the explanation?\n- Authors sort MI methods into two broad strategies: \n    - _where-then-what_ focuses on finding a subset of the network \u2013 a circuit \u2013 that captures most of the information flow from in- to outputs. Once this circuit is identified, the next step is to interpret its components (features) to derive the explanatory algorithm.\n    - _what-then-where_ starts by identifying candidate algorithms and then searches subspaces in the neural network where the algorithm may be implemented, using causal alignment between the explanatory algorithm\u2019s states and the network\u2019s internal states. \n- They stress testing both methods with toy models: small MLPs trained on logic gates. They performed three main types of searches to test different interpretability criteria: \n    - Circuits search: Looking for subnetworks that perfectly replicate the model's behavior\n    - Interpretations search: Trying to map neurons to logical gates in a way that's consistent with their activations\n    - Mappings search: Testing different ways to map logical gates to groups of neurons\n- Key findings in toy models: they found multiple valid interpretations for the same network.\n    - 85 different circuits that achieved perfect accuracy\n    - An average of ~536 possible logic gate interpretations per circuit\n    - 159 perfect minimal mappings between algorithms and neurons\n    - In total, over 45,000 different possible computational abstractions\n- Experiment larger NN trained on a subset of MNIST revealed similar dynamics\n    - The circuit search found over 3000 valid circuits"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The authors introduce their own taxonomy and formalization (e.g. where-then-what/what-then-where, Circuits, Mappings, etc) for important concepts discussed in the paper. While I haven\u2019t fully wrapped my head around the usefulness of the taxonomy, I appreciate the effort to deconfuse different interpretability methods. I think this is the strongest suit of the paper and I wish they had focused the paper on the taxonomy and less on the experiments. \n- I enjoyed the writing style, and it was easy for me to follow, particularly Sections 2 and 3. I also found Figure 2 to be helpful. For the _what-then-where_/_where-then-what_ split. \n- The paper touches on an important topic within interpretability \u2013 a lack of quality in the discourse around helpful metrics for interpretations."
            },
            "weaknesses": {
                "value": "- While I think the taxonomy could bear some relevant fruit, for experimentally testing the metrics using NN this small, it seems challenging to generate insights that are relevant to interpretability as a total. From my understanding, we are not close to having high-confidence circuit interpretations (\u201cperfect circuits\u201d) in the first place, so working under this assumption might be several steps ahead.\n- Continuing on the larger NN experiments: I struggled to understand the points in line 479 ff. What is in this case the \"valid circuit\"? I hoped this section could have bridged a gap toward showing how this metric could be used in the future but failed to show its limitations in this instance. (But maybe I simply oversaw that.)\n- Lastly, I think there is also too little emphasis on existing literature that clearly touches on the underlying problem: disentanglement. I would consider modeling the paper around the taxonomy and then focusing on existing research and problems in relation to known circuits, such as IOI."
            },
            "questions": {
                "value": "(From the weakness section)\n- What is in the case of the MNIST NN the \"valid circuit\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}