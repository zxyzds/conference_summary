{
    "id": "icNel2Thrt",
    "title": "Thermodynamic Natural Gradient Descent",
    "abstract": "Second-order training methods have better convergence properties than gradient descent but are rarely used in practice for large-scale training due to their computational overhead. This can be viewed as a hardware limitation (imposed by digital computers). Here we show that natural gradient descent (NGD), a second-order method, can have a similar computational complexity per iteration to a first-order method, when employing appropriate hardware. We present a new hybrid digital-analog algorithm for training neural networks that is equivalent to NGD in a certain parameter regime but avoids prohibitively costly linear system solves. Our algorithm exploits the thermodynamic properties of an analog system at equilibrium, and hence requires an analog thermodynamic computer. The training occurs in a hybrid digital-analog loop, where the gradient and Fisher information matrix (or any other positive semi-definite curvature matrix) are calculated at given time intervals while the analog dynamics take place. We numerically demonstrate the superiority of this approach over state-of-the-art digital first- and second-order training methods on classification tasks and language model fine-tuning tasks.",
    "keywords": [
        "optimization",
        "gradient descent",
        "natural gradient descent",
        "thermodynamic computing",
        "analog computing"
    ],
    "primary_area": "optimization",
    "TLDR": "We propose a hybrid digital-analog algorithm that makes natural gradient descent more scalable.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=icNel2Thrt",
    "pdf_link": "https://openreview.net/pdf?id=icNel2Thrt",
    "comments": [
        {
            "summary": {
                "value": "The paper presents thermodynamics natural gradient descent, an approximation to natural gradient descent that, instead of inverting the matrix needed for NG updates, approximates the inverse via a random walk. This is claimed to be implementable in hardware, thus offloading the matrix inverse from GPUs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The presented approach seems to be compatible with alternative (to GPUs) hardware, although I\u2019m not familiar with thermodynamic computing to judge it properly.\n2. The proposed natural gradient approximation is theoretically justified."
            },
            "weaknesses": {
                "value": "## Evaluation issues\nWhile the paper\u2019s approach is interesting, the evaluations could be significantly improved. \n\nFirst, MNIST performance. Fig.3b shows that \n1. Adam and TNGD achieve the same test error; (Line 367 claims \u201cTNGD outperforms Adam\u201d, and then line 371 claims TNGD achieves better test accuracy, while it is clearly not true in Fig3b.)\n2. TNGD overfits significantly more than Adamw (0 train error vs. somewhat close to test error for Adam); (Line 368 claims TNGD \u201cgeneralizes better\u201d.)\n3. Most importantly, both methods achieve about 94% test accuracy on MNIST. MNIST is an extremely easy dataset \u2013 pretty much any neural network should be able to get 97-99% test accuracy (https://paperswithcode.com/sota/image-classification-on-mnist?metric=Accuracy). As Sec. 5.1 uses a conv layer with two fc layers, it should be enough for good performance as well; this suggests the evaluation was not done correctly. \n\nSecond, NLP experiments in Sec. 5.2 only report the training loss (also, the TNGD-Adam combination presented there was not tested for MNIST).\n\nFinally, TNGD is an approximation NGD, but no NGD baselines are shown.\n\nWith only two datasets and not convincing performance, it is hard to judge if the proposed approach (or using NGD at all) is worth the computational overhead.\n\n\n## Justification for using NGD\n\nI think the paper does not successfully justify using NGD for training neural networks. Its performance benefits are unclear from the presented results (although probably explored in other papers), but the computational overheads are very significant: \n- Tab. 1 shows that TNGD is much more expensive that SGD;\n- the suggested (future) approach would involve using dedicated hardware."
            },
            "questions": {
                "value": "1. Natural gradient descent can be linked to mirror descent (see e.g. https://franknielsen.github.io/blog/NaturalGradientConnections/NaturalGradientConnections.pdf). They\u2019re not strictly identical, but mirror descent is a first order method with the same time complexity as gradient descent. If the goal is to improve performance by using better conditioned gradients, wouldn\u2019t mirror descent be a (computationally) better target algorithm than NGD?\n2. Eq. 12: the first $\\mathbb{I}$ should be $v$."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method of reducing the per-iteration computational cost of NGD (a second order optimization method that has been shown to converge in less iterations than GD), using a hybrid analog-digital algorithm. The algorithm has a runtime that scales linearly with the number of parameters and can be made close to the runtime of first-order optimizers."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper proposes a new algorithm that combines analog and digital computation to reduce the per-iteration cost of NGD, and performed thorough comparison of the scaling of the runtime between their method and various existing methods. The motivation and the method are clearly conveyed. The authors also performed several numerical experiments comparing the existing methods and TNGD."
            },
            "weaknesses": {
                "value": "The paper could be written more clearly and concisely. I find the introduction of certain aspects too detailed and not necessarily helpful for understanding the authors\u2019 contribution (such as the Fisher information etc.) and certain aspects too unclear for people not coming from an optimization background (such as the introduction of fast matrix vector product, the convergence speed and performance is only vaguely discussed). Also, this paper uses many different notations, and the introduction of these notations are often hidden in the text, making it very hard to read. I suggest that the author clearly and explicitly write out definitions and their claims. \nWhile the paper proposes a new algorithm, it seems to be a direct combination of existing works on natural gradient descent and using an SPU to solve linear systems, although because I am not familiar with analog computing, I could be wrong about my judgement on the novelty and contribution of this work. \nThe paper would also benefit from more numerical comparisons, both on more datasets and also tuning different hyperparameters (such as the different hyperparameters in adam) to demonstrate the robustness of their findings."
            },
            "questions": {
                "value": "1.\tIt seems unnecessary to take the detour by introducing the statistics point of view and Fisher Information and then the GGN, as GGN is a relatively common approach and can be derived from the loss function without necessarily introducing the KL divergence. The rest of the paper also does not ever use the Fisher Information matrix again and only uses the GGN. I suggest that the authors introduces GGN right after the loss function and cite relevant references relating the GGN to the FI matrix.\n2.\tThis is a minor point. The \u2018GGN vector product Gv \u2018 has not been previously introduced before line 191. I suppose it refers to product of GGN with an arbitrary vector v? Please state explicitly.\n3.\tWhat do you mean by \u201cconvergence in \\sqrt{k} steps \u2026 is not required \u2026\u201d, do you mean that the algorithm does not necessarily converge in \\sqrt{k} steps but achieve good performance despite not converging? This sentence is confusing to me. \n4.\tThe authors switch between using G and F, to my understanding, the authors stick to the GGN approximation of the fisher information matrix, so I suggest that the authors stick to one notation. \n5.\tI am not familiar with analog computing. From my understanding it seems that for solving the linear system, both the conjugate gradient method and the authors\u2019 thermodynamic natural gradient descent method can be understood as minimizing the same quadratic objective, only CG also takes orthogonal gradient steps at each iteration while the thermodynamic NGD is basically gradient descent plus thermal noise. Naively I would expect CG to converge in less steps than NGD, while the authors seem to suggest otherwise. I suppose the longer convergence time of NGD is manifested in this extra factor \u201ct\u201d? The authors then claim that t does not need to be very long for the performance to be good, however, this seems an empirical observation, it would be nice if the authors could provide some bounds on t to show that thermodynamic NGD is indeed superior.\n6.\tIn Fig.3, test loss almost always seems to be smaller than training loss except maybe at the end, is this because of some regularization method or do you have any explanations for this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a hybrid digital-analog technique for computing natural gradients, leveraging the analog properties of a thermodynamic, stochastic system to quickly invert the Fisher matrix. This inverted matrix can then be used alongside the gradient of the loss function to descend a loss function more efficiently than by using the gradient alone."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "I think this paper is very timely, and tackles an important topic. Using hardware (in this case, fundamental physics) to effectively compute will be essential to deep learning going forward. The paper is for the most part nicely written, and straightforward to follow. The experiments are convincing."
            },
            "weaknesses": {
                "value": "There are some terminological concerns/questions (below), which would improve the paper if addressed."
            },
            "questions": {
                "value": "- General terminology question: In the original Amari (1998) paper (page 4), the term 'natural gradient' refers to the expression $G^{-1} \\nabla \\mathcal{L}(w)$, where $G$ is an arbitrary symmetric positive definite matrix. In your paper (L127), you use 'natural gradient' exclusively for the case where $G$ is the Fisher information matrix. Which interpretation is correct?\n\n- L077: Condition number of what? Additionally, NGD-CG is not defined until later in the paper, so avoid using this acronym here.\n- L159: \"(By curvature matrix, we mean any matrix capturing information about the loss landscape).\" Could you clarify? Information about which aspect of the loss landscape?\n- L196: I looked at the cited references for the claim about $\\kappa$, but found no mention of the condition number of $F$. Also, since $F$ is not fixed, what do you mean by the condition number of F? An upper bound on the condition number across all possible $F$?\n- The authors may be interested in [recent work](https://arxiv.org/abs/2409.16422), suggesting that any effective learning rule can be formulated as natural gradient descent. This might help provide additional motivation for efficient natural gradient computation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}