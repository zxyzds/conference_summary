{
    "id": "NiCSyYOfex",
    "title": "Node-Level Topological Representation Learning on Point Clouds",
    "abstract": "Topological Data Analysis (TDA) allows us to extract powerful topological, and higher-order information on the global shape of a data set or point cloud. Tools like Persistent Homology or the Euler Transform give a single complex description of the global structure of the point cloud. However, common machine learning applications like classification require point-level information and features to be available. In this paper, we bridge this gap and propose a novel method to extract node-level topological features from complex point clouds using discrete variants of concepts from algebraic topology and differential geometry. We verify the effectiveness of these topological point features (TOPF) on both synthetic and real-world data and study their robustness under noise.",
    "keywords": [
        "Topological Data Analysis",
        "Hodge Laplacian",
        "Hodge Theory",
        "Geometry Processing",
        "Differential Geometry",
        "Algebraic Topology",
        "Point Clouds",
        "Representation Learning on Point Clouds"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "We use Persistent Homology and ideas from Differential Geometry to extract point-level topological features on point clouds.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=NiCSyYOfex",
    "pdf_link": "https://openreview.net/pdf?id=NiCSyYOfex",
    "comments": [
        {
            "summary": {
                "value": "Many machine learning applications like classification needs point-level information and features. This paper proposes a method to extract node-level topological features from 2D and 3D topologically structured point clouds."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors proposed topological point cloud clustering benchmarks that could benefit future works in TDA. The proposed method outperforms other methods on topological structured point cloud data and enjoys the `Robustness to noise' characteristic of persistence homology."
            },
            "weaknesses": {
                "value": "1. **Datasets:**\n\n    a) The Topological Clustering Benchmark Suite => TPCC is introduced by the authors in this paper to evaluate the effectiveness of the proposed clustering method. The construction of these datasets in this benchmark has not been sufficiently discussed in the paper/appendix. For instance, it is unclear how the ground truth labels have been obtained since all the datasets are synthetic in nature. \n\n    b) 3D datasets => What are the ground truth labels for these datasets, and how have these labels been obtained? Why were auxiliary points added in Cys123? \n\n2. **Experiments:**  \n      a) There are a total of 12 datasets presented in the paper: seven 2D datasets in the TPCC benchmark and five 3D datasets, as shown in Figure 5. Then why are results on only four 2D and three 3D datasets presented in Table 1? \n\n      b) Please provide a quantitative comparison with the baselines on the 3D datasets. \n \n      c) Why are some of the run-times in Table 1 0.0 seconds? What is the breakdown of TOPF run-times in terms of Steps 1-4 of Algorithm 1? \n\n3. **Computational Complexity:** What is the complexity of Algorithm 1, especially Steps 3 and 4? \n4. **Presentation:** The algorithm presentation needs significant improvement. Steps 3 and 4 of the TOPF algorithm are hard to read. Please explain these steps using a concrete, small-scale toy point cloud dataset. The terms \u201csimplex-valued harmonic representatives\u201d (line 337) and \u201csimplex-valued vector\u201d (line 341) are used without any explanation. What is the role of interpolation parameter $\\gamma$? \n5. **Novelty:** Which component of the algorithm is novel? Is it step 4, since Steps 1-3 are standard tools of TDA? \n6. **Limited Applicability:** TOPF only obtains meaningful results on point cloud with topological structure. However, many datasets do not have such a structure. Furthermore, TOPF is computationally expensive on datasets with a large number of points (e.g., those obtained from CAD designs or 3D Scanners).  While the authors mentioned using landmarks to improve performance, no experiments in the paper did so. It seems the method is only practical in a very limited setting, which is points with only topological shapes and a small number of points."
            },
            "questions": {
                "value": "1. What is the maximum homology dimension $d$ in the experiments?\n2. What is the relation between the output of step 3 ($\\hat {e}_k^i$) of Algorithm 1 with the Circular coordinates of De Silva & Vejdemo-Johansson (2009)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Paper proposes a novel idea for extraction of continuous topological features via projection of simplicial generators into harmonic subspaces. It works as follows. First, given a point cloud sample from a manifold, it computes persistent homology classes based on alpha or VR filtration of the point cloud. Second, it selects the most relevant classes based on the smallest quotient heuristic. Third, these relevant global classes are used to compute local topological features that indicate local \u2018strength\u2019 of chosen topological class. This is done via aggregation of simplices for chosen birth-depth timestep width and their projection into harmonic subspace of simplices. Finally, simplicial features are aggregated on point level to produce pointwise features. These features can be utilized to provide topology-aware visualizations and/or topology-aware clustering. \n\nProposed method evaluates the Topological Clustering Benchmark Suite (TCBS) that consists of 7 point clouds that represent complex topological arrangements."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "\u2014 Novel idea for extraction of continuous topological features via projection of simplicial generators into harmonic subspaces and subsequent extraction of pointwise features; \n\u2014 Good visual presentation of the method; \n\u2014 Somewhat good introduction into fundamentals of algebraic topology;\n\u2014 Ablation with respect to some of the hyperparameters of the method;\n\u2014 Quantitative results on the proposed TCBS benchmark seem to be strong."
            },
            "weaknesses": {
                "value": "\u2014 Main body of the paper focuses too much on introduction into algebraic topology and too little on practical considerations of the method; \n\u2014 Algorithm 1 is too schematic and it is hard to reproduce the method based on it. It can heavily benefit with more detailed substeps; \n\u2014 Applicability of proposed method to high-dimensional spaces (D>3) seems to be limited;\n\u2014 Related work discussion (including supplement) is limited; I recommend including some of the work discussed in [Rev 1] \n\u2014 Theorem 4.1 has very limited practical applicability for 3D data analysis;\n\u2014 No ablation with respect to sampling density; \n\u2014 No ablation with respect to non-uniform sampling from the manifold; \n\u2014 Learning-based baseline (PointNet) is very weak;\n\u2014 Important details in some figures are missing (see suggestions/questions). \n\n\n[Rev1] Wasserman, L., 2018. Topological data analysis. Annual Review of Statistics and Its Application, 5(1), pp.501-532."
            },
            "questions": {
                "value": "My main issue with the paper is that it focuses too much on the theory of algebraic topology that works with continuous spaces; and focuses too little on practical considerations that stem from the discretization aspect of topological data analysis. Overall, the proposed model is built upon discrete approximation of the manifold through N-simplices based on localized Delaunay triangulation based on epsilon radius neighbors. It means that two considerations are important for the method. First, number of points: we want enough points to approximate manifold topology but not too much because dense sampling can be expensive or unavailable. Second, sampling density: we want points to be sampled somewhat uniformly from the manifold (if all samples are the same point, it does not help us much). Good TDA method should ideally require as few points as possible to recover structure of the manifold while being robust to non-uniformity and noise in sampling. I think the current iteration of the paper largely ignores these considerations. Namely:\n\n\u2014 How sampling density affects performance of the method? For example, what happens with predicted labels in Figure 8 if we reduce sample sizes of point clouds by 2x? 3x? 5x? My concern is that the model might not work well on sparse point clouds. \n\n\u2014 Similarly, what happens with the method if we have non-uniform sampling from the manifold? For example, what happens when the target manifold has a high-curvature area that is not sampled densely enough? \n\n\u2014 My understanding is that the epsilon radius of local Delaunay triangulation (Definition B.2.) is the hyperparameter of the method. It is not clear how this parameter should be chosen and it looks like it can strongly affect the results: if it is too large, local Delaunay triangulation will start closing holes; if it is too small, the manifold will be disconnected. Do authors have some practical suggestions how this radius should be chosen, especially if we don\u2019t have good priors about the data?\n\n\u2014 Theorem 4.1 has very limited practical applicability to actual 3D data analysis. It assumes two sets of points: one set is sampled from the unit sphere; another set is sampled with distance at least 2 to this unit sphere. For the majority of 3D point cloud applications, point clouds are normalized to unit cube. My understanding is that in this case assumptions of Theorem 4.1 do not hold. Is my understanding correct? Maybe the setting that I have described can be adapted for Theorem 4.1 somehow?\n\n\u2014 On a related note, what is the normalization of the data for the experiments? Are point clouds normalized to unit cube? Is this normalization the same across all evaluated methods? \n\n\u2014 PointNet baseline is very weak. It was introduced in 2016 and was superseded by a large number of architectures: PointNet++, several versions of PointTransformers and other attention-based architectures. PointNet performance on the majority of benchmarks is significantly worse than for all of the mentioned methods. More up-to-date method can give significantly better performance on proposed benchmark. \n\n\u2014 Applicability of the method to high-dimensional spaces seems to be very limited. How well does the proposed method scale with the number of dimensions? My understanding is that local Delaunay triangulation will be significantly slower for higher dimensions. It is also not clear, how to choose the triangulation radius. \n\n\u2014 On a related note, the experiment in Figure 11 has very limited applicability. To analyze image features with TOPF one needs to train additional variational autoencoder to map image features in 3D space. This scenario does not seem to be practically appealing What happens if relevant information cannot be contained in 3 dimensions? What if one wants to run TDA on original features? \n\n\u2014 What dictated the sampling density of the TCBS dataset? Point clouds in Figure 7 have a number of samples that can differ by order of magnitude: 267 points for SphereInCircle and 4600 points for 2Spheres2Circles. Does method assume/TCBS data assume some upper bound on minimum distance to nearest neighbor for each point and that is what dictates sampling density? Or is it something else?\n\n\u2014 Figure 5 is missing information about the average distance from each point to its nearest neighbor. Without this information, it is hard to assess how large is the relative value of the noise compared to manifold sampling density. \n\n\u2014 Figure 10 is missing time units, so it is impossible to infer speed of the method from it. Also, how indicative are those running times (they are from two point clouds from the benchmark)?\n\n\u2014 Figure 13 seems to be missing 9 of 12 subfigures."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper develops a method to assign _topological information_ to each point $x$ of a given point cloud $X \\subset R^d$ a vector. \n\nRoughly speaking, assuming that $X$ exhibits a set $\\mathcal{F}$ of significant topological features (connected component, loops, or cavities, etc.), the idea is to quantify for each $x \\in X$ to which extend it contributes to each topological feature $f \\in \\mathcal{F}$. Eventually, each $x \\in X$ is thus turned into a vector in dimension $\\mathcal{F}$ that describes the relation between the local point $x$ and the global (topological / geometric) structure of $X$.\n\nThe construction is performed by relying on the Hodge decomposition theorem (in discrete form) and eventually boils down to a (actually two) least square problem(s) which can be solved efficiently. \n\nThe approach is then showcased in a broad range of experiments. In particular, a \"topological clustering benchmark\" is introduced."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Good introduction, clear and nicely written\n- I really appreciate the step-by-step description of Section 3, making the flow of the exposition pretty nice despite the technicality of the construction.\n- Nice illustrations overall.\n- Elegant and fairly-well motivated construction. \n- Nice set of experiments. The introduction of a \"topological clustering benchmark\" is a valuable contribution of the work, we need more of these in the community."
            },
            "weaknesses": {
                "value": "- The approach depends on several seemingly important hyper-parameters. This may be a necessary price to pay, but belongs to its weaknesses nonetheless. \n- Though acknowledged by the authors, the setting of Theorem 1 is very idealized. In particular, the $n$-sphere has somewhat \"maximal reach\" and it is less clear to me that the theorem remain (with probability $< 1$ but still reasonable) valid when the underlying manifold is a \"pinched\" sphere with low reach. The theorem remains nice nonetheless and its validity in applications is not crucial to the work.  \n\n## Minor (not real weaknesses)\n\n- Typo : \"hull hull\" in Theorem 1\n- In the first experiment (Topological Point Cloud Clustering Benchmark), I believe that including topological regularization term in point embedding methods (node2vec and pointnet) has been attempted in some works (e.g. in Topological Autoencoder of Moor et al., some experiments in Optimizing Persistent Homology based function of Carriere et al., Topological Node2vec of Meehan et al., etc.). I believe that this would not invalidate the claims made in that paragraph, namely that these methods requires specific training to work and may be slower than the proposed approach, but it may be worth to briefly mention these."
            },
            "questions": {
                "value": "1. Points $v$ in $X$ are vectorized in dimension $|\\mathcal{F}|$, where $\\mathcal{F}$ designates the set of \"most significant topological features\", typically detected with the proposed heuristic. However, I believe that this embedding dimension may be somewhat unstable to even small perturbation of $X$ (e.g. $X$ is made of a circle of radius 2 and a circle of radius 1; then the persistence of the big circle is twice the one of the smaller one, which is itself twice the one of small circles arising from the noise). Similarly, if one (theoretically) refrain from using an heuristic and pick all topological features for $\\mathcal{F}$, the dimension is (clearly, this time) unstable as well. Is there a way to somewhat \"smooth\" the choice of $\\mathcal{F}$ to avoid relying on a hard threshold but rather considering (possibly infinitely) many dimensions but whose relative importance would be weighted by $\\ell_i$? I ask this question out of curiosity, I do not expect an extensive answer in the rebuttal."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies point-level features by using persistent homology."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is written clearly enough."
            },
            "weaknesses": {
                "value": "Lines 177-178: \"the filtration value of a k-simplex in the \u03b1-filtration is the radius of its circum-k-sphere, which differs from its filtration value in the VR filtration\".\n\nThe concept of a \"filtration value\" appears here for the first time and is never defined in the paper, which leads to the major misunderstanding of persistence.\n\nLines 368-372: \"Theorem 4.1 (Topological Point Features of Spheres). Let X consist of at least (n + 2) points (denoted by S) sampled uniformly at random from a unit n-sphere in R^n+1 and an arbitrary number of points with distance of at least 2 to S. When we now consider the \u03b1-filtration on this point cloud, with probability 1 we have that (i) there exists an n-th persistent homology class generated by the 2-simplices on the convex hull hull of S\".\n\nEven a non-expert can notice that n-th dimensional (?) homology class cannot be generated by 2-dimensional (?) simplices for n>2. Also, Theorem 4.1(i) fails for n=1 and any 3 points on a unit circle that form a non-acute triangle. \n\nThere are infinitely many point clouds whose 1-dimensional persistence is empty. Look for a generic family of such clouds in J Applied and Comp Topology 2024.\n\nEven Figure 3 in the paper demonstrates that only 3 points in the 1D persistence represent reasonable cycles, hundreds of others represent only noise. In fact, there is no sense to apply persistence to proteins whose atoms are ordered. In this ordered case, the complete isometry invariant of a point cloud is the classical distance matrix requiring only quadratic time, which was known at least since 1935, see I.Schoenberg, Annals of Mathematics, 724-732, 1935.\n\nThe persistence homology is more suitable for clouds of unordered points but is much weaker than the collection of all pairwise distances, which determine any generic point cloud in general position uniquely under isometry, which has been known in computational geometry for more than 20 years, see Boutin and Kemper, 2004.\n\nThe stronger invariant (a local distribution of distances) has been widely studied in shape analysis, e.g. see Memoli, \"Gromov\u2013Wasserstein distances and the metric approach to object matching\" (FocM 2011), extended to a complete and polynomial-time invariant under rigid motion in any fixed Euclidean R^n (CVPR 2023). \n\nThe initial obstacle for the proposed research is the lack of a rigorous problem statement. The attempts to state problems in the final section seem a bit late:\n\nLines 514-520: \"selection of the relevant features is a very hard problem\" and \"efficient computation of simplicial weights leading to the provably most faithful topological point features is an exciting open problem\"\n\nThe words \"relevant\" and \"most faithful\" should be rigorously defined, else anyone can claim that their features are \"relevant\" and \"most faithful\"."
            },
            "questions": {
                "value": "Can the authors to consider 3 points (1,0), (0,1), (-1,0), draw three disks centered at the points with a growing radius alpha and check that the resulting union of disks always has trivial 1-dimensional homology without cycles?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Other reasons (please specify below)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "This paper is a minor revision of the submission to NeurIPS 2024, where one of the reviews provided a counter-example to Theorem 4.1. Submitting almost the same paper without correcting this claim raises serious ethical concerns."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on constructing node-level topological representation of point clouds. To achieve that, it proposes TOPF to do topological data analysis. By doing that, node-level topological features are extracted from complex point clouds through using discrete variants of concepts from algebraic topology and differential geometry. Experiments are conducted on both synthetic and real data for the verification."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The idea of topological representation on node-level for point clouds is somehow interesting;\n\n2. The English writing is acceptable"
            },
            "weaknesses": {
                "value": "1. Fig. 1 doesn't help understand the overall and core design of the TOPF. The authors are highly suggested re-drawing this part. From my personal perspective of view, this figure should consist the motivation and the overview of the proposed methods.\n\n2. For the experimental part, point cloud matching and registration are typical tasks that require good representation point clouds. I would suggest the authors add comparisons to those related methods, like the unsupervised ones (PPF-FoldNet, ECCV 2018 & WSDesc, TVCG 2022)."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method to extract node-wise topological information from point clouds. To extract the topological structure of a point cloud and to associate it with each point, they compute persistent homology and utilize information from the generators of these homology classes. Additionally, to make the topological information of each node more meaningful, they connect global and local information using the Hodge Laplacian."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The preliminaries and detailed explanations of the method in Sections 2 and 3 are appropriate and clear.\n2. The authors employ an interesting idea to obtain node-wise features using the information in the generators of persistent homology.\n3. They made an effort to stabilize the features by leveraging  insights from algebraic topology.\n4. Theoretical results intuitively explain the advantages of the method."
            },
            "weaknesses": {
                "value": "1. Regarding the hyper-parameters, such as $\\gamma$ on line 310 and $\\delta$ on line 342, there are some concerns about robustness and the way to determine them. Although the authors test the robustness of the hyper-parameters in Appendix D, these results are limited to the topological clustering dataset. It remains uncertain if they are effective on more practical data.\n2. The validity of the heuristics used in the paper should be further investigated. For example, the way to choose the points from persistence diagram in line 294, the strategy to give the topological information to each node in 342, and the process to aggregate the multiple information in line 350. These approaches should be validated with realistic datasets.\n3. Since various techniques are employed in the paper, it would be desirable to conduct an ablation study. Specifically, it should be tested on simple data how the effectiveness of features changes with or without the Hodge Laplacian.\n4. The intent of the experiments using VAE is unclear. It is difficult to understand what is meant by \u201ctopological structure inherent in the sample space,\u201d as described in Figure 11. Additionally, it is unclear there are any implication for any specific applications."
            },
            "questions": {
                "value": "1. Why did you use VR filtration for n>3?  In higher dimension, doesn\u2019t the computation cost of the persistent homology increase since the number of simplices in the VR complex increase?\n2. Is it possible to effectively use TOPF for machine learning tasks other than clustering, such as point cloud classification? Additionally, is quantitative evaluation feasible for such tasks?\n3. How did you choose the dimension of the persistent homology in the experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}