{
    "id": "0FxnSZJPmh",
    "title": "Physics-Informed Deep Inverse Operator Networks for Solving PDE Inverse Problems",
    "abstract": "Inverse problems involving partial differential equations (PDEs) can be seen as discovering a mapping from measurement data to unknown quantities, often framed within an operator learning approach. However, existing methods typically rely on large amounts of labeled training data, which is impractical for most real-world applications. Moreover, these supervised models may fail to capture the underlying physical principles accurately. To address these limitations, we propose a novel architecture called Physics-Informed Deep Inverse Operator Networks (PI-DIONs), which can learn the solution operator of PDE-based inverse problems without any labeled training data. We extend the stability estimates established in the inverse problem literature to the operator learning framework, thereby providing a robust theoretical foundation for our method. These estimates guarantee that the proposed model, trained on a finite sample and grid, generalizes effectively across the entire domain and function space. Extensive experiments are conducted to demonstrate that PI-DIONs can effectively and accurately learn the solution operators of the inverse problems without the need for labeled data.",
    "keywords": [
        "Inverse Problems",
        "Stability",
        "Operator Learning",
        "Physics-Informed Machine Learning"
    ],
    "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)",
    "TLDR": "We propose a novel architecture called Physics-Informed Deep Inverse Operator Networks (PI-DIONs), which can learn the solution operator of PDE-based inverse problems without any labeled training data.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=0FxnSZJPmh",
    "pdf_link": "https://openreview.net/pdf?id=0FxnSZJPmh",
    "comments": [
        {
            "title": {
                "value": "Response to reviewer knEo"
            },
            "comment": {
                "value": "Thank you for clearly summarizing our contributions and highlighting the strengths of our work.\n\nResponse to Weaknesses:\n\n1. To the best of our knowledge, PI-DIONs and PI-DIONs-v0 (in Appendix A) are the first physics-informed operator networks specifically designed for inverse problems. Due to structural limitations, existing architectures such as DeepONet and FNO cannot incorporate physics loss for inverse problems. A detailed description of the limitations are presented in Chapter 2 (lines 105-108). The relatively simple network utilized in this work was chosen to highlight the novelty of the architecture and loss function while ensuring a fair comparison, which we encourage you to consider.\nIn addition to the architectural novelty, we provide rigorous theoretical justification for our method, as detailed in Section 3. Specifically, we extend stability estimates to the operator learning framework, demonstrating that sufficiently small $\\mathcal{L}$ implies a small prediction error for both the solution and target functions. Furthermore, we present a universal approximation theorem for PI-DIONs, which guarantees that the loss function can be reduced to an arbitrarily small value. These theoretical contributions enhance the significance of our work, and to the best of our knowledge, these are the first theoretical results for inverse operator learning.\n\n2. Thank you for your valuable feedback regarding the comparison with the baseline. Solving forward and inverse problems using physics-informed methods, such as differential equations, is a relatively recent area of research. While NIO is a promising approach, a direct comparison with our model is challenging because it relies on fully supervised learning of the operator-to-function mapping. In contrast, our model can be trained without direct supervision on the target function, and to the best of our knowledge, there is no existing architecture that aligns with this approach. Nevertheless, we can also enhance our methodology by incorporating DeepONet\u2019s advanced model into PI-DIONs\nSince NIO utilizes DeepONet as a critical baseline, we selected the same model for comparison. If you could suggest relevant references addressing inverse problems with physics-informed methods, we would be glad to conduct further comparative experiments.\n\n3. The benchmark problems are widely used ones in physics-informed machine learning literature. In particular, even PINN approaches for solving the inverse source problem for the reaction-diffusion and the Helmholtz equations have been proposed very recently [2,3]. Our work extends recent works to the operator learning framework, where PI-DION rapidly predicts both the solution and target function from the partial measurements of the solution. Moreover, even in the absence of formal stability estimates, PI-DION consistently achieves accurate approximations for both solutions and target functions in the Darcy flow problem, suggesting its potential applicability to various differential equations. Furthermore, we have conducted additional experiments, including sensitivity analysis, detailed in Appendix C. Compared to PINNs, it offers significantly faster computation, while maintaining slightly larger but negligible relative errors.\n\n\n[1] Roberto Molinaro, Yunan Yang, Bj\u00a8orn Engquist, and Siddhartha Mishra. Neural inverse operators for solving pde inverse problems. 2023.\n\n[2] Hui Zhang and Jijun Liu. Solving an inverse source problem by deep neural network method with convergence and error analysis. 2023. Inverse Problems\n\n[3] Mengmeng Zhang, Qianxiao Li, and Jijun Liu. On stability and regularization for data-driven solution of parabolic inverse source problems. 2023. Journal of Computational Physics"
            }
        },
        {
            "title": {
                "value": "Continued"
            },
            "comment": {
                "value": "Response to Questions:\n\n1. As you mentioned, FNO and DeepONet are typically applied to forward problems. In our experiments, both models were trained in a fully supervised setting to learn the inverse mapping $u|_{\\Omega_m} \\rightarrow f$. PI-DIONs, on the other hand, were trained in both supervised and unsupervised settings for comparison. \n\n2. Thank you for highlighting the loss function with labeled training data. We have now clearly defined the loss function for supervised PI-DIONs at the beginning of Section 4 to address this concern. We appreciate your valuable feedback, which has helped improve the clarity and presentation of our work."
            }
        },
        {
            "title": {
                "value": "Response to reviewer 3Fuq"
            },
            "comment": {
                "value": "Thank you for clearly summarizing our contributions and highlighting the strengths of our work. \n\nResponse to Weaknesses:\n\n1. Thank you for the helpful feedback regarding the term on the right-hand side in Line 242. To improve clarity, we have revised the explanation by bringing the term $\\Vert f-f^*\\Vert_{L^2(\\Omega_m)}$ to the forefront of inequality. Additionally, we have enhanced the logical flow by adding more details to better connect the inequalities in Lines 233-237 with the final inequality in Lines 242-244. We appreciate your effort in identifying the potential source of confusion.\n\n2. Thank you for your insightful comment regarding inputs with variable counts and locations. The limitation in handling such inputs arises from PI-DION's reliance on the fundamental DeepONet framework. However, recent advancements, such as Variable Input Deep Operator Network [1] or GraphDeepONet [2] address this issue through aggregation functions with softmax. Incorporating these methodologies into the branch network could effectively resolve the above limitation. We have included this promising direction in the discussion section. We sincerely appreciate your valuable feedback.\n\n3. Thank you for suggesting a comparison with PINNs. As highlighted in Section 2, this study was motivated by the limitation that physics-informed loss functions cannot be directly applied to inverse problems in operator learning methods like DeepONet and FNO. To address this, we developed the novel architecture presented in Figure 1.\nWhile PINNs are effective for inverse problems, they require retraining from scratch for each new set of partial measurements, leading to substantial computational costs. In contrast, PI-DIONs enable real-time inference after a single training phase. As shown in Table 5 (Appendix C.1), PI-DIONs achieve significantly faster inference times than PINNs, with only a slight increase in relative error, which is negligible (see Table 4 in Appendix C.1).\n\n4. Thank you for suggesting ablation studies to further justify our model. Training details, including hardware specifications, are provided in Appendix B. In response to your feedback, we have now included training times in Table 5 (Appendix C.1) and the prediction errors for different sample sizes of 100, 500, 1000, and 2000 in Table 7 (Appendix C.2). As observed, approximately 1000 samples are required to achieve a relative error of about 1% for the target function, although smaller sample sizes may simplify optimization.\n\n5. Thank you for clarifying the theoretical contribution of our works. As you mentioned, we extended existing theoretical results from prior research. However, earlier works primarily focused on physics-informed neural networks (PINNs) for predicting the solution and target function from a single fixed partial measurement of the solution. In contrast, our work extends these results to an operator learning framework, providing stability estimates for predicting the solution and target function from any given partial measurement. \nSpecifically, Theorems 1 and 2 imply that, with sufficient measurements and samples, the difference between the continuous loss function $\\widetilde{\\mathcal{L}}(=\\widetilde{\\mathcal{L_{physics}}}+\\widetilde{\\mathcal{L_{data}}})$ and the loss function $\\mathcal{L}$ in Section 2 is small. Building on this, Theorem 3 guarantees that PI-DION can accurately approximate both the solution and target function from any given partial measurement by minimizing $\\mathcal{L}$. Finally, Proposition 1 establishes that, for any $\\epsilon>0$, a PI-DION structure can be constructed to optimize $\\mathcal{L}$ below $\\epsilon$. \nIn summary, minimizing the loss function $\\mathcal{L}$ in Section 2 allows PI-DION to efficiently and accurately predict both solution and target function. Since this approach can be applied to any equation with stability estimates for a single measurement, our work establishes the first general framework for physics-informed operator learning in inverse problems.\n\n6. Thank you for pointing out the possible confusion in the definition of $u$. We have included the definition of $u_l^{i}$ immediately after defining $\\mathcal{L}_{data}$ in line 155. Additionally, we have corrected the input of $f$ to be two-dimensional, specified by the points $(x, y)$. These revisions have improved the clarity of our paper, and we appreciate your feedback. \n\n\n[1] Michael Prasthofer, Tim De Ryck, and Siddhartha Mishra. Variable-input deep operator networks. 2022.\n\n[2] Sung Woong Cho, Jae Yong Lee, and Hyung Ju Hwang. Learning time-dependent pde via graph neural networks and deep operator network for robust accuracy on irregular grids. 2024"
            }
        },
        {
            "title": {
                "value": "Continued"
            },
            "comment": {
                "value": "6. Thank you for the valuable feedback regarding the verification of the theoretical bounds presented in Theorems 2 and 3. These theorems provide a rigorous foundation for PI-DION, ensuring that the approximated solution and target function converge to the true solution and target as the loss function $\\mathcal{L}$ is computed over a sufficiently large number of samples. As demonstrated in Table 5 of Appendix C.2, the relative error decreases with an increasing sample size. Since the theorems hold with a certain probability, empirically verifying the convergence rate (i.e., the order of error on sample size) would require extensive experimentation, which represents an interesting direction for future research. We have included a comment on this in the discussion section.\n\n[1] Roberto Molinaro, Yunan Yang, Bj\u00a8orn Engquist, and Siddhartha Mishra. Neural inverse operators for solving pde inverse problems. 2023."
            }
        },
        {
            "title": {
                "value": "Response to reviewer Y92e"
            },
            "comment": {
                "value": "Thank you for clearly summarizing our contributions and highlighting the strengths of our work. We completely agree with the strengths you have outlined.\n\nResponse to Weaknesses:\n\n1. The key distinction of PI-DION lies in its loss function, $\\mathcal{L_{physics}}$ Directly imposing $\\mathcal{L_{physics}}$ \u200b on models like DeepONet and FNO is not feasible, as they do not parameterize the solution and target function as functions of $x$. To address this limitation, we developed a novel architecture that enables the incorporation of $\\mathcal{L_{physics}}$  into the training process. We believe this innovative architecture and loss function are critical factors contributing to the observed performance improvements.\n\nResponse to Questions: \n\n1. Thank you for pointing out the missing information. We have addressed this by adding Table 3 in Appendix B, which provides the number of trainable parameters for all models listed in Table 1.\n\n2. Thank you for your insightful feedback. In response to your suggestion, we performed experiments by varying the relative weights between the physics and data losses across seven configurations: $\\(\\lambda_1, \\lambda_2) =  (100, 1), (10, 1), (1, 1), (1,0.1), (0.1, 1), (1, 10),  (1, 100)$ . As shown in Table 4 (Appendix C.2), the relative test error decreases as the weight assigned to the data loss increases. This observation aligns with our intuition that, during the early stages of training, a large $\\mathcal{L_{data}}$ will push $s_{\\zeta, \\theta}$ in the wrong direction, as $u_{\\eta, \\theta}$ differs from the true solution, leading to an inaccurate $\\mathcal{L_{physics}}$. We have included this discussion at the beginning of Section 4.\n\n3. Thank you for your valuable feedback regarding the comparison with other baselines, particularly the widely used NIO. While NIO is a promising approach, a direct comparison with our model is challenging because NIO relies on fully supervised learning for operator-to-function mapping. In contrast, PI-DION is specifically designed for function-to-function mapping and can be trained without direct supervision on the target function. To the best of our knowledge, no existing architecture aligns with this approach. Therefore, we compare our model with foundational models such as FNO and DeepONet. We appreciate your understanding of this context.\n\n4. Thank you for your critical question regarding the performance gap across problems. We believe the primary factor is the dimensionality of the problem. For the reaction-diffusion equation, the input function (partial measurement of the solution) is defined on a one-dimensional domain ($\\partial\\Omega$), and the target function is also defined on a one-dimensional domain ($\\\\{T\\\\}\\times\\Omega$). In contrast, the other two problems involve functions defined on two-dimensional domains.\nAs stated in Theorem 1 and 2, higher dimensionality increases the number of samples required to approximate the continuous version of loss functions $\\widetilde{\\mathcal{L_{physics}}}$ and $\\widetilde{\\mathcal{L_{data}}}$ with their discrete versions $\\mathcal{L_{physics}}$ and $\\mathcal{L_{data}}$.  This is due to the growth of constants $N_{\\mathcal{N}}, N_{\\mathcal{B}}$ and $N$ with dimensionality, given fixed bounds $R_{\\mathcal{N}}, R_{\\mathcal{B}}$ and $R$ in PI-DION.\nAdditionally, empirical evidence indicates that solving elliptic PDEs is inherently more challenging than parabolic PDEs, which may contribute to the observed performance gap.\n\n5. Thank you for considering the detailed stability estimates. As mentioned in Remark 1, these estimates are valid for certain equations, such as the inverse source problem for the reaction-diffusion equation and the Helmholtz equation. However, for the permeability function in the Darcy flow equation, relevant stability estimates have not yet been established. Despite this, empirical evidence presented in Section 4 confirms that PI-DION can still be consistently applied to the Darcy flow problem with the loss function described in Lines 152 and 352. These results suggest that PI-DION shows promise for approximating various inverse operators, even in the absence of formal stability estimates."
            }
        },
        {
            "title": {
                "value": "General response to reviewers"
            },
            "comment": {
                "value": "We sincerely thank the reviewers for their thoughtful and constructive feedback on our manuscript. The comments were both insightful and helpful, enabling us to make significant improvements. In our revisions, we have clarified the notation and explanations, as well as added further details, including the number of parameters, the relative weights, and the training and inference times, which are now provided in Appendices B and C. Furthermore, we have conducted additional experiments, detailed in Appendix C, to highlight and emphasize our contributions. For more specific updates, please refer to the individual responses.\n\nWe believe that most of the weaknesses and questions have been thoroughly addressed, and we look forward to further discussions."
            }
        },
        {
            "summary": {
                "value": "The authors propose Physics-Informed Deep Inverse Operator Networks (PI-DIONs), a novel architecture for solving PDE inverse problems without requiring labeled data. Theoretically, the authors extend stability estimates from traditional inverse problem theory to the operator learning setting, and prove universal approximation theorems for PI-DIONs. Empirically, the authors validate their proposed approach through experiments on reaction-diffusion equations, Helmholtz equations, and Darcy flow, achieving SOTA performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper engages with an important problem in SciML, learning to solve inverse problems based on physics losses without additional training data.\n- The theoretical results are quite interesting. The authors extend standard stability estimates for inverse problems to the operator learning setting. Promisingly, the theorems apply to the reaction-diffusion equation and the Helmholtz equation, two standard benchmarks in the literature.\n- The proposed method is simple and presented clearly and generally.\n- The empirical results are promising. On three standard benchmarks, the authors demonstrate SOTA performance of supervised learning and near-SOTA of unsupervised learning, compared to supervised DeepONet and FNO."
            },
            "weaknesses": {
                "value": "- The main weakness of the paper is that the empirical results, although promising, are relatively limited and could benefit from some clarification:\n  - In Table 1, PI-DION in the supervised learning setting (with 1k training examples) is shown to outperform two different DeepONets and FNOs. However, it's a bit unclear from the paper why this is true, and additional clarification about this would be helpful. Is there a difference in the model architecture / training objective / optimizer between the DeepONets and PI-DION in the supervised setting?\n  - See questions for more."
            },
            "questions": {
                "value": "- Questions about experimental results:\n  - What are the number of parameters of each of the models in Table 1?\n  - Could the authors provide a sensitivity analysis showing how performance changes as the relative weighting between physics and data losses is varied? This would provide valuable insight into the method's robustness.\n  - How does PI-DION compare to other methods for solving inverse problems, e.g. Neural Inverse Operators [1]?\n  - Any explanation about why the performance hit between supervised and unsupervised PI-DION is larger for Darcy Flow and Helmholtz equation than for reaction-diffusion?\n\n- How limiting is the assumption that there exists stability estimates for the inverse problem?\n- How well do the theoretical bounds from Theorems 2, 3 match the empirical results of Table 1 (reaction-diffusion and Helmholtz)?\n\n1. Neural Inverse Operators for Solving PDE Inverse Problems"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes an architecture called Physics-Informed Deep Inverse Operator Networks (PI-DIONs), which can learn the solution operator of PDE-based inverse problems without labeled training data. The architecture  of PI-DIONs is based on DeepONet, and trained with both the physics-infomred loss and data reconstruction loss. The stability estimates established in the inverse problem literature are extended to the operator learning framework. Experiments are conducted to demonstrate the effectiveness of PI-DIONs in learning the solution operators of the inverse problems without the need for labeled data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The integration of physics-informed losses into an inverse problem framework based on operator learning is novel, and in principle PI-DIONs can solve the inverse problems (at least in scenarios mentioned in experiments) fast and without the need for labeled data.\n2. Theoretical analysis of the stability estimates is provided."
            },
            "weaknesses": {
                "value": "1. Line 243, \"where the term \u2225f \u2212 f^\\star\u2225L2(\u2126m) in the righthand side\", there is no such term there. Please clarify the equation in line 242 and include all terms on the right-hand side of the equation. \n2. It seems that the input to the reconstruction and inverse branch networks is fixed in shape, corresponding to the partial measurement with given geometry. The observed data in PINNs can have variable count and locations. Please discuss how PI-DIONs might be adapted to handle variable measurement geometries and if there are any limitations on the types of measurement setups it can handle. \n3. In the experiments, PI-DIONs are compared with purely data-driven DeepONet and FNO, which both did not take physics information into account. If possible, please include comparisons with PINNs in the experiments, since both your PI-DIONs and PINNs are physics-informed methods for inverse problems.\n4. The simultaneous training of physics-informed losses for 1000 samples is a difficult task (similar to train 1000 PINNs simultaneously). I am curious about the training difficulties encountered. Please provide specific details on training time, hardware used, and any convergence challenges encountered. If possible, please also include an ablation study on the effect of sample size on PI-DIONs' performance since smaller sample size may lead to easier optimization.\n5. The theoretical analysis on stability estimate is extended from existing key results that considered the single element case. \n6. Please provide a clear definition of u in line 152 and describe its relationship with partial measurement. In line 456, it is better to write \"f(x,y) = 100x(1 \u2212 x)y(1 \u2212 y) \", so does line 450. \n\nConsidering the above weaknesses, I give a score of 3 to the current version of this paper."
            },
            "questions": {
                "value": "1. DeepONet and FNO are used for forward problems traditionally, how did they deal with inverse problems in your experiments?\n2. How is the labeled training target f mentioned in line 399 used?  The loss for target f is absent in line 152."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Physics-Informed Deep Inverse Operator Networks (PI-DIONs) for solving PDE-based inverse problems without the need for labeled data. The paper extends existing stability estimates from inverse problem literature to the operator learning framework, ensuring the robustness and generalizability of PI-DIONs across the entire function space and domain."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper provides a solid theoretical foundation for the proposed PI-DIONs.\n2. The proposed method demonstrates practicality and efficiency in addressing PDE-based inverse problems without the need for labeled data."
            },
            "weaknesses": {
                "value": "1. The contribution lacks novelty. The architecture relies on relatively simple components, such as CNNs and MLPs for the branch and trunk networks. It doesn't introduce significant advancements beyond well-established methods.\n2. The baselines used for comparison, such as DeepONet and FNO, are somewhat dated. The paper would benefit from comparisons with more recent and state-of-the-art methods to better demonstrate the model's competitiveness.\n3. The experimental evaluation is limited in range. Conducting experiments on a broader range of benchmarks would strengthen the validation of the proposed method's effectiveness across diverse problems."
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}