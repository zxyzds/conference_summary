{
    "id": "2edigk8yoU",
    "title": "Looped Transformers for Length Generalization",
    "abstract": "Recent work has shown that Transformers trained from scratch can successfully solve various arithmetic and algorithmic tasks, such as adding numbers and computing parity. While these Transformers generalize well on unseen inputs of the same length, they struggle with length generalization, i.e., handling inputs of unseen lengths. In this work, we demonstrate that looped Transformers with an adaptive number of steps significantly improve length generalization. We focus on tasks with a known iterative solution, involving multiple iterations of a RASP-L operation\u2014a length-generalizable operation that can be expressed by a finite-sized Transformer. We train looped Transformers using our proposed learning algorithm and observe that they learn highly length-generalizable solutions for various tasks.",
    "keywords": [
        "Transformers"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=2edigk8yoU",
    "pdf_link": "https://openreview.net/pdf?id=2edigk8yoU",
    "comments": [
        {
            "summary": {
                "value": "- This work studies the efficacy of Looped Transformers for Length Generalization of several algorithmic tasks whose computation complexity is known (as a function of the query length).\n- The paper proposes the definition of $n$-RASP-L, a generalization of the RASP-L computation model allowing the loop of RASP-L programs. It is shown, under a general framework called full-answer prediction (FAP), that some tasks (Copying binary sequence (allowing duplicates), Parity, and Binary Addition) have their own $n$-RASP-L program with a linear number of steps in problem length.\n- The authors propose training Looped Transformers (with input injection and curriculum learning) to learn $n$-RASP-L-programmable tasks, where the ground-truth number of steps is known for each task during training. They also propose two variants of inference methods: either we retain the knowledge about the number of steps at inference time (*Oracle*), or we adaptively decide the number of iterations based on the confidence of FAP (*Maximum confidence*).\n- The proposed method is tested on several algorithmic tasks."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1. The paper is written and organized well. Overall, the presentation of the methodology and empirical results is clear and easy to follow.\n\nS2. The idea behind the proposed method is neat and plausible. It is natural to think about adaptively scaling the depth of the model according to the problem length or the problem complexity. This paper successfully implements this idea to solve various interesting algorithmic tasks with the power of Looped Transformers. Also, $n$-RASP-L is an interesting but intuitive generalization of the RASP-L framework by allowing the loops. \n\nS3. The proposed answer-generation framework called FAP is also an interesting component of this work. It might be of separate interest to study.\n\nS4. The paper presents extensive ablation studies on several components of the proposed method. Also, the empirical results (length generalization performances) are impressive enough to convince the readers about the proposed method\u2019s efficacy."
            },
            "weaknesses": {
                "value": "**W1. The definition of $n$-RASP-L (Definition 3.1) can be improved.**\n\n- I think the equation \u201c$T(n): \\mathbb{N} \\rightarrow \\mathbb{N}$\u201d should be corrected to \u201c$T: \\mathbb{N} \\rightarrow \\mathbb{N}$\u201d because $T$ (instead of $T(n)$) is a function of input length $n$ representing the number of steps inside a task-solving $n$-RASP-L program.\n- In (2), I guess $P\u2019$ should be a RASP-L program, which is unspecified in the definition.\n- Should $P$ be decomposed to a sequential application of $P\u2019$, i.e., $P = (P\u2019)^{T(n)}$? I don\u2019t think this is exactly true because there are pre-/post-processing parts inside the proposed $n$-RASP-L programs (in Appendix A). Can the same RASP-L program $P\u2019$ handle such parts? (It might be true because of the experimental results, but I cannot fully understand this part.) If not, I guess the definition should be modified to include the pre-/post-processing parts. For example, $P = P_{\\tt pre} \\circ (P\u2019)^{T(n)} \\circ P_{\\tt post}$.\n\n**W2. \u201cGround truth\u201d number of steps?**\n\n- According to Definition 3.1, a program $P$ suffices to be an $n$-RASP-L if a corresponding $T(n)$ exists. Indeed, Propositions 3.2, 3.3, and 3.4 claim and prove the existence of $T(n)$ for the Parity, Copy (with duplicates), and Binary Addition tasks, respectively.\n- My question is about the uniqueness or optimality of such $T(n)$\u2019s. There might be a clever way to construct another RASP-L program $\\tilde{P}$ so that $P$ can be implemented with $\\tilde{T}(n)$ steps of applying $\\tilde{P}$, where $\\tilde{T}(n)$ is much smaller than the previously known $T(n)$ (e.g., $\\tilde{T}(n) \\in o(T(n))$). It may happen since there is no uniqueness guarantee or lower bound result on $T(n)$.\n    - If I venture a guess, I would say it might be possible to implement an $O(\\log n)$-step $n$-RASP-L solution for the Parity task by using the parallelism of the transformer architecture. Please correct me if I am wrong. Also, I understand if it is impossible to show whether this bold guess is true. If you are interested, there are some (probably) useful references about logarithmic-depth transformers [1,2].\n- However, the authors keep using the phrase \u201cground truth number of steps\u201d throughout the paper, which may lead to misunderstanding that the only way to implement the given $n$-RASP-L program is by using a loop of length $T(n)$.\n- If two different $T(n)$\u2019s can be applied to a single $n$-RASP-L-programmable task, it might be interesting to observe whether the model\u2019s performance changes depending on the choice of $T(n)$.\n- Furthermore, if multiple choices of $T(n)$\u2019s exist for a given task, does knowing only one of them suffice to train reasonably performant Looped Transformers? If we know more than one, how should we choose $T(n)$ when we train the model?\n\n**W3. Shouldn\u2019t we consider the input injection when implementing an $n$-RASP-L program for the given task?**\n\n- The input injection seems to be an important component of their experiments. Since it changes the input vectors of each layer, I guess the task-solving algorithm under input injection might be different from that without it.\n- However, I can\u2019t see that the $n$-RASP-L programs provided in Appendix A reflect the input injection. As I inspect inside the loop of each program, every iteration only reuses the calculation(s) from the previous iteration right before the current one.\n- Shouldn\u2019t we consider the very first input sequence and the result from the previous iteration when implementing the loops? Or is it a valid implementation of input injection? Getting even further, Is there any way to embed the input injection into the $n$-RASP-L programs?\n\n**W4. The proposed training method requires prior knowledge of the task\u2019s structure.**\n\n- The proposed method is limited in that it requires a prior understanding of the structure (e.g., $T(n)$) of the task where we want to train a model. This is because it hinders fully end-to-end training.\n- Are Looped Transformers still useful for achieving length generalization even when we don\u2019t (or cannot) know the exact expression of $T(n)$?\n- Besides, it seems that the depth of the decoder block is determined based on the complexity/difficulty of the subroutine $P\u2019$ at each step inside the loop (Appendix F). How are they actually chosen? Or, how should we decide the size of the repeating decoder block?\n\n**W5. Some experimental details seem missing or wrong.**\n\n- I guess Equation (2) has a typo: shouldn\u2019t it be arg-main instead of arg-max?\n- In Binary Addition, it seems that $T$ is chosen to be $n$ (the length of each operand). However, Proposition 3.4 claims that $T(n)=n+1$ for the same task. Why is there a discrepancy between theory and experiment?\n- In Binary Multiplication, I guess some words are used in a wrong way. In Lines 417-418, I think it should be: \u201cWe define the problem length to be the **length** of the second **number**, and set $T$ to be the product of the lengths of two **numbers**.\u201d\n- In Section 6.1.2, are input injections also applied to NTP-based methods? Also, I\u2019m not sure why it is fair to compare their method (based on FAP) to NTP methods with the architectural setting \u201c\u2026with a depth 20 times the depth of the looped block\u201d because such depth might be suboptimal for NTP-based methods.\n- Although the paper indirectly showcases that their adaptive decision of the number of steps works quite well via Figure 5, it would be better to display similar performance plots to Figure 4 (plots based on the \u201cOracle\u201d inference) but using the adaptive confidence-based method instead, at least in their appendix.\n\n**W6. Minor writing issues**\n\n- Section 4.1, fourth bullet point: I guess $T(n) \\in \\\\{T(1), \\ldots, T(n_{\\rm max})\\\\}$ is correct ($T(1)$ instead of $1$).\n- Equations (1) and (2) have several weird-looking brackets (too many open brackets etc.)\n- Line 510: Use *less* abbreviations like \u201cw.r.t.\u201d\n\n---\n\n**References**\n\n[1] Sanford, Clayton, et al. \"Transformers, parallel computation, and logarithmic depth.\"\u00a0ICML 2024.\n\n[2] Sanford, Clayton, et al. \"Understanding transformer reasoning capabilities via graph algorithms.\"\u00a0NeurIPS 2024."
            },
            "questions": {
                "value": "**Q1. Question on the visualization in Figure 3**\n\n- Why don\u2019t the illustrations in the figure contain any \u201c#\u201d (EOS) tokens? Is it due to the pre-processing?\n\n**Q2. Do the trained Looped Transformers simulate the $n$-RASP-L program?**\n\n- Although it might be difficult to reverse-engineer a trained transformer model to figure out what algorithm it actually simulates or implements, it might be interesting if we can observe any kind of similarity between it and the $n$-RASP-L program."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the length generalization problem of Transformer models, which refers to the inability of the model to deal with longer samples than encountered during the training phase. While recent literature has focused on modifying the positional embeddings and the input formats, this paper proposes to use Looped Transformers, which can dynamically adjust their computation steps according to the problem length. The authors define n-RASP-L problems to figure out which problems can be solved by Looped Transformers. Then, they train the models on these tasks (parity, copy, binary addition, binary sum, binary multiplication, unique set) under a full-answer prediction setup. Empirically, the trained models could successfully length-generalize to longer lengths by appropriately adapting the number of loops at inference time."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well-structured and clearly written.\n- The introduction of Looped Transformers is well-motivated and effectively argued.\n- The results are strong and solid. They do not require the use of a scratchpad. Also, the prediction is conducted using an end-to-end, full-answer prediction setup, which is a more general way than the conventional next-token prediction setup.\n- The paper clearly illustrates that the model can determine the number of steps to take on its own and does not require T(n) in the test time."
            },
            "weaknesses": {
                "value": "Weakness 1: Applicability Limited to n-RASP-L Tasks\n\n- The approach is limited to tasks that belong to n-RASP-L categories, as it requires the ground-truth number of steps in the training data.\n\nWeakness 2: Insufficient Experimentation.\n\n- ***Effect of Curriculum Learning.*** How does the model perform without curriculum learning? Is the use of curriculum learning necessary?\n\n- ***Tolerance to Step Counts.*** I am curious whether this method will still perform well with different choices of T(n). For example, for tasks like parity, would the model maintain its performance if T(n) were set to n+1 rather than n? What about 2n instead of n? This question stems from the possibility that there might be more efficient solutions to n-RASP-L problems than human-designed ones, which could work with fewer steps. Testing whether the model is robust under overestimated T(n) values could help verify the robustness of this approach.\n\n- Overall, the paper requires more ablation studies."
            },
            "questions": {
                "value": "Q1. In Figure 5, why do some tasks perform well even when exceeding the step count, while others degrade immediately? For instance, the performance of the parity task and the binary sum task immediately drops when executed with additional steps, whereas the addition, multiplication, and copy tasks retain accuracy to some extent.\n- Particularly for the copy task, the selected step count is significantly higher than the actual number of steps required, which seems unusual to me.\n\nQ2. Are there any tasks whose T(n) is nonlinear (e.g. sqrt(n), n^2) to the length of the input sequence? It would be interesting to see experimental results for such tasks.\n\nQ3. Why is the output reversed for binary multiplication (but not for binary addition)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper examines how looped transformers perform in terms of length generalization. The focus is on n-RASP-L problems, which are problems that can be tackled using a loop of a single RASP-L program. The concept is that Transformers can learn steps that are independent of length, employing a flexible number of iterations in the looped transformer to achieve length generalization. The authors first demonstrate that n-digit addition, n-bit parity, and copying n symbols can be addressed with n-RASP-L solutions. They then reveal that when utilizing the looped transformer with adaptive stopping time, the results exhibit significantly stronger length generalization compared to next token prediction (NTP) and other methods like using pause tokens or NTP-loop with a fixed stopping time."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Overall, I really liked the paper, I think that using a looped transformer to achieve length generalization is an interesting idea that was not studied in the past to my knowledge. This paper complements all the other techniques (universal transformers, different types of position emebedding, etc.) that were used in the past for length generalization The paper is well-written and well-explained. This is why I advocate for acceptance of this paper."
            },
            "weaknesses": {
                "value": "I would like to raise the following weaknesses/questions regarding this paper: \n\n- **Lack of other baselines**: What would happen if you have a very deep universal transformer? Universal transformers also have shared parameters and looks equivalent to the loop transformer. The depth may play the role of the number of loops. Would this be equivalent to the fixed loop NTP? It would be interesting to run the same experiments with a universal transformer.\n\n- **Comparison with other methods**: Where would you position the looped transformers in the list of all the tricks for length generalization? Are the effects similar or complementary to change of the input (index hinting, reverse order of operands, etc.) ? Changes of positional encoding? Chain of Thought? It would be interesting to understand this by making combinations of the tricks with looped transformers with other tricks and analyze the performance differences.\n\n-  What is the depth of the encoder block in the loop transformer? I think this information is important to put in the main paper. \n\n- **Adaptive inference time**: I think one weak point of the method is actually coming up with an adaptive inference time. The methods that are proposed are nice but may look a bit hacky. Do you think one could learn this adaptive inference time?\n\n- In Figure 2, which adaptive inference time method is used for FAP-Loop-Adaptive?\n\n- Lastly, this is a wild question: have you tried your method on problems where there is no n-RASP-L solutions?  Would it still work better than just doing NTP?"
            },
            "questions": {
                "value": "I listed my questions in the weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Empirically explores the ability of looped Transformers, i.e. Transformers that repeatedly apply the same block of layers, to length-generalize on several algorithmic tasks, including copy, parity, and addition. First, the authors manually derive length-generalizing solutions to the considered tasks through a variant of the RASP language, which they term n-RASP-L. Then, based on these ground truth solutions, they show that looped Transformers length-generalize well when trained with access to the true number of steps required to compute the output for a given input."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is mostly well-written and easy to follow.\n\n2. Demonstrates that, given knowledge of the number of steps required to perform a given task, a certain looped Transformer, which jointly predicts the full output sequence, tends to learn a length-generalizing solution. The length-generalizing capabilities of this looped Transformer are shown to surpass baselines that use next-token prediction."
            },
            "weaknesses": {
                "value": "1. The main weakness of the current paper is that the significance of the results is somewhat limited. In particular, I find that it falls somewhere in between works that are practically relevant and those that may not be practically relevant, but improve our understanding of certain phenomena. On the one hand, this work shows empirically that, in some algorithmic tasks for which we already know how to write explicitly a length-generalizing solution (in terms of looped Transformer weights), looped Transformers generalize well to longer lengths, if they have access during training to the number of steps required for solving the task for a given input. Consequently, the practical relevance is limited since the proposed method requires that we already know how to manually write a length-generalizing solution, in which case there is arguably no point in learning. On the other hand, this work does not provide much in terms of understanding why or how looped Transformers are able to length-generalize.\n\n    Note that one may consider the demonstration of such length generalization to be possible as a main contribution. Yet, the ability to extrapolate through recurrence of layers has been demonstrated in the past, albeit for other architectures (see Bansal et al. 2022 [1], which notably do not require knowing the ground truth number of steps in training).\n\n2. A related issue is the usage of ground truth stopping time during inference. The quantities reported in Figure 5 seem to be for a single training example, yet it is not entirely clear. If so, then how does the maximum confidence stopping criterion fair across the dataset? It would be useful to report results similar to those of Figure 4 but when using the proposed stopping criterion as opposed to the ground truth stopping time, which should be unknown.\n\nOverall, my assessment of the paper tends towards the positive side, yet it is not a clear accept due to the substantial limitations mentioned above. Specifically, the significance of the contributions can be greatly improved if it would be possible to remove the dependence on knowing the ground truth number of steps required to solve the task for a given input during training (and by how it seems from the current results, during test time as well).\n\n\nAdditional (more minor) comments:\n- In Definition 3.1, it seems that the intention is for $P\u2019$ to be some RASP-L program, as opposed to just a program. Otherwise, trivially any program $P$ is an n-RASP-L program by choosing $P\u2019 = P$ and $T(n) = 1$.\n- In Equation (2), I believe that the criterion should be an argmin over the cross entropy loss instead of an argmax.\n\n\n[1] Bansal, A., Schwarzschild, A., Borgnia, E., Emam, Z., Huang, F., Goldblum, M., & Goldstein, T. (2022). End-to-end algorithm synthesis with recurrent networks: Extrapolation without overthinking. Advances in Neural Information Processing Systems, 35, 20232-20242."
            },
            "questions": {
                "value": "1. Are the quantities reported in Figure 5 indeed for a single training example? When using the maximum confidence criterion, how do the results compare to the ones reported in Figure 4 with access to the ground truth number of steps?\n\n2. In Bansal et al. 2022, they avoid the need for knowing the exact number of steps during training and inference. Have you tried using similar heuristics?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}