{
    "id": "uxYbEAEWm4",
    "title": "Knowledge Lift Alignment Fine Tuning",
    "abstract": "We present a visual tuning framework, \\textbf{K}nowledge \\textbf{L}ift \\textbf{A}lignment \\textbf{F}ine \\textbf{T}uning (KLAFT), \nwhich enhances the expressive image captioning capabilities of Pre-trained Language Models (PLMs), including LLMs and VLMs.\nAs this task involves generating more detailed and comprehensive captions than basic image descriptions,\nthe core idea behind KLAFT is that fine-grained alignment could exploit the capabilities of PLMs and a given target domain dataset.\nThis idea motivates and challenges us to explore the framework that deeply understands both given images and text for this alignment and tuning PLMs towards expressive image captioning.\nThis direction modifies the attention mechanism (Modified Attention Mechanism, MAM) and develops both a Topic Control Mechanism (TCM) and their training objectives.\nThe innovation of KLAFT lies in its approach to addressing the disparities in knowledge - visual versus textual via MAM\nand source versus target domain via TCM.\nAs these hidden spaces are conceptualized as distinct sub-networks within the PLM, each possessing specific knowledge,\nKLAFT's unique contribution is in aligning and adjusting the weights of these sub-networks in a fine-grained manner,\nand fine-tuning this PLM.\nOur empirical studies demonstrate that KLAFT significantly improves expressive captioning tasks by aligning and amplifying target knowledge, with the potential for Parameter-Efficient Fine-Tuning (PEFT) at low computational cost.",
    "keywords": [
        "PEFT",
        "PLM",
        "LLM",
        "VLM",
        "Multi-modal",
        "Image captioning"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "KLAFT aims to generate detailed and comprehensive captions by leveraging fine-grained alignment between PLMs and target domain datasets.",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=uxYbEAEWm4",
    "pdf_link": "https://openreview.net/pdf?id=uxYbEAEWm4",
    "comments": [
        {
            "summary": {
                "value": "This work present a visual tuning framework, Knowledge Lift Alignment Fine Tuning (KLAFT), which enhances the expressive image captioning capabilities of Pretrained Language Models (PLMs). The innovation of KLAFT lies in its approach to addressing the disparities in knowledge - visual versus textual via MAM and source versus target domain via TCM. These hidden spaces are conceptualized as distinct sub-networks, each possessing specific knowledge, KLAFT adjusts the weights of these sub-networks in a fine-grained manner. The empirical studies demonstrate that KLAFT improves expressive captioning tasks by aligning and amplifying target knowledge."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The work propose design a Topic Control Mechanism (TCM) to emphasize the target domain-specific knowledge. Combined with Token Topic Modeling (TTM), Masked Region Modeling (MRM), and Text Image Matching (TIM), KLAFT highlights the target related\nknowledge (i.e., the knowledge lift).\n\n- The propsoed KLAFT improves expressive captioning tasks by aligning and amplifying target knowledge, with the potential for\nParameter-Efficient fine tuning (PEFT) at low computational cost."
            },
            "weaknesses": {
                "value": "-  The writting of this work is quite poor. The inroduction part does not clearly introduce the background and motivation of the problem to be solved. The approach part is also not written in a concise and well-organized manner.\n\n- The approaches compared in this work are not proposed recently. As a result, the validity and innovation of the proposed modules is not convincing.\n\n- The authors did not conduct sufficient ablation experiments for different loss functions and did not give more qualitative analysis."
            },
            "questions": {
                "value": "See the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Knowledge Lift Alignment Fine-Tuning (KLAFT) to enhance the expressiveness of image captioning capabilities in pretrained Vision-Language Models (VLMs). KLAFT primarily introduces a Topic Control Mechanism (TCM) combined with Token Topic Modeling (TTM) to enable topic-guided tuning and decoding for VLMs. The proposed method is evaluated across various settings."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Increasing the expressiveness of the generation process in VLMs is a promising research direction.\n* The proposed TCM/TTM approach is intriguing and shows potential.\n* The performance gains over baseline models are satisfactory."
            },
            "weaknesses": {
                "value": "* The presentation of the paper lacks clarity. I\u2019ve outlined some key issues below:\n  * The term \"KEIC\" appears three times\u2014once in Figure 1, once in the title of Section 4, and again in the conclusion. These are critical sections. I assume \"KEIC\" should actually be \"KLAFT.\"\n  * Some typos or unclear sentences can a problem, e.g. ``tokes`` appears multiple times.\n  * The image in Table 1 is not clear, and Figure 1 also lacks effective delivery and clarity.\n  * I assume all the citations use ``\\cite{}``. I think most citations should likely use ``\\citet{}`` for smoother integration.\n  * And some other issues, see question section as well.\n* Overall, the claims around the design of the Mapping Layer (MaL) and the Modified Attention Mechanism (MAM) may be overstated. These components are fairly common in VLMs, and MAM appears to be a direct adaptation from one of the primary baselines, VisualGPT.\n* The choice of main baselines seems outdated, although results from some recent VLMs (e.g., LLaVA, BLIP2) are also included."
            },
            "questions": {
                "value": "* In Figure 1 (left), a plot of token distributions is shown, but there is no information about the dataset. What are the source and target domains? How were these lines plotted? Given the significance of comparing the two domains, this should be one of the most important analyses in the paper.\n* I get the main idea of TCM. Yet, there are some phrases in the paper making the details of TCM unclear to me:\n  * In line 234, `\"distribution over tokes, $\\mathbf{V}$. What is  $\\mathbf{V}$? It only appears once in the paper.\n  * In the paragraph following Eq (5), the term $b_z$ is mentioned twice, though it doesn\u2019t appear in Eq (5). This inconsistency makes it challenging to understand Equation (5) fully. Additionally, the variable $w$ is not explained in the paragraph.\n* In lines 338-339, it's stated that three hyperparameters are all set to 0.1, which seems to be less commonly done. What is the rationale for this choice? Is there any ablation study to support it?\n* In Sec. 5.2, the authors compare S4 to S3 to showcase the effectiveness of TCM. Yet, these two settings differ a lot:\n  * S3: \"seq2seq under fine-tuning GPT-2 over 100% training data (COCO+Conceptual Captions)\"\n  * S4: \"prefix 100% training data with frozen setting (only COCO)\"\n  * Given the differences in both tuning paradigm and datasets, how can TCM\u2019s impact be fairly assessed in this context?\n* Could you provide more details on human evaluation?\n\n*I'm open to adjusting my rating after the authors' response.*"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a Knowledge Lift Alignment Fine Tuning (KLAFT) framework to improve the image captioning capability of multimodal LLMs. More specifically, this paper aims to encourage the generated captions to be detailed and comprehensive. To achieve that, this paper explores fine-grained alignment and designs MAM and TCM attention mechanisms. Experiments and quantitative comparisons are conducted on commonly used benchmark datasets, including MS COCO and Conceptual Captions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Improving the quality of captions generated from VLMs has great potential for several real-world applications, such as manufacturing and healthcare.\n2. The proposed framework is reasonable to address the considered task."
            },
            "weaknesses": {
                "value": "1. As described in the Abstract, the goal of this paper is to generate detailed and comprehensive captions. However, the benchmark datasets adopted by this paper (e.g., MS COCO) are typically coarse-grained with relatively short captions. As we know, current VLMs, like LLaVA, are good at generating detailed and fine-grained image captions. Is using COCO to evaluate the detailed caption capability of VLMs suitable? My concern lies in adopting such traditional image captioning benchmarks (e.g., COCO), which cannot properly measure the detailed caption capability of VLMs and reflect the actual improvement of the proposed method.\n2. What are the differences between the source and target domains claimed in this paper? Does the difference lie in the visual domain shift? Does the difference lie in the caption style? Does the difference lie in the amount of (labeled) data? More clarification and explanation are encouraged to make this paper more easy to read.\n3. The proposed method is trained by multiple training objectives. It is complicated to model training and balance the weights of each loss function (in Eq.(9)). How does this paper design experiments to determine the weight of each loss term? Is the model training sensitive to different weights of loss terms?\n4. The visual presentation of this paper has some room to improve. For example, the image in Table 1 is out of the table. In addition, for Table 3, it is better to directly indicate the meaning of each row in the table instead of just mentioning it in the captions."
            },
            "questions": {
                "value": "Please refer to the Weaknesses. The following is a minor question.\n\n1. Considering the fast-evolving nature of the VLM development, are some recently proposed and commonly used multimodal LLMs, such as VILA or InternVL-2, also applicable to this proposed fine-tuning framework?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}