{
    "id": "CgRkPuhTGm",
    "title": "SSNet: Skip and Split MLP Network for Long-Term  Series Forecasting",
    "abstract": "Time series forecasting is critical across various domains, \nincluding energy, transportation, weather prediction, and healthcare. \nAlthough recent advances using CNNs, RNNs, and Transformer-based models have shown promise, \nthese approaches often suffer from architectural complexity and low computational efficiency. \nMLP-based networks offer better computational efficiency but struggle to effectively model periodic and temporal relationships, \nwhich are essential for accurate time series forecasting. \nTo address these challenges, we propose the Skip and Split MLP Network (SSNet), \nfeaturing innovative Skip-MLP and Split-MLP components that adeptly handle these relationships. \nSSNet requires fewer parameters than traditional MLP-based architectures, improving computational efficiency. \nEmpirical results on multiple real-world long-term forecasting datasets demonstrate that \nSSNet significantly outperforms state-of-the-art models, delivering better performance with fewer parameters. \nNotably, even a single Skip-MLP unit matches the performance of high-performing models like PatchTST.",
    "keywords": [
        "Time Series Forecasting",
        "Deep Learning",
        "MLP"
    ],
    "primary_area": "learning on time series and dynamical systems",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=CgRkPuhTGm",
    "pdf_link": "https://openreview.net/pdf?id=CgRkPuhTGm",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces the Skip and Split MLP Network (SSNet) for time series forecasting, which offers superior computational efficiency compared to existing models. SSNet\u2019s Skip-MLP and Split-MLP components effectively capture both temporal and periodic patterns using much fewer parameters, outperforming SOTA transformer-based models on benchmark datasets. The paper highlights the efficiency and effectiveness of MLP-based methods over transformer-based approaches once again."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. SSNet achieves SOTA forecasting performance with significantly fewer parameters, providing a promising direction for time series forecasting research.\n2. The proposed Split-MLP and Skip-MLP components are well-suited to the characteristics of time series data.\n3. The paper is easy to follow, and the experiments are extensively conducted.\n4. The supplementary materials are well-prepared."
            },
            "weaknesses": {
                "value": "1. Although the authors provide the number of parameters to show efficiency, FLOPs are also an important factor. Please include FLOP measurements.\n2. The results in Figures 6 and 7 are blurry and overlap, please revise them.\n3. In the README, it is stated that \"A NVIDIA graphics card with 80GB of VRAM is required,\" but as shown in Figure 6, GPU usage is less than 10GB. Please explain it and provide more explanation in the README."
            },
            "questions": {
                "value": "1. K is an important hyper parameter in the model design and feature extraction, how does it affect the model performance?\n2. Please address the concerns raised by W1 and W3."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on long-range time series forecasting, and introduces a novel MLP-based network, comprising Skip-MLP and Split-MLP, combined into SSNet. The author conducted experiments on 7 datasets to compare the performance of SSNet with past methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The innovativeness of the model is good. \n\n2. The overall presentation of the article is very clear."
            },
            "weaknesses": {
                "value": "1. The experimental setup has some issues. The article used different look-back lengths for the models being compared, which is evidently unreasonable as the input length variable was not controlled. Although it must be acknowledged that different models may exhibit varying performance at different look-back lengths, a reasonable approach would be to evaluate all models using both long and short look-back lengths to compare their performance. Therefore, the results under the current setup are difficult to be convincing.\n\n2. The author seems to have a bias towards the models being compared. I noticed that the author focused on TimeMixer and PDF but did not pay attention to contemporaneous models like iTransformer[1], FITS[2], ModernTCN[3], or even earlier methods such as Koopa[4], CrossGNN[5], FourierGNN[6], WITRAN[7], and Basisformer[8]. The author should comprehensively compare all of them, as I believe this would enhance the quality of the article.\n\n[1] Liu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L., & Long, M. (2024). iTransformer: Inverted Transformers Are Effective for Time Series Forecasting. In The Twelfth International Conference on Learning Representations.\n\n[2] Xu, Z., Zeng, A., & Xu, Q. (2024). FITS: Modeling Time Series with $10 k $ Parameters. In The Twelfth International Conference on Learning Representations.\n\n[3] Luo, D., & Wang, X. (2024). Moderntcn: A modern pure convolution structure for general time series analysis. In The Twelfth International Conference on Learning Representations.\n\n[4] Liu, Y., Li, C., Wang, J., & Long, M. (2024). Koopa: Learning non-stationary time series dynamics with koopman predictors. In Thirty-seventh Conference on Neural Information Processing Systems.\n\n[5] Huang, Q., Shen, L., Zhang, R., Ding, S., Wang, B., Zhou, Z., & Wang, Y. CrossGNN: Confronting Noisy Multivariate Time Series Via Cross Interaction Refinement. In Thirty-seventh Conference on Neural Information Processing Systems.\n\n[6] Yi, K., Zhang, Q., Fan, W., He, H., Hu, L., Wang, P., ... & Niu, Z. FourierGNN: Rethinking Multivariate Time Series Forecasting from a Pure Graph Perspective. In Thirty-seventh Conference on Neural Information Processing Systems.\n\n[7] Jia, Y., Lin, Y., Hao, X., Lin, Y., Guo, S., & Wan, H. (2023). WITRAN: Water-wave Information Transmission and Recurrent Acceleration Network for Long-range Time Series Forecasting. In Thirty-seventh Conference on Neural Information Processing Systems.\n\n[8] Ni, Z., Yu, H., Liu, S., Li, J., & Lin, W. (2023). BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis. In Thirty-seventh Conference on Neural Information Processing Systems.\n\n3. It seems that the author has not described the details of the parameter search space for the compared baselines. Has the author validated the baseline methods by searching for the best parameters on the validation set? If this has been done, please disclose the results of the parameter search. If this work has not been carried out, it would not provide strong evidence that SSNet outperforms the compared methods. Because the experimental platform can also affect the accuracy of model training, in other words, the optimal parameters on different platforms should be different. To make a fair comparison of their performance, a sufficiently comprehensive parameter search would be conducted on the same platform for all methods within the same search space to ensure they all achieve the best performance. Otherwise, it is difficult to eliminate the significant impact of the experimental platform and parameter selection on the experimental conclusions. Therefore, the experimental conclusions presented in this paper are difficult to be convincing."
            },
            "questions": {
                "value": "1. Can the author provide details about the search space and make them public?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel architecture with Skip-MLP and Split-MLP components that effectively captures periodic and temporal relationships while maintaining computational efficiency. Through extensive evaluation of real-world datasets, this paper demonstrates that SSNet outperforms several models with fewer parameters, with even a single Skip-MLP unit achieving comparable performance to complex models like PatchTST."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. this paper studies a popular problem, i.e., time series forecasting.\n\n2. this paper revised MLP architectures and applied them for forecasting, which seems interesting."
            },
            "weaknesses": {
                "value": "1. This paper is not well-motivated. This paper mentioned that \"MLP-based networks offer better computational efficiency but struggle\nto effectively model periodic and temporal relationships, which are essential for accurate time series forecasting\". However, several models that utilized frequency-domain MLPs can effectively capture the frequency components, such as [1-2]. These methods can better capture the periodic and temporal patterns.\n\n2. The experimental results are less convincing. The experiments are lack of comparisons with other MLP time series forecasting models and SOTA transformer models [1-3]. Thus, the experimental results are not enough.\n\n\n[1] FITS: Modeling Time Series with 10k Parameters.\n\n[2] Frequency-domain MLPs are More Effective Learners in Time Series Forecasting.\n\n[3] iTransformer: Inverted Transformers Are Effective for Time Series Forecasting."
            },
            "questions": {
                "value": "Can the author revise or improve the motivations or better explain it?\n\nCan more experiments be added?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose a novel network structure, the Skip and Split MLP Network (SSNet), which integrates Skip-MLP and Split-MLP components. The SSNet model outperforms general MLP-based, CNN-based, and Transformer-based models in terms of parameter efficiency, computation time, and accuracy. The authors clearly explain the decomposition and prediction challenges in time series forecasting, providing detailed mathematical explanations for Skip-MLP and Split-MLP calculations.\n\nThe paper introduces the SS-MLP block, consisting of two Skip-MLP layers and two Split-MLP layers. In the SSNet section, they describe the structure of the Auto-correlation Block, which is based on periodicity and strength, and then present the overall SSNet architecture, including the Auto-correlation Block, Skip-MLPs, and SS-MLPs.\n\nThe SSNet model is tested on seven datasets and compared with other MLP-based methods (TimeMixer, DLinear), Transformer-based methods (PatchTST, PDF, FEDformer), and CNN-based methods (FiLM, TimesNet, MICN). SSNet consistently ranks as the best or second-best in terms of MSE and MAE. Additionally, the model demonstrates significantly lower running times and GPU/memory usage compared to other methods."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. The inclusion of clear visuals such as Figures 1, 3, 4, and 5 significantly aids in understanding the components of the model, including Skip-MLP, Split-MLP, and the overall structure of SSNet, SS-MLP Block, and the Auto-correlation Block.\n\n2. The concise mathematical derivation of Skip-MLP and Split-MLP helps readers grasp the underlying mathematical framework of the proposed model.\n\n3. The model was tested on multiple large-scale datasets and compared to several well-known deep learning models, offering substantial evidence of the model's strengths and performance.\n\n4. Sufficient data on computational time and GPU usage is provided, demonstrating that the model achieves improved accuracy without requiring additional time or resources, making it highly efficient."
            },
            "weaknesses": {
                "value": "1. Please explain the reasoning behind this specific number of layers and whether they experimented with different configurations\n\n2. Offer further explanations regarding the structure of Figure 5, provide a step-by-step explanation of the data flow through the SSNet architecture, including how the residual connections are incorporated.\n\n3. Describe the specific type of linear projection used (e.g., fully connected layer) and explain why this particular approach was chosen for the output layer.\n\n4. Provide a brief description of each dataset's domain and characteristics, and suggest including a sample visualization of the time series data from one or two representative datasets."
            },
            "questions": {
                "value": "I have just one question: Would changing the order of the Skip-MLP and Split-MLP in the SS-MLP block, or adding additional hidden layers, have any impact on the MSE or MAE?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 10
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}