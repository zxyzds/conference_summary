{
    "id": "k9GfyX1eqM",
    "title": "AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation",
    "abstract": "This paper studies the vulnerabilities of transformer-based Large Language Models (LLMs) to jailbreaking attacks, focusing specifically on the optimization-based Greedy Coordinate Gradient (GCG) strategy. \n  We first observe a positive correlation between the effectiveness of attacks and the internal behaviors of the models. For instance, attacks tend to be less effective when models pay more attention to system prompts designed to ensure LLM safety alignment. Building on this discovery, we introduce an enhanced method that manipulates models' attention scores to facilitate LLM jailbreaking, which we term AttnGCG. \n  Empirically, AttnGCG shows consistent improvements in attack efficacy across diverse LLMs, achieving an average increase of ~7\\% in the Llama-2 series and ~10\\% in the Gemma series. Our strategy also demonstrates robust attack transferability against both unseen harmful goals and black-box LLMs like GPT-3.5 and GPT-4. \n  Moreover, we note our attention-score visualization is more interpretable, allowing us to gain better insights into how our\n  targeted attention manipulation facilitates more effective jailbreaking.\n  We release the code at https://anonymous.4open.science/r/AttnGCG-5CD2/.",
    "keywords": [
        "AttnGCG",
        "Jailbreaking",
        "Attention Mechanism",
        "Optimization-based Attacks"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=k9GfyX1eqM",
    "pdf_link": "https://openreview.net/pdf?id=k9GfyX1eqM",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes enhancements to GCG jailbreak attacks. The central observation is that by increasing the model's attention to adversarial suffix, the effectiveness of the attack can be significantly improved."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper addresses a critical and relevant problem in the field of large language models (LLMs), emphasizing the necessity of studying jailbreak attacks.\n* The authors evaluate their approach on both open-source and closed-source models, providing a broader perspective on the attack's effectiveness.\n* The paper leverages the attention mechanism inherent in transformer architectures to design more effective attacks, showcasing a novel approach to enhancing jailbreak techniques."
            },
            "weaknesses": {
                "value": "1. The choice to target optimization-based jailbreak attacks raises questions, as this type often does not achieve higher success rates compared to alternatives (e.g., LLM-aided jailbreaking) and is more susceptible to defenses (e.g., simple perplexity analysis can defend against this type of attacks).\n2. The introduction references multiple figures, some of which appear on subsequent pages, disrupting the flow of reading and comprehension.\n3. The evaluation primarily compares against the GCG attack, which is relatively outdated. To substantiate the effectiveness of their approach, the authors should include comparisons with more recent state-of-the-art methods.\n4. The paper lacks a thorough analysis of existing literature that also utilizes the attention mechanism in transformer models for attack purposes. This oversight diminishes the contextual understanding of their contributions.\n5. The decision to use only 100 harmful questions from AdvBench, rather than the complete dataset of 520 prompts, raises questions about the representativeness and robustness of their findings.\n6. While the paper notes that outputs generated by GCG do not necessarily constitute valid jailbreak attacks\u2014even with a high probability of harmful tokens (lines 50-53)\u2014it fails to propose a solution to this issue. Furthermore, reliance on GPT-4 for output evaluation is not a novel contribution, as it is commonly used in other studies. Additionally, Table 1 indicates that jailbreak suffixes generated by AttnGCG also exhibit significant false positives in terms of jailbreak classification."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new strategy called AttnGCG, which builds on the existing Greedy Coordinate Gradient (GCG) method. It focuses on improving jailbreaking attacks on large language models (LLMs) by manipulating attention scores to optimize adversarial prompts. The study finds that increasing attention to adversarial suffixes can enhance attack success rates (ASR) and demonstrates that this approach outperforms traditional GCG in both direct and transfer attacks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The study utilizes the attention mechanism of LLMs to guide the optimization of jailbreak prompts, offering a new perspective on how adversarial inputs can bypass safety measures.\n\n2. The authors perform evaluations in both white-box and transfer attack settings, showcasing the robustness and transferability of AttnGCG.\n\n3. Attention heatmap visualizations enhance the interpretability of the attack process, providing insights into why specific jailbreak attempts succeed.\n\n4. The presentation is clear and well-structured, making it easy to follow and understand the main contributions of this work."
            },
            "weaknesses": {
                "value": "1. The improvements of AttnGCG over GCG are relatively limited: The experimental results indicate that the performance gains of AttnGCG over GCG are under 10% in most cases. \n\n2. Limitations of baseline comparisons: The experiments only compare AttnGCG with GCG. To strengthen the paper's credibility, it would be beneficial for the authors to include comparisons with other existing jailbreak attack methods, such as AutoDAN [1], COLD-Attack [2], AdvPrompter [3], and AmpleGCG [4]. This would help position AttnGCG's relative advantages and uniqueness within the current research landscape.\n\n3. Lack of evaluation under safety system prompts: The experiments focus solely on the attack efficacy without considering scenarios where safety system prompts are in use. To validate the real-world applicability of AttnGCG, testing its performance with active safety system prompts, especially in comparison with methods from [5,6,7], would help determine if it maintains or improves upon the advantages over GCG under enhanced security measures.\n\n4. Applicability to the latest models: Although the paper mentions that AttnGCG is less effective on the latest models (e.g., Gemini-1.5-Pro-latest and GPT-4o), there is no in-depth analysis or discussion provided. More research and data are needed to explore potential improvements or strategies to address these limitations, ensuring better applicability and robustness in future versions.\n\n[1] Liu X, Xu N, Chen M, et al. Autodan: Generating stealthy jailbreak prompts on aligned large language models[J]. arXiv preprint arXiv:2310.04451, 2023.\n\n[2] Guo X, Yu F, Zhang H, et al. Cold-attack: Jailbreaking llms with stealthiness and controllability[J]. arXiv preprint arXiv:2402.08679, 2024.\n\n[3] Paulus A, Zharmagambetov A, Guo C, et al. Advprompter: Fast adaptive adversarial prompting for llms[J]. arXiv preprint arXiv:2404.16873, 2024.\n\n[4] Liao Z, Sun H. Amplegcg: Learning a universal and transferable generative model of adversarial suffixes for jailbreaking both open and closed llms[J]. arXiv preprint arXiv:2404.07921, 2024.\n\n[5] Xie Y, Yi J, Shao J, et al. Defending chatgpt against jailbreak attack via self-reminders[J]. Nature Machine Intelligence, 2023, 5(12): 1486-1496.\n\n[6] Zhang Z, Yang J, Ke P, et al. Defending large language models against jailbreaking attacks through goal prioritization[J]. arXiv preprint arXiv:2311.09096, 2023.\n\n[7] Zhou Y, Han Y, Zhuang H, et al. Defending jailbreak prompts via in-context adversarial game[J]. arXiv preprint arXiv:2402.13148, 2024."
            },
            "questions": {
                "value": "The same as the weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper highlights the susceptibility of Large Language Models (LLMs) to adversarial attacks despite existing safety measures. Traditional strategies like Greedy Coordinate Gradient (GCG) often fail to effectively optimize the model's internal focus. The authors propose that AttnGCG manipulates attention scores to strengthen attack success, redirecting the model's focus from safety prompts to adversarial inputs. The paper conducts extensive empirical studies and justifies the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The AttnGCG forces LLM to pay more attention to the adversarial suffix, inducing more harmful contents from the model, which is novel and effective.\n- Extensive experiments have been conducted to provide a good insight into the components of the proposed method.\n- The paper is generally well-written, with clear illustrations and tables."
            },
            "weaknesses": {
                "value": "- The paper focuses on the development and success of the adversarial attack technique without sufficiently exploring how such attacks can be mitigated or defended against. This leaves a gap in the practical applicability of the research, as understanding potential defenses is crucial to improving model robustness in real-world scenarios.\n- As discussed in the limitation part, the AttenGCG failed to jailbreak the latest LLM like GPT-4o. The low transferability of GCG and AttenGCG shows the low risk of the reveal flaw.\n- The AttnGCG still relies on exclusive optimization on white-box LLM. Excessive resource requirements could also act as a defense method. It would be more attractive to manipulate the attention matrix directly to jailbreak the LLM."
            },
            "questions": {
                "value": "1. Discuss the effectiveness of the defense method for the AttenGCG, such as the perplexity defense [1] adopted by AutoDAN. \n2. Discuss the possibility of manipulating the attention matrix directly to jailbreak the LLM. \n\n[1] Jain, Neel, et al. \"Baseline defenses for adversarial attacks against aligned language models.\" arXiv preprint arXiv:2309.00614 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes an enhancement to the GCG algorithm by integrating attention scores to improve jailbreak attacks on LLMs. By incorporating attention mechanisms, the authors aim to identify and manipulate tokens that have a higher impact on the model's output, thereby enhancing the effectiveness of the attack.\n\nStrengths:\n1. Relevant Topic: The paper addresses the important area of robustness and adversarial attacks on LLMs, which is crucial for understanding and improving AI safety.\n2. Logical Extension: Integrating attention scores into the GCG algorithm is a reasonable approach, as attention mechanisms are central to how LLMs process information.\n\nWeaknesses:\n1. Limited Novelty: The primary contribution is the addition of attention scores to the existing GCG algorithm. This is an incremental improvement rather than a fundamentally new approach, since there are already paper that have suggested incorporating attentions cores to the loss function including https://arxiv.org/pdf/2001.06325, https://aclanthology.org/2023.findings-emnlp.716.pdf and more. The paper under review does not sufficiently differentiate its approach from these existing methods or provide substantial advancements over them. To be considered a significant contribution, the paper could:\nIntroduce a novel methodology for manipulating attention that leads to substantially improved attack success rates.\nProvide theoretical insights into the vulnerabilities of attention mechanisms in LLMs.\nOffer a comprehensive comparative analysis with existing attention-based attack methods to highlight unique benefits.\n\n2. Marginal Improvements: The empirical results show only slight enhancements in attack effectiveness compared to the original GCG algorithm. The performance gains may not be substantial enough to warrant publication. The empirical results indicate only slight enhancements over the original GCG algorithm. For instance, the attack success rate on average improved from 74.1% to 78.0%, a marginal increase that may not be statistically significant or practically impactful. In adversarial attack research, improvements are generally expected to be substantial (e.g., exceeding a 10% increase in success rate) to warrant publication. Additionally, the integration of attention scores adds computational complexity to the algorithm. This includes the overhead of computing attention weights and manipulating them during the attack process, which can increase the time and resources required. The paper should provide a detailed analysis of this trade-off, justifying whether the modest performance gains outweigh the additional complexity introduced.\n\n\n3. Unrealistic Experimental Settings: The experiments are conducted with a temperature setting of zero, leading to deterministic outputs from the LLM. In practical applications, models often operate with higher temperatures (e.g., between 0.7 and 1.0) to generate more diverse and creative responses. Using a temperature of zero may limit the generalizability of the results to real-world scenarios where outputs are stochastic.\n\nIt is recommended to conduct additional experiments with non-zero temperature settings to evaluate the robustness and effectiveness of the proposed attack under more realistic conditions. Analyzing whether the attention manipulation strategy remains effective when the model's outputs are less predictable would strengthen the paper. If there is theoretical justification for the method's performance at higher temperatures, including this analysis would be beneficial."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* Relevant Topic: The paper addresses the important area of robustness and adversarial attacks on LLMs, which is crucial for understanding and improving AI safety.\n* Logical Extension: Integrating attention scores into the GCG algorithm is a reasonable approach, as attention mechanisms are central to how LLMs process information."
            },
            "weaknesses": {
                "value": "* Limited Novelty: The primary contribution is the addition of attention scores to the existing GCG algorithm. This is an incremental improvement rather than a fundamentally new approach, since there are already paper that have suggested incorporating attentions cores to the loss function including https://arxiv.org/pdf/2001.06325, https://aclanthology.org/2023.findings-emnlp.716.pdf and more. The paper does not sufficiently demonstrate how this modification offers a significant advancement over existing methods.\n\n* Marginal Improvements: The empirical results show only slight enhancements in attack effectiveness compared to the original GCG algorithm. The performance gains may not be substantial enough to warrant publication, especially considering the added complexity.\n\n* Unrealistic Experimental Settings: The experiments are conducted with a temperature setting of zero, resulting in deterministic outputs from the LLM. In practical applications, models often operate with higher temperatures to allow for more diverse and creative responses. Using a temperature of zero may limit the generalizability of the results to real-world scenarios."
            },
            "questions": {
                "value": "* Assessment of Performance Improvements: The empirical results show only slight enhancements over the original GCG algorithm. Can you provide additional analysis to demonstrate the practical significance of these performance gains? How do you justify the added complexity of integrating attention scores given the marginal improvements observed?\n\n* Experimental Settings and Real-World Applicability: Since your experiments use a temperature setting of zero, which leads to deterministic outputs, how do you address the applicability of your findings to real-world scenarios where higher temperatures are common? Have you conducted experiments with non-zero temperature settings, and if so, could you share those results to illustrate the robustness of your method under more practical conditions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}