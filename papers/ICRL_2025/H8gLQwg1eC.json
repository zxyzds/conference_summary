{
    "id": "H8gLQwg1eC",
    "title": "Understanding Generalization of Preference Optimization Under Noisy Feedback",
    "abstract": "As large language models (LLMs) advance their capabilities, aligning these models with human preferences has become crucial. Preference optimization, which trains models to distinguish between preferred and non-preferred responses based on human feedback, has become a crucial component for aligning LLMs. However, most existing works assume noise-free feedback, which is unrealistic given the inherent errors and inconsistencies in human judgments. This paper addresses the impact of noisy feedback on preference optimization, providing generalization guarantees under these conditions. Unlike traditional analyses that assume convergence, our work focuses on finite-step preference optimization, offering new insights that are more aligned with practical LLM training. We establish generalization guarantees for noisy preference learning under a broad family of preference optimization losses such as DPO, IPO, SLiC, etc. Our analysis provides the basis for a general model that closely describes how the generalization decays with the noise rate. Empirical validation on contemporary LLMs confirms the practical relevance of our findings, offering valuable insights for developing AI systems that align with human preferences.",
    "keywords": [
        "Preference optimization",
        "noisy feedback"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=H8gLQwg1eC",
    "pdf_link": "https://openreview.net/pdf?id=H8gLQwg1eC",
    "comments": [
        {
            "summary": {
                "value": "This paper explores the generalization of preference optimization in large language models under noisy human feedback, providing theoretical guarantees and empirical validation in both synthetic and real-world settings."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper offers theoretical insights into noisy preference optimization, extends to a broad class of methods, and validates its findings through empirical analysis on controlled and real-world datasets."
            },
            "weaknesses": {
                "value": "1. Identical text to the previous work [1]\uff1a The text has significant overlap with prior work by Im and Li [1]. The authors should clarify how their contributions differ, especially when the preliminary section is almost identical to those in [1], and the equations are used verbatim.\n\n    [1] Shawn Im and Yixuan Li. On the generalization of preference learning with dpo\n\n2. The problem setting is artificial. The prompt embedding is assumed from a vMF distribution. In the extreme case ($\\gamma \\rightarrow +\\infty$), the problem becomes that there are only two possible prompts $\\mu^+$ and $\\mu^-$. And the preference learning is conducted only for the two prompts.\n\n3. The theoretical framework applies solely to single-token preference learning, which might limit its applicability, as LLM preference learning typically involves response sequences rather than isolated tokens.\n\n4. The setting is ambiguous and the mathematical treatment is poorly presented:\n   - There is no statement on the assumption of the ground-truth preference model. Is it assumed that noiseless human preference strictly obey an underlying reward model? If so it should be clearly stated. Otherwise, a circular preference relation $A \\succ B \\succ C \\succ A$ will guarantee the population risk can not reach $0$, while Theorem 3.1 here implies it can be achieved under any circumstances.\n   - The data generation procedure is poorly stated and artificial. Why there are two and only two clusters of prompt? And why must they be equally distributed. Additionally, given the sample from $\\mathcal{D}_+$ or $\\mathcal{D}_-$, how is the chosen and rejected token generated? What is their distribution and what preference model do they follow?"
            },
            "questions": {
                "value": "See Weaknesses.\n\n1. Can you clarify your choice of vMF distribution for prompt embedding and whether alternative distributions were considered for modeling prompt variance?\n2. Are there specific empirical results to further substantiate the claim that modern LLM embeddings indeed follow a vMF distribution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies preference optimization with noisy preference\nfeedback. Theoretically, the paper analyzes a fairly broad family of\nalgorithms under softmax-linear parametrization of the language model\npolicy. Experimentally, the paper mostly focuses on DPO loss both in\ncontrolled settings (but also considers some other losses) and on\nstandard RLHF benchmarks. The main technical tool is a notion of\nreward margin, essentially the implicit reward difference between\nprefered and dis-preferred responses and the main concept insight is\nthat stronger \"concentration\" (less variance of the data), more\nsamples, and increasing the \"angle\" between positive and negative\ndirections can improve performance.\n\nComments:\n\n1. I have a basic issue with the premise of this paper. The original\nderivation of DPO (and these related losses) _already_ incorporates\nnoise in the preference data. The data is assumed to be generated by\nthe Bradley-Terry model, in which preference pairs appear via\nP(y_1>y_2) \\propto \\sigma(r(y_1) - r(y_2)). This is already noisy! And\nthe BT model can nearly model the \"massart\" noise model considered\nhere, all \"true positives\" have reward r_1 and all true negatives have\nreward r_2 such that \\sigma(r_1 - r_2) = 1-eps. So why not just\nreframe the paper this way?\n\n2. The setup for the analysis is highly specialized and I feel this is\nrather limiting. Why is the vMF distribution natural or reasonable?\nWhy not use a more general noise model, like the original BT? Why this\nspecific policy parametrization? (I am ok with using softmax linear,\nbut I think this should be at the token level over the responses,\nwhich I don't think is what is happening in the paper)\n\n3. I am not particularly convinced by the controlled experiments. The\nsettings nearly perfectly matches the one in the theoretical\nanalysis. Thus the only conclusion one can make is, roughly, the\ntheorem is correct. But I do not know if the controlled experiments\nhave much bearing on practice.\n\n4. The real-world experiment is more interesting, but I am not sure if\nwe should view this as validation of the theory, because we do not see\nthe 1/(1-x)^2 type behavior very clearly. Instead we see linear\nbehavior, which could be presumably explained by many other models. As\njust a concrete example, in noisy classification (PAC learning with\nclassification noise) we expect that the error rate of ERM scales\nlinearly with the noise level, i.e., we expect err(ERM) \\leq OPT +\npoly(1/n) and OPT will incur error rate equal to the noise level. So\nthis is an equally valid, and much more general/simple/convincing,\nexplanation for the experimental results?  I get the stated reason for\nwhy we do not see the 1/(1-x)^2 behavior, but the point remains that\nno other competing hypothesese have not be convincingly ruled out.\n\n4. I looked at the prior work Im-Li-2024a and I noticed that\nnontrivial portions of the text seem to be copied verbatim. I am not\nsure if this is problematic or not...\n\nOverall: It's quite possible I am missing something and I'm open to\nhaving my mind changed about this. But, I feel the paper should just\nwork in the standard BT model, should consider a more general setup\nfor the analysis, and needs to have a more convincing experimental\nsection. As it stands, I feel the paper is below the bar for ICLR."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "see above"
            },
            "weaknesses": {
                "value": "see above"
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors studied the impact of noise in feedback on the alignment of large language models (LLMs) through preference optimization. In particular, they considered a fairly general alignment loss which includes DPO, IPO, and SLiC. Moreover, they assumed that the \"feature backbone\" $g(x)$ of the model (the term is not clearly defined in the paper but it seems that it refers to the last layer of the model) is fixed (which I think it means that weights of the model are frozen) and only the \"unembbeding matrix\" $W$ is learnable. They modeled the output of the model by softmax$(Wg(x))$ and analyzed the dynamic of gradient flow over $W$ under some assumptions on the distribution of the input (the von Mises-Fisher (vMF) distribution). The authors defined a notion of risk for preference learning (Definition 3.1) and they provided upper bounds on the expected population risk (Theorem 3.2) through the gradient flow analysis (under some assumption on the significance of noise (which is modeled by a Bernoulli RV with parameter $\\epsilon$) in the feedback and the run time $t$). Based on their analysis, they observed that the expected risk is bounded by $1/(1-c\\epsilon)^2$ when $\\epsilon$ is sufficiently below $1/2$ and then decreases linearly with $\\epsilon$ when $\\epsilon$ gets close to $1/2$. They verified their analysis on some synthetic data and also a real setting."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Originality: The authors studied the impact of noise in the feedback on the performance of preference optimization in LLMs. To me, the dependency of performance on the level of noise (which is characterized in this paper) is new in the field. \n\nQuality/Signgiance: The result is derived based on heavy assumptions on the training of LLM and the distribution of input. Therefore, it may not capture the exact dependency of the performance on $\\epsilon$.\n\nClarity: Some of the explanations are missing in the paper and assumptions are not mentioned in a separate item that we can refer to. Therefore, in this respect, I do not see any remarkable strength in the paper."
            },
            "weaknesses": {
                "value": "Presentation: The paper is somehow readable until the end of Section 2 but then it is hard to follow (sometimes the notations/terms are not clearly identified). Moreover, the main assumptions in the paper should be stated in an assumption item and then be referred to in the statements of theorems. Moreover, the abstract is somehow misleading as these assumptions are not mentioned there.\n\nJustifications for the assumptions: For some of the assumptions in the theorems, it is good to add some justifications on why these are reasonable assumptions. \n\nImplications of the analysis: It is good to mention the implication of this analysis in preference optimization (For instance, how this analysis can be helpful in designing a better alignment method). Moreover, it is hard to characterize the boundary between two regimes of $1/(1-c\\epsilon)^2$ and the linear one."
            },
            "questions": {
                "value": "1. Why is a binary loss suitable for defining risk in Definition 3.1? For instance, why not considering the reward margin itself as the risk?\n\n2. In line 208, the authors used the term \"feature backbone\" without explicitly defining it. As one of the key assumptions in the analysis is given here, I suggest clearly mentioning the assumption as a separate item and refer it in the statement of theorems. Otherwise, it is hard to know which assumptions are made in each theorem.\n\n3. I think it is good to talk a little bit about gradient flow why it is considered and the notations used there (such as the learning rate $\\tau$). The analysis is also limited to gradient-descent-like updates and may not capture the dynamics of other optimization algorithms (such as AdamW) which are often used in the alignment of LLMs.\n\n4. Please use an assumption item for the assumption on the input distribution. Moreover, I am not completely sure that the statement in line 258 has been observed in the literature. So, please give a citation for it. The experiments in Appendix C are limited to one dataset and it is not clear on what model has been tested. \n\n5. In the statement of Theorems 3.1 and 3.2, the run time $t$ should be bounded by a factor of learning $\\tau$. So, if I am not mistaken, it is limited to one step in the discretized version of the gradient descent algorithm.\n\n6. It is good to characterize the boundary between two regimes of $1/(1-c\\epsilon)^2$ and the linear one. \n\n7. In the experiments on real data, it is asserted that the linear dependency observed as the original data is very noisy in itself. I suggest annotating the dataset with a third-party model (such as chatGPT4) or using available datasets (such as Alpacafarm) and rerun the experiments with a data that is less noisy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the problem of preference optimization about the impact of noisy feedback. The key theoretical observation is that a formula of the risk upper bound that increases with extent of noise. When the noise rate approaches 1/2, the expected risk transitions grows at a linear rate. The theory also reveals that stronger concentration, more samples, and contrasting\ndirections for positive and negative samples yields tighter bounds and slower degradation in\naccuracy as the noise rate increases. Empirical evaluations are  done to support the theoretical findings."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.The studied policy optimization class is general, including many commonly used algorithms.\n\n2.The proposed generalization guarantees can reflect the empirical observation, as verified in the experiment part.\n\n3.The analysis of this problem is very novel."
            },
            "weaknesses": {
                "value": "1.It is unclear why the result of Lemma 3.1 is important. In my understanding, it is very technical and serves for the proof of the main results. The writing lacks intuition and presenting the proof is not helpful for understanding.\n\n2.Theorems 3.1 and 3.2 hold only when $t$ is not very large. What will happen after that? When $t$ is close to 0, it means that even without any training, but the risk has almost the same guarantee. Does that make sense?\n\n3.The presentation of formulas should be improved. The $f_\\theta$ in Line 207 is different from the $f$ defined in equation (9)? What role does it play in this setting? What are the \"one hot vectors of the token\" defined in Line 230? What\u2019s the dimension of this vector? And how do you get equation (11)? In Theorem 3.1, what are $\\tau$ and $D$?\n\n4.I don\u2019t quite get the message in the experiment part. In Section 4.1, the theoretical fits seem like linear? But the theoretical result is not? How do you choose $c$ in the graph? It is very strange to claim the approximation is good without specifying the parameters. Moreover, It says the approximation is good for $\\epsilon < 0.5$, but the experiment is only done for $\\epsilon < 0.4$? In Section 4.2, still I don\u2019t get how the empirical line is drawn, and why the approximation can reflect anything. Why can\u2019t we just treat the trend as linear and use a linear line to approximate it instead of the proved $1/(1-c\\epsilon)^2$?\n\nIn general, although there are many parts to be clarified, I can almost understand the theoretical results. However, the experiment part should be further explained and the paper should be further polished to convey the message clearly."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}