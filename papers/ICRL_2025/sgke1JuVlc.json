{
    "id": "sgke1JuVlc",
    "title": "Temporal Misinformation and Conversion through  Probabilistic Spiking Neurons",
    "abstract": "In the age of large neural network models and their high energy demand, Spiking Neural Networks (SNNs) offer a compelling alternative to Artificial Neural Networks (ANNs) due to their energy efficiency and resemblance to biological brains. However, directly training SNNs with spatio-temporal backpropagation remains challenging due to their discrete signal processing and temporal dynamics. Alternative methods, notably ANN-SNN conversion, have enabled SNNs to achieve performance in various machine learning tasks, comparable to ANNs, but often to the expense of long latency needed to achieve such performance, especially on large scale complex datasets. The present work deals with ANN-SNN setting and identifies a new phenomenon we term ``temporal misinformation'', where random spike rearrangement through time in the converted SNN model improves its performance. To account for this, we propose bio-plausible, two-phase probabilistic (TPP) spiking neurons to be used in ANN-SNN conversion. We showcase the benefits of our proposed methods both theoretically and empirically through extensive experiments on CIFAR-10/100 and a large-scale dataset ImageNet over a variety of architectures, reaching SOTA performance. Code is available on GitHub.",
    "keywords": [
        "spiking neural networks",
        "probabilistic spiking",
        "ANN-SNN conversion"
    ],
    "primary_area": "learning theory",
    "TLDR": "A novel method for ANN-SNN conversion through probabilistic spiking neurons",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=sgke1JuVlc",
    "pdf_link": "https://openreview.net/pdf?id=sgke1JuVlc",
    "comments": [
        {
            "summary": {
                "value": "This paper primarily conducts further research on the ANN-SNN conversion methods. In particular, the authors have discovered a new phenomenon termed \"temporal error information\" and proposed biologically plausible two-phase probabilistic (TPP) spiking neurons for ANN-SNN conversion. The experimental results demonstrate the advantages of this method.\n\nI believe that the method proposed by the authors is effective; however, I have some questions that need clarification, mainly regarding the explanation and interpretation of the phenomenon. At this stage, my rating is \"6: Marginally above acceptance threshold.\" If the authors could address and clarify these questions, I would be very willing to raise my score."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The method is simple and effective, supported by comprehensive experimental validation. The proposed approach achieves state-of-the-art (SOTA) accuracy on the CIFAR-10/100 and ImageNet datasets, advancing the further development of ANN-SNN conversion.\n\n2. Unlike SNN research that tends to focus on computational efficiency, the two-phase mechanism and probabilistic spike discharge proposed in this work are both biologically plausible and have similar implementations in certain neuromorphic hardware."
            },
            "weaknesses": {
                "value": "1. The description in lines 94-107 is difficult to understand. Table 1 indeed shows that the \"permuted\" model performs better, but how does this relate to the previously acknowledged yet erroneous assumption that \"the precise timing of the spikes should not affect the performance of the SNN\"? There is no explanation of what the \"permuted\" operation is or how it relates to \"the precise timing of the spikes.\" Additionally, why is the phenomenon named \"temporal error information\"? Where does the error manifest? Section 3.1 addresses these questions, but lines 94-107 should also provide some insight for readers encountering this for the first time.\n\n2. I find the experimental work convincing; however, the description of the phenomenon of \"temporal error information\" needs further clarification. The current description leaves me unclear about what constitutes \"temporal error information\" in the original spike sequence. Which time steps contain erroneous spikes? What portion of the temporal errors is addressed by the \"permuted\" operation, and how does this improve the accuracy of the conversion?"
            },
            "questions": {
                "value": "1. I am unclear about the details of the \"permuted\" operation. Figure 2(a) mentions that \"the second Spiking phase outputs the same spike trains, but permuted.\" Since the output spike sequences are the same, where does the permutation manifest? Is it the firing times that have been rearranged?\n\n2. In ANN-SNN conversion, even if the spike firing rates match the ANN activation values, the conversion is not lossless. Some works have discussed this, noting that in early time steps, there can be spikes that should not have been fired, resulting in erroneous spikes [1]. Is this related to the \"temporal misinformation\" mentioned in the paper? Furthermore, uneven errors also point to this issue [2]. What is the relationship between \"temporal misinformation\" and uneven errors? Could the authors provide some discussion on this aspect?\n\n[1] X. He, Y. Li, D. Zhao, Q. Kong, and Y. Zeng, \u201cMsat: biologically inspired multistage adaptive threshold for conversion of spiking neural networks,\u201d Neural Computing and Applications, pp. 1\u201317, 2024.\n\n[2] Tong Bu, Wei Fang, Jianhao Ding, PengLin Dai, Zhaofei Y u, and Tiejun Huang. Optimal ANN-SNN\nconversion for high-accuracy and ultra-low-latency spiking neural networks. In International\nConference on Learning Representations, 2022c."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the concept of \"temporal misinformation\" in the ANN-to-SNN conversion process. It employs two-phase probabilistic (TPP) spiking neurons as the neurons in the SNN. The converted SNN model improves performance by using probabilistic neurons that fires spikes randomly."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The method is relatively easy to understand.\n2. The problem is relevant to the scope of ICLR.\n3. 3. The paper identifies an interesting phenomenon in the ANN-SNN conversion process, which could serve as a complementary insight."
            },
            "weaknesses": {
                "value": "1. This work primarily proposes a spiking neuron model rather than a novel conversion paradigm, which limits its originality, as similar work on probabilistic neurons has been done previously.\n2. The authors\u2019 definition of \"temporal misinformation\" is unclear. While the paper introduces the \"temporal misinformation\" phenomenon and experimentally demonstrates its impact on SNN performance, it lacks a sufficient theoretical basis to explain this phenomenon and does not provide quantifiable error analysis.\n3. The authors use \"permutation\" to introduce two-phase probabilistic spiking neurons (TPP). Theorem 1 suggests that the spikes emitted by probabilistic spiking neurons achieve an optimal spike firing order; however, this proof lacks persuasiveness."
            },
            "questions": {
                "value": "1.  For T time steps, there are T! possible permutations. Does every permutation yield better results than the original, non-permuted approach?\n2. Please clarify what the additional c steps in Tables 1 and 2 represent. \n3. In Tables 4 and 5, when comparing with SNNC, data suggests that at short time steps such as T=4 and T=8, \"Permute\" seems more effective than TPP. I would like to see if a similar phenomenon occurs with QCFS. Please provide additional comparative experiments between \"Permute\" and TPP on QCFS across CIFAR-10 and CIFAR-100 datasets. \n4.  Does the probabilistic neuron model introduce extra overhead for hardware implementation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper analyzes the assumption in ANN-SNN conversion that information transfer relies solely on spike firing frequency. By permuting spike sequences, the paper observes the issue of \u201ctemporal misinformation\u201d within the temporal domain. Further, it proposes an accumulation -> firing approach to avoid temporal misinformation, establishing an efficient ANN-SNN training method. The method is validated on datasets such as CIFAR and ImageNet, demonstrating advantages in model performance and other aspects."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper presents a compelling perspective: there exists a phenomenon of \u201ctemporal misinformation\u201d in the ANN-SNN conversion method."
            },
            "weaknesses": {
                "value": "The novelty. The Ideas in this paper are quite similar to those in at least two other papers.\n\nPlease See the Question Part."
            },
            "questions": {
                "value": "1.Prior to this paper, many works have discussed the equivalence between spiking neurons with temporal information and neurons with quantized activation values. This includes papers in the ANN-SNN domain [A], as well as works in the direct training domain [B]. Could you highlight the key innovations in your paper?\n\n2.I noticed that you cited paper [A], pointing out precision issues in its conversion process. In fact, your methods appear quite similar to theirs; could you provide a comparison between your approach and theirs?\n\n3.Could your method be applied to direct training, as in [B]? Would you be able to discuss the similarities, differences, and advantages or disadvantages of your method when used in direct training compared to ANN-SNN conversion?\n\n[A]Hu, Y., Zheng, Q., Jiang, X., & Pan, G. (2023). Fast-SNN: fast spiking neural network by converting quantized ANN. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n[B]Luo, Xinhao, et al. \"Integer-Valued Training and Spike-Driven Inference Spiking Neural Network for High-performance and Energy-efficient Object Detection.\" arXiv preprint arXiv:2407.20708 (2024)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper identified a new phenomenon in SNNs, termed \u201ctemporal misinformation\u201d, and proposed a solution. Results are validated on CIFAR-10, and CIFAR-100, and ImageNet"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The overall problem identification in the introduction section sounds interesting. However, the writing of the paper is very poor and it is very hard to follow the paper to understand the motivation, method, and detailed strengths of the method. Also, I cannot find detailed descriptions of Figure 1, which prevents me from understanding the main motivation of this work. \n\nBasically, I lost at the very beginning phase when I read this paper, so have not been able to identify the strengths.\n\nI encourage the author to make major changes to the clearness of the paper and writing before any potential resubmission, so readers can understand the paper better."
            },
            "weaknesses": {
                "value": "1. Line 11. \"age of large neural network models\", what \"age of large neural network models\" means? Is this a scientific term that deep learning researchers usually use? When did this age start? Which paper used the term \"large neural network models\" before?\n\n2. I searched for the \"Fig\" but did not find any results that refer to Figure 1 explains Figure 1. Could you make it clearer to readers where the explanation in Figure 1 is located in your paper? It is NOT friendly for reviewers as you want to use Figure 1 to attract attention but then when a reviewer tries to enjoy the detailed explanation of this figure, you just hide explanations. \n\n3. Line 158 \"\\textbf{ANN-SNN} conversion\" and Line 188 \"\\textbf{Direct training} This method a\". Why \"conversion \"is not capitalized in Line 158, but \"This\" is capitalized in Line 188? What does \"ANN-SNN\" mean? Is this a term you created? If not, please cite papers that used \"ANN-SNN\" before, or explain this new term clearly.\n\n4. Line 25. \"Code is available on GitHub.\". What do you mean by \"Code is available on GitHub.\"? There is not any Github link in the paper. Please do not mention something that does not exist in the abstract."
            },
            "questions": {
                "value": "See Weakness for detailed questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}