{
    "id": "HuL2yba6Uf",
    "title": "Unpicking Data at the Seams: VAEs, Disentanglement and Independent Components",
    "abstract": "Disentanglement, or identifying statistically independent salient factors of the data, is of interest in many aspects of machine learning and statistics, having potential to improve generation of synthetic data with controlled properties, robust classification of features, parsimonious encoding, and greater understanding of the generative process behind the data. Disentanglement arises in various generative paradigms, including Variational Autoencoders (VAEs), GANs and diffusion models, and particular progress has recently been made in understanding the former. That line of research shows that the choice of diagonal posterior covariance matrices in a VAE promotes mutual orthogonality between columns of the decoder's Jacobian. We continue this thread to show how such *linear* independence translates to *statistical* independence, completing the chain in understanding how the VAE objective leads to the identification of independent components of the data, i.e. disentanglement.",
    "keywords": [
        "VAE",
        "variational autoencoder",
        "beta-VAE",
        "disentanglement"
    ],
    "primary_area": "generative models",
    "TLDR": "Disentanglement in VAEs is close to being understood in that the training objective is shown to promote orthogonality in the decoder's Jacobian. We show how that then relates to identifying statistically independent factors of the data.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=HuL2yba6Uf",
    "pdf_link": "https://openreview.net/pdf?id=HuL2yba6Uf",
    "comments": [
        {
            "summary": {
                "value": "This paper aims to make progress on understanding why VAEs are able to learn disentangled representations. To this end, the authors connect orthogonality of the decoder Jacobian to disentanglement by showing that the VAE learns statistically independent sub-manifolds in the observed space. Further, the paper provides additional insights into the VAE objective such as on the role of beta in a beta-VAE."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "* The paper addresses an important problem. Namely, developing a principled understanding of how and why disentanglement is possible in deep generative models.\n\n* The paper is well written and the presentation is well structured.\n\n* The theoretical results and insights are presented in an intuitive and accessible way, with nice visual intuition from the figures."
            },
            "weaknesses": {
                "value": "My main issue with this paper is that I do not think the contributions presented in this paper offer sufficient novelty relative to prior work to merit acceptance, and, moreover, I do not think the authors adequately compare their contribution to prior work. I discuss these points in detail below with specific examples.\n\n**Identifiability of Latent Factors in VAEs**\n\nThe authors build on several prior works which try to explain why VAEs are able to identify the ground-truth latent factors by showing that the VAE objective promotes the decoder to have an orthogonal Jacobian. The authors perform a similar analysis and present a result claiming to be an identifiability result for VAEs.\n\nUnderstanding the identifiability of VAEs, however, has been analyzed in detail in prior works. Specifically, the work of [1], rigorously showed that the VAE objective with vanishing decoder variance is equivalent to maximum likelihood under an independent Gaussian prior plus a regularization term enforcing that the Jacobian has orthogonal columns. The identifiability of such models with independent latents and orthogonal Jacobians was studied in depth by [2], who proved a certain form of identifiability for such models and showed the theoretical challenges in recovering a complete identifiability result.\n\nThe authors do not mention the work of [2] and wrt [1], they state that their analysis differs because [1] assumes statistically dependent factors such that there result is novel relative to this work. This statement is wrong. The work in [1] does not assume statistically dependent factors. To this end, I think the works of [1] and [2] conduct a more rigorous and comprehensive analysis of the VAE objective and its identifiability than the current work, such that I do not feel this works adds sufficient novelty.\n\nAdditionally, the analysis of the VAE objective and the identifiability analysis conducted in this work are not as rigorous as these prior works in [1, 2], and are closer to the results in [3, 4]. From this standpoint, however, I am also not sure what the novelty is of the authors SVD based identifiability argument, as similar ideas were presented in [3] as I understand. Perhaps, the authors can clarify this as well.\n\n**Understanding the Role of Beta in Beta-VAEs**\n\nAnother stated contribution of this work is the authors' analysis of the role of beta in a beta-VAE (Section 3.3). The authors state that beta can be interpreted \"as scaling the variance of the likelihood distribution,\". From what I can tell, this result also seems very similar to the result in [1] presented in Appendix A.3 on the role of beta. I am curious if the authors can comment on the novelty of their result relative to this prior work.\n\n\n**Theorem Statements**\n\nAs an additional point, I did not see proofs for Theorems 1 and 3, and am thus curious if I am missing something or if there are proofs for these results.\n\n**Bibliography**\n\n1. Embrace the Gap: VAEs Perform Independent Mechanism Analysis\n (https://arxiv.org/abs/2206.02416)\n\n2. Function Classes for Identifiable Nonlinear Independent Component Analysis\n(https://arxiv.org/abs/2208.06406)\n\n3. Variational Autoencoders Pursue PCA Directions (by Accident)\n(https://arxiv.org/abs/1812.06775)\n\n4. On Implicit Regularization in \u03b2-VAEs (https://arxiv.org/abs/2002.00041)"
            },
            "questions": {
                "value": "**1.** Can the authors comment on the novelty of their results relative to the prior results discussed above?\n\n**2.** Where are the proofs for Theorems 1 and 3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper provides a theoretical understanding of disentanglement within Variational Autoencoders when the columns of the jacobian of the decoder are orthogonal (theorem 2). \n\nStarting from this hypothesis, namely the linear independence of the columns of the decoder Jacobian, the author shows that it is possible to link statistically independent components of the output distribution to independent latent factors of variation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The authors have clearly defined the problem and the goal of the paper;\n\nThe analysis extends existing work, offering theoretical proof of why an architectural constraint such as diagonal posterior covariance induces an effective disentanglement in the VAE latent space."
            },
            "weaknesses": {
                "value": "The paper lacks an experimental part. \nEven if the contribution is theoretical, a small experiment on a synthetic dataset corroborates the claims and strengthens the contribution.\n\nSplitting the deductive reasoning between ll 210 to 259 in paragraphs highlighting the key logical steps can also be helpful for the reader.\n\nAdditional figures or clarification of the only existing one (Fig 1) can help the reader understand the sequence of logical steps more easily."
            },
            "questions": {
                "value": "How sensitive is the disentanglement effect to minor violations of the assumptions (orthogonality of the decoder Jacobian)?\n\nin 059: do you mean encoder or decoder Jacobian?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper connects orthogonality of the decoder Jacobian in VAEs to disentangling statistically independent latent factors."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper is\n- generally well-written,\n- the figures are high quality,\n- the structure is logical and easy to follow."
            },
            "weaknesses": {
                "value": "Despite the neat structure and the obvious effort the authors put into this work, **I honestly struggled to discern what the added value of the contribution of the paper is, compared to the literature** (while knowing the field and having worked on related projects). I will try to make the points that confused me clear below. Please let me know if my understanding is incorrect.\n\n## Major issues\n- The added value of the contribution is unclear to me. A potential statement for this could be the _wrong_ statement, made in L468: _\"Reizinger et al. (2022) relate the VAE objective to independent causal mechanisms (Gresele et al., 2021) which consider non-statistically independent sources that contribute to a mixing function by orthogonal columns of the Jacobian. This clearly relates to the orthogonal Jacobian bias of VAEs, but differs to our approach that identifies statistically independent components/sources\"_. Namely, **Reizinger et al. (2022) assume a standard VAE with statistically independent sources.** It seems to me that the authors themselves differentiate their work from Reizinger et al. (2022) only by the presumed (but not true) dependent/independent sources divide. As this is not the case (unless my understanding is incorrect), I cannot see the added value of the paper. This holds for the theorems, and also for the elucidation of the role of $\\beta,$ as Reizinger et al. (2022) also relate it to the decoder variance in their Appx. A.3.\n- The paper is a bit too heavy on notation in the main text. While I understand that the proofs require precision, communicating (the intuition behind) the results do not. Please rework the main text to include only the necessary notation. Also, please always explain what your notation means (e.g., the caption of Figure 1 is not self-contained)\n\n## Minor points\n- L167: expand of what you are trying to do with the two losses\n- Eq (9): define $s_i$\n- L248: why is the image manifold parallel to the $U$-basis?\n- Eq (12): it should include the log absolute determinant of teh Jacobian, and not $W$ as this is about the nonlinear case, right?"
            },
            "questions": {
                "value": "- What do the two types of bold D's in Eq. (6) denote?\n- L156-160: isn't statistical independence of latent factors assumed in VAEs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}