{
    "id": "UTnq6hJJYa",
    "title": "Harnessing Shallow Features in Pre-Trained Models for Out-of-Distribution Detection",
    "abstract": "Recognizing out-of-distribution (OOD) samples is essential for deploying robust machine learning systems in the open-world environments. Conventional OOD detection approaches rely on feature representations from the final layer of neuron networks, often neglecting the rich information encapsulated in shallow layers. Leveraging the strengths of transformer-based architectures, we introduce an attention-based fusion module, which dynamically assigns importance weights to representations learned by each Transformer layer and detects OOD samples using the Mahalanobis distance. Compared to existing approaches, our method enables a lightweight fine-tuning of pre-trained models, and retains all feature representations that are beneficial to the OOD detection. We also thoroughly study various parameter-efficient fine-tuning strategies. Our experiments show the benefit of using shallow features, and demonstrate the influence of different Transformer layers. We fine-tune pre-trained models in both class-balanced and long-tailed in-distribution classification tasks, and show that our method achieves state-of-the-art OOD detection performance averaged across nine OOD datasets. The source code is provided in the supplementary material.",
    "keywords": [
        "out-of-distribution detection",
        "long-tail learning"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=UTnq6hJJYa",
    "pdf_link": "https://openreview.net/pdf?id=UTnq6hJJYa",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a weighted feature fusion method from the top to bottom layers of a network for OOD detection. It argues that, instead of relying solely on the features of the penultimate layer in a deep network for OOD detection, incorporating features from shallow layers provides diversity in feature spaces, aiding in the separation of OOD examples from ID examples. The method uses the total variance of features across different neural network layers to estimate feature weights for OOD detection. Experiments are conducted on CIFAR-100 and Imagenet, two popular benchmarks for OOD detection, including a setup with a long-tail distribution in in-domain training examples. The experimental results show that the proposed method outperforms the compared method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow.\n\nExtensive experiments are presented to support the claims.\n\nThere are sufficient ablation studies."
            },
            "weaknesses": {
                "value": "I find the novelty of the method modest. There is a remark section in the paper highlighting that the idea of estimating layerwise importance is the contribution compared to the inspiring methods, Mahalanobis and TRUSTED. While I can see a performance comparison with Mahalanobis in the paper, I could not find one for TRUSTED. Moreover, based on Figures 4 and 5 from the ablation studies, it hardly supports any contribution of layerwise feature weighting in OOD detection. The last layer has significant weight, while the other layers have more or less similar weights. A baseline with uniform weighting across all feature layers is also important.\n\nAlthough not directly related, a paper on the variance of gradients for estimating the difficulty of images for OOD detection [ref1] is relevant and would support the argument.\n\n[ref1] Agarwal, C., D'souza, D., & Hooker, S. (2022). Estimating example difficulty using variance of gradients. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10368-10378)."
            },
            "questions": {
                "value": "I am not fully convinced that there is a significant role of layerwise weightage in OOD detection. Giving more weights for the penultimate layer and the same weight to other layers can still perform well.    A clarification with experiments may change my rating."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper works on out-of-distribution detection (OOD) questions and proposes to use the rich information contained in the shallow features to improve the detection of the OOD samples.  Experiments on both class-balanced and long-tailed datasets show the effectiveness of using such shallow features and the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The writing is good and the idea is easy to follow.\n2. The observation of existing methods neglecting the shallow features makes sense to me and Fig.1 well illustrates the comparison of using the fused features from all layers than only using the last layer. \n3. The achieved results are promising showing the proposed method outperforms other competitors."
            },
            "weaknesses": {
                "value": "1.  The contribution of using all features for OOD looks over-claimed given the fact that there are already two related works (Mahalanobis and TRUSTED) explored that for OOD.  Emphasis on the novel multiple-feature fuse method might be more suitable.  \n2. The novelty part is not that new to me. Though the adaptive fuse for multiple features is not explored for OOD, what I understand of the key idea is to improve the discriminates of different image features, from this perspective,  the idea of using multilayer features simply or adaptively is quite explored as in [ref1-4] for example. \n3.  The proposed fusion method is not comprehensively studied. For example, what about the comparison results with some simple fuse method e.g., average or fixed ratios, and other proposed fusion methods e.g. [ref1-4]?\n4. Open questions about the task motivation:  is it definitely better to detect all out-of-distribution data for the model training, or will the OOD data help with the model's generalization ability for the cross-domain testing which we can't ensure is always in the same domain with the training.  \n5. Minor: the bold font is recommended to apply for tables to make it easier to read the results. \n\n[ref1] Attentional Feature Fusion\n[ref2] Enhanced Blind Face Restoration with Multi-Exemplar Images and Adaptive Spatial Feature Fusion\n[ref3] Shallow Feature Matters for Weakly Supervised Object Localization\n[ref4] Multilayer Feature Fusion Network for Scene Classification in Remote Sensing"
            },
            "questions": {
                "value": "please see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "With the favor of transformer architecture,  this work proposes an out-of-distribution (OOD) score based on the fused feature entailed from each layer. Further, it is empirically observed that parameter-efficient fine-tuning (PEFT) is more robust than fully fine-tuning when using different learning rates. Finally, it demonstrates the effectiveness of the proposed OOD score along with PEFT on the OOD benchmark and also long-tailed OOD benchmark."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed OOD score based on the fused feature from each layer is novel. \n- This work aims to tackle the OOD detection on both class-balanced benchmark and long-tailed benchmark."
            },
            "weaknesses": {
                "value": "-  The title appears somewhat overstated, as the proposed method specifically requires transformer architectures. A more precise title, such as \"... in pre-trained transformers for ...\", would better reflect the scope of the method.\n- Line 103-104: The mention of \"measuring the degree of neural collapse of each layer\" is unclear. Could the authors elaborate on this point and clarify how it relates to determining the importance weights for each layer.\n- The proposed OOD score aligns with post-hoc OOD scores that require access to training data. To assess its effectiveness, the authors should consider evaluating it on a standard OOD benchmark, as outlined in [2].  Comparisons with other training-data-dependent OOD scores\u2014such as KL matching, Residual, and ViM\u2014would provide a more comprehensive evaluation (see Table 1 in [1] for reference).\n- It is unclear why the authors chose to fine-tune a network pre-trained on ImageNet-21k. Why not instead use models pre-trained directly on ImageNet-1k or CIFAR-100 and then perform OOD detection on a standard benchmark[2]?\n- Dependency on Fine-tuning: Could the authors discuss whether the proposed OOD score's effectiveness relies on the pre-trained models with extensive datasets? For instance, the selected models in this work is pre-trained ViT models on ImageNet-21k and  CLIP-ViT-B/16. Have you tested or considered testing with smaller pre-trained models or datasets to assess its effectiveness across different scales of pre-training.\n- Comparison with MCM (Line 394): The comparison with MCM, in which the authors claim that SFM outperforms MCM by ~2%, may not be fair. The original MCM model was not fine-tuned on ImageNet-1k, which could impact the validity of this comparison.\n\n\n\n\n\n\n         \n\n\n\n\nReference\n- [1] GEN: Pushing the Limits of Softmax-Based Out-of-Distribution Detection. In CVPR, 2023.\n- [2] OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection.  In  NeurIPS 2023 Workshop on Distribution Shifts."
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}