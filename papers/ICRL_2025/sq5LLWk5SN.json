{
    "id": "sq5LLWk5SN",
    "title": "Mitigating Robust Overfitting in Wasserstein Distributionally Robust Optimization",
    "abstract": "Wasserstein distributionally robust optimization (WDRO) optimizes against worst-case distributional shifts within a specified uncertainty set, leading to enhanced generalization on unseen adversarial examples, compared to standard adversarial training which focuses on pointwise adversarial perturbations. However, WDRO still suffers fundamentally from the robust overfitting problem, as it does not consider statistical error. We address this gap by proposing a novel robust optimization framework under a new uncertainty set for both adversarial noise (Wasserstein distance) and statistical error (Kullback-Leibler divergence). Our theoretical analysis establishes that out-of-distribution adversarial performance is at least as good as the in-distribution robust performance with high probability. Furthermore, we derive conditions under which Stackelberg and Nash equilibria exist between the learner and the adversary. Finally, through extensive experiments, we demonstrate that our method significantly mitigates robust overfitting and enhances robustness within the framework of WDRO.",
    "keywords": [
        "Adversarial examples; robust overfitting; WDRO"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=sq5LLWk5SN",
    "pdf_link": "https://openreview.net/pdf?id=sq5LLWk5SN",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a new method for distributional robust optimization by considering a relaxation of statistical error in the distribution sets"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper proposes a new method for distributional robust optimization"
            },
            "weaknesses": {
                "value": "see the questions below"
            },
            "questions": {
                "value": "The paper is overall well written. I just have a couple of technical questions:\n1. The result in Theorem 3 implicitly assumes that $e^{-\\gamma\\cdot n} (4/\\delta)^m<1$, which is equivalently requiring $\\gamma\\ge\\frac{m\\log(4/\\delta)}{n}$. As $m$ is the covering number, typically in the order of exp(d) in a d-dimensional space, this implies that $\\gamma\\ge \\exp(d)/n$. Is this a too strict assumption?\n2. It would be helpful to show in theory that the standard method without considering the statistical error relaxation fails, while the proposed method succeeds. \n3. Is there a formal theory for the output of Algorithm 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a novel approach called SR-WDRO to address robust overfitting in Wasserstein Distributionally Robust Optimization (WDRO) by incorporating Kullback-Leibler (KL) divergence. While the theoretical contributions are interesting, there are several concerns regarding clarity and experimental validation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Theory:**\n\nThe mathematical framework is well-developed with thorough theoretical analysis. This paper provides two sets of theoretical results.\n\nThe first one is the generalization/robustness bound. The main idea is to show a high probability bound on D \u2208 U(D_n) when the uncertainty set is defined with both two divergences. The second one is to establish the Stackelberg and Nash equilibria.\n\n**Experiments:**\n\nThe proposed method demonstrates some improvement in mitigating robust overfitting compared to UDR and HR."
            },
            "weaknesses": {
                "value": "**Clarity Issue:**\n\nThe main idea of the paper is summarized in Line 169: \"To mitigate this issue, we incorporate the Kullback-Leibler (KL) divergence in WDRO, specifically aiming to reduce statistical error caused by training on finite samples.\" However, this sentence lacks clarity in three critical aspects:\n\n1. What is the definition of statistical error?\n\nThe term \"statistical error\" appears multiple times starting from the abstract. However, while \"statistical\" and \"error\" are very general terms, it is hard to understand what this refers to exactly in the mathematical framework.\n\n2. Why is statistical error caused by training on finite samples?\n\nWithout a clear definition of statistical error, it is difficult to understand or verify this claim.\n\n3. Why does incorporating the Kullback-Leibler (KL) divergence in WDRO mitigate this issue?\n\nIn the rest of the paper, neither the theoretical results, including the bounds and Nash equilibrium, nor the experimental results provide a clear answer to this question.\n\n**Major Theoretical Concern:**\n\nSections 3 and 4 demonstrate that SR-WDRO possesses good bounds and equilibria properties. However, the paper does not discuss whether WDRO has or lacks these properties. Without this comparison, it is difficult to verify the necessity of introducing the SR- prefix.\n\nFor example, in Theorem 3, one could perform a simple sanity check: \n\n>By letting \u03b3=0 (which reduces SR-WDRO to WDRO), the generalization bound reduces to $P()\\geq0$, which is a meaningless trivial bound. This suggests SR-WDRO has a better generalization bound than WDRO. \n\nHowever, the authors should provide a deeper analysis than my simple observation.\n\nThe same question applies to Section 4 - do WDRO admit Stackelberg and Nash equilibria under similar assumptions? Without addressing these comparative aspects, the theoretical advantages of SR-WDRO over WDRO remain unclear.\n\n**Major Experiments Concern:**\n\n1. Regarding robust overfitting, while SR-WDRO outperforms UDR and HR, Figure 2 still shows a decreasing phase. This raises concerns about whether WDRO-type methods are truly necessary in adversarial training settings, especially considering that simpler approaches like SWA or EMA could mitigate the decreasing phase with better performance.\n\n2. The comparison of the WDRO-type approaches with other types of adversarial training methods is not provided. As far as I know, it is not competitive with other methods under similar setting, such as no additional data, no generative data, and on ResNet-18.\n\n3. The exclusive use of ResNet-18 is limiting, as the adversarial training community typically requires evaluation on larger models.\n\nTo summarize, while SR-WDRO appears to achieve the ceiling performance among WDRO-type algorithms, this result actually raises pessimism about applying WDRO to adversarial training. Despite WDRO's importance in other operations research problems, its effectiveness in adversarial training remains questionable."
            },
            "questions": {
                "value": "**Minor:**\n \n1.\tEq. (1) and Line 38: Eq.(1) is defined directly in Wasserstein distance rather than the uncertain set. So the description of U(D_n) in line 38 is not self-consistent.\n\n2.\tThe range of gamma in Theorem 3 is not stated.\n\n3.\tTheorem 3, line 204: internal covering number of Z. Line 210: covering number of Z. Line 745: internal covering number of A and covering number of Z. Since the definition with and without internal is different, please clarify these statement precisely."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a novel framework for the problem of Wasserstein distributionally robust optimization (WDRO) by introducing a novel ambiguity set. The effectiveness of the proposed approach is backed by the generalization certificate bound and the authors also establish the existence for the Stackelberg and Nash equilibria of the statistically robust WDRO problem. Empirically, the proposed practical training algorithm demonstrates its advantages on different adversarial robustness benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The proposed method seems theoretically sound,  and its effectiveness is supported by different empirical experiments. The authors offer a comprehensive analysis that encompasses both theoretical insights and empirical validations. I commend the authors for their commitment to reproducibility by making their code publicly available. Additionally, the detailed explanations of the experimental setup and the thoughtful interpretation of the results are particularly noteworthy."
            },
            "weaknesses": {
                "value": "- The experiments appear to be limited in scope: (i) they only compare against older baselines, and (ii) the model architecture and datasets used are relatively small-scale.\n- Empirically, the gains in adversarial robustness seem to come at the expense of natural accuracy, as observed in Table 1."
            },
            "questions": {
                "value": "- Step 10 in Algorithm 1 is not clear to me. How can we compute the optimal weights {pi}?\n- Why use $\\operatorname{sign}\\left(\\nabla_x L\\left(\\theta,\\left(x_i^{k-1}, y_i\\right)\\right)\\right)$ to update the adversarial examples instead of $\\left(\\nabla_x L\\left(\\theta,\\left(x_i^{k-1}, y_i\\right)\\right)\\right)$?\n- A related work [1] that also incorporates both local and global information to optimize distributional robustness is worth discussing.\n- As distributional robustness is known to address natural distributional shifts, how well do you expect the method to perform under such circumstances (e.g., domain adaptation/generalization)?\n\n[1] Phan, Hoang, et al. \"Global-local regularization via distributional robustness.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.\n\nMinor: Typo in line 104"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the Wasserstein distributionally robust optimization (WDRO) and considers both the adversarial attack and statistical error in the framework. Additional discussions are also provided in terms of the Stackelberg and Nash equilibria."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "(1) The paper is clear and easy to understand. \n\n(2) The theoretical analysis is also sound. \n\n(3) The Nash equilibria perspective is interesting."
            },
            "weaknesses": {
                "value": "(1) The numerical experiments only demonstrate limited improvements. \n\n(2) There is no enough highlight on the technical challenges.\n\n(3) Based on existing litertaure,\n\nLi, Binghui, and Yuanzhi Li. \"Why clean generalization and robust overfitting both happen in adversarial training.\" (2023).\n\nthe robust overfitting phenomenon is more severe in the scenario of neural networks. Could the authors point out any possible way of analyzing WDRO in neural networks?"
            },
            "questions": {
                "value": "Please address my comments in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}