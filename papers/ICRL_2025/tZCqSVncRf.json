{
    "id": "tZCqSVncRf",
    "title": "MIRAGE: Evaluating and Explaining Inductive Reasoning Process in Language Models",
    "abstract": "Inductive reasoning is an essential capability for large language models (LLMs) to achieve higher intelligence, which requires the model to generalize rules from observed facts and then apply them to unseen examples. We present {\\scshape Mirage}, a synthetic dataset that addresses the limitations of previous work, specifically the lack of comprehensive evaluation and flexible test data. In it, we evaluate LLMs' capabilities in both the inductive and deductive stages, allowing for flexible variation in input distribution, task scenario, and task difficulty to analyze the factors influencing LLMs' inductive reasoning. Based on these multi-faceted evaluations, we demonstrate that the LLM is a poor rule-based reasoner. In many cases, when conducting inductive reasoning, they do not rely on a correct rule to answer the unseen case. From the perspectives of different prompting methods, observation numbers, and task forms, models tend to consistently conduct correct deduction without correct inductive rules. Besides, we find that LLMs are good neighbor-based reasoners. In the inductive reasoning process, the model tends to focus on observed facts that are close to the current test example in feature space. By leveraging these similar examples, the model maintains strong inductive capabilities within a localized region, significantly improving its deductive performance.",
    "keywords": [
        "inductive reasoning",
        "large language model",
        "model explanation"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "Our work introduces Mirage, a synthetic dataset that assesses LLMs' inductive reasoning, showing they often fail to apply rules but excel at using neighbor examples in feature space to improve performance",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=tZCqSVncRf",
    "pdf_link": "https://openreview.net/pdf?id=tZCqSVncRf",
    "comments": [
        {
            "title": {
                "value": "Reply to Reviewer ctTK"
            },
            "comment": {
                "value": "Thanks for your response and the raise of confidence. We will add these experiments in the revised version of our paper."
            }
        },
        {
            "comment": {
                "value": "Thank you to the authors for providing clarification on my previous questions. Regarding Question 4, I would suggest fine-tuning the model on a single type of synthetic data, such as list transformation, and then evaluating its performance on other scenarios, such as RP and CG. This approach could provide clearer insights into the model's generalization ability across diverse data types. I maintain my current score, and improve my confidence."
            }
        },
        {
            "title": {
                "value": "Reply to Reviewer ctTK"
            },
            "comment": {
                "value": "Thanks for your careful and insightful reviews.\n\n+ Question 1: It\u2019s unclear where the features in the feature space originate.\n+ Reply 1: We apologize for not making this point more clear in the paper. Actually, we mention the feature in **lines 368-370**, which refers to the input vectors of different facts. Thanks for your suggestions, we will further emphasize this term in the revised version of our paper.\n\n\n\n+ Question 2: The data setting may lack sufficient complexity to effectively demonstrate inductive reasoning capabilities in real-life scenarios.\n+ Reply 2: As we mention in **lines 181-184**, in Mirage, we design the scenario of **real-world problem (RP)** to evaluate the inductive reasoning capabilities in real-life scenarios. Based on **Table 1**, in this task, all of the advanced LLMs achieve an accuracy of **less than 0.25** (D=8). This indicates that the task is already very challenging for models, further demonstrating that the task complexity is sufficient for current models (even though they are relatively easy for humans).\n\n\n\n+ Question 3: Additional insights or empirical evidence on the model's performance with real-world datasets or in less controlled environments.\n+ Reply 3: We show the experimental results of real-world problems (RP) in **Table 1 and 2**. Based on them, we demonstrate that models show poor performance on them. We infer that this is due to the presence of a large amount of irrelevant natural language text in real-world problems, which makes it significantly challenging for LLMs to extract mathematical patterns (e.g. rules, facts) from the context.\n\n\n\n+ Question 4: Does fine-tuning over the synthetic data help the inductive reasoning of LLMs?\n\n+ Reply 4: Thanks for your suggestions. Here, we fine-tune the Llama3-8b-chat model using LoRA on 8,000 training examples and evaluate its performance on the test set. We report the performance of the list transformation task with D=5, N=5 as follows:\n\n  | Task      | ICL  | Fine-tune    |\n  | --------- | ---- | ------------ |\n  | Inductive | 0.31 | 0.89 (+0.58) |\n  | Deductive | 0.27 | 0.82 (+0.55) |\n\n  The results demonstrate that fine-tuning the model on synthetic data can significantly improve its inductive reasoning capabilities. We will report the details and complete results of this experiment in the revised version of the paper once all experiments are completed."
            }
        },
        {
            "title": {
                "value": "Reply to Reviewer DD2X"
            },
            "comment": {
                "value": "Thanks for your careful and insightful reviews.\n\n+ Question 1: Claims such as \"we prove that LLMs are poor in inductive reasoning\" would be too strong.\n+ Reply 1: We apologize for the lack of clarity in our expression here. Thanks for your suggestion, we will modify these claims in our revised version. For example, \"**We demonstrate that LLM is a relatively poor rule-based inductive reasoner compared to direct deduction**\".\n\n\n\n+ Question 2: A deductive inference needs both input facts and the logic rules for deducing the outputs.\n+ Reply 2: We apologize for not making this point clear in the paper. Here, we distinguish between inductive and deductive **based on the form of the test questions rather than the formal definitions in logic**. In our paper, we aim to investigate whether the model must undergo rule induction to complete deductive tasks. Therefore, we design this task form without including the rule as input,  **allowing us to directly evaluate the model's performance on the deductive test examples**. Besides, we also **implement rule-based inference in the ID method described in Section 3.2**, and the experiments conducted with this method also support our conclusion. Thanks for the suggestion, we will further clarify this inconsistency in the revised version of our paper."
            }
        },
        {
            "title": {
                "value": "Reply to Reviewer 6pHZ"
            },
            "comment": {
                "value": "Thanks for your careful and insightful reviews.\n\n+ Question 1\uff1aThe writing is not clear.\n\n+ Reply 1\uff1aWe apologize that we don\u2019t illustrate these terms clearly. Here we provide additional details regarding them.\n\n  + a) As we mention in lines 414-415, \"**make the fact set X contain only one type of the fact**\". For each fact in the fact set, if it is not of the specified type, we remove it and regenerate a new fact until all facts in the set are of the specified type (i.e., IF, CF, or OF).\n  + b) As we mention in a), after finding IF, CF and OF, we replace facts that do not meet the definition. Therefore, we ensure that the entire fact set contains only one type of fact, which **allows the model to observe and perform the corresponding deductive task under different settings**.\n  + c) As  we emphasize in **lines 194-195**, the fundamental metric for our entire dataset is **accuracy**.\n\n  Thanks for your suggestions, we will further emphasize these terms in the revised version of our paper.\n\n  \n\n+ Question 2: The claim \"We prove that LLM is a poor rule-based inductive reasoner\" is too strong and unacceptable. What is the definition of \"poor\"? And have the authors proved it mathematically?\n\n+ Reply 2: We apologize for the lack of clarity in our expression here. Here, \"poor\" refers to **the model's low reliance on rules during the inductive reasoning process**. This does not refer to absolute performance, but rather indicates that the model's performance on inductive tasks is **relatively poor** compared to its performance on deductive tasks. For the proof, we believe that the poor reasoning performance of the model can only be demonstrated experimentally, not proven mathematically. Therefore, **through the experiments in Section 3, we provide empirical evidence to support this claim**. Thanks for your suggestion, we will modify this claim to \"**We demonstrate that LLM is a relatively poor rule-based inductive reasoner compared to direct deduction**\" in our revised version. \n\n\n\n+ Question 3: Have the authors proved the neighbor-based inductive reasoning mathematically? When LLM is a neighbor-based inductive reasoner?\n\n+ Reply 3: We apologize for the lack of clarity in our expression here. Like we mention in Reply 2, the reasoning performance can not be proven mathematically. Instead, we **provide empirical evidence to support this claim through the experiments in Section 4**. Thanks for your suggestion, we will modify this claim to \"**We demonstrate that LLM is a neighbor-based inductive reasoner**\" in our revised version. For the second question, we provide a detailed response in Reply 4.\n\n  \n\n+ Question 4: Lack of a more in-depth analysis of when LLMs use neighbor-based reasoning, and when LLMs use the general pattern thinking?\n\n+ Reply 4: In fact, we have conducted a preliminary exploration of this question in **Appendix C.4.** From the experimental results in Table 12, we can conclude that the model mainly uses the general pattern thinking **when there are few neighbor facts in the context** (e.g. OF). Otherwise, they rely more on neighbor-based reasoning. However, in some cases, the two paradigms may still be coupled together, and therefore, we cannot categorically exclude the influence of either paradigm on the reasoning process."
            }
        },
        {
            "title": {
                "value": "Reply to Reviewer cTBS"
            },
            "comment": {
                "value": "Thanks for your careful and insightful reviews.\n\n+ Question 1\uff1aIs \u201cNeighborhood-Based Reasoning\u201d reasoning or pattern matching? \n\n+ Reply 1: We apologize for not making this point clear in the paper. In fact, the experiment in **Section 4.4** was conducted precisely to address this question. As you can see, in **Figure 7b and 7d**, we test the model's deductive performance on arbitrary test samples, and the results show that the model maintains a deductive density of **over 70%**  under the IF setting. This indicates that even when the patterns of the test samples differ significantly from those of the observed samples, the model can still perform reasoning through neighbor-based reasoning. **If this is a type of pattern matching, the model would only maintain high performance on nearby test samples, rather than sustaining such high performance globally**. Therefore, we believe that neighbor-based reasoning is **a form of reasoning rather than pattern matching**. Thanks for your suggestions, we will further emphasize this term in the revised version of our paper.\n\n  \n\n+ Question 2: It is not surprising that LLMs are good at \"predicting\" based on neighborhood context.\n\n+ Reply 2: In fact, the neighborhood context in the next-word prediction task has, in essence, some fundamental differences from the neighboring facts identified in our work. For the former,  this type of \"neighborhood\" refers to **words that frequently co-occur in human language corpora**. However, for the latter, \"neighborhood\" refers to **the proximity between feature vectors in the vector space (see Section 4.1)**. This proximity in the mathematical space is rarely explicitly represented in the text. Therefore, it cannot be naturally assumed that training on the next-word prediction task will necessarily improve the model's ability for this type of neighbor-based reasoning.\n\n\n\n+ Question 3: Why do you only mention inductive reasoning in the title? \n\n+ Reply 3: We apologize that we don\u2019t illustrate this term clearly. Actually, as we mentioned in **lines 33-36 of the introduction**, most related works incorporate both the **rule induction (inductive)** and **rule application (deductive)** phases into the inductive reasoning process [1,2,3]. Following them,  we consider the entire inductive reasoning process as a type of reasoning, with inductive and deductive being its two components. As you can see, **several representative related works also use \"inductive reasoning\" alone in their titles [1,2,3]**.\n\n\n\n+ Question 4: The relations between the input and the output of rules? How do you distinguish deductive from inductive reasoning?\n\n+ Reply 4: From my understanding, you may be concerned that rule induction might be an associative byproduct of the model's inductive reasoning process rather than a necessary phase. \n\n  + In fact, the experiments in **Section 3** are conducted to investigate the relationship. In prior work, from the perspective of human reasoning, rule induction and rule application are clearly divided into two separate phases [1,2,3]. Therefore, the input and the output of rules are viewed as having a causal relationship. However, our experiments challenge this viewpoint, suggesting that rule induction may not be necessary\u2014that is, **it might simply be associative without implying causation**. \n  + We apologize that we don\u2019t illustrate this term clearly. Actually, our work aims to investigate the role of rule induction in the overall inductive reasoning process,  **without focusing on specific types of relationships**. We can measure the accuracy of induction through a rule-based question-answering task, thereby **assessing the correlation** between rule induction and the final performance. Measuring this correlation **does not require a strict distinction** between inductive and deductive reasoning. In this paper, we simply distinguish between inductive and deductive **based on the form of the test questions rather than the formal definitions in logic**.\n\n  Thanks for your suggestions, we will make further discussions on this term in the revised version of our paper. If I have misunderstood your question in any way, please feel free to point it out.\n\n\n\n[1] Wang, Ruocheng, et al. \"Hypothesis Search: Inductive Reasoning with Language Models.\" *The Twelfth International Conference on Learning Representations*. \n\n[2] Qiu, Linlu, et al. \"Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement.\" *The Twelfth International Conference on Learning Representations*. \n\n[3] Bowen, Chen, Rune S\u00e6tre, and Yusuke Miyao. \"A Comprehensive Evaluation of Inductive Reasoning Capabilities and Problem Solving in Large Language Models.\" *Findings of the Association for Computational Linguistics: EACL 2024*. 2024."
            }
        },
        {
            "summary": {
                "value": "This paper evaluates and analyzes LLM's ability for inductive reasoning. Compared to previous works in inductive reasoning, it adopts a new setting, which is to compare the performance of LLMs under two settings: (1) specific --> general --> specific, and (2) specific --> specific.\nIt is interesting to see that the performance of LLMs under the two settings is comparable. The result indicates that setting (2) will have a better performance, even though traditionally we think a general rule is important to make further inferences to unseen specific examples.\n\nAs for results, the main findings of this paper are that \n(1) setting (2) outperforms setting (1);\n(2) many techniques such as self-consistency can't improve performance in the proposed task; and \n(3) in-context demonstrations that are closer to the input can help more in terms of the accuracy of output."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The main finding is insightful, that setting (2) outperforms setting (1), which can bring in more discussions on how LLMs perform inductive reasoning.\n2. The analysis in terms of neighbor-based reasoning, especially the investigation into the scope is also interesting. However, it might not be surprising, since the similarity of in-context demonstrations has been discussed extensively in previous papers."
            },
            "weaknesses": {
                "value": "1. The writing is not clear. Specifically,\na). what does the \"substitution\" in line 414 and line \"443\" mean?\nb). after finding IF, CF, and OF, how are they used?\nc). what is the metric in Table 1?\n\n2. The claim \"We prove that LLM is a poor rule-based inductive reasoner\" is too strong and unacceptable. What is the definition of \"poor\"? And have the authors proved it mathematically? \n\n3. \"We prove that LLM is a neighbor-based inductive reasoner.\": Have the authors proved it mathematically? When LLM is a neighbor-based inductive reasoner?\n\n4. Although the performance that setting (2) outperforms setting (1) is provided. There might lack of a more in-depth analysis of when LLMs use neighbor-based reasoning, and when LLMs use the general pattern thinking? Or is it that LLMs can purely do neighbor-based reasoning, and never use the general pattern thinking at all? It seems that although each experiment is interesting, but I don't know what is the takeaway knowledge from this paper."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a new dataset for evaluating the induction and deduction ability of LLMs. The authors provide a very detailed explanation of the generation method of the dataset and make extensive experiments to evaluate current state-of-the-art models. The authors also propose two hypotheses about LLMs' reasoning behaviours, and the experiments are designed to verify these hypotheses. Detailed explanations and discussion are provided, and the supplementary material also gives good support to the conclusion."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper covered a broad range of related works and provided another valuable benchmark for evaluating LLMs' reasoning power. The tasks has been formulated in four representations to ensure the diversity.\n- The authors make two interesting hypotheses on LLM's behaviour, and carefully designed several experiments to verify them.\n- In order to evaluate LLMs' behaviours, the authors designed several metrics specifically, most of which make sense to me. Detailed discussions are provided, and all the charts and tables are explained well."
            },
            "weaknesses": {
                "value": "- First of all, all the experiments are behavioural studies and empirical evaluations, and there is no mathematical evidence about LLM's induction/deduction power. IMHO, if there is no mathematical proof, then claims such as \"we prove that LLMs are poor in inductive reasoning\" would be too strong.\n- It is fine to evaluate the models' induction ability by using the designed tasks. However, I wouldn't call the other task as \"deduction\" since the rules are not provided. From the definition from logic, a deductive inference needs both input facts and the logic rules for deducing the outputs."
            },
            "questions": {
                "value": "Please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the inductive reasoning abilities of LLMs, introducing Mirage, a synthetic dataset designed to comprehensively evaluate LLMs' inductive and deductive reasoning from various perspectives. Using Mirage, the authors demonstrate that LLMs are weak at rule-based reasoning but perform well with neighbor-based reasoning. These findings suggest that LLMs' deductive reasoning can be enhanced by leveraging similar examples to support a robust inductive reasoning process."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Strengths:\n- Present a synthetic data generation framework to evaluate inductive and deductive reasoning processes. The authors provide clear steps detailing the data generation process, specifically outlining the rule generation and question generation procedures.\n- Various advanced prompting techniques, including SR and HR, along with the latest LLMs, are tested in the experiments.\n- The effective scope analysis for neighbor reasoning supports the claim that LLMs are effective neighbor-based reasoners."
            },
            "weaknesses": {
                "value": "Weaknesses:\n- In the section on neighbor reasoning, the authors use the term \"Feature Space\" to denote the space of observed facts and test samples. However, it\u2019s unclear where the features in this space originate.\n- The data setting may lack sufficient complexity to effectively demonstrate inductive reasoning capabilities in real-life scenarios."
            },
            "questions": {
                "value": "Questions:\n- Could the authors offer additional insights or empirical evidence on the model's performance with real-world datasets or in less controlled environments?\n- Additionally, could the authors also check if fine-tuning over the synthetic data helps the inductive reasoning of LLMs?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Authors created a synthetic dataset MIRAGE to evaluate the reasoning ability of LLMs, with two conclusions: (1) LLMs are not good at deductive reasoning, (2) LLMs are good at performing inference based on neighborhood information, thus, they are \"neighbor-based inductive reasoner\". \n\nAuthors try to distinguish the ability of deductive reasoning from the ability of inductive reasoning. The missing experiment(s) is/are to distinguish the ability of reasoning from the ability of pattern matching. For example, it is necessary to show the so-called \"neighbor-based inductive reasoner\" is the ability of reasoning, instead of the ability of pattern-matching."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "A new synthetic dataset is created and contributed to the reasoning evaluation of LLMs. \nAuthors conclude that LLMs are poor in rule-based reasoning, and good at neighbor-based reasoning."
            },
            "weaknesses": {
                "value": "Authors coined the nice term \"neighbor-based reasoning\". But, is it reasoning or just pattern-matching? I am afraid there is no term of \"neighbor-based inductive reasoning\" in the literature of logic or psychology. \n\nIt is not surprising that LLMs are good at \"predicting\" based on neighborhood context, as this is one of the important methods to train them."
            },
            "questions": {
                "value": "1. why do you only mention inductive reasoning in the title? you also report experiments on deductive reasoning of LLMs. \n\n2. do you assume any causal relations between the input and the output of rules, or just associative relations? If only associative relations, how do you distinguish deductive from inductive reasoning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}