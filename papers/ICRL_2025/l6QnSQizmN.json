{
    "id": "l6QnSQizmN",
    "title": "Online Reinforcement Learning in Non-Stationary Context-Driven Environments",
    "abstract": "We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to ``catastrophic forgetting'' (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice), employ brittle regularization heuristics or use off-policy methods that suffer from instability and poor performance.\n\nWe present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic and real context traces, and find that it outperforms a variety of baselines in the non-stationary setting, while achieving results on-par with a ``prescient'' agent trained offline across all context traces.",
    "keywords": [
        "catastrophic forgetting",
        "reinforcement learning",
        "context-driven MDP",
        "online learning",
        "non-stationary"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "A reinforcement learning algorithm that solves catastrophic forgetting in non-stationary exogenous context-driven environments by constraining the policy optimization on out of distribution samples.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=l6QnSQizmN",
    "pdf_link": "https://openreview.net/pdf?id=l6QnSQizmN",
    "comments": [
        {
            "comment": {
                "value": "We thank the reviewer for their time and consideration. We appreciate the feedback, and will gladly upload a revision of the paper before the discussion deadline that incorporates them.\n\n---\n\n> Overall, I found the paper quite hard to follow. The method seems fairly straightforward but the paper lacks a clear and easy-to-follow structure I would expect from an ICLR paper. I am confused as to why Section 4 \"Locally-Constrained Policy Optimization\" and Section 4 \"Methodology\" are different sections. I would recommend the authors combine these sections as they both explain the method. If needed, use sub-sections to structure the content.\n\nWe apologize for the confusion. Section 4 \u201cLocally-Constrained Policy Optimization\u201d is meant to give intuition for why the LCPO constraint would reduce catastrophic forgetting, by comparing the idea to a tabular policy in a discrete space MDP. Section 5 \u201cMethodology\u201d formalizes LCPO, and describes one approach to impose the constraint in practice.\n\nWe can combine these two sections to one section with two subsections. We will revise the starting and ending paragraphs of these two sub-sections to improve the overall flow.\n\n---\n\n> Further to this, the figures are not explained thoroughly enough in the text. What does the y-axis in Figures 3 and 4 represent? I assume it is the cumulative distribution function but this should be stated in the paper. Further to this, how should the reader interpret the results? The authors should explain how to interpret the plots, including what the steepness of curves represents, what does it mean when curves are intersecting, etc? I suggest the authors check out rliable and use this to create figures. I am also not sure at what environment step these figures were created for. Again, this should be clear from the text.\n\nWe apologize for the confusion. We hope the explanations below help with clarification.\n* The y-axis in Figures 3 and 4 denotes the CDF, per the label on the y-axis \u201cCDF (%)\u201d. We will amend the captions to also include mentions to the y-axis being CDFs.\n* As for interpretation, we shall revise the main text in line 424 to mention that we seek higher lifelong returns, and will therefore rate baselines by how close their normalized lifelong returns are to 1 across various environments and contexts.\n* As covered in Section 2 (line 116), Lifelong return is the average episodic return across the context trace (after warm-up). The lifelong returns plotted in Figures 3 and 4 are after the entire context trace has finished (lengths are 20M and 8M steps per line 406), and do not pertain to returns of a specific environment step. \n* We will look into rliable and investigate how we can improve our figures using this library. We are thankful for the suggestion.\n\n---\n\n> Why are there confidence intervals for Figure 3a but not for Figures 3b or 4? In Section 6.3, the authors claim that buffer sizes less than $n_b=500$ results in a drop in performance. I am not sure you can make this claim without showing confidence intervals in Figure 4.\n\nWe will be glad to do so, and we can run experiments on more seeds to reduce the variance in confidence intervals if needed. We have reported all confidence intervals in the Appendix (Table 6 for Figure 3b, and Table 7 for Figure 4).\n\nAs for why we did not, the point of these experiments was to demonstrate that LCPO is resistant to changes in buffer size and OOD threshold (with the exception of $n_b=25$); most agents in the ablations are very close in performance (e.g., LCPO with $n_b=200K$ vs. LCPO with $n_b=500$), and their confidence intervals will naturally collide. We considered only plotting the confidence interval for $n_b=25$, since we wanted to point out the loss in performance, but ultimately decided against it since we were worried that singling out a specific agent\u2019s confidence interval would cause confusion.\n\n---\n\n> The authors have incorrectly cited throughout the paper. The authors should take time to become familiar with when to use textual citations and when to use parenthetical citations. If using the natbib package, the authors can use \\citet for textual citations and \\citep for parenthetical citations. The authors almost always cite such that they should be using parenthetical citations but they use textual citations throughout.\n\nWe apologize for this mistake. We used `\\cite`, which defaults to `\\citep` for numerical citations. We were not aware that for author-year citations, it instead defaults to `\\citet`. We will correct citations to use `\\citep` throughput the paper.\n\n---\n\n> What does the bolding in the table represent? Does it represent statistical significance under a t-test? This should be clear from the caption.\n\nWe apologize for the confusion, and will amend the captions to explicitly state the bolding. The bolding in Table 5 represents statistical significance under t-test with 25 seeds, and in other tables with less seeds it represents the best performing agent."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for their time and feedback. We will gladly revise the submission to incorporate their comments.\n\n---\n\n> Paper exceeds the 10-page limit\n\nIf the reviewer is referencing the \u201cReproducibility Statement\u201d, this does not count towards the page limit (https://iclr.cc/Conferences/2025/AuthorGuide):\n> \u201cThis optional reproducibility statement will not count toward the page limit, but should not be more than 1 page.\u201d\n\n---\n\n> The references within the paper seem to have improper notation (using \u00a7 instead of Sec.)\n\nWe apologize for the confusion. The \u00a7 symbol is called the \u201csection\u201d symbol, and is used instead of \u201cSection\u201d in papers, including published ICLR papers [1]. Has ICLR required submissions to not use the symbol this year? We\u2019d be glad to correct the text if this is the case.\n\n[1] Kong, X., Huang, W., & Liu, Y. Conditional Antibody Design as 3D Equivariant Graph Translation. In The Eleventh International Conference on Learning Representations.\n\n---\n\n> I am not sure about the contribution of the opening sentence in the introduction, as this is not the first paper introducing CF, and I do not see any specific relation to the proposed method.\n\nWe apologize for the confusion. To be clear, we do not claim to the be first paper studying CF. The relevance of the opening sentence is hinting at the importance of not forgetting the past, and LCPO operates by constraining the agent to not forget the past.\n\n---\n\n> Since the method uses a few components, adding a block diagram could help, but is not mandatory in the main body. Such block diagram should show the interconnections at each time step between the buffer, new sampled data, OOD detector, and the agent (policy).\n\nWe thank the reviewer for the suggestion, and will gladly create a block diagram for LCPO.\n\n---\n\n> While not exactly the same, maybe contextual MDPs could be related.\n\nThank you for the suggestion. Could the reviewer kindly clarify if they are referring to \u201cContextual Markovian Decision Processes\u201d [2], or broadly papers that focus on contextual MDPs? The works mentioned in lines 142-151 are focused on context-driven MDPs, and we will also add [2] to this section.\n\n[2] Hallak, A., Di Castro, D., & Mannor, S. (2015). Contextual markov decision processes. arXiv preprint arXiv:1502.02259.\n\n---\n\n> Have you tried learning the OOD proxy? e.g., using weighted norm with tunable parameters?\n\nThis sounds interesting, but could the reviewer kindly clarify their suggestion?\n\nWe briefly experimented with auto-encoders + clustering algorithms operating on the latent space, but ultimately found that simpler OOD metrics worked well enough. We also experimented with online gaussian mixture models but found them to be unstable on context traces from real workloads (line 409 and Figure 7b).\n\n---\n\n> You do discuss the differences between your proposed OOD detector and CPD, but have you tried comparing the \"whole package\"? i.e. using LCPO with CPD.\n\nWe briefly experimented mixing MBCD with LCPO on Pendulum-v1. The results were not promising. Upon further investigation, we found the reported change-points to be noisy, similar to that shown in Figure 2. We did not include these experiments in the paper.\n\nDespite this, we believe it is too soon to rule out CPD+LCPO; a \u201csoft\u201d version of CPD algorithms could ultimately benefit LCPO. MBCD is trying to find change-points in context traces that do not have any, and will understandably find it difficult. But if we relax the definition of change-points from hard binary shifts to fuzzy continuous shifts, it may circumvent the issues we found. We found exploring this avenue to be outside the scope of this paper, but an interesting future direction.\n\n---\n\n> 1. line 66: \"These assumptions are rarely met and lead to poor performance in practice.\" -- is there any reference to support this claim? If it cannot be backed, you can alter the sentence to be more soft: \"...and would likely lead to...\".\n\nThe comment in line 66 is from our own analysis (D.1, D.2 and D.3 in Appendix, ) and empirical evaluation (Section 6.1 and D.1 in the Appendix). We will revise the text to be softer, and will include references to these sections for clarity."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for their time and feedback. We will incorporate their comments in a revision of the paper.\n\n---\n\n> This is not a weakness but just a comment: Choosing of the out of distribution (OOD) detector is one obvious design decision when running the method in a specific environment. Section 6.2 discusses different thresholds for the OOD detector. Although OOD detection has been discussed in the supervised learning literature, the paper could provide a more thorough discussion on which kind of OOD detectors could be beneficial in which kind of RL environments.\n\nThis is a great suggestion. We can write a section in the Appendix that discusses what the OOD function aims to do, what class of OOD functions exist in the literature, and how to match different context dynamics to different OOD classes.\n\n---\n\n> The proposed algorithm places a constraint that aims to keep the new policy relevant for old samples. This constraint replaces the TRPO policy update constraint. The original TRPO formulation results in an approximate natural gradient due to which in TRPO the gradient update does not in principle depend on the parameterization of the (neural network) policy. This property is lost here? Do we get other properties?\n\nUnfortunately the property is lost. LCPO\u2019s constraint is being applied in a different part of the policy space (i.e., policy conditioned on OOD states) then the part the optimization is happening in (i.e., policy conditioned on recent states), and the parametrization of the policy becomes important in defining how these two different pieces of the policy distribution are related.\n\nFormally, the LCPO distance metric itself can be approximated with $(\\theta-\\theta_0)^T.\\hat{F}.(\\theta-\\theta_0)$, where $\\hat{F}$ is the Fisher Information Matrix w.r.t to the OOD state-context $<\\hat{s}, \\hat{z}>$. This distance metric is invariant w.r.t to the parametrization of the policy on the OOD state-context $\\pi(\\cdot|\\hat{s}, \\hat{z})$. However the optimization objective is w.r.t to some *recent* state-context $<s, z>$. The distance defined by $\\hat{F}$ is not invariant w.r.t the parametrization of the policy for the recent state-context $\\pi(\\cdot|s, z)$. They are different points in the policy manifold, and without some underlying parameterization to tie them together, the constraint does not affect the objective."
            }
        },
        {
            "comment": {
                "value": "> What happens if the reverse KL is used in the constraint proposed in Eq. 1?\n\nEmpirically, Algorithm 1 would not change. When observing the distributions $\\pi(\\cdot|s, z, \\theta)$ and $\\pi(\\cdot|s, z, \\theta_0)$, the forward and reverse KL are equivalent, at least up to the second order Taylor expansion at $\\theta=\\theta_0$. We show this below.\n\nThe minima for the forward and reverse KL is $\\theta=\\theta_0$, and therefore the zeroth and first order terms of the Taylor expansion will be zero.\n\nAs for the second order, for the forward KL we can derive the second order gradient and show that it is equal to the corresponding Fisher Information Matrix (FIM) entry.\n\nFor a shorthand, define $f(a,\\\\theta):=\\\\pi(a|s, z, \\\\theta)$, for some $ s \\in \\mathcal{S}, z\\in \\mathcal{Z}$.\n\nWe have:\n\\begin{align}\n   \\frac{\\partial^2D_{KL}\\left(f(a,\\theta_0)||f(a,\\theta)\\right)}{\\partial\\theta_i\\partial\\theta_j}|_{\\theta=\\theta_0} &= \\\\left[\\frac{\\partial^2}{\\partial\\theta_i\\partial\\theta_j} \\sum\\_{a \\in \\mathcal{A}}\\left[f(a,\\theta_0)\\\\left(\\ln(f(a,\\theta_0))-\\ln(f(a,\\theta))\\right)\\right]\\right]|\\_{\\theta=\\theta_0}\\\\\\\\\n   &=\\\\left[\\frac{\\partial}{\\partial\\theta_j}\\sum\\_{a\\in \\mathcal{A}}\\left[f(a,\\theta_0)\\left(-\\frac{\\partial f(a,\\theta)}{\\partial\\theta_i}\\frac{1}{f(a,\\theta)}\\right)\\right]\\right]|\\_{\\theta=\\theta_0}\\\\\\\\\n   &=\\sum\\_{a\\in\\mathcal{A}}\\left[f(a,\\theta_0)\\left(-\\frac{\\partial^2 f(a,\\theta)}{\\partial\\theta_i\\partial\\theta_j}\\frac{1}{f(a,\\theta)}+\\frac{\\partial f(a,\\theta)}{\\partial\\theta_i}\\frac{\\partial f(a,\\theta)}{\\partial\\theta_j}\\frac{1}{f(a,\\theta)^2}\\right)\\right]\\_{\\theta=\\theta_0}\\\\\\\\\n   &=\\sum\\_{a\\in\\mathcal{A}}\\left[f(a,\\theta_0)\\left(-\\frac{\\partial^2 f(a,\\theta)}{\\partial\\theta_i\\partial\\theta_j}|\\_{\\theta=\\theta_0}\\frac{1}{f(a,\\theta_0)}+\\\\left[\\frac{\\partial f(a,\\theta)}{\\partial\\theta_i}\\frac{\\partial f(a,\\theta)}{\\partial\\theta_j}\\frac{1}{f(a,\\theta)^2}\\right]|\\_{\\theta=\\theta_0}\\right)\\right]\\\\\\\\\n   &=-\\sum\\_{a\\in\\mathcal{A}}\\left[\\frac{\\partial^2 f(a,\\theta)}{\\partial\\theta_i\\partial\\theta_j}|\\_{\\theta=\\theta_0}\\right]+\\sum\\_{a\\in\\mathcal{A}}\\left[f(a,\\theta_0)\\frac{\\partial \\ln f(a,\\theta)}{\\partial\\theta_i}|\\_{\\theta=\\theta_0}\\frac{\\partial \\ln f(a,\\theta)}{\\partial\\theta_j}|\\_{\\theta=\\theta_0}\\right]\\\\\\\\\n   &=-\\frac{\\partial^2}{\\partial\\theta_i\\partial\\theta_j}\\\\left[\\sum\\_{a\\in\\mathcal{A}}f(a,\\theta)\\right]|\\_{\\theta=\\theta_0}+\\mathbb{E}\\_{a\\sim f(a, \\theta_0)}\\\\left[\\frac{\\partial \\ln f(a,\\theta)}{\\partial\\theta_i}|\\_{\\theta=\\theta_0}\\frac{\\partial \\ln f(a,\\theta)}{\\partial\\theta_j}|\\_{\\theta=\\theta_0}\\right]\\\\\\\\\n   &=-\\frac{\\partial^2}{\\partial\\theta_i\\partial\\theta_j}[1]|\\_{\\theta=\\theta_0}+\\mathbb{E}\\_{a\\sim f(a, \\theta_0)}\\\\left[\\frac{\\partial \\ln f(a,\\theta)}{\\partial\\theta_i}|\\_{\\theta=\\theta_0}\\frac{\\partial \\ln f(a,\\theta)}{\\partial\\theta_j}|\\_{\\theta=\\theta_0}\\right]\\\\\\\\\n   &=G\\_{i,j}\n\\end{align}\n\nwhere G is the FIM.\n\nNow for the reverse KL, we can similarly derive the second order gradient:\n\n\\begin{align}\n   \\frac{\\partial^2D_{KL}\\left(f(a,\\theta)||f(a,\\theta_0)\\right)}{\\partial\\theta_i\\partial\\theta_j}|\\_{\\theta=\\theta_0} &= \\left[\\frac{\\partial^2}{\\partial\\theta_i\\partial\\theta_j}\\sum\\_{a\\in\\mathcal{A}}f(a,\\theta)\\left[\\ln(f(a,\\theta))-\\ln(f(a,\\theta_0))\\right]\\right]|\\_{\\theta=\\theta_0}\\\\\\\\\n   &=\\left[\\frac{\\partial}{\\partial\\theta_j}\\sum\\_{a\\in\\mathcal{A}}\\left(\\frac{\\partial f(a,\\theta)}{\\partial\\theta_i}\\left[\\ln(f(a,\\theta))-\\ln(f(a,\\theta_0))\\right]+f(a,\\theta)\\frac{\\partial f(a,\\theta)}{\\partial\\theta_i}\\frac{1}{f(a,\\theta)}\\right)\\right]|\\_{\\theta=\\theta_0}\\\\\\\\\n   &=\\sum\\_{a\\in\\mathcal{A}}\\left[\\frac{\\partial^2 f(a,\\theta)}{\\partial\\theta_i\\partial\\theta_j}\\left(\\ln(f(a,\\theta))-\\ln(f(a,\\theta_0))\\right)+\\frac{\\partial f(a,\\theta)}{\\partial\\theta_i}\\frac{\\partial f(a,\\theta)}{\\partial\\theta_j}\\frac{1}{f(a,\\theta)}+\\frac{\\partial^2 f(a,\\theta)}{\\partial\\theta_i\\partial\\theta_j}\\right]|\\_{\\theta=\\theta_0}\\\\\\\\\n   &=\\sum\\_{a\\in\\mathcal{A}}\\left[\\frac{\\partial^2 f(a,\\theta)}{\\partial\\theta_i\\partial\\theta_j}|\\_{\\theta=\\theta_0}\\left(\\ln(f(a,\\theta_0))-\\ln(f(a,\\theta_0))\\right)\\right]+\\sum\\_{a\\in\\mathcal{A}}\\left[f(a,\\theta_0)\\left(\\frac{\\partial f(a,\\theta)}{\\partial\\theta_i}\\frac{\\partial f(a,\\theta)}{\\partial\\theta_j}\\frac{1}{f(a,\\theta)^2}\\right)\\right]_{\\theta=\\theta_0}+\\sum\\_{a\\in\\mathcal{A}}\\left[\\frac{\\partial^2 f(a,\\theta)}{\\partial\\theta_i\\partial\\theta_j}\\right]|\\_{\\theta=\\theta_0}\\\\\\\\\n   &=0+\\mathbb{E}\\_{a\\sim f(a,\\theta_0)}\\left[\\frac{\\partial \\ln f(a,\\theta)}{\\partial\\theta_i}|\\_{\\theta=\\theta_0}\\frac{\\partial \\ln f(a,\\theta)}{\\partial\\theta_j}|\\_{\\theta=\\theta_0}]\\right]+0\\\\\\\\\n   &=G\\_{i,j}\n\\end{align}\n\nSince the second order derivatives for the forward and reverse KL at $\\theta=\\theta_0$ turned out to be the same, the approach in Eq. 4 would be the same. Therefore, if we use reverse KL in Eq. 1, we would reach the same Eq. 4 as we would with forward KL."
            }
        },
        {
            "comment": {
                "value": "> Deep neural networks are known to overfit to the early samples [3], wouldn\u2019t regularizing with respect to the past data exacerbate this process?\n\nIt may exacerbate it, if exploration is not encouraged. If the context traces are truly different for past data and the neural network has \u201cenough capacity\u201d, the neural network should be able to learn a new policy for the new data without forgetting the old one. But to avoid overfitting and sub-optimal convergence, we need to motivate the agent to explore in new contexts. In this paper, we do this through entropy regularization (Eq. 3), but it could be done through any exploration methodology, e.g. curiosity-based exploration.\n\n---\n\n> How is the context information different from a task ID?\n\nIn summary, context information is a generalization of task ID. Task IDs are discrete indices and clearly separable, while the context process is continuous, multi-dimensional and can change gradually or abruptly.\n\nA task ID is a discrete and often bounded (2 to ~100 tasks, depending on the problem setup) variable, and for task IDs $i$ and $j$, it ensures that the MDPs subject to task IDs $i$ and $j$ are the same iff $i=j$. Task IDs ensure that the underlying MDP is piece-wise stationary; for any period of time where the task ID remains unchanged, the MDP behaves exactly the same way. When task IDs are available, we can solve CF by numerous ways such as learning separate policies per task, forcing gradient updates that are orthogonal to prior tasks, etc.\n\nNone of these are possible with context information. The context process is a continuous variable (possibly multi-dimensional) that may change slowly or quickly across time. Since it is continuous, we cannot learn a separate policy per each context variable anymore (there are millions of different context values for a 1-dimensional variable). We cannot force orthogonal updates since small changes in the context could be considered different \u201ctasks\u201d, but would in practice be too similar to the current \u201ctask\u201d.  The clear separation that task IDs promise does not exist in the context information.\n\nStrictly speaking, all task IDs are context variables, but context variables cannot be reduced to task IDs. We can only do so in the unique case where the context process is known to be piece-wise stationary. If we make such an assumption, we can map context values to task IDs, which is what change-point detection methods do. When the context process is not piece-wise stationary, change-point detection leads to noisy task IDs, such as that observed in Figure 2.\n\nTherefore, context information allows us to model and solve problems where the MDP changes may be slow and gradual, where there is no clear separation in MDP behavior across time. They also allow modeling problems where context can be multi-dimensional instead of single dimensional.\n\n---\n\n> In Sec 4, does the A2C use both context and state information to learn the policy?\n\nYes. A2C, Tabular A2C and LCPO all observe both the state $s_t$ and context $z_t$.\n\n---\n\n> Is it possible to approximate the proposed objective in the PPO style as opposed to the TRPO style to avoid using second-order information?\n\nIt is approximately possible, but not fully. In E in the Appendix, we experiment with a PPO style agent, where the KL distance is added as an auxiliary loss function (Eq. 18, page 25) to the PPO loss function (Eq. 19, page 25). It does not work as well as LCPO, since this formulation is trying to solve the Lagrangian dual of the original problem, but the Lagrange coefficient ($\\kappa$ in Eq. 19) can change for different contexts, making this approach hyperparameter sensitive.\n\nIt is not possible to embed the LCPO constraint in the proximal update rule of PPO. The proximal update rule needs the advantage value for whatever state it is being applied to (as PPO is an on-policy approach), and since we do not have advantage values for OOD states visited in past data, we cannot apply the constraint. Note that the advantage values we observed when we were collecting past data is obsolete, since the policy has changed since then, and cannot be used in an on-policy approach.\n\n---\n\n> What is a warm-up period and why is it needed in the experiments?\n\nThe goal of the problem setup is to optimize for lifelong returns\u2014defined in Section 2, line 116\u2014which are the asymptotic average episodic return across time. Lifelong returns cares about the eventual performance of the policy, and will ignore transient returns where the policy is exploring. In our empirical evaluation, context traces are finite in length. Therefore in calculating lifelong returns we exclude a portion of experiment at the start so the lifelong return is not dominated by transient exploration dynamics. This is particularly important since different approaches use different exploration strategies, e.g., DDQN using eps annealing vs. entropy regularization for actor critic agents."
            }
        },
        {
            "comment": {
                "value": "We thank the reviewer for their time and comments. We will thoroughly revise the paper to reflect their suggestions.\n\n---\n\n> The biggest weakness is that the paper considers a specific kind of non-stationarity in the study. Although exogenous processes could be common in real-world scenarios, many other kinds of non-stationarities exist and it is unclear whether the proposed approach works well in those cases. Besides, the OOD detector is an important piece in the algorithm and its effectiveness depends on the threshold value \u2013 a hyperparameter that is environment-dependent and unknown beforehand.\n\nThese are fair points. If the context is non-exogenous, the current formulation of the problem will lead to an adversarial POMDP, which may be in general difficult, if not impossible to learn a policy for.\n\nWe view the OOD detector as the single source of domain knowledge from the user. Without any knowledge of the context trace beforehand, it would again be difficult, if not impossible, to reason about how different two pieces of context are. It may be possible if we are allowed to see context traces from the same distribution, akin to the problem setups in Meta RL. This would be an interesting future direction.\n\n---\n\n> The approach also assumes that the context is observed, although it is not used as a task detector or task boundary detector. In many practical cases, the context information is unavailable and it is unclear how one could use the proposed approach in that case.\n\nWe agree with the reviewer that this problem setup does not encompass all possible settings of context-driven MDP. In cases where the context is latent, one could potentially combine LCPO with approaches that infer the latent context, such as those in lines 142-151.\n\n---\n\n> The proposed constrained optimization problem is approximately solved, which in itself is fine, but when paired with the fact that the approach doesn\u2019t use TRPO-style local constraint, the training procedure could be unstable.\n\nWhile the TRPO-style constraint is not enforced in the optimization stage, it is enforced in the line search stage (line 364-265, lines 9-10 in Algorithm 1). We do this by reducing the step size until the local TRPO constraint is also met.\n\n---\n\n> The paper should discuss how [1] and [2] are related to the proposed approach. [1] uses a KL constraint on the current policy to be closer to the global policy, and [2] learns a global value function as a baseline estimate for all contexts on top of which the learning happens. It seems like the global policy and the global value functions induce the regularization effect on the past that the paper proposes.\n\nWe thank the reviewer for their suggestions. We will add these papers to the related works section. Below, we discuss their relation to our work:\n\n* Distral [1] is a technique for the multi-task RL problem setup, where the goal is to learn multiple tasks in parallel faster than learning them separately, i.e. the goal here is positive transfer learning. Distral suggests learning a distilled policy that represents the shared dynamics of the multiple tasks. This distilled policy is used to direct task-specific policies towards faster convergence. This problem setup is different than ours and does not focus on continual RL or catastrophic forgetting.\n* PT-Q learning [2] is a technique focused on continual RL. The general idea is to learn a slow-changing Q function $Q^{(P)}(s, a; \\theta)$ and combine it with a fast changing & decayed Q function $Q^{(T)}(s, a, \\mathbf{w})$ to learn the environment. The slow-changing Q function allows learning a good bootstrap network that allows the fast changing Q function to rapidly converge to the optimal policy. We would gladly add PT-Q learning as a baseline, and will update the paper if the results are finished before the deadline.\n\n---\n\n> In Sec 4, the episodic returns are -4 and -6 and not -3 and -5 as mentioned in the paper. The ablation experiments and the results presented in the appendix can be summarized as one-line bullet points in the main paper.\n\nWe apologize for the confusion. We should clarify in the main text, that once the agent reaches the terminal state, the reward is 0. Thus the last step leading to the terminal state does not accrue a reward of -1."
            }
        },
        {
            "summary": {
                "value": "The paper proposes Locally Constrained Policy Optimization (LCPO), a policy gradient algorithm for non-stationary reinforcement learning where the context changes are observed and exogenous. LCPO\u2019s updates are constrained to preserve the policy trained on the past contexts while it maximizes returns on newer samples. Two replay buffers are maintained in order to store the samples belonging to the past contexts and the current context. Whenever the distribution of samples is different in these two buffers, which is detected using an OOD threshold-based detector, then a \u201cregularization\u201d update is performed to preserve past learnings. The proposed approach is tested on a range of environments with a whole suite of baselines to demonstrate its benefits."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The paper is well-written and easy to read: the introduction is thorough, the related works section is exhaustive, preliminary and the LCPO sections provide adequate details, and the methodology section is well-presented.\n* The specific kind of non-stationary problem that the paper considers is well-described. And, the proposed solution fits like a glove to this problem. The approach is also intuitive and parallels the TRPO algorithm.\n* The authors evaluate their approach against numerous baselines on several tasks, which provides substantial confidence in the presented results and conclusions. The ablation studies are also welcome, as they offer more insights into the proposed approach.\n* The illustrative examples discussed throughout the paper offer an intuitive understanding of various complex ideas \u2013 the authors have done a great job of including them."
            },
            "weaknesses": {
                "value": "* The biggest weakness is that the paper considers a specific kind of non-stationarity in the study. Although exogenous processes could be common in real-world scenarios, many other kinds of non-stationarities exist and it is unclear whether the proposed approach works well in those cases. Besides, the OOD detector is an important piece in the algorithm and its effectiveness depends on the threshold value \u2013 a hyperparameter that is environment-dependent and unknown beforehand.\n* The approach also assumes that the context is observed, although it is not used as a task detector or task boundary detector. In many practical cases, the context information is unavailable and it is unclear how one could use the proposed approach in that case.\n* The proposed constrained optimization problem is approximately solved, which in itself is fine, but when paired with the fact that the approach doesn\u2019t use TRPO-style local constraint, the training procedure could be unstable."
            },
            "questions": {
                "value": "**Decision:**\n\nAlthough the paper has some weaknesses, the positives outweigh them. The paper is well-executed and it is a good contribution to continual reinforcement learning. Therefore, I recommend an **acceptance.**\n\n**Areas of improvement:**\n\n* The paper should discuss how [1] and [2] are related to the proposed approach. [1] uses a KL constraint on the current policy to be closer to the global policy, and [2] learns a global value function as a baseline estimate for all contexts on top of which the learning happens. It seems like the global policy and the global value functions induce the regularization effect on the past that the paper proposes.\n* In Sec 4, the episodic returns are -4 and -6 and not -3 and -5 as mentioned in the paper.\nThe ablation experiments and the results presented in the appendix can be summarized as one-line bullet points in the main paper.\n\n**Questions:**\n\n* Deep neural networks are known to overfit to the early samples [3], wouldn\u2019t regularizing with respect to the past data exacerbate this process?\n* How is the context information different from a task ID?\n* In Sec 4, does the A2C use both context and state information to learn the policy?\n* Is it possible to approximate the proposed objective in the PPO style as opposed to the TRPO style to avoid using second-order information?\n* What happens if the reverse KL is used in the constraint proposed in Eq. 1?\n* What is a warm-up period and why is it needed in the experiments?\n\n**References:**\n\n[1] Teh, Yee, et al. \"Distral: Robust multitask reinforcement learning.\" Advances in neural information processing systems 30 (2017).\n\n[2] Anand, Nishanth, and Doina Precup. \"Prediction and control in continual reinforcement learning.\" Advances in Neural Information Processing Systems 36 (2024).\n\n[3] Nikishin, Evgenii, et al. \"The primacy bias in deep reinforcement learning.\" International conference on machine learning. PMLR, 2022."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses \"catastrophic forgetting\" in continual reinforcement learning with non-stationary dynamics. The dynamics depends on the current context which is observable but can change arbitrarily. A problem in these kind of settings is that the agent over time forgets how to behave in prior contexts. To address this problem the paper proposes a new on-policy algorithm that aims to keep policy updates such that the policy for prior contexts does not change. The proposed approach does not need task labels but only an out-of-distribution detector."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Well written paper. Clear definitions and derivations. The paper explains well all the design decisions in the proposed approach.\n\nThe main algorithmic idea of sampling states from the history buffer that differ from the samples currently collected from the environment, and, then using these samples to constrain the policy update is simple and based on the experimental results appears very effective.\n\nI like that the resulting algorithm is an almost straight forward modification of TRPO but for a completely different problem. Note that while being simple the main idea is only obvious in hindsight."
            },
            "weaknesses": {
                "value": "I could not find major weaknesses in the paper.\n\nThis is not a weakness but just a comment:\nChoosing of the out of distribution (OOD) detector is one obvious design decision when running the method in a specific environment. Section 6.2 discusses different thresholds for the OOD detector. Although OOD detection has been discussed in the supervised learning literature, the paper could provide a more thorough discussion on which kind of OOD detectors could be beneficial in which kind of RL environments."
            },
            "questions": {
                "value": "The proposed algorithm places a constraint that aims to keep the new policy relevant for old samples. This constraint replaces the TRPO policy update constraint. The original TRPO formulation results in an approximate natural gradient due to which in TRPO the gradient update does not in principle depend on the parameterization of the (neural network) policy. This property is lost here? Do we get other properties?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces an online RL method for environments with non-stationary (but observable) context, which affects the dynamics/reward. The main difficulty in such settings is catastrophic forgetting; while training the agent on an unobserved context, it forgets the optimal behavior in past contexts. The authors aim to solve this issue by constraining the agent to maintain current behavior for previously-observed contexts, which is possible due to an OOD detector -- basically a distance measure over contexts. To test the proposed method, the authors use a single toy problem and a large amount of benchmarks, the results show that the method indeed surpasses other baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I think the paper is well-written, easy to understand and comprehensive. Notably, the empirical sections present the advantages of the proposed algorithm. For my opinion, with a few minor changes, it passes the acceptance threshold.\n\n### Experiments\nThe experiments are extensive in terms of both benchmarks and baselines, and show good performance for the proposed method.\n\n### Illustrative Example:\nI think this empirical analysis helps to understand what's happening, and shows the value of the method."
            },
            "weaknesses": {
                "value": "**Paper exceeds the 10-page limit**\n\n### Presentation\n1. The references within the paper seem to have improper notation (using \u00a7 instead of Sec.) \n\n2. I am not sure about the contribution of the opening sentence in the introduction, as this is not the first paper introducing CF, and I do not see any specific relation to the proposed method.\n\n3. Since the method uses a few components, adding a block diagram could help, but is not mandatory in the main body. Such block diagram should show the interconnections at each time step between the buffer, new sampled data, OOD detector, and the agent (policy).\n\n### Related Work:\nWhile not exactly the same, maybe contextual MDPs could be related.\n\n### Minor comments:\n\n1. line 66: \"These assumptions are rarely met and lead to poor performance in practice.\" -- is there any reference to support this claim? If it cannot be backed, you can alter the sentence to be more soft: \"...and would likely lead to...\".\n\n2. line 268: \"proof in \u00a7B in the Appendix.\" -- rephrase."
            },
            "questions": {
                "value": "1. Have you tried learning the OOD proxy? e.g., using weighted norm with tunable parameters?\n\n2. You do discuss the differences between your proposed OOD detector and CPD, but have you tried comparing the \"whole package\"? i.e. using LCPO with CPD."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates online reinforcement learning (RL) in non-stationary environments, \nwhere the non-stationarity arises from a time-varying exogenous context process affecting the dynamics.\nIn particular, they study the catastrophic forgetting (CF) problem.\nThey propose a new method named locally constrained policy optimization (LCPO),\nwhich combats CF by constraining policy updates for a particular context on prior data from other contexts.\nThey evaluate their method in the classic control tasks from MuJoCo and computer systems environments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Whilst this paper is slightly out of my expertise, I think the authors are addressing an important problem as\nhandling non-stationarity in online RL is important.\nTo the best of my knowledge, the method presented in this paper seems original.\n\nI particularly liked Figure 1. I would suggest the authors move it earlier in the paper."
            },
            "weaknesses": {
                "value": "Overall, I found the paper quite hard to follow. \nThe method seems fairly straightforward but the paper lacks a clear and easy-to-follow structure I would expect from an ICLR paper.\nI am confused as to why Section 4 \"Locally-Constrained Policy Optimization\" and Section 4 \"Methodology\" are different sections.\nI would recommend the authors combine these sections as they both explain the method. \nIf needed, use sub-sections to structure the content.\n\nFurther to this, the figures are not explained thoroughly enough in the text. What does the y-axis in Figures 3 and 4 represent? I assume it is the cumulative distribution function but this should be stated in the paper. \nFurther to this, how should the reader interpret the results?\nThe authors should explain how to interpret the plots, including what the steepness of curves represents, what does it mean\nwhen curves are intersecting, etc?\nI suggest the authors check out [rliable](https://github.com/google-research/rliable/tree/master) and use this to create figures.\nI am also not sure at what environment step these figures were created for.\nAgain, this should be clear from the text.\n\nWhy are there confidence intervals for Figure 3a but not for Figures 3b or 4?\nIn Section 6.3, the authors claim that buffer sizes less than $n_b=500$ results in a drop in performance.\nI am not sure you can make this claim without showing confidence intervals in Figure 4.\n\nThe authors have incorrectly cited throughout the paper. \nThe authors should take time to become familiar with when to use textual citations and when to use parenthetical citations.\nIf using the `natbib` package, the authors can use `\\citet` for textual citations and `\\citep` for parenthetical citations.\nThe authors almost always cite such that they should be using parenthetical citations but they use textual citations throughout.\n\nWhat does the bolding in the table represent?\nDoes it represent statistical significance under a t-test?\nThis should be clear from the caption.\n\n# Minor comments\n- \"Mujoco\" should be \"MuJoCo\"\n- Line 191 - the policy is stochastic but on line 112 it's a deterministic mapping."
            },
            "questions": {
                "value": "- How can you improve the clarity of the writing?\n- How am I supposed to read Figure 3. What does the y-axis represent?\n- Why have you used textual citations everywhere?\n- What does the bolding in tables represent?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}