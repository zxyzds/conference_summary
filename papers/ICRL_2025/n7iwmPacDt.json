{
    "id": "n7iwmPacDt",
    "title": "Polybasic Speculative Decoding Under a Theoretical Perspective",
    "abstract": "Speculative decoding has emerged as a critical technique for accelerating inference in large language models, achieving significant speedups while ensuring consistency with the outputs of the original models.\nHowever, there is currently a lack of theoretical guidance in speculative decoding. \nAs a result, most existing works are dualistic target-draft model paradigm, which significantly restricts the hinders potential application scenarios.\nIn this paper, we propose a polybasic speculative decoding framework supported by a solid theoretical foundation.\nWe first deduce a theorem to control the ideal inference time of speculative decoding systems which is then serve as a design criterion that effectively expands the original dualistic speculative decoding into a more efficient polybasic speculative decoding. \nWe further theoretically analyze the sampling process, identifying variables that can be optimized to enhance inference efficiency in multi-model systems.\nWe demonstrate, both theoretically and empirically, that this system accelerates inference for the target model, and that our approach is orthogonal to the majority of existing speculative methods, allowing for independent application or combination with other techniques. \nExperimentally, we conducted comprehensive evaluations across a wide range of models, including those from the Vicuna, LLaMA2-Chat, and LLaMA3 families. \nOur method achieved remarkable latency speedup ratios of $\\textbf{3.31$\\times$-4.01$\\times$}$ for LLaMA2-Chat 7B, up to $\\textbf{3.87$\\times$}$ for LLaMA3-8B, and up to $\\textbf{4.43$\\times$}$ for Vicuna-7B, while maintaining the distribution of the generated text. Code is available in supplementary materials.",
    "keywords": [
        "Speculative Decoding; LLM Inference"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-13",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=n7iwmPacDt",
    "pdf_link": "https://openreview.net/pdf?id=n7iwmPacDt",
    "comments": [
        {
            "summary": {
                "value": "The authors propose speculative decoding where they use multiple models to draft and verify. The idea has theoretical backing to understand where multiple models make sense compared to existing setup of dualistic speculative decoding. The authors show impressive speedups over EAGLE. \n\n\u201cthere exists a significant correlation between the number of forward propagation executions for each model and the average token acceptance length between models.\u201d -> Interesting observation, but at some level why is this not intuitive, probabilisticly speaking more draft models \u00a0are run, higher the likelihood.\n\n\u201cThrough empirical analysis, we found that the system achieves its maximum acceleration ratio when the \u03c6i satisfies:\u201d -> Please link the figure for this, all your proofs depend on this.\n\n\u201cFigure 3: Performance Comparison across different tasks. Our method demonstrates its peak performance in the math task, achieving an impressive 4.43\u00d7 speedup with the Vicuna 7B model.\u201d -> Can authors comment on why in a certain case in EAGLE is faster ?\n\nHow will your system work with different paradigms which construct token tree \u00a0like SpecInfer or Medusa.\n\nFurther how will you handle ideas like Self-Speculative decoding, where intermediate layers are used as exit points to perform speculative decoding.\n\n\u201cWhen the success probability 1 \u2212 \u03b1 is high\u201d -> How do you calculate the success probability at inference ?\n\nI am also curious how does this work compare to works like SpecInfer which builts multiple models to build a token tree verifier."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I think the speedups reported are quite impressive. \n\nThe limitations section is very well written and highlights the problems with implementation."
            },
            "weaknesses": {
                "value": "See Summary.\n\nIf the authors are able to clarify some of the questions and fix some of the writing. I am happy to bump up the score to a 5."
            },
            "questions": {
                "value": "See Summary."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper propose a polybasic speculative decoding framework supported by a solid theoretical foundation.  The method achieves latency speedup ratios of 3.31\u00d7-4.01\u00d7 for LLaMA2-Chat 7B, up to 3.87\u00d7 for LLaMA3-8B, and up to 4.43\u00d7 for Vicuna-7B."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The empirical results are strong. The paper conducted experiments on 3 different models under a variety of settings, and showed 3-4x speedup, which surpasses the baseline method that only uses single draft models."
            },
            "weaknesses": {
                "value": "The theoretical parts are not sound:\n\n- The concept \"ideal forward count\" $\\phi_i$ is not properly defined. \"Optimal number of forward passes required to generate tokens that are likely to be accepted by the previous model Mi\u22121\". Is $\\phi_i$ a random variable? What do you mean by \"optimal\"? Is $\\phi_i$ obtained by optimizing some objective? What is \"likely to be accepted\"? Is this a high probability definition?\n- Line 205-209 also comes from a contingent source of evidence: the definition of $\\phi_i$ shouldn't be dependent on your specific empirical setting.\n- The statment of Lemma 3.1 is not rigorous. The proof is not rigorous as well. In the proof, the authors neglect the randomness of $L_i$ by a extremely vague argument \"when the variability of Li is sufficiently low relative to its mean, we can substitute L with its expected value E[L]\". Instead, a proper development of lemma 3.1 will involve certain form of concentration inequalty, and the authors should quantify the errors of Talyor expansion and dropping the higher-order terms.\n- In Theorem 3.2, the definition of $T_2'$ and $L_2'$ should be clearly stated in the theorem statement. \n- Why would we need $\\mathbb{E}[L_{D_i}] > \\mathbb{E}[L_{D_{i+1}}]$ in the first place? There is no theoretical result supporting this goal.\n- Line 275-277 the claim \"we found that using speculative sampling can lead to more stable acceptance token length.\" is weird.\n\n\nThe novelty of the empirical algorithm is limited. Cascaded speculative decoding idea is not new and there are already multiple previous papers, including but not limit to: \n\n- [1]. Cascade Speculative Drafting for Even Faster LLM Inference (https://arxiv.org/pdf/2312.11462)\n- [2]. Accelerating LLM Inference with Staged Speculative Decoding (https://arxiv.org/pdf/2308.04623)\n\nThe theory developed in the paper can extend to speculative decoding systems with more than 4 models, but the experiments only restrict to 3-model cases. \n\n\nThe theoretical part seems to be disconnected with the empirical part. The results of Theorem 3.2  is not used in guiding the determination of the number of models $n$ and the selection of models $M_1,M_2,\\dots, M_n$. E.g., I would expect the authors construct a pool of models and calculate the $T_i$ and $L_i$ as per Theorem 3.2 for each pair of the models. Besides, Theorem 3.3 is a simple characteristic of capped geometry variable and is a bit misleading (we want draft models with high acceptance rate instead of low acceptance length variance.)"
            },
            "questions": {
                "value": "The acceptance length should be the property of the target distribution and the draft distribution. Consider $M_1$ as the target model and $M_2$ as the draft model. Then using $M_3$ to speed up $M_2$ with speculative decoding shouldn't modify the distribution of $M_2$, i.e., the generated tokens have the same distribution as it was generated by $M_2$ only. If the draft distribution of $M_2$ is the same as $(M_2, M_3)$, then the average acceptance length shouldn't change as well. Can you clarify if I misunderstand the terms? What is the reason behind $\\mu$'s change in Table 2 for \"ours\" and \"eagle\"? Are the draft models different for \"ours\" and \"eagle\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "**Background**: Speculative decoding refers to a suite of techniques to allow fast inference of LLMs. A common form of speculative decoding is based on the draft-and-verify (or more intuitively \u201cpropose-then-accept\u201d) framework where a small, \u201cdrafter\u201d model is used to generate next tokens, which are then verified in parallel by the large, target LLM of interest.  \n\nThis paper proposes the so-called \u201cpolybasic speculative decoding\u201d, a framework which relies on a chain of n LLMs ordered in decreasing sizes. When the i-th model is used as the target LLM, the first i-1 models are used together to form a drafter. Theoretically, the paper contributes several theorems to characterize the expected acceptance rate of the new framework. Empirically, the paper shows on standard benchmarks that the new framework allows 3-4 times latency speedup compared to the vanilla setting (i.e., directly sampling from the target LLM) ."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "While the idea of using multiple drafting models is not entirely new, the attempt of arranging n LLMs in a chain and constructing a draft-and-verify pipeline on them appears to be interesting. Though, the paper ends up studying the case of n=3 (see Algo 1). Empirical results verifying that the new approach provides 3-4 times inference speedup is encouraging."
            },
            "weaknesses": {
                "value": "Overall writing could be improved. More importantly, many important mathematical statements are not precisely stated. This is a major concern. Many statements are imprecise to the point of hindering understanding of the work. For instance, Lemma 3.1 has only one sentence that states \u201cWe can substitute L with its expected value E[L].\u201d. I was left wondering \u201cSubstitute into where? What assumptions are made?\u201d There is no objective function written until Lemma 3.1 that would allow me to substitute L with E[L]. I guess that perhaps Lemma 3.1 is meant for  $\\phi_i$ (L207). This then leads to another trouble which is that the definition of $\\phi_i$ is never justified beyond the phrase \u201cThrough empirical analysis\u201d. No further detail is given. There are other examples. More  specific questions will be given in the Questions section of this form. Significance and in fact correctness of many statements are not easy to verify because of their impreciseness."
            },
            "questions": {
                "value": "**Major comments/questions:**\n\n1. Clarify throughout that all the theoretical analysis is independent of any prefix. For instance, the random variable L is meant to denote the length of accepted tokens *on average* (independently of any input sequence). This is never stated. At L190, it is assumed that $L \\sim \\mathcal{N}(\\mu, \\sigma^2)$. Could you please provide a justification for why this assumption makes sense? L is a discrete variable. A normally distributed variable can take a negative value. More importantly, where is this normality assumption used in the paper?\n\n2. Sec 3.1, L202. \u201cwe introduce the concept of ideal forward count, denoted as $\\phi_i$ for model $M_i$ , which represents the optimal number of forward passes required to generate tokens.\u201d This is arguably one of the most important parts in the paper. What does \u201cnumber of forward passes\u201c mean? Also, \u201cto generate tokens\u201d at what granularity (e.g., for one speculative decoding block, for one query, for the whole test set)? \n\n3. L203. The paper defines $\\phi_i$ \u201cthrough empirical analysis\u201d. What is the empirical analysis? How did you come up with $\\phi_i$ as defined in Sec 3.1? All further theoretical analysis appears to rely on this definition. But the definition itself is not fully justified.\n\n4. Lemma 3.1: \u201cWe can substitute L with its expected value E[L].\u201d Substitute L into where? What are the assumptions used for this Lemma?  The proof in Sec A.3 mentions \u201csmall coefficient of variation\u201d. If this is needed, please explicitly state it as an assumption in the Lemma.\n\n5. L217. Inference time is defined as $T = T_{M_1} + T_{D_2}$. Could you please define  $T_{M_1}$ and $T_{D_2}$? What is the unit of time? Is the inference time for one input sequence, or for the whole test set?\n\n6. Theorem 3.2: What is the meaning of the two conditions? What are their practical implications?\n\n7. Theorem 3.3 needs to be more precise. \u201cthe acceptance token length exhibits very low relative variability.\u201c Could you please quantify \u201cvery low\u201d and define \u201crelative variability\u201d? Is Theorem 3.3 proved?\n\n8. Figure 4: What does x-axis represent?\n\n\n**Important to fix but less severe than the major concerns above:**\n\n\n1. The words \u201cpolybasic\u201d and \u201cdualistic\u201d in the introduction are never defined. I think directly defining them and mentioning pros and cons in the introduction would be helpful. The current introduction does not tell the reader what the proposal is.\n\n\n2. L155: *\u201cAs proven in Appendix A.1 of speculative sampling, this method (speculative decoding) equates to sampling directly from the target LLM M_q.\u201d* A.1 is proving an entirely different statement. Specifically it proves the mean of a truncated geometric distribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to improve the speculative decoding (SD) process by generalizing the original SD framework that involves two models in the role of the draft and target model, respectively. This paper focused on leveraging multiple draft models to improve the speedup realized by the SD process. The paper claims to develop a solid theoretical framework to design the proposed polybasic SD with multiple draft models. It showcases significant speed-up on various benchmarks, including MT-bench, mathematical reasoning, summarization, translation, question-answering, and retrieval-augmented generation tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The paper aims to improve the existing SD framework by utilizing multiple models as potential draft models.\n2) The paper aspires to develop a theoretical foundation for SD and utilize it to guide the design of the polybasic SD method.\n3) On a wide range of tasks, the proposed polybasic SD method showcases impressive inference speedup."
            },
            "weaknesses": {
                "value": "Given that the paper claims the theoretical foundation for SD as one of their key contributions, the reviewer found the presentation and results in the paper somewhat underwhelming. Improving the presentation of existing results by properly setting up notation, formalizing theoretical statements, and rigorously justifying various assumptions is essential to improve the quality of a future submission. Some of the comments in this direction are as follows:\n\n1) The authors may want to first provide a clear and brief outline of what they want to establish via their theoretical analysis before stating their results. \n2) Section 3.1 starts by assuming that accepted token lengths follow a Gaussian distribution. What is the justification for this? Is this done only to make the analysis simpler? Without a rigorous justification, it is not clear if the rest of the analysis provides any realistic insight. Could authors assume this distribution to be a more plausible discrete distribution? \n3) The authors state that by empirical analysis they obtain a precise formulation for  $\\phi_i$. There are no details provided regarding this analysis. If the results heavily rely on this precise formulation, it needs to be further explained and justified.\n4) Please formally define *acceleration ratio* in Section 3.1.\n5) Please define what $T_{M_1}$ and $T_{D_2}$ represent in line 218. Similarly, please define what $T_i$ vs. $T'_i$ and $L_i$ vs $L'_i$ are in Theorem 3.2\n6) In Line 273, the authors talk about \"...unstable acceptance token lengths...\", which is not formally defined so far. \n7) Please make the main result of Theorem 3.3 formal. In its present form, the theorem makes a qualitative statement that needs to be formalized.\n\nThe description of the main algorithm is not clear enough. For instance, Algorithm 1 does not seem to be utilizing $M_1$ anywhere.\n\nThe paper has multiple typos. Please consider performing a thorough proofread to fix the typos similar to the following:\n\n1) In the abstract (line 18), `..is then serve` --> `...then serve`? Also, please change `theorem` to `theorems` Or `serve` to `serves`. \n2) Line 189,  the *acceptance* token length --> the *accepted* token length?\n3) Please consistently use *lemma* or *Lemma* throughout the text.\n4) Line 423,  'We approach consistently...' --> 'Our approach consistently....'?\n5) Section 4.3 title: 'Limitation and discuss' --> 'Limitation and discussion'?\n6) Line 483, 'As show in Figure....' --> 'As shown in Figure...'?"
            },
            "questions": {
                "value": "Please see the weaknesses section above. In addition, the reviewer has the following questions:\n\nLine 53 discusses Chen et al. (2023b) and mentions that they only utilize a single draft model in conjunction with the target model. Looking at Figure 1 of https://arxiv.org/pdf/2312.11462, they do seem to be considering a horizontal cascade where different draft models are utilized with a single target model. Could you please expand your discussion of the related work and comment on this?\n\nLine 330 states \".....ensuring that $M_1$ and $M_2$ have comparable capabilities while maintaining the performance advantage of the original model\". Could the authors clarify what they mean here? Are we trying to deliberately lower the quality of $M_1$ so that it's comparable to $M_2$?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}