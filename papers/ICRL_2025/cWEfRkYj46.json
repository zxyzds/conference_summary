{
    "id": "cWEfRkYj46",
    "title": "Towards Homogeneous Lexical Tone Decoding from Heterogeneous Intracranial Recordings",
    "abstract": "Recent advancements in brain-computer interfaces (BCIs) and deep learning have made decoding lexical tones from intracranial recordings possible, providing the potential to restore the communication ability of speech-impaired tonal language speakers. However, data heterogeneity induced by both physiological and instrumental factors poses a significant challenge for unified invasive brain tone decoding. Particularly, the existing heterogeneous decoding paradigm (training subject-specific models with individual data) suffers from the intrinsic limitation that fails to learn generalized neural representations and leverages data across subjects. To this end, we introduce Homogeneity-Heterogeneity Disentangled Learning for Neural Representations (H2DiLR), a framework that disentangles and learns the homogeneity and heterogeneity from intracranial recordings of multiple subjects. To verify the effectiveness of H2DiLR, we collected stereoelectroencephalography (sEEG) from multiple participants reading Mandarin materials containing 407 syllables (covering nearly all Mandarin characters). Extensive experiments demonstrate that H2DiLR, as a unified decoding paradigm, outperforms the naive heterogeneous decoding paradigm by a large margin. We also empirically show that H2DiLR indeed captures homogeneity and heterogeneity during neural representation learning.",
    "keywords": [
        "brain-computer interfaces; speech decoding; tonal language; Homogeneity-Heterogeneity Disentanglement"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cWEfRkYj46",
    "pdf_link": "https://openreview.net/pdf?id=cWEfRkYj46",
    "comments": [
        {
            "title": {
                "value": "Response to Reviewer VGSo (2/2)"
            },
            "comment": {
                "value": "## Response to Reviewer VGSo (2/2)\n---\n\n**Q2:Why the size of codebook and dimension of codebook are set as the same?**\n\n**R**: The dimensions and size of the codebooks are flexible parameters, which we set to 256 by default, as this configuration offers an effective balance between reconstruction performance and the expressive capacity of the learned representations. A comprehensive ablation study on various codebook sizes and dimensions, presented in Table 3 of our manuscript, shows that the proposed approach maintains stable performance across different configurations, with the best results achieved using the default setting."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer VGSo (1/2)"
            },
            "comment": {
                "value": "## Response to Reviewer VGSo (1/2)\n---\n\n# Reply to Weakness\n\n**W1: Since lexical tone decoding is not a very common task in the brain decoding area, I think the task / problem setting needs to be first introduced and defined in the methodology part.**\n\n**R**: We thank Reviewer VGSo for this suggestion. We will add the problem definition of tone decoding in the Method section in both the revised manuscript and the camera ready version. \n\n**W2: The paper lacks analysis of the generalization ability of captured common representation, i.e. if the shared codebook of three subjects well generalize to the fourth subject, and whether any three of the four subjects lead to similar shard codebook. The authors also mention this issue in limitations, but I think the dataset containing four subjects is enough to conduct such experiments.**\n\n**R**: As suggested, we conducted an additional ablation study to evaluate the generalization ability of the common representation captured by the public codebook. Specifically, we tested whether the public codebook, learned from three participants, could generalize to a fourth participant. We followed the same experimental setup described in Sec. 4 with a leave-one-subject-out protocol.\n\nFirst, we applied the homogeneity-heterogeneity disentanglement (H2D) on three subjects to acquire a public codebook. Then, we pre-trained the fourth subject using this previously acquired public codebook, while keeping it frozen and updating only the private codebook. Finally, we performed neural decoding (ND) on the fourth subject. The decoding performance was compared with baselines and variants our proposed H2DiLR. The results are summarized in the table below:\n\n|     Method     | Pre-training |     Subject 1    |     Subject 2    |     Subject 3    |     Subject 4    |       Avg.       |\n|:--------------:|:------------:|:----------------:|:----------------:|:----------------:|:----------------:|:----------------:|\n|    SPaRCNet    |       -      |  34.94$\\pm$2.17  |  36.73$\\pm$5.88  |  27.10$\\pm$3.22  |  27.10$\\pm$1.43  |  31.47$\\pm$3.16  |\n|      FFCL      |       -      |  37.47$\\pm$2.40  |  37.71$\\pm$5.42  |  26.61$\\pm$3.18  |  29.47$\\pm$1.69  |  32.82$\\pm$3.17  |\n|     TS-TCC     |      CL      |  39.59 $\\pm$1.68 |  41.47 $\\pm$2.78 |  28.82 $\\pm$1.75 |  32.33 $\\pm$2.17 |  35.55 $\\pm$2.09 |\n|      CoST      |      CL      |  43.95 $\\pm$1.48 |  40.41 $\\pm$4.77 |  31.02 $\\pm$1.00 |  34.53 $\\pm$2.39 |  37.47 $\\pm$2.41 |\n| ST-Transformer |       -      |  39.02 $\\pm$2.39 |  37.22 $\\pm$4.68 |  27.51 $\\pm$2.11 |  29.63 $\\pm$2.98 |  33.35 $\\pm$3.04 |\n|    NeuroBERT   |      MM      |  42.20 $\\pm$1.86 |  43.10 $\\pm$2.76 |  29.80 $\\pm$1.98 |  32.65 $\\pm$2.89 |  36.94 $\\pm$2.37 |\n|      BIOT      |      MM      |  42.45 $\\pm$6.99 |  40.90 $\\pm$5.87 |  33.55 $\\pm$2.95 |  33.88 $\\pm$1.89 |  37.47 $\\pm$2.26 |\n|   H2DiLR ours  |  UPaNT+ H2D  | 49.06 $\\pm$ 2.15 | 47.84 $\\pm$ 1.81 |  39.18 $\\pm$1.68 |  38.61 $\\pm$1.49 |  43.67 $\\pm$1.78 |\n|   H2DiLR ours  |  UPaNT only  |  45.47 $\\pm$3.04 |  44.65 $\\pm$1.84 |  35.59 $\\pm$2.37 |  35.76$\\pm$1.18  |  40.37 $\\pm$2.11 |\n|   H2DiLR ours  |   $\\nu = 0$  |  43.61 $\\pm$2.12 |  42.15 $\\pm$1.63 |  34.26 $\\pm$1.51 |  35.92 $\\pm$1.43 |  38.98 $\\pm$1.67 |\n| Generalization ablation |  UPaNT + H2D | 45.98 $\\pm$ 2.94 | 42.86 $\\pm$ 2.66 | 35.81 $\\pm$ 1.56 | 36.47 $\\pm$ 2.49 | 40.28 $\\pm$ 2.41 |\n\nThe row labeled \"Generalization Ablation\" represents the ablation study, where we froze the public codebook after training it on three subjects and evaluated it on the fourth. We observe that even with the public codebook frozen for three subjects, decoding on the fourth subject still outperforms other baselines. In fact, the performance is better than our H2DiLR model trained with $\\nu = 0$ and is comparable to the performance achieved when trained with UPaNT only. This confirms that the representation captured by the public codebook can be effectively applied to a new subject.\n\nAdditionally, we noted that the performance improvement on Subject 2 was smaller compared to the other subjects. We hypothesize that this could be due to the distinct electrode implantation location for Subject 2, which likely differs from that of the other participants.\n\n\n# Reply to Question\n\n**Q1: The usage of non-invasive brain recordings is an emerging topic. What's the performance of H2DiLR in decoding EEG signal?**\n\n**R**: In addition to tone decoding using intracranial recordings, we demonstrated the applicability of H2DiLR for seizure prediction using non-invasive EEG signals (Appendix C.1). Beyond neurological signals, our model can be applied to general time-series data with a multi-channel setup, addressing data heterogeneity across different acquisition setups. This broader applicability extends the impact of our work to various domains within the community."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer ScW5"
            },
            "comment": {
                "value": "## Response to Reviewer ScW5 \n---\n\n# Reply to Weakness\n\n**W1: I feel that the authors limited themselves somewhat in specializing their approach to tone decoding only. I feel that the problem of disentangling subject specific effects from general ones is a wider problem with many downstream applications, most notably for semantic decoding, but also for problems of encoding as well. I would have liked to see a deeper exploration of this in the work.**\n\n**R**: Indeed, our proposed H2DiLR is a general-purpose framework for homogeneity-heterogeneity disentangled neural representation learning, capable of being applied to various neural decoding tasks. In this work, we focused on tone decoding as a proof of concept to validate the framework's effectiveness.\n\nFurthermore, to demonstrate the versatility of H2DiLR beyond tone decoding, we applied it to a seizure prediction task using non-invasive EEG signals. The results, detailed in Appendix C.1, further illustrate the framework's adaptability to diverse neural decoding applications.\n\n**W2 and Q1 : There is also disappointingly little exploration of the abundant neuroscience questions here - I would like to have seen a location-based ablation in addition to the parametric ablations of the method that showed which regions are most useful for tonal decoding. An analysis studying if there are region-specific differences to representational homogeneity (i.e. some regions tend to be more idiosyncratic to a particular subject) would also have been interesting, although I recognize that the chosen data setting, sEEG, has challenges in answering this question due to coverage differences.**\n\n**R**: Due to the dual-stage training paradigm of our H2DiLR framework and the challenges of differentiating through quantization during backpropagation, we cannot directly analyze the contributions of specific regions during neural decoding.\n\nTo address this, we conducted a separate analysis by combining the homogeneity-heterogeneity disentanglement (H2D) pre-training and neural decoding stages into a single-stage model. We applied cross-entropy loss with a Transformer decoder to quantized representations, using a straight-through estimator (STE) to approximate gradients. Please note, however, that this setup may introduce training instability and slight performance degradation; it is used solely to analyze region contributions.\n\nWe calculated channel saliency scores through class activation maps, normalized between 0 and 1, and aggregated them across brain regions linked to speech processing (e.g., Superior Temporal Gyrus, Precentral Gyrus, Postcentral Gyrus), as well as reference regions like the Angular Gyrus. Our findings are summarized below:\n\n|       Brain region      | Brain structure | Contribution |\n|:-----------------------:|:---------------:|:------------:|\n|         Thalamus        |   Subcortical   |     0.92     |\n|       Hippocampus       |   Subcortical   |     0.85     |\n|         Amygdala        |   Subcortical   |     0.83     |\n|        Precentral gyrus |     Cortical    |     0.97     |\n|          Insula         |     Cortical    |     0.85     |\n|       Postcentral gyrus |     Cortical    |     0.94     |\n|  Inferior Frontal Gyrus |     Cortical    |     0.90     |\n| Superior Temporal Gyrus |     Cortical    |     0.85     |\n|   Supramarginal Gyrus   |     Cortical    |     0.60     |\n|      Angular Gyrus      |     Cortical    |     0.66     |\n\nOur analysis indicates that the Precentral and Postcentral Gyri are key contributors among cortical regions, with subcortical regions like the Thalamus also playing a significant role in tone decoding.\n\n# Reply to Question\n\n**Q2:Is there any benefit of this method for neural encoding models that predict neural responses from feature input?**\n\n**R**: Thank you for this question. In encoding tasks, while different subjects may produce distinct neural responses to the same stimuli due to physiological differences (heterogeneity), similar neural patterns can still emerge across individuals as a result of shared encoding mechanisms (homogeneity). By disentangling these heterogeneous and homogeneous components, H2DiLR can capture both individual-specific and shared neural response characteristics, potentially enhancing the model's ability to generalize across subjects in encoding tasks."
            }
        },
        {
            "summary": {
                "value": "This paper introduces the Homogeneity-Heterogeneity Disentangled Learning for Neural Representations (H2DiLR), a framework designed to improve neural decoding by separating homogeneous and heterogeneous components from intracranial recordings across multiple subjects. The study demonstrates that H2DiLR significantly enhances tone decoding performance in Mandarin-speaking participants using stereoelectroencephalography (sEEG) data. The framework outperforms traditional methods by effectively capturing and leveraging both shared and subject-specific neural features."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The introduction of the Homogeneity-Heterogeneity Disentangled Learning for Neural Representations (H2DiLR) is a novel approach that effectively addresses the challenge of data heterogeneity in neural decoding. By disentangling shared and subject-specific neural features, the framework enhances the decoding accuracy across multiple subjects.\n\nThe study employs a robust experimental setup using stereoelectroencephalography (sEEG) recordings from multiple Mandarin-speaking participants.\n\nThe proposed method demonstrates a substantial improvement in tone decoding accuracy over traditional subject-specific models."
            },
            "weaknesses": {
                "value": "The statement that a \"comprehensive set of 407 Mandarin syllables covers nearly all Mandarin characters\" requires further clarification. You need to provide the specific vocabulary list used in the rebuttal materials to support this claim.\n\nThe concept of private and shared codebooks is introduced in the paper but lacks a detailed explanation. A more comprehensive rationale and description are needed to clarify how and why these codebooks are defined and utilized.\n\nSharing the data and code would greatly enhance the validation and reproducibility of the proposed method by the research community. This transparency would contribute significantly to the field and support further advancements.\n\nThe claim of being \"the first\" to work with 407 Mandarin syllables is inaccurate, as the 2023 study titled \"A high-performance brain-to-sentence decoder for logosyllabic language\" also mentions using a dataset with 407 syllables. Although this work is cited, the related work section lacks a detailed discussion of this study and how the current work differentiates itself.\n\nThe paper asserts that it disentangles homogeneity and heterogeneity, yet the evidence provided relies on feature visualization of specific samples. A statistically robust analysis is necessary to substantiate this claim. \n\nWhile the framework allows for joint training across multiple subjects, the use of separate encoders for each subject may significantly increase computational costs, which should be addressed and optimized."
            },
            "questions": {
                "value": "Can you provide the specific list of 407 Mandarin syllables and explain how they represent nearly all Mandarin characters?\n\nHow and why are the private and shared codebooks defined and utilized within your framework?\n\nHave you considered the computational cost implications of using separate encoders for each subject, and are there plans to optimize this aspect?\n\nHow does your work differentiate from the 2023 study \"A high-performance brain-to-sentence decoder for logosyllabic language,\" which also uses a dataset with 407 syllables?\n\nWhat is the total duration of the sEEG data in the dataset you provided?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper discusses a method called Homogeneity-Heterogeneity Disentangled Learning for neural Representations (H2DiLR) for decoding tone from stereoelectroencephalography (sEEG) data.  H2DiLR disentangles shared (homogeneous) and individual-specific (heterogeneous) neural representations using a two-stage learning paradigm. The first stage involves unsupervised learning of neural features via vector quantization with shared and private codebooks, while the second stage applies these representations to decode tones using a transformer model\u200b. The authors claim that H2DiLR outperforms previous baseline methods by 12% when evaluated on Top-1 accuracy."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The topic of tone decoding is understudied, if somewhat narrow, and the idea to separate homogeneous and heterogeneous neural representations is interesting and could have use cases outside the specialized setting of tone decoding, if applied to semantic neural representations more generally. Overall, I think the idea is interesting enough to be in the \"borderline accept\" category of papers, mainly because I think the idea is general enough to have practicable value outside this particular application."
            },
            "weaknesses": {
                "value": "I feel that the authors limited themselves somewhat in specializing their approach to tone decoding only. I feel that the problem of disentangling subject specific effects from general ones is a wider problem with many downstream applications, most notably for semantic decoding, but also for problems of encoding as well. I would have liked to see a deeper exploration of this in the work. There is also disappointingly little exploration of the abundant neuroscience questions here - I would like to have seen a location-based ablation in addition to the parametric ablations of the method that showed which regions are most useful for tonal decoding. An analysis studying if there are region-specific differences to representational homogeneity (i.e. some regions tend to be more idiosyncratic to a particular subject) would also have been interesting, although I recognize that the chosen data setting, sEEG, has challenges in answering this question due to coverage differences."
            },
            "questions": {
                "value": "See weaknesses. What areas of the brain are most important to tone decoding? Does the exclusion of data from some electrodes have more of an impact than others? Is there any benefit of this method for neural encoding models that predict neural responses from feature input?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce a neural network architecture, H2DiLR, a self-supervised learning (SSL) method for pre-training models for sEEG tone decoding (4-way classification task) and a novel dataset (407-syllable reading sEEG dataset).\n\n**Dataset**: The patients are asked to read 407 syllables aloud. To make their pronunciation as close to natural speech as possible, carrier words are added to form full sentences. The dataset includes `1221` trials (i.e., 3 trials per syllable). Simultaneously recorded audio signals are then used to precisely identify time stamps for extracting the target syllable from each trial. Each trial is truncated based on these time stamps, resulting in a neural recording of up to `1` second, labeled with one of `4` tones. According to the task paradigm demonstrated in [1], the total recording time for each subject is approximately `5` hours, with each trial lasting around 12 seconds. Therefore, the total amount of pre-training dataset is about `20` hours.\n\n**Model**: They introduce two different codebooks to disentangle the shared and private parts of multiple subjects. During the pre-training stage, they reconstruct the original sEEG signals. After pre-training, they use the frozen pre-trained VQ-Encoder to obtain quantized embeddings of the input sEEG signals. These embeddings are then passed through a temporal Transformer for further temporal integration, followed by a classification head for tone decoding.\n\n**Experiments**: Previous pre-training methods (including BIOT, NeuroBERT, etc.) are compared. Besides, the authors conducted different ablation studies regarding pre-training (partition ratio, # of subjects, etc.)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Significance**: Open-source sEEG speech datasets are rare. Their promise of publishing the dataset upon acceptance (Line 780) is good news for the community as it will lower the entry threshold for future research. Additionally, they demonstrate how SSL-based pre-training allows improving performance compared to only supervised training.\n\n**Clarity**: The text has a good structure and is well-written. The figures also help in understanding the method."
            },
            "weaknesses": {
                "value": "**Major**\n1. The authors don\u2019t mention the introduction of data augmentation (e.g., temporal jittering, additive noise, etc.) to avoid overfitting, a common strategy used in sEEG-based speech decoding [2,3]. Since the number of available trials (i.e., $\\sim$1221 trials) within each subject is too small, I\u2019m not sure whether the improvement of pre-training comes from this.\n\n2. The preprocess details are missing. Could you provide a detailed description of sEEG preprocessing (i.e., from the originally collected sEEG signals at a sample rate of 2kHz to the epoched trials at a sample rate of 1kHz)?\n\n3. In [1], their CNN baseline achieves about 40% on average, and their NAR model achieves 45.75% on average. All these methods are supervised (heterogeneous) baselines, and these results make it hard to assess the contribution of H2DiLR.\n\n4. No code and demo dataset is provided. Without the code and data, it's difficult to verify the claims.\n\n5. Some aspects of the method are undefined or unclear. Please see the Questions section below. These aspects need to be clarified in the manuscript.\n\n**Minor**\n1. Figure 1: It could mislead readers into thinking that H2DiLR uses Product Quantization. In Figure 3 and Section 3.3, the authors actually quantize each item in the embedding sequence using one of different codebooks. However, in Figure 1, the \u201cdisentangled embedding\u201d suggests that different parts of each individual embedding are quantized with different codebooks.\n\n2. Line 155: typo \u201cLaBraM Anonymous (2024)\u201d\n\n3. Line 841: typo \u201cthe learning of 5e-5\u201d should be \u201cthe learning rate of 5e-5\u201d? Is the only difference between fine-tuning pre-trained models and training non-pretrained baselines the learning rate? Are all other settings the same?\n\n4. Use `\\citet` or `\\citep` instead of `\\cite` when referencing other works.\n\n5. Since you collected a 407-syllable reading sEEG dataset, how does H2DiLR perform on other types of syllable classification tasks, such as initial syllable classification? Including these additional results might further highlight H2DiLR's effectiveness.\n\n**Reference**\n\n[1] Feng C, Cao L, Wu D, et al. A high-performance brain-sentence communication designed for logosyllabic language[J]. bioRxiv, 2023: 2023.11. 05.562313.\n\n[2] Metzger S L, Littlejohn K T, Silva A B, et al. A high-performance neuroprosthesis for speech decoding and avatar control[J]. Nature, 2023, 620(7976): 1037-1046.\n\n[3] Willett F R, Kunz E M, Fan C, et al. A high-performance speech neuroprosthesis[J]. Nature, 2023, 620(7976): 1031-1036."
            },
            "questions": {
                "value": "1. Table 1: What is the codebook size of Heterogeneous H2DiLR with ($\\nu=0$)? Is it `K=32`?\n\n2. Did you use a different signal length for pre-training and downstream classification? Are there any differences between the pre-training dataset and the downstream classification dataset?\n\n3. Line 879: Did you apply normalization to each channel independently (i.e., calculating the mean and standard deviation for each channel separately) or across all channels together?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose H2DiLR to capture both homogeneous and heterogeneous information from intracranial recordings across multiple subjects for lexical tone decoding. The H2DiLR framework first obtains discrete vectors through a reconstruction task (vq-autoencoding manner), then builds shared codebook and private codebook for the decoding of different subjects. Experiments on a newly acquired, not publicly released sEEG dataset show the H2DiLR method outperforms other baseline methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The motivation of this paper is clear. The authors seek to address a very important problem in brain decoding: dig homogeneity within the brain responses of different subjects, and hope the obtained homogeneity representation can help improve decoding performance. The presentation of this paper is also good.\n2. The proposed vq-autoencoding based method outperforms previous models in decoding performance. Additional ablation analysis further confirms the effectiveness of model design.\n3. The authors build a new sEEG dataset (promise to release if this paper gets accepted) which has potential to boost researches related to lexical tone decoding."
            },
            "weaknesses": {
                "value": "1. Since lexical tone decoding is not a very common task in the brain decoding area, I think the task / problem setting needs to be first introduced and defined in the methodology part.\n2. The paper lacks analysis of the generalization ability of captured common representation, i.e. if the shared codebook of three subjects well generalize to the fourth subject, and whether any three of the four subjects lead to similar shard codebook. The authors also mention this issue in limitations, but I think the dataset containing four subjects is enough to conduct such experiments."
            },
            "questions": {
                "value": "1. The usage of non-invasive brain recordings is an emerging topic. What's the performance of H2DiLR in decoding EEG signal?\n2. Why the size of codebook and dimension of codebook are set as the same?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The dataset is related to human subjects, and has not released yet."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}