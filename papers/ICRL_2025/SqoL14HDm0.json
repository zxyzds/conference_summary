{
    "id": "SqoL14HDm0",
    "title": "When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust \"APIs'' for Human-AI Interaction",
    "abstract": "As large language models (LLMs) gain increasing capabilities, they are being widely applied in areas such as intelligent customer service, code generation, and knowledge management. Prompts written in natural language (NL) serve as the \"APIs'' for human-LLM interaction. To enhance prompt quality, best practices for prompt engineering (PE) have been established, along with writing guidelines and templates. However, due to the inherent ambiguity of natural language, even prompts that strictly follow these guidelines often fail to trigger LLM to consistently output high quality responses, particularly for complex tasks. To address this issue, this paper proposes a Controlled Natural Language for Prompt (CNL-P) which incorporates best practices in PE. To overcome the NL's ambiguity, CNL-P introduces precise grammar structures and strict semantic norms, enabling a declarative but structured and accurate representation of user intent. This helps LLMs better understand and execute CNL-P, leading to higher quality responses. To lower the learning curve of CNL-P, we introduce an automatic NL2CNL-P conversion agent based on LLMs, which allow users to describe prompts in NL from which the NL2CNL-P agent generates CNL-P compliant prompts guided by CNL-P grammar. We further develop a linting tool for CNL-P, including syntactic and semantic checks, making static analysis techniques applicable to natural language for the first time. CNL-P\u2019s design not only integrates best practices in PE but also adopts key principles from software engineering (SE). Extensive experiments show that CNL-P can improve the quality of LLM's responses through the novel and organic synergy of PE and SE. We envision that CNL-P has the potential to bridge the gap between emergent PE and traditional SE, paving the way for a new natural language centric programming paradigm.",
    "keywords": [
        "large language model",
        "human-LLM interaction",
        "API",
        "prompt engineering",
        "software engineering",
        "controlled natural language"
    ],
    "primary_area": "infrastructure, software libraries, hardware, systems, etc.",
    "TLDR": "By integrating prompt engineering and software engineering principles,this paper introduces a controlled natural language called CNL-P to improve the quality of LLM's responses.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=SqoL14HDm0",
    "pdf_link": "https://openreview.net/pdf?id=SqoL14HDm0",
    "comments": [
        {
            "summary": {
                "value": "- This paper proposes the use of Controlled Natural Language (CNL) by framing prompts as a form of API, which allows users to harness AI model capabilities without needing in-depth technical knowledge.\n- This work applies software engineering (SE) principles such as modularity, abstraction, and encapsulation to Controlled Natural Language (CNL), offering a structure that decouples prompts from code\n- The authors conducted experiments to evaluate how effectively CNL-P adheres to design principles and whether CNL-P or template methods improve the quality of LLM responses."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Clearly motivated framing of prompting as an API, enabling users to leverage AI model capabilities without extensive technical expertise.\n- Strong connection to first principles in SE, providing a foundation to address challenges in complex NL-PL conversion and prompt-code coupling. This approach is particularly beneficial for language experts and non-technical users by effectively decoupling prompts from code.\n- Dimensions to assess NL-to-CNL-P conversion quality are well-designed, covering diverse quality aspects."
            },
            "weaknesses": {
                "value": "- The specific aims of the work remain unclear; while high-level challenges and design considerations are presented, the precise goals are hard to identify.\n- Experiment setup in RQ1 lacks clarity on how the five dimensions are measured and how the 93 prompt instances were chosen. There are also no human validation results presented, even as partial samples.\n- RQ1 primarily assesses design considerations, while RQ2 focuses on accuracy. Given the current setup and task scope in RQ2, the advantages of CNL-P are not fully apparent, as other models also perform well.\n- For a more robust finding that CNL-P is better suited for weaker models, it would be beneficial to include additional experiments with weaker models beyond GPT-4-o mini."
            },
            "questions": {
                "value": "- Considering the setup and the consistency of output generation with single-turn GPT-4-o prompting, what advantages does CNL-P offer over single-turn generation with GPT-4-o?\n- Given the broad goals of this work and the multi-faceted design of CNL-P, what rationale led the authors to focus on these three specific research questions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Controlled Natural Language for Prompt (CNL-P), a novel framework that bridges prompt engineering (PE) and software engineering (SE) principles to enhance the clarity, predictability, and effectiveness of prompts for large language models (LLMs). CNL-P addresses inherent ambiguities in natural language prompts by formalizing grammar structures and semantic norms. The work's primary theoretical contribution is the formalization of prompt engineering through SE principles, supported by a novel static analysis approach for natural language prompts and empirical validation across multiple LLM architectures."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "Theoretical Innovation:\n- Novel synthesis of SE principles with PE practices\n- Comprehensive formal grammar for controlled natural language\n- Innovative application of static analysis theory to natural language\n\nTechnical Contribution\n- Formal specification of the CNL-P grammar\n- Theoretical framework for prompt verification\n- Rigorous performance analysis across LLM architectures\n- Novel approach to static analysis of natural language\n\nResearch Impact:\n- Opens new theoretical directions in prompt engineering\n- Bridges formal methods and LLM interaction\n- Provides a foundation for analyzing prompt properties\n- Advances understanding of structured approaches to PE"
            },
            "weaknesses": {
                "value": "Theoretical Limitations:\n- Formal analysis of expressive power could be stronger.\n- Completeness properties of the static analysis need more discussion.\n- Edge cases in the formal grammar require deeper analysis.\n- Theoretical bounds need more rigorous treatment.\n\nMethodological Concerns:\n- Formal comparison with other structured approaches could be deeper.\n- Statistical analysis could be more comprehensive.\n- Theoretical justification for design choices needs elaboration.\n- Formal properties of the conversion process require more analysis.\n\nValidation Gaps:\nLimited formal analysis of grammar properties.\nStatistical significance analysis could be more rigorous.\nTheoretical comparison with other formal methods needed.\nCompleteness of the static analysis approach not fully addressed."
            },
            "questions": {
                "value": "Comparative Evaluation: Could you provide a more detailed comparison of CNL-P\u2019s functionality and usability versus DSPy, LangChain, and Semantic Kernel? Specifically, how does CNL-P\u2019s approach to modularity and state management differ in terms of user accessibility and technical demands?\n\nAdvantages and Trade-offs: While CNL-P is designed to decouple prompts from code for accessibility, frameworks like DSPy offer robust control through tight integration with programming language abstractions. Could you discuss specific scenarios where CNL-P might outperform DSPy or vice versa, especially in terms of prompt complexity and user involvement?\n\nNon-Technical Accessibility: CNL-P is described as more accessible to non-technical users than PL-based methods like DSPy and LangChain. Could you elaborate on any studies, tests, or qualitative comparisons you conducted to evaluate this claim? This would clarify the extent to which CNL-P lowers the barrier for non-programmers.\n\nPerformance in Practical Applications: Do you have insights or preliminary results comparing the performance and user experience of CNL-P with DSPy, LangChain, and Semantic Kernel in specific application areas (e.g., dynamic prompt generation or complex workflow management)? Real-world examples could strengthen the practical context of CNL-P\u2019s advantages.\n\nFuture Integration with PL-Based Methods: Given that DSPy and other PL-based frameworks emphasize structured programming benefits, do you foresee potential for CNL-P to integrate with or complement these frameworks? A discussion on interoperability could highlight pathways for combining strengths across approaches.\n\nScoring and Evaluation: Can you elaborate on the specific criteria used to assign scores across the five evaluation dimensions (Adherence to Original Intent, Modularity, Extensibility and Maintainability, Readability and Structural Clarity, and Process Rigor)? Was there a weighting system applied to these dimensions, or were they treated as equally important?\n\nInterpretability of Results: The table of results is challenging to interpret due to its minimal contextual information. Could you provide a more detailed breakdown or rubric that explains how scores were derived, potentially with examples of how different prompt types scored across specific dimensions?\n\nComparative Analysis: Did you consider using statistical measures to compare the performance of CNL-P, RISEN, and RODES across evaluation metrics? This could strengthen the validity of the reported improvements.\n\nPresentation Improvements: The experimental results could benefit from a more visual presentation format, such as radar charts or bar graphs, for easier comparison across dimensions. Would you consider updating the results presentation in a revised version?\n\nCompleteness of Scoring Process: Did you perform any error analysis or additional validation to understand how CNL-P performs in specific scenarios where RISEN or RODES may excel, or vice versa? This would provide insight into potential edge cases for CNL-P."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Controlled Natural Language for Prompt (CNL-P), a novel prompt language to elicit high-quality responses from large language models (LLMs). CNL-P combines principles from software engineering with prompt engineering to create structured, accurate natural language prompts for interacting with LLMs. By defining clear syntax and components such as Persona, Constraints, Variables, and Workflow, CNL-P reduces natural language ambiguity and increases response consistency. Its structured, modular format allows for more reliable, maintainable, and interpretable prompts, acting as a natural \u201cAPI\u201d that makes human-AI interaction accessible and robust. In addition, the author proposed an NL2CNL-P agent to convert natural language prompts into CNL-P format allowing users to write prompts in natural language without expertise in learning syntax of CNL-P. A linting tool is proposed to check the syntactic and semantics of CNL-P. The experiments demonstrate the effectiveness of CNL-P in improving the consistency of LLM responses across various tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper effectively combines prompt engineering and software engineering to introduce CNL-P as a structured, precise language for prompt design.\n2. CNL-P\u2019s modular design enables independent development, testing, and maintenance.\n3. Its linting tool supports syntactic and semantic checks, which enables static analysis techniques for natural language."
            },
            "weaknesses": {
                "value": "I have several concerns regarding the evaluation section:\n1. For RQ1, the authors asked ChatGPT-4o to assess the quality of conversions from natural language prompts to CNL-P or NL style guides based on five criteria. However, the reliability of this evaluation is not properly validated:\n    - The authors did not provide evidence of how the evaluation results correlate with actual human evaluations, which would strengthen their claims. \n    - There is no guideline detailing how the scale for each category is defined, which makes it difficult to interpret the numbers in Table 1.\n2. For RQ2, the experiment lacks comprehensiveness:\n    - Diversity of tasks: Currently, all tasks fall under the classification category. It would be beneficial to include more complex tasks, such as reasoning and coding, to better demonstrate the effectiveness of CNL-P.\n    - The authors should conduct a more thorough evaluation across a broader range of both open-source and closed-source LLMs to validate the effectiveness of CNL-P.\n    - Insufficient error analysis of CNL-P across various LLMs and tasks: The performance of CNL-P varies among different models/tasks. The claim that weaker models benefit more from CNL-P lacks thorough discussion and validation. A comprehensive analysis should include task difficulty, the quality of natural language prompts, the quality of CNL-P, and their relationship to the task performance.\n3. Generalization of CNL-P:  \n    - As all the tasks in the experiment are classification tasks, the natural language prompts should not be overly complex (correct me if I am wrong). Consequently, the paper does not sufficiently address how CNL-P performs with very large and complex prompts. It also fails to clarify whether the linting tool can handle such complex CNL-P.\n4. In lines 789-799, the natural language prompt appears well-organized and detailed. I would like to ask:\n    - For an effective CNL-P prompt, is such a level of detail and organization required from the human input?\n    - How does the organization of the natural language prompt impact the quality of the converted CNL-P prompt?\n    - Does the CNL-P prompt still outperform a well-organized natural language prompt?\n5. The plots and figures for result analysis should be integrated into the relevant pages or paragraphs; otherwise, it is hard to follow the discussion and analysis."
            },
            "questions": {
                "value": "1. Which Llama model was used in your experiment?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}