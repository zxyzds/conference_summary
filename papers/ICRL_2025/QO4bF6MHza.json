{
    "id": "QO4bF6MHza",
    "title": "MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs",
    "abstract": "Recent large language models (LLMs) have demonstrated versatile capabilities in long-context scenarios. Although some recent benchmarks have been developed to evaluate the long-context capabilities of LLMs, there is a lack of benchmarks evaluating the mathematical reasoning abilities of LLMs over long contexts, which is crucial for LLMs' application in real-world scenarios. In this paper, we introduce MathHay, an automated benchmark designed to assess the long-context mathematical reasoning capabilities of LLMs. Unlike previous benchmarks like Needle in a Haystack, which focus primarily on information retrieval within long texts, MathHay demands models with both information-seeking and complex mathematical reasoning abilities. We conduct extensive experiments on MathHay to assess the long-context mathematical reasoning abilities of eight top-performing LLMs. Even the best-performing model, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over long contexts, achieving only 51.26\\% accuracy at 128K tokens. This highlights the significant room for improvement on the MathHay benchmark.",
    "keywords": [
        "Mathematical Reasoning",
        "Large Language Models",
        "Benchmark",
        "Long-Context Modeling"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We introduce MathHay, an automated benchmark designed to assess the long-context mathematical reasoning capabilities of LLMs.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=QO4bF6MHza",
    "pdf_link": "https://openreview.net/pdf?id=QO4bF6MHza",
    "comments": [
        {
            "summary": {
                "value": "In this paper, the authors introduce MathHay, a benchmark and data collection pipeline designed to evaluate the mathematical reasoning capabilities of LLMs over extended contexts. Unlike existing benchmarks, MathHay incorporates varied context lengths and introduces multiple documents (ranging from one to three) to assess the models\u2019 ability to filter relevant information from noise. Additionally, it includes multi-step tasks (up to three arithmetic steps) to measure reasoning depth. The benchmark also spans a range of topics, from financial market analysis to agricultural economics. Leading LLMs, including GPT-4, o1-preview, and Gemini-1.5-Pro-002, have been tested, with empirical results indicating that there is still room for improvement in current models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- relatively written paper, easy to follow\n- an automated and scalable way to construct a Denmark \n- I like the way of using time-sensitive Q&As to mitigate the contamination problem\n- the investigation is the impact of time period is interesting, which might motivate further research about the data contamination problem in LLMs."
            },
            "weaknesses": {
                "value": "- More quantitative and qualitative evaluation may be needed on the quality of the automated generated corpus."
            },
            "questions": {
                "value": "- I\u2019m curious if the topics and tasks affect the mathematical reasoning performance of LLMs?\n- In the classic Needle-in-a-Haystack paper (Kuratov et al., 2023), a pattern emerged where models struggled with retrieval when the \u2018needle\u2019 appeared near the beginning but not in the first sentence. However, I haven\u2019t observed any similar pattern in mathematical reasoning\u2014only a general decline in performance as the context window expands, regardless of the needle\u2019s position. This is expected and not particularly insightful. Do you have any educated guesses as to why?\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents MATHHAY, an automated benchmark developed to evaluate the long-context mathematical reasoning capabilities of large language models (LLMs). The benchmark construction involves four main stages: document collection, question generation, quality control, and haystack construction. Real-world mathematical reasoning documents are gathered, and four types of tasks with increasing complexity are generated\u2014ranging from single-step, single-document tasks to multi-step, multi-document tasks. A quality control process ensures data reliability by verifying solution consistency, and a haystack structure is created to challenge models with relevant documents embedded in noisy text. The contributions include: (1) a method to automatically generate a high-quality benchmark for long-context mathematical reasoning, (2) the introduction of MATHHAY with tasks of varying difficulty levels to evaluate reasoning abilities across different input lengths, and (3) experimental findings showing that existing LLMs face significant challenges in handling mathematical reasoning over long contexts, indicating room for improvement in this area."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1.\tMATHHAY employs an automated quality control and evaluation process, combining exact match and LLM-based assessment, which achieves high correlation with human evaluations. This automation facilitates scalable and efficient testing for future LLM development.\n2.\tThe study provides in-depth insights into how factors like document placement depth, input length, reasoning step count, time period, and topic influence model performance. This multi-angle approach highlights specific challenges for LLMs and provides a nuanced view of model capabilities and limitations."
            },
            "weaknesses": {
                "value": "1.\tThe quality control process relies on consistency across multiple solutions generated by the model, but this does not fully ensure mathematical correctness. Consistent solutions may still be incorrect, resulting in potential \"false positives\" in data quality. Without manual verification or deeper validation, the benchmark may include examples that lack rigor, reducing its reliability for assessing true mathematical reasoning abilities.\n2.\tThis approach relies heavily on adding noise and complex placements to increase task difficulty, which may primarily test the model\u2019s noise filtering rather than its core mathematical reasoning abilities. The setup could overly focus on retrieval challenges, potentially overlooking deeper aspects of mathematical understanding and reasoning.\n3.\tThe benchmark lacks clarity on ensuring that multi-step tasks genuinely require multiple steps. If questions can be solved with either single-step or multi-step solutions, it may lead to inconsistent categorization.\n4.\tThe benchmark does not address how it ensures that generated questions are genuinely related to document content. If the model generates irrelevant or \u201challucinated\u201d information, it could compromise the quality and reliability of the generated task."
            },
            "questions": {
                "value": "1.\tConduct Experiments on Noise Impact: Add experiments that analyze how the model's reasoning ability is affected as the amount of irrelevant (noise) documents gradually increases. This can provide a more nuanced understanding of how noise impacts long-context reasoning performance.\n2.\tIncorporate Diverse Reasoning Scenarios: Design experiments that assess different types of long-context reasoning skills (e.g., logical deductions, multi-step calculations) across varied noise levels, ensuring that the benchmark evaluates both retrieval and reasoning capabilities in a balanced way.\n3.\tThe paper only introduces four specific combinations for placing three relevant documents in reasoning tasks (First-First-First, Middle-Middle-Middle, Last-Last-Last, and First-Middle-Last). Why doesn\u2019t it explore all possible combinations of placements?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a method to generate long-context mathematical reasoning benchmarks for evaluating LLMs' capabilities of information extraction and mathematical reasoning under certain long-context noisy and question-irrelavant circumstance. This paper also evaluates the performance of several LLMs on proposed benchmark MathHay."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Proposed benchmark in thie paper is tailored  with varying context length, reasoning steps and spectrum noisy or irrelevant content in document to measure  comprehensive performance in LLMs\n2. existing method in mathmatical reasoning are difficult to achieve good performance, which reflects the necessity of this benchmark and the high difficulty of mathematical reasoning and information extraction."
            },
            "weaknesses": {
                "value": "1. Proposed methodology could not be considered as a contribution.The Architecture shown in Figure 1 is not novel while the process is well represented.\n2. No quantitative description of the quality of the answers to the questions is given in quality control pipline.\n3. The article does not provide specific details of the reasoning steps.\n4. The experimental part is not detailed enough, only describing different methods, without showing the connection between the benchmark and existing methods. Therefore, experiment is too simple to be a contribution in this paper."
            },
            "questions": {
                "value": "1.Will the benchmark be updated in the future? \n2. How did you do to filter documents? How  filtering step numerical content  is done?\n3. When consistent answers were incorrect?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces an automated benchmark MATHHAY for accessing the long-context mathematical reasoning capabilities of LLMs. Unlike previous benchmarks like Needle in a Haystack, which focuses primarily on information retrieval within long texts, MATHHAY demands models with both information-seeking and complex mathematical reasoning abilities. Experiments are conducted on 8 top-performing LLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper presents a novel approach to evaluating long-context mathematical reasoning in LLMs, addressing a gap in existing benchmarks. \n* The paper is well-structured and clearly written. The methodology is explained in detail, and the experimental setup and results are presented in a way that is easy to follow.\n* Experiments are conducted on 8 top-performing LLMs. Rich analyses are included."
            },
            "weaknesses": {
                "value": "Please see the questions below."
            },
            "questions": {
                "value": "* How does this method ensure the alignment between the question and the Python solution? \n* Concern about how to cover if a question has multiple correct solutions, especially based on the quality control strategy?\n* Is there an evaluation method to check if the generation question meets the SSSD/MSSD/SSMD/MSMD tasks? \n* The evaluation relies on GPT-4o performance, which is not sound enough. Moreover, 100 examples is too small scale for human evaluation. \n* In Table 2, the verified two-step questions and three-step questions are 0. Could you please explain this? And what are the standards for selecting the 126 verified questions?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a new benchmark for numerical reasoning in long contexts. The benchmark consists of 673 numerical questions, and each question is associated with a set of documents. Some questions can be answered by examining only one document from the set, others need information from multiple documents. Some questions need one arithmetic operation, others need up to 3 steps.\n\nThe authors generated the benchmark data semi-automatically, and then evaluated various LLMs with it. They point out that no LLMs were able to answer more than half the questions correctly, leading them to the conclusion that this benchmark would be useful in tracking the capabilities of LLMs."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The benchmark and the methodology to generate it are clearly presented.\nThe experiments show that the benchmark has sufficient value for helping evaluate future LLMs."
            },
            "weaknesses": {
                "value": "The benchmark is rather small (673 questions), only 20% of which were verified by humans. So in effect, LLMs are evaluating other LLMs!\n\nWhile the paper repeatedly talks about mathematical reasoning, the benchmark only tests very simple numerical calculations. It does not test true mathematical ability, such as solving problems in a math textbook. Documents are selected if they contain many numbers, as the questions are based on numbers.\n\nThe methodology for generating multi-step questions was not described clearly. Since the questions are machine generated, they tend to be rather trivial, and sometimes meaningless, as shown in the few examples in the Appendix."
            },
            "questions": {
                "value": "Will the benchmark become obsolete quickly? Will you be generating new versions frequently?\n\nDid you use LLMs to filter documents? Some numerical content may be in words, does your filtering step account for that?\n\nWhen you manually evaluate the answers, what percent of the consistent answers were incorrect? There is a significant gap in the performance on the verified and unverified sets. Also, the key statistics indicate that all the verified questions were single step, so how were the verified stats generated for MSSD and MSMD?\n\nSome typos or unclear statements.\nline 51: mathematic reasoning\n\nline 78: within a certain time period to support to form\n\nPage 19 - The documents have strange substrings like 0\u03060bb and Tesla2\u0306019s. Perhaps a copy/paste or LaTeX error?\n\nLine 1065: Figure 15: Examples of data for the Single-Step Single-Document (MSSD) task.\n\nLine 1121: Figure 16: Examples of data for the Single-Step Single-Document (SSMD) task.\n\nLine 1171: Figure 17: Examples of data for the Single-Step Single-Document (MSMD) task."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes an LLM-based methodology for generating mathematical benchmarks, composed of a question in some topic which involves a numeric answer which has to be calculated from a text provided as context to the question, together with the expected answer. It also generates a benchmark and evaluates the performance of several LLMs on it."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "In general, the paper is very well written. In particular, the introduction reads nicely and the goals are clearly and precisely stated. \n\nLooking for an automated methodology for generating benchmarks.\n\nThe partition of the difficulty of the tasks with varying number of operations and number of involved documents."
            },
            "weaknesses": {
                "value": "Related work reads like a survey but it is not clear the actual relation of the contribution of the paper with the cited work. In particular, the contribution with respect to InfiniteBenchAlmost all bibliographic references are from arXiv while there are (newer) published versions of some of the papers.\n\nThere is no clear evidence that the actual methodology could be considered to be a contribution. The Architecture shown in Figure 1 is reasonable but it does not seem to be a breakthrough. Several parts of the process are described in very vague way, in particular \u201cDocument filtering\u201d which is somewhat critical.\n\nThe proposed methodology  does not provision any mechanism for evaluating the quality of the generated questions. The quality control phase only mentions the evaluation of the answers with respect to the questions.\n\nThe generated problems does not seem to contain reasoning steps but only the question and the answer.\n\nExperiments focus on evaluating the performance of LLM on the constructed benchmark rather than on evaluating the relevance of the methodology and the benchmark themselves, which are claimed to be the main contribution.\n\nTherefore, there is a lack of adequate experimental evaluation to assess the performance of the methodology to generate relevant benchmarks."
            },
            "questions": {
                "value": "- Please clarify the differences with InfiniteBench and L-Eval which are the two benchmarks with long contexts and mathematical reasoning.\n\n- What does make your methodology a contribution and the generated benchmark a \u201cunique contribution\u201d to the area?\n\n- Give more details about document filtering, in particular the parameters used (number of distinct numerical values, etc.)\n\n- What do you precisely mean by \u201chigh-quality document\u201d? What is the metric used to evaluate it?\n\n- How do you evaluate the relevance of the questions?\n\n- The benchmark is said to be reviewed to ensure the correctness of the reasoning steps, however they are not explicit in the answer. Please explain how this is done.\n\nOther comments\n\nRevise and update references including published versions.\n\nThe sentence in Pg. 3, L. 132 \u201cOur benchmark \u2026 making it a unique contribution \u2026 \u201c does not appear to be clearly justified."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}