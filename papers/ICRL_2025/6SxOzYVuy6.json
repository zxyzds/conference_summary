{
    "id": "6SxOzYVuy6",
    "title": "DROSIA: Decoupled Representation on Sequential Information Aggregation for Time Series Forecasting",
    "abstract": "Time series forecasting is crucial in various fields, including finance, energy consumption, weather, transportation, and network traffic. It necessitates effective and efficient sequence modeling to encapsulate intricate temporal relationships. However, conventional methods often aggregate sequential information into representations of each time point by considering other points in the sequence, thereby ignoring the intra-individual information and suffering from inefficiency. To address these challenges, we introduce a novel approach, DROSIA: Decoupled Representation On Sequential Information Aggregation, which only integrates temporal relationships once as an additional representation for each point, achieving sequential information aggregation in a decoupled fashion. Thus balancing between individual and sequential information, along with a reduction in computational complexity. We select several widely used time series forecasting datasets, and previously top-performing models and baselines, for a comprehensive comparison. The experimental results validate the effectiveness and efficiency of DROSIA, which achieves state-of-the-art performance with only linear complexity. When provided with sufficiently long input data, the channel-independent DROSIA even outperforms the current best channel-dependent model, highlighting its proficiency in sequence modeling and capturing long-distance dependencies. Our code will be made open-source in the subsequent version of this paper.",
    "keywords": [
        "decoupled representation",
        "sequence modeling",
        "time series forecasting",
        "representation learning"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "We propose DROSIA to integrate temporal relationships as additional representation for each time point, which achieves the sequential information aggregation in a decoupled manner.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=6SxOzYVuy6",
    "pdf_link": "https://openreview.net/pdf?id=6SxOzYVuy6",
    "comments": [
        {
            "comment": {
                "value": "For questions regarding the empirical study:\n\n1. Could you please specify which hparams of the models that our choice \"are either unclear or arbitrary across different studies\"?\n\n2. We have reported multiple lookup in ablation study (figure 4), and we set the lookback window for overall experiment to 96 is to stay consistent with the corresponding paper of iTransformer, but we have conducted extensive ablation studies (figure 4) for the impact of various lookback window lengths of DROSIA and other forecasters (including PatchTST). Please let me explain for this ablation study:\n\n    * This ablation study is for the impact of various lookback window lengths {48, 96, 192, 336, 512}. We compared DROSIA with 4 forecasters with different types on various scale datasets.\n\n    * The experimental results (figure 4) show that when longer sequences are input, DROSIA surpasses PatchTST or other forecasters by an even greater margin, which demonstrated the effectiveness of DROSIA for sequence modeling and capturing long-distance dependencies.\n\n3. We apologize for our introduction to Table 4 makes you confused. Our motivation for DROSIA is to emphasize the importance of individual (patch or point) information, it is like a local-global architecture to enhance sequence modeling. Existing method may sacrifice the individual information when conduct sequence modeling although with the skip (or residual) connection, which has been demonstrated by the experimental results comparison between DROSIA and PatchTST.\n\n4. We are willing to add an appendix for all of our experimental results. Based on the current empirical study, we believe the effectiveness of DROSIA has been demonstrated, please let me explain:\n\n    * We use the same hyperparameters for DROSIA and all baselines for all experiments and ablation studies, which has been introduced in our paper. And we promise that we will never do \"better tuning\" only for our proposed models.\n\n    * The only difference between DROSIA and PatchTST is in the encoder, the results in table 1 show that DROSIA surpasses PatchTST for all scenarios, which demonstrates the effectiveness of DROSIA over Transformer. Besides, we will add a statistical analysis for the results in table 1 and ablation studies as soon as possible to prove the effectiveness of DROSIA, and to demonstrate its significance over other forecasters.\n\n    * We propose DROSIA not for an MLP-based forecasters, but an architecture to emphasize the individual information to enhance the sequence modeling, and choose time series forecasting task to demonstrate our motivation.\n\n    * The ablation study (figure 4) shows the better ability for capturing long-distance dependencies of DROSIA over Transformer encoder.\n\n    * The ablation study (table 4) shows that when concatenate the original patch embeddings with self-attention outputs, the forecasting performance of PatchTST would also increase, which demonstrate the effectiveness of our proposed architecture. We only choose ECL and Traffic datasets here not because that the same phenomenon does not exist on small-scale datasets, but that small-scale datasets are more influenced by random factors. Therefore, we have chosen to demonstrate using large-scale datasets only. We are willing to include results on datasets with various sizes in appendix."
            }
        },
        {
            "comment": {
                "value": "Thank you for the comments and suggestions. We have carefully considered each point and detail our responses below:\n\nFor weaknesses:\n\nWe apologize for the impression our paper has given you. Could you please specify which linear (MLP) based models that DROSIA has no outstanding distinctions from, and why you think that \"the motivation and justification behind the decoupling of sequence and patch level information is not well supported\". \n\nCould you please specify where of the benchmark or model parameters lacks details?\n\nFor questions regarding the theory:\n\n1. This question is great and important for us to clarify our contributions. Please let me explain for this:\n\n  * Firstly, as we mentioned in the introduction, our intuition is from the sociological perspective of Transverse Interaction: Individuals recognize the physical environment as a symbolic other and use this understanding to structure their interaction with a \u201cgeneralized other\u201d in paper [1]. We understand individuals and collective from this point of view, thus extract the sequential information only once as the \"generalized other\" for individuals. While the intuition of skip (or residual) connection is to enable training deeper network.\n\n  * The individuals interact with the collective to understand their position in the whole structure, thus enhance their representations. As the repeated process of multi-layer encoder goes, the whole structure will be well built, and the individuals will be in their accurate positions in the structure.\n\n  * As for the dot product attention with the skip connection, it has a quadratic computational complexity. Furthermore, it equips with skip connection not because the understanding that the individual information is compromised in their algorithm. But we emphasize its importance in sequence modeling.\n\n  * We demonstrate the effectiveness and efficiency of DROSIA with extensive experiments and ablation studies, especially in table 4, the forecasting performance of PatchTST (equipped with skip connections) could be further enhanced in \"P+S\" scenario with even less amount of parameters (embedding dimension from d to d/2), and with the same self-attention to extract the sequential information.\n\n  * As for the reasons, we think, are from that the summed method could not decouple the parameters of the information fusion layer (e.g. FFN), which is very important for balancing the individual and sequential information (they share the same parameter in the summed scenario).\n\n  * Consequently, we believe DROSIA is novel and important to the research fields of time series forecasting and sequence modeling.\n\n2. As we explained under equation 2: \"Equation (2) outlines the overall process of the DROSIA encoder, which will be described in detail from Equation (3) to Equation (7).\", our algorithm flows just from equation 3 to equation 7. \n\n  * Above equation 3, we explained: \"The output representations from the patch embedding or the previous layer of DROSIA encoder are first concatenated\". So S^1 represents the patch embeddings, and S^j denotes the input of the j-th encoder layer, or the output of the (j-1)-th layer. S^j_i is the i-th item of S^j, and the shape of all S is (b * c, n, d/2), b: batches, c: channels, n: patches, d: dimension.\n\n  * We concatenate S^j_i, i=1,2,...,n as C^j (equation 3), the shape of all C is (b * c, n * d/2).\n\n  * We extract sequential information from C^j that is useful for forecasting through any sequence information extractor (equetion 4, we choose MLP just for its simplicity, we have evaluated various methods for extraction in table 5), R^j is the extracted sequential information, the shape of all R is (b * c, d/2).\n\n  * Above equation 5 and equation 6, we explained: \"The extracted sequential information is concatenated with the original patch embeddings or the outputs from the previous encoder, as illustrated in Figure 2.\" We repeat R^j for n times (from (b * c, d/2) to (b * c, n, d/2)) and concatenate it with S^j (equation 5, figure 2), then nomalize the two as D (equation 5, equation 6), the shape of D is (b * c, n, d), finally fuse them to original dimension of S^j to derive S^{j+1}.\n\n3. The main reason that we patched the input is for a fair comparison between DROSIA and Transformer (employed by PatchTST). And we want to reduce the time consumption of DROSIA.\n\n* We have never claimed that the patch embedding is one of the contributions of this paper.\n\n* Although the patch size has few impact for overall performance of DROSIA, we need experimental results to prove it.\n\n* Since that is the case, we set the patch size to be consistent with PatchTST for fair comparison between DROSIA and Transformer.\n\nFor questions regarding the empirical study:\n\nDue to the character limit, we will add another official comment for responces to questions regarding the empirical study.\n\nReference\n\n[1] Transverse interaction: A pragmatic perspective on environment as other. Symbolic Interaction"
            }
        },
        {
            "comment": {
                "value": "Thank you for the comments and suggestions. We have carefully considered each point and detail our responses below:\n\nFor weaknesses:\n\n* We would like to know more about your concerns of novelty and the technical implementation, could you please clarify that why you think our idea lacks novelty, and where of the technical implementation is weak? This would be important for us to improve our paper.\n\n* Our motivation of this paper is to demonstrate the effectiveness and efficiency of DROSIA for sequence modeling and capturing long-distance dependencies. The reason that we choose the previously state-of-the-art channel-dependent model - iTransformer is also to demonstrate the motivation. Consequently, we believe that we should focus on various intra-channel modeling methods.\n\n* We have uploaded our codes and scripts to the supplementary material, which could be directly added in the Time-Series-Library (TSL) repo in GitHub (https://github.com/thuml/Time-Series-Library), and follow their running instructions. Our github link will be added in the paper after the anonymous period ends.\n\nFor questions:\n\n* We replace the Transformer encoder of PatchTST with DROSIA encoder, which is much different when compared to Transformer, and we conduct extensive experiments and ablation studies to demonstrate the effectiveness and efficiency of DROSIA for sequence modeling and capturing long-distance dependencies.\n\n* Because we unified the batch size and learning rate for all datasets and forecasters, and please let me explain for this.\n\n  1. We conduct all of the experiments based on the Time-Series-Library (TSL) repo, which is open-sourced by the research team of iTransformer.\n\n  2. TSL provides many scripts for model training. However, we found that the hyperparameters set in the scripts of different models are not always the same, such as the number of encoder layers and attention heads, batch sizes, learning rates, and so on.\n\n  3. We could reproduce the results of iTransformer reported in the corresponding paper using the provided scripts, however, the forecasting performance of other models (including DROSIA) could also benefit from the hyperparameters that are used for iTransformer.\n\n  4. To conduct a fair comparison, we set all hyperparameters to be the same. Not only for the hyperparameters we introduced in our experimental settings, we also set others to the default settings that the TSL team provided in their python model training file, but not to the scripts.\n\n* We believe that there is no perfect model, but only applicable scenarios for the model. DROSIA is not a hexagonal warrior, but it could reach the state-of-the-art performance over all channel-independent models, and over channel-dependent models when the input lengths of time series are sufficiently long, with only linear complexity. This is enough for us to demonstrate our motivation of this paper."
            }
        },
        {
            "comment": {
                "value": "Thank you for the comments and suggestions. We have carefully considered each point and detail our responses below:\n\nFor weaknesses:\n\n* We apologize for the impression our paper has given you. Could you please specify which components were not clearly justified? This information is of great significance for us to improve the paper.\n\n* Please see our responces to the questions.\n\n* We will conduct a statistical test as soon as possible to demonstrate the significance of DROSIA over PatchTST in small datasets, and report the results here and in the paper for your concerns. Additionally, please allow me to reiterate the motivation of this paper.\n\n  1. We aim to construct a simple, effective and efficient method for sequence modeling and have chosen to demonstrate its effectiveness using a variety of time series forecasting datasets.\n\n  2. PatchTST employs the Transformer encoder for sequence modeling, and the only difference between our approach and theirs is in the encoder. However, our method outperforms PatchTST, especially on complex datasets, as evidenced by Table 1.\n\n  3. Moreover, when longer sequences are input, DROSIA surpasses PatchTST by an even greater margin (figure 4). These results are sufficient to prove the effectiveness of our proposed method in sequence modeling and capturing long-distance dependencies, and it does so with only linear complexity.\n\nFor questions:\n\n1. We want to balance between individual and sequential information, and reduce the computational complexity, which have been explained in the paper. This intuition is similar to the local-global architectures, and we emphasize the importance of local information. Moreover, we conduct extensive ablation studies for the comparison to potential alternatives.\n\n2. All of the baselines are trained and evaluated with the same split settings of data, as well as the batch size, channels, input length, prediction hirizon, and so on. It is consistent with other work in the field of time series forecasting. We are confused of the \"less training data\" of channel-dependent models, could you please be more specific of what you mean?\n\n3. We are very willing to introduce other important forecasters, and may have missed some models and will add them as soon as possible. However, this work focuses on the effectiveness and efficiency of DROSIA for sequence modeling and capturing long-distance dependencies, which has been demonstrated.\n\n4. We have not evaluated the ability of DROSIA in few-shot scenarios, this is a valuable direction for us. In this paper, we focus on the effectiveness and efficiency of DROSIA for sequence modeling and capturing long-distance dependencies, rather than transfer learning."
            }
        },
        {
            "comment": {
                "value": "For questions:\n\nFirstly, we need to clarify that patch size does indeed have an impact.\n\n* When the input length L=96 and the patch size p=32, the model\u2019s performance declines. This is why we set the input length to 192 when analyzing the impact of various patch sizes, to enable a fairer comparison.\n\n* We believe that the model\u2019s predictive performance is related to the number of parameters and the depth of the network. Larger datasets require more parameters and deeper networks. When the patch size is set close to the input length, the sequence of patches is too short, leading to an insufficient number of parameters for the sequence information extractor, which is also evident from the analysis of the other three hyperparameters.\n\n* Since the sequence information is processed by the extractor from the concatenated patch embeddings, when the dimension ratio of patch embeddings is sufficiently high, the number of extractor parameters is also sufficient. The analysis of larger dimensions could confirm this point as well, and the number of encoding layers could confirm the impact of network depth on predictive performance.\n\nThere is no parameter sharing in DROSIA.\n\nWe have tested the longer horizons in Figure 4, but not in table 5, and we are willing to do so.\n\nScenario \"P\" means only original patch embeddings that are directly fed into FFN layer (equation 7), the dimension of patch embeddings is set to d for this, while d/2 for \"P+S\".\n\nWe are very willing to evaluate the effectiveness of DROSIA in other time series tasks, but in this paper, we focused on forecasting to demonstrate the capabilities of sequence modeling and capturing long-distance dependencies of DROSIA.\n\nThank you for your open-mindedness towards DROSIA. We will take your suggestions seriously and make revisions as soon as possible, hoping to gain your approval in the end."
            }
        },
        {
            "comment": {
                "value": "Thank you for the comments and suggestions. We have carefully considered each point and detail our responses below:\n\nFor weaknesses:\n\nWe apologize for the inaccurate use of the term \u201csignificantly\u201d while only reporting the average results of three trials, and will conduct a statistical test as soon as possible, report the results here and in the paper.\n\nWe believe that there is no perfect model, but only applicable scenarios for the model. We have demonstrated the effectiveness and efficiency of DROSIA in its applicable scenarios and consider that the requirements for these scenarios are not difficult to meet.\n\n* For example, the Weather dataset includes 21 indicators, such as temperature and humidity, recorded every 10 minutes. So we can wait 1 day and 8 hours to obtain a sequence of length 192, but we need 21 different types of sensors to gather the 21 channels. Hard to imagine what express delivery could bring 21 (or even 192) types of sensors home and have them functioning properly within 1 day and 8 hours, lol. Increasing the sequence length is not necessarily more difficult than increasing the number of channels.\n\n* Moreover, we can adjust to a finer time granularity or quickly obtain longer sequences through methods like interpolation to address this issue.\n\n* We have honestly reported the shortcoming of DROSIA in experiments and affirm the value of inter-channel information, emphasizing the importance of efficiently modeling both intra- and inter-channel information in future work.\n\n* DROSIA is slightly weaker than channel-dependent models only in the scenario of extremely many channels and short sequences, which might sound quite rare when described. We believe that this shortcoming of DROSIA cannot overshadow its success in other, more extensive fields.\n\nWe are very willing to position LLM-based forecasters in our study if you insist, and please allow me to explain why we haven\u2019t done so before.\n\n* Paper [1] (https://arxiv.org/pdf/2406.16964) thoroughly analyzed the LLM-based methods in the field of time series forecasting, and they claimed: \"Popular LLM-based time series forecasters perform the same or worse than basic LLM-free ablations, yet require orders of magnitude more compute\", and \u201cDespite the recent popularity of LLMs in time series forecasting, they do not appear to meaningfully improve performance\u201d.\n\n* Our motivation of DROSIA is to demonstrate its effectiveness and efficient in sequence modeling and time series forecasting, so we did not compare it with LLM-based forecasters but with various transformer-based models.\n\n* Research related to LLM requires many resources, which can be quite challenging for ordinary researchers (like me). Therefore, I hope to focus on small but meaningful work.\n\nWe will compare our model with FreTS you mentioned and other potential baselines as soon as possible, report the results here and in the paper.\n\nWe are pleased to conduct an ablation study for this component. However, the equation (7) in our paper fuse two types of information and compress the fused representation to the original dimension. It enables a full interaction between the two, and could not be simply removed in scenarios of multi-layer encoder. How would you like to conduct this ablation study?\n\nWe have uploaded our codes and scripts to the supplementary material, which could be directly added to the Time-Series-Library (TSL) repo in GitHub (https://github.com/thuml/Time-Series-Library), and follow their running instructions. Our github link will be added in the paper after the anonymous period ends.\n\nWe would add the non-averaged results of table 1 in the appendix, and fix typos, font size, and add another row \u201caverage+- standard deviation\u201d to each table as soon as possible.\n\nFor questions:\n\nBecause of the limit of characters, we would add another official comment for responses to the questions.\n\nReferences\n\n[1] Are Language Models Actually Useful for Time Series Forecasting? Neurips, 2024"
            }
        },
        {
            "summary": {
                "value": "In the papers the authors propose a MLP based time series forecasting framework named DROSIA which emphasizes the separate representations of patch (local) level information and sequence level information, i.e. the manual decoupling of the two. DROSIA prefers concatenation instead of summation to explicitly present the related info. They empirically benchmark DROSIA against other SotA methods and demonstrate its outstanding performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "DROSIA as proposed in the paper is a practical architecture to use, and it is relatively convincing that it can deliver good performances."
            },
            "weaknesses": {
                "value": "There are weaknesses in both the theoretical motivation and the empirical study in the paper. Novelty wise, the motivation and justification behind the decoupling of sequence and patch level information is not well supported, whereas besides this point DROSIA has no outstanding distinctions from other linear (MLP) based model. \n\nFor the empirical study there lacks many details regarding, e.g. benchmark model parameters, reasons for setting up the benchmark parameters, etc, which makes the empirical support for the decoupling claim weak. \n\nPresentation wise, the writing could use some revision, specially regarding the key parts, e.g. the algorithm of DROSIA, the reasoning behind the ablation study, etc."
            },
            "questions": {
                "value": "Regarding the theory:\n1. The dot product attention and the skip connection altogether are also capable of decoupling between the patch level presentation and its interaction with the whole sequence even when these two levels of presentations are summed together instead of concatenated. Can you provide a more rigorous or quantifiable definition on what the decoupling means here in the paper? \n2. Can you clarify the flow of the algorithm? For example, eq 3 through eq 7, where is S^j_1, assuming C^j is sequence level? How to get S^{j+1}_i?\n3. Empirical study reveals no benefit from patching. What's the motivation behind it?\n\nRegarding the empirical study:\n1. The choice of DROSIA and other baseline methods' hparams are either unclear or arbitrary across different studies. It would be better to have more details to back a fair comparison, e.g. all methods are tuned to near optimal.\n2. The lookback window for each tasks also seem a bit arbitrary, e.g. table 1 uses 96 whereas the original PatchTST paper reports 512. It would be more insightful to report multiple lookup for stronger empirical evidence.\n3. Table 4 Effectiveness of DROSIA is too limited in the sense that (1) the benchmark datasets are two and high dimensional, and (2) \"S\" for PatchTST's original setting is a bit misleading for claiming no patch level presentation there.\n4. Based on the current empirical study, it is unclear whether the performance edge in Table 1 is due to the decoupling or due to a better tuning of some linear / MLP structures which are already known to be also effective for forecasting tasks. Consider adding an appendix."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents Decoupled Representation On Sequential Information Aggregation (DROSIA) for time series forecasting. The key idea is to aggregate sequential information in a decoupled fashion, effectively balancing it with individual point information. The experimental results demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper is well-written and organized.\n* The proposed network architecture appears to be novel.\n* The method is lightweight, utilizing relatively fewer parameters than existing approaches."
            },
            "weaknesses": {
                "value": "* There are clarity issues in explaining the proposed method. Some components and their roles are not clearly justified.\n* Certain comparisons may not be entirely fair. In addition, several related works are not mentioned or compared (see the questions below).\n* For datasets with a small number of variates or low complexity, the improvements over other models, such as PatchTST, are marginal."
            },
            "questions": {
                "value": "1. It is not clear why each component of the proposed DROSIA is designed in its current form. What advantages do the chosen design choices provide compared to potential alternatives?\n2. It may not be fair to compare with channel-dependent methods e.g., iTransformer, as they have less training data.\n3. Some important channel-independent and channel-dependent baselines are not mentioned or compared, such as:\n\n[1] Unified Training of Universal Time Series Forecasting Transformers (ICML 2024)\n\n[2] Chronos: Learning the Language of Time Series (arXiv preprint arXiv:2403.07815)\n\n[3] One Fits All: Power General Time Series Analysis by Pretrained LM (NeurIPS 2023)\n\n[4] S\u00b2IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting (ICML 2024)\n\n[5] TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting (ICLR 2024)\n\n4. What is the performance of DROSIA in few-shot scenarios compared to the baselines?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "For the task of time series forecasting, authors propose decoupled representation to integrate temporal relationships once as an additional representation for each point, achieving sequential information aggregation in a decoupled fashion."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Nice paper writing.\n- Enough ablation study is also included."
            },
            "weaknesses": {
                "value": "- The idea of decoupled representation lacks novelty. The technical implementation is relatively weak.\n- Lack of in-depth analysis, e.g., channel-dependent model.\n- No available codes."
            },
            "questions": {
                "value": "- What is the big difference between your work and PatchTST? \n- You consider iTransformer as a strong baseline, but the results in Table 1 seem to be different from the results in iTransformer. Any experimental settings changed?\n- Efficiency analysis in Table 3 should include more linear-based methods: DLinear, and TiDE. It would be great to conduct more analysis like Figure 10 in iTransformer rather than only computational complexity."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "Nan"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Authors proposed DROSIA a method of time-series forecasting that incorporates both point-wise and temporal information by applying the following steps:\n- Patching similar to existing methods e.g., PatchTST\n- DROSIA encoding \n    - Sequence aggregation I.e., applying multiple encoding layers on vectorized patches \n    - Information extraction I.e., a MLP on concatenated representation of the previous step \n    - Representation fusion i.e., layer normalization of concatenated representations of information extraction + vectorized patches followed by a fully connected transformation (similar to residual connections) \n- Decoding (projection) i.e., making predictions \n\nAuthors studied performance of DROSIA on state-of-the-art of time series forecasting benchmarks and compared with some of the well-known methods in this area."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Authors have done extensive experiments ranging from benchmarks to complexity\n- DROSIA achieved better performance with relatively simpler model in Long-term forecasting task"
            },
            "weaknesses": {
                "value": "Although average over 3 trials is reported, standard deviations are not reported. Reporting standard deviation is crucial when performance gap is small. Particularly, when authors claim \u201csignificantly outperforming\u201d a method this needs to be confirmed by conducting a statistical test e.g., t-test or Wilcoxon test (based on assumptions).  \n\nThe proposed method needs adjustment in input length to outperform iTransformer particularly on datasets with a lot of variables and shorter horizons e.g., 96 which could be a disadvantage in applicability of the proposed method on real-world applications. Authors did not provide any instruction on how to find \u201csufficiently long\u201d input length for their method. \n\nRecently LLM-based methods for time series forecasting have shown state-of-the-art performance [1-4] some of which also based on patching [2] but there is no indication of this category in neither related works nor compared methods. Examples are: \nJust to be clear, I am not asking authors to compare with all of these LLM-based methods but I\u2019d like to know their at least their thoughts on positioning this line of work in their study. \n\nAuthors have compared their method with DLinear which is based on MLP that utilizes point-wise information but there are also MLP-based models such as [5] that incorporate global and local information which to me are more similar to the proposed method but is missing from compared methods. \n\nOne missing ablation experiment is related to the equation (7). What is the performance without this component? \n\nSome reproducibility information is missing such as code (cited in the abstract that it will be provided later), learning rate or any utilized regularizations. \n\nOriginal (non-averaged) results of table 1 should be provided in the appendix \n\n\nSmaller fixes: \n\n- Typo on page 4 \u201cIn Equation (4)\u201d -> Equation (3)\n\n- Typo on page 5 \u201cIn Equation (3) -> Equation (8)\n\n- Font size in figures is too small (at least for me)\n\n- It would be helpful to add another row \u201caverage+- standard deviation\u201d to each table to summarize the results per analysis\n\nReferences \n\n[1] Large language models are zero-shot time series forecasters. Neurips, 2023.\n\n[2] One Fits All: Power General Time Series Analysis by Pretrained LM. Neurips, 2023.\n\n[3] Time-LLM: Time Series Forecasting by Reprogramming Large Language Models. ICLR, 2024.\n\n[4] TEST: Text Prototype Aligned Embedding To Activate LLM\u2019S Ability for Time Series. ICLR, 2024.\n\n[5] Frequency-domain MLPs are More Effective Learners in Time Series Forecasting. Neurips, 2023."
            },
            "questions": {
                "value": "Do you have any intuition for certain behaviours in your sensitivity analysis? e.g., why patch size is not important, why for larger dataset in terms of number of channels e.g., Traffic high values of almost all hyper-parameters makes things worse? \n\nIs there any parameter sharing in DROSIA? \n\nAre observations made in table 5 going to hold for longer horizons e.g., H=720?  \n\nWhat is \u201cP\u201d model in table 4 last column? \n\nCompared baselines are also applicable to other time-series tasks including classification, short-term forecasting, imputation, and anomaly detection and often competitive in said tasks, but DROSIA is only studied in long-term forecasting do you have any sense on applicability of DROSIA in other time series tasks?\n\nI would be happy to revise my score if authors clarify questions/weaknesses particularly:\n- Missing experiments e.g., ablation or potentially missing baselines as well as information to evaluate performance better e.g., standard deviation or significance test. \n- Issue with number of features/channels and input length"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}