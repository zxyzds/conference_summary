{
    "id": "zPaTnGjgpa",
    "title": "Can Stability be Detrimental? Better Generalization through Gradient Descent Instabilities",
    "abstract": "Traditional analyses of gradient descent optimization show that, when the largest eigenvalue of the loss Hessian - often referred to as the sharpness - is below a critical learning-rate threshold, then training is \u2018stable\u2019 and training loss decreases monotonically. Recent studies, however, have suggested that the majority of modern deep neural networks achieve good performance despite operating outside this stable regime. In this work, we demonstrate that such instabilities, induced by large learning rates, move model parameters toward flatter regions of the loss landscape. Our crucial insight lies in noting that, during these instabilities, the orientation of the Hessian eigenvectors rotate. This, we conjecture, allows the model to explore regions of the loss landscape that display more desirable geometrical properties for generalization, such as flatness. These rotations are a consequence of network depth, and we prove that for any network with depth $> 1$, unstable growth in parameters cause rotations in the principal components of the Hessian, which promote exploration of the parameter space away from unstable directions. Our empirical studies reveal an implicit regularization effect in gradient descent with large learning rates operating beyond the stability threshold. We find these lead to excellent generalization performance on modern benchmark datasets.",
    "keywords": [
        "Gradient Descent",
        "Generalization",
        "Optimization",
        "Stability"
    ],
    "primary_area": "optimization",
    "TLDR": "This work identifies the implicit regularisation effects of instabilities in gradient descent, attainable through the use of large learning rates.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=zPaTnGjgpa",
    "pdf_link": "https://openreview.net/pdf?id=zPaTnGjgpa",
    "comments": [
        {
            "summary": {
                "value": "Authors study a new phenomenon in training NNs which they call \"Progressive flattening\" during the instability regime when inverse of learning rate is larger than sharpness. In this regime, they claim, via empirical investigations and a theoretical result about a toy model for diagonal linear network (DLN), that loss goes to flatter regions by changing the eigenvector directions of the Hessian, while in the stable regime it seems the eigenvector directions are reinforced. Experiments show this regularization (starting with large learning rates) and gonig through these instability phases, also has a better effect on generalization They also challenge sharpness as a measure of generalization.\nMoreover, their results seem to contradict that of Damian et al 2023 about the behavior of the instability regime."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Authors study a new phenomenon about reorientation of eigenvectors of the Hessian during the instability phase of training NNs so that the trajectory can go to flatter regions, which seems new and interesting; even though intuitively it is not clear to me why this is necessary for going to flatter regions. Their claim oppose the claim in Damian et al (2023) that the gradient is well-aligned with the gradient of the sharpness during the instability phase."
            },
            "weaknesses": {
                "value": "Major concerns: \nThe relation between \u201cgrowth of parameters\u201d that the authors discuss with the progressive flattening  and reorienting the eigenvectors, phenomenon is unclear. \n\nThe claims seems to oppose the sharpness largest eigenvector alignment with the gradient in the instability regime, so I think the experiments seem insufficient to back their claim, especially about reorienting the eigenvectors \n\nAuthors refer to reorienting the eigenvectors of Hessian as the major effect that algorithms goes back to stability, I can imagine reorienting the eigenvectors of the sharpness can have such effect, but the former is not clear to in why it should help. Authors seems not to explain this in their theorem for the toy diagonal model that. Additionally authors interchangeably also talk about the eigenvector of the sharpness Hessian instead of the Hessian of the loss, which is confusing which one is exactly the target of their claims.\n\nOther issues:\n033: is dependent \u2014> depends\n054:  in generalization \u2014> in the generalization \n075: grow without bound \u2014> grow unboundedly\nLine 087 definition of $\\alpha$ has an issue, there are two dots used after the two gradients\n092 last sentence not clear\n123: what is a \u201ctwo-parameter\u201d network?\n133: \u201csharpness hessian eigenvectors\u201d -> you mean loss hessian eigenvectors?\n148: very unclear: what does  \u201csharpest parameter\u201d mean?\n157: \u201cdespite originating from same loss function\u201d doesn\u2019t make sense, are you referring to different coordinates of gradient being different? Also you are referring to $z$ as loss function, but isn\u2019t that the link function in your notation?"
            },
            "questions": {
                "value": "077 what is the role of this threshold $S(\\theta)$ is it on the sharpness? If so we don\u2019t pick it but algorithm trajectory can enforce it, so the sentence \u201cbeyond which training is thought to destabilize\u201d is misleading.\n\nIs the theoretical claim about diagonal linear network true for any matrix dimension and depth?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This research paper investigates the relationship between gradient descent instabilities and generalization performance in neural network training. Using toy model and experiments on small scales, they demonstrate that these instabilities cause rotations in the eigenvectors of the loss Hessian.  Through various experiments on benchmark datasets like fMNIST and CIFAR10, the authors provide evidence supporting their claim that larger learning rates can significantly enhance generalization by promoting this implicit regularization effect."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The analysis of Hessian eigenvectors rotation during instabilities is new, best to my knowledge.\n* The paper is clearly written and is easy to follow."
            },
            "weaknesses": {
                "value": "* The authors show that instabilities move training towards flatter regions of the loss landscape. However, This has been shown extensively show in prior works such [1, 4, 5, 6, 7].\n\n* The authors claim that the effect of large learning rates pertains late in training, which they dub it as progressive flattening. First, the phrase can be misunderstood as progressive decrease in sharpness, which has been studied in [4, 5]. I would suggest to use some other phrase for this phenomena. Second, I think the result directly follows from learning rate decay experiments from [9]. If the learning rate is decayed later in training, it would require more time to reach the new EoS threshold. If all models are trained for fixed training time, the sharpness will take more time to increase in the case where learning rate is decayed later in training. I would suggest the authors to train the models long enough such that all models reach the new EoS threshold and then compare the performance.\n\n* The authors claim that their empirical results reveal a clear phase transition where generalization benefits only emerge for\nlearning rates beyond the stability threshold.  However, looking at Figure 5(b) the transition is not sharp, rather its continuous. I would not recommend using the phrase transition. Also, similar results are already shown in prior works [1, 4, 5, 6].\n\n* The authors claim that they identify that sharpness is not a great predictor of performance. However, this is already known in prior literature, such as [8]. In addition, authors claim that the rotation can be a better predictor than sharpness. But the experimental results are not conclusive. \nMoreover, my intuition is that such predictors can be misleading for very flat initializations (Figure 3. of [4]) which do not require instabilities to train at high learning rates. Therefore, I would predict there won't be any significant change in eigenvector rotation.\n\n* The authors claim that they identify that the effects of large learning rate are observed without progressive sharpening. Various prior works have already found that instabilities occur without progressive sharpening [1, 3, 4, 5, 6] and this result is not new. I would like to request the authors to highlight their new claims wrt these works. \n\n* The authors claim that they prove that for any network with depth > 1, instabilities cause rotation along the principle components of Hessian. This is only demonstrated theoretically for deep diagonal networks. This claim should be modified accordingly. \n\n* Similar models to the two parameter model studied in Section 3.1 is extensively analyzed in prior studies, such as [1, 2, 3]. \nIn particular,  when z is quadratic (which is mainly considered in this work) and the global minima is at zero at origin $\\theta_1 \\theta_2 = 0$ has been analyzed in [1] to demonstrate the catapult effect. As discussed in [3], this model does not exhibit progressive sharpening (also can be seen in equations in [1]), this model does not exhibit progressive sharpening and therefore, this model does not capture real-world behavior well. \n\n[1] The large learning rate phase of deep learning: the catapult mechanism\nA Lewkowycz, Y Bahri, E Dyer, J Sohl-Dickstein, G Gur-Ari\narXiv preprint arXiv:2003.02218\n\n[2] Understanding Edge-of-Stability Training Dynamics with a Minimalist Example \nXingyu Zhu, Zixuan Wang, Xiang Wang, Mo Zhou, Rong Ge\nICLR 2023\n\n[3] Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos\nDayal Singh Kalra, Tianyu He, Maissam Barkeshli\nICLR 2024 BGPT Workshop\n\n[4] Why Warmup the Learning Rate? Underlying Mechanisms and Improvements\nDS Kalra, M Barkeshli\narXiv preprint arXiv:2406.09405\n\n[5] Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning\nL Zhu, C Liu, A Radhakrishnan, M Belkin\narXiv preprint arXiv:2306.04815\n\n[6] Quadratic models for understanding neural network dynamics\nL Zhu, C Liu, A Radhakrishnan, M Belkin\narXiv preprint arXiv:2205.11787\n\n[7] A loss curvature perspective on training instabilities of deep learning models\nGilmer et al.\nICLR 2022\n\n[8] On the maximum hessian eigenvalue and generalization\nS Kaur, J Cohen, ZC Lipton\nhttps://arxiv.org/abs/2206.10654\n\n[9] Gradient descent on neural networks typically occurs at the edge of stability\nJ Cohen, S Kaur, Y Li, JZ Kolter, A Talwalkar\nInternational Conference on Learning Representations"
            },
            "questions": {
                "value": "* In Figure 5, the authors mark the 'stability threshold' using dotted vertical line. Is this stability threshold wrt initialization? \n* Can the authors contrast their work with existing literature referred above?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the dynamics of gradient descent in the edge of stability regime. It begins by analyzing gradient descent on a two parameter model $L(\\theta) = z(\\theta_1 \\theta_2)$. It then shows that in realistic settings, the eigenvectors of the Hessian rotate during the EOS dynamics, and conjectures that these rotations are responsible for the edge of stability dynamics. They also propose a \"progressive flattening\" mechanism which argues that training with large learning rates for longer will lead to flatter solutions, and studies the connection between learning rates, sharpness, eigenvector alignment, and generalization."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The loss landscape visualizations in Figure 3 and Figure 12 are a nice way to visualize the sharpening/flattening process"
            },
            "weaknesses": {
                "value": "- While Figure 2 supports the claim that the eigenvectors of the Hessian rotate at EOS, this is a correlational observation and doesn't support the claim of the paper that this rotation is relevant for stabilizing training, giving rise to the EOS dynamics, or exploring flatter regions of the loss landscape.\n- The paper does not precisely define \"progressive flattening\" \u2013 see my question below. In addition, the use of cross entropy acts as a strong confounder in these experiments, as the sharpness will converge to $0$ at the end of training. Therefore, a simple explanation for Figures 4,6 is that using a larger learning rate for longer will decrease the loss more, leading to a smaller overall sharpness. To establish progressive flattening as a distinct phenomenon, it would be valuable to repeat these experiments with MSE loss and train all models to near-zero training loss.\n- The paper mischaracterizes the progressive sharpening factor in Damian et al. 2023. Assumption 1 in this paper assumes $\\alpha > 0$ along their \"constrained trajectory,\" not the actual gradient descent trajectory. It is expected to be negative along the gradient descent trajectory since the sharpness decreases when self-stabilizing. Their experiments (Appendix E, top left) show $\\alpha > 0$ in all of the settings studied in Cohen et al. 2020.\n- This paper would benefit from a proper literature review. The only theoretical analyses of EOS cited are Damian et al. 2023 and Arora et al. 2022. There are many relevant missing papers, including papers that study EOS on diagonal linear networks [1,2,3], and [4] which observed a phenomenon possibly related to progressive flattening.\n\n[1] Chen and Bruna 2023: Beyond the edge of stability via two-step gradient updates\n\n[2] Even et al. 2024: (S)GD over Diagonal Linear Networks: Implicit bias, Large Stepsizes and Edge of Stability\n\n[3] Zhu et al. 2023: Understanding Edge-of-Stability Training Dynamics with a Minimalist Example\n\n[4] Kreisler et al. 2023: Gradient Descent Monotonically Decreases the Sharpness of Gradient Flow Solutions in Scalar Networks and Beyond\n\n\nMinor Points:\n- line 154: $theta$s -> $\\theta$s"
            },
            "questions": {
                "value": "- The authors claim that instabilities along the top Hessian eigenvector cause the changes in $S$. Is there any evidence for this? It seems that the main changes in sharpness in Figures 3,12 come from the overall curve changing, not from the sharpness differing at different parts of the curve.\n- What is the precise definition for progressive flattening?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the \"edge of stability\" phenomenon, where neural networks can be trained with a larger learning rate than prescribed via the descent lemma. The central claim of this paper is that during instability, the eigenvectors of the Hessian rotate, thus moving the model parameters to a flatter region of the loss landscape -- the authors call this behavior \"progressive flattening.\" The paper provides theoretical support via a diagonal linear network derivation, and shows empirically that the top eigenvectors do rotate during periods of instability, and that large learning rates lead to improved generalization."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- It is an important question to understand the dynamics of gradient descent in the large learning rate, or \"edge of stability\" regime.\n- The proposal of eigenvector rotation as a mechanism for reducing instability has not, to the best of my knowledge, appeared before in the literature.\n- The theoretical derivations in Section 3 appear to be sound."
            },
            "weaknesses": {
                "value": "- My main concern with this paper is that it does not adequately support the claim that eigenvector rotation is the *cause* of instabilities being resolved. The paper provides evidence that during instability, the top Hessian eigenvectors rotate, and moreover afterwards the training becomes stable. However, there is no justification that such eigenvector rotation causes the return to stability. Justification for such return to stability has already been given in prior works such as the self-stabilization mechanism of Damian et al., (2023) or the catapult effect of Lewkowycz et al. (2020), and to me it appears that eigenvector rotation is itself caused by instability (yet goes away when other mechanisms cause a return to stability).\n- The definition of \"progressive flattening\" is not clear. My interpretation is that progressive flattening is the phenomenon that higher order derivatives of the sharpness are reduced during instability. However, this effect is not quantified nor easy to observe in the experiments in Section 3.2. \n- The justification in Section 3.3 is also insufficient. One alternate hypothesis to Figure 4 that I find more plausible is the following. First sharpness is constrained to $2/\\eta$ when training with learning rate $\\eta$. Later when $\\eta$ is decreased progressive sharpnening occurs, yet the degree of progressive sharpening is proportional to the loss gradient, which is smaller later in training. Therefore reducing $\\eta$ later leads to a lower final sharpness.\n- Many of the results in Section 4 on the generalization benefit of large learning rates are known in the literature. This benefit is because training with a learning rate of $\\eta$ implicitly constrains the sharpness to be at most $2/\\eta$; lower sharpness is believed to correlate with better generalization."
            },
            "questions": {
                "value": "- I find the comparison to Damian et al. (2023) in the Related Work and Appendix D to be inaccurate. That paper proves that, during the unstable regime, 1) the parameters will oscillate and grow in the top eigenvector direction, and 2) via the third-order Taylor expansion, such oscillations will lead to an implicit regularization effect which decreases sharpness and drives the model to stability. This self-stabilization effect occurs whenever training is unstable, and does not require progressive sharpening. The progressive sharpening assumption only characterizes the fact that the sharpness increases during gradient descent for all learning rates, and hence training will eventually becomes unstable. To me, it seems that the \"progressive flattening\" mechanism presented in this work can be explained by the \"self-stabilization\" mechanism in Damian et al., (2023).\n\n- Line 244 \"Even after the instability is resolved, the similarity among individual eigenvectors fall while the subspace comparison remain largely similar\": This doesn't seem to be true in Figure 2, as after the instability the similarity is close to 1.\n\n- Much of the derivation in Section 3.1 is hard to follow. What are the quantities $\\gamma_\\Theta, \\gamma_{|\\theta_2^2 - \\theta_1^2|}$?Why is the stability limit $\\eta_{eos}$ defined to be twice the value of the asymptote?\n\n- In Figure 7, what are these quantities in the table? How is $\\rho$ defined? (i.e with respect to what is similarity calculated?)\n\nMinor comments/typos:\n- The descent lemma (line 68) does not necessarily require convexity.\n- Line 254: \"Figure E\" -> Figure 12."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper tries to understand the instabilities during neural network training when using large learning rates and their impact on generalization.\n\nThe paper theorizes that unstable, high learning rates improve generalization by rotating Hessian eigenvectors, helping the model explore broader regions in the loss landscape. Using a Diagonal Linear Network (DLN) as a toy model, the authors show that unstable parameter growth leads to these beneficial rotations. They confirm this effect experimentally with an MLP on fMNIST, where instabilities cause rotations of the sharpest Hessian eigenvectors.\n\nThen, the authors directly study generalization. They empirically study the impact of large learning rates in different settings (MLP/VGG, CIFAR/MNIST), showing that increasing the learning rate way beyond the theoretical stability limit improves generalization. The authors extend their analysis to decreasing learning rate schedules and try to untangle the impact of different variables correlated with instabilities (learning rate, eigenvector rotation, sharpness), finding that all these variables impact generalization performance, though the eigenvector rotation seems to play a crucial role (compared to sharpness which is often studied in the literature)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is well-written. In particular, the theoretical parts (sections 2 and 3) are clear and didactic. Similarly, while I'm not familiar enough with the literature to assess its exhaustivity, I found the related work well-written and easy to follow, giving context to the authors' work.\n\nI did not carefully check the computations but the theoretical parts seem rigorous.\n\nSection 4 provides rigorous experiments. We particularly appreciate that the authors try to empirically understand which specific factor of instabilities impacts generalization the most, and that they provide quite realistic experiments (section 4.4)."
            },
            "weaknesses": {
                "value": "Though it allows for interesting derivations, the DLN model is quite limited, and the paper will be stronger if it studies a more realistic model.\n\nI'm wondering how robust the conclusions in Figure 8 are (see questions).\n\n(Minor) Though I understand that it would require more compute that academic researchers might not have, experiments on harder tasks with more modern models would enrich the paper.\n\nSome figures are quite hard to read. In general, I would recommend increasing the font size, especially for Figures 5 and 6.\n\nFor Figure 2: it can seem that the $S(\\theta)$ plot is done in the lr reduction setting \n\nFor Figure 7, I find the 3d plot hard to read.\n\nFor Figure 8, it took me some time to understand what was represented at the top.\n\nFor Figure 9, it might be interesting to report the result for lr=6.4 to show that 3.2 is the last stable value.\n\nThe code is not currently available, though it seems that it contains interesting contributions (for instance a new Jax implementation of Hessian-Vector product).\n\n\n### Typos\n\n- Arora et al. (2022) demonstrated that gradient descent can lead to alignment between gradient the sharpest eigenvector of the Hessian \n- where exactly derivations are possible"
            },
            "questions": {
                "value": "For Figure 8:\n - Is 5 samples enough to estimate a rank correlation? Could you report the uncertainty in the rank correlation plot?\n - Why does the sharpness often have a high rank correlation with generalization performance, but a varying sign? (compared to rho which has a more consistent but smaller absolute correlation on average)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}