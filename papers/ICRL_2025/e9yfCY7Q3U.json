{
    "id": "e9yfCY7Q3U",
    "title": "Improved Techniques for Optimization-Based Jailbreaking on Large Language Models",
    "abstract": "Large language models (LLMs) are being rapidly developed, and a key component of their widespread deployment is their safety-related alignment. Many red-teaming efforts aim to jailbreak LLMs, where among these efforts, the Greedy Coordinate Gradient (GCG) attack\u2019s success has led to a growing interest in the study of optimization-based jailbreaking techniques. Although GCG is a significant milestone, its attacking efficiency remains unsatisfactory. In this paper, we present several improved (empirical) techniques for optimization-based jailbreaks like GCG. We first observe that the single target template of \"Sure\" largely limits the attacking performance of GCG; given this, we propose to apply diverse target templates containing harmful self-suggestion and/or guidance to mislead LLMs. Besides, from the optimization aspects, we propose an automatic multi-coordinate updating strategy in GCG (i.e., adaptively deciding how many tokens to replace in each step) to accelerate convergence, as well as tricks like easy-to-hard initialisation. Then, we combine these improved technologies to develop an efficient jailbreak method, dubbed I-GCG. In our experiments, we evaluate on a series of benchmarks (such as NeurIPS 2023 Red Teaming Track). The results demonstrate that our improved techniques can help GCG outperform state-of-the-art jailbreaking attacks and achieve nearly 100% attack success rate.",
    "keywords": [
        "Jailbreaking Attacks",
        "Large Language Models"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "Our paper presents I-GCG, an optimized jailbreaking method for large language models, achieving nearly 100% attack success rate by introducing diverse target templates, adaptive multi-coordinate updates, and easy-to-hard initialization.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=e9yfCY7Q3U",
    "pdf_link": "https://openreview.net/pdf?id=e9yfCY7Q3U",
    "comments": [
        {
            "summary": {
                "value": "The authors propose a new method for an optimization-based search of jailbreaking prompts for conversational LLMs fine-tuned for safety. While heavily inspired by prior work - notably the Greedy Coordinate Gradient (GCG), authors add three modifications that improve the attack success rate and convergence speed. First, they introduce the easy-to-hard initialization, starting by attacking questions that are known to be easier to jailbreak than others. Second, they add an additional target string to obtain in the jailbroken model, confirming that the output about to be generated is harmful. Third, the authors modify an update rule from a single-mutation evolutionary search on the jailbreaking string to within-step recombination of best-performing mutations. The authors call their method the Improved-GCG (I-GCG) and demonstrate its superior performance to the alternative methods on the AdvBench benchmark, achieving a 100% success rate, as well as faster convergence, achieving a 10x speed-up on the original method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The field of LLM jailbreaking is currently the focus of the LLM security community and is central to developing secure LLM-based products. As such, the research is timely and interesting.\n- Authors perform an extensive comparison of their method not only to the original method but also to other alternative methods (Table 2)\n- Authors investigate the attack convergence time and transferability, which are essential for real-world applicability\n- Authors perform an ablation study, providing information to other researchers in developing their own attack methods\n- Authors provide sufficient information and the code required to replicate their results"
            },
            "weaknesses": {
                "value": "- The paper is not well-structured, and even somebody familiar with the domain requires several re-reads to understand the authors' contribution and their interest in the real-world setting. A major rewrite is recommended, notably to better situate this work compared to prior knowledge, inspiration for this work, and the authors' contribution. For instance, putting forward benchmarking results would be helpful to understand the added value of this work better.\n- Notably, the first two paragraphs, providing an introduction and an overview of the field, are full of citations with unclear relevance. \n 1. I do not understand the choice of Kasceni et al. 2023 and Chang et al. 2023 as references for the concept of LLMs, which, in my opinion, are entirely unrelated, much less landmark papers or recent reviews. The same criticism applies to all the citations in the following paragraph. \n 2. There are missing fundamental papers in the field to situate the paper and prior work, even for a topic expert who has not followed the field for a year. I believe [1-2] would be mandatory.\n 3. In the second paragraph, each citation group needs a short introductory paragraph to explain their relevance/importance and to be broken into 2-3 citations at most. As such, groups of up to 8 citations are cited without clear motivation or reason to the reader and do not read as relevant.\n- Additional landmark papers needed to situate the topic of research and contribution for the general public are missing in the \"Related work\" section, eg [3] for LLM-based jailbreak methods\n- One of the essential improvements proposed by the authors - easy-to-hard initialization - requires knowledge of the themes for which LLMs are easier to jailbreak. This, in turn, would require a measured performance of the \"difficulty\" of jailbreaking across themes and LLM/LLM families, whether as found by the authors (e.g., convergence time to jailbreak from default initialization) or as reported by previous work. Not reporting such results makes the model's results significantly less useful and hard to validate.\n- The addition of an explicit harm awareness prompt (e.g., \"my output is harmful\") seems to reduce the scope of attack to the outputs LLMs have been safety fine-tuned to recognize as harmful or harmfulness for which they can recognize from the context present in their training data. This seems to narrow down the scope of the attack. While this does not reduce the effectiveness of the attack on standard benchmarks, I believe this narrowing of scope requires discussion.\n\n\n[1] Perez, F., & Ribeiro, I. (2022). Ignore Previous Prompt: Attack Techniques For Language Models. ArXiv, abs/2211.09527.\n[2] Wallace, E., Feng, S., Kandpal, N., Gardner, M., & Singh, S. (2019). Universal Adversarial Triggers for Attacking and Analyzing NLP. Conference on Empirical Methods in Natural Language Processing.\n[3] Perez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides, J., Glaese, A., McAleese, N., & Irving, G. (2022). Red Teaming Language Models with Language Models. Conference on Empirical Methods in Natural Language Processing."
            },
            "questions": {
                "value": "- L178-L182: Why did you classify this approach, dependent on the attacker LLMs, with optimization-based jailbreak methods rather than with LLM-based jailbreak methods?\n- L366-L367: While the usage of ChatGPT-3.5 is consistent with prior art, this model is known to be more prone to jailbreaks than more recent GPT-4, Claude, or LLaMA-3.X models. Could you please explain the choice of ChatGPT-3.5 here?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an improved optimization-based jailbreak method for large language models (LLMs) called I-GCG, building upon the existing Greedy Coordinate Gradient (GCG) attack. The authors address the limitations of GCG, such as its low efficiency in generating jailbreak prompts, by introducing several advancements. Key improvements include:\n\n\t1.\tDiverse Target Templates: Instead of using a single target template, I-GCG employs varied target templates with harmful self-suggestions to better mislead LLMs.\n\t2.\tAutomatic Multi-Coordinate Update Strategy: This strategy allows adaptive updates of multiple tokens at each iteration, accelerating convergence compared to the single-token updates in traditional GCG.\n\t3.\tEasy-to-Hard Initialization: I-GCG begins with simple jailbreak cases to generate initial suffixes, which are then used as starting points for more complex jailbreaks.\n\nExperimental results show that I-GCG achieves nearly 100% attack success rates on multiple LLMs and outperforms prior jailbreak methods in both effectiveness and efficiency. This approach significantly reduces the time required for successful attacks, advancing the field of adversarial attacks on aligned LLMs."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "\u2022\tEnhanced Attack Efficiency: The proposed I-GCG method significantly improves upon traditional GCG by accelerating convergence with an automatic multi-coordinate update strategy. This reduces the number of iterations required and, consequently, the total attack time.\n\t\u2022\tHigher Success Rate: I-GCG achieves a nearly 100% success rate across various LLMs, outperforming other state-of-the-art jailbreak methods in effectiveness, especially on models with stronger security alignments.\n\t\u2022\tDiverse Target Templates: By using varied target templates with harmful self-suggestions, I-GCG effectively bypasses LLMs\u2019 alignment mechanisms, showcasing a novel approach that enhances the jailbreak success rate.\n\t\u2022\tEfficient Initialization Strategy: The easy-to-hard initialization allows for effective scaling from simple to complex jailbreaks. This structured initialization improves both attack robustness and adaptability across a range of malicious prompts.\n\t\u2022\tComprehensive Evaluation: The authors rigorously test I-GCG on multiple benchmarks and models, including those from the NeurIPS 2023 Red Teaming Track, providing strong empirical evidence for its superiority over previous jailbreak methods.\n\t\u2022\tTransferability: I-GCG demonstrates improved transferability of jailbreak prompts across different LLMs, indicating its potential to generalize effectively to a broader set of models and attack scenarios.\n\t\u2022\tWell-Defined Optimization Techniques: The paper provides clear mathematical formulations and experimental validation of the techniques used, such as the multi-coordinate update strategy, which supports the paper\u2019s methodological rigor."
            },
            "weaknesses": {
                "value": "\u2022\tScalability to Larger Models: While I-GCG shows strong performance on models such as LLAMA2-7B, the paper does not address scalability issues for substantially larger models (e.g., 70B+ parameters), where optimization costs and computational demands may significantly increase.\n\t\u2022\tLack of Defensive Strategies Discussion: Although the study provides insights into vulnerabilities, it lacks a discussion on potential defense mechanisms or mitigations that could counteract the proposed jailbreak methods, which could be valuable for guiding future security improvements."
            },
            "questions": {
                "value": "\u2022\tHow is the ASR (Attack Success Rate) calculated? Does it only count as successful if it passes all three checks: rule-based judgment, GPT-3.5 check, and manual review?\n\t\u2022\tWhat does \u201caverage iterations\u201d in Table 1 mean? Does it refer to the average number of iterations required to achieve a successful jailbreak for the first time with ASR?\n\t\u2022\tThe paper mentions using GCG as the baseline; could you specify the number of candidates set for vanilla GCG in this context?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focuses on improving optimization-based jailbreaking techniques for large language models (LLMs). The authors note that while existing methods like the Greedy Coordinate Gradient (GCG) attack have made progress, there is room for improvement in attacking efficiency. The paper contributes by identifying limitations in existing jailbreaking techniques and proposing novel strategies to enhance both the effectiveness and efficiency of jailbreaking LLMs. The developed I-GCG method demonstrates significant improvements over previous methods in terms of attack success rate and transferability."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper presents a combination of techniques to improve optimization-based jailbreaking. The idea of using diverse target templates with harmful self-suggestion and guidance is original. The automatic multi-coordinate updating strategy adaptively decides the number of tokens to replace in each step. The authors use multiple datasets (AdvBench and HarmBench) and several threat models (VICUNA-7B-1.5, GUANACO-7B, LLAMA2-7B-CHAT, and MISTRAL-7B-INSTRUCT-0.2) to evaluate the proposed I-GCG method. This wide range of evaluations provides a robust assessment of the method's performance under different conditions."
            },
            "weaknesses": {
                "value": "The paper focuses on improving the jailbreak attack but does not extensively explore how the proposed techniques interact with existing or potential defense mechanisms in LLMs. Understanding how LLMs can defend against the enhanced I-GCG attack and proposing counter-defense strategies would make the research more complete. This could involve testing the method against LLMs with advanced safety features or fine-tuned with specific defense-oriented training.\n\nThe easy-to-hard initialization and the automatic multi-coordinate updating strategy are effective in the current setup, but they might be sensitive to the initial conditions and hyperparameter choices. A more in-depth analysis of the stability and robustness of these techniques under different initialization values and optimization parameter settings could strengthen the method."
            },
            "questions": {
                "value": "1\uff09 In the experiment using HarmBench (NeurIPS 2023 Red Teaming Track), the target response format was set differently than in the other experiments. What was the rationale behind this change, and how did it impact the comparability of the results?\u3001\n2\uff09The automatic multi-coordinate updating strategy seems to be a key improvement in I-GCG. However, the paper does not discuss how the choice of the top-K tokens and the token combination process might affect the diversity and quality of the generated jailbreak suffixes. Can you provide more insights into this?\n3\uff09The current work focuses on jailbreaking LLMs in the context of generating harmful text. How applicable are the proposed techniques to other potential security threats or malicious uses of LLMs, such as information extraction, influence operations, or evading content filters in different modalities (e.g., image generation)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The paper presents improved jailbreak techniques, which could be misused by malicious actors to cause harm. There is a need to address the ethical concerns associated with the research."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes I-GCG, an optimization-based jailbreak to improve ASR and efficiency. Compared to current methods, I-GCG formulates its optimization goal by integrating harmful information into the standard template. Also, they introduce an automatic multi-coordinate updating strategy that selects the top-p single-token suffix candidates to generate multi-token suffix candidates. Finally, the authors incorporate an easy-to-hard initialization mechanism, initially targeting a simpler jailbreaking task and then transferring the optimized suffix to more challenging tasks. Experimental results demonstrate the superior ASR and reduced time cost compared to other jailbreaks against several LLMs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors present specific examples (Figs. 1-2 and the highlighted sentences on page 5) to emphasize the limitations of current jailbreaks that rely on a single target template, thereby distinguishing this work from previous studies.\n2. This paper is well-structured, with a clear motivation and a methodology that is easy to follow.\n3. The study includes recent jailbreaks published in 2023 and 2024 for comparison. The results show a high ASR and reduced time costs, supporting the contributions claimed in this paper."
            },
            "weaknesses": {
                "value": "1. This paper presents an incremental improvement over existing works. While the three proposed techniques differ from previous jailbreaks, their novelty is limited.\n2. The reasons behind the improvements in jailbreak effectiveness and efficiency from the reformulated optimization goal and multi-coordinate updating strategy are not thoroughly analyzed.\n3. Fig. 5 illustrates different levels of difficulty associated with malicious questions for successful jailbreak attempts. However, this result is presented solely for LLAMA2-7B-CHAT, which weakens the motivation for using the easy-to-hard initialization approach, as the difficulty of jailbreaking questions may vary on other models. Besides, this approach incurs extra time costs by requiring the jailbreaking of an easy task, raising the question of whether allocating extra time to a simple task can actually result in greater time savings on a complex task.\n4. I-GCG achieves high ASR on only one dataset (AdvBench). Given the straightforward nature of the methodology, experiments on additional datasets and models would provide a more comprehensive assessment."
            },
            "questions": {
                "value": "1. In equation 6 and 7, $x^S(0)$ and $x_0^S$ serve as initial jailbreak suffixes for the start of optimization. It is unclear why the authors formulate them as constraints.\n2. The analysis of the difficulty of jailbreaking different questions, particularly whether this difficulty remains consistent across other models, is lacking.\n3. It is unclear whether allocating extra time to a simple task can actually result in greater time savings on a complex task.\n4. Jailbreaking results on additional models, such as LLAMA2-7B alongside LLAMA2-7B-CHAT, are worth discussing. Experimental results on more datasets could further validate the effectiveness of the proposed method.\n5. Providing further elaboration on the contributions of this work would be beneficial."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Potentially harmful insights, methodologies and applications"
                ]
            },
            "details_of_ethics_concerns": {
                "value": "The proposed jailbreak may pose risks to real-world LLM systems. I recommend including a discussion on the ethical considerations."
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}