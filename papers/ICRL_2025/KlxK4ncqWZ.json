{
    "id": "KlxK4ncqWZ",
    "title": "Shallow diffusion networks provably learn hidden low-dimensional structure",
    "abstract": "Diffusion models provide a powerful, general purpose framework for learning to sample from a target distribution. The remarkable empirical success of these models applied to high dimensional signals, including images and video frames, stands in stark contrast to the classical curse of dimensionality which arises in the general problem of learning distributions. In this work, we take a step towards understanding this gap. We show that learning diffusion models in Barron spaces---the function space of single-layer neural networks---provably adapts to simple forms of low dimensional structure. We combine our results with recent progress in analyzing the diffusion sampling process to provide end-to-end sample complexity bounds for learning to sample from structured distributions. Our results avoid exponential dependencies on the ambient dimension of the data, and instead reflect the intrinsic latent dimensionality of the underlying target distribution. Importantly, our results do not require specialized architectures which are specifically tailored  for particular latent structures, and instead rely on the low-index structure of Barron classes to adapt to the underlying distribution.",
    "keywords": [
        "Generative models",
        "diffusion models",
        "denoising score matching",
        "low dimensional structure",
        "Barron classes"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=KlxK4ncqWZ",
    "pdf_link": "https://openreview.net/pdf?id=KlxK4ncqWZ",
    "comments": [
        {
            "summary": {
                "value": "This paper investigates why diffusion models, specifically those utilizing shallow neural networks within Barron spaces, can effectively learn and sample from high-dimensional data distributions that possess hidden low-dimensional structures. By focusing on Barron spaces\u2014the function space of single-layer neural networks\u2014the authors demonstrate that diffusion models can adapt to simple forms of low-dimensional structure without the need for specialized architectures. They provide theoretical results showing that the sample complexity for learning these models depends polynomially on the intrinsic latent dimensionality rather than exponentially on the ambient dimension. The analysis includes end-to-end sample complexity bounds for learning to sample from structured distributions, highlighting how shallow diffusion networks can circumvent the classical curse of dimensionality by leveraging the low-index structure of Barron classes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper provides a rigorous theoretical framework that explains how diffusion models can overcome the curse of dimensionality by adapting to low-dimensional latent structures within high-dimensional data.\n- By leveraging Barron spaces to model shallow neural networks, the authors bridge the gap between theoretical tractability and practical relevance, as these spaces capture essential features of networks used in practice.\n- The work offers comprehensive sample complexity bounds that depend on the intrinsic latent dimensionality, providing valuable insights into the efficiency of learning diffusion models for structured distributions.\n- The results do not rely on specialized network architectures tailored to specific latent structures, emphasizing the general applicability of shallow diffusion networks in learning hidden low-dimensional structures."
            },
            "weaknesses": {
                "value": "- The analysis is restricted to shallow (single-layer) neural networks, which may not capture the complexities and representational power of deep neural networks commonly employed in state-of-the-art diffusion models.\n- The theoretical results are derived under idealized conditions, such as target distributions concentrated on low-dimensional linear manifolds or composed of independent components, which may not fully reflect real-world data complexities.\n- Optimizing over Barron spaces can be computationally challenging due to the infinite-dimensional nature of these function spaces, raising questions about the practicality of implementing the proposed methods.\n- Minor typo: Line 315 (difference -> different?)"
            },
            "questions": {
                "value": "- To what extent can the assumptions of Lipschitz continuity and sub-Gaussianity be relaxed? Are there alternative conditions under which similar results could be obtained, possibly broadening the applicability of the theory?\n- Can the authors elaborate on the practical aspects of optimizing over Barron spaces?\n- In practical scenarios, data often contain noise and may deviate from idealized models. How robust is the proposed approach to such imperfections, and what modifications, if any, are needed to handle real-world datasets?\n- Given that non-linear manifolds are noted as future work, could you share any preliminary insights on how extending to non-linear latent structures might impact sample complexity or theoretical guarantees?\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores the effectiveness of shallow diffusion networks in learning distributions with low-dimensional structure, challenging the traditional belief that high-dimensional data inherently suffers from the curse of dimensionality. The study provides sample complexity bounds for these models, showing that they depend more on intrinsic latent dimensions rather than the ambient space, thus offering insights into the strengths of diffusion models for structured data generation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The research presents a theoretically grounded approach to understanding the success of shallow diffusion networks, especially for low-dimensional structures in data."
            },
            "weaknesses": {
                "value": "The reliance on Barron spaces, while insightful, could pose challenges for scalability and model tuning in larger, more complex network architectures, which are crucial to modern success of diffusion-based generative models."
            },
            "questions": {
                "value": "- Can the analysis be adopted to flow models/matching, which instead regresses a network onto the vector field of probability ODE (rather than the score)? \n- Can the authors give comments on how the analysis can be extended to discrete state spaces, for applications such as discrete diffusion models for language modeling?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 1
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper shows that learning diffusion model in Barron spaces (spanned by single-layer neural networks) avoids curse of dimensionality by adapting to low dimensional subspace."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Solid paper with concrete mathematical analysis. The results in the paper could be of general mathematical interests for related fields.\n\nThe paper makes a meaningful step towards understanding the gap between curse of dimensionality in theory and no curse in reality.\n\nThe paper has nice connection with recent progress in diffusion sampling process."
            },
            "weaknesses": {
                "value": "I have problems with calling results on \"single-layer neural networks\" by \"results on shallow networks\". Neural networks with a few layers are also shallow and the authors don't prove for them here. I'm not going to start a lecture on logic, but a paper should always try avoiding unnecessary confusions, especially those that make people think more favorably than it actually deserves.\n\nThe setting of single-layer networks is way too simple. The industry of deep learning scales to using 100k H100s, yet theories still are struggling with analyzing single-layer networks.\n\nThe formula in the theorems are complicated. If a simple setting leads to such convoluted formula, what would the more general cases be like?  Usually simpler things are more useful. It would benefit the presentation if a neat bound could be given the main body and a more detailed bound given in the appendix.\n\nLack of analysis of the gradient descent training process of neural networks.\n\nLack of direct usable implications. What could practitioners benefit from the theories? Or what future development of the theories could lead to some algorithmic innovations that is beyond imagination of practitioners? Machine learning is rather noisy compared with things like Physics. It's the best if we can get something useful from theoretical understandings.\n\nLack of experiments. The authors have proposed a set of assumptions which they believe is meaningful and proved results under these assumptions. It would be extremely beneficial to prove that these assumptions are relevant for the real world tasks and experiments on either simulation or real world data support the claim of the theorem. In theories, one inevitably chooses many simplification to make things elegant, which is quite understandable. However, experiments are needed to show that these simplification doesn't make the theoretical results irrelevant."
            },
            "questions": {
                "value": "Included in weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This theoretical work shows that for a neural network which is infinitely wide, and where neuron parameters come from the hypersphere, \nand there is one of two hidden latent structures in the data, then there is a bound on the emprical risk minimizer which only\ndepends on the dimension of the latent structure and not the dimension of the ambient space. \n\nIf we denote the ambient dimension as D, the two hidden structures considered are 1) a linear combination of lower dimensional \nvectors (of dimension d << D) and 2) a linear combination of independent components all of lower dimension than the ambient \nspace, but but where the sum of the dimensions of the components sum to D."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1: Diffusion models are videly used, so theoretical work on them is highly relevant. \n\nS2: Going from needing a specific architecture to \"only\" needing the network to be infinitely wide to get a bound, is a step forward. \n\nS3: Corollary 3.4 and 3.8 give bounds on the KL-divergence between the true data distribution and that of the diffusion model, \ngiven enough training samples (and the extra assumptions) which is interesting."
            },
            "weaknesses": {
                "value": "W1: I found this paper difficult to follow. I count this as a weakness, since I am a machine learning researcher who is interested in \nthe theory of machine learning, so I should be the target audience. See my questions and suggestions for places where clarification is \nneeded."
            },
            "questions": {
                "value": "**Questions:**\n\nQ1: On line 42-44 you write: \"these works leave open the difficulty of statistical estimation, and therefore raise \nthe possibility that the sampling problem\u2019s true difficulty is hidden in the complexity of learning.\"\nCan you explain what you mean by this? \nEspecially, \"the difficulty of statistical estimation\", do you mean how close the estimation can get to the true distribution?\nAnd \"the sampling problem\u2019s true difficulty is hidden in the complexity of learning.\", what do you mean by this?\n\n\nQ2: Line 165: W.r.t. risk for score function being asymptotic to $n^{-2/(D+4)}$. Do you mean asymptotic as $n \\to \\infty$? \n(If yes, this could also be written in words, then you could avoid introducing the extra notation.)\n\n\nQ3: Line 175-179: Can you give some intuition of the F_1-norm? Is it right that F_1 is the functions for which the integral is bounded \nfor all their basis functions? And does that mean the F_1-norm is the smallest integral for the \"largest\" basis function? \n\n\nQ4: In line 180, you say that you only consider neuron parameters from the hypersphere. Is there a reason why this \ngeneralizes to neuron parameters chosen over the entire space? If there is, I would like that argument added. \nIf there is not, I think you should add this assumption in the part of the introduction where you state your contribution.  \n \n\nQ5: In line 231: In your shorthand notation, you define $\\mu_{t,x} = \\mu_0 \\lor \\sigma_t\\sqrt{D}$. I am used to $\\lor$ being \nthe logical \"or\", but that cannot be what you mean in this context. So what does it mean here? \n\n\nQ6: In your remarks under theorem 3.3, you say that you leave showing a $n^{2/(d+4)}$ to future work, but in 3.3 you give an upper \nbound, and $n^{2/(d+5)} < n^{2/(d+4)}$ for $n > 1$. So how is $n^{2/(d+4)}$ a better bound? \n\n\nQ7: I don't recognize the denpendence on $\\sigma_t$ which you mention in the remarks after theorem 3.3. Where do you have something \nbounded by $\\sigma_t^{-4/(d+5)}$? \n\n\nQ8: In line 491 you mention your \"truncation arguments\", could you say what you mean by this? And maybe give a reference to where \nyou make these arguments? \n\n\n**Suggestions:**\n\nU1: Please make sure to use the full name and not an abbriviation the first time you mention a concept.\n\tFor example: DDPM in line 66, ERM in line 74, DNN in line 90 and GD in line 97. \n\n\nU2: Please make sure to explain all the notation you use before you use it. If you feel it would take up too much space \nin the main paper, you can make a list of used notation in the appendix. \nSpecific notation, where I cannot find an explanation in the paper: \n$\\mathcal{F}_t$ in equation 3.7 (I am guessing this is the space of score functions), $\\asymp$ in line 165, \ng cts in line 176 in the definition of the Total-Variation norm, the $\\lor$ in line 231 and equation 3.10 and\nLaw(.) in last line of corollary 3.4. \n  \n\nU3: I feel the title over-promises. \"SHALLOW DIFFUSION NETWORKS PROVABLY LEARN HIDDEN LOW-DIMENSIONAL STRUCTURE\"\nsounds like it is _all_ shallow networks and _all_ hidden low-dimensional structures. Of course I understand \nif you feel the title would get too long if you add all the assumptions, so I would suggest finding a new title \nwhich is as short or shorter, and which does not make the result sound more general than it is. \n\n\nU4: Typo in line 189: Should be $B_2(r, d)$"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}