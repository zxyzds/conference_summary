{
    "id": "mOpNrrV2zH",
    "title": "CBGBench: Fill in the Blank of Protein-Molecule Complex Binding Graph",
    "abstract": "Structure-based drug design (SBDD) aims to generate potential drugs that can bind to a target protein and is greatly expedited by the aid of AI techniques in generative models. However, a lack of systematic understanding persists due to the diverse settings, complex implementation, difficult reproducibility, and task singularity. Firstly, the absence of standardization can lead to unfair comparisons and inconclusive insights. To address this dilemma, we propose CBGBench, a comprehensive benchmark for SBDD, that unifies the task as a generative heterogeneous graph completion, analogous to fill-in-the-blank of the 3D complex binding graph. By categorizing existing methods based on their attributes, CBGBench facilitates a modular and extensible framework that implements various cutting-edge methods. Secondly, a single de novo molecule generation task can hardly reflect their capabilities. To broaden the scope, we adapt these models to a range of tasks essential in drug design, considered sub-tasks within the graph fill-in-the-blank tasks. These tasks include the generative designation of de novo molecules, linkers, fragments, scaffolds, and sidechains, all conditioned on the structures of protein pockets. Our evaluations are conducted with fairness, encompassing comprehensive perspectives on interaction, chemical properties, geometry authenticity, and substructure validity. We further provide deep insights with analysis from empirical studies. Our results indicate that there is potential for further improvements on many tasks, optimization in network architectures, and incorporation of chemical prior knowledge. To lower the barrier to entry and facilitate further developments in the field, we also provide a unified codebase (supplementary) that includes the discussed state-of-the-art models, data pre-processing, training, sampling, and evaluation.",
    "keywords": [
        "Molecule Generation Benchmark",
        "Target-Aware Drug Design",
        "Generative Model"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-14",
    "forum_link": "https://openreview.net/forum?id=mOpNrrV2zH",
    "pdf_link": "https://openreview.net/pdf?id=mOpNrrV2zH",
    "comments": [
        {
            "title": {
                "value": "Thanks a lot!"
            },
            "comment": {
                "value": "We sincerely appreciate your valuable feedback. Your support serves as a driving force for us to continue our efforts in this field and contribute to its advancement in the future."
            }
        },
        {
            "title": {
                "value": "Thanks for the rebuttal"
            },
            "comment": {
                "value": "Thanks for the authors's timely response. My concerns are well addressed. I would like to raise my score from 6 to 8.\n\nBests,"
            }
        },
        {
            "title": {
                "value": "Response to Questions"
            },
            "comment": {
                "value": "**Reply to Questions:**\n\n1. Fragment growing typically involves a larger functional group, whereas side-chain decoration may involve multiple tiny fragments, such as one scaffold corresponding to five tiny fragments[3]. In fragment growing, we choose to cleave non-cyclic single bonds to obtain two fragments, which should satisfy the two mentioned rules. Figure 3 is merely to illustrate the concept of a scaffold. In the selection of scaffolds, we adopt the BM scaffold definition, meaning the scaffold includes all cyclic structures within the molecule.\u201d The detailed implementation can obtained through Line 314 as the definition of scaffold_decomp function in the file of  `./CBGBench-master/repo/datasets/parsers/molecule_parser.py`\n2. Firstly, in lead optimization, the goal is to generate molecules structurally distinct from the original, so we have intentionally avoided restrictive metrics like substructure distribution that could limit molecular diversity. Instead, we focus on comparisons based on Vina Energy and drug properties, which do not penalize methods that generate molecules with novel scaffolds. Secondly, in the t-SNE analysis, we followed the approach in Figure 1 of [4] to examine whether the distributions of generated molecules are chemically consistent with those of the actives. We do not consider methods with diverse distributions to be inferior; the visualization serves as an exploration tool rather than a ranking criterion. Additionally, we agree with your observation that some optimization methods can generate molecules with diversity and activity. In addition to genetic approaches, iterative target optimization (similar to genetic methods but not training-free) can also enhance molecular activity [5]. We intend to explore such methods in future work related to molecular optimization.\n\n[1]  Zheng, Kangyu, et al. \"Structure-based Drug Design Benchmark: Do 3D Methods Really Dominate?.\" arXiv preprint arXiv:2406.03403 (2024).\n\n[2]  Fergus Imrie et al. Deep Generative Models for Linker Design, https://pubs.acs.org/doi/10.1021/acs.jcim.9b01120\n\n[3]  Zhang et al. Deep Lead Optimization: Leveraging Generative AI for Structural Modification. https://pubs.acs.org/doi/10.1021/jacs.4c11686\n\n[4] Mingyang Wang et al. A deep generative model for structure-based de novo drug design https://pubs.acs.org/doi/10.1021/acs.jmedchem.2c00732\n\n[5] Zhou et al. DecompOpt: Controllable and Decomposed Diffusion Models for Structure-based Molecular Optimization, https://arxiv.org/abs/2403.13829"
            }
        },
        {
            "title": {
                "value": "Thanks! Response to Weakness"
            },
            "comment": {
                "value": "Thanks for your recognition of our contribution, and here we will answer your questions to eliminate your concerns one by one:\n\n**Reply to Originality:**\n\nWe have carefully reviewed the article by Zheng et al. [1] and found that it was published on June 4, 2024, contemporaneously with our work. Thus, we believe that concerns regarding the novelty of our research are unfounded. Additionally, we would like to highlight several distinctions and advantages of our study in comparison to this work:\n\n - 1. Zheng et al. primarily aim to demonstrate that 1D/2D molecular generation methods remain highly competitive compared to 3D approaches. In contrast, our work focuses solely on generative 3D SBDD tasks. Moreover, we provide a more comprehensive evaluation of 3D molecular generation methods, incorporating over 10 3D-based methods compared to the five included in Zheng et al.\n - 2. In terms of evaluation metrics, beyond Vina Dock Energy, SA, and QED for drug properties, we include additional assessments such as PLIP interaction analysis, clash analysis, and geometric fidelity. Our 3D-based comparisons are therefore broader and more detailed.\n - 3. We have incorporated four additional lead optimization tasks, including linker, fragment, scaffold, and side-chain design. These aspects are rarely covered in previous benchmark or SBDD task-oriented papers.\n - 4. Our work unifies the code framework, whereas Zheng et al.\u2019s published GitHub repository does not include training or generation code for the methods discussed. This difference implies a significantly greater engineering effort on our part, potentially making a substantial contribution to the community. Please refer to the '.zip' file in the supplementary material for further details.\n - 5. We conducted experiments on real disease targets, ADRB1 and DRD3, which are rarely explored in previous benchmarking and SBDD task-oriented studies.\n\nTherefore, we believe that (1) questioning the novelty of a contemporaneous study is not entirely fair in this context; and (2) CBGBench possesses distinct differences and advantages compared to the work you referenced. Besides, since Zheng et al. also gives lots of insights in SBDD tasks, we have added it as a reference in the newly updated Line. 539 for the reader to understand the field easily.\n\n**Reply to Quality:**\n\n- We choose 5 as the threshold for considering the group of atoms because too few atoms cannot form appropriate fragments. Criteria of `5 atoms' primarily references the atomic number constraints used in the DeLinker[2], where the design likely focuses on common pharmacophoric substructures like five-membered rings for fragment truncation. In practice, you can freely define these based on your own requirements, and the relevant code has been uploaded.\n- Thanks for you advice on the confidence intervals computing method. We think it is really valuable, since in the macro-biomolecule design, the task of `quality identification' is very important, to tell the biologist whether the generated biomolecules are reliable. We will take the advice as future work, to establish an SBDD-version generation quality identification model, to facilitate the judgment of whether one of the tested models is significantly better than the others.\n\n**Reply to Clarity:**\n\n- We aggree with you that the phrase is confusing, so we change it into disease target, since **DRD3** is a type of dopamine receptor primarily found in the brain, related to **Schizophrenia and Parkinson\u2019s disease** and **ADRB1** is a \u03b21-type adrenergic receptor\u00a0related to h**ypertension, heart failure and Arrhythmias.** In comparison, in CrossDocked, the protein pockets are derived from structures in PDBbind, forming protein-ligand complexes that exist in the real world. However, while these receptors are indeed real-world entities, they are not necessarily disease targets. Therefore, in the newly uploaded version, we have revised references to \u201creal-world target\u201d to \u201creal-world disease target\u201d or \u201creal-world target related to disease\u201d to avoid any potential ambiguity.\n- We have incorporated your suggestion and revised the text to: \u201cthe established evaluation protocols exhibit consistency and generalizability on real-world target data in evaluating the Vina Dock Energy.\u201d\n- We have changed the font size in the Figure in the latest updated version."
            }
        },
        {
            "title": {
                "value": "Thanks! Reply to Questions and Weakness."
            },
            "comment": {
                "value": "Thanks for your valuable comments and insightful questions, here we will reply to the two weakness with questions one by one.\n\n**Reply to W1:**\n\nFirstly, in our article, one-shot models are not limited to diffusion-based models alone. As shown in Table 1, one-shot models here refer to approaches that are in contrast to auto-regressive models. For instance, LiGAN uses a VAE model framework, which we also classify under one-shot models.\n\nSecondly, we included BFN in the diffusion model category because, in diffusion processes, the forward noising step adds noise to data using  $x_t = \\alpha_t x_0 + \\sigma_t \\epsilon$ , which can be viewed as adding noise to the mean of a Gaussian distribution parameterized by $\\mu( x_t) = \\alpha_t x_0$. The reverse sampling process follows a similar logic. However, while discrete state diffusion models like [1][2] also add noise in parameter space, BFN\u2019s update strategy for categorical variables is distinctly different, yet it can still be considered a nonlinear update process in parameter space, as opposed to the linear updating strategy with Markov transition probability in [1][2].\n\nThus, both traditional diffusion models and BFN exhibit the following features: (1)  unification in parameter space for noise addition and sampling; and (2) progressive updates with either a linear or nonlinear strategy. Based on these similarities, we classify BFN as a diffusion-based approach.\n\n**Reply to W2:**\n\nWe apologize for the lack of clarity in our explanation. We agree with your point that calculating clashes within the molecule itself is meaningless, as in structures like C=O-C, it is very likely that the distance between the two carbon atoms will be less than their van der Waals radii overlap. In fact, the term Ratio_cm here does not refer to clashes within the molecule itself; rather, it indicates the proportion of molecules with protein-molecule clashes relative to the total number of molecules. If any atom in a molecule causes a clash, we consider that molecule to have a clash. Therefore, this metric is stricter than Ratio_ca and will yield a higher ratio. We will make this explanation clearer in the next version (updated line 285) to avoid any unnecessary misunderstandings.\n\nMinor\uff1a Besides, we add the suggested reference in our updated paper. Thanks for your valuable advice.\n\n \n\n[1] [Emiel Hoogeboom](https://arxiv.org/search/stat?searchtype=author&query=Hoogeboom,+E), Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions, https://arxiv.org/abs/2102.05379\n\n[2] [Andrew Campbell](https://arxiv.org/search/stat?searchtype=author&query=Campbell,+A), A Continuous Time Framework for Discrete Denoising Models, https://arxiv.org/abs/2205.14987"
            }
        },
        {
            "title": {
                "value": "Thanks! Reply and extra test on KRAS12"
            },
            "comment": {
                "value": "We sincerely appreciate your recognition of our work. In response to the issues you raised, we provide the following replies:\n\nFollowing [1], we focused on the two targets ADRB1 and DRD3, both of which play significant regulatory roles in biological systems, making case studies on these targets meaningful.\n\nAdditionally, based on your valuable feedback, we recently included experiments with the KRAS12 target (PDB code: 7O83) and used the bound structure\u2019s protein configuration as the pocket (see Fig. 10 in the Appendix E3 in the latest article version). Using the methods incorporated in our case study, we generated 100 molecules for the target, then assessed their molecular fingerprint distribution as well as Vina Energy & LBE. The results have been added to Appendix E3 in Figures 11(a) and (b). Furthermore, we provided a summary table of Mean & Std values related to binding affinity of the generated molecules on KRAS12 for brief reading.\n\n| Method | E_vina Mean | E_vina Median | LBE Mean | LBE Median |\n| --- | --- | --- | --- | --- |\n| Pocket2Mol | -7.44 | -7.43 | 0.373 | 0.342 |\n| GraphBP | -3.91 | -3.72 | 0.268 | 0.255 |\n| TargetDiff | -9.29 | -9.29 | 0.334 | 0.337 |\n| DiffSBDD | -7.67 | -7.50 | 0.297 | 0.284 |\n| DiffBP | -7.46 | -7.57 | 0.294 | 0.297 |\n| FLAG | -5.35 | -5.05 | 0.267 | 0.271 |\n| D3FG | -8.73 | -8.60 | 0.341 | 0.326 |\n| MolCraft | -9.94 | -9.67 | 0.344 | 0.347 |\n\n[1] Haotian Zhang, Delete: Deep Lead Optimization Enveloped in Protein Pocket, https://arxiv.org/pdf/2308.02172"
            }
        },
        {
            "title": {
                "value": "Thanks! Reply to the questions."
            },
            "comment": {
                "value": "Thanks for your recognition of the value of our work. Here is our reply to your questions.\n\n**Reply to Q1**:\n\nWe here use GVP as the backbone architectures for Auto-regressive models and EGNN as the architecture for One-shot models, for the two reasons:\n\n1. GVP projects the vector (positions) into a higher dimensional space, and update the latent coordinates, while EGNN directly update the 3D Euclidean coordinates. From this perspective, GVP requires more memory and computational comlexity compared to EGNN. This is because GVP projects both coordinates and attributes into a high-dimensional space. This makes GVP more feasible in an auto-regression (AR) model, where the AR model only needs to model the coordinates of one atom (the next atom) at a time. However, in one-shot methods, such as diffusion, it must model the joint distribution of the coordinates and types of  $N_{\\text{atom}}$  atoms simultaneously, leading to potentially unaffordable computational costs. Therefore, we chose the GNN architecture  with the strongest possible representational capacity within feasible conditions, adapting it to different generation methods to minimize the negative impact of insufficient network expressiveness on performance and to explore various generation strategies and methods.\n2. Moreover, as described in the introduction, we aim to make comparisons under the same network architecture as much as possible. Based on previous studies [1], [2], we can infer that Pocket2Mol achieves state-of-the-art performance in AR, while others, like GraphBP, perform less favorably. This led us to explore whether the lack of network representational capacity or the difference in generation strategies is the main cause of the performance disparity. Therefore, we adopted the GVP architecture used in Pocket2Mol to reach conclusions like those in Line 82-83 and Line 401-408. Otherwise, it would be meaningless to draw these conclusions regarding the strengths and weaknesses of different generation strategies.\n\n**Reply to Q2**:\n\nWe are sorry about the newest revision on the experimental results in the Appendix is not updated. We give the result in Figure 8 and 9 in the Appendix of the latest updated version. The t-SNE visualization distribution changed since the MolCraft molecule is added and making the training data for t-SNE different. Thanks a lot for your mentioning our mistake. \n\n**Reply to Q3&Weakness**:\n\nSince this paper primarily focuses on the SBDD task, we have not discussed broader biomolecular generation tasks. Here, we can provide an overview of this direction. A common consensus is that **de novo** biomolecule design requires models to generate both the types and structures of basic units in biomolecules. \n\nTherefore, from a structural biology perspective, these basic units are amino acids in proteins and peptides, bases in DNA, and atoms in small molecules. Although it is possible to generate these biomolecules atom-by-atom from the bottom up, the inherent \u201cbiological language\u201d of proteins (e.g., AFCUDNE\u2026) and DNA (e.g., ATCG) complicates ensuring that atomic-level generation can successfully assemble corresponding amino acids or bases. Consequently, bottom-up generation is challenging for other macro biomolecules. In contrast, a top-down generation strategy that treats functional groups as fundamental units in proteins and amino acids can inspire molecular design and SBDD tasks. Examples of such approaches include [1] and [2].\n\nBesides, from a model design perspective, as the fundamental generation units for proteins and other biomolecules are functional groups, determining the atomic coordinates within each functional group becomes an essential issue in transitioning from coarse-grained to fine-grained generation. Current models often treat the protein backbone as a rigid body and side chains as flexible elements, generating the C-alpha positions and orientations of amino acids in the backbone. This approach has inspired the motivations in SBDD tasks like [1] and the generation strategies in lead optimization like [3].\n\nOverall, while no model currently unifies **de novo** design for both marco-biomolecules and small molecules, the fields are increasingly converging. For instance, AlphaFold3 focuses on protein&DNA&RNA-ligand bound structure prediction (though not yet **de novo** design), and research on unified approaches is expected to make steady progress in the coming years.\n\n[1] Haitao Lin et al., Functional-Group-Based Diffusion for Pocket-Specific Molecule Generation and Elaboration, https://arxiv.org/pdf/2306.13769\n\n[2] Jiaqi Guan et al., DECOMPDIFF: Diffusion Models with Decomposed Priors\nfor Structure-Based Drug Design, https://proceedings.mlr.press/v202/guan23a/guan23a.pdf\n\n[3] Jiaqi Guan et al., LinkerNet: fragment poses and linker co-design with 3D equivariant diffusion, Advances in Neural Information Processing Systems 36"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a benchmark for SBDD, including a unified framework of generative graph completion for multiple tasks in the field and a comprehensive evaluation protocol."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The unified code base is a nice contribution to the community and beneficial for future research.\n2. The evaluation protocol is comprehensive with a reasonable benchmark setting.\n3. The benchmarked methods are representative and state-of-the-art."
            },
            "weaknesses": {
                "value": "1. It would be better if the author could discuss the recent trend of training a unified model for small molecules and macromolecules such as proteins and nucleic acids, and its implications on the field of SBDD."
            },
            "questions": {
                "value": "1. Why do you choose different GNN architectures for auto-regressive and diffusion-based models?\n2. MolCraft seems to be missing from the t-SNE visualization in Figure 8. Also, the distributions of Vina Dock Energy and LBE are not provided. Could you please provide these results?\n3. [credit to Associate PC] How might your benchmark and evaluation protocol need to be adapted to assess unified models that can generate both small molecules and macromolecules like proteins?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes CBGBench, a comprehensive benchmark for SBDD tasks, which aims to unify various generative models in a fill-in-the-blank framework for 3D complex binding graphs. It introduces a modular and extensible framework and evaluates multiple state-of-the-art methods across different metrics, including interaction, chemical properties, geometric authenticity, and substructure validity. The paper also introduces four sub-tasks: linker, fragment, side-chain, and scaffold design, to provide insights into lead optimization applications."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper fills a notable gap in the SBDD domain by providing a well-structured and unified benchmarking framework. The comprehensive evaluation protocol addresses the diverse nature of generative tasks.\n\nThe study uses extensive metrics to evaluate models, including metrics like Ligand Binding Efficiency (LBE) to address the size bias in generated molecules.\n\nApplication to real pharmaceutical targets like ADRB1 and DRD3 demonstrates the practical potential of the benchmark and supports the generalizability of the findings.\n\nOverall it's a really well executed paper that focuses on an particular case of generative models, which is drug design, but deserved an acceptance to the main conference due to the relevance of the task."
            },
            "weaknesses": {
                "value": "Further testing on more diverse real world systems should be done. Exploring how this models behave in systems like KRAS12 for instance where the main goal is growing into subpockets, would enrich this study."
            },
            "questions": {
                "value": "Include more systems that showcase corner cases for a benchmark that are common in real scenarios. i.e KRAS12, BRD4"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces CBGBench, addressing the gap where the absence of standardization can lead to unfair comparisons and inconclusive insights. On one hand, this paper incorporates recent state-of-the-art studies into a unified framework for fair comparisons; on the other, it extends de novo SBDD tasks to include lead optimization and other related tasks. The paper conducts extensive experiments on these tasks and provides the corresponding code."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1. This paper has evaluated almost all the prevailing SBDD methods in generative models in AI conference. \nS2. It adapt some of the models to a range of tasks essential in drug design, considered sub-tasks within the graph fill-in-the-blank tasks.\nS3. It establish a training, sampling and evaluation codebase, which is comprehensive and effective affter my testing.\nS4. The benchmark evaluates all the models for the two real-world target proteins, as a solid case study."
            },
            "weaknesses": {
                "value": "W1. Doubts of classification: In Line 165, it states that MolCraft using BFN as the variant of the diffusion models. However, I have two questions, Firstly, it appears that \u201cone-shot model\u201d in the paper refers to diffusion-based models. Given this, is it reasonable to classify BFN as a diffusion model? MolCraft generates in parameter space, while diffusion generates in data space, which I believe is a distinction. Is the authors\u2019 classification reasonable in this regard?\n\nW2. Doubts of evaluation metrics:  The article compares the clash ratio between protein and molecule interactions, denoted as Ratio_cca . Additionally, it defines the internal clash ratio within molecules, denoted as  Ratio_cm . Regarding the internal clash within molecules, since there are bonds connecting atoms, the defined \u201cvan der Waals radii overlap by \u22650.4\u00c5\u201d does not hold. For example, if this definition were applied to a benzene ring, the clash ratio would be 1.0, which fails to reflect the actual structural integrity of the molecule. Therefore, I believe this metric is unreasonable. \n\nMinor: \nAdditionally, the article lacks citations for recent work related to SBDD and molecular generation, such as [1] [2]. I suggest that the authors include these references to enhance the completeness of the article.\n\n[1] Zaixi Zhang, Mengdi Wang, Qi Lium. FlexSBDD: Structure-Based Drug Design with Flexible Protein Modeling\n[2] Odin Zhang\uff0cet al. Deep Lead Optimization: Leveraging Generative AI for Structural Modification"
            },
            "questions": {
                "value": "See Weakeness. The W1 and W2 are my main concern, and if a satisfactory response can be provided to the above issues, I will consider increasing the score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes an extended benchmark protocol for SBDD methods, and does a large comparison of existing literature, and draws some insights from the results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper does a comprehensive benchmark, with lots of interesting results.\n- The authors contribute a unified codebase, which could be very useful for future research."
            },
            "weaknesses": {
                "value": "- The motivation of the work is weak. I\u2019m not sure why we need all of this stuff, or what questions the paper answers. The paper seems to establish an extended benchmark protocol that adds few extra evaluations to standard benchmarks. This could be useful, but I found the new additions to be a bit hand-wavy. The paper also proposes \u201cunified\u201d tasks and notation, but why? These tasks and notation is already common.\n- The novel contributions of the work seem limited. The data and tasks seem to be from earlier works, and the benchmarks are added with just few extra evaluations, which also all seem to already exist. The main contribution is collecting all of this into one place. The performance comparison is a good contribution, but similar comparisons are done in every paper. The proposed new \u201cunified\u201d notation or task framing seems all well known. The main new contribution is probably the unified codebase, which can be very useful. The paper draws a bunch of conclusions from the results, which are interesting, but it\u2019s unclear how novel they are.\n- As a benchmark paper the presentation of the results is limited: there are too many decimals, no standard deviations, and there are lots of massive tables of \u201craw\u201d performance values, which are not digested into transferable insights. The analysis is superficial, and there are no ablations or attempts to explain how or why the different methods perform differently. \n- As a survey paper I didn't find this paper particularly good at summarising or organising the domain in terms of methods, data or metrics. Use of math is inconsistent."
            },
            "questions": {
                "value": "- It would be useful to extend the method table by also including which tasks they support, and their architectures and losses\n- Training losses should be discussed\n- The paper first describes the main de novo task very superficially, then it describes the subtasks more in-depth, then it goes back to describing the de novo task more in detail, and finally goes back to subtasks. It would have been much clearer to separate the main/sub tasks into different parts of the paper.\n- The task description is missing the objectives/metrics. Overall I didn\u2019t really understand the role of the four subtasks. These are very specific, and surely each method needs to have custom support for them. Which ones do? Do we expect new methods to support these 4 tasks? Isn\u2019t it enough that a method is good at the \u201cbasic\u201d de novo generation? I\u2019m a bit confused why we care about these at all, or what is their significance? It\u2019s also unclear how they relate to the probabilistic model or ML modelling. Are these some kind of conditionals?\n- It\u2019s unclear if the 4 tasks in table 2 have something to do with the main task. That is, do they share data in some sense? Are the 4 tasks more fine-grained versions of the de-novo data?\n- I\u2019m a bit confused what\u2019s the point of the \u201cunified\u201d probabilistic model notation. It seems that this is used nowhere in the paper after it\u2019s introduction.\n- Why would you look at the MAE between two distributions? This doesn\u2019t seem sensible. The distribution sizes and domains and types are mostly undefined [please use precise math]\n- What does it mean to measure \u201catom types\u201d of functional groups? I don\u2019t understand. I though that you compared just some distributions of 25 groups, which should give you a probability vector of length 25 to be compared. The at/rt/fg should have no role here. [It would help so much to define the stuff in precise math so that I wouldn\u2019t have to guess.]\n- I fail to see where the accuracy of the de novo generation is described. I thought the task is to predict the correct ligand for a known protein (assuming there is only one correct ligand). Surely you want to measure how often you get this right. The paper also talks about probabilistic model of p(M|P). Where is this density evaluated: shouldn\u2019t we see some logp values in the results?\n- Or is the task to just generate some ligands (irrespective if it was the correct one), as long as they have good binding and properties, or come from some distribution? If the task really is generative, then one expects to use learning target of maximizing the model likelihood of the observed data (both training and test folds). I don\u2019t see this in the paper either. The learning problem needs to be precise.\n- Overall it seems that some metrics in the paper are about checking that the summary statistics of $p(ligand)$ or $p(ligand|protein)$ match the true distributions, and some are about making sure some $fitness(ligand,protein)$ is high. This is not formalised well, and it leads to the metrics and learning setting being vague and hand-wavy.  \n- I fail to see the motivation for the different Vina scores. If Vina is pathological, then why would we still use it? I don\u2019t see how IMP or MPBG helps either if they are based on Vina. I don\u2019t see convincing arguments why the LBE fixes the Vina pathology: the table 11 has values all of the place. I think here less would be more, and it would be much better to just have one good metric for binding than lots of binding metrics of varying quality. \n- I did not understand the PLIP stuff.\n- Overall the metrics are difficult to interpret. I don\u2019t really know what the numbers mean. Is 0.4382 MAE good or bad? Is a 0.2345 JSD high or low? Is Vina -3.75 good or bad? Is IMP 22 good? I have no idea. It would be insightful to visualise the distributions, or use some human-understandable metrics. For instance, you could use distribution overlap percentage or something else.\n- It would have been useful to show some example generations, and their corresponding metrics.\n- Using 5M iterates in each method seems unfair. Different methods train in different ways. I think you need to analyse the discrepancy between your results and the published results to clarify this.\n- It seems that you change some methods architectures for no reason. This is not fair and will nullify the corresponding results. You can\u2019t claim to benchmark method X if you change the method X from the publication.\n- It\u2019s unclear if you reimplement the methods in your codebase, or just collect published codebases in one place. Can you clarify?\n- \u201cWe generate 100 molecules per pocket in test\u201d. How do you do this? Why do you do this? Didn\u2019t we have test data that tells us this? Now there are 10000 test molecules, while table 2 says that there are only 100. The learning setting is confusing, and using math to describe things would help. It seems that the entire setting in this paper is some kind of $D[p_gen ||\u00a0p_obs]$, which is not properly formalised.\n- \u201cWe show the interaction analysis and chemical property in the main context and omit the interaction pattern analysis since maintaining the patterns is not necessary for lead optimization.\u201d I don\u2019t understand this. How do you use \u201cchemical property\u201d as context? How do you use \u201canalysis\u201d as context? What is \"context\"?\n- I don\u2019t understand what you do in the subtask training. Surely a method can\u2019t be just applied in eg. linker generation if it wasn\u2019t designed to do it? Are all 6 methods ones that support it, or do you somehow kludge this support to them? I\u2019m really confused what do you train for extra 1M steps. What are even the loss functions? This is all super vague.\n- The results of subtasks are all over the place. I\u2019m not sure what can you conclude from this.\n- Using ECFP to compare molecules in sec 5.3. is a very weak approach, and running it through tsne2d makes it even worse. This analysis has little to no value.\n- The setting in 5.3. is weak. Thera are only 100 random controls (why not way more, like a million?). It\u2019s unclear how many molecules you sample, but this should be a large number.\n- Sec 5 concludes that the evaluation protocols are very consistent with real-world behavior. By far the best method in sec 5 is the DiffSBDD, which has by far best LBE, and ok Vina\u2019s (Vina is pathological, so we should look at LBE anyways). But in Table 7 we see DiffSBDD being ranked 10/12, ie. one of the worst methods. It\u2019s pretty clear that the real-world performance and de novo benchmark performance is not consistent."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a new benchmark for generative models in structure-based drug design. Recently, many machine learning models have been proposed to help solve the problem of designing compounds that can interact with their protein targets. However, the comparison of these methods is difficult due to the lack of standards and relevant benchmarks. CBGBench is a benchmark that aims at standardizing model comparison and proposing evaluation metrics that can be used to assess the performance of generative models. Additionally, the benchmark contains tasks that can be used to evaluate models' capabilities of fragment linking, scaffold hopping, fragment growing, and side chain generation. Several recent generative models were compared using the proposed benchmark, and conclusions were drawn from these experiments. Moreover, a case study was conducted to demonstrate the quality of compounds generated by these methods for two selected GPCR targets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality:\n- A new benchmark is proposed for comparing the performance of recently developed SBDD methods.\n- This benchmark also includes additional tasks like fragment linking or scaffold hopping, for which do not exist any standardized benchmarks.\n- A taxonomy of SBDD generative models is proposed.\n\nQuality:\n- The benchmark covers a wide range of generative models, including autoregressive and one-shot generative models.\n- Both evaluation methods and scoring network architectures are standardized.\n\nClarity:\n- The benchmark is clearly defined, and the diagrams in Figures 2 and 3 explain different tasks and model types.\n- The evaluation metrics are explained in an easy-to-follow way.\n\nSignificance:\n- The proposed benchmark should accelerate research on new generative models that generate molecules binding to a given protein.\n- The benchmarking code is attached to this submission, making it easier to test new models in the future."
            },
            "weaknesses": {
                "value": "Originality:\n- This work does not discuss other benchmarks for generative methods. In particular, Zheng et al. [1] have recently introduced an SBDD benchmark for generative models.\n\n[1] Zheng, Kangyu, et al. \"Structure-based Drug Design Benchmark: Do 3D Methods Really Dominate?.\" arXiv preprint arXiv:2406.03403 (2024).\n\nQuality:\n- The requirement that \"each connecting fragment must consist of more than five atoms\" seems arbitrary. Why are models that occasionally generate fewer atoms considered insufficient?\n- The benchmark could propose a method of computing confidence intervals, e.g. by sampling the generated molecules. It would facilitate the judgment of whether one of the tested models is significantly better than the others.\n\nClarity:\n- \"Real-world targets\" may be a confusing name for the structures used in the case study. The protein structures in the CrossDocked2020 dataset were also real-world targets, which might confuse some readers. Maybe it would be better to call the structures in the case study as selected targets.\n- Additionally, I recommend rephrasing statements like \"the established evaluation protocols exhibit strong consistency and generalizability on real-world target data.\" The real-world target data could be understood as binding data from laboratory experiments, but docking experiments usually do not correlate with such experimental data.\n- The text in Figures 4 and 5 may be too small.\n\nMinor comments:\n- A typo in line 68: \"Vina enery\"\n- A typo in line 475 \"the neurotrans-625 mitter\""
            },
            "questions": {
                "value": "1. What is the difference between fragment growing and side chain decoration on the implementation level? The text mentions ligand decomposition for the fragment growing task, but the detailed procedure of the decomposition is not explained. In Figure 3, how is the scaffold selected in the case of side chain decoration?\n2. Some of the employed evaluation metrics compare the generated distribution of molecules to a known distribution of molecules. Would this approach not penalize methods that generate molecules with novel scaffolds? Also in the t-SNE plot (Figure 4) that compares the set of generated molecules to the actives, the compounds generated by a model can be significantly different from the known actives but still active. A good example of this behavior would be genetic algorithms that optimize docking scores. These methods are not trained on any data, so they can easily learn to produce compounds that have a unique structure and dock well to the target protein."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}