{
    "id": "Tn8EQIFIMQ",
    "title": "Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice",
    "abstract": "The observed similarities in the behavior of humans and Large Language Models (LLMs) have prompted researchers to consider the potential of using LLMs as models of human cognition. However, several significant challenges must be addressed before LLMs can be legitimately regarded as cognitive models. For instance, LLMs are trained on far more data than humans typically encounter, and may have been directly trained on human data in specific cognitive tasks or aligned with human preferences. Consequently, the origins of these behavioral similarities are not well understood. In this paper, we propose a novel way to enhance the utility of language models as cognitive models. This approach involves (i) leveraging computationally equivalent tasks that both a language model and a rational agent need to master for solving a cognitive problem and (ii) examining the specific task distributions required for a language model to exhibit human-like behaviors. We apply this approach to decision-making -- specifically risky and intertemporal choice -- where the key computationally equivalent task is the arithmetic of expected value calculations. We show that a small language model pretrained on an ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts human behavior better than many traditional cognitive models. Pretraining language models on ecologically valid arithmetic datasets is sufficient to produce a strong correspondence between these models and human decision-making. Our results also suggest that language models used as cognitive models should be carefully investigated via ablation studies of the pretraining data.",
    "keywords": [
        "cognitive model",
        "language model",
        "computational models of cognition",
        "rationality",
        "economics"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "Pre-training a small language model on ecologically valid arithmetic datasets effectively captures human risky and intertemporal choices.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=Tn8EQIFIMQ",
    "pdf_link": "https://openreview.net/pdf?id=Tn8EQIFIMQ",
    "comments": [
        {
            "summary": {
                "value": "The paper shows that by solely training next token prediction on arithmetic data requiring computations related to real-world decisions (e.g., computing expectations) it's possible to predict with good accuracy human behavior. Results improve (slightly?) when the pretrained computations involve numbers which resemble real-world distributions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- I found the results of the paper very surprising, I think that the approach is creative, and to the best of my knowledge, may perhaps offer a minimal explanation for why LLMs replicate human behavior in some cognitive biases - namely, something about the next word prediction mechanism and mathematical data? Without the need for any language-related observations at all."
            },
            "weaknesses": {
                "value": "I don't have strong concerns, and I'm generally positive about this paper, though I feel like Section 3.4 (Human Targets) contains many preprocessing decisions with some very specific factors (annual discount factor set to 0.85). I didn\u2019t understand the need for these, or whether they affect the results. For example, I don\u2019t understand this part: \u201cIn cases involving ambiguous gambles where probabilities are unknown, we used the special token to denote gambles with unknown probabilities.\u201d (Line 237) Can you please elaborate?"
            },
            "questions": {
                "value": "- I wonder if the positioning of the paper may be different - something along the lines of \"LLMs were observed to replicate human cognitive biases, here we show a possible explanation, indicating that this may be due to the task, architecture, and numerical reasoning, without requiring any natural language supervision, world knowledge or common sense.\" Does this make sense? If so, I think it would have been easier for me to understand the paper.\n- I wonder what would happen if the LLM was trained on other mathematical tasks, not relating to expectation? Would it still be able to explain human behavior to some extent?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presented a method to enable a model to act as a cognitive model for a cognitive task by training it on 1) computationally similar tasks 2) with similar distribution. The authors demonstrate their method on the task of risky and intertemporal choice, by pre-training on synthetic expected value calculations data of ecological distribution. The paper compares their trained model, Arithmetic-GPT, with Llama-3-70B, and a few traditional cognitive models. Arithmetic-GPT shows a good ability to explain human data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is clear and well-written.\n2. The method is novel and demonstrates strong results for a small model and relatively small data.\n3. The results showcase the method's ability to create a good cognitive model for the target task.\n4. The deduction of implicit values from ARITHMETIC-GPT is interesting and insightful."
            },
            "weaknesses": {
                "value": "1. The paper is a bit lean on the experimental side.\n2. Lack of experimentation with different model sizes that can allow a better understanding of their effect on the model's ability to act as a cognitive model. Similarly, there are no experiments designed to assess the effect of the data quantity that is emphasized as key factors that differentiate humans from LLMs.\n3. The paper claims that their approach is general, but they only train on one type of computational task, e.g. expected value calculations. Another example can enhance the paper, even pointing to other concrete examples can be valuable. For example, does this also hold to more language-oriented tasks? in that case, are smaller dedicated models will also outperform strong LLMs? \n4. Part of your rationale for choosing ecological distribution is that in humans this can lead to computational errors that are the basis for deviating from the rational decision. However, you didn't analyze both LLaMA3 and Arithmetic-GPT computational abilities both in and out of distributions, and examine their effect on the results.\n5. There is no discussion on the trade-off between a general model like LLaMA3 that can act as a cognitive model in a wide array of tasks and a small and dedicated model as you proposed."
            },
            "questions": {
                "value": "1. There are open models with open data e.g. LLM360 and OLMo, how does this affect your LLMs experiment in the context of the paper experiments?\n2. Is there a way to produce figures like the ones in Figure 2 from LLaMA3 embeddings?\n3. Can a strong model trained on the human data provide a better upper limit and also act as a cognitive model?\n\nComments:\n1. LLaMA3 is a bit better in Intertemporal choices and on par with the Arithmetic-GPT. The text there doesn't reflect that and does not explain it. Also, the top results from LLaMA3 are not in bold."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Humans often deviate from optimal choices in tasks involving expected value or temporal patterns. This behavior is also observed in LLMs, though embeddings do not fully capture this deviation. This study explores how this tendency arises in language models by pre-training a small model (Arithmetic-GPT) on expectation calculation tasks then testing its alignment with human choices. \n\nSynthetic datasets (uniform and ecological, each with perturbed and unperturbed versions) are created for model pre-training and tested on four human datasets related to risky and inter-temporal decisions. Model comparisons included pre-trained Llama (natural language input + log-likelihoods, natural language + text embeddings, arithmetic + embeddings), Arithmetic-GPT variants (four dataset versions and no training), four behavioral models, and MLPs directly trained on task features. Results show that Arithmetic-GPT trained on unperturbed ecological data shows the highest explained variance with human decisions (input: embeddings/log-likelihoods, output: human decisions)."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- **Originality**: \n     - Proposes a new test environment to evaluate the effectiveness of using language models as cognitive models.\n     - The ecological data pre-training approach is a simple approach to model principles with human decision-making patterns.\n\n - **Quality and Clarity**: \n     - Comprehensive benchmarking across multiple models, including classical behavioral and neural models.\n     - Very clear writing, but with minor typos (e.g., line 376, \"fitted\" \u2192 \"fit\")."
            },
            "weaknesses": {
                "value": "Overall, I am uncertain if the proposed task is too simple to make any sophisticated claims on language models as cognitive models. \n\n - Does Arithmetic-GPT\u2019s performance really represent actual human decision processes, or is it not possible that Arithmetic-GPT's \"human-like behavior\u201d could instead be a reflection of general statistical tendencies that happen to deviate from optimal choice, rather than an insight into genuine human cognition?\n- Table 3 shows that Arithmetic-GPT only marginally outperforms the larger Llama model. This suggests that Llama, without specialized pre-training, already captures much of the variance in human choices. I suppose the claim is somewhat valid given the limited capacity of the Arithimetic-GPT backbone, but the authors\u2019 emphasis on Arithmetic-GPT's superiority might be overstated given that its improvement is slight and comes from training on targeted synthetic data."
            },
            "questions": {
                "value": "- A minor question regarding the experiments: Why not also try training Llama or Arithmetic-GPT on the task features as the MLP?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors explore how language models can serve as cognitive models by investigating their behavior in risky and intertemporal choice tasks. The authors introduce Arithmetic-GPT, a small language model specifically trained on arithmetic tasks that simulate expected value (EV) and present value (PV) calculations essential to decision-making. By training Arithmetic-GPT (A-GPT) on a non-ecologically and ecologically valid datasets, the authors show that A-GPT exhibits decision-making patterns that closely resemble human choices (in particular for the ecologically valid dataset). Moreover, A-GPT outperforms both traditional behavioral models and some large, general-purpose language models in predicting human risk and time preferences. The work porposed by the authors underscores the importance of tailored training datasets for aligning language model behavior with human cognitive processes and provides insights into how synthetic datasets may enhance models' ability to predict human decision patterns."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This work exemplifies the potential of LLMs to gain insights in human cognitive processing, and as a results showcases the elegance of the auhtors thought process.\n\n2. The auhors compare their model to a relevant suite of other models, and use several datasets to drive their point. This systematic comparison highlights the quality of this work and further amplifies the results obtained with A-GPT.\n\n3. The paper is clear and well structured, allowing for smooth reading. The figures are informative and clearly illustrate the points made in the text."
            },
            "weaknesses": {
                "value": "Within the range of what the paper is tackling, I do not see any weaknesses in this work.\n\nI do however believe that the authors could have gained further understanding of human risky and intertemporal choices by generating several other datasets that systematically target specific aspect of EV and PV calculations, i.e., parametrically changing the ablation level of specific EV and PV compononents. \n\nMoreover, it would be interesting to relate the A-GPT results with explainability in order to potentially provide more interpretability to the embeddings.\n\nI will however not discount any points in my rating. I understand the limitations in terms of computation,cost, time and manuscript space."
            },
            "questions": {
                "value": "Typo:\n\nThere is an issue with the presentation of Figure A2 which falls under appendix F rather than E."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}