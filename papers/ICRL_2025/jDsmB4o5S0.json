{
    "id": "jDsmB4o5S0",
    "title": "Dual Process Learning: Controlling Use of In-Context vs. In-Weights Strategies with Weight Forgetting",
    "abstract": "Language models have the ability to perform in-context learning (ICL), allowing them to flexibly adapt their behavior based on context. This contrasts with in-weights learning (IWL), where memorized information is  encoded in model parameters from iterated observations of the data (e.g., common sayings). An ideal model should be able to maintain both of these abilities. Despite their apparent ability to learn in-context, language models are known to struggle when faced with unseen or rarely seen tokens (Land & Bartolo, 2024). Hence, we study $\\textbf{structural in-context learning}$, which we define as the ability of a model to execute in-context learning on arbitrary novel tokens \u2013 so called because the model must generalize on the basis of e.g. sentence structure or task structure, rather than content encoded in token embeddings. We study structural in-context algorithms on both synthetic and natural tasks using both toy models and MultiBERT models (Sellam et al., 2021). We find that structural ICL appears before quickly disappearing early in LM pretraining. While it has been shown that ICL can diminish during training (Singh et al., 2023), we find that prior work does not account for structural ICL. Building on the Chen et al. (2024) 's active forgetting method used to help models learn new languages, we introduce a pretraining method that can modulate the preference for true structural ICL and IWL. Importantly, this allows us to induce a $\\textit{dual process strategy}$ where in-context and in-weights solutions coexist within a single model.",
    "keywords": [
        "Natural Language Processing",
        "In-Context Learning",
        "In-Weights Learning",
        "Active Forgetting"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "We explore why in-context learning abilities on rare and unseen tokens disappears over training, and introduce a training intervention that fixes these failures.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=jDsmB4o5S0",
    "pdf_link": "https://openreview.net/pdf?id=jDsmB4o5S0",
    "comments": [
        {
            "summary": {
                "value": "This paper studies the emergence and disappearance of in-context vs in-weights learning in masked language models; in particular, a kind of ICL termed \u201cstructural ICL\u201d, which refers to the ability to infer (and then use) information about the structural role (e.g., POS) of words from the context. In experiments, this ability appears and then disappears over the course of MLM training. A recently proposed \u201cactive forgetting\u201d approach helps keep the ability, leading to models that use both in-context and in-weights learning."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper studies a question of substantial interest, nicely continuing prior work on different kinds of learning\n- considers both real-world LM and toy models\n- Provides novel insights by distinguishing \"structural\" from \"conditional\" ICL\n- Provides method for mitigating the loss of structural ICL in a toy setup"
            },
            "weaknesses": {
                "value": "- Structural ICL is operationalized using performance on random token embeddings. However, as the paper also demonstrates (Figure 18), such embeddings are drastically OOD for a trained model. Wouldn't a fairer test use novel embeddings that match the distributional properties of the embeddings of the trained model?\n- Results on Structural ICL in MultiBERT (Section 3) are based on POS = noun vs adjective as the only property. It remains unclear how robust findings are to other properties, such as other pairs of POS, or other properties, such as grammatical number.\n- Results on Structural ICL in MultiBERT (Section 3) are based only on probing accuracy, not behavior of the model. This seems suboptimal given that in-context learning as an emergent property of LLMs is generally thought of as an ability appearing in next-token prediction (e.g., complete a prompt with an appropriate label), One could test for ICL in properties such as POS behaviorally by creating contrastive examples where the key word has different POS and comparing model probabilities.\n- The term \u201cDual Process\u201d here is used to refer to models using both in-weights and in-context learning, and the paper refers to Kahneman 2011; Miller 2000. The link to dual process theory appears tenuous: dual process theory refers to thse use of implicit, unconscious vs explicit, conscious processes; the link to in-weights vs in-context inference is at best hand-waved in line 90-91, but no explicit justification for the link is given. The choice of the term and the link to dual process theory thus appears a bit of a stretch to apply the in this context.\n- It remains unclear if the proposed strategy for keeping structural ICL abilities would transfer to real-world LMs. While training an LLM is understandably out of scope, even a modest LM could provide substantial insight here."
            },
            "questions": {
                "value": "See weaknesses.\n\nMinor note: Line 317-8: it seems the sentence is missing a verb"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper examines \"structural\" in-context learning (ICL), a specific type of ICL where some tokens in the model\u2019s prompt are randomly initialized. The motivation for studying structural ICL is based on prior observations that ICL behaves unexpectedly when prompts include tokens that are rarely encountered during training, i.e., undertrained tokens. This paper uses synthetic tasks to demonstrate that structural ICL develops early in training but diminishes as training progresses. The authors show that periodically reinitializing token embeddings during training (active forgetting) preserves structural ICL abilities, albeit at the expense of memorizing information in the model\u2019s embeddings. The paper further demonstrates that both structural ICL and memorization are maintained if active forgetting is employed, but only for the first N steps of training. This procedure is referred to as \u201ctemporary forgetting.\u201d"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. I\u00a0appreciated that\u00a0this\u00a0work distinguishes\u00a0structural\u00a0ICL\u00a0from\u00a0ICL in\u00a0general. This seems\u00a0like\u00a0a\u00a0natural and\u00a0interesting\u00a0extension\u00a0to prior\u00a0work\u00a0that studies in-weights\u00a0versus in-context\u00a0learning.\n2. Despite\u00a0the lack of\u00a0experiments on\u00a0downstream\u00a0tasks, I\u00a0found the\u00a0breadth\u00a0of the\u00a0experiments\u00a0satisfactory. I\u00a0also\u00a0recognize the\u00a0creativity\u00a0employed\u00a0by the\u00a0authors\u00a0given\u00a0their\u00a0compute constraints."
            },
            "weaknesses": {
                "value": "1. No\u00a0experiments on downstream\u00a0tasks. While the\u00a0motivation\u00a0to\u00a0address the\u00a0issue\u00a0of undertrained\u00a0tokens is\u00a0interesting, it is\u00a0difficult to\u00a0assess\u00a0how\u00a0training with\u00a0temporary\u00a0forgetting\u00a0will affect\u00a0language models\u00a0in\u00a0real-world applications. Given\u00a0the\u00a0author\u2019s limited\u00a0computational\u00a0resources, they\u00a0might consider\u00a0a\u00a0variant of temporary\u00a0forgetting\u00a0that\u00a0can be\u00a0employed\u00a0as a fine-tuning step, e.g., fine-tune with\u00a0active forgetting\u00a0for\u00a0$k$ steps before\u00a0resetting\u00a0the\u00a0token\u00a0embeddings\u00a0to\u00a0their\u00a0original\u00a0states, leaving\u00a0only the\u00a0model parameters\u00a0changed.\n2. Limited\u00a0technical novelty. Unless\u00a0I misunderstood\u00a0the\u00a0paper, temporary forgetting is\u00a0simply\u00a0active\u00a0forgetting applied for\u00a0$K$ steps. This\u00a0is only\u00a0a\u00a0minor weakness in\u00a0my\u00a0view\u00a0and is\u00a0excusable\u00a0if\u00a0the\u00a0authors could\u00a0find\u00a0a\u00a0way\u00a0to\u00a0perform experiments\u00a0on downstream\u00a0tasks.\n3. The\u00a0presentation can\u00a0be improved. For\u00a0example, there\u00a0are\u00a0instances where\u00a0the figure\u00a0legends contain\u00a0terms\u00a0that\u00a0are not\u00a0defined. In Figure\u00a06 (right),\u00a0\u201cVanilla\u00a0Forgetting\u201d is\u00a0mentioned. Figure\u00a011 uses\u00a0the term\u00a0\u201cStop\u00a0Forgetting,\u201d which\u00a0I believe\u00a0is intended\u00a0to\u00a0mean\u00a0\u201cTemporary\u00a0Forgetting.\u201d"
            },
            "questions": {
                "value": "1. What\u00a0are\u00a0the\u00a0limitations of\u00a0temporary forgetting? Is\u00a0it strictly\u00a0beneficial, or\u00a0did\u00a0the authors\u00a0find\u00a0that\u00a0some model\u00a0abilities\u00a0deteriorated? Given\u00a0that\u00a0evaluation wasn\u2019t\u00a0performed\u00a0on downstream\u00a0tasks, what\u00a0synthetic\u00a0experiments can\u00a0be\u00a0conducted\u00a0to\u00a0evaluate\u00a0the\u00a0trade-offs of temporary forgetting?\n2. Suggestion: Since\u00a0no evaluation\u00a0was\u00a0performed\u00a0on downstream tasks, this\u00a0paper could\u00a0be\u00a0improved\u00a0by\u00a0expanding on why studying undertrained\u00a0tokens\u00a0is\u00a0important (beyond what\u00a0is\u00a0already\u00a0discussed\u00a0in\u00a0the\u00a0introduction). I\u00a0am familiar\u00a0with this\u00a0area, so the introduction was\u00a0adequate\u00a0for\u00a0me, but\u00a0adding\u00a0such a\u00a0section\u00a0might\u00a0make this\u00a0paper\u00a0more broadly accessible."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a distinction between *structural* and *conditional* in-context learning, where structural ICL can generalise to novel tokens because it does not depend on token embeddings. They show that by default, structural ICL emerges and then sharply diminishes during pretraining, and they introduce a novel pretraining method based on periodically reinitialising the embedding matrix, which prevents the loss of structural ICL."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The central thesis is clear\n- The contributions are well-grounded in prior literature\n- The distinction between structural and conditional ICL is compelling\n- The experimental results support the thesis well, and are quite extensive, especially on the dynamics of structural versus conditional ICL"
            },
            "weaknesses": {
                "value": "I feel there are two main weaknesses.\n\nFirstly, it is not clear to me how important structural ICL actually is as an ability for models\n  - The connection to glitch tokens is fair, but I don't expect these to be very common, and even when they do occur I'm not sure that preserving structural ICL is the best response - for instance, it wouldn't have fixed all the canonical solidgoldmagikarp problems, because structural ICL only preserves syntactic knowledge of glitch tokens, but fundamentally cannot give semantic knowledge of unknown tokens\n  - The application to very rare tokens is likewise fair but by definition not very common, and again it is unclear to me whether in these cases there are any important capabilities which are purely syntactic\n\nSecondly, it is not clear to me how far active/temporary forgetting impairs other abilities in models\n- My intuition is that repeatedly reinitialising the embedding matrix is encouraging structural ICL by damaging other kinds of learning\n- While the temporary forgetting approach might ameliorate this, it is not clear to me by how much or in what way it does this\n\n\nOverall, I find much of the paper compelling in its presentation and exploration of structural ICL as a phenomenon, but I would be even more enthusiastic about it if it were clearer to me:\n- that structural ICL is an important property (which could be demonstrated by natural examples of its relevance)\n- what the side-effects are of the proposed methods for preserving structural ICL"
            },
            "questions": {
                "value": "**Q1**\n\nTo what extent does active forgetting impair other model capabilities, if at all? What about temporary forgetting? I'd be interested to hear hypotheses, and particularly any experimental results you have. \n\n**Q2**\n\nDo you believe the problem of glitch tokens will recur in future, and in such a case, do you think preservation of structural ICL would be a useful strategy?\n\n**Q3**\n\nCan you give an example (or ideally a few examples) of cases where models might have a hard time performing a natural task because they are incapable of performing structural ICL on a rare token? \n\n---\n\nI have provisionally put down a marginal acceptance, but would be happy to revise my score upwards if these questions are appropriately addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the limitations and corresponding solutions for in-context learning with unseen tokens (i.e., structural ICL). Under this scenario, language models (LMs) learn the sentence structure (i.e., simple syntax structure) instead of token semantic meanings. Thus, with carefully designed forgetting strategies, the authors show that they can let LMs perform well under the structural ICL setting."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.  This paper investigates an interesting research question: comparing ICL with parameter-tuning. Some explorations are helpful and provide useful insights. \n2.  Besides, the idea of combining ICL with structures is inspiring. Most previous ICL works run experiments on the sentence classification datasets, which are highly semantic-related. The discussion about sentence structures instead of prompt structures are limited."
            },
            "weaknesses": {
                "value": "1. Some statements about ICL explanation are not up-to-date. The authors mentioned that typical ICL algorithms do not have stable learning manner and named it as conditional ICL. However, some recent works [1,2] have shown that ICL learns the task composition: the model only learns how to compose the task based on the learned tasks from pretraining, instead of learning from scratch based on the ICL examples. Thus, some claims in paragraph 2 of the introduction should be updated.\n\n2. Experiment settings are a bit problematic. This work is based on previous findings. However, these findings are not general across all settings. For example, Singh et al., 2023 find that ICL slowly dissipates as models are overtrained ONLY for decoder-based LMs without pretraining. In their limitation section, they also mentioned that the conclusion may not hold for pretrained LMs (e.g., Multi-BERT here) under other scenarios. I can\u2019t list all of them, but in general, what I mean is that we should be careful about the assumptions of previous work, and cannot simply generalize and rely on previous conclusions.\n\n    Second, since this paper uses encoder-based LMs, it may not be able to perform ICL as there is little discussion about it. Besides, ICL ability is often used for large models. Implementing experiments on Multi-BERT with probing accuracy instead of generation is not convincing to show the conclusions of this paper is useful in practice.\n\n3. The sentence structure is learned from pretraining instead of predefined. Thus, the model may not treat sentences as human do (i.e., with linguistic structures). Even if for the simple POS triple structure it doesn\u2019t matter, other more general or complex structures may not work.\n\n4. The potential impact of this work may be a bit limited. The motivation of injecting novel tokens under the ICL setting is not clear to me. As aforementioned, ICL is often used for large models, where their tokenizer vocabulary is large enough and it\u2019s not necessary to add extra new tokens (e.g., LLaMA 3-8B). While for small models that may require to extend the tokenizer (frequently), it may not perform ICL well and they are not expected to be used under this scenario. \n\n[1] Hahn M,; Goyal N. A theory of emergent in-context learning as implicit structure induction[J]. arXiv preprint arXiv:2303.07971, 2023.\n\n[2] Li, J.; Hou, Y.; Sachan, M.; and Cotterell, R. What Do Language Models Learn in Context? The Structured Task Hypothesis. ACL 2024"
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}