{
    "id": "vvi5OjPhbu",
    "title": "Youku Dense Caption: A Large-scale Chinese Video Dense Caption Dataset and Benchmarks",
    "abstract": "With the explosive growth of video content, video captions have emerged as a crucial tool for video comprehension, significantly enhancing the ability to understand and retrieve information from videos. However, most publicly available dense video captioning datasets are in English, resulting in a scarcity of large-scale and high-quality Chinese dense video captioning datasets. To address this gap within the Chinese community and to promote the advancement of Chinese multi-modal models, we develop the first, large-scale, and high-quality Chinese dense video captioning dataset, named Youku Dense Caption. This dataset is sourced from Youku, a prominent Chinese video-sharing website. Youku Dense Caption includes 31,466 complete short videos annotated by 311,921 Chinese captions. To the best of our knowledge, it is currently the largest publicly available dataset for fine-grained Chinese video descriptions. Additionally, we establish several benchmarks for Chinese video-language tasks based on the Youku Dense Caption, including retrieval, grounding, and generation tasks. Extensive experiments and evaluations are conducted on existing state-of-the-art multi-modal models, demonstrating the dataset's utility and the potential for further research.",
    "keywords": [
        "Chinese Video Datasets",
        "Retrieval",
        "Grounding",
        "Generation"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vvi5OjPhbu",
    "pdf_link": "https://openreview.net/pdf?id=vvi5OjPhbu",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces Youku Dense Caption, a large-scale Chinese dense video captioning dataset. Dataset addresses the scarcity of high-quality Chinese video captioning resources, containing 31,466 short videos with 311,921 Chinese captions. A strategies is proposed to improve benchmark quality by filtering out redundant or low-quality annotations. The authors establish several benchmarks for Chinese video-language tasks and conduct extensive experiments demonstrating the dataset's utility and potential for research. They also discuss challenges related to the linguistic and cultural differences between Chinese and English video data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. A Chinese video captioning dataset is proposed to fill the research gap in the Chinese community for video captioning data.\n2. A embedding-based similarity and a Non-Maximum Suppression method is used to set up a Chinese PRVR benchmark that effectively reduces annotation redundancy.\n3. The work reduces redundancy in video captioning and grounding by filtering out videos with high self-BLEU scores and minimal scene changes,  which is measured through color histogram correlation, ensuring a diverse and representative dataset."
            },
            "weaknesses": {
                "value": "1. The statement in the section \u201cChinese Characteristics\u201d seems unclear. The English translation of the so-called \u201cfine-grained Chinese captions\u201d could also serve as fine-grained English captions in an English-language context. For me, only the localized data part is valuable, as it highlights a major difference between Chinese and English video captions. Adding more data and statistics to support this distinction would strengthen the paper.\n2. In the experiment, when translated back to Chinese. It's kind of blur that which attributes of Chinese dataset lead to the poor performance, the analysis failed to state clearly about the language differences between Chinese and English.\n3. In ablation study, mixing of different datasets only strike a balance between different tasks but failed to achieve idealized performance across different tasks. And the best performance comes from larger data scale rather than data distribution and video-caption pair. \n4. Overall, the dataset serve as a valuable data source for Chinese community in video caption domain, but the value and key attributes of the dataset remain unclear and is not fully proved by the experiment."
            },
            "questions": {
                "value": "see weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents Youku Dense Caption, the largest publicly available dataset for Chinese dense video captioning, comprising 31,466 videos annotated with 311,921 captions. Collected from the Youku video platform, this dataset addresses the lack of high-quality Chinese video captioning datasets and promotes advancements in Chinese multi-modal research. It provides benchmarks for key tasks such as retrieval, grounding, and generation, and extensive experiments demonstrate its effectiveness on state-of-the-art multi-modal models. The dataset\u2019s scale and quality make it a valuable resource for future research in video-language understanding."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Youku Dense Caption contains 31,466 short videos annotated with 311,921 captions, making it the largest dataset for fine-grained Chinese video descriptions.It addresses the scarcity of high-quality Chinese dense video captioning datasets, promoting advancements in Chinese multi-modal models and video-language research.The dataset establishes benchmarks for key video-language tasks such as retrieval, grounding, and generation, with extensive experiments demonstrating the utility of the dataset on state-of-the-art multi-modal models."
            },
            "weaknesses": {
                "value": "1. Lack of Caption Diversity and Detail: In Figure 1, the captions for different segments of Video ID 1070344446 show high similarity with little distinction, lacking sufficient variability. Additionally, the descriptions are relatively simple and do not provide background information about the visual content.\n2. Potential Hallucination in Captions: In the D. Implementation Details of Baselines section, the authors mention that they convert videos to 320p resolution and remove the audio component. However, in Figure 1, the second frame of Video ID 1192027222 shows the caption: \u201cThe old lady boasts that young women who work hard at chopping can do it.\u201d It is difficult to determine solely from the visual content that the old lady is boasting, raising concerns about the potential for hallucinated captions, especially for those tied to audio-related information."
            },
            "questions": {
                "value": "1. Vocabulary Statistics and Comparison: Can the authors provide vocabulary statistics and a comparison with other datasets? The captions appear to contain a high degree of repetition.\n2. Threshold for Average Self-BLEU: Why was the threshold for Average Self-BLEU set to 0.15?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper details a novel Chinese language based dense video caption dataset."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "+ This work collected a new Chinese based dense video captioning dataset (named Youku Dense Caption). The dataset has several benchmarks for Chinese video-language tasks, including retrieval, grounding, and generation tasks. \n+ This allow developer and researcher to train multmodal foundation model with a fair benchmark. \n+ This submission has validate the impact of large scale dataset on existing multimodal model. Providing empirical evidence of the advantage of rich data in the context of Chinese language."
            },
            "weaknesses": {
                "value": "- The proposed dataset's video are evenly sampled from the Youku-mPLUG dataset based on dedicated (sub)categories. So the assumption is that the licensing should not be an issues. To properly handle the copyright concerns, please details the licensing terms for the Youku-mPLUG dataset, and discuss the coverage of usage right, redistribution policies, and any restriction."
            },
            "questions": {
                "value": "- Has the author consider to release the English caption of the proposed dataset? In my opinion, this will broaden the impact of this dataset and benefit more downstream tasks. Please confirm if this is a a planned future work, as well as to discuss the plan for creating high-quality English translation of the captions. Further analysis on the English translation with open-source tool in Fig 1, it is clear that it requires professional proofreading or checked by someone (crowdsourcing?) who are fluent in both Chinese and English language.\n\n- This paper should provide addition details related to the annotation progress. Please provide detilas on: \n1. The total number of human annotators involved.\n2. The qualifications or expertise of the annotators (e.g., native Chinese speakers, etc.) and how are them recruited. \n3. The cost and time spent on annotation per video. \n4. The total number of annotations per annotator and the overall duration. \n5. Any quality control measures sued during the annotation process."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Youku Dense Caption dataset, which is the largest publicly available Chinese dense video captioning dataset for now. The dataset is annotated by human to guarantee the quality of the dataset.\nBuilding upon the proposed dataset, the paper also establishes benchmarks for video-language tasks.\nThe experiments demonstrate that existing state-of-the-art multi-modal models can benefit from this dataset."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper focuses on the topic of Chinese dense video captioning dataset, which is an interesting and under-explored research area."
            },
            "weaknesses": {
                "value": "- Doubtful value of the proposed dataset\n    - First, while the proposed dataset is claimed to be a dense video captioning dataset, the collection pipeline is similar to regular video captioning datasets, like HD-VILA-100M or Panda-70M, where a long video is first segmented into multiple clips and each clip is annotated with a caption. Could the authors provide more differences between the dataset collection pipelines of your dataset and a regular video-text dataset?\n    - Second, it is unconvincing to state that \"Chinese and English have significant linguistic differences, so a Chinese dataset is needed\". I appreciate the authors show the errors of translation in line 211 and Section 3.2.2. However, I use ChatGPT to translate the provided samples and it can produce correct results in most of the cases. Take the leftmost sample in Figure 3 as example, I got this: \"A group of motorcyclists is resting by the roadside, chatting.\" from ChatGPT, which is totally correct.\n\n- Lack of necessary experiments: to evaluate the value of the proposed dataset, the authors need to train a model on different datasets and show that the one trained on the proposed dataset is more robust than the others. Such experiment should be conducted on different tasks, such as dense video generation, partially relevant video retrieval. However, none of this experiment is presented.\n\n- It has been shown that long and detailed prompts are beneficial to various tasks, such as video generation. However, the caption annotations are short and less detailed, limiting the value of the dataset.\n\n- For the scene change detection algorithm mentioned in lines 345~364, TransNet-v2 should be more robust than the adopted pixel-based algorithm."
            },
            "questions": {
                "value": "- How is a long video segmented into multiple clips? What is the splitting criteria?\n\n- Could you show more data samples in Youku Dense Caption dataset? It is helpful for checking the diversity, visual quality, and annotation quality  of the proposed dataset?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}