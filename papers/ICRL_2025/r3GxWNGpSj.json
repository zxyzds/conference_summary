{
    "id": "r3GxWNGpSj",
    "title": "XTransplant: A Probe into the Upper Bound Performance of Multilingual Capability in LLMs via Cross-lingual Transplantation",
    "abstract": "Current large language models (LLMs) often display significant imbalances in their multilingual capabilities and cultural adaptability, primarily due to their unbalanced and English-centric pretraining data.\nFor these English-centric LLMs, the disparities between English and non-English languages hinder their ability to utilize their robust English-based capabilities within non-English contexts, while also limiting access to valuable multilingual knowledge derived from non-English \"language-specific neurons\" within English contexts.\nMotivated by this, our work explores the possibility for LLMs to leverage the strengths of both English and non-English languages, aiming to further unlock their multilingual potential.\nTo this end, we propose a probing method named $\\mathcal{X}$Transplant, which directly transplants feed-forward activations from English input to non-English (or from non-English to English) during inference stage, allowing the model to benefit from both English and additional multilingual knowledge.\nThrough extensive experiments on our pilotsets and representative LLMs across different tasks and languages, we empirically prove that both the multilingual capabilities and cultural adaptability of LLMs hold the potential to be significantly improved by the cross-lingual feed forward transplantation, respectively from $\\texttt{En} \\rightarrow \\texttt{non-En}$ and $\\texttt{non-En} \\rightarrow \\texttt{En}$. \nAdditionally, we also establish the upper bound performance of LLMs obtained through $\\mathcal{X}$Transplant (relative growth of +80\\% in multilingual capabilities, +39\\% in cultural adaptability), highlighting the underutilization of current LLMs' multilingual potential. \nWe do hope our further analysis and discussion could suggest promising directions for deeply unlocking the multilingual potential of current English-centric LLMs.",
    "keywords": [
        "Large language models",
        "Multilingual capability",
        "Feed forward activations",
        "Upper Bound Performance"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=r3GxWNGpSj",
    "pdf_link": "https://openreview.net/pdf?id=r3GxWNGpSj",
    "comments": [
        {
            "title": {
                "value": "Reply to your comments (part 4)"
            },
            "comment": {
                "value": "7. **Section 5.1: Language consistency**: Language consistency refers to the ability that LLMs should maintain the same language for both input and output if no extra requirements are asked. The reason we conducted such analysis is to investigate whether these feed-forward activations from other language might cause shifts in the output language, as you just mentioned.\n\n    We understand your intuition that transplanting feed-forward activations from other languages might result in shifts in output language. However, our results indicate that XTransplant maintains language consistency nearly perfectly.\n\n    There are several possible reasons for this outcome:\n    - XTransplant only modifies the activations of a single layer, and for models like LLaMA-2-7B-Chat, which has 32 layers, this single-layer editing may be not enough to lead to a shift in the output language.\n    - XTransplant does not directly modify the attention mechanism, which is responsible for the core generative capabilities of the model. This may further explain why language consistency is preserved.\n    \n    To provide a clearer understanding, we include some actual response examples from LLaMA-2-7B-Chat on the Chinese subset of the XQuAD dataset. We hope these examples will help illustrate the language consistency observed in practice.\n\n    ```\n    # LLaMA-2-7B-Chat's original responses\n    1 \"23\u201316\u3002\"\n    2 \"\u91ce\u9a6c\u961f\u6253\u8d25\u4e86\u7b2c 49 \u5c4a\u8d85\u7ea7\"\n    3 \"17 \u79d2\u3002\"\n    4 \"\u4e39\u4f5b\u3002\"\n    5 \"\u57f9\u987f\u00b7\u66fc\u5b81\u3002\"\n    6 \"\u6c83\u5fb7\u3002\"\n    7 \"\u90fd\u5e02\u90e1\u6216\u90fd\u5e02\u53bf (powiat grodzki)\u3002\\n\\n\u4eba\u7c7b\uff1a\"\n    8 \"\u7531\u4e8e\u7b2c\u4e8c\u6b21\u4e16\u754c\u5927\u6218\u7684\u7206\u53d1\u800c\u505c\"\n    9 \"\u8bfa\u66fc\u4eba\u5728\u610f\u5927\u5229\u3001\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\"\n    10 \"\u52a0\u90a3\u5229\u7fa4\u5c9b\u5728\u975e\u6d32\u5927\u897f\u6d0b\u6cbf\u5cb8\u3002\"\n\n    # LLaMA-2-7B-Chat's responses after applying XTransplant\n    1 \"23\u201316\u3002\"\n    2 \"\u91ce\u9a6c\u961f\u6253\u8d25\u4e86\u65b0\u82f1\u683c\u5170\u7231\u56fd\u8005\"\n    3 \"17 \u79d2\u3002\"\n    4 \"\u4e39\u4f5b\u3002\"\n    5 \"\u57f9\u987f\u00b7\u66fc\u5b81\u3002\"\n    6 \"\u6c83\u5fb7\u3002\"\n    7 \"\u6ce2\u5170\u7b2c\u4e8c\u7ea7\u884c\u653f\u533a\u662f\u90e1 (county)\u6216\"\n    8 \"\u534e\u6c99\u8bc1\u5238\u4ea4\u6613\u6240\u505c\u6b62\u8fd0\"\n    9 \"\u8bfa\u66fc\u4eba\u5728\u610f\u5927\u5229\u3001\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\"\n    10 \"\u52a0\u90a3\u5229\u7fa4\u5c9b\u5728\u975e\u6d32\u5927\u897f\u6d0b\u6cbf\u5cb8\"\n    ```\n\n    According to the above comparisons, we can tell that XTransplant has an impact on the answers of No.2, No.7, and No.8 cases. Regardless of whether XTransplant has an impact on the final answer, we can find that XTransplant does a good job in maintaining language consistency.\n\n8. **Line 203-204: \"Questions in above datasets are in different multilingual languages\"**: Thank you for pointing this out. In XNLI, XQuAD and XCOPA datasets, all of the questions are linguistically parallel across languages. This means that the questions in each language split are essentially the same, with differences only in the language versions. We will revise the statement for better clarity. \n\n9. **Line205: \"we we performed\" -> \"we performed\"**: Thank you for your careful observation, we will correct the typo error in our paper."
            }
        },
        {
            "title": {
                "value": "Reply to your comments (part 3)"
            },
            "comment": {
                "value": "> **3. Response to your \"Suggestions / Questions\"**\n\n1. **Curse of multilinguality and negative interference**: We agree with your observation that these two terms are related. We will condense the sentences to improve clarity and avoid redundancy.\n\n2. **Figure 1 caption**: We appreciate your feedback. We will revise the caption to include a concise description of how the proposed method works, to clarify the process for the readers.\n\n    ```\n    Overview of how XTransplant achieves cross-lingual feed forward transplantation from one language to another language, **taking the direction of En -> non-En as the example**. During the prediction of the first new token when prompting in non-English, XTransplant transplants the feed-forward activations of certain decoder layer from English input into the inference process of non-English input, with the forward propagation andsubsequent token generation to proceed with the transplanted activations.\n    ```\n\n3. **Line 74: \"Given a certain question\"**: We understand your concern and will limit the scope of the question to one that may requires knowledge learned from texts in other text. This will provide greater clarity to the reader.\n\n4. **Line 145: \"Another version\" of x_s**: Thank you for pointing this out. Your assumption is right and we will specify that the \"another version\" of x_s refers to its translation in another language.\n\n5. **Line 161: Intuition behind changing only the first new token**: We appreciate your question and we are sorry for not providing a more detailed explanation of this issue in our paper.\n\n    - The reason we only applying XTransplant when predicting the first new token is that, in **autoregressive generation**, applying XTransplant during the generation of the first new token essentially introduces the benefit of feed-forward activations from another language across the entire sequence generation process. This is because all subsequent tokens are influenced by the activations cached from earlier steps. If XTransplant were applied during the generation of every token, it would be a redundant operation and could even cause the model's output to break down.\n\n    - If the first new token does not change before and after the transplanting, **the remaining tokens still hold the potential to be influenced**. Because during the generation, we know that the attention results are cached in **\"past_key_values\"**. Although we only modify the feed-forward activations, these changes will change the next input to affect the attention results for the next token prediction. These modified attention results are then cached in \"past_key_values,\" which continue to influence the prediction of the remaining tokens.\n\n    We hope this clarifies the reasoning behind.\n\n6. **Section 3.2: Bi-directional transplan**: Thank you for your thoughtful feedback. The term \"bi-directional transplant\" was chosen to highlight that XTransplant can be applied in two directions: from En -> non-EN and from non-EN -> En, respectively in multilingual and culture-aware tasks. And we understand your point, and terms like \"Mutual\" or \"Parallel\" seem better fitting. We will take your suggestion into serious consideration and review the terminology accordingly."
            }
        },
        {
            "title": {
                "value": "Reply to your comments (part 2)"
            },
            "comment": {
                "value": "> **2. Response to your concern about \"The upper bound results and maybe transplant self-attention outputs\"**\n\nThank you for raising this concern, which shows that you have carefully considered our paper. We truly appreciate your feedback!\n\n- First, we would like to clarify that the upper bound results presented in our main experiments are not intended to represent the absolute theoretical limits of the model\u2019s capabilities. Rather, we view them as an exploration of the model\u2019s upper bound within the setting of our XTransplant mechanism.\n\n    And we think that **the exact value of this upper bound is not the primary focus of our work**. The key point is that the cross-lingual feed-forward activation transplantation mechanism in XTransplant demonstrates the potential to substantially unlock the multilingual capabilities of LLMs. As highlighted in our paper\u2019s title, XTransplant  serves as a **\"probe\"** to investigate the latent potential, rather than claiming to achieve the absolute maximum performance of the model.\n\n- The reason we focus on transplanting only feed-forward activations rather than the entire hidden states or self-attention outputs is twofold:\n\n    - **one is about our motivation and some related work**: Our approach aims to enable LLMs to fully leverage both English and non-English multilingual knowledge during the inference stage. And the feed-forward layers have been shown in many studies to play a crucial role in **storing factual knowledge**[1][2][3], which is why we chose to focus on transplanting feed-forward activations.\n\n        [1] Transformer feed-forward layers are key-value memories.\n\n        [2] Knowledge neurons in pretrained transformers.\n\n        [3] Locating and editing factual associations in gpt.\n\n    - **Another reason is about Practical Considerations with Model Performance**: Based on the above-mentioned studies, it can be understood that the general workflow of the model consists of \"attention for thinking\" and \"feed-forward for knowledge\". The attention mechanism plays a decisive role in the overall generation process. If we were to patch the entire hidden states, it would inevitably affect the attention outputs as well, causing the model's output to break down. We provide some examples of such breakdowns as follows:\n    ```\n    Model: Llama-2-7b-chat  Dataset: XNLI  Language: Chinese (zh)\n\n    # XNLI is a multilingual Natural Language Inference dataset, the answers in Chinese should be one of \"(1) \u8574\u6db5\", \"(2) \u4e2d\u7acb\", \"(3) \u77db\u76fe\"\n\n    # Results after XTransplant only feed-forward activations\n    \"\\n\\n(1) \u8574\u6db5\\n\\n\u8be6\u7ec6...\"\n    \"\\n\\n(1) \u8574\u6db5\u3002\\n\\n\u6839\u636e\u6211\u7684...\"\n    \"\\n\\n(1) \u8574\u6db5\u3002\\n\\n\u6839\u636e\u6211\u7684...\"\n    \"\\n\\n(1) \u8574\u6db5\\n\\n\u6839\u636e\u6211\u7684\u7406\u89e3\"\n    \"(2) \u4e2d\u7acb\u3002\\n\\n\u89e3\u91ca\uff1a\u5728\u8fd9\u79cd...\"\n    \"\\n\\n(1) \u8574\u6db5\u3002\\n\\n\u6839\u636e\u524d\u63d0\u548c...\"\n    \"\\n\\n(1) \u8574\u6db5\u3002\\n\\n\u6839\u636e\u524d\u63d0\u548c...\"\n    \"\\n\\n(1) \u8574\u6db5\\n\\n\u6839\u636e\u6211\u7684\u7406\u89e3...\"\n    ...\n    ----------------------------------------------------\n    # Results after XTransplant with entire hidden states (including attention part)\n    \"Portail\u3002\"\n    \"Portail\u3002\"\n    \"Portail\u3002\\n\\n\u8be6\u7ec6\u89e3\u91ca\uff1a\\n\\n\uff081\uff09...\"\n    \"Portail\u3002\"\n    \"Portail\u3002\\n\\n\u6839\u636e\u8bed\u5883\uff0c\u6211\u4eec\u53ef\u4ee5\u77e5\u9053...\"\n    \"Portail\u3002\"\n    \"Portail\u3002\\n\\n\u6839\u636e\u4e0a\u9762\u7684\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u77e5\u9053\uff1a\\n...\"\n    \"Portail\u3002\\n\\n\u4eba\u7c7b\uff1a\u597d\uff0c\u6211\u53ef\u4ee5\u7406\u89e3\u3002\u4f46\u662f\uff0c...\"\n    ...\n    ```\n    Based on the above generation results comparisons, we can easily tell that the generation ability of Llama-2-7b-chat will be significantly damaged when \"XTransplant with entire hidden states\". And we are sorry for not explaining this issue in our manuscript, and we will reorganize and add this explanation in our paper.\n\nOnce again, thank you for your valuable feedback, and we hope this addresses your concerns."
            }
        },
        {
            "title": {
                "value": "Reply to your comments (part 1)"
            },
            "comment": {
                "value": "Thank you so much for your valuable and Insightful review comments. We have responded to your questions in detail and look forward to your reply to address any further questions you may have.\n\n> **1. Response to your concern about \"The computational cost\"**\n\nThank you for your insightful feedback. We fully acknowledge the computational cost of exhaustively investigating all N\u00d7N combinations in our main experiments.\n\n- However, it is important to emphasize that the extensive experiments serve to demonstrate the potential of XTransplant as a simple yet promising mechanism for enhancing multilingual capabilities and cross-lingual transfer, without any modifications to the LLM itself. **This does not imply that future work must replicate the same approach in terms of computational expense**.\n\n- As the first work to explore this mechanism, we felt that the effort spent on a comprehensive analysis was justified, and we believe it has successfully showcased the substantial potential of XTransplant.\n\n- Additionally, while our work involves significant computational expense, we do not intend for future research to replicate this approach in a resource-intensive manner. **Our goal is to inspire future work that can utilize the concept/mechanism of cross-lingual feed-forward activations transplantation to model design or training phases**, such as exploring cross-lingual feed-forward interactions and connections between different layers or language-specific regions, to enable the model better leverage its intrinsic multilingual knowledge. \n\n- Furthermore, the idea of transplantation is not limited to multilingual or cross-lingual scenarios but could also be applied to cross-task or cross-domain settings.\n\n- Lastly, we hope that the large-scale experiments we conducted, along with our subsequent analysis and discussions, can inspire future research and provide valuable insights for the broader research community."
            }
        },
        {
            "title": {
                "value": "Reply to your comments (part 1)"
            },
            "comment": {
                "value": "Thank you so much for your valuable and Insightful review comments. We have responded to your questions in detail and look forward to your reply to address any further questions you may have.\n\n> **1. Response to your concern about \"The computational cost\"**\n\nThank you for your insightful feedback. We fully acknowledge the computational cost of exhaustively investigating all N\u00d7N combinations in our main experiments.\n\n- However, it is important to emphasize that the extensive experiments serve to demonstrate the potential of XTransplant as a simple yet promising mechanism for enhancing multilingual capabilities and cross-lingual transfer, without any modifications to the LLM itself. **This does not imply that future work must replicate the same approach in terms of computational expense**.\n\n- As the first work to explore this mechanism, we felt that the effort spent on a comprehensive analysis was justified, and we believe it has successfully showcased the substantial potential of XTransplant.\n\n- Additionally, while our work involves significant computational expense, we do not intend for future research to replicate this approach in a resource-intensive manner. **Our goal is to inspire future work that can utilize the concept/mechanism of cross-lingual feed-forward activations transplantation to model design or training phases**, such as exploring cross-lingual feed-forward interactions and connections between different layers or language-specific regions, to enable the model better leverage its intrinsic multilingual knowledge. \n\n- Furthermore, the idea of transplantation is not limited to multilingual or cross-lingual scenarios but could also be applied to cross-task or cross-domain settings.\n\n- Lastly, we hope that the large-scale experiments we conducted, along with our subsequent analysis and discussions, can inspire future research and provide valuable insights for the broader research community."
            }
        },
        {
            "title": {
                "value": "Reply to your comments (part 3)"
            },
            "comment": {
                "value": "> **5. Response to your concern about \"Limited practical applicability\"**\n\nThank you for raising this important concern. You are absolutely right that XTransplant requires parallel bilingual data (non-English and corresponding English sentences) as input, which may not always be readily available in real-world applications. Perhaps this problem can be alleviated through translation techniques, but we still acknowledge this limitation and are actively exploring ways to extend the applicability of our method in scenarios where parallel data may be scarce. We appreciate your feedback and we will have a discussion about potential solutions and future work in our paper.\n\n---\n> **6. Response to your question about \"The input and output language of our datasets (XQUAD)\"**\n\nWe are happy to provide more details about the dataset as follows:\n\n- Regarding the multilingual datasets (XNLI, XQuAD, and XCOPA), all of the QA pairs are linguistically parallel across languages. This means that the questions and answers in each language split are essentially the same, with differences only in the language versions.\n\n    **\u300cInvolved Languages\u300d**\n\n    XNLI (15): ar, bg, de, el, en, es, fr, hi, ru, sw, th, tr, ur, vi, zh\n\n    XQuAD (12): ar, de, el, en, es, hi, ro, ru, th, tr, vi, zh\n\n    XCOPA (11): en, et, ht, id, it, sw, ta, th, tr, vi, zh\n\n- Regarding the multicultural dataset, GlobalOpinionQA, all the questions and answers are in English. The purpose of this dataset is to explore how well models respond to questions from different cultural backgrounds within an English context.\n\n    **\u300cInvolved Cultures\u300d**\n\n    GlobalOpinionQA (24 cultures): am, ar, bn, de, el, en, es, fr, hi, id, it, ja, ko, nl, pt, ru, sv, sw, tl, tr, uk, ur, vi, zh-CN (QA pairs are all presented in English)\n\nWe hope this resolves your issue.\n\n---\n> **7. Response to your question about \"the performance on the English XNLI test set increases significantly from 60 to 94\"**\n\nThank you for your question, we are glad share you more with the details you just mentioned. \n\nActually, \"the performance on the English XNLI test set increases significantly from 60 to 94\" benefits from a \"English2English\" setting. Under the \"English2English\" setting, XTransplant simplifies to replacing the feed-forward activations between different decoder layers within the same English input.\n\nWe would like to emphasize that this result is indeed logical.\n\nBecause different decoder layers of LLMs capture distinct features of the input and activate different neurons (i.e., knowledge). Thus, the transplanting operation between these layers can **strengthen feature propagation** and **encourage feature reuse**, leading to performance improvements. This phenomenon is analogous to the dense connections in **DenseNet[1] (Huang and Liu)**, which has been shown to enhance feature flow and overall performance.\n\nWe hope this explanation addresses your concern. And we are sorry for not clarifying this issue in the original manuscript. We will include this explanation in the revised version of the paper.\n\n[1] Densely Connected Convolutional Networks.\n\n---\n> **8. Response to your question about \"The details on how the layers are chosen and how the transplantation is performed\"**\n\nWe are happy to provide further clarification on the details.\n\n- Firstly, as shown in Table 1 of the main experiments, we present the **instance-aware upperbound results of XTransplant**. For each question, there are N^2 possible combinations of source and target layers selection (where N is the number of layers in the model). These upper bound results are obtained by exhaustively enumerating all N^2 combinations of source and target layers, representing the best performance achieved throught XTransplant mechanism. The significant gap between the upper bound results and the model\u2019s original performance in Table 1 further demonstrates that the model has substantial untapped potential for multilingual and multicultural tasks without any modifications to its original parameters.\n\n- Regarding the results presented in Figure 5, as illustrated with the example of LLaMA-2-7B-Chat on the XNLI dataset, each cell in the grid, denoted as grid(i, j), corresponds to the accuracy obtained when performing XTransplant by selecting the i-th layer as the source layer and the j-th layer as the target layer for all instances in XNLI.\n\n    For the layer-specific upper bounds in Figure 5, these represent the upper bound results when either the source layer or the target layer is fixed. For example, the value of 72.3 in the lower-left corner of Figure 5 indicates the upper bound result when the target layer is fixed as the 1st layer and the source layer is enumerated over all N possible choices.\n\nWe hope this clarifies our experimental procedure."
            }
        },
        {
            "title": {
                "value": "Reply to your comments (part 2)"
            },
            "comment": {
                "value": "> **3. Response to your concern about \"Why not patching the whole hidden states, but only feed-forward activations\"**\n\nThank you for raising this insightful question, which shows that you have carefully considered our paper. We truly appreciate your feedback!\n\nThe reason we focus on transplanting only feed-forward activations rather than the entire hidden states is twofold:\n\n- **One is about our motivation and some related work**: Our approach aims to enable LLMs to fully leverage both English and non-English multilingual knowledge during the inference stage. And the feed-forward layers have been shown in many studies to play a crucial role in **storing factual knowledge**[1][2][3], which is why we chose to focus on transplanting feed-forward activations.\n\n[1] Transformer feed-forward layers are key-value memories.\n\n[2] Knowledge neurons in pretrained transformers.\n\n[3] Locating and editing factual associations in gpt.\n\n- **Another reason is about Practical Considerations with Model Performance**: Based on the above-mentioned studies, it can be understood that the general workflow of the model consists of \"attention for thinking\" and \"feed-forward for knowledge\". The attention mechanism plays a decisive role in the overall generation process. If we were to patch the entire hidden states, it would inevitably affect the attention outputs as well, causing the model's output to break down. We provide some examples of such breakdowns as follows:\n```\n# Model: Llama-2-7b-chat  Dataset: XNLI  Language: Chinese (zh)\n\n# XNLI is a multilingual Natural Language Inference dataset, the answers in Chinese should be one of \"(1) \u8574\u6db5\", \"(2) \u4e2d\u7acb\", \"(3) \u77db\u76fe\"\n\n# Results after XTransplant only feed-forward activations\n\"\\n\\n(1) \u8574\u6db5\\n\\n\u8be6\u7ec6...\"\n\"\\n\\n(1) \u8574\u6db5\u3002\\n\\n\u6839\u636e\u6211\u7684...\"\n\"\\n\\n(1) \u8574\u6db5\u3002\\n\\n\u6839\u636e\u6211\u7684...\"\n\"\\n\\n(1) \u8574\u6db5\\n\\n\u6839\u636e\u6211\u7684\u7406\u89e3\"\n\"(2) \u4e2d\u7acb\u3002\\n\\n\u89e3\u91ca\uff1a\u5728\u8fd9\u79cd...\"\n\"\\n\\n(1) \u8574\u6db5\u3002\\n\\n\u6839\u636e\u524d\u63d0\u548c...\"\n\"\\n\\n(1) \u8574\u6db5\u3002\\n\\n\u6839\u636e\u524d\u63d0\u548c...\"\n\"\\n\\n(1) \u8574\u6db5\\n\\n\u6839\u636e\u6211\u7684\u7406\u89e3...\"\n...\n----------------------------------------------------\n# Results after XTransplant with entire hidden states\n\"Portail\u3002\"\n\"Portail\u3002\"\n\"Portail\u3002\\n\\n\u8be6\u7ec6\u89e3\u91ca\uff1a\\n\\n\uff081\uff09...\"\n\"Portail\u3002\"\n\"Portail\u3002\\n\\n\u6839\u636e\u8bed\u5883\uff0c\u6211\u4eec\u53ef\u4ee5\u77e5\u9053...\"\n\"Portail\u3002\"\n\"Portail\u3002\\n\\n\u6839\u636e\u4e0a\u9762\u7684\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u77e5\u9053\uff1a\\n...\"\n\"Portail\u3002\\n\\n\u4eba\u7c7b\uff1a\u597d\uff0c\u6211\u53ef\u4ee5\u7406\u89e3\u3002\u4f46\u662f\uff0c...\"\n...\n```\nBased on the above generation results comparisons, we can easily tell that the generation ability of Llama-2-7b-chat will be significantly damaged when \"XTransplant with entire hidden states\".\n\nWe hope this explanation clarifies our decision to focus on feed-forward activations rather than patching the entire hidden states. Thank you again for your valuable comment!\n\n---\n> **4. Response to your suggestion about \"Providing deeper insights into the practical applications of XTransplant\"**\n\nThank you very much for your valuable suggestion. Our exploration of XTransplant as a bold and novel attempt to enhance multilingual capabilities and cross-lingual transfer is indeed intended to inspire future work, including strategies for continued training, as you mentioned. We are more than happy to include some of our thoughts on how XTransplant may inform future directions in the \"Discussion Section\" of our paper, and we greatly appreciate your feedback.\n\nAdditionally, we would be happy to share a few of our preliminary thoughts here:\n\nIn the area of multilingual LLMs, the isolation between language-specific neurons for English and those for other languages[1] has made it challenging for models to simultaneously leverage the knowledge stored in both the English-specific neurons and those for other languages. XTransplant addresses this by enabling cross-lingual feed-forward activations transplantation, allowing LLMs to take advantage of the strengths of both English and non-English languages. This concept could naturally be extended to the training phase of models, where cross-lingual interactions/connections between different layers and language-specific regions could be established, facilitating knowledge sharing across languages. We do hope that XTransplant will inspire further intriguing avenues for future research.\n\nThank you again for your insightful suggestion.\n\n[1] How do Large Language Models Handle Multilingualism?"
            }
        },
        {
            "title": {
                "value": "Reply to your comments (part 1)"
            },
            "comment": {
                "value": "Thank you so much for your valuable and Insightful review comments. We have responded to your questions in detail and look forward to your reply to address any further questions you may have.\n\n> **1. Response to your concern about \"Lack of comparative comparisons\"**\n\nThank you very much for your valuable feedback and for suggesting additional comparative settings. We had indeed considered these aspects during the design of our experiments, but we feel that these methods may not be entirely suitable for our experiments. And I would like to clarify the reasons behind our choices as follows:\n\n- (Regarding Supervised Fine-Tuning methods)\nAs noted, the experiments we designed for comparisons, such as XTransplant, Parallel Input in Multiple Languages (PIM), and Chain-of-Thought (CoT), are all training-free methods aimed at further unlocking the model's potential during **inference stage**. These methods are focused on activating the model\u2019s capabilities **without altering its parameters through additional training data**.  And the purpose of XTransplant, as well, is to show that the LLM has a lot of room to fully realize its capability in multilingual and culture-aware tasks without any changes to LLM itself.\n    \n    On the other hand, task-specific supervised fine-tuning modifies the model\u2019s parameters based on extra training data, leading to performance improvements that **cannot be considered as an additional unlocking of the model's inherent potential, since the model itself is being modified**. Therefore, we chose not to include supervised fine-tuning methods as a direct comparison.\n\n    Moreover, it is worth mentioning that our proposed XTransplant mechanism can also serve as a post-enhancement technique after supervised fine-tuning, allowing for a \"second jump\" in performance beyond the improvements achieved through fine-tuning alone. (**similar to how Chain-of-Thought can be applied both before and after supervised fine-tuning.**)\n\n- (Regarding Translation-then-Inference approaches) \nWe would like to clarify that the \"translation-then-inference\" pipeline is not suitable for the tasks in our study. It seems there might be some aspects of our work that were not fully clear, which led to your concern about this.\n\n    Our study involves 4 specific datasets: XNLI, XQuAD, XCOPA, and GlobalOpinionQA. \n\n    - For the multilingual datasets (XNLI, XQuAD, and XCOPA), all of the questions are linguistically parallel across languages. This means that the questions in each language split are essentially the same, with differences only in the language versions. Therefore, the \"English Set\" in these datasets already represents the translated version, as you described with a \"machine translation\" approach. As such, there is no need to apply additional translation, which is why we do not consider \"translate-then-inference\" as a suitable baseline for these tasks.\n\n    - Regarding the multicultural dataset, GlobalOpinionQA, all the questions and answers are in English. The purpose of this dataset is to explore how well models respond to questions from different cultural backgrounds within an English context. Therefore, machine translation would not be applicable here either.\n\nWe hope this clarifies the reason why we do not utilize \"supervised fine-tuning\" and \"translation-then-inference\" as comparative comparisons.\n\n\n> **2. Response to your concern about \"No comparisons between LLMs of different model sizes\"**\n\nThank you very much for pointing this out. We acknowledge that this could be a potential limitation in our work. The reason we did not compare different model sizes is that the scope of our experiments was already exceptionally large.\n\nAs noted between \"lines 213\u2013215\" of our manuscript, to obtain the instance-aware upper bound of XTransplant, we perform inference on all N^2 possible source and target layer selection strategies for each instance. For example, in LLaMA-2-7B-Chat with N=32, N^2=1024 times inferences are conducted per instance. Our main experiments involve 3 LLMs and 4 pilotset datasets, resulting in over 800 hours of computation on 8 * A800-SXM4-80GB. Therefore, including comparisons across different model sizes would nearly double or triple this computational cost, which would be challenging given our current resources.\n\nWe appreciate you for pointing this out and hope you could understand the computational constraints."
            }
        },
        {
            "title": {
                "value": "Reply to your comments (part 2)"
            },
            "comment": {
                "value": "> **4. Response to your question about \"Why do you only use En->non-En configuration in cross-lingual tasks and non-En->En in cross-cultural tasks?\"**\n\nThank you for your question. The choice of configurations for multilingual and culture-aware tasks is tied to the specific characteristics of the datasets we use. Allow me to elaborate:\n\nOur study involves four datasets: XNLI, XQuAD, XCOPA, and GlobalOpinionQA.\n\n- For the multilingual datasets (XNLI, XQuAD, and XCOPA), all of the questions are linguistically parallel across languages. This means that the questions in each language split are essentially the same, with differences only in the language versions. These datasets assess the model's multilingual capabilities by asking questions in various languages such as Chinese, Spanish, German, French, etc. When posing questions in these non-English languages, we aim for the model to benefit from feed-forward activations derived from English. Therefore, for multilingual tasks, we perform \"En->non-En\" XTransplant, where questions are asked in non-English languages, and activations from English are transplanted to the non-English languages.\n\n- Regarding the culture-aware dataset, GlobalOpinionQA, all the questions and answers are in English. The purpose of this dataset is to explore how well models respond to questions from different cultural backgrounds within an English context. When asking questions in English, we want the model to leverage feed-forward activations from non-English languages to better capture cultural nuances. Hence, for culture-aware tasks, we perform \"non-En->En\" XTransplant, where the questions are in English, but activations from non-English languages are transplanted into the English context. For example, when asking a question related to Chinese culture, we ask the question in English but feed-forward activations from Chinese are transplanted to help.\n\nI hope this clarifies the rationale behind the configurations we use for these distinct tasks.\n\n\n> **5. Response to your question about \"The 0.0 accuracy in the English subset of XCOPA for Qwen-2\"**\n\nThank you for pointing out the \"0.0 accuracy\" in the English subset of XCOPA for Qwen-2. We are sorry for not providing a detailed explanation of this result in our paper. \n\nUpon observing this result, we were also puzzled, so at that time, we specifically revisited Qwen-2's responses to the English subset of XCOPA.\n\nWe found that the \"0.0 accuracy\" issue stemmed from the model's failure to effectively follow the instructions in our prompt. The exact prompt we used was:\n```\nYou are assigned to complete a two-category classification task.\n\nPremise: The girl squeezed her nose.\nOptions: (1) The baby drools on the bib.\n(2) The baby soiled his diaper.\n\nPlease determine which of the two options is more likely to be the cause of the given premise.\n\nYour Answer:\n```\n\nHowever, Qwen-2's responses were as follows:\n```\n\" Option 1 (The baby drools on the bib) is less likely to be the cause of\"\n\" Option 1, \\\"The audience clapped their hands to the music,\\\" is more likely to be\"\n\" Option 1 is more likely to be the result of the given premise. If the man expected the\"\n\" Option 2, \\\"Her opponent felt sorry for her,\\\" is more likely to be the result of\"\n\" Option 2, The products are made by child labor. \\n\\nExplanation: The premise states that radicals\"\n\" Option 2, \\\"It's snack time,\\\" is more likely to be the cause of the given\"\n...\n```\n\nOur evaluation script considers a model's response correct only if it contains the correct option (e.g., (1) or (2)) and excludes all other options. But as you can see above, Qwen-2's responses do not match this format, leading to the \"0.0 accuracy\".\n\nTo ensure fairness in evaluation, we can not arbitrarily modify our evaluation script based solely on Qwen's responses on the English subset of the XCOPA dataset. Therefore, we have retained this result in our main experimental table.\n\nWe hope this clarifies the issue, and again, we are sorry for not clarifying this issue in the original manuscript."
            }
        },
        {
            "title": {
                "value": "Reply to your comments (part 1)"
            },
            "comment": {
                "value": "> **1. Response to your concern about \"Your suspicion that the improvements of the UpperBound method result mainly from lurking into the gold answer\"**\n\nThank you for your feedback. \n\nFirstly, we would like to clarify that, in the entire mechanism of XTransplant, **there is no possibility of golden answer leakage**. The only input to XTransplant consists of two different language inputs, and the core operation involves transplanting the feed-forward activations of a specific decoder layer from one language input into the inference process of another language input. This process does not involve any interaction with the gold answer. **If there is any lingering doubt regarding this, we have provided the source code for XTransplant in the Supplementary Material, and we would be happy for you to check it**.\n\nRegarding your suspicion about the improvement in the \"English2English\" setting, we would like to emphasize that this result is indeed logical. In this setting, XTransplant simplifies to replacing the feed-forward activations between different decoder layers within the same input. Different decoder layers of LLMs capture distinct features of the input and activate different neurons (i.e., knowledge). Thus, the transplanting operation between these layers can **strengthen feature propagation** and **encourage feature reuse**, leading to performance improvements. This phenomenon is analogous to the dense connections in **DenseNet[1] (Huang and Liu)**, which has been shown to enhance feature flow and overall performance.\n\nWe hope this explanation addresses your concern.\n\n[1] Densely Connected Convolutional Networks.\n\n\n> **2. Response to your concern about \"The computational cost of choosing the upper bound pairs of layers\"**\n\nFirstly, we would like to emphasize that **XTransplant is not a specific methodology but rather a mechanism** \u2014 one that has not been explored in previous works \u2014 using cross-lingual feed-forward activation transplantation.\n\nTherefore, our work does not focus on \"how to choose the upper bound pairs of layers?\". The essence of our work is to explore cross-lingual feed-forward activation transplantation as a bold, novel attempt in the area of enhancing multilingual capabilities and cross-lingual transfer. \n\nWe begin by conducting extensive experiments (over 800 hours of computation on 8 * A800-SXM4-80GB) to demonstrate the underutilization of current LLMs' multilingual potential. And through the upper bound results of XTransplant and subsequent analysis (including the alleviation you just mentioned), we try to show that XTransplant is a feasible and promising mechanism for improving both multilingual capabilities and cultural adaptability. While we certainly hope our work can inspire more related future research, such as the more efficient and accurate identification of appropriate pairs of layers, our focus in this paper is on the exploration and analysis of this novel cross-lingual feed-forward activation transplantation approach, rather than providing a specific methodology.\n\n> **3. Response to your concern about \"The lack of results for base models\"**\n\nThank you for raising this concern. I would like to explain why we chose to use \"chat/instruct\" models rather than base models in our experiments:\n\n- The concern you raised may stem from the fact that many works in the field of \"model editing\" and similar areas often focus on base models. However, we believe that working with base models may not be as practically meaningful, as they are typically limited to theoretical explorations. In real-world applications, we mostly work with instruction-tuned models like \"chat/instruct\" models, which have been adapted for task-specific responses. Therefore, in our experiments related to XTransplant, we chose to focus on \"chat/instruct\" models.\n\n- One significant issue with using base models is their lack of instruction-following capabilities. This would require us to prompt the models with multiple demonstrations in a few-shot setting, introducing extra complexity and noise into our analysis. To avoid this additional interference and ensure a cleaner evaluation of XTransplant, we chose to use \"chat/instruct\" models, which are better equipped to answer a wide range of questions directly from the dataset.\n\nI hope this clarifies our reasoning for selecting these models, and we appreciate this thoughtful feedback."
            }
        },
        {
            "title": {
                "value": "Reply to your comments (part 2)"
            },
            "comment": {
                "value": "> **3. Response to your concern about \"Not utilizing machine translation as a baseline\"**\n\nThank you for pointing this out. However, we would like to clarify that the \"translate-then-xxx\" pipeline is not suitable for the tasks in our study. It seems there might be some aspects of our work that were not fully clear, which led to your concern about this.\n\nOur study involves 4 specific datasets: XNLI, XQuAD, XCOPA, and GlobalOpinionQA. \n\n- For the multilingual datasets (XNLI, XQuAD, and XCOPA), all of the questions are linguistically parallel across languages. This means that the questions in each language split are essentially the same, with differences only in the language versions. Therefore, the \"English Set\" in these datasets already represents the translated version, as you described with a \"machine translation\" approach. As such, there is no need to apply additional translation, which is why we do not consider \"translate-then-xxx\" as a suitable baseline for these tasks.\n\n- Regarding the multicultural dataset, GlobalOpinionQA, all the questions and answers are in English. The purpose of this dataset is to explore how well models respond to questions from different cultural backgrounds within an English context. Therefore, machine translation would not be applicable here either.\n\nWe hope this clarifies the reason why we do not utilize \"machine translation\" as a baseline.\n\n\n> **4. Response to your concern about \"The computational cost of choosing the upper bound pairs of layers\"**\n\nAs emphasized in our first response, **XTransplant is not a specific methodology but rather a mechanism**. Therefore, our work does not focus on \"how to choose the upper bound pairs of layers?\" The essence of our work is to explore cross-lingual feed-forward activation transplantation as a bold, novel attempt in the area of enhancing multilingual capabilities and cross-lingual transfer. We begin by conducting extensive experiments (over 800 hours of computation on 8 * A800-SXM4-80GB) to demonstrate the underutilization of current LLMs' multilingual potential. And through the upper bound results of XTransplant and subsequent analysis, we try to show that XTransplant is a feasible and promising mechanism for improving both multilingual capabilities and cultural adaptability. While we certainly hope our work can inspire future research, such as the more efficient and accurate identification of appropriate pairs of layers, our focus is on the exploration and analysis of this novel cross-lingual feed-forward activation transplantation approach, rather than providing a specific methodology.\n\n> **5. Response to your question about \"What is the reason for applying XTransplant only when generating the first new token?\"**\n\nThank you very much for noticing this question. We are sorry for not providing a more detailed explanation of this aspect in our paper.\n\nIn autoregressive generation, applying XTransplant during the generation of the first new token essentially introduces the benefit of feed-forward activations from another language across the entire sequence generation process. This is because all subsequent tokens are influenced by the activations cached from earlier steps. If XTransplant were applied during the generation of every token, it would be a redundant operation and could even cause the model's output to break down.\n\nWe hope this clarifies the reasoning behind."
            }
        },
        {
            "title": {
                "value": "Reply to your comments (part 1)"
            },
            "comment": {
                "value": "> **1. Response to your concern about \"The unfairness of comparisons\"**\n\nThank you for your feedback. It seems there might be a **misunderstanding** regarding the purpose of our main experiments. While we have presented both the baseline results and the upper bound results of XTransplant in the same table, our intention was not to claim that XTransplant is a superior method to the other baselines. We do not make any such comparison or assertion in the conclusions of our experiment.\n\nIn fact, one of the key objectives of our main experiments is to highlight that both the PIM results and the upper bound results of XTransplant show performance improvements beyond the original model\u2019s capabilities. This serves to demonstrate that, for current LLMs, simply prompting them under multilingual or culture-aware contexts does not fully exploit their multilingual potential. And by showcasing the upper bound results of XTransplant, we aim to highlight that there is still significant room for improvement.\n\nAdditionally, we would like to emphasize that **XTransplant is not a specific methodology but rather a mechanism** \u2014 one that has not been explored in previous works \u2014 using cross-lingual feed-forward activation transplantation.\n\nWe hope this clarifies our intentions.\n\n> **2. Response to your concern about \"The test set is too small\"**\n\nAs you pointed out, we have retained 50 instances for each language in the test set (aka the pilotset in our paper). The specific details are as follows:\n\n**\u300cInvolved Languages / Cultures\u300d**\n\nXNLI (15): ar, bg, de, el, en, es, fr, hi, ru, sw, th, tr, ur, vi, zh\n\nXQuAD (12): ar, de, el, en, es, hi, ro, ru, th, tr, vi, zh\n\nXCOPA (11): en, et, ht, id, it, sw, ta, th, tr, vi, zh\n\nGlobalOpinionQA (24 cultures): am, ar, bn, de, el, en, es, fr, hi, id, it, ja, ko, nl, pt, ru, sv, sw, tl, tr, uk, ur, vi, zh-CN (QA pairs are all presented in English)\n\n**\u300cSample Size (50 samples per language / culture)\u300d**\n\nXNLI: $50 \\times 15 = 750$\n\nXQuAD: $50 \\times 12 = 600$\n\nXCOPA: $50 \\times 11 = 550$\n\nGlobalOpinionQA: $50 \\times 24 = 1200$\n\nAnd for multilingual tasks (XNLI, XQuAD, XCOPA), we ensure that each language set consists of the same 50 questions, presented in different language versions.\n\nThe reason we have sampled only 50 instances per language is due to the extensive scale of our experiments. As noted between \"lines 213\u2013215\" of our manuscript, to obtain the instance-aware upper bound of XTransplant, we perform inference on all N^2 possible source and target layer selection strategies for each instance. For example, in LLaMA-2-7B-Chat with N=32, N^2=1024 times inferences are conducted per instance. Our main experiments involve 3 LLMs and 4 pilotset datasets, resulting in over 800 hours of computation on 8 * A800-SXM4-80GB.\n\nWe appreciate you for pointing this out and hope you could understand the computational constraints."
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose a new method, XTransplant, to exploit English-based capabilities within non-English contexts using English-centric Large Language Models (LLMs). For this purpose, XTransplant utilizes the feed-forward activations of a decoder layer on English text into a layer of the decoder on non-English input (or from non-English into English) while predicting the first new token. To select optimal pairs of layers in different languages, the authors investigate all possible combinations and their performance, referred to as the instance-aware upper bound in this paper. The experimental results on XNLI, XQuAD, and XCOPA with LLaMA-2-7B-Chat, Mistral-7B-Instruct-v0.3, Qwen2-7B-Instruct, and Chinese-Alpaca-2-7B show that the proposed XTransplant improve the performance under the setting of the instance-aware upper bound."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- In an ideal situation, XTransplant can enhance task-solving performance by accessing the knowledge of centric languages like English.\n- In an ideal situation, XTransplant can work on both English- and Chinese-centric LLMs.\n- This work investigates all possible pairs of source and target layers."
            },
            "weaknesses": {
                "value": "- In the experiments, the authors compared the upper bound results of XTransplant using the best combination of layers with baseline results. This is unfair and should not be reported as the main result of XTransplant. Reporting the result for analyses is acceptable. Instead, the authors can use the average or median performance in Figure 5 as the main result.\n- For fair comparison, the authors need to decide the pair of source and target layers based on the performance of validation data.\n- Furthermore, the test set size for each language is limited to 50 instances. This size is quite small. The authors need to consider the variance of the results for each language in such a small setting. Thus, increasing the test set size is required to make the results more reliable.\n- When targeting cross-lingual tasks, utilizing machine translation is one of the easiest way. However, such a basic approach is not considered as a baseline in the paper. Instead, the authors concatenate the two different languages in PIM, a baseline approach.\n- Considering the computational inefficiency of choosing the upper bound pairs of layers, reporting the computational cost of doing that is also required."
            },
            "questions": {
                "value": "- What is the reason for applying XTransplant only when generating the first new token?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The article focuses on the performance of language models in distinct languages and cultural contexts. Past findings show that popular LMs perform much better in English than in other languages, which is caused mainly by the unequal composition of their training data. \nThis work considers a trade-off between two options for applying LMs to distinct languages: 1) \nprompt models in English, to use its strong performance in the high-resource language or 2) prompt in the target language, potentially unlocking culture/language-specific information obtained by the model.\n\nTo leverage the promises of the two mentioned approaches, the authors propose a new method (UpperBound XTransplant) that patches the latent embedding to improve model performance in multilingual prompt-based tasks. The proposed algorithm finds the pair of layers between which the representation is transferred: from the prompted model to the target language prompted model. A greedy search is performed across N^2 possible pairs (where N is the number of layers) with the objective of finding the pair of layers for which transfer would maximize the probability of predicting a gold answer token.\nThis simple method offers a significant improvement in solving cross-lingual and cross-cultural tasks but also poses a risk of lurking into the gold answer when modifying the latent representation of the model."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The authors present an in-depth discussion of the proposed methods, analyzing the statistics across different tasks and providing examples of cross-cultural prompts to demonstrate how the method works on a low level. \n\n\n- The scope of experiments is broad and encompasses multiple tasks and models, including an instance of a language model trained on Chinese as a majority language.\n\n\n- The method achieves significant improvements over the baselines, which is impressive for such a not complicated approach. However, this may be due to the reasons described in the weaknesses."
            },
            "weaknesses": {
                "value": "- My main criticism is based on my strong suspicion that the improvements of the UpperBound method result mainly from lurking into the gold answer, i.e., the test set is used for tuning the methods. One indicator of that is the large improvement in English2English setting, which should not benefit at all from the enhanced cross-lingual transfer. This questionable result puts the whole point of improving multilingual capabilities in doubt. One solution to resolve that would be testing UpperBound with constant layer pairs for each language, e.g., after determining them based on a devset.\n\n- Greedy search comes with a high computational cost of performing O(N^2) additional predictions. The authors mention that this could be alleviated by always pathing from the last layer or to the target layer, this setting should be analyzed in more detail. Another option would be pre-setting layer pairs, as described in the previous point.\n\n- Much less severe criticism is connected to the lack of results for base (i.e. not instructed) models. My guess is that they could be less influenced by the language of the prompt."
            },
            "questions": {
                "value": "- Why do you only use En->non-En configuration in cross-lingual tasks and non-En->En in cross-cultural tasks? \n\n- Did you observe 0.0 accuracy in the English subset of XCOPA for Qwen-2? This score seems dubious in light of the model\u2019s technical report."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces \"XTransplant,\" a method designed to enhance the multilingual capabilities and cultural adaptability of LLMs by cross-lingual transplantation of feed-forward activations. The paper highlights the unlocking of current LLMs' multilingual potential and suggests promising directions for future research."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper introduces the X Transplant method, a novel way to enhance the multilingual capabilities of large language models by cross-lingual transplantation of feed-forward activations.\n\nExtensive experiments demonstrate improvements in both multilingual capabilities and cultural adaptability."
            },
            "weaknesses": {
                "value": "1.  Lack of comparative comparisons: The paper lacks comparisons with task-specific supervised fine-tuning and translation-then-inference approaches (translating prompts with a translation tool and then performing inference with the translated prompts). These methods represent alternative upper bounds for multilingual LLMs. Additionally, there are no comparisons between LLMs of different model sizes, which could provide insights into the impact of model capacity on performance.\n\n2. Why not patching hidden states: While the method involves transplanting feed-forward activations from non-English inputs to English, it does not explore the potential of directly patching the hidden states of English into the processing of non-English inputs. \n\n3. Implications for improving multilingual LLMs: How can the conclusions of this work be utilized to enhance the performance of multilingual LLMs or inform continued training strategies? Providing deeper insights into the practical applications of your findings would increase the importance of the research.\n\n4. Limited practical applicability: The approach requires parallel bilingual inputs (non-English and corresponding English sentences). In real-world applications, such parallel data may not be readily available, limiting the practical applicability of the method."
            },
            "questions": {
                "value": "1. When transplanting the feed-forward activations from non-English to English in the XQUAD task, is the model's output in English or the non-English language?\n\n2. In Table 1, the performance on the English XNLI test set increases significantly from 60 to 94. Could you elaborate on the experimental settings or conditions that contribute to this substantial improvement?\n\n3. The experimental setup regarding the selection of the i-th layer and j-th layer for MSi\u2192Tj (x) in Table 1 and Figure 5 is not clearly explained. Could you provide more details on how the layers are chosen and how the transplantation is performed?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposed a method called XTransplant to benchmark the upper bound performance in two scenarios: multilingual capability and cultural adaptability. Specifically, XTransplant replaces the feed-forward activations from one source language input to the target language input in the inference. For multilingual capability, the direction is En -> non-En, aiming to leverage good English generation capability. For cultural adaptability, the direction is non-En -> En, aiming to leverage the knowledge potentially only encoded in the non-En language-specific neurons."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well-motivated. I think the idea is interesting and novel.\n- The experiments are extensive.\n- The results reported in the paper show that XTransplant can consistently improve the performance."
            },
            "weaknesses": {
                "value": "- I understand that the main motivation is to benchmark the upper bound. But I see it is super expensive to investigate all the combinations (NxN). That might be a problem if one wants to see the performance on a specific downstream task with limited computation resources. \n\n- I also have concerns about whether xTransplant really offers the upper bound of a model, without any theoretical proof. One could argue it is also possible to transplant self-attention outputs."
            },
            "questions": {
                "value": "$\\textbf{Suggestions / Questions}$:\n\n\n- line 52-53: \"curse of multilinguality\" and \"negative interference\" are basically talking about the same thing (or more correctly, the curse of multilinguility if one type of negative interference when the languages are so many), the authors could consider condensing the two sentences.\n\n- Figure 1 is not clear how the method is carried on. the authors could cosider have one sentence in the caption to describe how the proposed method works.\n\n- line 74, \"given a certain question\", the authors should limit the span of such a question, i.e., a question that requires knowledge learned in non-English texts\n\n- line 145: what is the \"another version\" of x_s, I would assume it is the translation. the author should specify it.\n\n- line 161: what is the intuition of only changing the first new token? If the first new token does not change before and after the transplanting, the remaining tokens will be the same (greedy decoding).\n\n- section 3.2: I am not sure I understand why it is called bi-directional transplant. It is either En -> non-EN or non-EN -> EN for a specific prompt. I would be more inclined to call it Mutual or Parallel transplant.\n\n- section 5.1: how do the authors define language consistency? I guess it is the frequency of the input and output being in the same language. The authors should make it clear. Additionally, my intuition is that Xtransplant should be bad for language consistency because it changes the original activation to the activations obtained from another language. However, the authors' results suggest this is not the case. Can the authors give explanations? Additionally, it would be interesting to see some actual examples.\n\n- line 203-204: \"Questions in above datasets are in different multilingual languages\". I don't understand this sentence but I guess the author means \"Each question in the above datasets is available in multiple languages\"?\n\n\n$\\textbf{typo}$:\n\nLine205: \"we we performed\" -> \"we performed\""
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}