{
    "id": "jIAKjjEmWi",
    "title": "Attention Is All You Need For Mixture-of-Depths Routing",
    "abstract": "Advancements in deep learning are driven by training models with increasingly larger numbers of parameters, which in turn heightens the computational demands. To address this issue, Mixture-of-Depths (MoD) models have been proposed to dynamically assign computations only to the most relevant parts of the inputs, thereby enabling the deployment of large-parameter models with high efficiency during inference and training. These MoD models utilize a routing mechanism to determine which tokens should be processed by a layer, or skipped. However, conventional MoD models employ additional network layers specifically for the routing which are difficult to train, and add complexity and deployment overhead to the model. In this paper, we introduce a novel attention-based routing mechanism *A-MoD* that leverages the existing attention map of the preceding layer for routing decisions within the current layer. Compared to standard routing, *A-MoD* allows for more efficient training as it introduces no additional trainable parameters and can be easily adapted from pretrained transformer models. Furthermore, it can increase the performance of the MoD model. For instance, we observe up to $2$\\% higher accuracy on ImageNet and up to $2\\times$ faster transfer learning, for the first time showing the benefits of MoD on various computer vision tasks.",
    "keywords": [
        "Mixture of Depths",
        "attention",
        "parameter free routing."
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "We propose an integrated routing mechanism for Mixture of Depths using attention.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=jIAKjjEmWi",
    "pdf_link": "https://openreview.net/pdf?id=jIAKjjEmWi",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a parameter-free routing method for mixture-of-depths (MoD) models. \nIt uses attention maps to discover the importance of each token. \nThe authors also tested the results on a series of DeiT/ViT models."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper explores the token importance evaluation through the existing attention maps and thus reduce the overhead of extra layers."
            },
            "weaknesses": {
                "value": "1. There is no comparison with the current SOTA models since DeiT/ViT are relatively old. \n2. The idea is quite similar to the following papers. It would be great to include the comparison (such as accuracy and latency) with these papers. \nDynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification and \nToken Merging: Your ViT But Faster"
            },
            "questions": {
                "value": "1. what's benefits of the proposed approach compared to traditional ConvNet? It would be great to include some comparison with ResNext etc\n2. What's the advantage of the proposed approach against the transformer / convnet hybrid architecture such as LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference or FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a parameter-free routing method to improve the routing mechanism in the mixture-of-depths (MoD) method. The basic idea is to directly use the attention scores in Transformer."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Using the transformer attention in the MoD routing makes sense.\n- Extensive results do prove that the proposed A-MoD outperforms MoD."
            },
            "weaknesses": {
                "value": "- The entire study is too narrow and limited. This paper is specifically targeted at MoD. However, MoD is just an arxiv paper. Does MoD represent the SoTA in terms of the Pareto frontier? All the experiments are mainly compared with MoD? What about other SoTA methods? BTW, MoD is not impressive in Table 1, where it is even inferior to a simple baseline, isoFLOP.\n\n- Why is higher average attention in (4) corresponding to higher importance? What is the semantic meaning? If so, shouldn't the background tokens in Fig. 4 not be skipped since they have high attentions with many other background tokens? BTW, what are the different columns in Fig. 4 and Fig. 5?"
            },
            "questions": {
                "value": "Please address the issues pointed out in the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this work, the authors aims to deal with an interesting problem: How to reduce the token number need to be computed in a layer, and proposed A-MOD. A-MOD is simply based on attention score to select tokens need to be calculated, and the rest will be directly skiped. Experimental results show promising results.\n\nSuch a topic is always interesting and important, especially in the era of large models. However, almost none is applied to indutry because these methods are always only efficient in theory. I encourage more explorations in this topic. Regarding this work, I believe many experimental settings are not reasonable or not as expected."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors tackle an important issue in improving the efficiency of ViTs by proposing a method that dynamically selects the most relevant tokens for computation. This approach aligns with similar techniques in the field, such as A-ViT, which also prioritize token selection for efficiency gains.\n\n2. Additionally, the experimental results show promising performance."
            },
            "weaknesses": {
                "value": "The main idea behind the proposed method makes sense to me, but I have several concerns, particularly regarding the experiments and their settings.\n\n1) First, the authors mention in Line 234 that they continue training from a previous checkpoint for an additional 100 epochs. I\u2019m curious whether this approach is justified. Why not simply start training from scratch? \n\n2) I also find the transfer learning setup a bit confusing, especially since the authors do not use fixed pretrained weights. Why is transfer learning considered in a work that focuses on efficiency? How does it contribute to the overall goals of the work? Clarity on this point would be help.\n\n3) Regarding the baseline, I expected to see a vanilla ViT for comparison, but instead,  the authors reduced the original ViT layers to match FLOPs directly. This choice makes no sense to me if the goal is to demonstrate that A-MOD improves effiency. Additionally, I noticed that A-MOD significantly lowers the original performance of ViT. For instance, the original ViT-Tiny achieves ~74% accuracy, whereas A-MOD only reaches 69.76% and 71.8%. \nThe abstract is is also very misleading; I initially was thinking the method reduces the number of tokens while improving accuracy on ImageNet by 2% (a huge improvement on ImageNet).\n\n4) Is the same selection ratio applied across all layers, from shallow to deep? we know that shallow layers typically do not generate meaningful attention maps.\n\n5) At a high level, A-MOD aims to reduce token number for computation, which is similar to the approach taken in the TO-ME paper (\u201cToken Merging: Your ViT but Faster,\u201d ICLR 2023). However, I noticed that the performance of A-MOD is much worse, and there are no speed tests provided to show efficiency gains.\n\n6) In Fig. 4,  the last image of the first row, it appears that MOD selects almost all unimportant boundary patches and parts of the bird's head. Does this make sense? If it does, I would like to know why.\n\n7) Generally, attention maps in shallow layers do not reliably indicate importance. Given this, how should we interpret the results shown in Fig. 5? Were the specific attention heads or examples chosen selectively, or could you provide additional much more examples (besides the bird and car used throughtout the paper) to support the findings?\n\n8) Finally, in Line 420, the authors state, \u201cattention maps do not always learn semantically meaningful scores.\u201d This leads me to two questions: How did you conclude that this issue is only limited to larger models? If this only happens in larger models, why does A-MOD work for ViT-Tiny?"
            },
            "questions": {
                "value": "1. Are there any other reproducible results for MOD on ImageNet?\n\n2. How are different heads within a single layer handled? Will different heads select different tokens for computation in a layer?\n\n3. As indicated in the abstract, it\u2019s unclear how this method improves training and inference efficiency on an ImageNet-scale dataset. Are these efficiency improvements achieved in practice (like in training and inference speed) or just in theory?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Mixture of Depth methods have recently been proposed as an alternative to Mixture of Expert methods. Instead of deciding to which expert to route a token, MoD takes a decision whether or not to skip the current expert. The main purpose shared by these methods is that of finding a better compute/performance tradeoff, which is currently of particular interest in the context of LLMs and large generative models. One of the main challenges for building such methods is the design of stable routing mechanisms. The authors propose A-MoD, a routing method which performs pruning of tokens traversing the current layer, based on the respective attention scores in the previous layer. The authors evaluate their method on classification tasks on a set of DeiT and DiT models of varying size."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The method is simple and clearly explained throughout the paper. It seems possible to implement it easily for small models operating on short token sequences.\n- The proposed method is simple and the idea of eliminating learnable routing, a major source of training instabilities, is an interesting research direction.\n- Some performance improvements are shown under some settings of a classification task."
            },
            "weaknesses": {
                "value": "- The method requires access to the attention matrices. Modern attention implementations (e.g. \"FlashAttention\") do not materialize attention matrices due to their large materialization cost. An implementation of A-MoD viable for an LLM or large generative model would require non-trivial implementation effort to match the performance of modern attention implementations.\n- The chosen evaluation setting is composed only of classification tasks. Token pruning is particularly suited for this task, as all tokens with information not relevant to the depicted object can be safely discarded and the model output consists in only an aggregation of the tokens. A more significant evaluation would have taken segmentation, language modeling, or image/video generation problems into account. In these tasks token pruning can be problematic, as every token will be returned as an output. Pruned tokens are at risk of generating artifacts in such context. Lack of evaluation in these context is a significant limitation.\n- The method presents a limitation in the maximum achievable speedup with respect to a corresponding dense architecture. In particular, as each A-MoD block bases its pruning on the previous block, it can only be applied at alternating blocks, posing a maximum achievable speedup to 2X over a corresponding non A-MoD model. In contrast, a learnable routing mechanism would allow application of MoD to each layer, allowing for higher theoretical speedups, making the pursuit of this paradigm more promising than A-MoD.\n- Some recent MoE architectures (Snowflake Arctic LLM) show that it is beneficial to apply MoE at every layer. A-MoD can apply it only every other layer\n- Table 1 proposed evaluation with an isoFLOP baseline. Under 3/8 configurations, the isoFLOP baseline has <90% of the flops of the A-MoD model. Such comparison does not appear fair\n- Table 1 would benefit from being accompanied by a Pareto curve visualization of performance vs compute, which could highlight better the Pareto front for the C=12.5% case, where compute of the baseline and A-MoD method is mismatched.\n- The paper does not take inference throughput into consideration. In this paper, FLOPs are an incomplete indicator of model cost in that they do not highlight performance losses caused by the need to materialize the attention matrix. It is likely that the isoFLOP baseline when implemented carefully with an efficient attention implementation would outperform A-MoD in terms of inference throughput and memory utilization, especially if evaluated in practically-relevant language modeling or generative tasks with large amounts of tokens.\n- Discussion in 4.5 and Fig. 7 are not convincing. In Fig 7 (a) performance is aligned to the isoFLOP baseline, in Fig 7 (b) compute is too mismatched between baselines to draw conclusions.\n\nIn summary, the pursuit of A-MoD seems less promising than traditional MoD or MoE paradigms due to its limited theoretical maximum achievable speedup of 2X. Evaluation is performed on classification only, missing some important performance metrics (throughput/latency) and in some circumstances under unfair evaluation settings for the baseline. Evaluation does not comprise important dense prediction tasks such as language modeling or image/video generation. No discussion or remedy is presented for the need of materializing full attention matrices, limiting applicability of the method for large models operating on long sequences, which could benefit most from MoD/MoE.\n\nFor these reasons, I do not believe the method implementation and evaluation are solid enough for the paper to be accepted."
            },
            "questions": {
                "value": "- Can the authors provide an implementation of an efficient attention mechanism such as \"Flash Attention\" with support for A-MoD? If yes, how would the performance of this implementation compare to the original implementation?\n- Can the authors show performance of the method on a task such as semantic segmentation, language modeling or image/video generation that involves producing an output token for each input token to confirm that token dropping would not affect output quality?\n- Could the authors revise Table 1 to perform a fairer comparison with isoFLOP baselines and show results using a Pareto curve?\n- Could the authors revise qualitative evaluations to include throughput measures rather than FLOPS only, ensuring baseline methods make use of efficient attention implementations?\n- Could the authors revise quantitative evaluation to include a task involving large numbers of tokens (>=4k tokens) to show that the attention operation implementation can remain efficient as the input size grows?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}