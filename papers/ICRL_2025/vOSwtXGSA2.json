{
    "id": "vOSwtXGSA2",
    "title": "An Adaptive Defense Against Adversarial Patch Attacks For Vision Transformers",
    "abstract": "Vision Transformers (ViTs) have become the prominent architecture for various computer vision tasks due to their superior ability to capture long-range dependencies through the self-attention mechanism. However, recent research indicates that ViTs are highly susceptible to carefully crafted adversarial patch attacks, presenting a significant challenge for practical deployment, particularly in security-critical applications. Existing approaches towards robust ViT frameworks often sacrifice clean accuracy and/or achieve suboptimal robustness, likely due to their uniform handling of diverse input samples. In this paper, we present NeighborViT, a novel adaptive defense framework specifically designed to counter adversarial patch attacks for ViTs. NeighborViT stands out by detecting and categorizing different types of attacks on inputs and applying adaptive, tailored defense mechanisms for each type of attack. To realize effective attack detection, categorization, and mitigation, NeighborViT explores the information in neighbor patches of the target patch and strategically employs them for defense. Our experimental results on the ImageNet dataset using various state-of-the-art ViT models demonstrate that NeighborViT significantly enhances robust accuracy without compromising clean accuracy. Our code is available at https://anonymous.4open.science/r/NeighborViT-8255.",
    "keywords": [
        "vision transformer; adversarial patch attack;  adptive defense"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=vOSwtXGSA2",
    "pdf_link": "https://openreview.net/pdf?id=vOSwtXGSA2",
    "comments": [
        {
            "summary": {
                "value": "This paper presents NeighborViT, a novel adaptive defense framework designed to counter adversarial patch attacks for ViTs. NeighborViT stands out by detecting and categorizing different types of attacks on inputs and applying adaptive, tailored defense mechanisms for each type of attack. Experimental results demonstrate that NeighborViT significantly enhances robust accuracy without compromising clean accuracy."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The framework of NeighborViT is detailed presented.\n2. The idea of distinguishing different types of adversarial patch attacks and adopting corresponding defense methods is pretty interesting."
            },
            "weaknesses": {
                "value": "1. The attacks in experiments include one patch, such as Figure 5, while adopting corresponding defense methods for various attacks. It makes me quite confused. Therefore, I don't think the results can demonstrate the idea of adopting corresponding defense methods. For instance, experiments on adversarial examples with more adversarial patches should be performed to validate the performance of the proposed and baseline methods.\n2. Since the attack detector and area detector are extra modules, the additional time cost (e.g., seconds per epoch), computational cost (e.g., FLOPs), and GPU memory usage of the framework should be carefully illustrated. In practice, training costs play an important part in the application. Improving adversarial robustness may not pay the overhead training cost without extra experiments. For example, authors can perform a detailed ablation study of each module and other baselines in their default settings.\n3. The framework is an input process defense method, which aims at filtering out the adversarial perturbations for better adversarial robustness. In the manuscript, authors compare their method with some representative robust ViT frameworks. However, some classical input process defense methods are not considered in this paper, leading to thin arguments. For example, classical input process defense methods like smoothing, quantization, JPEG compression, and the recent work \"Diffusion Models for Adversarial Purification\" should be compared in their setting to figure out the effectiveness of their method.\n4. The writing of the paper should be improved, which is unclear to me. For example, the illustration of \"catastrophic\" and \"non-catastrophic\" attacks is confusing. It is first proposed in the \"Background & Related Works\" without any explanation or reference. Only a simple introduction in L151-153 says \"The catastrophic attacks represent the attacks occurred in the essential areas and non-catastrophic attacks represent the attacks located in the non-essential parts\" with almost no information but two new words \"essential\" and \"non-essential\" parts. In subsection 3.2, the authors demonstrate that the \"essential\" parts contain essential features for model classification while the \"non-essential\" parts do not. However, there seems to be no detailed explanation of essential features, except from a plain description in L291-292. On the contrary, excessive words focus on how to get essential features by their area detector, leading to a reversed order. Papers can reveal their novelty through clever words but not rely on unintelligible sentences to cover their confused ideas."
            },
            "questions": {
                "value": "Please see the \"Weakness\" part for my questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a novel adaptive defense framework -- NeighborViT which can detects and categorizes different types of attacks; applies tailored defense mechanisms for each attack type and leverages information from neighboring patches for effective detection and defense."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The method analysis is clear and understandable.\n2. The visualization is helpful.\n3. The proposed method is simple but works well and maintain high clean accuracy with minimal reduction."
            },
            "weaknesses": {
                "value": "1. Lack of ablation study about different operators. \n2. Lack of experiments about different attack strength.\n3. Although the result shows the proposed method does not bring a lot extra computation cost, the detection and defense mechanisms can be further improved.\n4. Bar graphs can be further improved (color, layout)."
            },
            "questions": {
                "value": "1. It seems that using sobel operator is more like a emperical design, have you ever tried differen operators?\n2. I am curious about the performance of the proposed method when attack patch with $L_p$ constraint.\n3. Is that possible to make the dynamic scan of the patches and use more similar patch to replace but not just neighbour patches?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces NeighborViT, an adaptive defense framework for Vision Transformers (ViTs) against adversarial patch attacks. Existing defense methods sacrifice clean accuracy or achieve suboptimal robustness. NeighborViT detects and categorizes different types of attacks, applying tailored defense mechanisms. The framework leverages information from neighboring patches to enhance robust accuracy without compromising clean accuracy. Experimental results demonstrate its effectiveness on various ViT models using the ImageNet dataset."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper introduces an adaptive defense strategy that distinguishes between different types of adversarial attacks, providing tailored solutions, which successfully enhances robust accuracy while preserving clean accuracy, addressing a common trade-off in existing methods.\nThe authors provide extensive experimental results across multiple ViT models and attack approaches, demonstrating the framework's robustness.\nThe authors proposes a lightweight adversarial patch detection method that doesn\u2019t require auxiliary models or multiple queries, reducing computational costs."
            },
            "weaknesses": {
                "value": "The experiments focus on specific attack types. Including a broader range of attacks could enhance the robustness of the evaluation.\nSome technical aspects, such as the definition of similarity in Section 3.2, parameter cl in table 10, could be explained in greater detail. \nThe proposed strategy may require fine-tuning for different datasets or attack scenarios, potentially affecting generalizability.\nThe paper lacks discussions of the limitations of the proposed method."
            },
            "questions": {
                "value": "Since the proposed ENED relies on the detection results of AD, if naturalistic adversarial patches and other non-noise forms of adversarial patches are introduced for attacks on ViT later on, would the entire method become ineffective? In other words, does the low pixel continuity assumption hold true for these patches (e.g., TnT Attacks! Universal Naturalistic Adversarial Patches Against Deep Neural Network Systems) and (e.g., Generating transferable adversarial examples against vision transformers)?\n\nIn Table 1, the performance improvement of this method seems to be only around 2% compared to the secondary defense methods under most settings. Furthermore, Jedi can support multitasking (such as object detection) and various attacks (like naturalistic adversarial patches), suggesting that this method may have more scenario limitations.\n\nSection 4.4 devotes a significant amount of space to discussing the values of hyperparameters. I am concerned that the impact of these hyperparameters may be too substantial. Further, do the hyperparameters need to be adjusted for different ViT architectures and datasets, and if so, how should this be approached?\n\nThe experiment does not specify the ratio of catastrophic attacks to non-catastrophic attacks, and it does not clearly differentiate which category each attack method belongs to.\n\nFor other suggestions or questions, please refer to weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates defense strategies against adversarial patch attacks aimed at Vision Transformers. The proposed method, NeighborViT, encompasses attack detection, classification of various attack types, and corresponding mitigation strategies. Specifically, the authors employ the sober operator with dynamic windows to pinpoint the locations of adversarial patches and use average predictions from the reconstruction of these patches to classify the type of attack. When adversarial patches are located in non-essential areas, they select a neighboring patch to fill the masked adversarial patches. In contrast, for essential areas, they reweight the attention weights for adversarial tokens. They conduct experiments on several ViTs to demonstrate the superior performance of their method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors observe that different types of attacks require tailored defense approaches, classifying them as non-catastrophic and catastrophic based on the location of adversarial patches. This observation, validated by Table 1, demonstrates a logical approach that improves defense performance.\n2. The authors conduct extensive experiments, which show better performance compared to existing defense."
            },
            "weaknesses": {
                "value": "1. The authors delve into adversarial defense within the filed of ViTs. the proposed defense methods present challenges when applied to defending CNNs. This limitation restricts the technological contribution and general application of the paper.\n\n2. The proposed method includes three hyperparameters. While the authors present a wide range of values for these parameters, allowing the method to surpass other defenses. However,  there remains some limitation, as shown in Table 6. The optimal value of $gamma$ varies with different attack patch sizes.\n\nThese two weaknesses pose challenges to the practical application of the proposed adaptive method."
            },
            "questions": {
                "value": "The authors should consider adaptively tuning hyperparameters to enhance the method\u2019s generalization.\n\nAttack types are classified based on the location of adversarial patches, but could a more fine-grained classification further enhance detection and defense effectiveness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}