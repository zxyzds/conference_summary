{
    "id": "cCRlEvjrx4",
    "title": "Learning Conditionally Independent Marginals Enables Logical Compositions in Conditional Diffusion Models",
    "abstract": "How can we learn generative models to sample data with arbitrary logical compositions of statistically independent attributes? The prevailing solution is to sample from distributions expressed as a composition of attributes' conditional marginal distributions under the assumption that they are statistically independent. This paper shows that standard conditional diffusion models violate this assumption, even when all attribute compositions are observed during training. And, this violation is significantly more severe when only a subset of the compositions is observed. We propose CoInD to address this problem. It explicitly enforces statistical independence between the conditional marginal distributions by minimizing Fisher\u2019s divergence between the joint and marginal distributions. The theoretical advantages of CoInD are reflected in both qualitative and quantitative experiments, demonstrating a significantly more faithful and controlled generation of samples for arbitrary logical compositions of attributes. The benefit is more pronounced for scenarios that current solutions relying on the assumption of conditionally independent marginals struggle with, namely, logical compositions involving the NOT operation and when only a subset of compositions are observed during training.",
    "keywords": [
        "logical compositionality",
        "generative models",
        "diffusion models"
    ],
    "primary_area": "generative models",
    "TLDR": "",
    "creation_date": "2024-09-14",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=cCRlEvjrx4",
    "pdf_link": "https://openreview.net/pdf?id=cCRlEvjrx4",
    "comments": [
        {
            "summary": {
                "value": "This paper considers the problem of conditional generation for logical compositions of attributes. The authors claim that existing conditional diffusion models violate the conditional marginal independence assumption of the attributes. This makes them struggle in generating samples given arbitrary logical compositions of attributes, even if trained with uniform data over all possible combinations of attributes, and even worse if there is a statistical correlation or only some attribute combinations are seen. The paper then tackles this problem by training score-based models (with their diffusion model interpretation) with an additional loss term to enforce this conditional independence assumption, motivated by Fisher\u2019s divergence between the joint and product of marginals of attributes. The assumption about marginal independence empirically validated by observing that Jensen-Shannon divergence of joint distribution and the product of conditional marginal distributions has a negative correlation with an accuracy metric called conformity score (CS)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper addresses the problem of generating samples for logical compositions of attributes, including combinations previously not seen in data, which is highly relevant.\n\nThe proposed approach trains a single model to handle arbitrary compositions of attributes, and thus scales with the number of attributes. The proposed loss function is derived from a theoretically sound objective based on the Wasserstein distance between the true conditional likelihood and the learned likelihood satisfying the conditional independence assumption.\n\nExperiments are thorough, including different support settings (uniform, non-uniform, partial/orthogonal) for two benchmark datasets, and the results support the claims about conditional independence assumption. \n\nThe paper is overall well structured, although some details could be clarified (see below)."
            },
            "weaknesses": {
                "value": "I have a couple of concerns about soundness and clarity. While the proposed loss function (Eq 6 and 7) has a clear theoretical justification of bounding Wasserstein distance, this is somewhat lost in the actual training objective of CoInD (Eq 8 and 9) due to simplifications for practical implementation. It was also not very clear how the modified L_{CI} in Eq 9 is derived.\n\nIn several places (including the problem statement), the authors seem to conflate the distribution of attributes being uniform vs. conditionally independent. The conditional distribution of attributes can be factorized without necessarily being uniform. Another issue is that even if each attribute had uniform probability, after conditioning on a logical formula in general, each marginal probability may no longer be uniform. \n\nMoreover, the paper mentions \u201cincorrect marginals\u201d multiple times as being a limitation of current approaches, but I believe the main claim is about conditional independence (factorizability). It would be clearer to refer to independence directly because it\u2019s possible to have the correct marginal probability of each attribute while still violating the conditional independence assumption.\n\nAs the authors also discuss, the proposed method is restricted to a composition of a closet set of attributes, and cannot easily add compositional attributes without retraining from scratch. However, while a limitation, open-set compositions are outside of the scope of this paper and not a major weakness."
            },
            "questions": {
                "value": "I found the definition of conformity score to be not very clear. First, I assume results are reported in percentage rather than as defined (between 0 and 1). My main confusion is about the distribution p(X,C) in the definition (line 199). For the generated samples, is the score measuring whether all consistent combinations of attributes appear following a certain distribution, or is it only measuring whether the attributes (given by the classifier) satisfy the logical composition?\n\nCan this method be used for compositions using general logical formulas beyond single AND and NOT operations?\n\nCan CoInD generate attributes having non-uniform probability (while still being conditionally independent)? For instance, to generate shapes in a scene following a certain distribution, or images for demographic groups following a certain population distribution.\n\nThe causal graph in Fig 2a implies independence among the attributes, but not necessarily conditional independence given X. It wasn\u2019t clear whether (around line 121) the claim is that conditional independence is implied by this causal graph or it is an additional assumption made by the authors."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper shows that standard conditional diffusion models violate this assumption, even when all attribute compositions are observed during training. The authors propose COIND to address this problem. It explicitly enforces statistical independence between the conditional marginal distributions. Quantitative experiments demonstrate a significantly more faithful and controlled generation of samples for arbitrary logical compositions of attributes. The benefit is more pronounced for scenarios that current solutions relying on the assumption of conditionally independent marginals struggle with, namely, logical compositions involving the NOT operation and when only a subset of compositions are observed during training."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The problem is interesting and the authors proposed a solution to the problem. The writing of the paper is clear and easy to follow. The method is simple and effective. The authors provide experiments to validate the proposed method."
            },
            "weaknesses": {
                "value": "1. The datasets in the paper are simple MNIST and synthetic 3D shapes. Experiments on real-world images could strengthen the paper. Experiments on natural images could provide more convincing evidence for the proposed method.\n\n2. What are the practical applications of NOT operation in the composition generation? What are the motivations for applying logic operations on the attributes with multiple values, e.g. c_i  in  [0, 1, 2, 3, 4, 5]?\n\n3. How does the model process logic expressions? How is the model implemented to process logic expressions? If not implemented, what are the motivations for applying logic operations? Are AND operations sufficient for composition generation with different attributes?"
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper performs compositional image generation with and, or, and not operators. The proposed method trains the diffusion models with a score function that enforces the attributes in the generated images to be independent. Finally, the authors show performance on two synthetic datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is very well written. Every detail is provided in a clear manner. The description is intuitive. The authors also provided insightful observations."
            },
            "weaknesses": {
                "value": "### Minor weakness:\n* Some motivational examples of the utility of the proposed method in the introduction would be nice.\n* The conformity score with notation should be defined/discussed briefly in the experiment section again.\n* An initial look at Figure 3 might give readers the impression that the proposed approach is effective for such scenarios as well. This should be made clearer.\n* The numbers in line 417 can be written in a more detailed way.\n* The authors should give some examples of open and closed set attribute compositions.\n\n### Major weakness:\n* The only major issue with this paper is that there is no experiment on a real-world image dataset. One of the baselines, LACE, presents results for human faces with independent attributes, e.g., smile, eyeglasses, etc. Can the authors show performance on such datasets?"
            },
            "questions": {
                "value": "### Questions:\n\n* What happens if $\\lambda$ is set to a very high value? Would that give perfect conditional independence? Will that be an issue while minimizing $L_{\\text{score}}$?\n* What issues will the proposed algorithm face if the attributes are actually dependent in the true data-generating process?\n* How many attributes can be composed together? How does the proposed approach behave when you increase the number of attributes?\n* Based on Figure 3: if the training dataset has no [color: green, digit: 9], can the model generate such images? The proposed approach has a 55% conformity score (CS) for partial support. Does that mean that, among the generated images with the attributes [color: green, digit: 9], 55% contain these attributes correctly?\n* Can the authors provide images with minimal attribute combinations where the proposed method, along with all baselines, fails?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the limitations of standard diffusion models in sampling data with arbitrary logical compositions of statistically independent attributes. It reformulates the problem as minimizing the Fisher divergence between the joint distribution and the product of marginals, demonstrating the effectiveness of this approach."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "This paper provides a clear and well-articulated statement of the problem, supported by intuitive examples that help the reader understand the limitations of current conditional diffusion models. It effectively highlights the insufficiencies in these models, particularly in capturing accurate marginal distributions, and offers a novel solution to address these gaps. The authors ensure that the proposed model is designed to learn precise marginal distributions, an essential improvement over existing methods. The paper also includes rigorous experimental validation, utilizing a range of extensive datasets, which adds credibility to the findings and demonstrates the model\u2019s effectiveness across diverse scenarios."
            },
            "weaknesses": {
                "value": "(1) Some claims in the paper need additional rigor. For instance, the assertion that the drop in conformity scores is due to the violation of independence relations in the learned model lacks clear justification. Providing stronger reasoning or citing relevant references would help substantiate this point.\n\n(2) The proof process could be clarified for readers. For example, the derivation of Equation 6 from previous equations is not immediately clear, and similar issues appear in other parts of the theoretical development. Additionally, some expected theoretical support is missing from the appendix, which could help make the proofs more accessible.\n\n(3) In the experiments section, the model is only evaluated on two real-world datasets, with no simulation results to provide more quantitative insights into the model\u2019s effectiveness. Including simulation studies would strengthen the empirical validation by offering additional quantitative measures."
            },
            "questions": {
                "value": "(1) How is the failure of standard diffusion models linked to the violation of independence assumptions? Could you provide evidence or proof supporting this claim?\n\n(2) How does the model perform on more diverse datasets? Could additional results on simulated data be included to further validate its effectiveness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce COIND, a novel approach that enforces conditional independence between marginal distributions by minimizing Fisher's divergence between joint and marginal distributions. Traditional models struggle to handle arbitrary compositions, especially when only partial data compositions are available during training. In comparison, COIND leverages causal independence to improve the generation of images that meet desired attribute compositions. The experiments show COIND\u2019s performance in generating images with diverse and accurate compositions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper introduces conditional independence analysis into the training of conditional diffusion models, integrating insights from both causal structure learning and generative modeling. By uncovering latent causal structures, it enhances the performance of traditional conditional diffusion models, which is quite innovative.\n\n2. The derivation of error decomposition in Section 4 is particularly impressive. By decomposing the Wasserstein distance, the paper introduces a novel loss function, providing a strong theoretical justification for incorporating causal structures like conditional independence, which is very intriguing.\n\n3. On the selected datasets, the proposed method consistently outperforms comparative approaches across various settings, demonstrating a significant improvement. Moreover, the presentation of experimental results is comprehensive and meticulous. Notably, Appendix A.7 provides an exhaustive comparison of experimental settings across methods, which is commendable."
            },
            "weaknesses": {
                "value": "1. While the paper uses illustrative images to present the experimental settings for Non-uniform and Partial support scenarios, the lack of mathematical descriptions limits the rigor of this section.\n\n2. The paper\u2019s exploration of causal structures introduced during training remains relatively simple, focusing primarily on conditional independence, and lacks investigation into more complex causal graphs and mechanisms.\n\n3. The study only examines the model's performance on the Colored MNIST and Shapes3D datasets, which, compared to papers like Liu et al. (2023), leaves it without results on more complex, real-world datasets to substantiate the research conclusions."
            },
            "questions": {
                "value": "1. As pointed out by (Karras et al. (2022), Fu et al. (2024)), generative diffusion models often introduce an early stopping time rather than reverting completely to time zero in the generation process to avoid numerical instability. Did you consider the impact of setting an early stopping time in your experiments? If so, could you elaborate on it? If not, could you explain why this setting might not significantly affect the results?\n\n2. The selection of the hyperparameter \u03bb in the loss function is crucial. Could you share the principles you followed when choosing it (e.g., Bayesian optimization, cross-validation)? The analysis in Section 5.2 does not seem to fully address this aspect.\n\n3. In the experimental scenarios considered in this paper, the features are all  discretized. Do you believe it is possible to extend this method to continuous features, or potentially adapt it with certain techniques to handle continuous feature scenarios?\n\n4. The paper uses Jensen-Shannon Divergence (JSD) to measure conditional independence, though there are many metrics for conditional independence. For instance, Conditional Mutual Information (CMI) (Mukherjee et al., 2020; Li et al., 2023) has shown strong performance for continuous feature scenarios mentioned in Q3. Could you explain the advantages of using the JSD metric in this context?\n\nReference:\n1.\tLiu, N., Li, S., Du, Y., Torralba, A., & Tenenbaum, J. B. (2022, October). Compositional visual generation with composable diffusion models. In European Conference on Computer Vision (pp. 423-439). Cham: Springer Nature Switzerland.\n\n2.\tKarras, T., Aittala, M., Aila, T., & Laine, S. (2022). Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35, 26565-26577.\n\n3.\tFu, H., Yang, Z., Wang, M., & Chen, M. (2024). Unveil conditional diffusion models with classifier-free guidance: A sharp statistical theory. arxiv preprint arxiv:2403.11968.\n\n4.\tMukherjee, S., Asnani, H., & Kannan, S. (2020, August). CCMI: Classifier based conditional mutual information estimation. In Uncertainty in artificial intelligence (pp. 1083-1093). PMLR.\n\n5. Li, S., Zhang, Y., Zhu, H., Wang, C., Shu, H., Chen, Z., ... & Yang, Y. (2023). K-nearest-neighbor local sampling based conditional independence testing. Advances in Neural Information Processing Systems, 36."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}