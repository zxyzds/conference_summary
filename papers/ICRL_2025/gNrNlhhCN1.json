{
    "id": "gNrNlhhCN1",
    "title": "FAACL: Federated Adaptive Asymmetric Clustered Learning",
    "abstract": "Asymmetric clustering has remained an unexplored problem in Clustered Federated Learning (CFL), diverging from the traditional approach of forming independent, non-interacting clusters. Previous methodologies have been limited to either separating devices with different data quality into distinct clusters or merging all devices into a single cluster, both of which compromise either data utilization or model accuracy. We propose a new federated learning technique where some devices may contribute to the training of the models of other devices, but without enforcing reciprocity, leading to a form of asymmetric clustering.  This is beneficial in a variety of situations including scenarios where it is desirable for a device with high quality data to help train the model of a device with low quality data, but not vice-versa. This method not only enhances data utilization across the devices, but also maintains the integrity of high-quality data. Through a rigorous theoretical analysis and empirical evaluations, we demonstrate that our approach can efficiently find high quality (asymmetric) clusterings for numerous devices, achieving competitive performance metrics on existing CFL benchmarks.",
    "keywords": [
        "Federated Learning",
        "Distributed Learning",
        "Clustering"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=gNrNlhhCN1",
    "pdf_link": "https://openreview.net/pdf?id=gNrNlhhCN1",
    "comments": [
        {
            "summary": {
                "value": "Authors discuss the clustered learning approach in Federated Learning and relax the following constraints:\n1. Number of clusters being fixed (Adaptive)\n2. Each device contributes to a single cluster (Adaptive) \n3. Model sharing is reciprocal in nature (Asymmetric)"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "S1: Introduced asymmetric clustering \nS2: Source code is shared for reproducibility."
            },
            "weaknesses": {
                "value": "W1: This comment is in relation to the Section 4.2 and 4.3 -> Some of the related works may need to be studied and probably be included as part of the comparison metrics. [1-2]\n\nW2: How does this handle a newly added device in the Federated Learning? Do we check against all the clusters and find if it supports or not individually or against the clusters only? Does that a performance hit if the clusters are already prepared.  \n\n[1] Vardhan, Harsh, Avishek Ghosh, and Arya Mazumdar. \"An Improved Federated Clustering Algorithm with Model-based Clustering.\" Transactions on Machine Learning Research (2024).\n\n[2] Kim, Heasung, Hyeji Kim, and Gustavo De Veciana. \"Clustered Federated Learning via Gradient-based Partitioning.\" Forty-first International Conference on Machine Learning."
            },
            "questions": {
                "value": "Q1 :  line 053 - \u201cand all existing techniques assume that each device contributes to a single cluster.\u201d - What does this line mean here? And how do address the same in your paper? \n\nQ2: Does it have the same privacy guarantees as other models proposed in the same vein such as [1] and others. \n\nQ3: How does the computational complexity compare to other algorithms? \n\nQ4: How far off is this clustering from ground truth? Additionally is there a comparison again Oracle? [2] \n\nQ5: Was there an ablation study performed without merging of clusters and just having asymmetric contribution? \n\n[1] Sattler, Felix, Klaus-Robert M\u00fcller, and Wojciech Samek. \"Clustered federated learning: Model-agnostic distributed multitask optimization under privacy constraints.\" IEEE transactions on neural networks and learning systems 32.8 (2020): 3710-3722.\n\n[2] Jothimurugesan, Ellango, et al. \"Federated learning under distributed concept drift.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a new federated learning technique where some devices may contribute to the training of the models of other devices, but without enforcing reciprocity, leading to a form of asymmetric clustering. This approach not only enhances data utilization across the devices, but also maintains the integrity of high-quality data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The proposed approach tackles the underexplored issue of asymmetric clustering in Clustered Federated Learning, providing a fresh perspective.\n\n2. The experimental results are comprehensive."
            },
            "weaknesses": {
                "value": "1. The paper presents a clustered federated learning method; however, it lacks an introduction to the implementation of federated learning (FL) and clustered federated learning (CFL). I recommend including details about the specific training processes and objectives of both FL and CFL in the BACKGROUND section to enhance clarity and context.\n\n2. How can the proposed approach be effectively implemented in real-world scenarios? In practical federated learning settings, it seems challenging to determine whether cluster C2 supports cluster C1. Given that devices do not have visibility into each other's data during training, the primary method for assessing support appears to require multiple rounds of training and testing. This approach could lead to considerable resource inefficiencies.\n\n3. The main operations of the method are presented in algorithmic form in the appendix (e.g., Algorithms 5 and 6). However, it may be more beneficial for readers to understand these concepts if they are described in text form within the main body of the paper.\n\n4. In the experimental setup, there is no detailed information on how the data is divided or the rationale for this division. Different data partitioning methods can impact the final performance of federated learning."
            },
            "questions": {
                "value": "See weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a clustered federated learning method named FAACL, which aims to adaptively cluster devices and dispatch models for each device according to its cluster."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The author provides a theoretical analysis of its proposed cluster strategy."
            },
            "weaknesses": {
                "value": "This paper seems to be an incremental work of CFL. The contribution is limited.\n\nThis paper is poorly written. E.g., 1) the author should add a figure of framework and workflow. 2) The expression of lines 186-196 seems to detail the contributions and steps of the proposed method, rather than the components. I advise the author to reorganize the expressions.\n\nThe author attempts to utilize CFL-based strategy to address the problem of non-IID data. However, the author lacks comparison the proposed method with SOTA tradition federated learning optimization methods [1-4].\n\nThe proposed method cannot adapt to the secure aggregation method. The author should analyze the privacy of the proposed method.\n\nThe experiments are mainly based on MNIST and its variants. I would like to know how the method performs on more complex datasets, such as ImageNet. Please note that the experimental results show that the proposed method is not effective enough on the cifar10 dataset and is even inferior to the baseline."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}