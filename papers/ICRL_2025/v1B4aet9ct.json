{
    "id": "v1B4aet9ct",
    "title": "Schur's Positive-Definite Network: Deep Learning in the SPD cone with structure",
    "abstract": "Estimating matrices in the symmetric positive-definite (SPD) cone is of interest for many applications ranging from computer vision to graph learning. While there exist various convex optimization-based estimators, they remain limited in expressivity due to their model-based approach. The success of deep learning motivates the use of learning-based approaches to estimate SPD matrices with neural networks in a data-driven fashion. However, designing effective neural architectures for SPD learning is challenging, particularly when the task requires\nadditional structural constraints, such as element-wise sparsity. Current approaches either do not ensure that the output meets all desired properties or lack expressivity. In this paper, we introduce SpodNet, a novel and generic learning module that guarantees SPD outputs and supports additional structural constraints. Notably, it solves the challenging task of learning jointly SPD and\nsparse matrices. Our experiments illustrate the versatility and relevance of SpodNet layers for such applications.",
    "keywords": [
        "sparsity",
        "graphical lasso",
        "lasso",
        "deep learning",
        "neural networks"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "We propose a novel and generic learning module with guaranteed SPD outputs that can jointly handle additional structural constraints such as sparsity.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=v1B4aet9ct",
    "pdf_link": "https://openreview.net/pdf?id=v1B4aet9ct",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a new method SPODNET for learning SPD matrices by elements, supported by the classical Shur\u2019s condition, where the matrix elements (u and v) to update are learned using neural networks. The work demonstrates the use of SPODNET for the sparse precision matrix learning task, and proposes three new model architectures to perform the learning, including UBG, PNP and E2E. Two sets of experiments were conducted for evaluation, using a synthetic data and a real-world data. The results show the effectiveness of the proposed methods, and their advantages over the compared ones."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "\u2022\tTo my knowledge, the proposed method is original. I like the proposed SPODNET, building on classical theory, simple but elegant.\n\u2022\tThe paper is well written and well structured.\n\u2022\tExperiment design is appropriate for demonstrating the effectiveness of the proposed method. I particularly like the result of UBG in Figure 6, highlighting more distinct structure that is interesting  for this real-world data."
            },
            "weaknesses": {
                "value": "\u2022\tThe paper would be stronger if they could include another real-world learning problem over SPD manifold.  But I don\u2019t see this as a major issue.\n\u2022\tLack of discussion on the limitation of the proposed work.\n\u2022\tThere are a couple of things that could be explained better, see my questions.\n\u2022\tFigures 4 and 5 are too small, hard to read."
            },
            "questions": {
                "value": "\u2022\tAlthough the proof is straightforward, it would be useful for the reader and for completeness to explain how Eq. (3) is derived in proof.\n\u2022\tFor clarify, the role/design rationale behind each term in Eq. (5) can be explained briefly, although it is an existing method.\n\u2022\tIn line 364, it is mentioned that the MSE reconstruction loss is used. How is this implemented together with the GLasso loss in Eq. (5)?\n\u2022\tHow does the proposed method perform in terms of training time/cost?\n\u2022\tWhat do different colours mean in Figure 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a deep learning-based approach to solving SPD problems such as covariance selection. The authors start with the block coordinate descent (BCD) algorithm and then unroll the optimization using neural networks. This follows with three different unrollings, where each preserves different levels of the problem structures (or inductive bias in the deep learning words). The authors evaluate the proposed SpodNet on synthetic data and the animal dataset against GLAD, GLasso, and other traditional approaches."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-written and easy to follow. The idea of unrolling the column-row BCD algorithm to ensure SPD seems novel."
            },
            "weaknesses": {
                "value": "I think this is a borderline paper in its current form. I value the novelty of the paper, but its numerical performance is not the most convincing. I think solving the following points will make the paper stand more firmly at my rating.\n\n1. GLAD-Z: SpodNet's NMSE performance on synthetic data seems to be consistently worse than GLAD-Z's. I understand the authors' argument that GLAD-Z is not SPD, but what if Z is projected onto the SPD cone? Will the projected Zs remain the lower NMSE scores? Because GLAD uses an ADMM-like algorithm, the learned $\\Theta$ matrices are not projections if I understand correctly.\n2. Large sample regime: The NMSE performance of SpodNet is no better or only marginally better than the baselines when $n>p$.\n3. Real-world datasets: The results of GLAD on the animal dataset are missing. Also, the paper will benefit from adding at least another real-world dataset.\n4. Figures: Some figures can be hard to read, especially Fig. 4-5. I suggest the authors use thinner lines and/or redesign their layout to make line plots larger."
            },
            "questions": {
                "value": "1. Training details: What are the exact steps of the unrolling algorithm? How many unrollings are needed? From line 363, does the training posit an MSE loss on the intermediate $\\Theta$s or only the last one?\n2. GLasso: In Fig. 5, the F-1 performance of GLasso decreases when $n$ gets larger. This is weird because I expect it to recover the graph perfectly when $n\\gg p$. What are the authors' explanations about this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a method for jointly learning Symmetric Positive Definite (SPD) and sparse matrices using a learning based approach. The proposed method SpodNet is an iterative method which learns column-row pairs stepwise where each step is parametrized by a neural network. The SPD constraint is then ensured via Schur\u2019s condition."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. SpodNet enables increased expressivity for estimating SPD matrices with a neural network based parametrization, at the same time maintaining constraints like sparsity unlike other model based approaches, as verified by experiments on synthetic data.\n2. In order to make the method tractable the authors leverage the block structure of SPD matrices to restrict the complexity of each update step to $O(p^2)$.\n3. Experiments on synthetic data and graph topology estimation using SpodNet are presented, highlighting the effectiveness of the method. On the synthetic data, the proposed method is similar to model based approaches while achieving both SPD and sparsity at the same time."
            },
            "weaknesses": {
                "value": "1. The proposed SpodNet provides a novel approach to leverage neural networks and increase expressivity on constrained manifolds. However, the design of the neural networks itself is not discussed in detail. It is not clear to me why the authors choose the input features for  $g$ as $\\theta_{22}$, $s_{22}$ and $\\theta_{12} \\theta_{11}^{-1} \\theta_{12}$. Similar for each of the three described approaches the explanation for choosing the input features is missing. I assume the features are chosen to best suit the model based approach and that performs well with gradient descent but the paper would benefit from a detailed explanation of the same.\n2. The method still seems relatively expensive in spite of the improved update rule. The overall cost as the authors mentioned is of the order of $O(p^3)$, how does this compare with the other model based approaches?\n3. Can the SpodNet framework maintain other structure constraints for example structural sparsity. In general what conditions would the constraints need to satisfy in order to be optimized with a SpodNet layer.\n\nSince the general literature of SPD matrix estimation points towards applications in computer vision, it would be informative to see an experiment for a vision task with SpodNet to verify the comparison with baselines and its scalability given its computational requirements."
            },
            "questions": {
                "value": "See above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}