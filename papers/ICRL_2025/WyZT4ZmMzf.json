{
    "id": "WyZT4ZmMzf",
    "title": "Evaluating Representational Similarity Measures from the Lens of Functional Correspondence",
    "abstract": "Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain\u2014spanning alignment-based, CCA-based, inner product kernel-based, and nearest-neighbor methods\u2014we found that metrics like linear CKA and Procrustes, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research.",
    "keywords": [
        "Representational Similarity",
        "Vision",
        "Deep Neural Networks",
        "Behavior"
    ],
    "primary_area": "applications to neuroscience & cognitive science",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=WyZT4ZmMzf",
    "pdf_link": "https://openreview.net/pdf?id=WyZT4ZmMzf",
    "comments": [
        {
            "summary": {
                "value": "The authors assess the degree of correspondence between various representational similarity measures and task metrics for networks.  They show that certain representational metrics (CKA and Procrustes) distinguish trained from untrained models and align with task metrics.  Other representational metrics (linear predictivity) did not align well with task metrics.  In total they examine 19 models, 17 datasets, 9 task measures, and 8 representational measures."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is mostly straightforward and clear.  Although an empirical study, it tackles a useful question.  Figures 4 and 5 provide a clear and simple message."
            },
            "weaknesses": {
                "value": "The captions for each figure can be improved by including more information about what properties are being averaged over and what are being correlated (e.g. the dots in Figures 3 represent datasets).  \n\nThe use of the word \u201cbehavior\u201d is awkward to this reader, particularly when (unless I\u2019m mistaken) the only thing you are considering is classification.  This also makes the results sound much more general than they likely are.  It is certainly reasonable that the success of CKA and Procrustes in this case is due at least in part to the fact that the task in question is a discrete classification task.  It\u2019s easy to imagine that other tasks might yield better correspondence with different representational metrics.  \n\nOne has to wonder about the variability of different model instances.  Unless I\u2019m mistaken, you\u2019ve taken one pre-trained instance of each model.  \n\nThere isn\u2019t really any attempt at providing any theoretical justification for the results.  I realize that\u2019s a tall order and don\u2019t see it as necessary here, but it would be nice to have some understanding of why Procrustes and Linear CKA show such good agreement."
            },
            "questions": {
                "value": "What can you say about the variability of these metrics over different model instances?\n\nCan you apply this to other types of tasks beyond classification?  Do they still show good agreement with Procrustes and Linear CKA?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors used a collection of representational similarity measures to the activations in the penultimate layers of a large collection of deep neural networks trained on ImageNet. They then compare the results they get align with the results based on comparisons of the outputs of the network, i.e. their behaviour. They observe that the behavioural measures discriminate untrained vs trained networks well, but struggle to distinguish architectures, are are more related to RSA, linear CKA and Procrustes alignment than to nearest neighbour analyses and linear predictions and CCA measures."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Deeper understanding of the representational similarity measures is an important topic.\n\n\u2028Comparisons to the actual classification behaviour of the models is an interesting new viewpoint.\n\nThe basic results fit with earlier analyses and appear to be solid."
            },
            "weaknesses": {
                "value": "While I generally believe the results and they are somewhat interesting, I think the analyses could be much more thorough and broad. \n\nThe formula for RSA comparisons is wrong. To represent the classic formulation of RSA requires a standardisation of the X values and an important vectorisation step implied in $\\tau$. And there are many more modern and preferred variations of this technique today. Similarly the linear encoding model description given here says nothing about the important steps of dimensionality reduction, regularisation and cross-validation. Thus, unfortunately, I have doubts whether the authors implemented a sensible state of the art version of these comparisons. Also it is clear that they are missing quite a few methods that are used in practice.\n\nThe authors focus only on penultimate layers in imagine trained networks. This is a small subset of the representations we are interested in studying with representational similarity measures. Especially, how the relation to behavioural measures brakes down when earlier layers are used would be very informative.\n\nAs the authors admit, they do not provide a theoretical understanding why some measures are more or less aligned with behaviour. There is some literature discussing connections and differences between the different similarity measures. Thus, there would be something to discuss for sure.\n\nMost of the paper covers corse overall patterns in the data. I would have appreciated a deeper dive into what networks (pairs) the measures disagree about, on which tasks they behave differently, etc. Perhaps even some theoretical insight could be found there."
            },
            "questions": {
                "value": "I found the high correlations in A.3 quite surprising. How does it come that all tasks produce so similar similarity structure among models?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work seeks to answer the question: what does representational alignment between two models imply about their behavioral alignment? To address this question, they apply representational similarity metrics and behavioral metrics (based on object recognition) to a collection of computational models that include both trained and untrained models. They find that linear CKA and Procrustes are more correlated with behavioral metrics, whereas methods such as linear predictivity and CCA are much less correlated with behavioral metrics."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "There are several representational similarity metrics in the literature and there is relatively little understanding of their functional relationship. This work addresses this problem by comparing several metrics on a variety of behavioral tasks and models. The main strength of this paper is that it evaluates several similarity metrics (a total of 8 or 12 depending on if each k-NN is counted separately) and behavioral metrics (a total of 9). Thus, this works offers benchmarks for the representational alignment community as well as guidance for the experimental neuroscience and machine learning communities that use representational similarity metrics to evaluate representations."
            },
            "weaknesses": {
                "value": "In many ways, this paper seems incomplete. Much of the paper reads like a methods paper even though no new methods are introduced. Arguably, much of sections 1.1 and 1.2 could be relegated to the appendices. Apart from measuring the correlation between similarity metrics and behavioral metrics, there is little interpretation or investigation in the results section. In this way, this paper has the feel of exploratory analysis without follow up scientific hypothesis and analyses. Overall, I think this is a contribution that practitioners may find useful, but it could be significantly improved with scientific analysis of their findings."
            },
            "questions": {
                "value": "- Why does Procrustes correlate well with the behavioral metrics but CCA does not? My understanding is that these are part of a single family of representational metrics with the main difference being that CCA first whitenings the representation before aligning them (Williams et al. 2021). If so, what can we conclude about the model representations? \n- Some of the metrics are negatively correlated with behavioral metrics! What's going on there? Is this just noise?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors performed a systematic numerical investigation of different metrics of similarity/dissimilarity/distance between representations learned in deep neural networks. \nThe thesis is that understanding such metrics are important for both AI and neuroscience.\nThe contribution of the paper is to view this problem through the lens of \u2018behavioral outcomes\u2019. The authors conclude that two metrics (Procrutes and linear Centered Kernel Alignment), which emphasize overall geometric structure, are best aligned with behavioral outcomes."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "While this paper is well written, systematic, and of potential interest,"
            },
            "weaknesses": {
                "value": "there are major issues that preclude acceptance in its current form.\nThere is no technical novelty at all. The authors simply compare existing methods.\nWhile the numerical investigation is systematic, \nThe emphasis on methods that align with network outputs (i.e., behaviors) is parochial and poorly motivated. One could easily argue that being able to distinguish between different models with similar outputs is more interesting as it distinguishes between different mechanisms that produce the same outcome. IMHO, from a neuroscience perspective, this is actually the more interesting and important desiderata. A much more balanced approach to the presentation of the results is necessary in this manuscript."
            },
            "questions": {
                "value": "none"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}