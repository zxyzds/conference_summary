{
    "id": "OSmjkkF6Uy",
    "title": "FunBO: Discovering Acquisition Functions forBayesian Optimization with FunSearch",
    "abstract": "The sample efficiency of Bayesian optimization algorithms depends on carefully crafted acquisition functions (AFs) guiding the sequential collection of function evaluations. The best-performing AF can vary significantly across optimization problems, often requiring ad-hoc and problem-specific choices. This work tackles the challenge of designing novel AFs that perform well across a variety of experimental settings. Based on FunSearch, a recent work using Large Language Models (LLMs) for discovery in mathematical sciences, we propose FunBO, an LLM-based method that can be used to learn new AFs written in computer code by leveraging access to a limited number of evaluations for a set of objective functions. We provide the analytic expression of all discovered AFs and evaluate them on various global optimization benchmarks and hyperparameter optimization tasks. We show how FunBO identifies AFs that generalize well in and out of the training distribution of functions, thus outperforming established general-purpose AFs and achieving competitive performance against AFs that are customized to specific function types and are learned via transfer-learning algorithms.",
    "keywords": [
        "Bayesian optimization",
        "LLM",
        "acquisition function",
        "meta-learning"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We propose FunBO, an LLM-based method that can be used to learn new acquisition functions for Bayesian Optimization that perform well across a variety of experimental settings.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=OSmjkkF6Uy",
    "pdf_link": "https://openreview.net/pdf?id=OSmjkkF6Uy",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces FunBo, a method based on FunSearch that leverages large language models to automatically design acquisition functions (AFs) in computer code. FunBo takes an initial AF as input and learns a better one by iteratively modifying the initial one based on a limited number of evaluations from a set of objective functions. FunBO formulates the learning of AFs as an algorithm discovery problem, enabling it to explore the space of AFs systematically. Extensive experiments show the superiority of FunBo."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The idea of applying LLM to help generate AFs is interesting and novel.\n2. The authors perform extensive computational experiments, demonstrating that AFs generated by the proposed FunBo perform better than general-purpose AFs and are comparable to function-specific AFs."
            },
            "weaknesses": {
                "value": "1. The presentation can be improved and more consistent. For example, the algorithm in Figure 1 has some inconsistent parts with the figure in Figure 1. The positions of figures can also be more considerable. For example, Figure 1 is on the top of page 3 and first mentioned at the end of page 4.\n2. Algorithm pseudocode would be easier to read than the original code.\n3. Figures 1, 4, 5, 6, and 7 can be more professional."
            },
            "questions": {
                "value": "1. The authors claim that \u201cthis is the first work exploring AFs represented in computer code\u201d, are there any works that explore AFs represented in other forms? If there are any others, could you provide a detailed literature review on them? \n2. It is mentioned that this work focuses on Python programs, can the proposed FunBo be applied to design AFs in other languages? \n3. Could you clarify the difficulties or challenges that learning AFs written in computer code faced, and how FunBo deals with them?\n4. What does it mean by \u201ch(Left)\u201d in the Setup of Figure 1\u2019s algorithm?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose a method to leverage LLMs to programmatically adapt and improve acquisition functions (AF) in Bayesian optimization (BO). Notably, the proposed method seeds an LLM with a python implementation of a standard Expected Improvement function and iteratively samples proposed values from a suite of objective functions (not necessarily of the same domain), running an LLM-generated acquisition function, and scoring the output. This method expands previous work, notably FunSearch (Romara-Paredes et al., 2023) and MetaBO (Volpp et al., 2020), by allowing the objective functions (that one wants to optimize) to take values of differing dimensions, and allows more diverse input types for the generated acquisition functions.\n\n\nRomera-Paredes et al., \"Mathematical Discoveries from Program Search with Large-Language Models\", 2023.\n\nVolpp et al., \"Meta-learning acquisition functions for transfer learning in Bayesian Optimization\", 2020."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "In general, this paper is well-written and proposes an interesting idea that is supported by a suite of experiments. I am not well-acquainted with the state of this research area, but the motivation and proposed method in this paper are sound to me. By ensuring the output of the LLM is an annotated program, this maintains the interpretability of outputted AF. Additionally, the proposed method seems to leverage the \"in-context learning\" ability of LLMs to shape AFs that interpolate between generic all-purpose AFs (which may be generally suboptimal) and a specifically tuned AF (which may be ungeneralizable beyond the specific case) by training on a suite of objective functions, such as the various optimization test functions in the paper. Without access to a generalist learner, proposing a performant (and interpretable) new AF just by looking at a collection of test functions seems like a hard problem, thus the direction of this paper might lead to alleviated development time for downstream compute/evaluation-restricted tasks.\n\nThe numerical experiments seem to support that the method is actually sensible (at least on the function classes tested), showing that the iterative scoring/refinement scheme can actually retrieve optimal solutions in relatively few iterations compared to out-of-the-box standard AFs, which can converge slowly, or may not converge at all."
            },
            "weaknesses": {
                "value": "I have a couple questions (detailed in the respective section), which once clarified may solve some of the listed weaknesses.\n\nThe main numerical examples in this paper are low-dimensional optimization test functions and hyperparameter-tuning of a small-scale classification problem. I am wondering how this method can be scaled up efficiently to larger-scale problems (or real-life ones), where function evaluations are even more expensive; even ignoring the prompting cost, the evaluation loop is performed on each function of the provided class, which introduced a significant fixed cost to the algorithm. For example, in the context of hyperparameter-tuning, it is not clear from the experiments of this paper whether the proposed method can scale to large-scale, say, computer vision or robot learning tasks, where there may be far more moving parts (dimensions) to the BO problem than tested in this work. Some more evidence demonstrating scale-transfer, a la mu-parameterization (Yang et al. 2022), showing that FunBO training on smaller-scale problems can actually generate AFs that zero-shot transfer to the larger scale problem at hand, would be very enlightening on this front.\n\nAnother related issue that I do not see addressed in the paper is whether (or to what extent) the proposed method is scale-sensitive. In many settings (e.g. robot simulations), the scales of various parameters of the problem vary wildly, and are to some extent arbitrary by choice of unit. However, by using an LLM to generate the AF, it is not clear to me that the proposed method is sensitive to these possible variations of scale and thus possible source of covariate shift between the training and test tasks.\n\n\nYang et al. \"Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer\", 2022."
            },
            "questions": {
                "value": "In addition to looking for clarification on the weaknesses above, I have the following questions:\n\n- Can this be extended to generating optimization algorithms? At a high-level, instead of determining a procedure to select the next point only from sampled objective values, one can envision use a similar scheme to generate code for a descent step. If one proposes a class of building blocks, e.g. momentum, regularization, modular norms (Large et al, 2024), can this scheme be extended to generate optimizer code?\n\n- How well does the proposed method perform on (higher-dimensional) landscapes that are potentially very non-smooth and sensitive to collapse? For example, does the proposed scheme show promise for settings such as adversarial training or robot reinforcement learning that are otherwise challenging to tune manually?\n\n\nLarge et al., \"Scalable Optimization in the Modular Norm\", 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes the design of novel Bayesian optimization (BayesOpt) acquisition functions by using an LLM to generate the code for those acquisition functions.\nAs the code for the acquisition function is returned, this works towards improving interpretability for BayesOpt methods.\nExperiments show that the designed acquisition functions lead to better optimization performance than that of traditional acquisition functions and of meta-learned ones."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The application of FunSearch to designing BayesOpt acquisition functions is to my knowledge novel.\nThe writing is clear and easy to follow.\nThe experiments are convincing in showing that the generated acquisition functions perform better than included baselines."
            },
            "weaknesses": {
                "value": "I have two main concerns about the approach the paper takes.\nFirst, it seems the settings in which the proposed method can be used are where repeated optimization runs can be conducted, so that the LLM can gain information about the search space and generate well-performing acquisition functions.\nAt least, we would need access to objective functions drawn from the same distribution as the one we are targeting.\nIf I'm dealing with a one-off optimization problem, this seems to exclude the proposed method, which raises questions about the usability of the method.\n\nI also find the authors' argument for the desirability of the generated acquisition functions being interpretable not quite convincing.\nTo me, the general-purpose acquisition functions are quite interpretable, including the ones the authors flagged for being \"hard to implement and expensive to evaluate\" (entropy search and knowledge gradient).\nThis is because the motivations for the acquisition functions are clear (e.g., reducing entropy of the objective function's optimizer/optimum or increasing the expected one-step posterior mean optimizer), so even if the functional forms of these functions are complicated, I still view them as interpretable.\nThe proposed method, on the other hand, limits itself to acquisition functions whose inputs include the predictive mean and standard deviation, which only offers simply ways to address the exploration\u2013exploitation tradeoff and potentially sacrifices performance.\nOn line 399, is that functional form really that interpretable?\nSay the acquisition function is not doing well, could interpretability help us modify the functional form in a way that increases performance, for example?\n\nThe authors could consider including entropy search policies, knowledge gradient, as well as portfolio-style policies that automatically adjust the trade-off parameter of UCB as baselines."
            },
            "questions": {
                "value": "- Lines 202\u2013214 sound like we still require a training set of functions from $\\mathcal{G}$, as opposed to what is said on line 192.\nDo you have any comments on how to gauge the sufficient size for the training set given a specific optimization problem?\n- Lines 225--230: why sample only 2?\n- Perhaps other choices for the scoring function (e.g., negative cumulative regret) are possible.\nDo you have any results from ablation studies on this?\n- Fig. 3 (right panel) seems too busy.\nThe authors could consider adopting the style of e.g. [1] which shows queries as ticks at the bottom of the plots?\n- I'm not so sure Fig. 3 demonstrates the benfits of your method well.\nCan we see the associated GP predictions?\nEI seems to be \u201cstuck\u201d at the global optimum, which is not a bad thing.\nWhich value of the trade-off parameter $\\beta$ is UCB using? Can\u2019t the over-exploitation be fixed by increasing $\\beta$? This motivates including policies that automatically adjusts $\\beta$ as baselines.\n\n[1] Garnett, Bayesian Optimization, Cambridge University Press 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Given an expensive black box function $f\\:\\mathcal{X}\\to \\mathbb{R}$ where $\\mathcal{X}\\subset \\mathbb{R}^d$ and $d<10$, Bayesian optimization (BO) is a class of algorithm for finding global minimum  $\\min_x f(x)$. Typically, the two main design choices in BO are a Gaussian process that is fit to the past function observations $\\\\{x_i, f(x_i)\\\\}_i$, and an acquisition function (AF) that quantifies the benefit of a new sample point $\\alpha: \\mathcal{X}\\to \\mathbb{R}$. These choices are usually ad-hoc or based on expert knowledge.\n\nIn the case where we have a history of similar objective functions, we may use this history to help make these BO design choices for us.\n\nThis work proposes FunBO, a pipeline that takes as input a GP model and a set of objective functions, and uses a genetic algorithm (GA) for searching the space of acquisition functions returning the best acquisition function. GA requires specifying a mutation operator and a fitness function. This work uses the python source code generation ability of a large language model to take the source code of existing AFs and generate new AFs. The fitness is determined by taking an AF and executing BO with the GP model on all the objective functions and measuring the quality and number of iterations for the BO to converge.\n\nThe GA + LLM was first proposed by FunSearch and this paper adapts the framework for acquisition functions and BO.\n\nExperiments are performed and compared to MetaBO that also learns an optimal AF from past functions via a different procedure."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Intuitive: at a high level this seems very simple\n- well written: it was clear and easy to follow"
            },
            "weaknesses": {
                "value": "My concerns are mostly minor and arguably subjective.\n\n- __discretization__ the learnt AFs take as input a collection of points, does this not suffer the curse of dimensionallity? Typically AFs are continuously optimized by gradient ascent.\n\nMy following points focus essentially on the messaging of what is being learnt\n\n- __Structure of past functions__  I personally feel the paper should more clearly explicitly make the distinction between meta-learning a surrogate model and meta-learning an AF. Given an empirical distribution over functions, $\\mathcal{G}$, if they all have the same peak, or similar shapes e.g. linearity, unimodailty or peridicity (e.g. use a method like [Neural Processes](https://arxiv.org/pdf/1807.01622)), FunBO does not learn or exploit any such information, at multiple times the paper expresses \u201cstructure of past functions\u201d and I feel this is misleading and risks accidentally blurring the boundary between surrogate model and AF. A sentence to clarify this boundary would help especially for a broader non-expert audience e.g. \n> \u201cNote that meta learning surrogate model explicitly learns structure and features from past functions, e.g. minima in the same region of space, unimodality, linearity, and such common structure can be leveraged when fitting a surrogate model to a new function of the same class. FunBO does not itself explicitly learn any structure from past functions, rather, it takes as inputs an existing surrogate modelling method and history of functions and and outputs the best possible AF. In experiments in this work we use the most popular choice of surrogate modelling method, a GP with an RBF kernel.\u201d\n\n- __scope of learnable AFs__ the AFs only take predictive mean, predictive variance, and incumbent as arguments. Hence expensive exotic methods like Entropy search and KG that require covariance cannot be learnt.\n\n- __why does AF vary for different function classes?__ I realise this is out of scope of this work and the authors may argue this is an empirical observation from past works. Assume we have two different sets of training objectives $\\mathcal{G_1}$ and $\\mathcal{G}_2$ and we learn two different acquisition functions, $h_1()$ and $h_2()$, then given one set of predictive means, variances, and incumbent $(\\underline{\\mu}, \\underline{\\sigma}, y^*)$ and we pass these into each acquisition function, why should we get a different recommendation back? Particularly since $x$ is not an input, and the points are order invariant, the AF has no knowledge of where the points are or and where the peak may be. (Personally, my only intuition is that if the GP is a bad model fit for one function class, the AF may increase exploration as that is all it can do. In which case FunBO is learning AFs that learn to proportionally compensate for quality of GP model fit. Similarly the past works/BO community is a crowd sourced genetic algorithm making the same mistakes and is not specific to FunBO.)\n\n- __Conditioned on surrogate model__ Following from above, as the FunBO inputs are a surrogate modelling method and set of functions, then the output AF is conditioned on the choice of surrogate model, I may have missed it but this should also be explicitly stated. One possible corollary is that changing the surrogate model (new kernel or prior mean function) would also require re-running FunBO and finding the new AF. (e.g. as above with a better suited GP, FunBO can find an AF that trades off explore and exploit appropriately)\n\n- __sentitivity for out of distribution__ In the limitations, the authors state that FunBO was run multiple times to get the out-of distribution results, I thank you to the authors for the honesty and transparency, however I am not confused, this wording makes it sound like cherry picking, is not the average of all repeated runs reported?"
            },
            "questions": {
                "value": "- Given two AFs trained on different function classes and we pass the same arguments, what is the intuition that they should differ in their output? If it is compensating for a quality of the model fit?\n- the underlying assumption of finding an AF that works out of distribution suggests that the underlying distribution does not matter? There is one and only one true best AF?\n- how sensitive is FunBO to the choice of GP model?\n- can the authors clarify how the out of distribution results were tested?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper extends the LLM-based FunSearch algorithm by proposing a novel framework, FunBO, for discovering acquisition functions in Bayesian Optimization (BO). FunBO's main pipeline is based on an iterative optimization scheme to incrementally improve a baseline acquisition function. At each iteration, the algorithm proposes new acquisition functions, evaluates them through a complete BO loop, and records their performance in a database, ultimately selecting the best function for usage. FunBO is tested on standard BO benchmarks, specific out-of-distribution (OOD) tasks, and in a few-shot adaptation scenario."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Explainable Acquisition Function**: In the context of meta-learned acquisition function prediction using neural networks, FunBO provides a pipeline for creating interpretable acquisition functions.  \n\n**Exhaustive Benchmarking across Domains**: The paper thoroughly benchmarks FunBO across different application settings. Notably, the generated acquisition functions may also work out of the box for arbitrary objective functions, potentially addressing generalization issues common in meta-learning frameworks."
            },
            "weaknesses": {
                "value": "**Restrictive Acquisition Function Input**: Unlike previous approaches, this framework only uses the model\u2019s marginal predictive distribution (predictive mean and variance) as inputs for the acquisition function. This restriction may limit its applicability compared to more complex but empirically powerful acquisition functions (e.g., Entropy Search, Knowledge Gradient).\n\n**Limited Comparison with Recent Meta-BO Work**: While the paper includes comparisons with MetaBO [1], adding comparisons with more recent meta-acquisition functions (e.g., [2]) would provide a clearer picture of FunBO\u2019s empirical performance.\n\n\n**Lack of Theoretical Insight**: As with most LLM-based models, FunBO primarily relies on an empirical pipeline, without providing theoretical insights into the underlying methodology. It remains unclear to what extent this method will be useful in practice.\n\n**Computationally Intensive Training**: Similar to other RL-based meta-acquisition function frameworks, FunBO\u2019s iterative learning scheme requires a full BO loop evaluation at each iteration, which is computationally cumbersome.\n\n\n[1] Volpp, M., Fr\u00f6hlich, L. P., Fischer, K., Doerr, A., Falkner, S., Hutter, F., & Daniel, C. (2019). Meta-learning acquisition functions for transfer learning in bayesian optimization. arXiv preprint arXiv:1904.02642.  \n\n[2] Maraval, A., Zimmer, M., Grosnit, A., & Bou Ammar, H. (2024). End-to-end meta-bayesian optimisation with transformer neural processes. Advances in Neural Information Processing Systems, 36."
            },
            "questions": {
                "value": "**Surprisingly good random search**: On OOD-Bench: One surprising result is the strong performance of random search, suggesting it performs well across a wide range of problems. This raises concerns about the experiment\u2019s credibility. Could the authors elaborate on why random search appears so effective in this context? This is essentially saying BO is not useful for a lot of problems."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}