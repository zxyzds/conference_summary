{
    "id": "KGRV73Zcqt",
    "title": "Cross Resolution Encoding-Decoding For Detection Transformers",
    "abstract": "Detection Transformers (DETR) are renowned object detection pipelines, however\ncomputationally efficient multiscale detection using DETR is still challenging. In\nthis paper, we propose a Cross-Resolution Encoding-Decoding (CRED) mechanism\nthat allows DETR to achieve the accuracy of high-resolution detection while\nhaving the speed of low-resolution detection. CRED is based on two modules;\nCross Resolution Attention Module (CRAM) and One Step Multiscale Attention\n(OSMA). CRAM is designed to transfer the knowledge of low-resolution encoder\noutput to a high-resolution feature. While OSMA is designed to fuse multiscale\nfeatures in a single step and produce a feature map of a desired resolution enriched\nwith multiscale information. When used in prominent DETR methods, CRED\ndelivers accuracy similar to the high-resolution DETR counterpart in roughly 50%\nfewer FLOPs. Specifically, state-of-the-art DN-DETR, when used with CRED\n(calling CRED-DETR), becomes 76% faster, with \u223c 50% reduced FLOPs than its\nhigh-resolution counterpart with 202 G FLOPs on MS-COCO benchmark. We plan\nto release pretrained CRED-DETRs for use by the community.",
    "keywords": [
        "Cross Resolution",
        "Encoding and Decoding",
        "DETR",
        "Detection"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "Computationally Efficient High Resolution DETR",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=KGRV73Zcqt",
    "pdf_link": "https://openreview.net/pdf?id=KGRV73Zcqt",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a Cross-Resolution Encoding-Decoding (CRED) mechanism to enhance the efficiency of DETR for object detection. CRED consists of two key modules: the Cross Resolution Attention Module (CRAM) and One Step Multiscale Attention (OSMA). CRAM transfers knowledge from low-resolution outputs to high-resolution features, while OSMA fuses multiscale features in one step. By integrating CRED into DETR models, such as DN-DETR, the approach achieves similar accuracy to high-resolution DETR with fewer FLOPs and faster performance."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The proposed approach feeds the encoder with low-resolution features while supplying the decoder with high-resolution features from the backbone. This method achieves an effective speed-accuracy tradeoff. The results are demonstrated using MS-COCO dataset."
            },
            "weaknesses": {
                "value": "The idea of combining low-resolution and high-resolution features or using multiscale features to enhance DETR performance is not new, as similar approaches have been used in previous studies, such as Zhang et al., (2023a); Zhao et al., (2024b); and Li et al., (2023). How do you justify the novelty of your contributions? \n\nThe proposed approach has only been evaluated on a single dataset, MS COCO. While it outperforms the baselines, as shown in Table 1, it achieves only competitive results compared to state-of-the-art methods in Table 2. It would be more convincing if the approach were tested on additional datasets."
            },
            "questions": {
                "value": "The proposed OSMA combines multiscale features along the channel dimension. However, multiscale features typically have different numbers of channels\u2014high-resolution features usually have fewer channels than low-resolution ones. How did you handle this discrepancy when aggregating them by channel? Did you reduce the number of channels for the low-resolution features? Could you provide more details about the implementation? \n\nAdditionally, the citation format needs adjustment. For instance, in some places, citations like \"Zhang et al. (2023a)\" should be written as \u201c(Zhang et al. 2023a).\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a cross-resolution encoding-decoding mechanism to improve the efficiency of DETR. Two important components CRAM and OSMA are introduced to convert the knowledge of the low-resolution encoder output into high-resolution features. Extensive analyses and experiments have demonstrated the effectiveness of the method."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. CRED improves the computational efficiency of DETR by allowing for the accuracy of high-resolution detection while operating at the speed of low-resolution processes.\n\n2. Enhance the DETR block CRAM and OSMA are two main module for CRED. CRAM can facilitate the info transfer between high resolution and low resolution. OSMA simplifying the integration of detailed features across different scales.\n\n3. The ablation experiment is comprehensive and adequate."
            },
            "weaknesses": {
                "value": "1. More datasets is better, only on MS-COCO2017 didn't quite convince me.\n\n2. While FLOPs and FPS metrics are provided, a more in-depth discussion of the tradeoffs between accuracy, computational cost, and speed of inference would be beneficial a lot.\n\n3. Why is augmenting global information to high-resolution features better than sparse sampling methods(e.g.,IFMA)?\n\n4. The main experiments have used DETR as a baseline, and I would like to know how the method performs on other more advanced detection algorithms, which would be valuable in understanding the generalizability of the method."
            },
            "questions": {
                "value": "Please see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new Cross-Resolution Encoding-Decoding (CRED) mechanism aimed at improving the computational efficiency of Detection Transformers (DETR) while maintaining high detection accuracy. The CRED mechanism consists of two key modules: the Cross-Resolution Attention Module (CRAM) and the One Step Multiscale Attention (OSMA). CRAM transfers knowledge from low-resolution encoder outputs to higher-resolution decoder inputs, while OSMA fuses multiscale features in a single step, generating feature maps enriched with multiscale information. When applied to state-of-the-art DETR variants, the proposed method significantly reduces computational complexity, with the authors claiming a 50% reduction in FLOPs and a 76% increase in FPS, all without sacrificing detection accuracy. The paper presents extensive experiments, ablation studies, and comparisons with existing methods to support the claims."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The CRED mechanism is a sound approach to reduce the computational cost of DETR models by balancing low-resolution encoding with high-resolution decoding. \n2. Extensive experiments are implmented of the CRED-enhanced DETR models on the MS-COCO 2017 benchmark. The results show large improvements in FLOPs and runtime without loss in detection accuracy.\n3. The paper provides detailed ablation studies, analyzing the contributions of the individual CRAM and OSMA modules. This shows the influence of each component and provides insight into the design choices.\n4. The CRED mechanism is shown to be effective across different DETR variants and backbone architectures, indicating that it has a good scalability and can be applied to various object detection pipelines."
            },
            "weaknesses": {
                "value": "1. The overall design is more like an engineering design and lacks technical novelty. While the empirical performance improvements are well-documented, the paper could benefit from a more in-depth theoretical analysis of the CRED mechanism. For example, why does transferring low-resolution encoder information to high-resolution decoder inputs improve performance?\n2. While the authors claim that CRED improves the detection of small objects, the improvements in average precision for small objects (APS) are relatively modest in some configurations. This suggests that while the CRED mechanism provides overall efficiency gains, its impact on small object detection could be further optimized.\n3. Although the authors compare CRED against several state-of-the-art DETR models (such as Deformable DETR, DN-DETR, and IMFA), there is a lack of comparison with some of the latest advancements in efficient transformer architectures, such as sparse transformers and vision transformers (ViTs). \n4. The paper provides limited qualitative analysis through visualizations of detection results (Figures 7\u201310). While these figures are informative, a more detailed qualitative analysis of how CRED affects detection in challenging cases (e.g., cluttered backgrounds, occlusions) would strengthen the paper's narrative."
            },
            "questions": {
                "value": "1. Including the comparison of latest efficient transformers would provide a more comprehensive evaluation of CRED\u2019s competitiveness.\n2. Given the modest gains in small object detection (APS), the authors could explore further optimizations to make CRED more effective for this class of objects."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper present a novel method named Cross-Resolution Encoding-Decoding (CRED), which can reduce the computational cost of the traditional DETR model , especially for high-resolution detection. By using CRAM module and OSAM module, the CRED effectively combines low-level and high-level feature to achieve the balance between detection accuracy and computational efficiency. The authors validate the effectiveness of CRED through a series of experiments and ablation studies."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The motivation of the paper is well-founded. The high computational burden in high-resolution detection is an important problem, and the combination of both low-level and high-level feature is a reasonable approach.\n\nThe experiment results show that the proposed CRED approach improves performance compared to the original DETR, which demonstrates the effectiveness."
            },
            "weaknesses": {
                "value": "Compare with original DN-DETR\uff0cthe improvement of CRED-DERT is not significant. More comparative analysis and deeper insights into the benefits of CRED over DN-DETR will strengthen the contribution.\n\nThe motivation of introducing multi-scale features is reasonable, but the CRAM has a similar structure to FPN, which seems to be more of an adaptation rather than a breakthrough\n\nLack of the comparation with SoTA method, such as RT-DETR. [1]. As far as I know, RT-DETR achieves higher throughput and is designed for real-time application. The absence of comparisons with such methods weakens the evaluation of CRED\u2019s performance, particularly in terms of speed and efficiency, which are key claims of the paper.\n\nLack of visual comparison with other SoTA methods, especially the detecting result of small objects, which is the evidence of the advantage of CRED.\n\n[1] DETRs Beat YOLOs on Real-time Object Detection, CVPR, 2024"
            },
            "questions": {
                "value": "Is the local aggregation of multiscale features module in OSMA really efficient in practice and friendly to the hardware? This module requires a lot of memory discontinuous operations, which may limit the actual running speed of the model.\n\nAs shown in Figure1, the input of CRAM module is the feature from Stage-4 or Stage-3, why we do not use Stage-2 or combine the feature from Stage-4 and Stage-3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}