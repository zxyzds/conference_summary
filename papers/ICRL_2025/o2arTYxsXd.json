{
    "id": "o2arTYxsXd",
    "title": "Overcoming Catastrophic Forgetting in Federated Class-Incremental Learning via Federated Global Twin Generator",
    "abstract": "Federated Class-Incremental Learning (FCIL) increasingly becomes essential in the decentralized setting, where it enables multiple participants to collaboratively train a global model to perform well on a sequence of tasks without sharing their private data. In FCIL, conventional Federated Learning algorithms such as FedAvg often suffer from catastrophic forgetting, resulting in significant performance declines on earlier tasks. Recent works based on generative models produce synthetic images to help mitigate this issue across all classes. However, these approaches' testing accuracy in previous classes is still much lower than recent classes, i.e., having better plasticity than stability. To overcome these issues, this paper presents Federated Global Twin Generator (FedGTG), an FCIL framework that exploits generative-model training on the global side without accessing client data. Specifically, the server trains a data generator and a feature generator to create two types of information from all seen classes. Then, it sends the synthetic data to the client. The clients then use feature-direction-controlling losses to make the local models retain knowledge and learn new tasks well. We extensively analyze the robustness of FedGTG on natural images and its ability to converge to flat local minima and achieve better predicting confidence (calibration). Experimental results on CIFAR-10, CIFAR-100, and tiny-ImageNet demonstrate the improvements in accuracy and forgetting measures of FedGTG as well as the robustness of domain shifts compared to previous frameworks.",
    "keywords": [
        "federated learning",
        "continual learning",
        "federated continual learning",
        "generative model"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=o2arTYxsXd",
    "pdf_link": "https://openreview.net/pdf?id=o2arTYxsXd",
    "comments": [
        {
            "summary": {
                "value": "This paper introduce FedGTG (Federated Global Twin Generator), a novel FCIL framework that leverages generative models on the server side without accessing client data. FedGTG trains a data generator and a feature generator to produce synthetic representations of all seen classes, which are then shared with clients to help balance knowledge retention (stability) and learning new tasks (plasticity). Through extensive experiments on CIFAR-10, CIFAR-100, and tiny-ImageNet, FedGTG shows significant improvements in both accuracy and resistance to forgetting"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper writing is clear and easy to follow.\n2. The proposed FedGTG absorbs the advantage of many other algorithms, leading to a strong framework for FCIL.\n3. The experimental results are good. The proposed FedGTG outperforms all baselines on each benchmark and each incremental task. And the ablation study shows that each loss term is crucial and  has a large impact on the final results."
            },
            "weaknesses": {
                "value": "1. The proposed lacks enough novelty. All loss functions used in FedGTG are proposed by other papers.\n2. The framework contains too many hyperparameters, as each loss term needs one. It seems the framework is hard to be finetuned to the optimal result, as there are too many combinations of choice of hyperparameters. And the paper lacks the experiments and discussion of these hyperparameters.\n3. More baselines should be compared. Except for generative-based methods, authors should compare FedGTG with other SOTA FCIL algorithms."
            },
            "questions": {
                "value": "Why do you design the model architecture of data and feature generation as you shown in the paper? Are there any key ideas of why using these architectures?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Federated Global Twin Generator (FedGTG), and proposes a framework leveraging generative models on the global side, sending synthetic data to clients for retaining knowledge and learning new tasks effectively. FedGTG shows enhanced accuracy, reduced forgetting, and robustness against domain shifts on CIFAR-10, CIFAR-100, and tiny-ImageNet."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is overall well-written in terms of writing.\n2. The introduction part clearly especify the problem about generators.\n3. The experiments are showing nontrivial improvements over compared techniques."
            },
            "weaknesses": {
                "value": "1. Not clear what the biggest contributions are. Is it the two generator structure? If so, the novelty does not seem very high. The paper also introduces a direction-controlling objective, which should also stand out, but not very clearly in the paper (e.g., Figure 1).\n2. Need to include training time or cost in the comparison. It seems that training two generators are quite costing, and it already needs more parameter during the training.\n3. Is the method an exemplar-free technique? If so, the literature reviewer is not very complete. Quite a few new works on exemplar-free CIL are missing in the continual learning section, such as the analytic continual learning branch [1-3], and prototypes-based CIL.\n\n[1] \"ACIL: Analytic class-incremental learning with absolute memorization and privacy protection.\" Advances in Neural Information Processing Systems 35 (2022): 11602-11614.\n\n[2] \"GKEAL: Gaussian kernel embedded analytic learning for few-shot class incremental task.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[3] \"DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free Class-Incremental Learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 15. 2024."
            },
            "questions": {
                "value": "see weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "- This paper focuses on federated continual learning, federated class-incremental learning to be more specific. It proposes a new method of using two generator to generate synthetic data and feature. The synthetic data and feature are sent to the client for local training. In local training, this work proposes multiple losses to improve the training process."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The propose method of using an additional generator is simple and intuitive, while it is not straightforward to make it work. It is technically sound that the authors propose additional loss functions to stablize the training. \n- The paper is generally well-written and easy to follow.\n- Extensive experiments are conducted through 3 datasets with detailed ablation studies. Sufficient exisiting works are compared. The proposed method achieves much strong performance than the compared counterparts."
            },
            "weaknesses": {
                "value": "- Figure 1 is not easy to  understand. It would be useful if the authors could provide more explanations or making it more intuitive.\n- The abstract claims that \u201cthese approaches\u2019 testing accuracy in previous classes is still much lower than recent classes\u201d, but it seems the paper does not provide experiments showing that the proposed method addresses this issue.\n- It seems that only a single backbone is used in the evaluatiion. \n- Typo in line 83, should be \u201cmethods\u201d."
            },
            "questions": {
                "value": "- Does this work focus on cross-silo or cross-device FL? What is the number of clients evaluated?\n- What is the meaning of \u201cstability-plasticity\u201d ?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "Federated Learning (FL) is a privacy-preserving machine learning approach, but it faces challenges related to resource limitations and data heterogeneity, especially as client data distributions evolve over time. Federated Class-Incremental Learning (FCIL) combines FL and Class-Incremental Learning (CIL), enabling models to continuously learn new knowledge from distributed data sources without forgetting prior knowledge. However, existing methods such as TARGET and MFCL, which use data generation to balance knowledge retention for new and old tasks, still encounter forgetting issues. To address this, this paper proposes the Federated Global Twin Generator (FedGTG) framework, which trains and shares data and feature generators on the server side, using direction-control loss to enhance stability and plasticity on the client side. Experiments demonstrate that FedGTG outperforms existing methods in terms of accuracy and forgetting rate."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper enhances current GAN methods for Federated Incremental Learning by integrating both a feature GAN and a data GAN, with the feature GAN employing regularization to achieve improved performance and stability across incremental tasks.  \n2. Extensive experiments across multiple datasets demonstrate the robustness and validity of the proposed model, showcasing its effectiveness in various scenarios.  \n3. Unlike prior GAN-based methods, which often suffer from catastrophic forgetting, this work introduces innovative mechanisms that significantly mitigate such issues, representing a valuable contribution to the field."
            },
            "weaknesses": {
                "value": "1. The experiments in the paper lack critical details, including the ratio of generated data used in each training cycle, the proportion within individual batches, and the hyperparameters for each loss function, which should ideally be explored through ablation studies. Additionally, the class-incremental setup is not clearly explained, leaving me unsatisfied with the experimental rigor.\n\n2. The datasets used throughout the paper are relatively simple; I recommend conducting further experiments on high-resolution datasets such as ImageNet to strengthen the evaluation.\n\n3. The contributions of the paper are limited. For instance, incorporating a feature GAN as a regularization term is not substantially different from traditional regularization methods in incremental learning.\n\n4. It is unclear whether the GAN trained after each task is independently trained or incrementally trained based on the previous task. If trained independently, this approach would impose excessive storage and communication costs; if incrementally trained, the GAN itself could suffer from catastrophic forgetting. These aspects present limitations in the proposed method."
            },
            "questions": {
                "value": "see weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}