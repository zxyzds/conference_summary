{
    "id": "S8gbnkCgxZ",
    "title": "Redefining the task of Bioactivity Prediction",
    "abstract": "Small molecules are vital to modern medicine, and accurately predicting their bioactivity against protein targets is crucial for therapeutic discovery and development. However, current machine learning models often rely on spurious features, leading to biased outcomes. Notably, a simple pocket-only baseline can achieve results comparable to, and sometimes better than, more complex models that incorporate both the protein pockets and the small molecules. Our analysis reveals that this phenomenon arises from insufficient training data and an improper evaluation process, which is typically conducted at the pocket level rather than the small molecule level. To address these issues, we redefine the bioactivity prediction task by introducing a million-scale structural small molecule-protein interaction dataset for unbiased bioactivity prediction task (SIU), which is 50 times larger than the widely used PDBbind. The bioactivity labels in SIU are derived from wet experiments and organized by assay types, ensuring greater accuracy and comparability. The complexes in SIU are constructed using a majority vote from three commonly used docking software programs, enhancing their reliability. Additionally, the structure of SIU allows for multiple small molecules to be associated with each protein pocket, enabling the redefinition of evaluation metrics like Pearson and Spearman correlations across different small molecules targeting the same protein pocket. Experimental results demonstrate that this new task provides a more challenging and meaningful benchmark for training and evaluating bioactivity prediction models, ultimately offering a more robust assessment of model performance.",
    "keywords": [
        "Bioactivity Prediction",
        "New Dataset"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=S8gbnkCgxZ",
    "pdf_link": "https://openreview.net/pdf?id=S8gbnkCgxZ",
    "comments": [
        {
            "summary": {
                "value": "The paper addresses significant issues in current evaluation strategies for bioactivity prediction, identifying problems with data preparation and evaluation metrics that result in inadequate benchmarking. For instance, approaches that only consider protein features in protein-small molecule interaction predictions may outperform methods that consider both due to overfitting. To tackle these challenges, the authors propose a new large dataset SIU based on mining wet lab bioactivity data and software-based docking to obtain structures, along with new evaluation metrics. The evaluations demonstrate the superiority of SIU over existing datasets and support the adoption of the new metrics."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "**Originality**. The paper presents original research with novel isights into the issues in bioactivity predictions and novel methodology for constructing bioactivity data for machine learning.\n\n**Quality**. The experiments are sound and convincing, clearly demonstrating the inadequacies of prior evaluation methods and the value of the new dataset and metrics.\n\n**Clarity**. The paper is easy to follow.\n\n**Significance**. The paper addresses a highly important issue of inadequate benchmarking in bioactivity prediction. The new SIU dataset is substantially larger than existing alternatives."
            },
            "weaknesses": {
                "value": "**Major Comments**\n\n- The availability of the dataset is not discussed. Will it be publicly accessible, and if so, in what form?\n- Data deduplication and diversity measurement methods are only briefly mentioned and not described anywhere in the text. Specifically, please elaborate on the ECFP and FLAPP methods used for deduplication of small molecules and pockets, respectively, and analyze the diversity and statistics after deduplication.\n- Although the paper proposes a large dataset, further analysis of the data splitting approach would be beneficial. The meanings of 0.6 and 0.9 non-homology levels are unclear (Does 0.6 signify 60% sequence identity upon alignment? How are sequences aligned?). Furthermore, is sequence similarity-based splitting sufficient with respect to data leakage? Recent work suggests that structure-based splitting may be a better choice [[1](https://arxiv.org/abs/2402.18396), [2](https://www.biorxiv.org/content/10.1101/2024.07.17.603955v1)].\n- The related work section is brief, only covering a few related datasets. Including related work on bioactivity prediction baselines to justify the selected methods for experiments and discussing prior evaluation metrics would strengthen the paper.\n- A table summarizing the statistics (for example, number of proteins, ligands, pairs, unique pockets, unique ligands, etc.) for the training and test sets of SIU and other datasets like PDBbind would be beneficial.\n\n**Minor Comments**\n\n- Abstract: please clarify what SIU stands for. From the abstract it is not clear that SIU is a dataset.\n- Figure 1: What is the GNN architecture?\n- Figure 1C: the meaning of the red line and the target are not clear.\n- Line 153: please specify the cutoffs being referenced.\n- Figure 2: Poses are visualized in the same way as ligands, which is confusing.\n- Line 209: \u201cidentified by PDB IDs\u201d contradicts earlier text, which states that multiple pockets may arise from the same PDB ID.\n- Figure 3: clarify the meaning of RMSD\u2014is it the RMSD between a docking pose and the ground truth from PDB, or between docking poses obtained with different methods to quantify consensus?\n- Section: \u201cStructural data construction via multi-software docking\u201d: Same as above, RMSD is used in two different senses, which is not always clear. \t\n- Section: \u201cStructural data construction via multi-software docking\u201d: Are the three chosen docking methods independent? Why were these specific methods selected?\n- Line 280: the claim \u201cdiverse small molecules\u201d is unsupported by experiments.\n- Line 346: \u201cfor each target\u201d is slightly confusing\u2014are there only 10 targets in the dataset?\n- Line 392: clarify the statement, \u201cWe conducted non-homology analyses at two levels, 0.6 and 0.9\u201d\u2014does this imply a maximum of 60% and 90% sequence identity between training and test proteins?\n- Line 424: UniMol reference appears to be missing.\n- Tables 1 and 2: highlighting the highest values in bold would improve clarity.\n- Line 496: \u201cpdb\u201d should be in uppercase (PDB)."
            },
            "questions": {
                "value": "- Would a complementary analysis focused solely on small molecules, rather than only proteins, lead to the same conclusions? For example, could training a small-molecule-only baseline on PDBbind yield similar insights?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper has two main contributions. First, it presents a new large-scale dataset of small molecule-protein interactions for unbiased bioactivity prediction, named SIU. Second, the authors redefine evaluation metrics for bioactivity prediction by proposing to average correlation coefficients per protein before calculating the overal coefficients, and by stratifying performance evaluation across four key types of bioactivity labels. The benefits of the proposed dataset and metrics are demonstrated empirically through the evaluation of several standard models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Originality\n\nThe work\u2019s originality lies primarily in its focus: rather than routinely proposing new models using existing datasets without analyzing their limitations, it critically examines the dataset and evaluation metrics themselves.\n\nQuality\n\nThe technical quality of the work is high. The methods employed are appropriate and well-justified.\n\nClarity\n\nThe paper includes a thorough introduction that provides clear context and a strong foundation for the study.\n\nSignificance\n\nThis work is highly significant for the machine learning community, as it introduces both a new dataset and an updated evaluation protocol for the important task of bioactivity prediction, while effectively demonstrating limitations in previous datasets and evaluation methods."
            },
            "weaknesses": {
                "value": "Major Concerns:\n\n- Data and Code Availability: A primary contribution of this work is the benchmark for bioactivity prediction, which includes a new dataset, evaluation metrics, and a specific data split. However, without readily accessible resources, the value of the work may be limited, as reproducing the benchmark independently could be challenging (e.g., requiring extensive runs across multiple docking software tools). It would be helpful if the authors addressed data availability, perhaps by providing a link to an anonymous GitHub repository.\n\n- Simulated Poses vs. Crystal Structures: The dataset consists exclusively of simulated poses rather than crystal structures. Since the dataset is described as high-quality, a discussion of the tradeoffs between experimental crystal structures and generated poses would add valuable context. For example, the dataset is constructed based on a consensus from two out of three docking tools. Additional information on whether this approach assures high quality\u2014especially if one tool generates a different pose\u2014could be beneficial. The authors might also consider presenting results from training on SIU and evaluating on PDBbind to explore any potential distribution shifts.\n\n- Sequence Identity-Based Data Splitting: The paper uses sequence identity-based data splitting; however, since the task is defined in terms of 3D structures, a structure-based split may offer a more appropriate approach, especially considering that similar structures may arise from different sequences.\n\n- Validation Fold Construction: Details on the construction of the validation fold would clarify this important aspect, as it affects both model training outcomes and the usability of the dataset in future work.\n\n- Test Fold Curation: While the manual curation of the test fold is appreciated, it would be helpful to understand the choice of focusing on only 10 protein targets, as this might affect the generalizability of the evaluation. Could the authors expand it to more proteins?\n\n- Redefining the Bioactivity Prediction Task: Since the paper redefines the bioactivity prediction task, a clearer description of the specific inputs and outputs considered would be helpful. For instance, it is unclear which ligand pose is used as input when three docking tools are involved. Additionally, Section 3.3, \"Reframing the Bioactivity Prediction Task,\" does not discuss the RMSE and MAE metrics used for evaluation.\n\n- Figure 3 Interpretation: Interpreting Figure 3 is challenging without a more precise definition of the evaluation using co-crystal poses from PDB complexes. The text does not clarify which PDB complexes were included and does not clearly define Success Ratio and Remaining Ratio.\n\nMinor Concerns:\n\n- The abstract does not define what SIU stands for.\n- The process for filtering based on extended-connectivity fingerprints could be elaborated. Is it based on pairwise Tanimoto similarities between fingerprints?\n- Line 330: The statement, \"Our structured approach facilitates nuanced assessments, such as evaluating the impact of specific small molecule modifications on protein interactions or comparing the efficacy of different compounds within the same protein pocket context,\" is not clear, especially as some filtering of ligands was involved based on extended-connectivity fingerprints.\n- Line 282: The acronym \"AIDD\" could be defined for clarity.\n- Line 346: It appears that Figure 4(D) should be replaced with Figure 4(B)."
            },
            "questions": {
                "value": "- Could the authors clarify how the RMSE and MAE metrics are averaged? It would also be helpful to understand why RMSE* and MAE* are not defined using the same approach as Pearson* and Spearman* from Pearson and Spearman, respectively.\n- Could the authors explain why grouping by pockets is considered equivalent to grouping by PDB IDs? Is not it possible for a single protein to have multiple pockets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a new benchmark for training and testing of bioactivity prediction models, a critical task in computational biology. The curation of the training data involves filtering a set of more than one million bioactivities, cluster them into predictions around specific pockets and predict approximate structures for each of them. The curation of the test set the selection of a small subset of test complexes and the definition of the task as correlations within each target. Finally, the authors train a number of models on this new benchmark and highlight how the poor results on this assessment suggests more work needs to be done."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "I believe that the curation of a dataset for protein small molecule activity prediction is very important. Current datasets are too small and used in the incorrect way as the authors also represent. In this work, the authors do a very extensive and carefully thought out job towards building such dataset."
            },
            "weaknesses": {
                "value": "Building and proposing a new benchmark constitutes a significant responsibility: once the benchmark is released and, as I hope, the community adopts it, it will become very hard to correct any wrong choice made at this stage and these may lead to a lot of work being largely wasted (as I believe it is happening with current benchmarks for this task). Therefore, my bar is not just having an improvement over what is public but making sure that researchers in the field would not have concerns about adopting such a dataset.\n\nBased on this premise below I\u2019m presenting a list of concerns that I would appreciate the authors responding to or addressing:\n\n1. The authors only retained datapoints for which the docking methods were consistent in their majority. While this appears sensible, it also creates a bias in the kind of molecules that the model considers (intuitively only \u201ceasier\u201d complexes). It could be interesting to ablate the addition of non-consensus poses in the training set or in the test set for some of the baselines.\n\n2. The success proportion presented appears somewhat misleading. It is well known that redocking or holo-structure docking (that the authors use to validate the structure generation) is significantly easier than cross-docking (docking to the protein structure resolved when binding to a different protein, which the authors have to use when generating new structures). Therefore the experiments on the success should be performed on the cross docking task. This can be performed by taking from PDB structures of ligands bound to the same pocket.\n\n3. Saying that structures generated with the docking programs are of high quality seems a bit exaggerated or at least lacking experimental backing (see comment above on the difference between redocking and cross-docking). Multiple works in the molecular docking community have shown significant issues with the apo or cross docked performance of the methods applied for docking in this work.\n\n4. Assay types might be a bit misleading as the name of the \u201clabel type\u201d as there are many different types of assays producing for example Kd values.\n\n5. The definition of different pockets from PDB IDs of co-crystal ligands and the subsequent deduplication are very unclear. While this seems a relatively non-important detail it becomes critical once the authors decide to use this assignment to compute correlation over.\n\n6. For protein ligand complexes from the bioactivity assays the way that these are assigned to a specific PDB ID / pocket is not clear to me. The available information is the Uniprot but then how does one go from having a specific Uniprot to deciding which of the PDB IDs corresponding to the different pockets to assign a complex to.\n\n7. Basing the benchmark of the output of the docking models e.g. by filtering out complexes with non-consensus poses and grouping by predicted pocket, will not allow fair comparisons for non-structure based methods or methods based on a different docking algorithm. I would recommend the authors remove such filtering steps for the test set to avoid the applicability of the benchmark being limited.\n\n8. Line 326 the authors say that the dataset is unbiased. Could they specify clearly what they mean by this term? Once again the claim does not seem fully justified.\n\n9. In paragraph 3.3, the authors discuss the value of dividing the prediction for different assay types. To motivate this they apply statistical significance test to the mean of the distribution of different assay types. However, it is unclear to me why this would be a good motivation for that. Differences in averages might just mean that these assays were applied to different types of proteins, while what the authors would need to show to motivate this point is that if they were applied to the same complex the results would be different depending on the assay. This work was actually performed by a recent paper [1] which however showed no very clear outcomes when it comes to the differentiation between different assay types. An alternative way to try to demonstrate this point would be to put all datapoints in the same \u201ctask\u201d and then train and evaluate a single task model.\n\n10. I believe that saying that \u201cIn the traditional approach\u201d or \u201cconventional approach\u201d people would look for pearson across different targets is misleading. This has been indeed unfortunately done recently in a number of ML papers, however, more established benchmarks in the field such as FEP+ do look at correlations computed per target.\n\nI would raise my score if a convincing response or revision were made on these points.\n\n[1] Combining IC50 or Ki Values from Different Sources Is a Source of Significant Noise. Landrum, Riniker."
            },
            "questions": {
                "value": "1. Figure 3: how is success ratio defined?\n\n2. What type of assay does the test set contain? I assume it does not contain measurements across all different assay types. In this case how are spearman and person correlations computed for al assay types in the test set?\n\n3. How will the dataset be released? What format and what license?\n\n4. Is Pearson computed also on inactives? What proportion of the test and training set are inactive?\n\n5. Do I understand correctly that the test set only has 10 targets? \n\n6. In line 365 I assume the assay type is also used for the grouping, could the authors confirm?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors present a new dataset for bioactivity prediction together with a more fair metric for evaluation of ML models for bioactivity prediction. The authors present valuable insights regarding the weaknesses of current state of the art which convincingly motivates their work. The new carefully curated dataset together with the new metric tackles the identified weaknesses of current SOTA."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Identification of the issues with current SOTA: overfitting to protein pockets due to small diversity of current datasets in terms of containing different small molecules docked to a single protein pocket; issues in reporting the overall correlation instead of computing the partial correlations per target and pooling the partial correlations which can help hide the problem of overfitting to protein pockets\n\n2. Introduction of a new large dataset for bioactivity prediction\n\n3. Careful curation of the dataset using a custom method of majority vore over different docking tools used for generation of the docked pose. Separation by different essay types is also useful."
            },
            "weaknesses": {
                "value": "1. There are few smaller unclarities in the text:\n- On line 174, I did not understand what it is meant by \"anti-logged\"\n- I did not find what method was used for generation of the small molecule conformers, it would be good to clarify this near the statement \"Initial 3D conformations for the small molecules were generated prior to docking.\"\n- Figure 4 is discussed in the text, but the caption could use more detailed description. Just by looking at the figure it is not clear what we are looking at.\n- The statement \u201cWe conducted non-homology analyses at two levels, 0.6 and 0.9,\u201d is not clear. Did the authors check for sequence identity less than 60/90 pct. between test and train set? Or the complement of 40/10 pct.? Please clarify.\n\n2. No mention of the Papyrus dataset ( https://jcheminf.biomedcentral.com/articles/10.1186/s13321-022-00672-x ). It should be discussed in related work and authors should compare SIU to Papyrus in all relevant aspects, such as size, data origin and level of curation.\n\n3. Detailed, practical description of the dataset, which would facilitate its usage by developers of ML models is missing. I suggest putting to supplementary material more detailed and practical description of what is provided in the dataset. PDB files containing the structure of the protein-ligand complexes as docked by some of the docking software? And the labels are just a single scalar value per datapoint? Please explain in detail."
            },
            "questions": {
                "value": "1. The presented majority vote system over the 3 docking system is interesting even from the point of view of docking. What percentage of poses passed this filter? Did you notice some 2 of the 3 tools to tend to agree more with each other? It would be interesting to benchmark this method against the individual tools on some docking dataset (e.g. PDBBind) - for example take the performance of the 3 individual tools evaluated on PDBBind and compare it to the performance of the best of the 3 tools evaluated just over the datapoints which passed the majority vote filter. This benchmarking is not a strict requirement for the rebuttal period from me, but I would be really interested to see it.\n\n2. Could you please describe in more detail how were the models trained for the bioactivity prediction? E.g. the docking model UniMol is described in SuppMat to have an MLP on top of it. What is the input and what is the output of such model. Is the MLP trained to predict a single scalar value for a protein-ligand pair? Please explain."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}