{
    "id": "fvUVe2gJh0",
    "title": "What Matters for Model Merging at Scale?",
    "abstract": "Model merging aims to combine multiple expert models into a more capable single model, offering benefits such as reduced storage and serving costs, improved generalization, and support for decentralized model development. Despite its promise, previous studies have primarily focused on merging a few small models. This leaves many unanswered questions about the effect of scaling model size and how it interplays with other key factors\u2014like the base model quality and number of expert models\u2014, to affect the merged model\u2019s performance. This work systematically evaluates the utility of model merging at scale, examining the impact of these different factors. We experiment with merging fully fine-tuned models using four popular merging methods\u2014Averaging, Task Arithmetic, Dare-TIES, and TIES-Merging\u2014across model sizes ranging from 1B to 64B parameters and merging up to 8 different expert models. We evaluate the merged models on both held-in tasks, i.e., the expert\u2019s training tasks, and zero-shot generalization to unseen held-out tasks. Our wide range of experiments provide several new insights about model merging at scale and the interplay between different factors. First, we find that merging is more effective when experts are created from strong base models, i.e., models with good zero-shot performance, compared to pre-trained ones. Second, larger models facilitate easier merging. Third merging consistently improves generalization capabilities. Notably, when merging eight large expert models, the merged models often generalize better compared to the multitask trained models. Fourth, we can better merge more expert models when working with larger models. Fifth, different merging methods behave very similarly at larger scales. Overall, our findings shed light on some interesting properties of model merging while also highlighting some limitations. We hope that this study will serve as a reference point on large-scale merging for upcoming research.",
    "keywords": [
        "model merging",
        "weight averaging",
        "averaging",
        "composition",
        "modular model",
        "generalization"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=fvUVe2gJh0",
    "pdf_link": "https://openreview.net/pdf?id=fvUVe2gJh0",
    "comments": [
        {
            "summary": {
                "value": "This paper systematically analyzes the impact of different model sizes, base model quality, model merging methods, and the number of expert models on the effectiveness of model merging, and draws five key conclusions. In general, this paper is of certain significance to the model merging community."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This paper systematically reveals the impact of different model sizes, quality, quantity, and merging methods on the effectiveness of model merging.\n- The figures and tables in this paper are very clear.\n- This paper is well organized and clearly written."
            },
            "weaknesses": {
                "value": "- Some inconsistencies lack explanation:\n    - (1) In Figure 1, why is multi-tasking better than single-tasking in 8B and 24B, but multi-tasking is not better than single-tasking in 1B and 64B? How does this relate to model size?\n    - (2) In Figure 5 (PaLM-2-24B, PaLM-2-64B), why is the generalization performance when the number of experts is 8 not as good as when the number of experts is 6? Why does the TIES method perform worse than the pre-trained model when the number of experts increases in PaLM-2-24B?\n    - (3) In Figure 6, under PaLM-2-Held-Out, 64B is significantly better than 24B. Why is 64B not as good as 24B under PaLM-2-IT-Held-Out.\n    - (4) In Figure 7, why is the performance of merging 8 experts better than merging 4 and 6 experts under the Held-In-64B setting? The greater the number of tasks, shouldn't task conflicts be more serious?\n- There is a lack of outlook or suggestions for future directions based on the phenomena observed in this paper.\n- Lack of source code and checkpoints. As this is an evaluation paper, the author can consider open-sourcing the resources used in the paper to facilitate further reproduction and research by the model merging community.\n- The author can consider adding discussions of the following related work.\n    - Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities. Arxiv, 2024.\n    - Fine-tuning large language models for domain adaptation: Exploration of training strategies, scaling, model merging and synergistic capabilities. Arxiv, 2024.\n    - FusionBench: A Comprehensive Benchmark of Deep Model Fusion. Arxiv, 2024.\n    - Arcee's MergeKit: A Toolkit for Merging Large Language Models. Arxiv, 2024.\n- Some minor errors:\n    - References are repeated, \"Language models are super mario\" appears twice, \"Extend model merging from fine-tuned\" appears three times, and \"Model ratatouille\" appears twice. The author needs to check carefully whether other references are repeated.\n    - Reference year error: Ties-merging was published in NeurIPS 2023 instead of 2024. Similarly, \"Task arithmetic in the tangent Space\" was also published in NeurIPS 2023 instead of 2024. The author needs to check the year of other references."
            },
            "questions": {
                "value": "Please refer to the Weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This is an empirical study paper that answers questions for model merging. The author explores research questions about model merging. \n\n- What is the effect of using pretrained vs. instruction-tuned base models for creating expert models for merging?\nThe instruction-tuned base models outperform the pretrained models in the merging process.\n- Does model merging become easier or harder as the model size increases?\nLarger models consistently showed better performance in merging, indicating that they are easier to merge effectively.\n- How does merging affect zero-shot generalization to held-out (not seen) tasks, and how is this influenced by model size?\nThose based on larger and instruction-tuned base models, has improved zero-shot generalization ability. Sometimes surpassing multitask baselines.\n- How many expert models can be merged without performance loss, and how does this depend on model size?\nLarger models could effectively merge more expert models without significant performance degradation, whereas smaller models experienced performance drops when merging more experts."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This is a comprehensive evaluation, systematically examines multiple factors (model size, base model quality, number of experts, and merging methods) across a large-scale experimental setup, providing robust insights."
            },
            "weaknesses": {
                "value": "- It seems when comparing merging pretrained \"experts\" and finetuned \"experts\", after the merging process, the pretrained one is never finetuned. I think it might be unfair to compare between a never finetuned checkpoints and a finetuned checkpoints (althrough it is a merged checkpoint). And thus, it is very natural to predict that merging finetuned \"experts\" is better than merging pretrained \"experts\".\n- All the tasks (held-in and held-out) are text based. It would be better if involving some vision based tasks.\n- The smallest model is 1B. It is small for text models but probably still fairly large for vision models.\n- If adding vision tasks, it would be great to check both vision transformer based models and resnet based models.\n- Besides, I am also wondering if the way of training the \"expert\" matters. e.g. Zero-shot contrastive loss for classification vs supervised learning for classification."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper focused on the model merging issue, exploring various dimensions including model sizes (1B-64B parameters), merging techniques (Averaging, Task Arithmetic, TIES-Merging, Dare-TIES), and the number of expert models (up to 8). The evaluation covers both the tasks the experts were trained on (held-in tasks) and unseen tasks (zero-shot generalization).\n  Different with previous studies for small model scales, the study extended the exploration to the effect of scaling model size, as well as the base model quality and number of expert models.\n  Key findings include: (1) Merging is more effective when using strong instruction - tuned base models compared to pretrained ones. (2) Larger models are easier to merge. (3) Merging improves generalization capabilities, and with strong base models and increasing numbers of merged experts, the merged model can outperform multitask trained models. (4) Larger models can merge more expert models effectively."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The objective of this study is to offer profound insights regarding the scalability aspect of model merging, which indeed represents a significant direction within the realm of \"scaling\".\n2. The research presented herein exhibits a comprehensive and meticulous experimental design, which encompasses multiple dimensions such as model sizes, merging methods, and the count of experts. The results are presented in a highly satisfactory manner. Through a sequence of well-conducted experiments, it has been clearly demonstrated that the merged model can effectively harness the diverse expert knowledge. This beneficial effect becomes more pronounced with the increase in model size and when instruction-tuned base models are utilized.\n3. The paper is generally well-written and easy to follow."
            },
            "weaknesses": {
                "value": "1. The fact that the study's exclusive concentration lies on PaLM-based models does give rise to legitimate concerns regarding the generalizability of the findings to other architectural frameworks such as GPT, LLaMA, and Qwen.\n2. Incomplete theoretical exploration: The paper is heavily empirical, lacking necessary theoretical analysis to explain the observed phenomena. For example\uff0cthe relationship between weight disentanglement and merging effectiveness.\n3. Constraints in Experimental Design:The experimental design of the paper is primarily focused on a narrow range of model sizes (1B to 64B parameters) and a limited number of expert models (up to 8)."
            },
            "questions": {
                "value": "1. What are the theoretical and practical implications that arise when the merging process extends beyond involving 8 experts? Additionally, does there exist an anticipated performance ceiling in such a context? \n2. It is of great significance to explore the following aspects: Firstly, how could the findings obtained from the current study be extrapolated and applied to other model architectures? Secondly, which specific architectural features might exert an impact on the performance of the merging process? \n3. In Section 4.3, a pertinent query arises regarding the strength of the multitask baseline. Specifically, one might question whether the multitask baseline is overly potent. For example, in the context of 6-expert merging, should the baseline be trained solely on those specific 6 tasks rather than on a combination of all 8 tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}