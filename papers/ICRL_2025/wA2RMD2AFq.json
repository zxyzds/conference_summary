{
    "id": "wA2RMD2AFq",
    "title": "Efficient Low-Bit Quantization with Adaptive Scales for Multi-Task Co-Training",
    "abstract": "Co-training can achieve parameter-efficient multi-task models but remains unexplored for quantization-aware training. Our investigation shows that directly introducing co-training into existing quantization-aware training (QAT) methods results in significant performance degradation. Our experimental study identifies that the primary issue with existing QAT methods stems from the inadequate activation quantization scales for the co-training framework. To address this issue, we propose Task-Specific Scales Quantization for Multi-Task Co-Training (TSQ-MTC) to tackle mismatched quantization scales. Specifically, a task-specific learnable multi-scale activation quantizer (TLMAQ) is incorporated to enrich the representational ability of shared features for different tasks. Additionally, we find that in the deeper layers of the Transformer model, the quantized network suffers from information distortion within the attention quantizer. A structure-based layer-by-layer distillation (SLLD) is then introduced to ensure that the quantized features effectively preserve the information from their full-precision counterparts. Our extensive experiments in two co-training scenarios demonstrate the effectiveness and versatility of TSQ-MTC. In particular, we successfully achieve a 4-bit quantized low-level visual foundation model based on IPT, which attains a PSNR comparable to the full-precision model while offering a $7.99\\times$ compression ratio in the $\\times4$ super-resolution task on the Set5 benchmark.",
    "keywords": [
        "Low-Bit Quantization",
        "Multi-Task Learning",
        "Co-Training",
        "Quantization-Aware Training",
        "Quantization Scale"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wA2RMD2AFq",
    "pdf_link": "https://openreview.net/pdf?id=wA2RMD2AFq",
    "comments": [
        {
            "summary": {
                "value": "The study finds that directly applying co-training to existing QAT methods significantly degrades performance. The main issue identified is the inadequacy of activation quantization scales in the co-training framework. To address this, the authors propose a Task-Specific Scales Quantization method suitable for multi-task co-training."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This work effectively incorporates quantization-aware training into co-training and significantly reduces the performance gap between multi-task co-trained models and their 4-bit quantized counterparts.\n2. The authors design task-specific learnable multi-scale activation quantizer and SLLD to solve the issues of naive integration.\n3. From the experimental results, it appears that the author's techniques are effective."
            },
            "weaknesses": {
                "value": "1. In table 1, quantitative results for super-resolution tasks are shown. But I am still curious about the results of deraining and denoising tasks.\n2. How about the parameters change of your method?\n3. Can you provide the some comparisons with some SOTA single task methods to further demonstrate the superiority of your method?"
            },
            "questions": {
                "value": "See weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Task-Specific Scales Quantization for Multi-Task Co-Training (TSQ-MTC) to address the performance degradation issue of existing quantization-aware training (QAT) methods when integrated with co-training. The proposed method introduces a task-specific learnable multi-scale activation quantizer (TLMAQ) to enrich the representational ability of shared features across different tasks and a structure-based layer-by-layer distillation (SLLD) to ensure that the quantized features effectively preserve the information from their full-precision counterparts. Extensive experiments on two co-training data scenarios demonstrate the effectiveness of TSQ-MTC, which achieves a 4-bit quantized low-level visual foundation model based on IPT with a PSNR comparable to the full-precision model and a 7.99\u00d7 compression ratio in the \u00d74 super-resolution task on the Set5 benchmark."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The authors provide a comprehensive evaluation of the challenges of the task, i.e., directly integrating multi-task co-training with QAT. This helps clarify the bottleneck of existing QAT methods and motivates the proposed TSQ-MTC method to address the performance degradation issue.\n\n2. The proposed TSQ-MTC method introduces two novel components, TLMAQ and SLLD, to enhance the representational ability of shared features across different tasks and preserve the information from full-precision features in deeper layers of the Transformer model. \n\n3. The experimental evaluation across the main text and appendices provides detailed insights into the effectiveness and versatility of the proposed TSQ-MTC method in two co-training data scenarios."
            },
            "weaknesses": {
                "value": "1. This paper referred the challenge of computational and memory overhead of co-trained models (Lines 047-049), but the computational complexity and efficiency of the proposed TSQ-MTC method are not discussed in detail. \n\n2. The authors detailedly analysis the proposed SLLD in Table 4 and Figure B. However, according to the ablation studies provided in Table 3, Table D, Table E, the proposed SLLD has minor improvements in performance given the potential for minor fluctuations in experimental results."
            },
            "questions": {
                "value": "As listed in Weaknesses, there are two questions:\n1. Could the authors provide the computational cost of training and inference compared to existing QAT methods?\n2. Could the authors report the variance of the results of the proposed SLLD method in the ablation studies?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel method for multi-task network quantization-aware training, addressing performance degradation caused by model quantization. The approach incorporates two techniques: (1) Task-Specific Learnable Multi-Scale Activation Quantizer (TLMAQ): Addressing the scale conflict in quantizing diverse task features, improving the quantized model's representational capabilities across tasks. (2) Structure-based Layer-by-Layer Distillation (SLLD): Strengthening full-precision model supervision over quantized models, reducing information distortion from quantization."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "(1) The proposed method effectively addresses the scale discrepancies among multi-task features, aligning with intuitive expectations.\n(2) The novelty is good, the introduction of task-aware quantization into multi-task learning represents an innovative approach.\n(3) Method is architecture-agnostic, featuring a wide range of applicability.\n(4) originality, quality, clarity, and significance: This paper demonstrates good originality, is well-written, and clearly communicates ideas. Furthermore, the application of quantization to enhance model efficiency holds significant importance for multi-task learning"
            },
            "weaknesses": {
                "value": "(1) There is a minor error in Section 2.1 - regarding the order of description for the \"single-modal task-related data scenario\" and the \"multi-modal data scenario\" are misaligned with the order of \"former scenario\" and \"latter scenario\". \n(2) The paper only considered the low-level multi-task scenarios; however, However, it lacks effective exploration of high-level visual tasks. Specifically, Can the proposed method still maintain effectiveness when processing high-level visual multitasking data such as NYUD-v2, Pascal Context and Cityscapes.\n(3) The experiment involved a comparison of model quantization techniques, including LSQ (2019) and PAMS (2020). Nevertheless, a comparison with the most recent Quantization-Aware Training (QAT) methods is conspicuously absent."
            },
            "questions": {
                "value": "Apart from the points (2) and (3) outlined in the \"Weaknesses\" section, the following are additional concerns:\n(1) While the model design is good and interesting, the model seems a little complex. Would it be possible to provide some comparative analyses regarding the model's speed and complexity?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}