{
    "id": "1TJSnL3ywS",
    "title": "LLM Distillation for Efficient Few-Shot Multiple Choice Question Answering",
    "abstract": "Multiple Choice Question Answering (MCQA) is an important problem with numerous real-world applications, such as medicine, law, and education. The high cost of building MCQA datasets makes few-shot learning pivotal in this domain. While Large Language Models (LLMs) can enable few-shot learning, their direct application in real-world scenarios is often hindered by their high computational cost. To address this challenge, we propose a simple yet effective approach that uses LLMs for data generation and scoring. Our approach utilizes LLMs to create MCQA data which contains questions and choices, and to assign probability scores to the generated choices. We then use the generated data and LLM-assigned scores to finetune a smaller and more efficient encoder-only model, DeBERTa-v3-base by leveraging distillation loss. Extensive experiments on the Massive Multitask Language Understanding (MMLU) benchmark demonstrate that our method improves accuracy from 28.9\\% to 39.3\\%, representing a gain of over 10\\% compared to a baseline finetuned directly on 5-shot examples. This shows the effectiveness of LLM-driven data generation and knowledge distillation for few-shot MCQA.",
    "keywords": [
        "Few-shot learning",
        "Multiple Choice Question Answering (MCQA)",
        "Data generation",
        "Knowledge distillation",
        "Multiple Choice Question Answering (MCQA)"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "uses a large language model for few-shot multiple-choice question answering by generating synthetic training data and distilling knowledge into a smaller model, significantly boosting its performance.",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=1TJSnL3ywS",
    "pdf_link": "https://openreview.net/pdf?id=1TJSnL3ywS",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes the method of distillation of the large language model to the smaller one for efficient solving of Multiple Choice Question Answering task,  via data generation and distillation loss. Two methods of data generation are considered: generate the whole question-answer structure with answer options in json format (via 5-shot prompting); or generate question-answer pairs obtaining each option separately. Then, the smaller model is trained on the generated data by distillation loss, learning to predict the larger model's probability of the generated options. The evaluation is done on MMLU benchmark. For the ablation study, ARC dataset is used"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is clearly written, and presents a practical method of LLM distillation to a smaller encoder-only model\n- A nice ablation study is provided. There are several interesting observations, e.g. increasing the performance with the distill loss + temperature adjustment, or the usage of the format correctness as an implicit sign of the model's confidence."
            },
            "weaknesses": {
                "value": "- The method is rather straightforward and does not contain a significant novelty, although the presented analysis is good\n- The practical usefullness of the considered task is not so clear. Indeed, Multiple Choice Question Answering is the specific QA format convenient for LLM's evaluation, but the MCQA results are not necessarily directly connected to the general QA abilities of the model. For encoder-only LLMs, classification-based approach looks more appropriate (i.e. scoring the correctness of the QA pair)\n- The model does not outperform Tasksource model which is obtained by the multi-task training of the same backbone: the improvement on MMLU is marginal (+0.5), and on ARC data the proposed approach works significantly worse."
            },
            "questions": {
                "value": "Q1. Do you have any idea, if the huge improvement of the performance of DeBerta after distillation is related to improving of the model's question answering ability, or just due to  learning of Multiple Choice QA format?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to enhance the performance of low-computation-cost, only-encode models for few-shot multiple-choice question answering (MCQA) tasks. It leverages large language models (LLMs) to generate a high-quality, task-specific MCQA dataset for training and introduces a training approach that applies distillation loss based on LLM-assigned scores. Experimental results demonstrate the effectiveness of the proposed method: LLM-driven data generation and knowledge distillation for few-shot MCQA."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The method is relatively simple and clearly explained.\n2. The paper explores the effectiveness of using LLMs to construct data for MCQA tasks, and the proposed distillation loss training method shows notable performance improvements.\n3. The paper conducted a relatively comprehensive ablation experiment."
            },
            "weaknesses": {
                "value": "1. The method's performance improvement is limited and depends on the strength of the base model. While the gains are more pronounced with the weaker DeBERTa-base model, they are minimal with the stronger Tasksource model, and even slightly decreases in the case of Decompose. \n2. Additionally, when using DeBERTa-base, the best performance (JSON distill) achieved by using only the constructed dataset does not surpass that of a multi-task fine-tuned model (Tasksource)."
            },
            "questions": {
                "value": "The experiments lack a more detailed analysis of the two data generation methods (e.g., example-based analysis): Why do the two methods (JSON and Decompose) lead to different outcomes in performance? Why does JSON outperform Decompose?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the possibility of encoder model with LLM-generated dataset and knowledge distillation. To address current effortful MCQA benchmark making, this paper utilizes LLM\u2019s ability in few shot prompting with two formatting strategies. Then, by distilling the loss of bigger LLM into small, encoder-only model, the paper shows the efficient way to achieve performance nearing that of bigger LLM."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "* This paper sheds light again on the encoder-only model, which had been receiving less attention recently.\n* The methodology's adaptability to existing domain-specific benchmarks suggests its potential for broad application across diverse fields."
            },
            "weaknesses": {
                "value": "**[Novelty is limited]**\n\nThe paper\u2019s novelty appears limited, as it does not introduce a new dataset and relies primarily on formatting prompts either in full JSON format or as segmented parts, raising questions on whether these methods constitute a genuinely novel approach. Furthermore, the distillation technique applied here does not seem particularly innovative, as it essentially reduces to a form of fine-tuning.\n\n**[Using Encoder-only Models - limited Experimental setups]**\n\nAdditionally, while the paper suggests the encoder-only model\u2019s powerful capabilities, this claim is primarily based on improvements from distillation and model size reduction. These factors alone may not suffice to substantiate the model\u2019s claimed \"power\" without more substantial baseline comparisons, particularly in tasks beyond fine-tuning.\n\n**[Inadequate analysis of suggested method]**\n\nThere is inadequate validation of the quality of the LLM-generated dataset, which raises further concerns about the reliability and applicability of the findings."
            },
            "questions": {
                "value": "* Has there been any comparison with recently released lightweight models, such as those with a 1B parameter size?\n* Is there a specific reason why only the DeBERTa encoder model was tested?\n* Was there a particular reason for employing few-shot learning in an encoder model instead of using a masked language model (MLM)?\n* Does this paper really aligns to the ICLR conference is questionable. Any other natural language processing conference seems more suitable."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel approach to address few-shot multiple choice question answering (MCQA) by leveraging large language models (LLMs) for data generation and knowledge distillation into a smaller, efficient encoder-only model, DeBERTa-v3-base. The study addresses the computational challenges associated with using LLMs directly in real-world applications and provides a three-step framework involving synthetic data generation, LLM-based scoring, and distillation training. Experimental results demonstrate significant improvements in accuracy over baseline models on the Massive Multitask Language Understanding (MMLU) benchmark, as well as competitive performance compared to larger models like LLaMA-7B and Flan-T5-250M. The paper also includes ablation studies on various generation methods, scoring techniques, and hyperparameters."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The approach addresses a relevant problem in natural language processing, providing a practical solution for scenarios where computational resources are limited.\n2. The framework is straightforward, and the two methods of data generation (JSON and decomposed) are described in detail, with thoughtful consideration of their benefits and limitations.\n3. The paper presents extensive experiments, including performance comparisons, ablation studies, and evaluations on the MMLU benchmark."
            },
            "weaknesses": {
                "value": "1. The method relies heavily on the availability of robust LLMs, which may not be readily accessible in languages other than English or for certain domain-specific tasks.\n2. The decomposed generation method, while reducing parsing errors, often results in noisy data due to longer and less structured answers."
            },
            "questions": {
                "value": "Please refer to the weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors use a few task-specific multiple choice questions as seed examples to get a LLM to generate task-specific, synthetic multiple choice data. They explore two ways of prompting the LLM to generate this data. They train a small, encoder-only model via knowledge distillation using soft labels assigned by the LLM. They show that training on synthetic data via distillation is better than just training on a few non-synthetic task-specific data points directly, and also compare to some other models. The authors also conduct ablation studies regarding the amount of synthetic data, synthetic data generation temperature, and choice of LLM."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "(Originality) While synthetic data generation with LLMs and knowledge distillation into transformer based models are both widely used and studied, the authors consider the specific setting of MCQA and distilling a decoder-only model into an encoder-only model, which is a new setting.\n\n(Quality) The authors report results across several random seeds. They also do some nice ablation studies. The limitations section was also of high quality.\n\n(Clarity) The paper was generally clear and easy to follow.\n\n(Significance) As mentioned in originality, this paper explores a setting that is slightly different from past work."
            },
            "weaknesses": {
                "value": "* A couple typos (these didn\u2019t affect my review at all, but just mentioning them)\n  * Line 69/70 missing a space\n  * Line 179 \u201ca scalar values\u201d -> \u201cscalar values\u201d\n* When generating synthetic data, how can you be sure you\u2019re not generating questions that are in the MMLU/ARC test sets (or that are quite close?). It would be nice to see something like nearest neighbors of generated questions, or something like overlap of answer options with answer option sets from the test sets.\n* A note on the tasksource+decompose/JSON is I don\u2019t think it can necessarily be concluded that tasksource+JSON is better than tasksource as 0.5 is quite a narrow margin.\n* In my mind the main weakness of this paper would be lack of significance.\n  * In practice, in the resource constrained setting there are already compelling alternatives to the approach described in this paper. For example, tasksource has the same amount of parameters, comparable performance, and faster inference as it only needs one forward pass. It also doesn't require synthetic data generation for each task. Furthermore, performance is less good than that of e.g., Gemma-2-2b-it and similar models which can be run quite cheaply on even a laptop (especially after quantization). I don\u2019t see when \u201cdistillation into DeBERTa\u201d would be used in practice because there are already very compelling alternatives. I'd be happy to hear the authors' take on this, though.\n  * A paper definitely doesn't need to be the best \"in practice\" option to be useful, as it might provide surprising/intuitive insights compared to previous work. However, I don\u2019t find the results of this paper particularly surprising in light of past work. Knowledge distillation is already widely used with LMs, and distillation from larger LLMs to smaller LLMs is done all the time with good results. Synthetic data generation with LLMs is also frequently done, and has been shown to work well. That more synthetic data works better is as expected.\n\n*Strengths & Weaknesses tl;dr*: I think the authors\u2019 study is well thought out and put together, and mostly easy to follow. However, I don\u2019t think it provides substantial insight/methods beyond what already seems to be common knowledge in the research community. I\u2019ve assigned a rating of 3, but I\u2019d choose 4 if it were an option because I think the paper is overall well made but just doesn\u2019t have the level of impact I typically associate with ICLR papers."
            },
            "questions": {
                "value": "* On line 245, why is it \u201capproximately 4000 MCQA examples\u201d? Shouldn\u2019t this be exact?\n* Why was number of negative examples set to 5 when MMLU and ARC only have 3?\n* What temperature was used for the MMLU experiments?\n* In Section 3.2, how is a sequence is being transformed into a scalar \u2014 [CLS] token? Pooling?\n* From Section 3.2 it is my understanding that handling a MCQ with n options requires n forward passes. Is that correct?\n* How was inference done for the baseline models?\n* When JSON samples are not properly formatted, are they resampled, or are less than 1024 samples used?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}