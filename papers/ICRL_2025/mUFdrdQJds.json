{
    "id": "mUFdrdQJds",
    "title": "Hybrid MILP to efficiently and accuratly  solve hard DNN verification instances",
    "abstract": "Deep neural networks have demonstrated remarkable capabilities, achieving human-like or even superior performance across a wide range of tasks. However, their robustness is often compromised by their susceptibility to input perturbations. This vulnerability has catalyzed the verification community to develop various methodologies, each presenting a unique balance between completeness and computational efficiency. $\\alpha,\\beta$-CROWN has won the last 4 VNNcomp(etitions), as the DNN verifier with the best \ntrade-off between accuracy vs computational time. VNNcomp however is focusing on relatively easy verification instances (network, inputs (images)), with few {\\em unstable nodes}. In this paper, we consider harder verification instances. On such instances, $\\alpha,\\beta$-CROWN displays a large gap ($20-58$%) between instances that can be verified, and instances with an explicit attack. Enabling much larger time-outs for $\\alpha,\\beta$-CROWN only improves verification rate by few percents, leaving a large gap of undecided instances while already taking a considerable amount of time. Resorting to other techniques, such as complete verifiers, does not fare better even with very large time-outs: They would theoretically be able to close the gap, but with an untractable runtime on all but small {\\em hard} instances.\n\nIn this paper, we study the MILP encoding of ReLU-DNNs, and provide new insights in the LP relaxation. This allows us to carefully craft a {\\em partial MILP} solution which selects automatically few neurons encoded as integer variables, the rest using the LP relaxation. Compared with previous attempts, we can reduce the number of integer variables by around 4 times while maintaining the same level of accuracy. Implemented in {\\em Hybrid MILP}, calling first $\\alpha,\\beta$-CROWN with a short time-out to solve easier instances, and then partial MILP for those for which $\\alpha,\\beta$-CROWN fails, produces a very accurate yet efficient verifier, reducing tremendously the gap of undecided instances ($8-15$%), while keeping a reasonable runtime ( $46s-417s$ on average per instance ).",
    "keywords": [
        "Safety; Neural Netowrks; Verification; robustness; MILP"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "An efficient verification method to verify Neural Networks, more accurate than the SOTA to certify robustness on hard instances,  based on a novel way to choose few neurons to explore exactly.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=mUFdrdQJds",
    "pdf_link": "https://openreview.net/pdf?id=mUFdrdQJds",
    "comments": [
        {
            "title": {
                "value": "pMILP vs inductive full MILP"
            },
            "comment": {
                "value": "We did another interesting test to highlight why chosing nodes to open is so important.\nIt is not that important for layer 3, full MILP is runnable (although it takes much longer than pMILP for a very negligeable improved accuracy).\n\n\nMore interesting is what happens in layer 5 (last before output), where using full MILP would be a very bad idea, even for the smallest 5x100 DNN.\n\n\nWe run pMILP to compute the bounds for the first 4 layers. We use these bounds to run pMILP vs full MILP on layer 5.\nThe results are as follows:\n\n\n for pMILP:\n\nnbr..time .... acc\n\n1 ..14 .. 3.233021901\n\n2 ..15.3 ..\t3.140309921\n\n3 .. 17.21 ..\t3.059083103\n\n4\t.. 17.4 ..\t2.986166762\n\n6 .. 19.2 ..\t2.856229765\n\n7 .. 20.95 ..\t2.799248232\n\n8 .. 23.7 ..\t2.746167245\n\n9 .. 26.67 .. 2.69485246\n\nNow, if we run full MILP (with the many undecided nodes of image 59), here is what we get by using different time out settings:\n\ntime ... acc\n\n21.1\t... 3.348236261 \n\n27.6\t... 3.24604282\n\n38.2 ...\t3.196640184\n\n47.1\t... 3.164298172\n\n56.69 ... 3.146913614\n\n106.7 ...\t3.108035223\n\n156.32 ...\t2.900438725\n\nThis shows that chosing nodes is much more efficient for time/accuracy trade-off than using full MILP. \nHelping Gurobi with the nodes to consider is extremely effective:\n\nWe obtain <2.9 accuracy in 19seconds, while full MILP needs >150 seconds to obtain it, a 8x speed up.\n\nWe obtain <2.7 accuracy in 27 seconds, while full MILP is likely to spend way longer than 8x27s to reach judging by the trend.\n\nAnd this is for the smallest DNN we considered (500 hidden neurons, far from the biggest 20k neuron DNN we experimented with)"
            }
        },
        {
            "title": {
                "value": "Q2: time and accuracy for different number of nodes open"
            },
            "comment": {
                "value": "> Q2\n\nHere are the results we have on pMILP with different number of open nodes, and time to compute the full 100 nodes of layer 3:\nNotice that the image chosen (59) is particularly hard (because there are lots of uncertain nodes), hence the quite large time to compute the layer.\n\n\nThis is the same test with comparable results as in Table 3, with previous layer L2 computed using full MILP.\n\n nbr          ..            time    .................                                   acc\n\n1\t.. 5.992132187  ...\t1.702986873\n\n2 ..\t9.024885654 ...\t1.654690348\n\n3 ..\t10.6186769 ...\t1.612137282\n\n4 ..\t11.60420394 ...\t1.571001109\n\n5 ..\t12.92842579 ...\t1.531925404\n\n6  ..\t12.98235846 ...\t1.49535638\n\n7 ..\t13.66908836 ...\t1.46189314\n\n8 ..\t14.47656775 ...\t1.429953534\n\n9 ..\t15.1073184 ...\t1.400636414\n\n10 ..\t16.18551326 ...\t1.37112038\n\n11 ..\t17.52154756 ...\t1.343824578\n\n12 ..\t17.83653903 ...\t1.31833565\n\n13 ..\t18.52695084 ...\t1.293869092\n\n14 ..\t19.86539316 ...\t1.2690508\n\n15 ..\t21.07112885 ...\t1.247510672\n\n16 ..\t23.13534665 ...\t1.224306561\n\n17 ..\t 25.1482892 ...\t1.203179112\n\n18 ..\t27.09270978 ...\t1.183947413\n\n19 ..\t29.32548857 ...\t1.164465319\n\n20 ..\t33.14418769 ...\t1.145618187\n\n21 ..\t40.39810514 ...\t1.126125249\n\n22 ..\t45.63475466 ....\t1.108974532\n\n23 ..\t53.25520349 ...\t1.093124202\n\n24 ..\t62.35421562 ...\t1.07730883\n\n25 ..\t101.9047432 ...\t1.060928575\n\n26 ..\t113.4482219 ...\t1.045715429\n\n27 ..\t154.7245564 ...\t1.030605317\n\n28 ..\t196.723217 ...\t1.016058572\n\n29 ..\t228.8618577  ...\t1.00108374\n\n30 ..\t279.520607 ...\t0.987652522\n\n\nwe can clearly see the exponential scaling with the number of open nodes, taking more and more time for each additional open node. On this layer, our standard pMILP prototype will open 21 nodes (cf Table 5). \n\n> If Layer 2 is done with LP instead of full MILP, the accuracy of layer 3 is way worse:\n\nnbr .. time ..............acc with L2 LP ....acc with L2 with fullMILP for reference\n\n5\t.. 9.362099648 ...\t3.247378853 ...\t1.53\n\n10\t.. 10.64601064 ...\t3.022145891 ...\t1.37\n\n15\t.. 11.97100925 ...\t2.823833569 ...\t1.25\n\n20\t.. 13.12460947 ...\t2.638627159 ...\t1.15\n\n25\t.. 16.02304268 ...\t2.47324218 ...\t1.06\n\n30\t.. 28.33429575 ...\t2.327935601 ...\t0.99\n\n35\t.. 48.17112136 ...\t2.195061671\t\n\n40\t.. 89.42136264 ...\t2.071075926\t\n\n\nThis shows how important it is to compute quite accurately layer by layer.\nThe exponential complexity in the number of open nodes is also very clear.\ntime per similar number of nodes is better compared with the previous test, but time per similar accuracy is way worse (which is what matters)."
            }
        },
        {
            "title": {
                "value": "Utility Function vs Branching Heuristics."
            },
            "comment": {
                "value": "> Major Concern: Utility functions, in principle, appear similar to branching heuristics (e.g., BaBSR).\n\n\nAfter investigating several papers, we can say the following:\n\nMost (if not all) heuristics in DNN verification consider how each (uncertain) neuron affects a certain node X (e.g. an output neuron X): e.g. BaBSR, Huang et al. 2020, etc.\n\nOur Utility function is no different in that respect: it ranks neurons by how important they are to a given node X, to only consider the most important neurons accurately.\n\nSimilarities stop there with BaBSR and previous heuristics.\n\nNow, the key difference, and the reason why our Utility is so efficient (it can even make the usually very inefficient MILP solvers more efficient than the otherwise state of the art BaB solvers. Compared to Huang et al. 2020, it needs to open 4 times less neurons for the same accuracy) is the following.\n\nIt uses the solution to an LP call of a min/max query on node X (as computed by Gurobi) (line 312). As it is LP, the solution computed is also the bound in the dual space. The cost of this LP call is negligeable wrt the forthcoming call to pMILP, and thus the time taken here is certainly worth.\n\nNow, we use this solution, which associates each variable/neuron with a specific value, to have a much more accurate understanding of how each variable/neuron affects X. That\u2019s where Proposition 1 is crucial, in telling us how much accuracy is gained by opening that neuron vs LP relaxation, and we can rank them extremely accurately (Table 3 is extremely clear in that respect). Proposition 2 is interesting, but is far from capturing how close UTILITY is to IMPROVEMENT (in our test, we were 99% close- of course this cannot be proven in general as we could manually generate degenerated cases where the distance is much larger).\n\nCompared to e.g. Huang et al 2020, it allows to have a very precise answer *locally* for that input, while Huang et al. considers mostly the DNN, and less the input. This is also what happens with BaBSR, although comparing pMILP and BaB cannot be made very directly.\n\nTo the best of our knowledge, this is the first time such a solution (through an LP call) is used to refine the contribution of each neuron to a given node X."
            }
        },
        {
            "title": {
                "value": "Utility Function vs branching heuristics"
            },
            "comment": {
                "value": "> Utility functions, in principle, appear similar to branching heuristics (e.g., BaBSR).\n\n\nAfter investigating several papers, we can say the following:\n\nMost (if not all) heuristics in DNN verification consider how each (uncertain) neuron affects a certain node X (e.g. an output neuron X): e.g. BaBSR, Huang et al. 2020, etc.\n\nOur Utility function is no different in that respect: it ranks neurons by how important they are to a given node X, to only consider the most important neurons accurately.\n\nSimilarities stop there with BaBSR and previous heuristics.\n\nNow, the key difference, and the reason why our Utility is so efficient (it can even make the usually very inefficient MILP solvers more efficient than the otherwise state of the art BaB solvers. Compared to Huang et al. 2020, it needs to open 4 times less neurons for the same accuracy) is the following.\n\nIt uses the solution to an LP call of a min/max query on node X (as computed by Gurobi) (line 312). As it is LP, the solution computed is also the bound in the dual space. The cost of this LP call is negligeable wrt the forthcoming call to pMILP, and thus the time taken here is certainly worth.\n\nNow, we use this solution, which associates each variable/neuron with a specific value, to have a much more accurate understanding of how each variable/neuron affects X. That\u2019s where Proposition 1 is crucial, in telling us how much accuracy is gained by opening that neuron vs LP relaxation, and we can rank them extremely accurately (Table 3 is extremely clear in that respect). Proposition 2 is interesting, but is far from capturing how close UTILITY is to IMPROVEMENT (in our test, we were 99% close- of course this cannot be proven in general as we could manually generate degenerated cases where the distance is much larger).\n\nCompared to e.g. Huang et al 2020, it allows to have a very precise answer *locally* for that input, while Huang et al. considers mostly the DNN, and less the input. This is also what happens with BaBSR, although comparing pMILP and BaB cannot be made very directly.\n\nTo the best of our knowledge, this is the first time such a solution (through an LP call) is used to refine the contribution of each neuron to a given node X."
            }
        },
        {
            "title": {
                "value": "Proposition 1"
            },
            "comment": {
                "value": "> Major Concern: Proposition 1 is established in prior works.\n\nWe are looking at each of the 4 papers mentioned by the reviewers on the issue.\n\n[A] \u201cFormal Verification of Piece-Wise Linear Feed-Forward Neural Networks\u201d, ATVA, 2017.\n\n[B] \u201cInput-Relational Verification of Deep Neural Networks\u201d, PLDI, 2024.\n\n[C] Singh et al. \"An abstract domain for certifying neural networks\" Equation 2.\n\n[D] Salman, et al. \"A convex relaxation barrier to tight robustness verification of neural networks. 2019\". Theorem 4.2\n\nWe did not find a clear and explicit statement which would be equivalent with our Proposition 1. For papers [B] and [D], we are exploring further to see if their results (specifically Theorem 4.2) would imply Proposition 1 or if they would suggest in writing something similar to Proposition 1. For two papers: [A] and [C], the work are clearly orthogonal to our statement.\n\nWe believe there might be a misunderstanding in what our statement claims, which would explain why [A] and [C] at least are suggested as establishing the results by the reviewers.\n\nTo rephrase in English, Proposition 1 claims that the LP relaxation (of the standard exact MILP encoding of ReLU) is EQUIVALENT with the triangle approximation. And [A;C] are simply NOT dealing with LP relaxation!\n\nTo say differently, Proposition 1 does not claim the triangle approximation to be novel \u2013 it certainly isn\u2019t ([A],[B],[C],[D] are non exhaustive examples of that). But the EQUIVALENCE may be new, and at least it is not that obvious or well-known. That being said, it might already be proved in another paper ([D] or somewhere else). The proof is not that complicated, but this is certainly very important and instrumental in the way our Utility function is so much more efficient than what was proposed before (more on the utility function later). If Proposition 1 is already proved somewhere else, then it does not impact our paper much.\n\nWe certainly agree that the idea of a triangle approximation appeared countless time before (in these 4 papers specifically, but in numerous other as well). We actually note such a fact immediately after Proposition 1, line 296 \u201cNotice that this is close to the DeepPoly abstractions Singh et al. (2019b)\u2026\u201d"
            }
        },
        {
            "title": {
                "value": "Proposition 1"
            },
            "comment": {
                "value": ">\tProposition 1 is established in prior works.\n\nWe are looking at each of the 4 papers mentioned by the reviewers on the issue.\n\n[A] \u201cFormal Verification of Piece-Wise Linear Feed-Forward Neural Networks\u201d, ATVA, 2017.\n\n[B] \u201cInput-Relational Verification of Deep Neural Networks\u201d, PLDI, 2024.\n\n[C] Singh et al. \"An abstract domain for certifying neural networks\" Equation 2.\n\n[D] Salman, et al. \"A convex relaxation barrier to tight robustness verification of neural networks. 2019\". Theorem 4.2\n\n\nWe did not find a clear and explicit statement which would be equivalent with our Proposition 1. For papers [B] and [D], we are exploring further to see if their results (specifically Theorem 4.2) would imply Proposition 1 or if they would suggest in writing something similar to Proposition 1. For two papers: [A] and [C], the work are clearly orthogonal to our statement.\n\nWe believe there might be a misunderstanding in what our statement claims, which would explain why [A] and [C] at least are suggested as establishing the results by the reviewers.\n\nTo rephrase in English, Proposition 1 claims that the LP relaxation (of the standard exact MILP encoding of ReLU) is EQUIVALENT with the triangle approximation. And [A;C] are simply NOT dealing with LP relaxation! \n\nTo say differently, Proposition 1 does not claim the triangle approximation to be novel \u2013 it certainly isn\u2019t ([A],[B],[C],[D] are non exhaustive examples of that). But the EQUIVALENCE may be new, and at least it is not that obvious or well-known. That being said, it might already be proved in another paper ([D] or somewhere else). The proof is not that complicated, but this is certainly very important and instrumental in the way our Utility function is so much more efficient than what was proposed before (more on the utility function later). If Proposition 1 is already proved somewhere else, then it does not impact our paper much.\n\nWe certainly agree that the idea of a triangle approximation appeared countless time before (in these 4 papers specifically, but in numerous other as well). We actually note such a fact immediately after Proposition 1, line 296 \u201cNotice that this is close to the DeepPoly abstractions Singh et al. (2019b)\u2026\u201d"
            }
        },
        {
            "comment": {
                "value": "First I want to apologize for the misunderstanding on cited paper [1]. The issue was there were several [1] used in your original review (I was not aware of that), and I mistakenly took the wrong one (which also train networks and certifiy them, so it was easy to believe it was the one you were refering to, both at ICLR 2023). Anyway, our point is that there are many networks out there, we cannot test them all, and in real life you cannot necessarily choose the easiest one learnt for a specific task. \n\nWe choose to experiment with standard DNNs already tested with abCROWN such that we can compare and use already mature configuration files.\n\n> the number of MILP calls\n\nThis is 2 (one for lower bound, one for upper bound) per neurons, thus the O(N), for all DNNs per image per epsilon.\nFor Figure 2, we have time to query ~6 different epsilon in 10 000s, in a binary search.\n\n> average and maximum time per MILP call\n\nWe did not keep the statistic you are asking for. \nFor average, it is easy to approximate accurately:\naverage run time per  image x 20 (threadhs in parallel) / [ 2 (min and max) x Number of nodes].\nThis gives: \n\n5x 100: 0.92s\n\n5x 200: 0.71s\n\n9x 100: 0.76s\n\n9x 200: 0.49s\n\n6x 500: 1.34s\n\nCNN-B-Adv: 0.8s\n\nmaximum is a lot higher. In particular for output nodes which have a higher number of open nodes, in particular for CNN-B-Adv. \nWe also rerun pMILP on 1 image of each case:\n\nCIFAR on image 76, hard image as abCROWN  cannot certify with 2000s time out (gap to certification 0.25)\n\nCNN-B-Adv, epsilon = 2/255:  average 1s per MILP calls. \nmax 609s (that is for the hardest output neuron - hence the number of open nodes is large, this is an extreme outlier case).\n\nWe are running another easier image (that abCROWN certifies) for comparison.\n\n\n\nMNIST: For image 3, results are:\n\n6x 500, epsilon=0.035 (representative of average case): average 1.4s per MILPcall, max: 3.5s. \n\n\n6x 500, epsilon=0.1, very close to the falsification epsilon (very hard, not representative of the average case): avg: 22.3s, max: 155s\n\n\n5x 100, epsilon=0.026: avg: 0.41      max: 1.87\n\n5x 200, epsilon=0.015: avg: 0.75       max 5.31\n\n8x 100, epsilon=0.026: avg 0.39\t max: 1.406\n\n8x 200, epsilon=0.015: avg 0.49\tmax: 1.63\n\n\n> hyperparameters.\n\nIf you mean MIP Gap or time out, we have our own ones. MIPGap varies from 0.001 to 0.1 depending on improvement, distance to deciding the neurons, etc.  This is low level details which will be available in the code on a Github after the paper is accepted.\n\n>Why didnt you try ith Refined $\\beta$-CROWN/ERAN for this experiment\n\nRefer to line 467: Last but not least, Refined \u03b2-Crown cannot scale to larger instances (6\u00d7500, CNN-B-Adv), while Hybrid MILP can. \n\nSpecifically, the engine return a 'not implemented' when we tried to run it. Anyway, running >500 nodes with full MILP would be extremely slow (already for the first layer using MILP - next layer wont probably run at all);\n\n For Eran, refer to Table 6 page 14 in appendix, with results worse than abCROWN.\n\n> Specifically, I am interested in the $\\epsilon$ values close to the best falsifiable epsilon, but the network cannot be attacked.\n\nSee above the case of image 3 with epsilon=0.035 and epsilon=0.1 (much harder).\nFigure 2 shows that still Hybrid MILP can certify images quite close to this boundary, much closer than abCROWN."
            }
        },
        {
            "comment": {
                "value": "> In [1], the trained to be certifiable for robustness CIFAR10 ResNet 4B certification results is around 54% with epsilon= 1/255,(table 3 page 7 of https://openreview.net/pdf?id=7GEvPKxjtt).\n\nIt seems there might have been a misunderstanding. I would kindly suggest that the authors review the cited papers more carefully before responding. Specifically, I referred to the paper \"Certified Training: Small Boxes are All You Need\" (https://openreview.net/forum?id=7oFuxtJtUMH, see Table 1). The certified accuracy at $2/255$ is reported as $62.84$%, which is higher than the upper bound of $62.0$% mentioned by the authors. That said, I do agree that there are certain benchmarks (networks) and specific properties (such as $\\epsilon$ values) where using a MILP formulation can indeed help reduce the number of unverified instances. \n\n> Figure 2 page 10\n\nI may have overlooked the runtime details. Could you please point me to or provide specifics on the runtime, such as the number of MILP calls, average and maximum time per MILP call, and the hyperparameter values used? Additionally, could you share the same details for the other networks presented in Table 1? Furthermore, is there a particular reason why the authors did not consider Refined $\\beta$-CROWN/ERAN for this experiment, as it also utilizes an LP/MILP-based approach?\n\n> $\\epsilon$ values\n\n\"It depends on the distance to the frontier between certifiable and falsifiable. asking for epsilon >> best falsifiable epsilon would return very fast a (negative) answer, similarly if epsilon << best certifiable epsilon.\" - I completely agree with this. However, I wanted to see the runtime details (e.g., MILP calls, hyperparameters, etc.) for different $\\epsilon$ values in the range [best certifiable epsilon, best falsifiable epsilon]. Specifically, I am interested in the $\\epsilon$ values close to the best falsifiable epsilon, but the network cannot be attacked."
            }
        },
        {
            "comment": {
                "value": ">Technical concern (major concern), \n\nlet us investigate before providing an answer - we will.\n\nOn other concerns:\n\n>Motivation (minor concern): \"CIFAR-10 show verified accuracies higher than the upper bound in Table 1.\"\n\nEDIT: the following is due to a misunderstanding on the paper [1] (there were several reference [1], in the same ICLR 2023 conference)\n>This is factually incorrect.\n\n>In [1] , the trained to be certifiable for robustness CIFAR10 ResNet 4B certification results is around 54% with epsilon= 1/255,>(table 3 page 7 of https://openreview.net/pdf?id=7GEvPKxjtt).\n\n>The upper bound for CIFAR10 CNN-B-Adv is at 62%, which is higher than 54%, not lower as claimed. \n>Even more importantly, the upper bound is for A MUCH HARDER epsilon=2/255 (perturbation twice as high). \nNotice that with epsilon= 3/255 [1] reaches 0% of certification (epsilon= 2/255 is not reported in  [1], so direct comparison is not that easy).\n\n>Even better, Hybrid MILP actually succeeds to certify 51% certified robustness for CIFAR 10 CNN-B-Adv for the MUCH HARDER epsilon= *2*/255, with perturbations twice as large than epsilon= 1/255. This is a much stronger result than the \n54% of certification with epsilon= 1/255, and this can be cerified thanks to Hybrid MILP.\n\nThis still stands:\nMore generally, in academic world, we can train our own networks, but in the real world, you have to do with the network trained by the industrial partner, and the training pipeline often cannot be changed. This is also what is advocated in Dathathri et al. (2020). As we said, you cannot expect all the networks to be specifically trained to be easy to verify: some will be, some will not be - and we need new methods to tackle them all.\n\nWhat we did is that we experimented on standard benchmarks of the community.\n\n> Pseudo code and complexity: \"The worst-case complexity, is too high even for small networks. This approach is feasible only when the number of unsettled neurons is low.\"\n\nFactually, the experimental results point to the exact opposite direction :) : \n\npMILP efficiently certifies CNN-B-Adv, which has 20, 000 neurons and quite a lot of unsettled neurons (otherwise, abCROWN would have very few undecided images).\n\n\npMILP is computing bounds for 20 neurons in parallel (as indicated line 689, page 13), very similar to PRIMA or Refined bCROWN (16 neurons in parallel), and much less than using >4000 GPU core in standard abCROWN.\n\nSeveral things to consider:\n1) The complexity  allows to understand what is the crucial parameter to keep in track. Here, it is the number K of open nodes. And this is exactly what our main contribution does: identifying few very important nodes. If K is small, then the complexity is manageable efficiently even for large network (complexity polynomial in N).\n\n2) worst-case complexity IS in the worst case. The average case is usually behaving better. We use the Gurobi solver, which has powerful heuristic which avoids the worst case complexity most of the time. That being said, our prototype is much more efficient than any previous verifier primarily using Gurobi, so this sole factor is only a small part of the reason.\n\n>  I strongly recommend that the authors provide a runtime analysis using a higher  values, for example, \n values say  for the MNIST  network (Table 1 row 1), using higher  epsilon values. This is necessary to understand the scalability of the algorithm.\n\nYou should look at figure 2 page 10: we certified some instances with epsilon as high as 0.1, for the complex 6x500 networks.\nObviously, scaling to larger epsilon is not an issue. Actually, runtime does not depend directly on epsilon. It depends on the distance to the frontier between certifiable and falsifiable. asking for epsilon >> best falsifiable epsilon would return very fast a (negative) answer, similarly if epsilon << best certifiable epsilon."
            }
        },
        {
            "comment": {
                "value": "Thanks for the prompt reply. \n\n**Motivation (minor concern)**\nThe main point behind the motivation question was that none of the networks' verified accuracies could match those of a certifiably robust, state-of-the-art (SOTA) model (even with a perfect verifier). For reference, \"in recent papers on certified training [1], networks trained on MNIST and CIFAR-10 show verified accuracies higher than the upper bound in Table 1.\" Therefore, I wanted to understand the practical importance of solving these challenging instances for an ML practitioner, given that there are already better models with higher verified and, in many cases, standard accuracy [1].\n \n**Technical concern (major concern)**\nThe technical concerns have not been addressed. The authors should clearly highlight the key technical contributions, especially considering that the following contributions are already known in the verification community:\n- Proposition 1 is established in prior works.\n- Selectively encoding unsettled ReLU nodes with binary variables improves verification accuracy.\n- Bound tightening of intermediate layer neurons (as shown in Figure 3 [A]) improves verification accuracy.\n- Utility functions, in principle, appear similar to branching heuristics (e.g., BaBSR).\n\n[A] \"Fast and Complete: Enabling Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers\",  ICLR, 2021.\n\n**Pseudo code and complexity** \nThanks for providing the pseudo-code. However, it is still incomplete and does not mention any hyperparameters or whether iterations of the inner loop are executed parallelly.\nThe worst-case complexity, $O(N \\cdot MILP(N, K))$, is too high even for small networks. This approach is feasible only when the number of unsettled neurons is low. I strongly recommend that the authors provide a runtime analysis using a higher \n$\\epsilon$ values, for example, $\\epsilon$ values say $\\epsilon \\in [0.03, 0.035, 0.04, 0.05, ...]$ for the MNIST $5 \\times 100$ network (Table 1 row 1). This is necessary to understand the scalability of the algorithm."
            }
        },
        {
            "comment": {
                "value": ">Q1) The key part of my question is after ab-CROWN. I.e. how does pMILP compare to full MILP (and other utility functions) when run after ab-CROWN using the resulting relatively tight intermediate bounds.\n\nTwo answers: \na) we do not use any intermediate bounds computed from ab-CROWN when running pMILP. This might be doable, but would need a low level integration. In our prototype, ab-CROWN is only used to solve easy instances. Hard instances (that is, instances for which abCROWN fails to answer) are restarted from scratch. This is already very efficient, although it could be made even more efficient by using intermediate abCROWN bounds.\n\nIn this respect, on 6x100, full MILP did not solve any of the hard instances not solved by a fast run of ab-CROWN, although the time out was 10.000s.\n\nb) We tried 6x100 with computing intermediate bounds using full MILP (iterative full MILP). \nSome hard instances not solved by abCROWN can be solved by this iterative full MILP, but the runtime to solve such instances is 20x higher than what is achieved by pMILP, without solving more images than pMILP. \n\n> Q2: \nwe have run the numbers. Scroll down to see the result (no space to input the full result here)\n\n\n\n> Q4) While Table 5 provides details on the number of open neurons and timeouts, ab-CROWN has a large number of further parameters that are not reported.\n\nThe networks we experiment with have already been tested by abCROWN.\nWe thus use the configuration files which come for each network in abCROWN (except for time-out which we explicitly mention).\n\ne.g.: for CNN-B-Adv:\nhttps://github.com/Verified-Intelligence/alpha-beta-CROWN/blob/main/complete_verifier/exp_configs/beta_crown/cifar_cnn_b_adv.yaml\n\"solver:\n  batch_size: 512\n  beta-crown:\n    iteration: 20\"\n\nand\n\nfor 6x100:\nhttps://github.com/Verified-Intelligence/alpha-beta-CROWN/blob/main/complete_verifier/exp_configs/beta_crown/mnist_6_100.yaml\n\n\"solver:\n  batch_size: 1024\n  beta-crown:\n    iteration: 20\"\n\nOur experimental results are in line with published numbers.\n\n> Q5) A single equation containing the whole (inductive) definition of the utility function instead of individual terms distributed across half a page of writing would improve clarity.\n\nUnderstood. We will provide a simpler form definition.\n\n> Q6) The ResNet is irrelevant given its training and poor accuracy and 20k neurons is quite small for recent verification methods. \n\nNumber of neurons is not the only parmeter to take into account. What you are saying is true for EASY instances. For hard instances, as we shown, even small networks cannot be tackled fully by current verification methods. \n\nOur tool scales to  at least 20k neurons. \n\n> \"20k neurons is quite small for recent verification methods. The CIFAR-10 networks from Ferrari et al. could be interesting.\"\n\nFrom Ferrari et al,  2 CIFAR10 Networks stand out as being \"harder to verify\":\nResNet6A  (20% undecided images) and ResNet8A (47% undecided images), Table 1 page 7 of https://arxiv.org/pdf/2205.00263 .\n\nFor other CIFAR10 networks in Ferrari et al., current methods are efficient enough(<4% undecided images), and better accuracy wont tremendously change the picture.\n\nThese 2 networks ResNet6A and ResNet8A actually have LESS neurons (<12k, Table 3 page 14 of https://arxiv.org/pdf/2205.00263 ) than CNN-B-Adv (~20k), so I do not really understand the comment: \"20k neurons is quite small for recent verification methods.\"\n\nThanks for pointing out more <20k neurons networks which are not easy to verify, this actually makes our point stronger.\n\nThat being said, I perfectly understand where you come from, as I had the same belief before working on this paper: \n\"current verification tools are fine up to ~100 000 neurons.\" I changed my mind. One of the paper contribution is actually to expose that fact.\n\nThe fact is, we tried to be as fair as possible (by using networks where abCROWN has already been tested, with configuration files provided by the abCROWN authors, with results matching the published results), and what we found suprised at least ourselves."
            }
        },
        {
            "comment": {
                "value": "I thank the authors for their quick reply. However, it unfortunately does not address the majority of my questions. In particular:\n\nQ1) The key part of my question is *after ab-CROWN*. I.e. how does pMILP compare to full MILP (and other utility functions) when run after ab-CROWN using the resulting relatively tight intermediate bounds.\n\nQ2) No, Table 3 does not address this question, as solve time and number of encoded neurons are only loosely correlated for real solvers (some neurons are very easy to stabilize via cutting planes). A \"cactus plot\" would be the best way to visualize this.\n\nQ4) While Table 5 provides details on the number of open neurons and timeouts, ab-CROWN has a large number of further parameters that are not reported.\n\nQ5) A single equation containing the whole (inductive) definition of the utility function instead of individual terms distributed across half a page of writing would improve clarity.\n\nQ6) The ResNet is irrelevant given its training and poor accuracy and 20k neurons is quite small for recent verification methods. The CIFAR-10 networks from Ferrari et al. could be interesting."
            }
        },
        {
            "comment": {
                "value": "Thanks for your very detailled review.\n\nSome answers now to the questions (which are already in the paper), more later:\n\n>Q1: Can you include experiments using a full-MILP encoding \n\nfull MILP is well known not to scale. Still, we reported an experiment on the smallest of the network (6x100): see Table 2 page 2 \"full MILP\", realized with a very long 10.000s per image: 50% undecided inputs (90%-40%) for average 6100s per image, compared with 13% undecided images and 46s using Hybrid MILP. And this is the smallest network.\n The analysis is stoppedf for each output neuron (which is not the predicted class) as soon as we have ceritification  or falsitication of robustness, which is the most efficient complete test. Do you think it is necesarry to perform this test for larger networks? The outcome is extremely obvious.\n\n> and different partial MILP encodings using other utility functions \n\nthe only other utility function proposed was by Huang et al. 2020. Table 3 page 8 gives actually a more precise picture that just raw % of robustness, by looking at precise accuracy results. Again, we can do it on say 6x100, but the outcome is also extremely obvious (accuracy number from alpha beta CROWN alone, because the accuracy of pMILP with other utility function and the same number of nodes is very poor). Unless we push the number of nodes of pMILP with other utility function to much higher number, but then, the runtime will be extremely high. We will provide both results to give an exact picture. But again, the overall picture is already very clear from Table 3.\n\n\n>Q2 How does your partial MILP encoding compare to a full MILP encoding (of unstable ReLUs) in terms of the achieved upper bounds over time? This is particularly interesting, as a popular approach to solving MILP problems is exactly to relax binary variables (automatically and based on the solvers strategies).\n\nIsnt Table 3 page 8 answering this question as well? \nThe max line corresponds to full MILP. Specifically, Read line 420 and 421: opening 35 neurons with our utility function reaches 95% of the accuracy of full MILP (all 116 unstable nodes open).\n\n>Q3 How is the solve time in Table 3 affected by the choice of neurons? \n\nWe will report that later. The complexity analysis tells us that it is exponential in the number of chosen neurons.\n\n> And how do the results change if no exact MILP bounds but only LP bounds are available for Layer 2?\n\nWe will report such numbers. We expect the number of open nodes in layer 3 has to be considerably higher to reach the same accuracy. Notice that computing bounds in layer 2 is fast (even full MILP can actually be done for layer 2 in short time).\nIn our prototype, we open 48 nodes for layer 2, and the accuracy is >99.99% of full MILP. But we understand it as ablation study to  see the impact of it on accuracy.\n\n\n>Q4 Can you discuss the experimental details as per the weaknesses?\n\"Key parameters of the experimental setup for the main experiment in Table 4 are not discussed\"\n\nWell, this is provided in Table 5, page 13 (appendix).\n\n>Q5 Can you include a formulation of the inductive definition of the utility function?\n\nDo you mean for more than 2 layers back?\n\n>Q6 Can you include experiments on larger networks to investigate the limitations of the proposed method?\n\nCNN-B-Adv has ~20.000 neurons, CIFAR Resnet has almost ~100,000 neurons.\nWe will certainly not scale past what alpha, beta Crown is capable of. \nOur method is meant to be more accurate than alpha beta Crown, not faster or more scalable."
            }
        },
        {
            "comment": {
                "value": "Thanks a lot for your  comments.\n\n1) MaxPool can be treated using MILP, see (Huang et al. 2020). The coding is however more complex, and I doubt we will have time to implement that during the short discussion period.\n\n2) Concerning complexity analysis of pMILP, it is as follows:\n\nO(N \u00b7 MILP(N, K)), where N is the number of nodes of the DNN, and MILP(N, K) is the complexity of solving a MILP program with \nK binary varialbes and N-K linear variables. We have that MILP(N, K) < 2^K LP(N), where LP(N) is the complexity of running LP, polynomial in N, thus the exponential is only on K. The key factor is thus to keep K as small as possible, which our Utility function enables (see Table 3).\n\nThis explains why pMILP is so efficient.\n\nFor reference, here is the pseudo code the analysis is based on.\nHybridMILP pseudo-code:\n\n    Run Alpha-Beta Crown with small time out (e.g. 10s).\n\n    If image undecided then run pMILP.\n\npMILP code:\n\nInput: Bounds [\u03b1_m, \u03b2_m] for input nodes m at layer 0 (input neighbourhood)\n\nOutput: Bounds [\u03b1_n, \u03b2_n] for every output node n\n\nfor layer k = 1 \u00b7 \u00b7 \u00b7 K do\n\n       for neuron n in layer k do\n\n           Compute the set Z of the K most important nodes before n according to Utility(n)\n\n          Run partial MILP(Z as binary, nodes / Z as linear variables, with constraints every node m<n satisfy x_m \\in [\u03b1_m, \u03b2_m]  ) \n         to obtain bounds [\u03b1_n, \u03b2_n] for node n.\n\nreturn Bounds [\u03b1_n, \u03b2_n] for every output node n"
            }
        },
        {
            "comment": {
                "value": "Thanks a lot for all the pointers.\nWe are going to consider all of them, in particular run test on GCP-CROWN  on at least some of the benchmarks.\n\nHowever, to run our independant experiments, IBM CPLEX solver is needed to run GCP-CROWN. \nWe just required the permission to use IBM CPLEX solver and waiting for the approval. We will run experiements when its done.\n\nWhat we can say for now: \nfrom [General Cutting Planes for Bound-Propagation-Based Neural Network Verification, NeurIPS 2022]:\n\nOn CNN-B-ADV, GCP-CROWN is 2% more accurate than alpha,beta CROWN while taking 50% more time.\nAt least on this important benchmark, this will not change the conclusion tremendously: we have 11% of undecided images vs 20-22% for alpha beta CROWN depending on the time-out duration. Our improvement is much larger than the 2% between alpha,beta CROWN and GCP-CROWN. \n\n\nOn Parameters used when running abCROWN:\nThe networks tested have already been tested by abCROWN.\nWe thus use the configuration files which come for each network in abCROWN (except for time-out which we explicitly mention).\n\ne.g.: for CNN-B-Adv:\nhttps://github.com/Verified-Intelligence/alpha-beta-CROWN/blob/main/complete_verifier/exp_configs/beta_crown/cifar_cnn_b_adv.yaml\n\"solver:\n  batch_size: 512\n  beta-crown:\n    iteration: 20\"\n\nand\n\nor for 6x100:\nhttps://github.com/Verified-Intelligence/alpha-beta-CROWN/blob/main/complete_verifier/exp_configs/beta_crown/mnist_6_100.yaml\n\n\"solver:\n  batch_size: 1024\n  beta-crown:\n    iteration: 20\"\n\nOur experimental results are in line with published numbers.\nSo far we did not use GCP-CROWN nor cuts.\n\n\n\nQ1:  Two important things. \n1) \"This work focuses on networks that are challenging to verify because they are not robustly trained\".\nThis is factually incorrect. Consider for instance CNN-B-Adv. It has been trained to be robust against Adversarial attacks.\nAlso, we spent quite some time in the introduction to explain that while training to be robust and/or to be more easily verifiable is indeed possible, this can be done only up to a point before hurting accuracy, which is worst than being non robust. \nWe certaintly agree that  for easy instances, alpha, beta Crown is very good, this is our point indeed. Then you have the not so easy (but not complexity worst case) instances where alpha, beta CROWN (and every other verifier we tested) does not work that well. And that is our focus (as the easy case can be considered \"solved\").  Please read the paragraph in the introduction starting at line 052 and ending by \"one cannot expect only\neasy verification instances: hard verification instances need to be explored as well\".\nSimilar argument is made in the excellent Dathathri et al. (2020).\n\n2) The fact that there are indeed some (rare) truely hard instances is not an excuse for alpha,beta-CROWN (and every other current verifiers we tested) for performing so poorly on so many instances. These instances where alpha,beta CROWN fails are also not \"hard\" instances in terms of worst case complexity, since our prototype succeeds into solving them in reasonable time (and our prototype does nothing magical - it is just carefully designed). This is a proof that alpha,beta Crown has some important gaps we try to solve (which is the case of MN-Bab also obviously ). We will test to see if it is the case for GCP-CROWN as well, and report on the results.\n\nConcerning the pseudo code:\n\nHybridMILP pseudo-code:\n\n    Run Alpha-Beta Crown with small time out (e.g. 10s).\n\n    If image undecided then run pMILP.\n\npMILP pseudo code:\n\nInput: Bounds [\u03b1_m, \u03b2_m] for input nodes m at layer 0 (input neighbourhood)\n\nOutput: Bounds [\u03b1_n, \u03b2_n] for every output node n\n\nfor layer k = 1 \u00b7 \u00b7 \u00b7 K do\n\n       for neuron n in layer k do\n\n           Compute the set Z of the K most important nodes before n according to Utility(n)\n\n          Run partial MILP(Z as binary, nodes / Z as linear variables, with constraints every node m<n satisfy x_m \\in [\u03b1_m, \u03b2_m]  ) \n         to obtain bounds [\u03b1_n, \u03b2_n] for node n.\n\nreturn Bounds [\u03b1_n, \u03b2_n] for every output node n\n\n\n\nThe complexity for pMILP is the following:\n\nO(N \u00b7 MILP(N, K)), where N is the number of nodes of the DNN, and MILP(N, K) is the complexity of solving a MILP program with \nK binary varialbes and N-K linear variables. We have that We have that MILP(N, K) < 2^K LP(N), where LP(N) is the complexity of running LP, polynomial in N, thus the exponential is only on K.\n\nThe key factor is thus to keep K as small as possible, which our Utility function enables (see Table 3)."
            }
        },
        {
            "summary": {
                "value": "The authors propose a new algorithm for verifying neural networks against local robustness properties. This method takes a hybrid approach, combining an existing \u201cBranch and Bound\u201d verifier ($\\alpha,\\beta$-CROWN) with MILP-based verifiers. The authors show that this hybrid approach effectively reduces the number of undecided verification instances for non-robustly trained networks, where verification is known to be hard."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- Apart from a few typos, the paper is clearly written. \n- The proposed approach effectively reduces undecided verification instances for non-robustly trained networks within a reasonable time limit."
            },
            "weaknesses": {
                "value": "**Motivation of the work**\n\nQ1. This work focuses on networks that are challenging to verify because they are not robustly trained. Typically, standard-trained networks have low verified accuracy and are vulnerable to attacks. Given that these networks lack robustness, why should we even attempt to verify such difficult cases? For example, in recent papers on certified training [1], networks trained on MNIST and CIFAR-10 show verified accuracies higher than the upper bound in Table 1. Therefore, no sound verifier can achieve verified accuracy beyond what is reported in [1] on these networks. Additionally, even for ReLU networks robustness verification is known to be NP-hard, meaning that in the worst case, complete verification will require exponential time with respect to the number of unsettled ReLU nodes (assuming $ P \\neq NP $). So in theory there will always be these hard instances and this was the reason behind certifiable training that makes verification easier.\n\n[1] \u201cCertified Training: Small Boxes are All You Need\u201d, ICLR, 2023.\n\n**Missing related works**\n\n(Lines 296 - 298) The use of two lower bounds (commonly known as the triangle relaxation) within an LP formulation is not new and has been applied in prior works [1] and more recently in [2].\n\n\n**Technical Contributions:**\n\nThe main technical contributions of this work are not entirely clear to me. \n\n- Q1: Proposition 1 appears to be a well-known result and is used in existing works [1, 2]. The authors should cite the relevant papers.\n\n- Q2: The authors do not provide sufficient detail on the hybrid-MILP algorithm. Authors should include a pseudo-code outlining the key steps of the proposed algorithm.\n\n-  Q3: The high-level idea behind the utility function described in Section 4 closely resembles branching heuristics, such as BaBSR (cited by the authors), in terms of evaluating the importance of an unsettled neuron with respect to a specific verification property. The authors should clarify how their approach differs from existing branching heuristics.\n\n\n[1] \u201cFormal Verification of Piece-Wise Linear Feed-Forward Neural Networks\u201d, ATVA, 2017.\\\n[2] \u201cInput-Relational Verification of Deep Neural Networks\u201d, PLDI, 2024.\n\n**Missing Experiments with current SOTA verifiers**\n \n$\\alpha,\\beta$-CROWN is no longer the SOTA verifier for local robustness. The authors should consider providing comparisons with GCP-CROWN and if possible with MN-BaB (on all networks table 4).\n\n**Missing details in the experimental setup**\n\nQ1. The authors should provide the config files (example - https://github.com/Verified-Intelligence/alpha-beta-CROWN/blob/main/complete_verifier/exp_configs/beta_crown/cifar_resnet_2b.yaml) they are using for comparing with $\\alpha,\\beta$-CROWN including the following details \n- Branching heuristic \n- Batch size \n- Cuts are applied or not (as done in GCP-CROWN)\n\n**UAP/Hyperproperties/Relational property verification:**\n\nIs the proposed approach applicable to verifying robustness against UAP or more general hyperproperties? It appears that prior works [1,2,3] take a similar approach, combining abstract interpretation or bounding techniques with a MILP formulation that is easy to optimize. \n\n[1] \u201cTowards Robustness Certification Against Universal Perturbations\u201d, ICLR, 2023. \\\n[2] \u201cInput-Relational Verification of Deep Neural Networks\u201d, PLDI, 2024. \\\n[3] \u201cRelational DNN Verification With Cross Executional Bound Refinement\u201d, ICML, 2024.\n\n\n\n**Typos and Minor comments**\n\n- Line 77 - Missing citation\n- Line 174 - incomplete sentence \n- Line 233 - \u201cBox abstraction\u201d - Interval or Box domain is a well-known abstract domain used in traditional program analysis and not introduced by the cited paper. The cited paper introduced DeepPoly a type of symbolic interval domain or restricted polyhedra domain. \n- Line 310 - Typo `nappears`"
            },
            "questions": {
                "value": "Refer to the Weaknesses section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Hybrid MILP, a method designed to efficiently solve complex verification problems in deep neural networks (DNNs), particularly those involving ReLU-based architectures. The paper addresses the limitations of the current state-of-the-art verifier, \u03b1, \u03b2-CROWN, which performs well on relatively simple instances but struggles with more challenging ones. The authors propose a hybrid approach that initially applies \u03b1, \u03b2-CROWN with a short time-out to quickly address easier instances. For instances that remain undecided, Hybrid MILP selectively applies a partial MILP (Mixed Integer Linear Programming) that combines integer variables with linear relaxations for a subset of neurons, reducing the number of integer variables by approximately four times compared to prior methods. This approach effectively narrows the undecided instance rate and is experimentally validated to be both accurate and efficient. Results on benchmark datasets, including MNIST and CIFAR, demonstrate Hybrid MILP\u2019s effectiveness in reducing undecided cases by up to 43%, with a manageable runtime."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "-  Hybrid MILP innovatively combines MILP and linear relaxation techniques to handle challenging verification tasks effectively.\n- Experimental results highlight substantial improvements in both verification accuracy and runtime efficiency."
            },
            "weaknesses": {
                "value": "- No complexity analysis of the proposed method.\n\n- Limited to ReLU activation functions."
            },
            "questions": {
                "value": "- Could the proposed method extend to MaxPool nonlinear layer? If can, how is the performance of Hybrid MILP compared to $\\alpha,\\beta$-CROWN?\n- MILP is complete but time-consuming. I am curious about the complexity of the hybrid MILP and why it is more efficient than other baseline?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "/"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposed Hybrid MILP, a neural network verifier which first uses the optimisation based ab-CROWN to solve easy instances before invoking a MILP solver to solve the remaining hard instances. They key to Hybrid MILP is to only encode a subset of unstable neurons using binary variables (i.e., to 'open' them). This subset is chosen based on an upper bound on the individual neurons utility in such an encoding. The resulting method demonstrates strong performance on hard verification instances on small DNNs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* Novel utility function based on the primal pre-activation values of neurons instead of just their bounds.\n* Empirically high effectiveness across a range of neural networks\n* Extensive background and discussion of (some) related work."
            },
            "weaknesses": {
                "value": "* Lack of ablation studies confirming the importance of the proposed utility function and partial encoding (see below).\n* Key parameters of the experimental setup for the main experiment in Table 4 are not discussed, e.g., how was K chosen, how were intermediate bounds computed, how was 'z' chosen for the utility function computation, were different sets used for robustness against different alternative classes\n* Lack of theoretical and empirical, comparison (or even mention) of the closely related work on branching heuristics (e.g. Bunel et al, De Palma et al., Ferrari et al. and Henriksen et al.) , similarly trying to estimate the importance of encoding neurons exactly in BaB.\n* Lack of comparison to the optimizer's relaxation strategy at equal runtime, i.e., the importance of partial encodings in the first place.\n* Applicability only to very small DNNs, which previous work (Ferrari et al.) found to be more easily solvable by LP/MILP based verifiers like ERAN, which were not compared to.\n* Overclaims regarding the novelty of Proposition 1, which was discussed in similar form in Singh et al Equation 2 and Salman et al. Theorem 4.2.\n* Poor copywriting and large number of typos, including in and the abstract formulas (e.g. Line 174 (half sentence missing), Line 292 (UB - UB -> UB - LB), first expression in Line 359 (three closing but only one opening bracket))\n\n**Minor Comments**\n* BaB-based methods such as ab-CROWN and MN-BaB are complete, but the opposite is implied in several places.\n* Completeness is binary and can not be traded of with efficiency, but precision (at a given timeout) can.\n\n**References**  \n* Ferrari et al. \"Complete verification via multi-neuron relaxation guided branch-and-bound.\"\n* Salman, et al. \"A convex relaxation barrier to tight robustness verification of neural networks.\"\n* Singh et al. \"An abstract domain for certifying neural networks.\"\n* Bunel et al. \"Branch and bound for piecewise linear neural network verification.\"\n* De Palma et al. \"Improved branch and bound for neural network verification via lagrangian decomposition.\"\n* Henriksen et al. \"DEEPSPLIT: An Efficient Splitting Method for Neural Network Verification via Indirect Effect Analysis.\""
            },
            "questions": {
                "value": "1) Can you include experiments using a full-MILP encoding and different partial MILP encodings using other utility functions after ab-CROWN to better assess the contribution of the novel utility function vs. the combination of ab-CROWN with (any) MILP based strategy?\n2) How does your partial MILP encoding compare to a full MILP encoding (of unstable ReLUs) in terms of the achieved upper bounds over time? This is particularly interesting, as a popular approach to solving MILP problems is exactly to relax binary variables (automatically and based on the solvers strategies).\n3) How is the solve time in Table 3 affected by the choice of neurons? And how do the results change if no exact MILP bounds but only LP bounds are available for Layer 2?\n4) Can you discuss the experimental details as per the weaknesses?\n5) Can you include a formulation of the inductive definition of the utility function?\n6) Can you include experiments on larger networks to investigate the limitations of the proposed method?\n\n**Conclusion**  \nThe work is a promising new direction for solving hard verification instances on small Networks. However, comparison to key related work is missing and experimental validation is too granular to assess the effectiveness of the novel components proposed in this work (see weaknesses). In addition, the limitations of this work with regards to applicability to larger and perhaps more relevant networks are not investigated. Overall, I believe this work does thus not meet the bar for acceptance at ICLR but I am more than happy to reconsider this assessment should my concerns be addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}