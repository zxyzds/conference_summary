{
    "id": "YXewbZ8FgU",
    "title": "Let the Rule Speak: Enhancing In-context Learning Debiasing with Interpretability",
    "abstract": "In-context learning, which allows large language models to perform diverse tasks with a few demonstrations, is found to have imbalanced per-class prediction accuracy on multi-class text classification. Although notable output correction methods have been developed to tackle the issue and simultaneously improve downstream prediction accuracy, they may fail to answer the core interpretability challenges: why and which certain classes need corrections, and more importantly, to have an easy-to-understand transformation for correcting each class. To address such interpretability gaps, we first find that the imbalance arises from certain classes consistently receiving high ICL output probabilities, whereas others receiving lower or mixed ranges, so the former is more frequently chosen, resulting in higher accuracy; more crucially, we find that these ranges have significantly varying degrees of influence on the accuracy bias, highlighting the need for precise, interpretable probability corrections by range. Motivated by this, we propose FuRud, a fuzzy rule optimization based debiasing method, that (1) detects which classes need corrections, and (2) for each correction-needed class, detects its probability ranges and applies asymmetric amplifications or reductions to correct them interpretably. Notably, across seven benchmark datasets, FuRud reduces the pairwise class accuracy bias (COBias) by more than half (56%), while achieving a relative increase of 21% in accuracy, outperforming state-of-the-art debiasing methods. Moreover, FuRud can optimize a downstream task in a few-shot manner, with as few as 10 optimization examples. Furthermore, FuRud can work for prompt formats that lead to highly skewed predictions. For example, FuRud greatly improves ICL outputs which use letter options, with 44% relative accuracy increase and 54% relative COBias reduction.",
    "keywords": [
        "In-context learning",
        "debiasing",
        "fuzzy rule",
        "interpretability",
        "multi-objective optimization"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=YXewbZ8FgU",
    "pdf_link": "https://openreview.net/pdf?id=YXewbZ8FgU",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a fuzzy-rule based method to debias probabilities of"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Overall paper is well-written with a novel idea proposed.\n2. Experiment results show strong improvements over baselines."
            },
            "weaknesses": {
                "value": "1. Authors need to clarify the differences between 'debias' and 'calibration' better and earlier (currently mostly discussed in 5.4.). Many references and comparison methods here are 'calibration' based methods such as Batch Calibration (\"Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering.\") and \"Calibrate Before Use: Im- proving Few-shot Performance of Language Models.\". \n2. I think this method is not limited to ICL in LLM only. This is not a weakness of the method per se, as if the method can be used somewhere else means it has greater impacts. Does it really not work/applicable to any other classification probabilities setup? My intuition is that with this debias method, it can probably give improvements in other setting as well. But this is the consequence of introducing more computes. Authors need to justify and perform more analysis why this is a method tailored to ICL in LLM to better motivate the paper. \n3. When comparing with these calibration method, need to list computation cost, calibration errors as reference metrics to justify the comparison."
            },
            "questions": {
                "value": "Mostly described in the weakness section"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes FuRud, an interpretable fuzzy rule optimization based debiasing method for LLM in-context learning. FuRud addresses both the inter-class surface bias and also the intra-class range-wise influences."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "I am first of all impressed by the quality of work done, and also the extensiveness of the paper's discussions. The method is also straightforward with promising results and qualitative analyses. Overall, the paper was well written and easy to follow, and many interesting experiments were done."
            },
            "weaknesses": {
                "value": "- One concern is that the experiments were done on a single model, Llama2-13B. I would like to see if this approach is applicable to other model families and sizes.\n- It is well known that the performance of In-Context Learning is largely dependent on how the demonstrative examples are selected. However, I don't think there was any analysis on this, other than on the number of samples used. How were the samples samples selected -- were they selected randomly? Will there be certain example selection strategies that are incompatible with FuRud?"
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a debiasing/output-correction method for in-context learning applications, using fuzzy-rule based corrections. It achieves comparable accuracy and debiasing performance to other state-of-the-art (sota) methods on one LLM (Llama-2-13B)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The method is original and has not been investigated prior.\n- The range of datasets used is strong, and can provide comprehensive insights (though the range of models used is insufficient).\n- The paper is relatively clear despite the specifics being hard to comprehend quickly.\n- The topic is relevant and timely."
            },
            "weaknesses": {
                "value": "1. Only one, relatively small LLM is experimented on (Llama-2-13B). Bias towards a particular class is central to the author's claims (see questions), though this is likely more prevalent in weaker/smaller models, or at least simpler to model/correct for interpretably. The setup is thus fairly simple and it is yet to be seen how biases may present themselves for more sophisticated models, or how it may be fixed.\n2. I am unsure about the baselines. They lack proper description based on my reading of the paper. It is hard to understand exactly what they propose. Please see questions.\n3. Interpretability is a key motivator but it is not clearly explained why other methods are uninterpretable/why this is a major problem.\n4. A major drawback was in places a lack of readability. For instance, it took a long time to try to understand that the process is as follows: full few-shot examples + test questions are passed through the LLM --> probabilities are measured across the answers for each test question --> these are aggregated across all test questions in the multi-objective optimization step (?) --> probabilities are calibrated according to rules learnt. I was confused in Figure 1 about what terms like \"optimization set\" refer to."
            },
            "questions": {
                "value": "1. How does a more interpretable baseline of simple calibrated accuracy [1] compare? I understand the new baselines are reported to outperform this, but it is a useful comparison point, and it is not always clear how much trust can be put on previous results which are conducted on presumably different models/datasets (again, it is not reported in the paper here).\n2. Why are fuzzy rules any more interpretable than other basic class correction methods? Please provide comparison/examples.\n3. Please clarify my understand of the algorithmic process as outlined above in weaknesses.\n\n[1] Calibrate before use: Improving few-shot performance of language models, Zhao et. al., 2021"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work aims to promote ICL's performance. In particular, the authors focus on the imbalanced prediction issue. To this end, the authors propose a fuzzy rule optimization-based debiasing method. Some experiments are conducted to evaluate the proposed methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The studied topic is promising, as the imbalanced prediction issue of ICL poses significant challenges to the community. \n\n2. Introducing fuzzy rules is novel and exciting."
            },
            "weaknesses": {
                "value": "1. I fail to locate the definition of \"per-class accuracy bias\" throughout the paper, which makes the work difficult to follow. Specifically, what is per-class accuracy bias? What is the difference between the standard scenario and the mentioned per-class accuracy bias?\n\n2. I did not figure out how to optimize Eqs. 4-7 since no clear explanation is provided. The authors did not give a clear picture of the proposed method. Consequently, reproducing this work will be challenging.\n\n3. The performance gain is limited, especially when comparing FuRud to DNIP. For instance, DNIP achieves 6.6\\% performance gain over BC, while FuRud is 6% more accurate than DNIP. Similar cases can be found in the COBias metric.\n\n4. The motivation of the proposed method is confusing. I cannot figure out why the proposed method works and why methods are designed using such an approach. In particular, the authors should clarify why Eqs. 4-7 can address the challenge of class correction. Since there is space for one page to allow authors to add necessary explanations, authors may consider adding detailed clarifications."
            },
            "questions": {
                "value": "Which modules in the experiments or the proposed method are related to Eqs 4-7?\n\nWhat is the explicit connection between the proposed method and the mentioned interpretability?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}