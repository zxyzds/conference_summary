{
    "id": "w10KdRwcMk",
    "title": "Revisiting the Variational Information Bottleneck",
    "abstract": "The Information Bottleneck (IB) framework offers a theoretically optimal approach to data modeling, though it is often intractable. Recent efforts have optimized supervised deep neural networks (DNNs) using a variational upper bound on the IB objective, leading to enhanced robustness against adversarial attacks. In these studies, supervision assumes a dual role: sometimes as a random variable with the empirical distribution of the data, and at other times as a random variable distributed according to the classification performed. This work proposes an extension to the framework, and consequently to the derivation of the bound, that resolves this duality. Applying the resulting bound as an objective for supervised DNNs induces substantial empirical improvements.",
    "keywords": [
        "information bottleneck",
        "information theory",
        "representation learning",
        "adversarial attacks",
        "regularization",
        "supervised learning"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "A new variational adaptation of the IB for supervised DNN optimization",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=w10KdRwcMk",
    "pdf_link": "https://openreview.net/pdf?id=w10KdRwcMk",
    "comments": [
        {
            "summary": {
                "value": "The paper claims to provide the \"theoretical optimal approach to data modeling\", and to extend the framework, by deriving the a variational bound to resolve some problems of the previous framework. \n\nMy understanding is that eq. 16 (derived by eq.3)  is the contribution of this work. The paper provides derivations to justify eq. 16. \n\nIn the experimental session, the authors evaluate the performance in image and text classification of the new loss and the robustness to adversarial attack. \n\nThe authors claim that the new loss outperforms the previous loss."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "If the paper were clearer, the paper presents many contributions and analyses of the proposed terms. \n\nTwo experiments (image and text classification) compare the \"vanilla\" and VIB with the new loss. \n\nHistorical overview of IB."
            },
            "weaknesses": {
                "value": "The paper is largely unclear. \n\nIt starts with the history of the IB, more than presenting the contribution of the work.\n\nThe presentation is not clear on the steps of the new loss.  \n\nIn the new loss, there seems to be a new contribution on the predictor, but the predictor (or classifier) is already included in the loss in the VIB. \n\nThe impression is that the new loss introduces a new regularization term, but its justification is not clear. \n\nThe abstract is unclear, what are the two points of the \"dual role\"? What is the \"theoretically optimal approach to data modeling\"?"
            },
            "questions": {
                "value": "Would be nice to understand the difference of eq. 16 and the standard VIB."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics is foreseen in this work."
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper revisits the variational information bottleneck and extends it to a supervised variational information bottleneck.  The experiment on ImageNet and text classification shows that SIB achieves better results than VIB."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "SIB performs better than VIB regarding classification accuracy and adversarial robustness."
            },
            "weaknesses": {
                "value": "Overall, the novelty is insufficient, and the motivation is unclear. Some related works are missing.\n\n### Novelty:\n1. Using variational lower or upper bonds to approximate mutual information is not novel.\n2. SIB's application focuses on traditional image classification and adversarial attacks. It doesn't include other applications like time series or more challenging scenarios like out-of-distribution or few-shot learning. \n3. Compared to VIB, SIB adds $H( \\hat{Y} \\mid Z)$, involves new hyperparameters and new terms to approximate, and cannot guarantee to reach an accurate value for $H( \\hat{Y} \\mid Z)$.\n\n### Motivation:\n1. The title of the paper uses \"revisiting,\" which refers to why the VIB is revisited and what the problem of VIB is.  These two parts are not clear in the paper. Figure 1 cannot demonstrate well since an overfitted decoder can also exist in SIB if the $\\lambda$ is not set appropriately. \n\n### Missing Reference:\n\n[1] Kolchinsky, Artemy, Brendan D. Tracey, and David H. Wolpert. \"Nonlinear information bottleneck.\" Entropy 21.12 (2019): 1181.\n\n[2] A. Zaidi and I. E. Aguerri, \u201cDistributed deep variational information bottleneck,\u201d in Proc. IEEE 21st Int. Workshop Signal Process. Adv. Wirel. Commun., 2020, pp. 1\u20135\n\n[3] S. Sinha, H. Bharadhwaj, A. Goyal, H. Larochelle, A. Garg, and F. Shkurti, \u201cDIBS: Diversity inducing information bottleneck in model ensembles,\u201d in Proc. AAAI Conf. Artif. Intell., 2021, pp. 9666\u20139674\n\n[4]  S. Mai, Y. Zeng, and H. Hu, \u201cMultimodal information bottleneck: Learning minimal sufficient uni modal and multimodal representations,\u201d IEEETrans. Multimedia, vol. 25, pp. 4121\u20134134, 2022\n\n[5] K. W. Ma, J. P. Lewis, and W. B. Kleijn, \u201cThe HSIC bottleneck: Deep learning without back-propagation,\u201d in Proc. AAAI Conf. Artif. Intell., 2020, pp. 5085\u20135092.\n\nThe paper should compare the above methods as well and also put them into a related work section."
            },
            "questions": {
                "value": "1. Could you please provide more experiments on PGD and AutoAttack?\n2. Could you please visualize the latent representations of SIB and compare them with VIB?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "- The paper picks up an issue identified in the Deep Variational Information Bottleneck paper, where the classifier can overfit to the learned representation, Z, of a VIB model, and proposes a new framework for supervised learning with IB, which they call Supervised Information Bottleneck (SIB), and a corresponding variational approach, SVIB.\n- The core theoretical contribution is to add a constraint to the IB and VIB objectives that minimizes an upper bound on I(\\hat{Y},Z), which is equivalent to maximizing a lower bound on H(\\hat{Y}|Z). The paper shows that this new constraint is tractable in the SVIB setting.\n- The paper provides experiments comparing SVIB to VIB and \u201cvanilla\u201d Maximum Likelihood models (trained with cross entropy) on ImageNet and natural language sentiment analysis."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well-written and easy to read.\n- Constrained maximization of H(\\hat{Y}|Z) will clearly achieve the goal of preventing the classifier from overfitting to the representation.\n- The theoretical approach is plausibly useful. A careful set of experiments could demonstrate its value beyond using VIB or CEB."
            },
            "weaknesses": {
                "value": "- In general, the experiments are of the correct form (comparisons between different IB approaches and Maximum Likelihood on clean and adversarial test sets), but they are unconvincing at supporting the main claim that SVIB substantially improves on other proposed tractable IB approaches, as pointed out in more detail below.\n- One shortcoming of all of the experiments is that the VIB models are not given the same amount of hyperparameter tuning as the SVIB models \u2013 it appears that in all cases, the SVIB models get three times as many runs with different hyperparameters to find a setting that outperforms the VIB models.\n- VIB on classification tasks often benefits from having a mixture distribution for r(z), whether learned or just distributed across part of the domain of Z, rather than having a single isotropic Gaussian distribution for r(z). It\u2019s likely that your selected values of \\beta would perform better in that setting, as it becomes easier for the model to learn to assign classes to different mixture elements as it sees fit, which makes the model more powerful (more powerful models can tolerate higher compression/higher values of \\beta). This would likely benefit SVIB as well, so that it more reliably outperforms the Maximum Likelihood baseline on the test set.\n- The paper is missing an important citation: CEB Improves Model Robustness, Entropy 2020. Overlooking this reference is a major shortcoming of the paper, since it studies the same question on one of the same datasets using the same Information Bottleneck framework, and it achieves substantially better results on that dataset than reported in this paper (its VIB results are also stronger than your VIB and SVIB results).\n- The ImageNet table highlights SVIB results in settings where the VIB results appear to strongly overlap \u2013 it seems a stretch to claim that SVIB is doing better than VIB with a result of 53.4%+/-1.8% compared to 53.5%+/-0.2% for FGS with \\epsilon=0.1, for example (and similarly but to a lesser extent for FGS with \\epsilon=0.5).\n- For all experiments, hyperparameter selection for VIB is questionable, as test set performance on the clean data appears to still be improving substantially at the smallest value of \\beta. As \\beta goes to 0, its performance should match the vanilla model on the clean data, but you stop exploring \\beta when the test set performance is substantially worse than the vanilla model, indicating that probably neither the VIB nor the SVIB models are very close to optimally configured.\n- The Conditional Entropy Bottleneck paper showed that CEB reliably outperforms VIB on both clean and adversarial examples on a variety of image datasets. The CEB Improves Model Robustness paper further explores that in detail on ImageNet. Since implementing CEB can be made parameter-equivalent to implementing VIB (and consequently SVIB), it seems like an important point of comparison. \n- In Figure 1, right-hand side, the H(\\hat{Y}) circle is drawn in a way that does not respect the Markov chain constraint Y-X-Z-\\hat{Y}. It is not possible to have H(\\hat{Y}) overlap H(Y) in any area where H(Z) does not also overlap H(Y). Compare this to the Venn diagrams in the Conditional Entropy Bottleneck paper you cite, where similarly the Markov chain Z-X-Y prevents H(Z) from overlapping H(Y) anywhere that H(X) does not also overlap H(Y).\n- Line 360: repeated word: \u201cis uninformative about about Y\u201d."
            },
            "questions": {
                "value": "- I think the theoretical contribution is solid and valuable to share with the community, but I think the empirical treatment is weak. I would be very happy to increase my rating if the experiments were improved, even if they did not show that SVIB is reliably better than VIB or CEB in all of the settings considered. Whatever the outcome for SVIB on more careful preliminary experiments would be a valuable scientific contribution."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper extends the variational information bottleneck by adding an entropy regularizer to the model that predicts the target y given the latent z. This is motivated by adding and variational bounding a second info bottleneck."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper boils down to a simple to implement and intuitive loss function."
            },
            "weaknesses": {
                "value": "There is a lot of justification that is somewhat verbose and subjective. The derivation is long and elaborate for what boils down to an extra regularizer with an extra tuning parameter."
            },
            "questions": {
                "value": "Here are some detailed comments and questions:\n\nUse \\log in latex and format the integral d.\n\nThe writing may be a little verbose. Examples: lines 260-264 restates things. (7) follows trivially from (4). The sentences preceding both (4) and (7) are also similar. lines 217-254 is repeat well known material.\n\nLine 198 why carry p(x,y) around if everything is conditional on those later? I suspect dropping that saves some hassle later.\n\nLines 271 to 296 would be better expanded after moving lines 217-254 to an appendix. You could be explicit about the use of the chain factorization etc (although maybe the previous point about line 198 can avoid needing to deal with this?).\n\nLine 249 who\u2019s -> whose\n\nLine 334 why is Z left as an r.v.? Please explain how to handle this with sampling. I feel like this is just the entropy of the y given the sampled z, so it should be written explicitly as such?\n\nLine 452 Val column bolded wrongly, the vanilla method should be shown as the winner not the proposed method?\n\nTable 1: it seems as though VIB best performance is at the boundary of your sweep, so we can\u2019t tell if SVIB beats VIB?\n\nTable 2: as previous comment.\n\nA plot instead of tables 5 and 6 would be easier to absorb.\n\nTables 1 and 2: can you not fix lambda and show improvement generally? Varying this in-sample looks like overfitting to the untrained eye (but I think this is an illusion and the results are good). It just seems like sub optimal presentation given tables 5 and 6 show good robust performance over lambda. A plot >> tables of numbers.\n\nSection 4: showing robustness is nice, and the methodology seems very good i.e. adversarial approaches.\n\nLine 478 private -> special"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}