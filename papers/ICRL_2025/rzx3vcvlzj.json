{
    "id": "rzx3vcvlzj",
    "title": "TIGER: Time-frequency Interleaved Gain Extraction and Reconstruction for Efficient Speech Separation",
    "abstract": "In recent years, much speech separation research has focused primarily on improving model performance. However, for low-latency speech processing systems, high efficiency is equally important. Therefore, we propose a speech separation model with significantly reduced parameters and computational costs: ***Time-frequency Interleaved Gain Extraction and Reconstruction network (TIGER)***. TIGER leverages prior knowledge to divide frequency bands and compresses frequency information. We employ a multi-scale selective attention module to extract contextual features, while introducing a full-frequency-frame attention module to capture both temporal and frequency contextual information. Additionally, to more realistically evaluate the performance of speech separation models in complex acoustic environments, we introduce a dataset called ***EchoSet***. This dataset includes noise and more realistic reverberation (e.g., considering object occlusions and material properties), with speech from two speakers overlapping at random proportions. Experimental results showed that models trained on EchoSet had better generalization ability than those trained on other datasets to the data collected in the physical world, which validated the practical value of the EchoSet. On EchoSet and real-world data, TIGER significantly reduces the number of parameters by **94.3**% and the MACs by **95.3**% while achieving performance surpassing state-of-the-art (SOTA) model TF-GridNet. This is the first speech separation model with fewer than **1 million parameters** that achieves performance comparable to the SOTA model.",
    "keywords": [
        "Speech separation",
        "lightweight model",
        "time-frequency domain"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=rzx3vcvlzj",
    "pdf_link": "https://openreview.net/pdf?id=rzx3vcvlzj",
    "comments": [
        {
            "summary": {
                "value": "The paper developed a deep learning model for speech separation; and it not only focuses on improving model performance but model efficiency. The model is lightweight, and is constructed of a new band-split strategy and a new frequency-frame interleaved (FFI) block. Additionally, to reduce the gap between synthetic data and real-world, it introduced a dataset, named EchoSet, with different noise and realistic reverberation. Experimental results demonstrates the effectiveness and efficiency of the proposed model, and the generalization of the dataset to real-world audio."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Overall the paper is well-written and easy to follow. The proposed model architecture looks reasonably nice with the introduction of a new band-split strategy and a new FFI block. Experimental results show that the model not only archieves competitive performance on all datasets, but also it is very lightweight in terms of model sizes, MACs. Its efficiency in training (GPU time & memory) and inference (CPU time, GPU time & memory) looks good, too.\nThe motivation of creating EchoSet dataset is clear and reasonable with a complete analysis of existing dataset. Experimental results on the real-world data with different models, trained on different datasets respectively, demonstrate its generalization ability."
            },
            "weaknesses": {
                "value": "The FFI block follows a common design of dual-path architecture, it consists of 2 different parts: frequency path and frame path. Each path has two main modules: multi-scale selective attention (MSA) and full-frequency-frame attention (F^3A). While F^3A looks familiar with self-attention mechanism, MSA extracts features through a selective attention mechanism at multiple scales. We may need an ablation study of the MSA architecture with different scales (e.g. 1,2,3,4) to see how it affects model performance.\nThe evalution on model efficiency is conducted with a fixed audio input length of 1 second. It would be more complete if the evaluation was with different audio input lengths (each audio in EchoSet is 6 seconds, LRS2-2Mix: 2 seconds, Libri2Mix: 3 seconds)"
            },
            "questions": {
                "value": "The model source code is open, the description of EchoSet is reasonable, but it would be better if the configuration/code of data generation is also open."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel model architecture that has small number of parameters and performs as well as other larger models on various separation tasks.\n\nThe model architecture is similar to bandsplit RNN in its encoder/decoder part and in its core part, it has two novel attention-based submodules called MSA and F3A which are applied in the frequency direction and time (frames) direction separately in identical fashion. These modules are repeated a fixed number of times. When repeating, the parameters are tied, so the number of parameters in the model do not increase when the repetition count is increased.\n\nThe paper also introduces a new dataset called EchoSet which includes reverberant speech mixtures which improved performance on a \"real\" dataset. The \"real\" dataset is not real in the sense of real speech conversations, but it is obtained through playback in a real acoustic environment of individual sources which are mixed later for evaluation.\n\nThe performance on real data shown in Figure 4 is much lower compared to the results with artificial mixtures even when training on EchoSet, which shows there is still a lot of room for improvement when applying these models on real data."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The paper is well written and the model is described in a detailed manner even though there may be room for improvement in the model description for more clarity.\n\nThe method applies to many different sampling rate data due to bandsplit RNN based encoder/decoder. The model is also used to perform cinematic audio separation at 44.1 kHz.\n\nThe ablations and various comparisons with state-of-the-art on multiple relevant datasets are all appropriate and impressive."
            },
            "weaknesses": {
                "value": "Loss function was not mentioned in the main text (or I missed it). Is the loss in (10) in the appendix used for all tasks, or only for cinematic sound separation?\n\nThe math in the MSA module description gets a bit hard to follow, so maybe a more detailed Figure that tracks along with mathematical equations would help. How does selective attention (SA) work? It was not described in the paper."
            },
            "questions": {
                "value": "1. What was the loss function?\n2. How does selective attention (SA) work?\n3. What is the reasoning behind repeating the same FFI block (with tied parameters)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an approach named TIGER for speech separation, emphasizing both efficiency and performance. The proposed approach utilizes frequency band-splitting, selective attention modules, and F3A to manage both temporal and frequency information. In addition, the authors introduced a new data set named EchoSet, that considers the realistic environmental factors such as noise, reverberation, and varied speaker overlap ratios. Experimental evaluations reveal that the proposed approach achieves competitive results across multiple benchmarks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The authors' provision of open-source code enables researchers in the field to reproduce and build upon this work.\n2) The authors have a detailed ablation study, as it clarifies the impact and effectiveness of each proposed module."
            },
            "weaknesses": {
                "value": "1) The proposed approach does not demonstrate clear performance advantages over the current SOTA method, with a noticeable performance gap.\n2) Although the authors claimed their approach is more lightweight, the comparison is not entirely fair. A comparison with other systems that specifically employ lightweight methods would provide a more accurate assessment of the model's efficiency."
            },
            "questions": {
                "value": "1) The authors mentioned that the band-split module is inspired by prior knowledge of music separation. it would be helpful to clarify if the targeted speech separation task has similar prior knowledge and to highlight any key differences or similarities between the band-split module used here and that in the original paper.\n2) In Figure 2, the authors mention that \"Residual connections are used to retain original features and reduce learning difficulty.\" Could you please indicate where these residual connections are depicted in the figure?\n3) In Figure 4, the authors use a line chart to present the results. This may be misleading because the results on the three datasets are independent of each other. A different visualization style, such as a bar chart, might provide a clearer comparison.\n4) What are the differences between TIGER (small) and TIGER (large), why do they have the same model size?\n5) In the ablation study, it looks like the proposed modules appear to involve trade-offs among various performance criteria. Could the authors discuss how to balance this tradeoff and what could be the best to consider?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents TIGER, an efficient speech separation model with significantly reduced parameters and computational costs, optimized for low-latency applications. Leveraging frequency band segmentation and attention mechanisms, TIGER captures rich contextual information. The authors introduce EchoSet, a realistic dataset for evaluating model robustness in complex environments. TIGER achieves state-of-the-art performance with 94.3% fewer parameters and 95.3% fewer MACs than existing models.\n\nThe result is very impressive and the details is rich. They also provide a high quality dataset which would be beneficial to the field."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. Provide a new way of speech separation with very efficient paramater usage. It is very important to the industry and would be very influential \n2. provide a new dataset which provides different content compared to all other similar kinds before."
            },
            "weaknesses": {
                "value": "1. The model focused on two speaker separation which makes the task much simpler. May need to provide a way to generalize the framework."
            },
            "questions": {
                "value": "1. How would the model deal with the audio with background noise? \n2. Does the method still work when two people speaking in different setup? e.g. one people is much further to the speaker than another. \n3. How should the model accommodate variable number of speakers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors propose a speech separation setup which relies on multi-scale selective attention and interleaved time-frequency attention modules. The design is relatively lightweight, significantly reducing the number of model parameters and consequently, computing resources in inference. They also introduced   a speech separation corpus, EchoSet, that allegedly has more realistic noise and reverberation. They conducted comprehensive empirical evaluations of several comparable models and public data sets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is well-written overall, following a logical organization and making sound arguments. The development of the proposed system is easy to follow. The details of the modules inside the systems are adequately presented. The evaluation results, given the nature of the paper as an empirical study, can seem a bit under-developed, but the major evaluations are made and presented. The ablation studies are important to help readers understand the importance of the MFA and F3A modules. That being said, similar ablation studies can also be performed on the EchoSet, to determine which aspect of the simulation set this new corpus apart from the former ones. \nThe paper highlighted the strong performance of the proposed speech separation system and its small footprint in terms of model size."
            },
            "weaknesses": {
                "value": "The comparison between the various corpus is not strictly fair, given the data sets have different sizes. From the data provided by the authors, EchoSet contains 33.78 hrs, whereas LRS2-2Mix has 11.1 hours and Libri2Mix has 58 hrs. Did the authors augment/select the data to ensure that the same amount of data is used in training to evaluate the systems in Table 2?\nThe argument in Sect. 6.1 is not entirely clear to me. What does data collected in the real world refer to? Is there any description of this data?\nTo measure the model complexity, model parameters are acceptable, but MACs is inadequate. To go beyond theoretical analysis, the authors can measure the memory usage, power consumption, throughput and latency on server or edge devices. \nRelated to the comments above, Sect. 6 can be beefed up with more analysis. For example, does TIGER or models trained with EchoSet perform better in lower SNR/stronger reverb compared to other models? Can we break it down? How does Tiger/EchoSet fare on male-male pair vs. male-female pair, etc.?"
            },
            "questions": {
                "value": "In Table 4, how does LowFreqNarrowSplit compare with Mel-split?\nHow scalable is Tiger for multi-talker separation?\nHow scalable is Tiger for audio separation beyond speech, as alluded to in Table 9, where music and environmental sound is still a lot harder than speech. What's the path forward?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper has two main contributions:\n1. The TIGER architecture which is a lightweight time-frequency model\nfocusing on reducing model size while keeping separation accuracy high.\n2. The EchoSet dataset for two speaker speech separation which attempts\nto resemble real world scenarios more closely than previous datasets by using\n3D environments."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper proposes a novel dataset, the EchoSet, which not only includes noise\nand reverberation, but also object occlusion and different materials through\n3D environments. The proposed speech separation architecture achieves high\naccuracy on this new dataset while having few trainable parameters."
            },
            "weaknesses": {
                "value": "There are many issues with the experimental part of the paper, especially the\nevaluation of the proposed model architecture.\n\nThe first issue is the datasets that were chosen. The main dataset for\nspeech separation, the WSJ0-2Mix, was not used. Instead, three others were - the proposed EchoSet, the LRS2-2Mix and the Libri2Mix. While choosing\nthe LRS2-2Mix over WHAMR! is motivated, the main problem concerns the\nLibri2Mix. The results of the Libri2Mix reported here do no coincide with\nprevious reported results. The Conv-TasNet normally reaches 14.7 dB SI-SDRi\nwhile this paper only report 12.1 dB SI-SDRi. Since all of the results reported\nin this paper disagree with previous results, I assume that this is caused by\nthe choice of limiting the sequence length of the Libri2Mix to 3 seconds, as is\nmentioned in the appendix. It also appears that this paper only uses utterances\nfrom train-100 and leaves out utterances from train-360 as is used in the original\nLibri2Mix. Therefore, this paper significantly changed the dataset of the Libri2Mix\nwhile still calling it Libri2Mix. Since the training set was clearly significantly\naltered, the results obtained here are very difficult to interpret because the\nnormal Libri2Mix contains more and significantly longer training data.\n\nIf one wanted to include two speaker speech separation, why not just use the\nstandard WSJ0-2Mix which basically everyone uses? And how are future papers\nmeant to compare themselves to the results reported in this paper? Is everyone\nnow meant to use this altered Libri2Mix? Since the source code is included,\nothers could run experiments to figure out the accuracy of the TIGER model\non the normal Libri2Mix or the WSJ0-2Mix. But it simply does not make sense\nto have others go through said effort instead of having the original paper contain\nthis basic information.\n\nThere are, however, even more aspects of the results that are unusual. The\nspeed and memory measurements were taken using an input sequence containing\n16k elements. This does not represent any of the datasets used. The EchoSet\nis reported to use 6 second inputs, meaning it should contain 96k for 16 kHz,\nthe LRS2-2Mix uses 2 second inputs at 16 kHz, meaning a sequence length of\n32k, and the Libri2Mix uses 3 second inputs, meaning a sequence length of 48k\nat 16 kHz. Choosing any (or preferably all) of the sequence lengths used in the\ndatasets would be logical, but showing the results of a much shorter sequence\nlength is not.\n\nHowever, even if all results reported in the paper were correct, they would\nstill not make a convincing case for the TIGER architecture. The biggest\nstrength of it is its low model size. While the experiments concerning accuracy\nas well as computational cost are flawed as described above, the results of the\nTIGER architecture are still not very convincing except for its accuracy on\nthe EchoSet. It is neither particularly fast nor memory efficient and was only\ncompared to a single other recent SOTA architecture, the TF-GridNet, which\nit cannot match in accuracy in 2 of the 3 datasets tested.\nWhile the paper includes an ablation study, it doesn\u2019t answer the most\npressing questions. What kind of performance (in terms of accuracy/speed/memory\nusage) would the TF-GridNet have if it shared model weights across its blocks\nand reduced some parameters to match the TIGER\u2019s model size? If such a\ncomparison was made and the TIGER architecture would be more accurate/faster/used\nless memory, then there would be a better argument for it. As of now, it just\nlooks like it sacrifices accuracy for model size.\n\nThe paper also fails to mention perhaps the most significant previous work\nfor speech separation models with few trainable parameters, which is Group\ncommunication [1]. It is incorrectly claimed that the TIGER architecture is\nthe first speech separation model with less than 1 million parameters, when the\naforementioned paper includes models with less than 100k parameters and is\nroughly 4 years old.\n\nThe paper also claims that the models trained on the EchoSet perform better\non real world signals than the Libri2Mix and LRS2-2Mix. This might be true,\nbut is difficult to say with certainty due to the lack of information in the paper.\nThe real world data that was used is described in the appendix - however,\nimportant details like its sampling rate and duration were left out. The sampling\nrate for the speech separation datasets is only specified for the LRS2-2Mix to be\n16 kHz but I assume it to be the same for the Libri2Mix and the EchoSet since\nthe training configuration appendix talks of frequency ranges from 0-8 kHz.\nThe duration for these datasets, however, is very different - 2 seconds for the\nLRS2-2Mix, 3 seconds for the Libri2Mix and 6 seconds for the EchoSet. If the\naverage duration of the real world data is also closer to 6 seconds, then it would\ngive the EchoSet an unfair advantage over the other datasets. Again, this might\nnot be the case, but without clarification of the duration and sampling rate of all\nthe data used the results given cannot be verified. In fact, the reported results\nin Figure 4 are very unexpected. How is the LRS2-2Mix often even worse than\nthe Libri2Mix (despite using background noise and reverb, same as the real\nworld data) while the EchoSet is massively better than both? The paper never\nattempts to explain this oddity.\n\nReferences\n\n[1] Yi Luo, Cong Han, and Nima Mesgarani. \u201cGroup Communication With\nContext Codec for Lightweight Source Separation\u201d. In: IEEE/ACM Transactions\non Audio, Speech, and Language Processing 29 (2020), pp. 1752\u20131761."
            },
            "questions": {
                "value": "1. The assembly of the real world data is somewhat confusing. As I understand\nit, the utterances as well as the noise was rerecorded in the real world and then\nmixed together at different SDRs. Why did you not play multiple utterances\nand noise from different locations in the room at the same time which would\nresemble a real world mixture more closely rather than mixing them together\nafterwards?\n2. Computational cost was measured on a one second input at 16 kHz.\nThis choice is never motivated and in the context of speech separation illogical.\nWhile there are some previous papers that have done the same [2, 3], neither\nof them had any justification for this choice either. The standard for speech\nseparation would be an input of at least 4 seconds [4, 5]. The reasoning is\nthat the WSJ0-2Mix, contains utterances with an average length of about 4-5\nseconds. The EchoSet, meaning the dataset the paper proposes, uses 6 second\nutterances. Why choose a completely different length for the calculation of\nspeed and memory usage?\n3. The sampling rate for the speech separation datasets is only specified\nfor the LRS2-2Mix as 16 kHz. Is it the same for the other two (Libri2Mix and\nEchoset)? If so, why? Typically, 8 kHz is used and while there is nothing to be\nsaid against using 16 kHz, it still makes future comparisons difficult to not also\ninclude 8 kHz data.\n\nReferences\n\n[2] Kai Li, Runxuan Yang, and Xiaolin Hu. \u201cAn efficient encoder-decoder\narchitecture with top-down attention for speech separation\u201d. In: ICLR.\n2023.\n\n[3] Chen Chen et al. \u201cA Neural State-Space Modeling Approach to Efficient\nSpeech Separation\u201d. In: Proc. INTERSPEECH 2023. 2023, pp. 3784\u20133788.\ndoi: 10.21437/Interspeech.2023-696.\n\n[4] Zhong-Qiu Wang et al. TF-GridNet: Integrating Full- and Sub-Band Modeling\nfor Speech Separation. 2023. arXiv: 2211.12433 [cs.SD].\n\n[5] Cem Subakan et al. Exploring Self-Attention Mechanisms for Speech Separation.\n2023. arXiv: 2202.02884 [eess.AS]."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}