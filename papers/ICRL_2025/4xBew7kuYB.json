{
    "id": "4xBew7kuYB",
    "title": "Studying the Effects of Training Data on Small Language Models",
    "abstract": "Prior work has found that training very small language models (SLMs) on synthetic children's stories allows them to generate coherent text, comparable to much larger models. These stories are claimed to encompass the vocabulary and factual knowledge base of a 3-4 year old child, capturing the \"essence of natural language.\"\nBecause of these claims, it is tempting to attribute the findings to the simple language of children's stories, drawing a parallel to how children learn language.\nIs the human concept of readability relevant in the context of language model training, or are these findings better explained by other propeties of the data?\nIn this study, we investigate this by first validating several automatic readability measures. We then create synthetic corpora with varying levels of readability and assess the coherence of text generated by SLMs trained on these corpora.\nWe find no relationship between the readability of training data and the generation abilities of SLMs. Specifically, SLMs trained on data with substantially more complex language also exihibit the same abilities as those trained on simple language. Moreover, training on simple language does not lead to earlier development of coherence during training.",
    "keywords": [
        "small language models",
        "pretraining"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4xBew7kuYB",
    "pdf_link": "https://openreview.net/pdf?id=4xBew7kuYB",
    "comments": [
        {
            "summary": {
                "value": "The authors investigates the impact of training data's readability to the generation abilities of very small language models (SLM). They challenge the claim that training SLMs on simple language is the reason for their ability to generate coherent text. They create synthetic corpora with varying level of readability, and found no impact to the coherence of text generated by SLMs, and also found training on simple language does not lead to earlier development of coherence during training."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper tested a very meaningful assumption of whether simple language in training data can lead to better generation abilities of SLMs.\n2. The readability measurement approaches are comprehensively studied and analyzed."
            },
            "weaknesses": {
                "value": "1. The quality measurement is limited to perplexity and coherence (coherent, according to the llm-as-judge prompt, is considered \"well-structured and well-organized\", \"not just a heap of related information, but should build from sentence to sentence\"). The ignorance of other dimensions of quality (for example, as authors also mentioned, clarity and fluency) makes any statements about \"generation abilities of SLMs\" an overclaim.\n2. The quality measurement doesn't use any metrics from the original TinyStories paper: grammar, creativity, consistency with the beginning of the story (Eldan & Li, 2023). That makes the results from the two papers in comparable. Because of that, there is no evidence that \"SLMs trained on data with substantially more complex language also exhibit the same abilities as those trained on simple language\" can also hold the measurement in Eldan & Li (2023).\n3. While the authors rule out some factors not contributing to coherent SLMs, it is unclear what factors are contributing."
            },
            "questions": {
                "value": "See Weaknesses.\n\nTypos:\n1. line 19: propeties -> properties\n2. line 86: exihibit -> exhibit\n3. line 527: thire -> their"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the question: are small LMs capable of learning TinyStories because it is *readable* (i.e., simple vocabulary, concepts, and grammatical structures) or some other feature, notably the dataset's lack of diversity (templated sentences with uniform structure)? The authors of TinyStories, and subsequent citations, only consider the former interpretation, but there is no evidence to eliminate the latter.\n\nThis paper carefully investigates this question by generating two datasets with the same synthetic data generation process, differing only in the vocabulary and the intended audience that the model is asked to use & consider. They call these two datasets $\\\\texttt{LlamaTales-Jr}$ and $\\\\texttt{LlamaTales-GRE}$. The two datasets are equally coherent, but $\\\\texttt{LlamaTales-Jr}$ is much more readable. They find that small LMs are *equally* capable of learning both $\\\\texttt{LlamaTales-Jr}$ and $\\\\texttt{LlamaTales-GRE}$, showing that *readability* does not necessarily explain small LMs' ability to learn TinyStories. Instead, they hypothesize it is the lack of diversity in the data."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is cleanly scoped and clearly written.\n- It corrects a widespread misinterpretation of a result in the NLP literature. This result has been used to motivate LM development inspired by human language learning."
            },
            "weaknesses": {
                "value": "- The scope of the paper is relatively narrow. While it shows that the community has widely misinterpreted the results of a particular paper, it's not clear how much it matters. Moreover, I believe the main surprising finding of $\\\\texttt{TinyStories}$ still stands, which is that SLMs are capable of learning the language of 3-4 year olds (regardless of why).\n- I believe the overall paper can use some reorganization.\n    - I find it odd that \u00a73 and \u00a74 (which are all about measuring the readability and quality of the existing dataset, $\\\\texttt{TinyStories}$) are ordered before \u00a75 (about constructing the datasets used in this paper). Wouldn't it make more sense to first describe the data creation methodology, *then* validate that they have the expected readability and quality? Right not, we don't get to the meat of the paper until halfway through page 7.\n    - The connection between figures and claims in the running text of the paper is all over the place. For instance, most of the main claims in \u00a73 and \u00a74 are supported by figures in the Appendix.\n- The presentation of tables and figures can be more readable.\n   - Figures 2, 3, 6 are hard to interpret due to lack of textual explanation, and I think there must be a better way to present the results. My understanding is that in Figure 2, I should see that in (b), the *green* dots (SLMs trained on $\\\\texttt{LlamaTales-Jr}$) are approximately as high as the best gray dots (LLMs), and in (c), the *blue* dots (SLMs trained on $\\\\texttt{LlamaTales-GRE}$) are ALSO approximately as high as the best gray dots (LLMs). Wouldn't it be better for these to be on the same axes, so the reader can compare directly whether LlamaTales-GRE is as learnable as LlamaTales-Jr? Subplots (a) for $\\\\texttt{TinyStories}$ and (d) for $\\\\texttt{FineWeb}$ should be in the Appendix, since they aren't used to support the main claims. I'm not sure what Figure 3 is doing in the main paper, since it's not discussed in the running text.\n    - Table 1 contains results for many metrics which are not discussed in the running text of the main paper. To prevent reader confusion, I recommend moving the results for these metrics to the Appendix, where the metrics are described. The different metrics also don't seem to tell a different story.\n    - I recommend a table with examples from $\\\\texttt{LlamaTales-Jr}$ and $\\\\texttt{LlamaTales-GRE}$."
            },
            "questions": {
                "value": "I would love to hear the authors' response to my interpretable of the tables / figures, in case there is any misunderstanding.\n\nI am open to raising my score if there is a strong argument for why correcting this misunderstanding is important for the community, as it is my main concern about the paper."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper discusses the effects on training small language models by changing some features related to the concept of readability of one particular dataset. The experiments show no effects."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The paper provides a new dataset with some added features in relation to a previous dataset (TinyStories)."
            },
            "weaknesses": {
                "value": "The scientific contribution of this paper is limited, as it tackles a very narrow (and somewhat artificial) research question, and the experiments show no discernible effects whatsoever. The research question is somewhat artificial in the sense that the concept of readability (in humans) concerns the cognitive load of *interpreting* a text, which is not the same thing as *learning* a statistical language model from a text. In particular since readability is usually defined in terms of features related to frequency and length of individual tokens, but the paper does not discuss the influence of tokenization on the learning abilities of language models. It is therefore not at all clear (to me) why the concept of readability would have anything at all to do with how well a statistical language model performs. The experiments included in the paper confirms that it does not. The paper also contains an experiment that shows that the concept of readability and the concept of text quality (as interpreted in terms of perplexity and coherence) are unrelated, which is exactly what you would expect given the definition of these concepts. As such, it is difficult to see what novel knowledge this paper contributes with."
            },
            "questions": {
                "value": "When measuring quality, you use a set of open models. Why not simply use a state of the art model such as GPT-4o or Claude3.5 instead?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}