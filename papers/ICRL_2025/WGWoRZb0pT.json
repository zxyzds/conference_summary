{
    "id": "WGWoRZb0pT",
    "title": "FairCoT: Enhancing Fairness in Diffusion Models via Chain of Thought Reasoning of Multimodal Language Models",
    "abstract": "In the domain of text-to-image generative models, biases inherent in training datasets often propagate into generated content, posing significant ethical challenges, particularly in socially sensitive contexts. We introduce FairCoT, a novel framework that enhances fairness in diffusion models through Chain-of-Thought (CoT) reasoning within multimodal generative large language models (LLMs). FairCoT employs iterative CoT refinement and attire-based attribute prediction to systematically mitigate biases, ensuring diverse and equitable representation in generated images. By integrating iterative reasoning processes, FairCoT addresses the limitations of zero-shot CoT in sensitive scenarios, balancing creativity with ethical responsibility. Experimental evaluations across multiple models, including DALL-E and various Stable Diffusion variants, demonstrate that FairCoT significantly improves fairness and diversity metrics without compromising image quality or relevance. Our approach advances ethical AI practices in generative modeling, promoting socially responsible content generation and setting new standards for fairness in AI-generated imagery.",
    "keywords": [
        "diffusion models;fairness; bias; chain of thought; text to image; multimodal LLMs"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=WGWoRZb0pT",
    "pdf_link": "https://openreview.net/pdf?id=WGWoRZb0pT",
    "comments": [
        {
            "summary": {
                "value": "This paper presents FairCoT, a new framework for improving fairness in text-to-image diffusion models by leveraging Chain-of-Thought (CoT) reasoning within multimodal large language models (MLLMs). The key idea is to use iterative CoT refinement and attire-based attribute prediction to systematically reduce biases in generated images. Experiments across multiple models show improved fairness and diversity without sacrificing image quality or relevance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Reducing the biases and improving fairness in image generation models are very important topics for both research communities and the real-world applications of the models.\n- The proposed approach shows significant improvement over gender, race, age, and religion attributes while maintaining image quality (measured via CLIP-T scores)."
            },
            "weaknesses": {
                "value": "- The technical contributions and the novelty of this work are not significant. Previous works [1] also explored using an iterative strategy to improve the fairness of Text-to-Image diffusion models.\n- In addition, the authors did not compare the several more relevant baselines, such as IDA [1].\n- It is not clear how the approach iterative updates the CoT and consequently improves the fairness of the images. It would be helpful if the authors could provide a more clear and concrete example of the refinement process.\n- The presentation of the paper is not clear and should be further improved. For example, the diagram shown in Figure 3 is not very clear.\n\n[1] Debiasing Text-to-Image Diffusion Models. He, et al. 2024."
            },
            "questions": {
                "value": "- While the quantitative results of CLIP-T score only have small variations, in the qualitative comparison in Figure 6, the images generated by FairCoT look pretty weird and of poor quality. Could you have an explanation for this? Is this a common issue of FairCoT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper present FairCoT: an iterative CoT refinement and attire-based attribute prediction technique mitigate biases present in text to image generative models. The paper develops a MLLM and CLIP guided method to develop a CoT pool, which is then used to guide a T2I model towards generating de-biased outputs. Experiments on DALL-E and various Stable Diffusion variants demonstrate that FairCoT significantly improves fairness and diversity metrics without compromising image quality or relevance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. The paper tackles an important problem in T2I models - and aims to develop a method that makes them generate images which represent a broader sample of the society.\n2. The paper studies multiple aspects such as age, religion, gender and race."
            },
            "weaknesses": {
                "value": "1. Technically limited contribution. The work iteratively applies CoT to generate multiple images to steer them towards being fair, which in my opinion does not offer many novel insights or findings.\n2. The paper used MLLMS and CLIP throughout their pipeline, as a means to alleviate bias in T2I models. However, the paper does not account for the bias present in these models, which has been extensively studied in [1,2].\n3. Computationally inefficient. The pipeline involves multiple calls to a T2I model. The authors could refer to [3] for a survey of methods on this topic which perform for example, efficient, embedding-based methods.\n\n[1] https://arxiv.org/abs/2108.02818\n[2] https://arxiv.org/abs/2203.11933\n[3] https://arxiv.org/pdf/2404.01030"
            },
            "questions": {
                "value": "1. In Section 3.1.2, why is \"attire\" chosen to be a good representation of a given religion? I am not sure about Figure 2? How does a man in a suit define the religion of that person? A person of a given religion might not always be wear a given attire. I don't think it is appropriate to generalize this.\n2. I would like to understand how the pipeline works with the SD set of models, given they cannot be prompted the same way DALL-E can be since its integrated with GPT-4.\n3. General suggestions - I think all the captions of the figures and tables should be improved."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The present paper proposes a novel method to tackle the lack of diversity in text-to-image models\u2019 outputs displaying persons. To this end, the authors make use of pre-trained language models and CLIP and a discovery stage employing a Chain-of-thought process to create a set of prompt extensions. The approach is evaluated based on a prompt set covering gender, race, age, and religion and compared to a wide range of existing \"debiasing\" approaches."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper addresses a current and crucial problem in research and industry as exemplified by text-to-image model applications such as DALL-E and Gemini being restricted in their capabilities of generating humans by their creators to prevent the display of social biases and, in turn, a negative social impact.\n\n- While simple, the introduced method is flexible and model-agnostic.\n\n- The paper provides an extensive overview of related work.\n \n- Extensive evaluation covering multiple diffusion models and other \u201edebiasing\u201c approaches.\n\n- The evaluation goes beyond the display of single persons and single concepts commonly used in previous work."
            },
            "weaknesses": {
                "value": "- Instead of leveraging existing benchmark datasets such as [1], the evaluation is based on prompts selected by the authors.\nUsing existing datasets would enhance the comparability and reproducibility of results.\n\n- More importantly, my main concern is the use of CLIP-T within both the methodology and evaluation metric, which could lead to a flawed evaluation. Therefore, I would urge the redesign of the evaluation process. \n\n- While the prompts used are listed within the Appendix, it would increase clarity if at least the number of prompts for each category is listed in the main text.\n\n- Section 4.3 lacks details on the hand-labeled images. Can you provide details on who performed the labeling, the annotators\u2019 qualifications, the evaluation criteria, and the process used? Further, will this dataset be shared as part of the contribution?\n\n- Figure and table captions are very sparse. More descriptive captions would improve clarity.\n\nReferences:\n\n[1] Luccioni et al. (NeurIPS 2023): https://proceedings.neurips.cc/paper_files/paper/2023/file/b01153e7112b347d8ed54f317840d8af-Paper-Datasets_and_Benchmarks.pdf"
            },
            "questions": {
                "value": "- In Section 3.1, the term \u201ctraining stage\u201d may be misleading, as no training is conducted. This phase appears to be a bias discovery stage rather than training. Using \u201ctraining\u201d could lead readers to believe that model parameters are being tuned.\n\n- The appendix indicates the use of GPT-3.5 as the language model. To improve reproducibility, please specify the exact version used. Additionally, have other open-weight language models been tested to reduce reliance on proprietary models?\n\n**Minor comments**\n- The abbreviation \u201cMLLM\u201d is introduced multiple times.\n- There are missing spaces in several locations, such as in line 266 \u201c(Figure1)\u201d and line 410 \u201cfairness.Further.\u201d"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents FairCoT, a framework designed to enhance fairness in text2image models by leveraging CoT. It includes iterative CoT refinement, attire-based attribute prediction, and bias assessment via entropy scores. Their experiments show that FairCoT achieves high fairness and diversity metrics without compromising image quality or relevance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. It tackles significant questions in AI. \n\n2. They successfully adapt CoT to the field of Fairness and debiasing \n\n3. this is a simple, model-agnostic method that can be applied to both open-source and closed-source models without requiring retraining or parameter adjustments.\n\n4. They achieve good debiasing without lowering the quality of the generated image"
            },
            "weaknesses": {
                "value": "1. While I generally enjoyed reading this paper, the excessive use of bullet points makes it appear less professional and somewhat like an unfinished outline.\n2. The paper relies heavily on CLIP for attribute prediction, but since CLIP was not originally designed as an exhaustive demographic classifier, this reliance might lead to skewed results and miss more subtle and nuanced features.\n3. I am also interested in understanding the typical length of the CoT path used in the experiments. Does it require a large number of inferences? Additionally, is CoT useful for other downstream applications in text-to-image models like image editing and classification?\n4. In qualitative examples 5 and 6, I noticed that FairCoT often produces images with multiple people, while the baseline typically shows a single person. Does this mean that FairCoT sacrifices some control over the image? Additionally, in Figure 6, it appears that FairCoT introduces some green noise into the image."
            },
            "questions": {
                "value": "I would appreciate it if the authors addressed the questions mentioned in the weaknesses section, and I will raise the score if their responses are convincing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}