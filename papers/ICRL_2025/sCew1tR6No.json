{
    "id": "sCew1tR6No",
    "title": "Partial Gromov-Wasserstein Metric",
    "abstract": "The Gromov-Wasserstein (GW) distance has gained increasing interest in the machine learning community in recent years, as it allows for the comparison of measures in different metric spaces. To overcome the limitations imposed by the equal mass requirements of the classical GW problem, researchers have begun exploring its application in unbalanced settings. However, Unbalanced GW (UGW) can only be regarded as a discrepancy rather than a rigorous metric/distance between two metric measure spaces (mm-spaces). In this paper, we propose a particular case of the UGW problem, termed Partial Gromov-Wasserstein (PGW). We establish that PGW is a well-defined metric between mm-spaces and discuss its theoretical properties, including the existence of a minimizer for the PGW problem and the relationship between PGW and GW, among others. We then propose two variants of the Frank-Wolfe algorithm for solving the PGW problem and show that they are mathematically and computationally equivalent. Moreover, based on our PGW metric, we introduce the analogous concept of barycenters for mm-spaces. Finally, we validate the effectiveness of our PGW metric and related solvers in applications such as shape matching, shape retrieval, and shape interpolation, comparing them against existing baselines.",
    "keywords": [
        "Optimal transport",
        "Gromov-Wasserstein problem",
        "Unbalanced optimal transport"
    ],
    "primary_area": "optimization",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=sCew1tR6No",
    "pdf_link": "https://openreview.net/pdf?id=sCew1tR6No",
    "comments": [
        {
            "summary": {
                "value": "This paper improves unbalanced approaches to the optimal transport (OT) problem. \nThe objective is to compare probability measures between different metric spaces (GW distances).\nTo overcome the non-convexity of the  formulation, which usually relies on the Frank-Wolfe (FW) \nalgorithm. \n\nThe authors propose a new metric rooted in the so called Partial Gromov-Wasserstein (PGW) problem, \nand the main novelty is to use a total variation penalty as well as to provide efficient solvers. \n\nThe paper commences by introducing OT and its partial extension POT.  Then, it comes out the regularization\nterm which in principle is the Kullback-Leibler divergence which is replaced by Total Variation (TV). \nThen, the FW algorithm is adopted. \n\nThe toy experiment: \n\"TOY EXAMPLE: SHAPE MATCHING WITH OUTLIERS\nWe use the moon dataset and synthetic 2D/3D spherical data in this experiment.\"\n\nshould be explained earlier in the paper to motivate the reader. Formal language should not be \nan enemy of intuition.\n\nPros: \n- Nice formal language. \n\nCons:\n- Not intuitive for general readers. \n- The contribution is relevant (avoid strong outliers) \n\nQuestions:\n- Is there any formalism for Bregman divergences?"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Nice formalization."
            },
            "weaknesses": {
                "value": "Very hard to read even for an experienced reviewer."
            },
            "questions": {
                "value": "* Adequacy to Bregman divergences?\n* Put synthetic examples in the beigning as a motivation and smooth the formal language."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The Gromov-Wasserstein (GW) distance has drawn a lot of attention in recent years for the comparison of measures in different metric spaces. This paper focuses on the Unbalanced Gromov-Wasserstein (UGW) problem, which they called Partial Gromov-Wasserstein (PGW). They showed PGW is a well-defined metric between metric measuring spaces and discussed some theoretical properties. They designed a Frank-Wolfe algorithm-based method to solve PGW. Convergence analysis is provided. Numerically, they tested it on some datasets with existing baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Pros:\n\nThey propose PGW, a total variation version of the UGW problem with theoretical properties analysis.\n\nThe experiment is showing their good performance in PGW."
            },
            "weaknesses": {
                "value": "Cons:\n\n1. The main concern is the novelty and significance since the problem itself is not well-generalizable to other problems. Meanwhile, some techniques are adopted from existing works. For example:\n\na. What is the unique technique of your method design compared to the Frank-Wolfe solver presented in (Chapel et al., 2020) except their problem setting is UGW?\n\nb. How is the convergence analysis partially harder than the existing method?\n\nc. Since the algorithm only guarantees stationary points (this is fine, the reviewer does not require a global optimality), how can we convinced it works well in more real scenarios? Is there any more evidence?\n\n\n\n2. For the FW algorithm in the optimization problem in line 331, it\u2019s the main subroutine in each iteration, how is the complexity of solving it? This is important in the analysis since FW usually could be dramatically slowed down by the subroutine.\n\n\n3. Minor issue. There are some citation errors in lines 165 and 177."
            },
            "questions": {
                "value": "Please address the question in the weakness section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the Partial Gromov-Wasserstein (PGW) metric, a novel adaptation of the Gromov-Wasserstein (GW) distance tailored for metric measure spaces. Unlike traditional GW, which requires equal mass in spaces for meaningful comparison, PGW accommodates unbalanced settings through the incorporation of a total variation penalty. The authors present theoretical results, including proof of PGW as a metric, and develop two Frank-Wolfe solvers for computational efficiency. Experiments on shape matching, retrieval, and interpolation demonstrate PGW's robustness to some extent."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper rigorously proves that PGW qualifies as a metric.\n- The two Frank-Wolfe solvers for PGW demonstrate effective performance, making the method feasible for practical applications.\n- Experimental results indicate that PGW outperforms traditional GW in handling unbalanced data with outliers, enhancing its utility in real-world tasks."
            },
            "weaknesses": {
                "value": "- Comparisons with alternative unbalanced OT metrics are somewhat limited, which leaves the practical performance of PGW against state-of-the-art methods less explored.\n- The robustness parameter $\\lambda$ requires careful tuning, which could pose a challenge for practical applications.\n- The computational efficiency of PGW may still be restrictive in high-dimensional settings or for very large datasets."
            },
            "questions": {
                "value": "- The baseline selection of the experiment is too limited to demonstrate the effectiveness of your methods. Why don't you compare the methods [1,2,3] based on robust OT? Please add corresponding experiments.\n- - [1] Nietert S, Goldfeld Z, Cummings R. Outlier-robust optimal transport: Duality, structure, and statistical analysis[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2022: 11691-11719.\n- - [2] Wang X, Huang J, Yang Q, et al. On Robust Wasserstein Barycenter: The Model and Algorithm[C]//Proceedings of the 2024 SIAM International Conference on Data Mining (SDM). Society for Industrial and Applied Mathematics, 2024: 235-243.\n- - [3] Le K, Nguyen H, Nguyen Q M, et al. On robust optimal transport: Computational complexity and barycenter computation[J]. Advances in Neural Information Processing Systems, 2021, 34: 21947-21959.\n- In Wasserstein barycenter's experiment for robustness, why only show the interpolation between two shapes? Can you show me how the algorithm computes the average shape of several shapes?\n- It seems that all the experiments you provide can be replaced by the equivalent of robust OT, which cannot truly reflect the advantages of robust Gromov-Wasserstein barycenter.\n- Is the parameter $\\lambda$ easy to search? Different proportions of noise seem to require different $\\lambda$s. Does this hinder the practicality of this method? It seems that the specific $\\lambda$ search process and the influence of $\\lambda$ on the experimental effect are not given in the experiment."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The Gromov-Wasserstein (GW) distance is a metric on probability measures supported on different metric spaces. This paper extends this notion to more general Radon measures, not necessarily probability measures, termed partial Gromov-Wasserstein (PGW) distance. The authors show that PGW is indeed a metric. Two Frank-Wolfe (FW) algorithms are proposed to compute PGW, and three examples are provided to illustrate the performance of PGW."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1, The main contribution is to establish the metric property of partial GW. \n\n2, Simulations in subsection 6.2 clearly demonstrate improved performance of PGW compared to MPGW.\n\n2, The simulation examples are interesting and illustrative."
            },
            "weaknesses": {
                "value": "1, The current manuscript has many typos. For example, the citations on lines 165 and 177 are not displaying properly."
            },
            "questions": {
                "value": "1, Please read the paper thoroughly and correct any other typos.\n\n2, Is the proposed Algorithm 1 the same as Algorithm 1 in the paper \"Partial Optimal Transport with Applications on Positive-Unlabeled Learning\"? What are the novelties in the optimization algorithms?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Partial Gromov-Wasserstein (PGW), a particular case of UGW, and prove that it is a metric between mm-spaces. The authors propose Frank-Wolfe-based algorithms with convergence guarantees for solving the PGW problem and extend the concept of barycenters to mm-spaces. PGW\u2019s effectiveness is demonstrated through applications like shape matching, retrieval, and interpolation, showing improved performance over existing baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper is well-written and easy to follow.\n  \n2. The paper rigorously proves that PGW, a specific case of UGW with the TV norm as the chosen $f$-divergence, defines a valid metric between mm-spaces, contributing to the theoretical understanding of these transport frameworks.\n  \n3. The authors propose using the Frank-Wolfe algorithm to solve the PGW problem and provide a convergence guarantee, enhancing the computational tractability of the method."
            },
            "weaknesses": {
                "value": "1. Limited Experimental Scope: The experimental evaluation in this paper could be expanded. It might be helpful to include experiments on PU learning (positive-unlabeled learning), similar to those conducted in the UGW paper, for a more comprehensive comparison.\n  \n2. Hyperparameter Tuning in Baselines: For the toy examples and point cloud matching experiments, the baselines' performance might improve with more thorough hyperparameter tuning since authors use fixed values mentioned in Appendix N. Given that UGW has been shown to be robust to outliers both theoretically and empirically (as demonstrated in the original UGW paper and also [A]), better-tuned baselines could potentially mitigate the impact of outliers more effectively in these experiments.\n\n3. Minor Issues: Several broken references and citations are present in the paper, such as those on lines 165-166, 894, and 1025. Addressing these would improve the paper's readability and professionalism.\n\n\n[A] Tran, Quang Huy, et al. \"Unbalanced co-optimal transport.\"\u00a0*Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 37. No. 8. 2023."
            },
            "questions": {
                "value": "1. Could the authors incorporate other experiments, such as PU learning (positive-unlabeled learning), to provide a broader evaluation?\n  \n2. Could the authors report the results of varying hyperparameters in the baselines to examine their impact on performance?\n  \n3. While the KL divergence in UGW is not a rigorous metric, it has desirable properties such as robustness to outliers. Could the authors explain why the TV norm used in PGW results in better performance in the numerical experiments compared to UGW? Alternatively, is the improved performance due to the use of the Frank-Wolfe (FW) algorithm instead of Sinkhorn?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}