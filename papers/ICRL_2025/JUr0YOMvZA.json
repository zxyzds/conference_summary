{
    "id": "JUr0YOMvZA",
    "title": "DAMO: Decoding by Accumulating Activations Momentum for Mitigating Hallucinations in Vision-Language Models",
    "abstract": "Large Vision-Language Models (VLMs) exhibit significant potential in multimodal tasks but often struggle with hallucinations\u2014responses that are plausible yet visually ungrounded. In this work, we investigate the layer-wise prediction tendencies of VLMs and conduct an in-depth analysis of their decoding mechanism. We observe that VLMs tend to ``overthink'' during the final stages of decoding, making significant prediction shifts in the last few layers often favoring incorrect results, which leads to a surge in hallucinative outputs.\n\nLeveraging this localized pattern, we propose a novel decoding strategy inspired by the momentum analogy used in gradient descent-based optimizers. Our method enforces decoding consistency across layers in an adaptive manner during forward passes\u2014an under-explored approach in existing works. This strategy significantly improves the reliability and performance of VLMs in various multimodal tasks, while introducing only negligible efficiency overhead.",
    "keywords": [
        "Vision-Language Models (VLMs)",
        "Hallucinations",
        "Decoding Method",
        "Momentum Techniques"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "To address the hallucination problem in Vision-Language Models (VLMs), we propose a novel decoding method inspired by momentum techniques.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=JUr0YOMvZA",
    "pdf_link": "https://openreview.net/pdf?id=JUr0YOMvZA",
    "comments": [
        {
            "summary": {
                "value": "This paper observes that VLMs tend to make prediction shifts in the last few layers, which leads to a surge in hallucinative outputs. The authors propose a decoding strategy inspired by the momentum analogy used in gradient descent-based optimizers, which enforces decoding consistency adaptively across layers during forward passes. The proposed method outperforms existing approaches on several public datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The motivation of hallucinations frequently emerge in the later layers seems interesting.\n2. The main technical pipeline is clear."
            },
            "weaknesses": {
                "value": "1. Writing and diagrams require improvement.\n1). Large Vision-Language Models (VLMs) mentioned in the abstract abbreviated as LVLMs would be better.\n2). A single-paragraph abstract would improve conciseness.\n2). Figure 1(c) requires a more detailed description.\n3). In Section 2, lines 151\u2013152, the statement \u201cHowever, these methods do not correct hallucinations during the inference process\u201d could be reconsidered. In my view, methods like VCD, which contrast output distributions, are indeed part of the inference process.\n4). It would improve readability if the introduction were revised to emphasize the primary contributions of this work more clearly.\n\n2. Could you increase the number of instances shown in Figure 1(b) to provide a more comprehensive view?\n\n3. Section 3.2, line 198-200, \"VLMs are already proficient in capturing detailed visual information, so further intensifying image-text fusion is unnecessary.\" how to validate \"VLMs are already proficient in capturing detailed visual information\" from your results?\n\n4. Section 3.4, line 261-261, \"Varying the starting layer for refinement can enhance different model capabilities (e.g., layer 16 excels in OCR tasks, while layer 24 improves positional perception).\" (e.g., position scores do not consistently increase, nor do OCR scores consistently decrease). This makes it challenging to conclude that varying the starting layer directly enhances distinct capabilities. Could you provide additional evidence to support this claim?\n\n5. The performance improvement appears modest. For instance, on the MME benchmark, the score of 1515.89 is slightly lower than OPERA\u2019s 1518.36 on the LLaVA baseline. Additionally, it would strengthen the evaluation if more hallucination benchmarks, such as CHAIR, were included.\n\n6. I am concerned about the quality of text generated at preceding layers. Further evaluation metrics for text quality, such as BLEU or other relevant scores, would provide a clearer understanding."
            },
            "questions": {
                "value": "My primary concern lies in the performance and  the quality of text generated at preceding layers. I will be happy to raise my score if my current questions and concerns can be addressed."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes DAMO (Decoding by Accumulating Activations Momentum), a novel decoding strategy designed to reduce hallucinations in Vision-Language Models (VLMs) by maintaining consistency across layers during inference. DAMO introduces a momentum-based mechanism in transformer forward computation to smooth activation updates layer-by-layer, mitigating hallucinations by preserving visual grounding in predictions. DAMO shows good results on various benchmarks (MME, POPE) and models (LLaVA1.5, INF-MLLM1, mPLUG-Owl2) with minimal memory overhead. Its adaptability is demonstrated through successful application to LLM benchmarks such as TruthfulQA and FACTOR, highlighting DAMO\u2019s transferability across various tasks."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper presents an interesting observation that VLMs are often able to generate accurate predictions in the initial layers, with hallucinations surfacing in later layers. This suggests that hallucinations could stem from disruptions in the decoding sequence rather than an absence of underlying knowledge, providing a novel diagnostic angle on VLM hallucinations.\n- The paper\u2019s approach to using \"momentum\" within the forward computation of transformers is intuitive and impactful. By building consistency across activations, DAMO effectively mitigates hallucination emergence in later layers, reinforcing layer-wise stability and preserving visual grounding.\n- The authors extend their method beyond VLMs to yield strong performance in LLM benchmarks like TruthfulQA, FACTOR, StrQA and GSM8K, showcasing its versatility across various language tasks."
            },
            "weaknesses": {
                "value": "- While the Q&A format in the experiments section is engaging, it\u2019s overused, which might dilute its effectiveness. Reserving this style for only the most critical insights could enhance focus and readability.\n- Although DAMO shows good results on the MME benchmark, the gains seem modest for hallucination evaluation. Emphasizing the \"celebrity\" score on mPLUG-Owl2 might not strongly correlate with reduced hallucinations. The MME benchmark may be more reflective of general VLM performance rather than specific hallucination robustness. To strengthen authors\u2019 claims on hallucination mitigation, it would be beneficial to test its performance on other benchmarks that more directly target hallucination evaluation.\n- The paper introduces a threshold \\tau as a cosine similarity criterion for switching between coefficients \\beta_1\u200b and \\beta_2\u200b, which is critical to DAMO\u2019s performance. However, the manuscript lacks explanation on how \\tau was determined, its specific value, and its impact on model effectiveness. Additional details would improve understanding and reproducibility."
            },
            "questions": {
                "value": "- The results on the MME dataset show only modest improvements in hallucination reduction, with much of the gains appearing in tasks that may be more knowledge-based (e.g., Celebrity, Posters, Landmark, Artwork) rather than directly related to hallucination robustness. Would you consider testing DAMO on additional hallucination-focused benchmarks? Alternatively, could you clarify how the MME dataset captures DAMO's effectiveness in mitigating hallucinations?\n- Given that the threshold \u03c4\\tau\u03c4 is central to controlling the switch between coefficients \\beta_1\u200b and \\beta_2\u200b, could you provide more information on how its value was determined? Additionally, could you discuss any observed sensitivity of DAMO\u2019s performance to variations in \\tau?\n- Could you provide further insights on the actual layers where the adaptive coefficient adjustment activates? For instance, does the transition of \\beta from \\beta_1\u200b to \\beta_2\u200b tend to occur around specific layers, such as the after 24th, as observed in your analysis? Clarifying whether these transitions align with the predefined analysis layers would help in understanding DAMO\u2019s consistency and effectiveness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the layer-wise prediction tendencies of VLMs and proposes a novel decoding strategy called DAMO to mitigate hallucinations. DAMO enforces decoding consistency across layers in an adaptive manner during forward passes, amplifying visual semantics consistently extracted throughout the inference while reducing hallucination biases. Experiments on various VLM benchmarks demonstrate that DAMO significantly mitigates hallucinations, resulting in more visually grounded and accurate predictions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Novel insights consider the layer-wise outputs, especially regarding to language bias in LVLMs.\n\n2. Simple but effective solutions named DAMO based on the proposed findings.\n\n3. Extensive experiments and pleasant results are achieved.\n\n4. Source codes and partial experimental results are provided."
            },
            "weaknesses": {
                "value": "1. Although this method is proposed for LVLMs, this paper does not mention the DoLa [r1] in the main body. DoLa's claims are somewhat opposite to DAMO's. More analysis is needed to clarify the insights and differences.\n[r1] Dola:  Decodingbycontrastinglayers improves factualityinlarge languagemodels. arXivpreprint\n arXiv:2309.03883,2023.\n2. Applying DAMO to LVLMs and LLMs seems to be contradictory, especially considering the DoLa's findings.\n3. I check the appendix and think some critical experimental details of DAMO are missing, like temperature, max token, decoding strategies.\n4. GPT4-V experiments should be better conducted rather than GPT4, following VCD.\n5. typo: INF-MLLM1 $\\rightarrow$ INF-MLLM."
            },
            "questions": {
                "value": "1. How about some complex reasoning VQA performances in terms of Figure 1(b) (c). As indicated in DoLa [r1], deep layers facilitate the complex reasoning. Also, the reasoning steps is more complex as in [r1], please add more explanations.\n[r1] Dola:  Decodingbycontrastinglayers improves factualityinlarge languagemodels. arXivpreprint\n arXiv:2309.03883,2023.\n2. In 'Figure 1(c) Proportion of samples in a small dataset....',  Detailed descriptions in the Figure 1 caption are better illustrated.\n3. Better re-organize Figure 1 to save more space.\n\nI consider adjusting my score based on the clarification of DAMO and DoLa."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No ethics review needed."
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the issue of hallucinations in Large Vision-Language Models (VLMs) by analyzing their layer-wise prediction tendencies and decoding mechanisms.  The authors propose a decoding strategy inspired by the momentum concept in gradient descent optimizers, which ensures adaptive decoding consistency across layers."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The authors conduct a detailed layer-wise analysis of VLMs' prediction tendencies, identifying that hallucinations often emerge in the final layers due to significant shifts in decoding. This insight may provide a deeper understanding of the underlying mechanisms causing hallucinations.\n\n2. The proposed Decoding by Accumulating Activations MOmentum (DAMO) method is somewhat effective. By accumulating momentum across layers, DAMO ensures consistent decoding and reduces the impact of late-stage hallucinations."
            },
            "weaknesses": {
                "value": "1. The paper draws inspiration from the momentum concept and contrast decoding, but it lacks a detailed analysis and comparison with recent related work, such as [1, 2]. Additionally, the paper does not adequately explain why the proposed method theoretically offers advantages over previous approaches.\n\n2. Poor presentation: The paper lacks comprehensive framework diagrams that clearly illustrate the specific content and workflow of the proposed method, making it difficult to understand the method's details.\n\n3. The paper does not provide a detailed analysis of how sensitive DAMO is to hyperparameters, such as the momentum coefficient. Understanding the optimal settings for different models and tasks could help in practical implementation.\n\n\n[1] Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization\n\n[2] Ibd: Alleviating hallucinations in large vision-language models via image-biased decoding"
            },
            "questions": {
                "value": "Please refer to Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}