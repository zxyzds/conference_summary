{
    "id": "wTm4W39GdD",
    "title": "Emergence of Hierarchical Emotion Representations in Large Language Models",
    "abstract": "As large language models (LLMs) increasingly power emotionally engaging conversational agents, understanding how they represent, predict, and potentially influence human emotions is critical for their ethical deployment in sensitive contexts. In this work, we reveal emergent hierarchical structures in LLMs' emotion representations, drawing inspiration from psychological theories of emotion. By analyzing probabilistic dependencies between emotional states in LLM outputs, we propose a method for extracting these hierarchies. Our results show that larger models, such as LLaMA 3.1 (405B parameters), develop more intricate emotion hierarchies, resembling human emotional differentiation from broad categories to finer states. Moreover, we find that stronger emotional modeling enhances persuasive abilities in synthetic negotiation tasks, with LLMs that more accurately predict counterparts' emotions achieving better outcomes. Additionally, we explore the effects of persona biases\u2014such as gender and socioeconomic status\u2014on emotion recognition, revealing that LLMs can misclassify emotions when processing minority personas, thus exposing underlying biases. This study contributes to both the scientific understanding of how LLMs represent emotions and the ethical challenges they pose, proposing a novel interdisciplinary perspective on the issue.",
    "keywords": [
        "LLM",
        "emotion"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wTm4W39GdD",
    "pdf_link": "https://openreview.net/pdf?id=wTm4W39GdD",
    "comments": [
        {
            "summary": {
                "value": "This study reveals key advancements in how LLMs perceive, predict, and influence human emotions. As model size increases, LLMs develop hierarchical emotional representations consistent with psychological models. The research highlights that personas can bias emotion recognition, underscoring the risk of stereotype reinforcement. Additionally, the study demonstrates that LLMs with refined emotional understanding perform better in persuasive tasks, raising ethical concerns about potential manipulation of human behavior. These insights call for robust ethical guidelines and strategies to mitigate risks of emotional manipulation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The writing in this paper is clear, and the figures are intuitive, making the author's ideas easy to understand.\n\n2. The paper astutely identifies that LLMs' potential to comprehend emotions could enhance their capacity to manipulate emotions, which provides critical ethical considerations for the further development of LLMs.\n\n3. The proposed hierarchical emotion extraction method appears simple and effective, offering a powerful tool for further analysis."
            },
            "weaknesses": {
                "value": "1. After reading the introduction, I expected Chapter 3 to discuss the model's ability and limitations in **perceiving** emotions, especially focusing on the circumstances under which the model fails. However, the paper mainly discusses how larger models outperform smaller ones in understanding emotions, which is rather obvious and does not provide sufficient novel insight.\n\n2. Chapter 4 employs synthetic data for testing but lacks sufficient quality validation. Including human and LLM prediction accuracy in a figure, such as Figure 6, would be beneficial, even if only for a subset.\n\n3. The contributions of the paper are somewhat scattered, covering three different aspects, but the discussions on these points are inadequate. Given that \u201cinfluencing human emotions\u201d is highlighted as a major contribution, I expected more extensive coverage on this topic. While I understand that involving human subjects may incur additional costs, drawing conclusions solely from LLM dialogues in isolated scenarios lacks persuasiveness. This section also lacks deeper analysis."
            },
            "questions": {
                "value": "1. What is the underlying principle behind Chapter 3? Your algorithm extracts more nuanced and hierarchical emotional information, but can you elaborate on what further conclusions can be drawn from this? If I understand correctly, does the model's ability to use more emotion-related vocabulary lead to greater hierarchical richness?\n\n2. Chapter 4 provides quantitative analysis from multiple perspectives, but could you offer specific examples of how different character background settings lead to different model emotion predictions? This would help provide more substantial insights.\n\n3. Could you clarify what new insights your experiments provide to advance previous work in perceiving, predicting, and potentially influencing human emotions? Some aspects have been discussed individually in previous studies."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper explores the development of hierarchical emotion representations in large language models (LLMs), particularly focusing on models like LLaMA 3.1 with up to 405B parameters. The authors propose methods to extract emotion hierarchies from LLM outputs by analyzing probabilistic dependencies between emotional states. They claim that larger models exhibit more intricate emotional hierarchies resembling psychological theories of emotion. Additionally, the paper examines the impact of persona biases (e.g., gender, socioeconomic status) on emotion recognition and explores the relationship between emotional modeling and persuasive abilities in synthetic negotiation tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Innovative Approach: The paper introduces a novel and interesting methodology for extracting hierarchical structures of emotions from LLMs, bridging computational models with psychological frameworks.\n\n2. Relevance and Timely: The topic is timely, addressing the intersection of AI, emotion modeling, and ethics."
            },
            "weaknesses": {
                "value": "1. Emotion Extraction Technique Concern: The method for extracting hierarchical structures based on next-word probabilities lacks rigorous justification. There is no comparison with alternative methods or validation.\n\n2. Threshold Selection: The paper sets a threshold (0 < t < 1) for determining parent-child relationships but does not explain how this threshold is chosen or its impact on the results.\n\n3. Quantitative Metrics: Although the visual representations of emotion hierarchies are compelling, incorporating additional quantitative metrics or comparisons with human-annotated emotion hierarchies could provide stronger validation of the proposed method.\n\n4. The font in Figure 2 is too small to see easily."
            },
            "questions": {
                "value": "1.  Can you provide a more detailed justification for using next-word probabilities to extract hierarchical emotion structures? \n\n2. How did you determine the appropriate threshold value (0 < t < 1) for establishing parent-child relationships between emotions? Was this threshold empirically validated?\n\n3. Besides visual representations, can you use some quantitative metrics to validate the integrity and accuracy of the extracted hierarchical emotion structures?\n\n4. Besides emotion, I guess your method can visualize the structure of other entities. Can you extend this part more to enlarge the generalization of your method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies the emergence of hierarchical emotional representations in large language models and explores their abilities to predict and manipulate emotions. The focus of this study is on models such as LLaMA and GPT, analyzing their emotional hierarchies and the potential biases they may exhibit when identifying emotions of minority character roles. The study also assesses the performance of models in comprehensive negotiation tasks, revealing the correlation between emotional prediction accuracy and negotiation outcomes."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "\u2022  The analysis and extraction of the emotional hierarchy in LLaMA validate its similarity to human emotional structures, with the complexity of emotional hierarchy positively correlated with model parameter volume.\n\n\u2022  It validates that different roles and scenarios significantly affect LLMs\u2019 emotion recognition abilities, providing guidance for how to avoid such biases in the future.\n\n\u2022  It analyzes the connection between emotional prediction ability and persuasive ability in negotiation tasks, offering practical insights for the application of artificial intelligence in emotionally sensitive environments."
            },
            "weaknesses": {
                "value": "\u2022  The first two conclusions are quite obvious and lack in-depth exploration of their underlying causes. For example, what is the relationship between the breadth and depth of model emotional stratification and model parameters and pre-training corpora?\n\n\u2022  The discussion on ethics and biases is somewhat coarse in terms of categorization by region, ethnicity, cultural background, and other living conditions.\n\n\u2022  There is a lack of discussion on how to leverage LLMs\u2019 emotional prediction capabilities to optimize downstream dialogue tasks."
            },
            "questions": {
                "value": "\u2022  The bias experiment could be expanded to more detailed demographic attributes or a broader set of test roles.\n\n\u2022  The analysis of the relationship between emotional prediction and other abilities (such as negotiation, persuasion) could be further expanded, rather than being limited to sales.\n\n\u2022  The wording around ethical issues in the abstract and introduction could be strengthened by providing specific examples of potential real-world impacts.\n\n\u2022  The presentation of Fig 6 needs to be optimized, with biases of different roles not being prominent enough."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This study reveals three findings about emotional intelligence in large language models (LLMs) and its practical implications. First, as LLMs scale up, they develop hierarchical representations of emotions that align with psychological models. Second, the study uncovers how different personas (based on gender, socioeconomic status, etc.) can bias LLMs' emotion recognition, particularly showing systematic biases for minority attributes. Finally, through a synthetic sales negotiation task, the research demonstrates that better emotional prediction capabilities directly correlate with improved persuasion and negotiation outcomes."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "There are three main strengths of this paper:\n\n1. **High originality**: The exploration of hierarchical emotional representations in LLMs is novel and important. While previous studies have examined emotions in LLMs from various angles, none have investigated their hierarchical nature. Additionally, few works have explored the personalization of emotions, which this paper thoroughly investigates.\n\n\n2. **High quality and clarity**: The paper presents solid evidence through multiple experiments and maintains clear, fluid expression throughout.\n\n\n3. **Significant impact**: The findings on emotional hierarchy and personalized emotional biases provide valuable insights for future research and have important implications for LLMs' emotional reasoning and recognition."
            },
            "weaknesses": {
                "value": "There are three main weaknesses of this paper:\n\n1. **Limited data for emotion tree construction**: The study utilized 5,000 prompts to test 135 emotion types, resulting in an average of only 37.03 prompts per emotion type. This relatively small sample size per emotion suggests the need for expanded data collection to construct a more detailed and robust emotion tree.\n\n2. **Dataset limitations**: The study exclusively relies on GPT-4 generated datasets. It would benefit from incorporating data from real-world scenarios (such as EDOS[1], EmpatheticDialogues[2], and GoEmotions[3]) for experimental validation.\n\n3. **Format error**: (3.1) Citation formats require standardization (inconsistencies noted in lines 34, 48, 249, and 250); (3.2) Possible typo: \"eutral\" appears on line 362 (should this be \"neutral\"?)\n\n[1] A taxonomy of empathetic response intents in human social con\u0002versations\n\n[2] Towards empathetic open\u0002domain conversation models: A new benchmark and dataset.\n\n[3] Goemotions: A dataset of fine-grained emotions."
            },
            "questions": {
                "value": "1. **Methodology Consideration**: Did the research employ various prompt types when constructing the emotion tree? Given that LLMs are typically sensitive to prompting, different prompt structures might elicit varying responses. This could potentially affect the emotional relationships identified in the study - for example, the strong connection observed between fear and shock in Llama3.1_8b might be weakened or altered with different prompt formulations. Therefore, I suggest conducting an ablation study on prompt sensitivity to quantify how different prompts affect the emotional hierarchy.\n\n2. **Data Representation Query**: Considering that all the data used in the study was generated by GPT-4o, to what extent might this deviate from authentic human emotional expressions and patterns? I recommend that the authors compare GPT-4o generated data with existing human-annotated emotion datasets to quantify any differences. Additionally, human experts could evaluate the differences between GPT-4o generated data and real-world data."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}