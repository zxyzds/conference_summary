{
    "id": "chwxOOwzFR",
    "title": "Flex: End-to-End Text-Instructed Visual Navigation with Foundation Models",
    "abstract": "End-to-end learning directly maps sensory inputs to actions, creating highly integrated and efficient policies for complex robotics tasks. However, such models are tricky to efficiently train and often struggle to generalize beyond their training scenarios, limiting adaptability to new environments, tasks, and concepts. In this work, we investigate the minimal data requirements and architectural adaptations necessary to achieve robust closed-loop performance with vision-based control policies under unseen text instructions and visual distribution shifts.\nTo this end, we design datasets with various levels of data representation richness, refine feature extraction protocols by leveraging multi-modal foundation model encoders, and assess the suitability of different policy network heads. Our findings are synthesized in Flex (Fly-lexically), a framework that uses pre-trained Vision Language Models (VLMs) as frozen patch-wise feature extractors, generating spatially aware embeddings that integrate semantic and visual information. These rich features form the basis for training highly robust downstream policies capable of generalizing across platforms, environments, and text-specified tasks.\nWe demonstrate the effectiveness of this approach on quadrotor fly-to-target tasks, where agents trained via behavior cloning on a small simulated dataset successfully generalize to real-world scenes, handling diverse novel goals and command formulations.",
    "keywords": [
        "Robot learning",
        "Foundation Models",
        "Visual Navigation"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "This work investigates the data and model design requirements for robust end-to-end text-instructed vision-based control policies that effectively generalize across diverse settings, demonstrated in quadrotor fly-to-target scenarios.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=chwxOOwzFR",
    "pdf_link": "https://openreview.net/pdf?id=chwxOOwzFR",
    "comments": [
        {
            "summary": {
                "value": "This work studied robust multimodal representations for drones. It combines spatial and lexical features via patch-wise descriptors from VLMs. The training pipeline for closed-loop visual navigation agents that generalize across unseen environments, using real-time natural language instructions to achieve adaptability well beyond the training scope. Extensive experiments on drone fly-to-target tasks, showcasing the ability in generalization and real-world deployment"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper is well written and easy to read.\n2. The experiment part is good with analysis in feature robustness quality. Real-world experiments are given."
            },
            "weaknesses": {
                "value": "1. The motivation of using mask-based patch features is unclear to me. I think it can be simply replaced by ViT style patch feature encoding and attention-based interaction. And for some downstream policy nets like MLP, the spatial feature is eliminated by average pooling. Why the spatial dimension is necessary? \n2. Following above, I do not find any ablation studies on different feature designs."
            },
            "questions": {
                "value": "See weakness part."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces FLEX, a method for making drones understand and follow natural language commands. Instead of building everything from scratch, it cleverly uses an existing vision-language model (BLIP-2) to process both camera images and text instructions. The system was trained on simple simulated scenarios with just two colored balls, yet surprisingly, it worked well in real-world tests with various objects. The key innovation is how it breaks down camera images into patches and processes them alongside text commands, allowing the drone to understand both what it sees and what it's looking for."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Integrating visual navigation with the vision-language foundation model is an interesting direction.\n2. The performance of this paper is reasonable with targeting and query on a limited set of objects.\n3. The paper writing is in general good except the introduction.\n4. The paper evaluates their methods in both simulation and real scenarios."
            },
            "weaknesses": {
                "value": "1. The paper writing is strange where the introduction section is quite similar to related work.\n2. The training data seems to be rich with 1, 1M, 2, 2M, however those four datasets contain only a limited amount of target, and query, which I think is not enough for an open world navigation paper.\n3. In addition the paper didn't handle how to solve image sequences for navigation work, which means that they predict action frame by frame.\n4. One interesting thing is that in order to correctly navigate in the scene, actually we need to know the depth information, however, the amount of data the paper trained is not enough for that kinds of generalization."
            },
            "questions": {
                "value": "1. I am mostly interested on how the evaluation data is different from the training data. I am happy to fix my score, if I could understand the performance is not from scene memorization."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper conducts an empirical study on using features extracted by frozen Vision-Language Pretrained Models (VLMs) for downstream policy learning in object-guided quadrotor visual navigation. The policy is trained with synthetic data in simulation and demonstrates a real-world deployment example, providing an initial test of generalizability."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The motivation to investigate frozen VLM encoders for semantically rich and spatially aware embeddings without extensive retraining is clear and well-grounded, offering a promising approach to simplify training in complex robotic tasks.\n2. The experimental setup is thorough, exploring various dataset configurations and policy architectures, which provides valuable insights into design choices for multi-modal robotic policies."
            },
            "weaknesses": {
                "value": "1. Limited Task Scope: While the use of VLM features for visual navigation is compelling, the experiments are restricted to quadrotor object navigation. Broader experiments across different robotic tasks or navigation scenarios could help demonstrate Flex\u2019s general applicability, aligning with the paper\u2019s claims of \u201cclosed-loop visual navigation agents that generalize across unseen environments.\u201d\n2. Feature Extractor Generalizability: Although the study aims to investigate \u201csuitable feature extractors for text and vision-based robotics learning,\u201d the experiments focus solely on BLIP-2 and modifications to its Q-former. Including other VLMs, such as CLIP or ImageBind, would strengthen the claim of generalizability and demonstrate the adaptability of the proposed method across diverse VLM architectures.\n3. Task Complexity: In the current setup, the target is observable from the beginning, and language instructions are relatively simple. Visual navigation in complex settings involves learning semantic associations between language and visual cues and exploring the environment effectively to locate and reach the target. Here, the task primarily involves target identification and navigation, with limited indication of the benefits of end-to-end learning over modular approaches like GOAT. Clarifying the advantages of end-to-end learning in this setup would improve the paper\u2019s impact.\n4. Language Instruction Scope: The difficulty and complexity of language instructions in the dataset are not discussed. Unlike works in Zero-Shot Object Navigation (ZSON) or Zero-Shot Vision-and-Language Navigation, which leverage the language comprehension capabilities of VLMs, the findings in this paper are limited to basic instructions. This limitation makes it challenging to support claims of Flex being \u201cuser-interactive,\u201d as the current level of interaction falls short of handling more complex language instructions effectively."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}