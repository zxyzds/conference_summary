{
    "id": "hrGOMrfc2z",
    "title": "Interactive Dialogue Agents via Reinforcement Learning with Hindsight Regenerations",
    "abstract": "Recent progress on large language models (LLMs) has enabled dialogue agents to generate highly naturalistic and plausible text. However, current LLM language generation focuses on responding accurately to questions and requests with a single effective response.\nIn reality, many real dialogues are interactive, meaning an agent's utterances will influence their conversational partner, elicit information, or change their opinion. Accounting for how an agent can effectively steer a conversation is a crucial ability in many dialogue tasks, from healthcare to preference elicitation. Existing methods for fine-tuning dialogue agents to accomplish such tasks would rely on curating some amount of expert data. However, doing so often requires understanding the underlying cognitive processes of the conversational partner, which is a skill neither humans nor LLMs trained on human data can reliably do. Our key insight is that while LLMs may not be adept at identifying effective strategies for steering conversations a priori, or in the middle of an ongoing conversation, they can do so post-hoc, or in hindsight, after seeing how their conversational partner responds. We use this fact to rewrite and augment existing suboptimal data, and train via offline reinforcement learning (RL) an agent that outperforms both prompting and learning from unaltered human demonstrations. We apply our approach to two domains that require understanding human mental state, intelligent interaction, and persuasion: mental health support, and soliciting charitable donations. Our results in a user study with real humans show that our approach greatly outperforms existing state-of-the-art dialogue agents.",
    "keywords": [
        "offline reinforcement learning",
        "language models",
        "self-reflection"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We use offline RL to fine-tune LLMs to perform interactive dialogue tasks such as counseling and persuasion.i",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=hrGOMrfc2z",
    "pdf_link": "https://openreview.net/pdf?id=hrGOMrfc2z",
    "comments": [
        {
            "summary": {
                "value": "The paper looks at training an LLM to play the role of a conversational agent, particularly in situations which require some manipulation for lack of a better term of the user towards a goal, for example making a donation to charity. \n\nThe paper's main idea is that rather than looking at conventional RL based exploration guided only by a numeric reward function, to look at generated dialogs in full and to with \"hindsight\" identify turns where the dialog may be judged as having been suboptimal and steered the user away rather than towards the desired goal. Such data can then be regenerated from that sub-optimal turn forwards. This is done and that data is then used in an offline RL algorithm to update the LLM playing the role of the system. \n\nThe paper compares to some prior works and reports results on 2 datasets."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The observation that feedback is more precise at the level of dialog regenerations having seen a full \"rollout\" is valid, given reward based feedback alone is a more difficult search space to optimise. \n\nThe paper is clear in its motivations, compares against other published works and reports reasonable results based on an evaluation with different users."
            },
            "weaknesses": {
                "value": "One minor suggestion: it's claimed that the results in table 1 are statistically significant (line 453) however there's no detail given for how this was determined. This should be included."
            },
            "questions": {
                "value": "* Would doing RLHF/DPO on the bad versus better turn be a valid comparison to include here? By \"bad\" I mean the turn first identified as being sub-optimal in the hindsight stage, and by \"good\" I mean it's regenerated version. Unclear how similar this is to the offline ILQL algorithm which I admit to not looking up now. \n\n* what happens if you do multiple rounds of hindsight re-generation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a method for enhancing dialogue systems using pre-trained language models (LLMs) through hindsight reinforcement learning (RL).\nIt utilizes LLMs to simulate human responses and generate new dialogue samples, which are then used to improve or supplement the original dataset, addressing the problem of lacking effective strategies in dialogue systems. The authors conducted experiments on two challenging tasks and demonstrated the effectiveness of this method. Additionally, they showed how to adjust dialogue strategies based on user feedback to achieve more natural and effective conversational interactions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The article proposes a data augmentation method to improve the performance of dialogue agents.\n\n2. It significantly outperforms existing fine-tuning methods in terms of efficiency, naturalness, and usefulness."
            },
            "weaknesses": {
                "value": "1. The article's innovation is limited.\n\n2. The introduction of the article's methodology is not sufficiently clear.\n\n3. Table 1 is too long.\n\n4. The experimental analysis is insufficient.\n\n5. There is a lack of objective evaluation metrics."
            },
            "questions": {
                "value": "NA."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper explores training dialogue agents that use reinforcement learning (RL) to improve their interactive capabilities, especially in complex tasks like mental health support and charitable persuasion. Unlike standard models focused on one-off responses, the authors propose a method called hindsight regeneration. This approach involves augmenting existing dialogue datasets by having the model review conversations in hindsight, identifying optimal actions post-interaction, and suggesting better responses to refine the dataset. The method employs offline RL, avoiding costly real-time exploration, and allows agents to generate more effective strategies over multiple dialogue turns."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "By refining conversations after they occur, the model can learn from suboptimal dialogues and incorporate more effective conversational strategies. This approach allows for the development of more adaptive and goal-directed dialogue agents capable of managing complex, multi-turn interactions."
            },
            "weaknesses": {
                "value": "The main contribution of this paper seems to be only a dataset construction method, without any innovations in model or methodology. Since this is not a dataset track paper, the authors appear to be primarily reporting experimental results rather than providing a substantial contribution. If the authors can convince me otherwise, I would be willing to adjust my score."
            },
            "questions": {
                "value": "1. Although the author believes that BLEU and ROUGE metrics may not fully capture the model\u2019s performance in dialogue, including these metrics could strengthen the experimental results.\n\n2. It seems the author did not specify the datasets used for SFT and RL training.\n\n3. The author introduces a human evaluation conducted by a group of 15 users. Could the author provide details on these users\u2019 backgrounds, educational levels, and native languages? Additionally, was any consistency check performed on the scores given by these 15 individuals?\n\n4. The author\u2019s experiments use only LLaMA-7B as the base model. To demonstrate the generalizability of the approach, has the author considered adding other base models? For reference, here are a few that could be selectively added: LLaMA2-7B, LLaMA2-13B, LLaMA3-8B, Vicuna, WizardLLM, ChatGLM, MiniCPM, Qwen.\n\n5. In Section 5, the author states, \u201cNote that we train on a much smaller model than used in the prompting baselines, yet as we will show later, we still are able to outperform such more sophisticated LLMs.\u201d Given that the model size for ChatGPT-3.5 has never been disclosed and the model itself has gone through several iterations, this statement may risk overclaiming."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce a method to augment existing datasets with hindsight regenerations. This method uses three components (i.e., hindsight controller, forward model and reward model) and four steps (Section 4 and Figure 2 provide a good overview of the method), in short the hindsight controller proposes alternative actions (using the full dialogue), a forward model simulates completed dialogues, the reward model evaluates dialogues and the generate dialogues (plus the original once) are used as trajectories in an offline reinforcement learning to learn the final policy.\n\nThe authors compare the trained model with advanced CoT baselines, and three method and data ablations. The human evaluation results  shows improvements in two domains such Mental health support and Soliciting charitable donations"
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "Originality\n- the proposed hindsight methods is original (to the best of my knowledge) and provide an effective way to train RL based dialogue systems. \n\nClarity\n- the overall paper is clear and easy to follow. I would have preferred more detail on the RL methodology, especially in section 4.3 ( Policy Optimization), where the authors provided a compacted explanation for a well known concept in RL but less well-known when applied to LLMs."
            },
            "weaknesses": {
                "value": "Significance & Quality \n- The human evaluation lacks of rigor and details. The paper does not provide enough details on how these 15 users has been instructed nor from what demographic comes from (e.g., english proficiency, etc). This is important to evaluate the significance of the results. \n- The results provide in Table 1 shows high variance, and no t-test or annotator inter-agreement is provided. Also the fact that a GPT-3 based prompting (ProCoT) has lower fluency is strange, usually LLM gets high human evaluation number.  Nevertheless this might be an artifact of a small annotator pool.\n- Although not super accurate, the authors did not provide any automatic evaluation, this would have provided, even if minimal, a hook to compare different models."
            },
            "questions": {
                "value": "- How are the annotators instructed and trained for the task?\n- What is the statistical significance of the results in Table 1?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}