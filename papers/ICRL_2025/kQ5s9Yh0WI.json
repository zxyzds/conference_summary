{
    "id": "kQ5s9Yh0WI",
    "title": "LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs",
    "abstract": "Current long context large language models (LLMs) can process inputs up to 100,000 tokens, yet struggle to generate outputs exceeding even a modest length of 2,000 words. Through controlled experiments, we find that the model's effective generation length is inherently bounded by the sample it has seen during supervised fine-tuning (SFT). In other words, their output limitation is due to the scarcity of long-output examples in existing SFT datasets. To address this, we introduce AgentWrite, an agent-based pipeline that decomposes ultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to generate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we construct LongWriter-6k, a dataset containing 6,000 SFT data with output lengths ranging from 2k to 32k words. By incorporating this dataset into model training, we successfully scale the output length of existing models to over 10,000 words while maintaining output quality. We also develop LongBench-Write, a comprehensive benchmark for evaluating ultra-long generation capabilities. Our 9B parameter model, further improved through DPO, achieves state-of-the-art performance on this benchmark, surpassing even much larger proprietary models. In general, our work demonstrates that existing long context LLM already possesses the potential for a larger output window--all you need is data with extended output during model alignment to unlock this capability.",
    "keywords": [
        "long context",
        "large language model",
        "long-form generation"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-19",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=kQ5s9Yh0WI",
    "pdf_link": "https://openreview.net/pdf?id=kQ5s9Yh0WI",
    "comments": [
        {
            "summary": {
                "value": "This paper studies the problem of long-form generation for large language models (LLMs). The authors find correlation between the output length limitation of current LLMs and the scarcity of long-output examples in existing supervised fine-tuning datasets. As a remedy, they generate long-output texts by decomposing ultra-long generation (over 10k words)  tasks into subtasks and prompt LLMs for each subtask, and then fine-tune LLMs with the long output. Preference tuning with DPO is also applied. They measure the required output length and the quality of long-form generation judged by GPT-4 as the main metrics. As a result they can enable a 9B/8B parameter model to generate very long sequences over 10k words.\n\nEssentially, the paper focuses on data augmentation for fine-tuning LLMs to a specific problem, which is long-form generation to tens of thousands of words. The idea is straightforward to mix in training examples of ultra-long sequences from 2k to 32k words so that the model generation does not stop after around 2k words."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The focus of the paper is very clear. The paper presents very logical steps around the core idea of enabling LLMs to generate very long sequences. This makes the paper easy to understand (although I feel there might be too many closely related but different names such as LongWrite-Ruler, LongBench-Write, etc. that are a bit confusing).\n\n- The empirical results of demonstrating current LLMs\u2019 cap at around 2k output words provide interesting insights. And connecting it back to the training data distribution is very reasonable leading to the data augmentation used in the paper.\n\n- The data generation pipeline, AgentWrite, is useful for generating long sequences based on instructions. The experimental results are promising, showcasing mixing in long-output sequences can enable the model to generate longer outputs."
            },
            "weaknesses": {
                "value": "- The novelty of the paper is somewhat limited. I like the idea of enabling the model to generate ultra-long sequences, but essentially it boils down to adjusting the training data length distribution to be better correlated with the testing scenario. Model behaviors are following what models are being trained on, so data augmentation or adjusting training data distribution is always a basic solution.\n\n- The evaluation of ultra-long generation quality is less satisfactory, as it mainly relies on GPT-4. No variance of the LLM-based evaluation is provided, making it a bit hard to gauge the performance differences. From Table 3, it seems the generation quality under this metric is not improved with LongWriter, and the main improvement is on the model becoming able to generate longer sequences, which is not surprising since that was mixed in the training data (e.g. having training examples with end-of-generation token after 10k words instead of 2k).\n\n- Following the above point on long-form generation quality evaluation, there is not enough details provided for the human evaluation in Figure 9 either. Long-form generation is very hard to evaluate, even for humans for texts over 10k words. How can we guarantee that the GPT-4 and human evaluations are trustworthy?\n\n- There are certain baselines lacking for comprehensively comparing long-form generation, compromising the experimental claims. For example, direct comparisons with AgentWrite + other models, which can be a strong baseline with good quality indicated from results in Table 3. Also, for the NLL loss comparison, no baseline models were compared with.\n\n- Efficiency is another concern, which was also mentioned in the conclusion. The paper could discuss more efficiency details such as computational cost, although this might be an orthogonal direction."
            },
            "questions": {
                "value": "1. Line 194: \u201cwe collect 120 varied user writing prompts\u201d: how did you collect? \n\n2. Line 254: \u201cwe see that AgentWrite does not compromise the quality of the output while expanding its length\u201d: the scores do drop when the length is expanding in Table 2 right?\n\n3. In Table 3, it seems the quality score judged by GPT-4 does drop compared to other models such as GPT-4o and GLM-4-9B-chat. The main increase of scores for LongWriter come from respecting the required output length. This makes the argument of both length and quality a bit compromised. If users want to generate long output of good quality, it seems AgentWrite is already good or even better.\n\n4. In the paragraph lines 398-409, for NLL comparison, could you compare with other models to see how they score the long generations to make this metric more justifiable? \n\n5. In the ablation study in section 4.3.2, it mentions in lines 476-480 that \u201ctaching the model to first output its reasoning process before generating the writing content does not significantly improve task performance\u2026\u201d As I understand this claim is made solely with the GPT-4 evaluation, could there be a problem with the evaluation metric? In fact, I am wondering if the authors have more comments on how to evaluate ultra-long-form generations reliably.\n\n\n6. Typo: Line 189: \u201c(Introduced in Sec)\u201d\n\n7. Table 4: the green and red colors are not very friendly for accessibility reasons."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses a very crucial gap in LLM research of overcoming limitations regarding long form output token limits. The paper investigates the root cause for this limitation in a systematic manner and also comes up with a clever solution to augment LLM solutions to 2k+ token limits. An elaborate evaluation and scoring strategy - that takes into account both the number of tokens generated and the quality of those tokens - has been described, along with DPO alignment and intuitive ablation studies."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper presents a systematic approach towards investigating the reason behind limitations around long generations in LLMs. \n2. The proposed agentic pipeline is pretty intuitive and seems to solve the issue very effectively.\n3. Extensive validation checks and comparison of SOTA models against the finetuned models has been provided.\n4. Good work with seeing the lift provided by DPO alignment and further ablation studies to strengthen the hypothesis."
            },
            "weaknesses": {
                "value": "1. Need for Human Eval to assess AgentWrite quality - While the paper proposes AgentWrite for generating long-form content, the validation of output quality is primarily based on automatic metrics using GPT-4o as a judge. More rigorous human evaluation would strengthen the quality assessment.\n2. Dependency on proprietary models - The AgentWrite pipeline relies on GPT-4o for generating training data, which makes the approach dependent on proprietary models and potentially difficult to reproduce. Analysis and comparison with an open-sourced version of AgentWrite would be helpful to understand how scalable and adaptable the pipeline is.\n3. Need for Plan validation in Step 1 : The \"Plan\" phase of the pipeline comes up with the various subtasks needed for the instruction. There needs to be a validation step to assess the quality and relevance of the subtasks being generated in the first place. The \"quality metric\" defined later would not take this into account.\n4. More elaboration needed on the controlled experiment on Section 2 : this subsection mentions using GLM-4-9b as the base model and then its further finetuned with a subset of GLM-4\u2019s chat SFT data. Has the model not been fine-tuned on this data already? Wouldn't it just overfit on the specific data subset again and that might be the reason why the model performed better when you add longer output length instances in the sft data (due to iterative finetuning on the same instances)? This would nullify the hypothesis that the model\u2019s output limit is due to insufficient output length in the SFT data."
            },
            "questions": {
                "value": "1. How did you determine the optimal paragraph length range (200-1000 words) for AgentWrite? Were other ranges tested?\n2. Is there a theoretical upper limit to how much the output length can be scaled using your approach?\n3. Could you elaborate more on what motivated the choice of using token-level loss averaging instead of sequence-level averaging during training?\n4. Does the quality of generation degrade differently for different types of content (e.g., technical vs creative writing)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper addresses the limitation of current long context large language models (LLMs) in generating lengthy outputs despite their capacity to process extensive inputs. The authors identify the primary constraint as the scarcity of long-output examples in supervised fine-tuning (SFT) datasets. To overcome this, they introduce AgentWrite, an agent-based pipeline that decomposes ultra-long generation tasks into subtasks, enabling LLMs to produce coherent long outputs. They construct the LongWriter-6k dataset, containing 6,000 SFT data points with output lengths from 2k to 32k words, and incorporate it into model training, successfully scaling output length to over 10,000 words while maintaining quality. Additionally, they develop LongBench-Write, a benchmark for evaluating ultra-long generation capabilities, demonstrating their 9B parameter model's state-of-the-art performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is written clearly, making it easy to understand.\n2. This paper focuses on the very practical issue of model output length limitations and provides a systematic research approach."
            },
            "weaknesses": {
                "value": "I believe there's room for improvement in the experimental aspect.\n1. Is it possible to directly leverage AgentWrite to generate long responses with the target model, e.g., Llama-3.1-8B? Can it meet our requirements for output length and quality? Can the output be used to train the model? I think only having results from GPT4-o is not sufficient to demonstrate the effectiveness of AgentWrite.\n2. As shown in Table 3, the performance of LongWriter-9B (w/ and w/o DPO) and LongWriter-8B is worse than the original model in the [0, 500) subset, and the performance of LongWriter-9B is also worse in the [500, 2k) subset. Given that only \"over 1%\" of user prompts require such lengthy outputs, is it worth sacrificing performance on shorter texts? Furthermore, to me, LONGWRITER seems to be essentially about adjusting the model's generation behaviors with longer data plus using GPT4-o for knowledge distillation. The improvement in long-text generation quality through knowledge distillation seems trivial, and changing the model's generation behaviors in this way does not seem to be a good way to enhance the model's fundamental capabilities, as demonstrated by the declined performance on shorter outputs.\n\nMissing reference: Line 189, \"Sec\""
            },
            "questions": {
                "value": "1. If we directly set min_new_tokens = max_new_tokens = target length during generation, how would the quality of output from various models be affected?\n2. I have doubts about LongWrite-Ruler: the current prompts provided are too vague. If we provide more detailed prompts, will the model generate longer outputs (also with higher quality)? For now, it feels like an assessment of the model's capability to follow instructions in generating outputs of a certain length. Also, as shown in Table 2, even after training the model with longer texts, the model's output still largely fails to meet the required length, which seems to confirm that the model lacks this instruction-following ability, rather than being incapable of generating the required output."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The SOTA LLMs can support the input lenght up to million tokens. However, their output length is limited to a few thousand, which greatly constrains their application in many areas. This work first investigates the reason of such limitation and found that the main reason is because the SFT training sample lenghts are limited. To address this issue, this work introduces an agentic pipleine, AgentWrite, that can generate long output responses by decomposing long generation tasks into subtasks. By using the data generated by this pipeline to train SFT LLMs, the trained LLMs are able to generate responses over 10,000 words while maintaining output quality.\nThe main contributaions of this work include the following:\n1. Analyze and identify the primary reason limiting the output lenght of LLMs.\n2. It proposes an agentic framework to construct SFT samples with long output lengths. It also contributes the LongWriter-6k dataset.\n3. The empirical results show that the proposed approach is able to scale the current LLMs output lenght to up to 10,000 words."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This work studies an interesting question of LLM output length limitation, compared to othere works that focus on input lenght. It investigates and finds the reason why output lenght is limited for current LLMs. Specifically, the output lenght is mainly limited by the output length of the SFT training samples.\n2. Besides finding the reason of model response lenght limitation, this work also proposes the AgentWrite approach for generating long response SFT data. LLMs trained with this dataset is able to scale its output lenght. The empirical results also show its efficacy.\n3. The experiments involves 4 proprietary and 5 open-source models, which makes the conclusions/results of the experiments more convincing."
            },
            "weaknesses": {
                "value": "1. This work including experiments focsues on only one task, i.e., writing. However, it's not clear how generalizable this approach is to other areas such as coding, which usualy has long output length, especially for large coding projects. Conducting experiments on more tasks can make the proposed approach more compelling.\n2. The related work section doesn't mention if there's any exsiting work on this topic. If yes, they should be used as baseline. If no, it should be mentioned explicitly."
            },
            "questions": {
                "value": "Please see the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}