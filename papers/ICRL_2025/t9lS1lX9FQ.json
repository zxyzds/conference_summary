{
    "id": "t9lS1lX9FQ",
    "title": "Node Identifiers: Compact, Discrete Representations for Efficient Graph Learning",
    "abstract": "We present a novel end-to-end framework that generates highly compact (typically 6-15 dimensions), discrete (int4 type), and interpretable node representations\u2014termed node identifiers (node IDs)\u2014to tackle inference challenges on large-scale graphs. By employing vector quantization, we compress continuous node embeddings from multiple layers of a Graph Neural Network (GNN) into discrete codes, applicable under both self-supervised and supervised learning paradigms. These node IDs capture high-level abstractions of graph data and offer interpretability that traditional GNN embeddings lack. Extensive experiments on 34 datasets, encompassing node classification, graph classification, link prediction, and attributed graph clustering tasks, demonstrate that the generated node IDs significantly enhance speed and memory efficiency while achieving competitive performance compared to current state-of-the-art methods. We provide our code in the supplementary material and will make it publicly available upon acceptance.",
    "keywords": [
        "Graph Neural Networks",
        "Graph Tokenizers",
        "Symbolic Compression",
        "Efficient Graph Learning"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "We propose an end-to-end framework that generates compact (6-15 dimensions), discrete (int4), and interpretable node representations, offering competitive performance and improved efficiency across tasks.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=t9lS1lX9FQ",
    "pdf_link": "https://openreview.net/pdf?id=t9lS1lX9FQ",
    "comments": [
        {
            "comment": {
                "value": "Dear Reviewer w4zV,\n\nWe sincerely appreciate your valuable feedback and recognition of our contributions. We hope our response below will further enhance your confidence in our work.\n\n**W1: Potential Loss of Information**\n\n> The quantization of node embeddings into discrete node IDs may result in the loss of important structural information, as nuanced variations in node characteristics could be oversimplified into a single codeword.\n\nThis is a valid point. Our approach may lead to some information loss, as the quantized node represenations are highly compact and discrete. Our theoretical and empirical analyses demonstrate that these representations can preserve essential information in GNN embeddings while reducing redundancy, potentially offering a high-level abstraction userful for various downstream tasks. As Reviewer K2Gp noted, \"as a VQ paper, the goal is not to really 'beat' anything \u2013 just do more with less.\"\n\nInterestingly, this abstraction can **sometimes exceed the performance of the base GNNs**, as demonstrated in Table 1 with heterophilic datasets and in Table 5's attributed graph clustering tasks. This phenomenon is intriguing and warrants further exploration.\n\n\n**W2 & Q1: Computational Complexity of Joint Training**\n\n> The framework necessitates joint training of the GNN and vector quantization components, which raises concerns about increased time complexity. The authors should provide an analysis of this complexity, particularly focusing on the impact of joint training on computational efficiency and scalability with larger datasets.\n>\n> How does the joint training of the GNN and vector quantization components affect the overall computational efficiency, especially when scaling to larger datasets?\n\nWe appreciate your suggestion. In fact, we already analyzed the computational complexity of the joint training of the GNN and vector quantization components in **Appendix E.2**. There, we compared the training time per epoch of our $\\text{NID}_\\text{SAGE}$ with standard SAGE on the largest dataset used in our experiments, ogbn-products, which contains 2,449,029 nodes. The results show that the training time of our NID model (5.9s) is comparable to that of standard SAGE (5.8s), indicating minimal computational overhead from our vector quantization components.\n\nFollowing your suggestion, we have also performed additional experiments on another two large-scale datasets, ogbn-proteins and pokec, to further evaluate the efficiency of our approach. The results below confirm that our method scales efficiently without significant increases in computational demands.\n\n| The empirical training runtime per epoch | ogbn-proteins | ogbn-products | pokec      |\n| ---------------------------------------- | ------------- | ------------- | ---------- |\n| \\# nodes                                 | 132,534       | 2,449,029     | 1,632,803  |\n| \\# edges                                 | 39,561,252    | 61,859,140    | 30,622,564 |\n| SAGE                                     | 0.45s         | 5.8s          | 2.7s       |\n| $\\text{NID}_\\text{SAGE}$                 | 0.51s         | 5.9s          | 3.1s       |"
            }
        },
        {
            "comment": {
                "value": "Dear Reviewer k2Gp,\n\nWe sincerely appreciate your careful reading and attention to detail. Your positive comments are truly encouraging. We understand your concern regarding the surprisingly high results of vanilla GNNs, and we would like to address this.\n\n**W1, W2 & Q1: Clarification of Experimental Result**\n\n> Egregious experimental result reuse raises a number of inconsistencies for the few methods the authors have recomputed.\n>\n> Its difficult to place the results in context with the related work (ie the Polynormer paper results).\n>\n> The core issue is that in this paper you show weak methods (e.g. GCN) performing well on datasets. Lets take Photos for example. In Polynormer, GCN is the weakest method (92%). In your paper, GCN is the strongest method (96%). This could be because you did a better grid search (good!) but if you don't similarly do a great grid search for the other baselines then I don't trust them at all :)\n>\n> It makes it very hard understand how your method compares at all to the baselines. (As a VQ paper the goal is not to really \"beat\" anything -- just do more with less)\n\nFirstly, we would like to address the **misconception** that basic GNNs, such as GCN, are inherently weak models. Recent benchmarking research [1] demonstrates that classic GNNs, including GCN, GAT, and SAGE, are actually **strong** models that can compete with or even surpass the latest state-of-the-art graph transformers (e.g., SGFormer or Polynormer) in node classification, when tuned within the same hyperparameter search space. This suggests that the performance of basic GNNs has been significantly underestimated due to *suboptimal hyperparameter tuning*.\n\nAs stated in lines 355-357 of our manuscript, we follow the experimental settings outlined in [1], as they match the hyperparameter search space used in the Polynormer paper. This approach ensures consistency and fair comparison, avoiding conducting a more extensive grid search for GNNs than for other models like Polynormer. \n\nTo further resolve your concern, we have also retrained Polynormer using the same hyperparameter search space outlined in [1] on our devices. The results align with those reported in [1], as detailed in the table below. This reaffirms our conclusion that NID performs competitively with SOTA methods on both homophilic and heterophilic graphs.\n\n|                         | Cora\u2191        | CiteSeer\u2191    | PubMed\u2191      | Computer\u2191    | Photo\u2191       | CS\u2191          | Physics\u2191     | WikiCS\u2191      | Squirrel\u2191    | Chameleon\u2191   | Ratings\u2191     | Questions\u2191   |\n| :---------------------- | :----------- | :----------- | :----------- | :----------- | :----------- | :----------- | :----------- | :----------- | :----------- | :----------- | :----------- | ------------ |\n| Polynormer              | 88.11 \u00b1 1.08 | 76.77 \u00b1 1.01 | 87.34 \u00b1 0.43 | 93.18 \u00b1 0.18 | 96.11 \u00b1 0.23 | 95.51 \u00b1 0.29 | 97.22 \u00b1 0.06 | 79.53 \u00b1 0.83 | 40.87 \u00b1 1.96 | 41.82 \u00b1 3.45 | 54.46 \u00b1 0.40 | 78.92 \u00b1 0.89 |\n| Polynormer (retrained)  | 88.32 \u00b1 0.96 | 77.02 \u00b1 1.14 | 88.96 \u00b1 0.51 | 93.78 \u00b1 0.10 | 96.57 \u00b1 0.23 | 95.42 \u00b1 0.19 | 97.18 \u00b1 0.11 | 80.26 \u00b1 0.92 | 41.97 \u00b1 2.14 | 41.97 \u00b1 3.18 | 54.96 \u00b1 0.22 | 78.94 \u00b1 0.78 |\n| GCN                     | 88.77 \u00b1 0.61 | 77.53 \u00b1 0.92 | 90.04 \u00b1 0.25 | 93.78 \u00b1 0.31 | 96.14 \u00b1 0.21 | 95.94 \u00b1 0.28 | 97.36 \u00b1 0.07 | 80.91 \u00b1 0.81 | 44.50 \u00b1 1.92 | 46.11 \u00b1 3.16 | 53.57 \u00b1 0.32 | 77.40 \u00b1 1.07 |\n| $\\text{NID}_\\text{GCN}$ | 87.88 \u00b1 0.69 | 76.89 \u00b1 1.09 | 89.42 \u00b1 0.44 | 93.41 \u00b1 0.08 | 96.17 \u00b1 0.04 | 95.52 \u00b1 0.10 | 97.34 \u00b1 0.04 | 78.55 \u00b1 0.15 | 45.09 \u00b1 1.72 | 46.29 \u00b1 2.92 | 53.55 \u00b1 0.13 | 96.85 \u00b1 0.10 |\n| GAT                     | 88.22 \u00b1 1.24 | 77.08 \u00b1 0.84 | 89.47 \u00b1 0.25 | 93.53 \u00b1 0.18 | 96.27 \u00b1 0.15 | 94.46 \u00b1 0.14 | 97.17 \u00b1 0.09 | 80.98 \u00b1 0.83 | 38.72 \u00b1 1.46 | 43.44 \u00b1 3.00 | 54.88 \u00b1 0.74 | 78.35 \u00b1 1.16 |\n| $\\text{NID}_\\text{GAT}$ | 87.35 \u00b1 0.57 | 76.13 \u00b1 1.35 | 88.97 \u00b1 0.36 | 93.38 \u00b1 0.16 | 96.47 \u00b1 0.27 | 94.75 \u00b1 0.16 | 97.13 \u00b1 0.08 | 79.56 \u00b1 0.43 | 37.68 \u00b1 2.04 | 42.83 \u00b1 3.42 | 54.92 \u00b1 0.42 | 97.03 \u00b1 0.02 |\n\n[1] Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification, NeurIPS 2024 Datasets and Benchmarks Track."
            }
        },
        {
            "summary": {
                "value": "This paper proposes a vector quantization for GNN representations created from different \"depths\" of the network at each node.  Its similar to other recently proposed works (I'm most reminded of VQGraph), but its not trained with a reconstruction loss.  Extensive experimentation is provided, but it seems like most baselines are just copied from tables in recent work.  In general the quantization seems to work effectively, performing at about the same level as the original GNN.\n\nNote: Interestingly when a baseline is recomputed by the authors (e.g. GCN) it seems to differ from the result reported in the paper the majority of the results come from (Polynormer).  This raises a substantial red flag, as the author's method should only perform as well as the baseline it quantitizes.  If we instead assume the author's methods perform within epsilon of the corresponding baseline in Polynormer, they would not be superior methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "+ very well written paper about a pressing topic (graph quantization)\n+ extensive results help provide details about the method\n+ I'm confident the method seems to work well (but perhaps doesn't actually win on baselines, see weaknesses)"
            },
            "weaknesses": {
                "value": "- Egregious experimental result reuse raises a number of inconsistencies for the few methods the authors have recomputed.\n- Its difficult to place the results in context with the related work (ie the Polynormer paper results)."
            },
            "questions": {
                "value": "Nice paper.  However when I dove into the experimental results of the Polynormer paper it raised a lot of questions.\n\n1. The core issue is that in this paper you show weak methods (e.g. GCN) performing well on datasets.  \nLets take Photos for example.  In Polynormer, GCN is the weakest method (92%).  In your paper, GCN is the strongest method (96%).  This could be because you did a better grid search (good!) but if you don't similarly do a great grid search for the other baselines then I don't trust them at all :)\n\nIt makes it very hard understand how your method compares at all to the baselines.  (As a VQ paper the goal is not to really \"beat\" anything -- just do more with less)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new framework that creates compact and interpretable node representations, called node IDs, using vector quantization to convert GNN embeddings into discrete codes. Experiments across 34 datasets show that these node IDs improve speed and memory efficiency while maintaining competitive performance in various graph tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper introduces a new framework for generating highly compact, discrete node representations (node IDs) that effectively addresses the inference challenges encountered in large-scale graph applications.\n2. The authors conduct comprehensive experiments across 34 diverse datasets and tasks, demonstrating the effectiveness of their method. \n3. The paper is well-structured and easy to read."
            },
            "weaknesses": {
                "value": "1. The quantization of node embeddings into discrete node IDs may result in the loss of important structural information, as nuanced variations in node characteristics could be oversimplified into a single codeword. \n2. The framework necessitates joint training of the GNN and vector quantization components, which raises concerns about increased time complexity. The authors should provide an analysis of this complexity, particularly focusing on the impact of joint training on computational efficiency and scalability with larger datasets."
            },
            "questions": {
                "value": "How does the joint training of the GNN and vector quantization components affect the overall computational efficiency, especially when scaling to larger datasets?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes an end-to-end framework NID, which represents nodes with highly compact, discrete vectors, to deal with the inference challenges on large-scale graphs. Specifically, vector quantization is adopted to resident embeddings in each layer of the GNN, and NID can be optimized in either self-supervised or supervised learning ways. Extensive experiments show the effectiveness of the proposed framework."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe paper is well-written and solid, especially practical in the case of large-scale graphs.\n2.\tThe overview of the proposed framework helps readers to understand the main pipeline of the work.\n3.\tThe experiment part is sufficient to clarify the effectiveness and efficiency of NID."
            },
            "weaknesses": {
                "value": "1.\tThe interpretability of the discrete representations of NID is not discussed in detail in the paper. As the authors point out that the most existed real-valued embeddings often lack interpretability, there should be a detailed discussion.\n2.\tSome concepts in the paper are vague. Refer to questions for more details. \n3.\tThe introduction of codebook seems useless since the codeword is used in neither loss of VQ nor loss of tasks."
            },
            "questions": {
                "value": "1.\tWhat does it mean by \u201cshare similar 1-hop structures\u201d in line 71 in the description of Figure 1? Why node in case A&B, C&D have similar 1-hop structure? What is the definition of so-called similar 1-hop structures? It is vague.\n2.\tLet\u2019s extend the first question, so if two nodes have exactly the same second ID code in the figure 1 (but the first ID code is different), does that mean they have \u201csimilar 2-hop structures\u201d? If yes, how can they have similar 2-hop structures on the basis of totally different 1-hop neighbors? Which neighbors of the 1-hop neighbors should be considered? If no, what does the same second ID code mean in that case?\n3.\tThe definition of the M is vague. What does M mean in the size of codebook L*M?\n4.\tIn subsection 3.2, the paper says that the Node_ID of each node will be sent into the MLP network, but my question is are you sending the id like \u201c2341\u201d (eg. 4-layer encoder) as the node representation into the MLP network? Why don\u2019t you send the corresponding codeword into the MLP network? I know that code vectors are not used for a reconstruction task, so I am here talking about the downstream tasks.\n5.\tWhat will happen if two different nodes have the exactly same Node_ID? Since the size of each codebook is certain, let\u2019s take figure 1 (2-dimensional) as an example, the codebook size of each layer is 5, then you actually can have 25 (5^2) different discrete representations. So there must be some nodes sharing the same Node_ID (collision). \n6.\tFor the argument in box 4.1, as I say, why directly use IDs as representations instead of corresponding vectors in the codebook?\n7.\tWhat is the interpretation of your discrete representations?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}