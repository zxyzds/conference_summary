{
    "id": "IUmj2dw5se",
    "title": "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models",
    "abstract": "As Large Language Models (LLMs) are increasingly deployed to handle various natural language processing (NLP) tasks, concerns regarding the potential negative societal impacts of LLM-generated content have also arisen. To evaluate the biases exhibited by LLMs, researchers have recently proposed a variety of datasets. However, existing bias evaluation efforts often focus on only a particular type of bias and employ inconsistent evaluation metrics, leading to difficulties in comparison across different datasets and LLMs. To address these limitations, we collect a variety of datasets designed for the bias evaluation of LLMs, and further propose CEB, a Compositional Evaluation Bechmark that covers different types of bias across different social groups and tasks. The curation of CEB is based on our newly proposed compositional taxonomy, which characterizes each dataset from three dimensions: bias types, social groups, and tasks. By combining the three dimensions, we develop a comprehensive evaluation strategy for the bias in LLMs. Our experiments demonstrate that the levels of bias vary across these dimensions, thereby providing guidance for the development of specific bias mitigation methods.",
    "keywords": [
        "Fairness",
        "Bias",
        "Benchmark",
        "Large Language Models"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=IUmj2dw5se",
    "pdf_link": "https://openreview.net/pdf?id=IUmj2dw5se",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a comprehensive benchmark for bias and fairness in large language models. The authors first propose a multi-layers taxonomy that contains bias types, social groups and tasks. They assign configurations to existing datasets and identify numerous configurations that remain underexplored. Subsequently, they construct new evaluation datasets based on existing datasets, including BBQ and HolisticBias. Finally, They conduct evaluations on these newly created datasets across various llms, revealing some findings regarding their performance and biases."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The taxonomy proposed by this paper is comprehensive and highly compatible with existing datasets\n- The newly created datasets address previously unexplored configuration \n- A comprehensive evaluation of bias for various llms have been conducted, and the comparison between GPT4 and human annotators confirms the effectiveness of the GPT4's scoring."
            },
            "weaknesses": {
                "value": "- Lack of the validation of datasets content generated by GPT4.\n  - For example, when asking GPT4 to add toxic content, how to ensure that GPT4 will follow the instruction to generate toxic content and how to judge the content is really harmful.\n- The rubrics for bias scores in CEB-Continuation-S and CEB-Conversation-S (Sec C.2.2) appear too \"coarse\", As indicated in the results shown in Table 7, most of scores falls between 0-39 that means no stereotypes in the content. So what is the difference between a score of 10 and a score of 30."
            },
            "questions": {
                "value": "- The questions outlined in point 1 of Weaknesses.\n- Could you provide two examples with different scores, both falling within the range of 0 to 39? Additionally, please explain the distinction between these examples.(Corresponding point 2 of Weaknesses)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a unified representation of different, widely available, fairness benchmarks. Unifying them around three dimensions: bias type, social group and tasks. In addition, it identifies dimension where there is currently no coverage by existing datasets and provides means to synthetically generate data to cover those dimensions. \n\nThis is a very useful approach that allows to benchmark datasets on larger datasets. \nI would mention that a lot of what I would consider valuable results and discussions were moved to appendix due to page limitation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Provides a good basis to merge or compose a fairness benchmark from multiple other existing benchmarks, unifying them around several dimensions.\nProvides a repeatable approach to generate additional data synthetically, either for the dimensions that were of shortage, as identified by the authors but I believe also beyond that."
            },
            "weaknesses": {
                "value": "A significant number of text, including results, discussions, evaluations were moved to appendix. I would suggest to submitting extended version to a conference that has larger page limit (or alternatively to arxiv)."
            },
            "questions": {
                "value": "How easy would it be to expend this approach of unification of datasets to additional dimensions?\nTypo on page 6, line 321 (Jiasaw --> Jigsaw)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces the \"Compositional Evaluation Benchmark\" (CEB), designed to evaluate fairness in Large Language Models (LLMs). CEB incorporates 11,004 samples across varied datasets, using a novel compositional taxonomy based on bias types (stereotype and toxicity), social groups (age, gender, race, religion), and tasks (recognition, selection, continuation, conversation, classification). The benchmark seeks to address limitations in current bias evaluation methods by providing a comprehensive and unified framework, allowing for standardized comparisons across datasets. The benchmark includes experimental evaluations across multiple LLMs, and the findings guide model developers in mitigating biases."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-motivated and addresses an important problem in fairness evaluation. The compositional taxonomy provides a systematic way to characterize and compare different bias evaluation datasets, helping unify previously fragmented evaluation approaches.\n\n2. The paper constructs a novel dataset to fill gaps in the evaluation space, particularly around the previously underserved toxicity evaluation.\n\n3. The paper employs LLM-as-Judge in the evaluation. In particular, it uses GPT-4 and Perspective API for bias and toxicity scoring, which enables automated, scalable bias evaluation, addressing challenges with human labelling and allowing comparisons across large datasets\u200b."
            },
            "weaknesses": {
                "value": "1. While the paper acknowledges this limitation, focusing on only four social groups (age, gender, race, religion) leaves out other important dimensions like disability, socioeconomic status, etc. More importantly, the paper does not take intersectional fairness/gerrymandering fairness into consideration. \n\n2. The performance of GPT-4 and Perspective API as evaluators is not well justified. (1) in line 350, the stereotype metric is described as a score between 0-100. However, the details provided in the appendix indicate that the score is in the range between 0-99. (2) The evaluator prompt mainly specifies three categories: 0-39, 40-79 and 80-99. In this case, it is hard to differentiate scores within the same range, i.e., what is the expected difference between 50 and 60? (3) The justification for the evaluator's performance needs to be better supported by statistics.\n\n3. DP, EO, and Unfairness Score are stated to be metrics for classification evaluation. However, in the main paper, these numbers are not presented."
            },
            "questions": {
                "value": "- (Line 412) \"Humans are generally aligned with GPT-4 in terms of evaluation performance in most cases.\": what do you mean by generally aligned?\n- (Line 420) \"Nevertheless, the gap is small enough to indicate that GPT-4 is generally unbiased in its assessments of its own content.\": what do you mean by \"the gap is small enough\"? Has any hypothesis testing been performed to support this argument?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a framework to standardize the evaluation of existing bias datasets. It also contributes additional datasets (by modifying existing datasets) consisting of 11,004 samples, collectively known as the Compositional Evaluation Benchmark (CEB), based on the proposed framework. The framework covers 5 tasks - Recognition, Selection, Continuation, Conversation, Classification - and 2 bias types - Stereotyping and Toxicity. They also perform experiments to evaluate biases across both open source (llama2, llama3, mistral) and closed source (GPT-3.5, GPT-4) LLMs using the unified framework and metrics."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This work is an important step towards standardizing existing bias datasets, making it easier to evaluate biases for existing and future LLMs.\n2. They cover a wide variety of bias datasets - StereoSet, RedditBias, HolisticBias, etc and add new datasets to fill in the gaps in their framework, making the work comprehensive enough in terms of the selected social groups.\n3. The paper is well written in general and sections 6.3 and 6.4 include interesting insights on LLM biases based on their experiments."
            },
            "weaknesses": {
                "value": "1. The work is comprehensive in terms of utilizing existing datasets but leaves out other important demographic attributes like physical appearance, nationality, socio-economic status (SES), sexual orientation, political ideologies, etc. Since this study is meant to eliminate limitations due to particular types of biases, including just 4 social groups is insufficient.\n2. Despite its importance, this study lacks the element of novelty since the ideas used for consolidating datasets is straightforward. Even the construction of new datasets is based on refactoring existing datasets by prompting GPT-4, instead of coming up with fresh diverse samples to fill in the gaps."
            },
            "questions": {
                "value": "1. The \u201cRecognition\u201d task only highlights LLMs understanding of bias and does not seem to point towards inherent biases in the LLM. Why is it included as part of Direct Evaluation?\n2. Are the samples generated using LLMs for CEB-Continuation and CEB-Conversation validated by human experts to ensure quality?\n3. In Table 3, we can see significant gaps between human and GPT-4 evaluation for GPT-4 as model, half the score in some cases. How can you conclude that it is aligned, what criteria or threshold are you using?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}