{
    "id": "9pW2J49flQ",
    "title": "DeepLTL: Learning to Efficiently Satisfy Complex LTL Instructions",
    "abstract": "Linear temporal logic (LTL) has recently been adopted as a powerful formalism for specifying complex, temporally extended tasks in multi-task reinforcement learning (RL). However, learning policies that efficiently satisfy arbitrary specifications not observed during training remains a challenging problem. Existing approaches suffer from several shortcomings: they are often only applicable to the finite-horizon fragment of LTL, are restricted to suboptimal solutions, and do not adequately handle safety constraints. In this work, we propose a novel learning approach to address these concerns. Our method leverages the structure of B\u00fcchi automata, which explicitly represent the semantics of LTL specifications, to learn policies conditioned on sequences of truth assignments that lead to satisfying the desired formulae. Experiments in a variety of discrete and continuous domains demonstrate that our approach is able to zero-shot satisfy a wide range of finite- and infinite-horizon specifications, and outperforms existing methods in terms of both satisfaction probability and efficiency.",
    "keywords": [
        "reinforcement learning",
        "linear temporal logic",
        "ltl",
        "generalization"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "",
    "creation_date": "2024-09-24",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=9pW2J49flQ",
    "pdf_link": "https://openreview.net/pdf?id=9pW2J49flQ",
    "comments": [
        {
            "comment": {
                "value": "We thank you for the detailed feedback and are pleased that you found our approach \"compelling\" and our paper \"well-written and effectively presented\". Nevertheless, we would like to point out some possible misunderstandings that might have negatively impacted the assessment.\n\n**Comparison to [1], [2], and [3]**\n\nWe appreciate these references and agree that they are generally relevant in the context of our work. However, please note that these approaches tackle a **different/simpler problem** than the one we consider and are **not applicable** to our setting. In particular, our approach is realised in a **multi-task RL** setting and we train a single policy that can **zero-shot execute arbitrary unseen LTL specifications at test time**. In contrast, the methods in [1], [2], and [3] only learn a policy for a **single, fixed task**. This is a crucial difference: our approach trains a single policy once, which can then satisfy arbitrary tasks such as the ones in our evaluation (see Tables 2 and 3). The given references would have to train a separate new policy for every specification, and cannot generalise to new tasks at test time.\n\nWe have submitted an updated version of the paper to hopefully make this distinction clearer. The updated version includes a changed title that explicitly mentions multi-task RL, and minor corresponding edits to the abstract and introduction. We have also updated the related work section (Section 6) to more explicitly point out the differences to methods that only handle a single task, and include an extended discussion of related work in the appendix (see Appendix C in the updated paper), which includes the provided references.\n\n> In my opinion, these results appear to weaken the authors\u2019 claim that \u2018Our method is the first approach that is also non-myopic [...]'\n\nWe appreciate that previous methods that learn a policy for a single LTL specification are generally non-myopic and can handle infinite-horizon specifications. However, we are not aware of any non-myopic method that is able to satisfy infinite-horizon specifications in the multi-task RL setting. If you are aware of such a method, we would greatly appreciate a reference.\n\n**2D grid-world environments**\n\nPlease note that our experiments include the **high-dimensional ZoneEnv** environment with **continuous state and action spaces**, which is a standard environment in previous research on multi-task RL with LTL tasks (Vaezipoor et al. 2021, Qiu et al. 2023). This is a Mujoco environment consisting of a robot navigating a planar world with continuous acceleration and steering actions while observing sensory information, including lidar observations about various coloured zones. The state-space of this environment is 80-dimensional (compared to the 25-dimensional Fetch environment and the 5-dimensional Carlo environment). For a description of the environment see Section 5.1 and Appendix E.\n\nWe also consider the FlatWorld environment, which similarly features a continuous state space (albeit of lower dimensionality 2).\n\nWe therefore believe our experiments already demonstrate the performance of our approach in high-dimensional and continuous environments.\n\n> Additionally, as a minor note, there is a typo on line 066 of the paper; it should read (c) instead of (b).\n\nMany thanks, we fixed the typo in the updated version of the paper.\n\nWe appreciate your comments and hope that our response with the corresponding edits in the revised version of the paper has made our contribution clearer; in particular the difference to related work that handles only a single LTL specification, and our evaluation in high-dimensional environments. Please let us know if this mitigates your concerns!"
            }
        },
        {
            "comment": {
                "value": "Thank you for taking the time to review our paper! We provide answers to your questions below. In particular, we would like to point out some possible misunderstandings that might have negatively impacted the assessment.\n\n**Existing literature on $\\omega$-regular objectives**\n\nWe appreciate the given references and agree that they are relevant in the context of our work. However, please note that these approaches tackle a **different/simpler problem** than the one we consider and are **not applicable** to our setting. In particular, our approach is realised in a **multi-task RL** setting and we train a single policy that can **zero-shot execute arbitrary unseen LTL specifications at test time**. In contrast, the methods in [1-4] only learn a policy for a **single, fixed task**. This is a crucial difference: our approach trains a single policy once, which can then satisfy arbitrary tasks such as the ones in our evaluation (see Tables 2 and 3). The given references would have to train a separate new policy for every specification, and cannot generalise to new tasks at test time.\n\nWe have submitted an updated version of the paper to hopefully make this distinction clearer. The updated version includes a changed title that explicitly mentions multi-task RL, and minor corresponding edits to the abstract and introduction. We have also updated the related work section (Section 6) to more explicitly point out the differences to methods that only handle a single task, and include an extended discussion of related work in the appendix (see Appendix C in the updated paper), which includes the provided references.\n\nIn the context of multi-task RL with LTL specifications, which is the problem we consider in our paper, we are only aware of a single work that can handle $\\omega$-regular specifications (Qiu et al. 2023). As we discuss in the paper (Section 4.6 and lines 518-525) our approach has various theoretical advantages (also see below), and we demonstrate that it performs better in practice (see Table 1, Figure 6, Table 5, Figure 9).\n\n**Answers to questions**\n\n> Q1\n\nAs illustrated in Figure 4, the sequence module takes as input a reach-avoid sequence $\\sigma$ and outputs a corresponding embedding $e_\\sigma$ that is used to condition the policy. For example, if the current task is F (a & F b), the corresponding reach-avoid sequence is (({a}, {}), ({b}, {})) (where all avoid sets are empty). The sequence module maps this sequence to some embedding $e\\in\\mathbb R^n$ which conditions the trained policy to first reach proposition a and subsequently reach proposition b.\n\n> Q2\n\nWe mainly opted for RNNs for the sake of simplicity: while Transformers excel at long-distance tasks, the sequences arising from the LTL formulae we consider are generally relatively short (2-15 tokens). Furthermore Transformers are known to be difficult to train and require large amounts of training data (Liu et al. 2020). We also note that our choice is consistent with previous works, which mainly use GRUs for sequence modelling (Kuo et al. 2020, Vaezipoor et al. 2021, Xu and Fekri 2024).\n\n[1] Liu et al (2020). \u2018Understanding the Difficulty of Training Transformers\u2019. In *EMNLP'20*.\n\n> Q3\n\nLet $\\sigma$ be a truncated reach-avoid sequence that visits an accepting state $k$ times, and denote the length of $\\sigma$ as $n$. As per our training procedure (Section 4.4) we have that $i = n + 1$ iff the agent has satisfied all assignments in $\\sigma$, i.e. successfully finished \"executing\" the sequence. This means the agent has visited an accepting state $k$ times. The expected value\n$$\n\\mathbb E_{\\tau\\sim\\pi|\\sigma}\\left[ \\sum_{t=0}^\\infty \\mathbb 1[i = n+ 1]\\right]\n$$\nis thus exactly the probability of the policy reaching an accepting state $k$ times by following $\\sigma$.\n\n> Q4\n\nYes, in comparison to GCRL-LTL our method is **non-myopic** and considers **safety constraints during planning** (cf. Section 4.6 and lines 520-523). These theoretical advantages explain why our approach outperforms GCRL-LTL in terms of efficiency and satisfaction probability. In Appendix F.2 we also provide a further comparison to GCRL-LTL on tasks with safety constraints, which highlights the differences in the planning approaches.\n\n> Q5\n\nAs discussed above, our paper proposes a novel method for zero-shot execution of arbitrary LTL specifications in a multi-task RL setting. This is fundamentally different from the approaches implemented in [4], which train a policy for a single, fixed specification and cannot generalise to different tasks. Do you still think such a comparison is useful and required, given the fundamentally different problem statements?\n\nWe appreciate your comments and hope that our response along with the corresponding edits in the revised version of the paper has made our contribution clearer. If anything else is unclear, we would be more than happy to engage in further discussion. Please let us know if this mitigates your concerns!"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a method that leverages linear temporal logic (LTL) to formulate reinforcement learning (RL) tasks. The authors claim that their method is applicable to infinite-horizon tasks and are non-myopic. The preliminaries and problem setting are presented in a clear and logical flow, and the experimental results are well-reported. However, the authors seem to have completely missed highly relevant literature in this area (see references below)."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1) The paper presents an interesting approach to learn policies to satisfy omega-regular specifications based on visiting accept states in an automaton without discounting states between the visits. \n2) It incorporates policies parameterized as neural networks.\n3) It uses the structure of the automaton specification."
            },
            "weaknesses": {
                "value": "The main weakness of this paper is that it ignores significant body of literature that deals with training policies for omega-regular objectives. Without a detailed comparison, it is difficult to evaluate the novelty in this paper. In fact, the technique of discounting seems quite similar to the zeta parameter used in the Hahn et al. paper from TACAS 2019. The authors should clarify how their approach is different.\n\nReferences:\n1. Hahn, E. M., Perez, M., Schewe, S., Somenzi, F., Trivedi, A., & Wojtczak, D. (2019, April). Omega-regular objectives in model-free reinforcement learning. In International conference on tools and algorithms for the construction and analysis of systems (pp. 395-412). \n2. Hahn, E. M., Perez, M., Schewe, S., Somenzi, F., Trivedi, A., & Wojtczak, D. (2020). Faithful and effective reward schemes for model-free reinforcement learning of omega-regular objectives. In Automated Technology for Verification and Analysis: 18th International Symposium, ATVA 2020, Hanoi, Vietnam, October 19\u201323, 202\n3. Le, Xuan-Bach, Dominik Wagner, Leon Witzman, Alexander Rabinovich, and Luke Ong. \"Reinforcement Learning with LTL and $\\omega $-Regular Objectives via Optimality-Preserving Translation to Average Rewards.\" arXiv preprint arXiv:2410.12175 (2024).\n4. Hahn, E. M., Perez, M., Schewe, S., Somenzi, F., Trivedi, A., & Wojtczak, D. (2021). Mungojerrie: Reinforcement learning of linear-time objectives. arXiv preprint arXiv:2106.09161."
            },
            "questions": {
                "value": "Questions:\n1) In section 4.2 and 4.3, the explanation of the sequence module, which encodes reach-avoid sequence, is unclear. What are the inputs and the outputs of this module?  Could you provide an example to clarify?\n2) Why did you use an RNN? Transformer-based NN architectures outperform RNNs in many problems.\n3) In section 4.5, the statement \u201cthe value function is a lower bound of the discounted probability of reaching an accepting state k times via\u2026\u201d does not sound correct. How is the right hand side of the inequality equal to \u201cthe discounted probability of reaching an accepting state k times\u201d ? Can you explain your reasoning? \n4) GCRL-LTL also works for infinite-horizon tasks. The experiment results imply that your method outperforms GCRL-LTL. Is there a theoretical explanation for why your method is better than GCRL-LTL? \n5) It is difficult to evaluate the novelty of this paper without a thorough comparison to approaches such as those used in the tool Mungojerrie [4]. Will such a comparison be possible in a short time?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a multi-task RL approach using goals specified in Linear Temporal Logic. The approach builds on recent work by reasoning about *accepting cycles* in the form of reach-avoid sequences and learns a goal-conditioned policy that can generalize to unseen specifications by finding the highest-valued reach-avoid sequence in the new specification('s automata), where the reach-avoid sequence goals are cast as learned embeddings. The approach is trained in a multi-task setting with a simple curriculum, and experimental results demonstrate that the DeepLTL approach outperforms previous approaches to goal-conditioned LTL-modulo-RL."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper is overall well-written and nicely constructed. \n* The problem of multi-task RL is, in my opinion, one of the most salient applications of using structured logical specifications. I think the paper does a nice job of trying to extend this.\n* The paper does a god job contextualizing some of the recent theory (e.g. regarding the eventual discounting objective) and discussing the relevance of it in a practical context.\n* The idea of using embeddings, cyclical acceptance, and predicate-conditioned learning builds directly on recent work [1] [2] [3] and I think these principles are helpful in the aim to scale automata-driven RL further to large scale applications.\n\n[1] Compositional Automata Embeddings for Goal-Conditioned Reinforcement Learning. Yalcinkaya et. al 2024.\n\n[2] LTL-Constrained Policy Optimization with Cycle Experience Replay. Shah et. al 2024.\n\n[3] Instructing Goal-Conditioned Reinforcement Learning Agents with Temporal Logic Objectives. Qiu et. al. 2023"
            },
            "weaknesses": {
                "value": "Although building on very recent work is a good way to step the field forward, it does also beg a bit the question of significance. This work bears strong similarities to [3], with the primary change being to condition over reach-avoid sequences rather than individual atomic propositions or predicates that represent transitions within an automaton. The latter approach, which is what is done in [3], requires a planning-based approach each time a new automaton is seen. The authors do compare against [3] experimentally, and show that on individual challenging tasks their approach is better, which is appreciated. However, I'd like to see a more thorough experimental analysis of the DeepLTL approach itself. Since the DeepLTL approach is quite similar to prior work, this analysis-style work would greatly benefit the field. At what level of complexity of specification does the approach break down? Does a larger alphabet (and therefore a larger class of reach-avoid sequences) make the problem harder by expanding the space of possible embeddings?\n\nRegarding the writing: I don't think including the discussion on eventual discounting [4] (problem 3.1 and theorem 3.1) is totally necessary and the small extension of the theory that the authors provide is more or less orthogonal to their main contribution, which obscures the writing a bit. The authors use a discounted version of LTL as their objective but do not cite recent work that thoroughly explores this problem setting [5]. In section 4.1, the authors discuss reasoning over pre-computed accepting cycles, which bears strong similarities to an identical approach in [2]. Although [2] is cited it would be good for the authors to mention it in section 4.1 given these similarities.\n\nLastly, the approach from [1] is a highly similar approach to automata-goal-conditioned RL that also uses an embedding based approach. Although this work is contemporaneous, a previous version did appear [6] earlier and I think some sort of comparison, if not an explicitly direct one, would be important in strengthening this work.\n\n[1] Compositional Automata Embeddings for Goal-Conditioned Reinforcement Learning. Yalcinkaya et. al 2024.\n\n[2] LTL-Constrained Policy Optimization with Cycle Experience Replay. Shah et. al 2024.\n\n[3] Instructing Goal-Conditioned Reinforcement Learning Agents with Temporal Logic Objectives. Qiu et. al. 2023.\n\n[4] Eventual Discounting Temporal Logic Counterfactual Experience Replay. Voloshin et. al. 2023.\n\n[5] Policy Synthesis and Reinforcement Learning for Discounted LTL. Alur et. al. 2023.\n\n[6] Automata Conditioned Reinforcement Learning with Experience Replay. Yalcinkaya et. al. 2023."
            },
            "questions": {
                "value": "* Can the authors compare against [1]/[6] in the previous section(s) and reason about why their approach may be preferable? The approaches are different in how they condition and compute embeddings but an argument by the authors advocating their own approach is important given the similarity of the work.\n* The authors include a curriculum-based ablation in the appendix that supports the presence of a curriculum. What other choices of curricula were considered? Do the authors have ideas on how a choice of curriculum would affect learning? \n* Section D.3 in the appendix seems to be missing. Can the authors provide this?\n* At what level of complexity of specification does the deepLTL approach break down? Does a larger alphabet (and therefore a larger class of reach-avoid sequences) make the goal-conditioned RL problem harder by expanding the space of possible embeddings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a novel approach, called DeepLTL, to address the challenge of learning policies that ensure the satisfaction of arbitrary LTL specifications over an MDP. This approach reduces the myopic tendencies found in previous works by representing each specification as a set of reach-avoid sequences of truth assignments. It then leverages a general sequence-conditioned policy to execute arbitrary LTL instructions at test time. Extensive experiments demonstrate the practical effectiveness of this approach."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "The proposed approach is tailored to address key challenges of quality, clarity, and significance. Unlike existing techniques, this method is designed to handle infinite-horizon specifications and mitigate the non-myopic tendencies of previous approaches that often lead to sub-optimality. Additionally, it naturally incorporates safety constraints, represented through negative assignments, to guide the policy on propositions to avoid, which is an essential concept for effective planning. In general, the paper is well-written and effectively presented."
            },
            "weaknesses": {
                "value": "The approach proposed by the authors is compelling and aims to address an important problem. However, one concern is that the authors appear unaware of works like [1], [2], and [3], which introduced model-free reinforcement learning (RL) methods to tackle the same challenge of maximizing the probability of satisfaction for LTL specifications, expressed as B\u00fcchi automata and deterministic parity automata. These methods have even been extended to nondeterministic, adversarial environments (expressed as stochastic games) where nonrandom actions are taken to disrupt task performance, beyond standard MDPs. In such approaches, the LTL specifications are translated into limit-deterministic B\u00fcchi automata (LDBAs) to form product MDPs. Rewards are derived from automata using a repeated reachability acceptance condition, allowing controller strategies that maximize cumulative discounted rewards to also maximize satisfaction probabilities; standard RL algorithms are then used to learn these strategies. In my opinion, these results appear to weaken the authors\u2019 claim that \u2018Our method is the first approach that is also non-myopic, as it is able to reason about the entire structure of a specification via temporally extended reach-avoid sequences.\u2019 Please discuss how your approach compares to and differs from the methods in [1], [2], and [3], with particular attention to handling non-myopic reasoning and addressing infinite-horizon specifications.\n\nA. K. Bozkurt, Y. Wang, M. M. Zavlanos, and M. Pajic, \u201cControl synthesis from linear temporal logic specifications using model-free reinforcement learning,\u201d in Proc. Int. Conf. Robot. Automat., 2020, pp. 10349\u201310355\n\nE. M. Hahn, M. Perez, S. Schewe, F. Somenzi, A. Trivedi, and D. Wojtczak, \u201cOmega-regular objectives in model-free reinforcement learning,\u201d in Proc. Int. Conf. Tools Algorithms Construction Anal. Syst., 2019, pp. 395\u2013412.\n\nLearning Optimal Strategies for Temporal Tasks in Stochastic Games Alper Kamil Bozkurt , Yu Wang , Michael M. Zavlanos , and Miroslav Pajic"
            },
            "questions": {
                "value": "The examples provided by the authors are all based on 2D grid-world environments. To evaluate the approach's performance in higher-dimensional settings, it would be valuable to experiment with environments like the 5-dimensional Carlo environment from [1], as well as other high-dimensional settings, such as the Fetch environment in [2], as utilized in [3]. Additionally, as a minor note, there is a typo on line 066 of the paper; it should read (c) instead of (b).\n\n[1] Cameron Voloshin, Abhinav Verma, and Yisong Yue. Eventual Discounting Temporal Logic Counterfactual Experience Replay. In Proceedings of the 40th International Conference on Machine Learning, pp. 35137\u201335150. PMLR, July 2023. \n\n[2] M. Plappert et al., \u201cMulti-goal reinforcement learning: Challenging robotics environments\u2002and request for research,\u201d 2018, arXiv:1802.09464.\n\n[3] Learning Optimal Strategies for Temporal Tasks in Stochastic Games Alper Kamil Bozkurt , Yu Wang , Michael M. Zavlanos , and Miroslav Pajic"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a reinforcement learning based policy synthesis method for a robot to satisfy a Linear Temporal Logic (LTL) specification. The salient features that distinguish this paper from prior work are the following: (1) the proposed method does not aim to generate a policy for a fixed LTL formula but rather to deal with any arbitrary one, (2) it can deal with specifications that can be satisfied only through infinite length execution,\u00a0 and (3) it ensures the satisfaction of the safety requirements, and (4) it optimizes the length of the trajectory. The proposed method is based on the observation that the satisfaction of a specification primarily depends on the loops including the final states in the Buchi automaton equivalent to the given specification. For a given LTL formula, the sequence of the sets of actions that lead to the satisfaction and violation of the specification is identified\u00a0and the policy is trained based on those sequences. On the test time, the policy for the target LTL formula can utilize the policy learnt based on many different LTL specifications and thus the learnt policy can be used in a zero-shot manner. The authors evaluate their method on three benchmark environments and compare it with two baselines. Experimental results establish the proposed method to be superior to the state-of-the-art methods both in terms of the rate of success in satisfying the test specifications and the optimality of the length of the trajectories."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "This paper improves the state-of-the-art for reinforcement learning with LTL specifications in several directions. Unlike the earlier methods, the proposed technique can deal with arbitrary LTL specifications at test time, supports infinite-horizon LTL specifications, ensures the satisfaction of the safety constraints, and attempts to optimize the trajectory length. Thus the technical contribution of the paper is significant.\n\nThe experimental evaluation is quite exhaustive, establishing the efficacy of the proposed method compared to the state-of-the-art."
            },
            "weaknesses": {
                "value": "The presentation in some parts of the paper could be improved. Specifically, a running example could help understand several complex ideas. For example, Section 4.2 could be easier to understand had an example been provided. Similarly, the paragraph on representing the reach-avoid sequence on page 6 could also be accompanied by an example. Furthermore, an example of how the negative assignments help could help convince readers about their necessity."
            },
            "questions": {
                "value": "In Example 1, why can\u2019t we replace the transition on $\\epsilon_{q_2}$ by a transition on the action $a$ to generate an equivalent Buchi automata?\n\nIn Line 252, in $\\delta(q_i, a) \\ne q_i$, wouldn\u2019t the second $q_i$ be $q_{i+1}$?\n\nIs it not the case that restricting the actions in the set $A_i^+$ will ensure that the actions are not from the sets $A_i^-$? \u00a0These two sets appear to be mutually exclusive. Then why do we need to keep track of both?\n\nSome of the terms used in the paper have never been introduced. For example, what is $sup(\\xi)$? How to interpret $\\tau \\sim \\pi | \\varphi$?\n\nOn Line 107, please use $\\equiv$ instead of \u201c=\u201c to denote formula equivalence."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No Concerns."
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}