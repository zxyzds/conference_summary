{
    "id": "mgjRSXGnF0",
    "title": "GlocalCLIP: Object-agnostic Global-Local Prompt Learning for Zero-shot Anomaly Detection",
    "abstract": "Zero-shot anomaly detection (ZSAD) is crucial for detecting abnormal patterns in target datasets without using training samples, specifically in scenarios where there are distributional differences between the target domain and training data or where data scarcity arises because of restricted access. Although recently pretrained vision-language models demonstrate strong zero-shot performance across various visual tasks, they focus on learning class semantics, which makes their direct application to ZSAD challenging. To address this scenario, we propose GlocalCLIP, which uniquely separates global and local prompts and jointly optimizes them. This approach enables the object-agnostic glocal semantic prompt design to effectively capture general normal and anomalous patterns without dependency on specific objects in the image. We refine the text prompts for more precise adjustments by utilizing deep-text prompt tuning in the text encoder. In the vision encoder, we apply a V-V attention layer to capture detailed local image features. Finally, we introduce glocal contrastive learning to improve the complementary learning of global and local prompts, effectively detecting abnormal patterns across various domains. The generalization performance of GlocalCLIP in ZSAD was demonstrated on 15 real-world datasets from both the industrial and medical domains, achieving superior performance compared to existing methods.",
    "keywords": [
        "Zero-shot anomaly detection",
        "Prompt learning",
        "Object-agnostic glocal semantic prompt design",
        "Deep-text prompt tuning",
        "Glocal contrastive learning"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=mgjRSXGnF0",
    "pdf_link": "https://openreview.net/pdf?id=mgjRSXGnF0",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces a novel method for zero-shot anomaly detection. GlocalCLIP separates global and local prompts and optimize them together, enabling detection of general anomalies without object dependence. Through refined text prompts and a V-V attention layer for detailed image features, GlocalCLIP effectively captures both normal and abnormal patterns. Tested on 15 industrial and medical datasets, it outperforms current ZSAD methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+  An object-agnostic global  and local prompts are supposed to learn normal and abnormal patterns.\n+ Align visual features and text features by jointly optimizing global and local prompts through contrastive learning to enhance robustness.\n+  The proposed method demonstrate excellent performance on multiple datasets."
            },
            "weaknesses": {
                "value": "This paper primarily focuses on improvements to AnomalyCLIP, particularly with regard to enhancements in text prompts. The paper has some unclear explanations that can make it difficult to understand."
            },
            "questions": {
                "value": "+ How to demonstrate the differences between global prompts $g_n$, $g_a$and local prompts $l_n$, $l_a$?\n+ Does 'semantic prompt design' refer to Sec3.3 ? Why does it perform poorly on 'single', and even lead to a decrease in performance in table 4?\n+ What causes the F3 to reduce image-level performance so drastically in medical applications in table 3? Could you provide a detailed explanation and analysis?\n+ What' s the anchor prompts?\n+ Some minor issues, such as line 316."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose GlocalCLIP, which introduces separable global and local prompts through an object-agnostic glocal semantic prompt design and jointly optimizes them to address the zero-shot anomaly detection task. They also employ contrastive learning to enhance the learning of global and local visual features. The effectiveness of the method is validated on 15 datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The authors propose GlocalCLIP, which introduces separable global and local prompts through an object-agnostic glocal semantic prompt design and jointly optimizes them to address the zero-shot anomaly detection task. They also employ contrastive learning to enhance the learning of global and local visual features."
            },
            "weaknesses": {
                "value": "See questions."
            },
            "questions": {
                "value": "1. The structure of the proposed method appears to be almost identical to AnomalyCLIP, except for improvements in prompt design and contrastive learning. There is no fundamental change at the framework level.\n2. The design of the V-V attention layer has already been experimented with and used in AnomalyCLIP.\n3. I believe the incremental experiments in Table 3 should be conducted starting from AnomalyCLIP to demonstrate the effectiveness of the newly proposed innovations.\n4. In Table 3, adding F4, which corresponds to GCL in Figure 4, does not seem to significantly improve the pixel-level metrics and even shows some decline. However, in Figure 4, adding GCL shows a clear difference in pixel-level performance compared to not adding GCL. Please explain the reason for this discrepancy."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work investigates the task of zero-shot anomaly detection and proposes a model named GlocalCLIP, which employs a dual-branch approach for both global and local modeling of image and text inputs based on CLIP. Results across multiple datasets demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The paper is clearly presented, including both text and images, making it easy to reproduce based on existing work.\n- Quantitative and qualitative experiments on multiple datasets."
            },
            "weaknesses": {
                "value": "- The novelty of the method is limited compared to AnomalyCLIP. The framework still follows the standard Winclip framework and subsequent works. The authors' claim of the \"first framework\" is exaggerated, as prompt tuning and V-V attention have already been used in anomaly detection, particularly in AnomalyCLIP.\n- The design of global and local branches is not novel. The authors should provide a more detailed explanation of what the two types of tokens model in zero-shot anomaly detection.\n- The global loss does not use multi-stage feature fusion, unlike the local loss. Additionally, the novelty of the GCL loss is trivial, lacking any impressive design.\n- The introduction resembles related work, and the motivation is not convincing.\n- For Fig. 1, it is unclear whether the authors aim to highlight the novelty of the zero-shot anomaly detection task or the disadvantages of other settings. I disagree with the authors' claim in Sec. 2.4 that few-shot and multi-class models (e.g., UniAD) are unfriendly for practical applications.\n- The improvement over AnomalyCLIP is not significant.\n- Please report the differences in training overhead, computational costs, and efficiency comparisons of different methods."
            },
            "questions": {
                "value": "Please refer to the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to adapt CLIP for the challenging task of zero-shot anomaly detection. The authors propose enabling object-agnostic learning to capture both global and local anomaly semantics. Experimental results seem to demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. This paper focuses on a challenging and valuable field that is practical to the real world.\n2. The authors conduct experiments across diverse datasets to support their claims."
            },
            "weaknesses": {
                "value": "1. This paper presents similar technological contributions and organization to AnomalyCLIP. However, the authors do not provide a comprehensive comparison and discussion with AnomalyCLIP. A detailed analysis in the introduction section is necessary to explicitly tell the main technological differences from AnomalyCLIP.\n2. The manuscript lacks proper citations in several sections, notably in 3.2 (Prompt Design) and 3.5 (Training and Inference). The authors should carefully review and appropriately cite previous research to acknowledge foundational work in this field.\n3. The illustration in the paper is unclear. For example, it is not evident what the term \"anchor\" refers to in Eq. 5, and the rationale for using V-V attention instead of other attention mechanisms, such as Q-Q or K-K attention, should be clearly explained."
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}