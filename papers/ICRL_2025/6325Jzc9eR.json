{
    "id": "6325Jzc9eR",
    "title": "VEditBench: Holistic Benchmark for Text-Guided Video Editing",
    "abstract": "Video editing usually requires substantial human expertise and effort. However, recent advances in generative models have democratized this process, enabling video edits to be made using simple textual instructions. Despite this progress, the absence of a standardized and comprehensive benchmark has made it difficult to compare different methods within a common framework. To address this gap, we introduce VEditBench, a comprehensive benchmark for text-guided video editing (TGVE). VEditBench offers several key features: (1) 420 real-world videos spanning diverse categories and durations, including 300 short videos (2-4 seconds) and 120 longer videos (10-20 seconds); (2) 6 editing tasks that capture a broad range of practical editing challenges: object insertion, object removal, object swap, scene replacement, motion change, and style translation; (3) 9 evaluation dimensions to assess the semantic fidelity and visual quality of edits. We evaluate ten state-of-the-art video editing models using VEditBench, offering an in-depth analysis of their performance across metrics, tasks, and models. We hope VEditBench will provide valuable insights to the community and serve as the standard benchmark for TGVE models following its open-sourcing.",
    "keywords": [
        "Benchmark",
        "Generative Models",
        "Video Editing"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=6325Jzc9eR",
    "pdf_link": "https://openreview.net/pdf?id=6325Jzc9eR",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces VEditBench, a new benchmark dataset for text-to-video editing tasks. The benchmark contains 420 real-world videos across six editing tasks. Additionally, the authors propose nine evaluation metrics to assess the semantic fidelity and visual quality of edits. They also provide a quantitative analysis of the performance of ten state-of-the-art text-to-video editing models on the newly introduced benchmark and evaluation metrics."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ The proposed benchmark could be valuable for advancing research in text-to-video editing.\n+ The quantitative analysis provided is strong.\n+ The paper is generally well-written."
            },
            "weaknesses": {
                "value": "* The dataset collection and annotation process is not very clear \n    - How many videos were collected per category?\n    - How was GPT-4o used to caption the videos? What prompts were used? It would be helpful if the authors provided a sample video with corresponding captions obtained through this process.\n    - How many people participated in the manual review and revision process?\n    - No sample dataset is provided as a reference.\n    - Concerns regarding the copyright and continued accessibility of the curated dataset are not addressed.\n\n* The proposed tasks and evaluation metrics are not strongly motivated\n\n    - The proposed editing tasks are not novel, as they have been utilized in prior works. What is the purpose of presenting them as new? Why were these six tasks selected? It is important to discuss how the proposed dataset and tasks differentiate themselves from existing datasets.\n    - The inclusion of all nine evaluation metrics seems redundant. Why is each metric necessary? For instance, what is the rationale for a spatio-temporal metric when individual spatial and temporal metrics are already provided?\n    - The evaluation process relies heavily on numerous pretrained models, which may introduce errors and impact the reliability of the proposed metrics. Did the authors implement any measures to mitigate potential error propagation?\n    - Different editing cases should ideally weigh metrics differently. For instance, spatial alignment is nearly irrelevant for a motion change task unless the caption specifically references motion (which would be rare). This is a critical consideration in designing evaluation metrics and appears to be overlooked in the paper.\n    - Did the authors consider using optical flow for the motion similarity metric?\n\n\n* The experimental analysis in the paper is limited and lacks depth\n    - Beyond the qualitative results, what insights can we draw about the models? Why do some models perform well on one metric but poorly on another?\n    - Given the limited technical contribution, the submission would benefit significantly from a detailed analysis from the perspectives of model architecture, dataset composition, and training settings.\n    - The paper lacks a video analysis. Including a demo video to showcase the different quantitative results would provide a clearer understanding of the outcomes discussed.\n    - The quantitative charts do not yield any meaningful insights, as the models seem to behave inconsistently across metrics. To what extent does this issue stem from the evaluation metric design itself?\n    - What is the takeaway message of the paper? What are the key research directions that remain unexplored? The paper briefly mentions the need for \u201cspecialized architectures and training strategies tailored to the specific challenges of long video editing,\u201d but this statement is vague and lacks depth."
            },
            "questions": {
                "value": "Please refer to the questions (or issues) mentioned in the Weaknesses section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes VEditBench, a comprehensive benchmark designed for the text-guided video editing (TGVE) task. It contains 420 real-world videos spanning six categories, which are further divided into short (2-4 seconds) and long (10-40 seconds) clips. VEditBench encompasses a wide range of editing tasks, such as object insertion, removal, swapping, etc. It evaluates existing state-of-the-art approaches from multiple dimensions, including semantic fidelity and visual quality, leading to insightful discussions/conclusions in TGVE."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The benchmark for text-guided video editing is both crucial and captivating. The work notably bridges a substantial gap, laying an important foundation for future studies of video editing.\n- The paper is crafted with a coherent structure and logical flow, making it accessible and comprehensible.\n- The proposed benchmark reveals the capabilities and limits of existing methods, demonstrating the trade-offs in performance across various tasks and dimensions."
            },
            "weaknesses": {
                "value": "- The absence of visual results makes it difficult to fully assess whether the quantitative metrics align with the intended objectives or functionality.\n- [Motion] Motion plays a vital role in video generation and editing, yet the proposed benchmark doesn't fully address this aspect. Is the CLIP score sensitive to the video motion (both object and camera motion) and text description? How to measure the performance of different methods in editing videos with varying levels of motion?\n- [Diversity in video] The video collection lacks diversity. While it includes various categories, it's unclear if there is style diversity, such as cartoons and paintings (different levels of abstraction).\n- [Diversity in task] Simultaneous multi-element editing with text isn't addressed in VEditBench. Typically, the editing requirements can be combinatorial and complicated. \n- [Metric] The structure preservation is not considered, which is common and important. Imagine that users intend to edit the motion of the main character while preserving the overall layout.\n- [Metric] The efficiency of different editing methods is not considered in the benchmark.\n\n\nTypo:\n- Lack of figure index in L402."
            },
            "questions": {
                "value": "- How are the starting points selected for the tracking of motion similarity metric? How significantly does this impact the evaluation?\n- How can different methods be measured and compared if they only support editing videos with fixed-length frames (and distinct from each other)?\n- Why do most metrics in VEditBench-long show better results compared to VEditBench-short?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce a text-guided video editing benchmark that includes a large-scale collection of videos across diverse categories, varying durations, and editing tasks grouped into six categories. Additionally, they define nine evaluation metrics to assess outputs across multiple dimensions. By addressing the limitations of previous methods, which relied on private or unreleased datasets and non-standardized evaluation pipelines, this paper seeks to standardize the evaluation of text-guided video editing. The authors also apply their benchmark to evaluate prior approaches, providing a consistent framework for comparison."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- I find this benchmark highly valuable for the field, as I agree with the authors' observation that each method currently introduces a unique evaluation pipeline, leading to non-standardized assessments. This work provides a standardized framework for evaluating new methods, which will benefit future research.\n\n- The categorization based on video duration enhances the benchmark, as some methods cannot handle longer videos. This categorization will be useful in differentiating capabilities among approaches.\n\n- The benchmark also covers a wide range of evaluation types. I find the inclusion of motion similarity particularly innovative; if this metric is novel, please cite the relevant sources, as I have not encountered previous works using this measure. Motion similarity offers a meaningful assessment of how well the resulting video maintains motion consistency.\n\n- The paper is very well presented and written."
            },
            "weaknesses": {
                "value": "- A potential enhancement could be to further categorize object-swapping tasks into two subcategories: one for swapping objects of similar size and another for cases that necessitate substantial motion adjustments (e.g., swapping a car with a bike). These scenarios involve distinct challenges, and such categorization would allow for a more precise assessment of method capabilities.\n\n- Additionally, incorporating GPU requirements and runtime as evaluation metrics would improve the benchmark\u2019s comprehensiveness by enabling comparisons on computational efficiency and scalability.\n\n- I have a question regarding the evaluation of methods with different prompt requirements. Methods such as InstructVid2Vid rely on instructional prompts, while others like TokenFlow and RAVE use target-based prompts. How does the benchmark account for these differences in prompt style? The instructional format used for all editing prompts, as indicated in Supplementary A.1, may not align with some methods that were not trained with instructional prompt styles, potentially impacting their performance.\n\n- In Table 1, it would be beneficial to include the dataset used in RAVE, as it features longer videos categorized by frame count, providing a clear basis for evaluating duration-dependent performance.\n\n- When discussing video duration (e.g., in lines 205 or 207), it would be more informative to also specify the frame rate (FPS), as duration in seconds alone does not provide a complete measure of the content length without this context.\n\n- In line 402, the figure number is missing within the parentheses, which should be corrected for clarity."
            },
            "questions": {
                "value": "Could you provide qualitative video results for each method in a downloadable format, such as a PDF or a hosted HTML link? Additionally, example prompts from the dataset would be helpful, as they are currently not visible.\n\nPlease refer to my questions in the weaknesses section, as addressing these points would allow me to reconsider and potentially increase my score."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a new benchmark, VEdit Bench, for evaluating text-guided video editing methods. The benchmark comprises 420 videos, including both short and long formats, and covers six distinct editing tasks. It provides a standardized evaluation framework with nine metrics that assess performance in terms of **semantic fidelity** and **visual quality**.  Furthermore, it conducts a comprehensive evaluation of existing video editing methods, providing insights into their performance across various tasks and metrics."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The new VEdit benchmark includes a significantly larger dataset, with 420 videos compared to the previous LOVEU-TGVE23\u2019s 76 videos.\n2. The benchmark addresses long video editing (10-40 seconds), which is essential for real-world applications.\n3. VEdit Bench introduces a novel motion similarity metric that considers both positional and directional costs, distinguishing it from the motion fidelity metric in DMT.\n4. The VEdit benchmark provides a comprehensive evaluation of most existing video editing methods, primarily those based on Stable Diffusion T2I models, with the exception of DMT, which is based on a T2V model. It also offers new insights, such as the importance of balancing semantic fidelity and visual quality as a direction for future research."
            },
            "weaknesses": {
                "value": "1. **Metric Weakness 1 (Coarse Semantic Alignment)**: The metric for semantic fidelity focuses primarily on text alignment and video alignment. However, relying solely on CLIP\u2019s text and visual embeddings may not fully capture text alignment, as CLIP\u2019s global representation is often coarse. The authors could enhance this by incorporating object masks, such as calculating visual-text alignment only within the foreground object\u2019s mask region.\n\n2. **Metric Weakness 2 (Editing Accuracy)**: For video editing methods based on T2I models, an essential issue is the model\u2019s ability to accurately follow the instruction prompt. Therefore, **editing accuracy** should be a key metric to evaluate whether edits align with the prompt. Additionally, some methods unintentionally alter the background or other video elements even when only the object should be edited. Accurately assessing whether edits are limited to the specified target is essential.\n\n3. **Task Weakness 1 (Multi-Object Editing)**: The benchmark lacks tasks sensitive to multiple object editing, such as multi-object video editing. For instance, VBench [1] includes specific metrics for multi-object editing, which could provide a more nuanced assessment of the model\u2019s performance in handling complex scenes with multiple subjects.\n\n4. **Task Weakness 2 (Subject Consistency)**: The benchmark does not adequately address subject consistency as a key quality indicator. Calculating overall motion consistency without distinguishing between foreground and background may be inadequate. Similar to VBench[1], separately evaluating subject consistency and background consistency could offer a more fine-grained metric for realistic video editing quality.\n\n5. **Task Weakness 3 (Inadequate for T2I Models)**: The object addition and removal tasks fall under inpainting or outpainting. Current T2I models may lack the capability to perform object addition in a time-consistent manner, meaning these tasks likely require video generation models with temporal priors.\n\n6. **Task Weakness 4 (Multi-Attribute Editing)**: The object swap task may involve simultaneous scene replacement, making it a challenging task. Editing multiple subjects or instances, also known as **multi-attribute editing**, should be considered a distinct task to evaluate complex editing capabilities.\n\n7. **Task Weakness 5 (T2I Limitations on Motion)**: Tasks like motion change and object addition are challenging for T2I-based editing methods due to the lack of video generative priors. T2I models excel in visual quality but often lack temporal consistency, whereas T2V models offer better temporal consistency but lower aesthetic quality. Balancing the evaluation for both T2I and T2V models\u2019 editing abilities is an important consideration.\n\n8. **Visualization Weakness (Lack of Qualitative Demos)**: A comprehensive benchmark should include qualitative visualizations, such as video or GIF demos. VBench, for instance, visually demonstrates what a high or low temporal consistency score looks like, helping readers intuitively understand the metric. VEdit Bench, however, focuses heavily on quantitative metrics, lacking qualitative demonstrations that could help readers better grasp how current metrics align with human perception and what quality aspects they evaluate.\n\n[1] Huang, Z. et al., \"VBench: Comprehensive Benchmark Suite for Video Generative Models,\" CVPR, 2024."
            },
            "questions": {
                "value": "please see weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "no ethics concerns"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents VEditBench, a new benchmark for text-guided video editing (TGVE). VEditBench enjoys several key features, including (1) 420 real-world videos spanning diverse categories and durations, consisting of 300 short videos (2-4 seconds) and 120 longer videos (10-20 seconds); (2) defining six editing tasks that capture a broad range of practical editing challenges like object insertion, object removal, object swap, and style translation; (3) suggesting nine evaluation dimensions to assess the semantic fidelity and visual quality of edits. Furthermore, the authors evaluate ten state-of-the-art video editing models using VEditBench, and conduct an in-depth analysis of their performance across metrics, tasks, and models."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The main strength of this paper is that the authors introduce a new benchmark for text-guided video editing by collecting 420 short and long videos, which includes more videos than previous benchmarks. The authors also suggest some metrics for evaluation and some editing tasks for experiments. This paper is also well-written, making the main text easy to follow and easy to read. The authors also conduct numerous expeirments of recent state-of-the-art methods using VEditBench, offering a comprehensive comparison and experiments."
            },
            "weaknesses": {
                "value": "As a benchmark paper, I would expect that the authors provide open-source datasets or links to the dataset/website/codes, etc. Unfortunately, I cannot find them in this paper. I believe this is necessary and the authors should elaborate more on that.\n\nAnother concern is that this paper seems a bit incremental. Many of the metrics and tasks defined in this paper are actually come from prior works or have already being used in the community. Furthermore, the categories of the gathered videos are only restricted to six categories. The authors could consider further expanding them and including categories like clothes, plants, buildings, etc.\n\nFurthermore, it seems that the tasks introduced by VEditBench is a bit simple. For example, as shown in Table 2 and Table 3, existing methods can achieve quite strong performance under many tasks and metrics in VEditBench."
            },
            "questions": {
                "value": "- what do you think are the most key and novel components of your benchmark compared to previous works?\n- why do you not include some long videos (say 200s) like V2VBench? Would the editing tasks and metrics still be reliable for long videos (e.g., 200s)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}