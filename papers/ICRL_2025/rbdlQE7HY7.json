{
    "id": "rbdlQE7HY7",
    "title": "Uniform Wrappers: Bridging Concave to Quadratizable Functions in Online Optimization",
    "abstract": "This paper presents novel contributions to the field of online optimization, particularly focusing on the adaptation of algorithms from concave optimization to more challenging classes of functions. Key contributions include the introduction of uniform wrappers, establishing a vital link between upper-quadratizable functions and algorithmic conversions. Through this framework, the paper demonstrates superior regret guarantees for various classes of up-concave functions under zeroth-order feedback. Furthermore, the paper extends zeroth-order online algorithms to bandit feedback counterparts and offline counterparts, achieving a notable improvement in regret/sample complexity compared to existing approaches.",
    "keywords": [
        "DR-submodular Optimization",
        "Convex Optimization"
    ],
    "primary_area": "optimization",
    "TLDR": "",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=rbdlQE7HY7",
    "pdf_link": "https://openreview.net/pdf?id=rbdlQE7HY7",
    "comments": [
        {
            "summary": {
                "value": "This paper provides an in-depth exploration of advanced techniques for online optimization, particularly focusing on transitioning algorithms from concave optimization to more complex classes. This work contributes by introducing \"uniform wrappers,\" a framework for adapting standard online convex optimization (OCO) algorithms to handle non-convex, quadratizable functions while preserving sublinear regret bounds."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper considers a challenging and important problem in online optimization, namely, how to extend algorithms from the convex to the non-convex setting.\n- The core idea of uniform wrappers allows traditional OCO algorithms to handle quadratizable functions by converting feedback and actions within a structured wrapper framework. This wrapper effectively bridges the gap between simpler convex optimization and more complex quadratizable function classes. Moreover, this framework is general and can be applied to a wide range of function classes.\n- This paper obtains or matches the state of the art algorithm in several online optimization settings considered. For classes such as weakly DR-submodular functions under\nzeroth order feedback, the framework provides superior regret guarantees, surpassing prior results in the field."
            },
            "weaknesses": {
                "value": "- The paper is quite dense and difficult to follow. The theoretical framework is complex, with multiple layers of definitions (e.g., quadratizability, up-super-gradients, and uniform wrappers) that might limit accessibility for a broader audience. While this complexity is necessary to cover a broad class of functions, it risks making the approach difficult to understand for readers unfamiliar with the field. The presentation could be improved by providing more intuition and examples to help readers understand the core ideas and contributions. \n- The paper cites and builds upon prior work in DR-submodular and up-concave optimization (e.g., Pedramfar & Aggarwal, 2024), but it lacks a clear differentiation of its unique contributions. The proposed framework appears to be an incremental improvement rather than a groundbreaking advance. I suggest the authors to clarify the novelty and significance of their work compared to existing methods in the field.\n- I am quite confused about the necessity of the uniform wrappers. The paper does not provide a clear motivation for why uniform wrappers are needed or how they improve upon existing methods. A more detailed discussion on the limitations of current approaches and how uniform wrappers address these limitations would be beneficial."
            },
            "questions": {
                "value": "- I wonder the significance of the upper quadratizable/linear functions. Why are these functions important, and how do they relate to real-world applications? It would be helpful to provide more context on the motivation behind this class of functions and why they are relevant in practice.\n- What are the conditions under which uniform wrappers can be applied? Are there any limitations that restrict the applicability of this framework to certain function classes or settings?\n- I am interested in the regret defined in Line 203. Can the results reduce to the dynamic regret and adaptive regret? If so, how do they compare to existing methods in these settings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work gives a framework that reduces the problem of online optimization with quadratizable functions (Pedramar & Aggarwal, 2024a) to OCO. This results in state-of-the-art guarantees in several settings and improvements in some others."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- The framework provides improvements in the state-of-the-art regret bounds in several settings.\n- The presentation is sufficiently clear."
            },
            "weaknesses": {
                "value": "Although there might be some small improvements in the analysis of Pedramar & Aggarwal (2024a), the framework appears to be nearly identical to that in Pedramar & Aggarwal (2024a). Nearly every technical result in the paper is due to Pedramar & Aggarwal (2024a), e.g. Lemma 1, Lemma 2, Lemma 3, ... I do not see any novelty in techniques or approach."
            },
            "questions": {
                "value": "What is the technical novelty with respect to Pedramar & Aggarwal (2024a)?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This article develops a general framework for transferring the algorithms and their regret guarantees from online convex/concave optimization to online quadratizable optimization. Within this framework, the authors employ a variant of Follow-the-Regularized-Leader (FTRL) from [1] to enhance the regret guarantees for several classes of weakly DR-submodular functions under zeroth-order and bandit feedback.\n\n[1] Ankan Saha and Ambuj Tewari. Improved regret guarantees for online smooth convex optimization with bandit feedback. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 636\u2013642. JMLR Workshop and Conference Proceedings, 2011.\u201d"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.The authors have improved the regret bound for online optimization problems across three function classes under zeroth-order feedback, from $\\tilde{O}(T^{3/4})$ to $\\tilde{O}(T^{2/3})$.\n\n2.The authors have enhanced the regret bound for online optimization problems across three function classes under bandit feedback, improving it from $\\tilde{O}(T^{4/5})$ to $\\tilde{O}(T^{3/4})$."
            },
            "weaknesses": {
                "value": "1.The technical contribution is quite limited. Although the upper quadratizable function is highly non-convex, many previous articles, especially [2], have shown that this upper quadratizable function satisfies a first-order variational inequality similar to that of a convex function (Lemma 1, 2, 3). Therefore, we can consider the upper quadratizable function as a special type of \"quasi-concave\" function. Consequently, it is quite natural for this paper to use a variant of FTRL [1] to improve the regret bounds in zero-order and bandit scenarios.\n\n2.This paper lacks an in-depth discussion of the applications of the upper quadratizable function and a practical evaluation of the newly introduced algorithms. This statement may make readers feel that the authors are merely exploring a new function without clarifying its relevance or impact on the machine learning community.\n\n3.From the full text and Definition 2, it appears that the primary purpose of introducing the upper quadratizable function is to study $\\gamma$-weakly continuous DR-submodular functions. So, why not focus on \"$\\gamma$-weakly continuous DR-submodular functions\" as the main subject of the paper? Can the authors provide examples of applications where the target function is upper quadratizable but not $\\gamma$-weakly continuous DR-submodular functions?\n\n\n[1] Ankan Saha and Ambuj Tewari. Improved regret guarantees for online smooth convex optimization with bandit feedback. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 636\u2013642. JMLR Workshop and Conference Proceedings, 2011.\n\n[2]Mohammad Pedramfar and Vaneet Aggarwal. From linear to linearizable optimization: A novel framework with applications to stationary and non-stationary DR-submodular optimization. arXiv preprint arXiv:2405.00065, 2024a."
            },
            "questions": {
                "value": "1. Could the authors provide a detailed personal perspective on how their technical contributions differ from those in references [1] and [2]?\n2. Could you provide a detailed explanation of the applications of upper quadratizable functions and conduct an empirical evaluation of the newly proposed algorithms?  \n3. Can the authors provide examples of applications where the target function is upper quadratizable but not $\\gamma$-weakly continuous DR-submodular functions? \n4. Why not focus on \"$\\gamma$-weakly continuous DR-submodular functions\" as the main subject of the paper?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}