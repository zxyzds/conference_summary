{
    "id": "xlxGsX1pc7",
    "title": "U-MATH: A University-Level Benchmark for Evaluating Mathematical Skills in LLMs",
    "abstract": "The current evaluation of mathematical skills in LLMs is limited, as existing benchmarks are relatively small, primarily focus on elementary and high-school problems, or lack diversity in topics. Additionally, the inclusion of visual elements in tasks remains largely under-explored. \n    \nTo address these gaps, we introduce **U-MATH**, a novel benchmark of 1,125 unpublished open-ended university-level problems sourced from teaching materials. It is balanced across six core subjects, with 20\\% of problems requiring image understanding. Given the open-ended nature of U-MATH problems, we employ an LLM to judge the correctness of generated solutions. To this end, we release **$\\boldsymbol\\mu$-MATH**, an additional dataset to evaluate the LLMs' capabilities in assessing solutions.\n\nThe evaluation of general domain, math-specific, and multimodal LLMs highlights the challenges presented by U-MATH. Our findings reveal that LLMs achieve a maximum accuracy of only 53\\% on text-based tasks, with even lower 30\\% on visual problems. The solution assessment proves challenging for LLMs, with the best LLM judge having an F1-score of 76\\% on $\\mu$-MATH.\n    \nWe open-source U-MATH, $\\mu$-MATH, and evaluation code on GitHub.",
    "keywords": [
        "Large Language Models (LLMs)",
        "Mathematical Reasoning",
        "Benchmarking",
        "University-Level Mathematics",
        "Multimodal",
        "Automatic Evaluation",
        "Solution Assessment"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "U-MATH, a challenging university-level math benchmark with both textual and visual problems, and additional \u03bc-MATH benchmark to evaluate solution assessment capabilities.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=xlxGsX1pc7",
    "pdf_link": "https://openreview.net/pdf?id=xlxGsX1pc7",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces the U-Math datasets, based on university-level mathematics, addressing the issues of insufficient thematic diversity and a lack of visual information question types in current datasets for evaluating the mathematical abilities of large language models. The U-Math datasets was tested on several large language models, revealing that the highest accuracy for text-based tasks was only 53%, while the highest accuracy for visual tasks was only 30%."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well-organized, providing a clear outline of the datasets, experimental setup, and evaluation metrics. The authors explain each component in a structured manner, making it accessible to readers.\n\n2. The datasets include a range of mathematical subjects and problem types, which reflects an effort to cover diverse aspects of mathematical reasoning, though the depth and breadth could still be improved.\n\n3. The introduction of U-MATH and \u00b5-MATH provides additional benchmarks for evaluating LLMs in mathematical tasks, which may offer a reference point for similar studies."
            },
            "weaknesses": {
                "value": "1. Although the U-MATH datasets consists of 1,125 samples and covers six subjects, the sample size is still too small. Evaluating the mathematical abilities of large models using a limited amount of data is not sufficiently convincing.\n\n2. Although the 340 samples in the \u00b5-MATH datasets have been carefully selected to provide a challenging test, a larger sample size could enhance the representativeness of the evaluation, especially across different topics and problem types."
            },
            "questions": {
                "value": "1.It is recommended to expand both the U-MATH datasets size and the number of subjects.\n\n2.It is recommended to expand the \u00b5-MATH datasets size.\n\n3.In Table 4, you only use accuracy to present the results. Since the study involves math problems, which are more complex than simple classification tasks, could you consider adding additional evaluation metrics like perplexity or WinoGrande ACC (to assess whether ambiguous problems are correctly identified)? This would give readers a clearer picture of how well the models truly understand and respond to university-level math questions. For more details, you might refer to examples in this paper: https://proceedings.mlr.press/v235/dao24a.html."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces U-MATH, a comprehensive university-level mathematical benchmark designed to evaluate the performance of Large Language Models (LLMs) in solving advanced mathematical problems. The dataset consists of 1,125 problems sourced from university coursework, covering six core topics such as Algebra, Calculus (Differential and Integral), Multivariable Calculus, Sequences, and Series, with approximately 20% of the tasks involving visual components. To complement the U-MATH dataset, the authors also present \u00b5-MATH, a meta-evaluation set for assessing the accuracy and reliability of LLM-based evaluators."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "S1. The inclusion of university-level problems offers a significant advancement over existing datasets that mainly focus on elementary or high school-level tasks.\n\nS2: By integrating visual tasks alongside traditional textual ones, the dataset challenges LLMs to interpret and reason across multimodal formats.\n\nS3: \u00b5-MATH introduces a novel approach to evaluate LLMs' ability to assess solutions, addressing biases and limitations in current evaluation practices."
            },
            "weaknesses": {
                "value": "W1: The reliance on LLMs as judges (e.g., GPT-4o) to evaluate free-form answers could introduce biases and inconsistencies, particularly since LLMs may struggle with complex derivations or nuanced interpretations of mathematical expressions.\n\nW2: The \u00b5-MATH set includes LLM-generated solutions, which may limit the diversity and challenge of evaluation due to inherent model tendencies or training biases. This could result in less rigorous meta-evaluation as models may overfit to known patterns or heuristics."
            },
            "questions": {
                "value": "What measures have been taken to mitigate potential biases introduced by using LLMs as judges for solution correctness?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduced a new benchmark dataset, U-MATH, designed to evaluate large language models (LLMs) on university-level math problems. The proposed U-MATH benchmark includes 1,125 college-level math problems collected from real educational materials, covering six core mathematical subjects, with 20% of the problems involving image understanding. Additionally, the paper introduces a meta-evaluation dataset named \u00b5-MATH, aimed at assessing the ability of LLMs to judge the correctness of mathematical solutions."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.U-MATH Benchmark: This is a publicly available dataset of university-level math problems, covering six topics: Pre-Calculus, Algebra, Differential Calculus, Integral Calculus, Multivariable Calculus, and Sequences & Series. A unique aspect of this dataset is its inclusion of open-ended questions that require LLMs to perform multi-step reasoning.\n2.\u00b5-MATH Meta-Evaluation Benchmark: This benchmark is specifically designed to test LLMs\u2019 ability to assess the correctness of mathematical solutions. It contains 340 questions selected from U-MATH, accompanied by LLM-generated answers manually labeled as correct or incorrect, aimed at evaluating the capacity of LLMs to act as \u201cjudges.\u201d\n3.Model Comparison: The paper compares the performance of various LLMs, including general-purpose models, specialized math models, and multimodal models, demonstrating the significant challenges LLMs still face in both text and visual tasks. For instance, the highest accuracy for text-based questions is 53%, while performance on visual questions is even lower, with an accuracy of only 30%.\n4.Challenges for LLMs as Math Judges: LLMs perform poorly when evaluating mathematical solutions, with the best-performing LLM judge achieving an F1 score of only 76% on \u00b5-MATH, indicating that there is still room for improvement in this task."
            },
            "weaknesses": {
                "value": "1.The U-MATH dataset introduced in the paper supplements the current math datasets by addressing college-level gaps, while the \u00b5-MATH meta-evaluation dataset enables assessment of large models\u2019 ability to evaluate college-level math solutions. However, aside from knowing that this training set focuses on university mathematics and includes six subjects, we lack information about the dataset\u2019s question diversity, difficulty, reasoning steps required to solve the problems, and other aspects. Additionally, the dataset\u2019s size may be insufficient.\n2.The paper mentions that the dataset has been released but does not provide an access link, so I have no direct way to review the dataset.\n3.The experiments in the paper provide valuable insights into the capabilities of current text-based and multimodal LLMs in solving university-level math problems.\n4.The paper states that U-MATH aims to promote further research and improve LLMs' ability to handle complex math problems. How is \"complex\" defined here? Does it refer to higher-grade, more challenging (for humans) knowledge, or does it mean problems requiring more and deeper reasoning steps?"
            },
            "questions": {
                "value": "I don't have further questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces U-MATH, a novel benchmark designed to evaluate the mathematical reasoning capabilities of Large Language Models (LLMs) at the university level. It comprises 1,125 unpublished, open-ended problems sourced from actual teaching materials, balanced across six core mathematical subjects, with 20% of the problems requiring image understanding. Additionally, the paper presents \u00b5-MATH, a meta-evaluation dataset aimed at assessing the ability of LLMs to evaluate free-form mathematical solutions. The experiments conducted reveal significant challenges in advanced mathematical reasoning and visual problem-solving, with the best-performing models achieving only 53% accuracy on text-based tasks and 30% on visual problems. The paper also highlights the difficulty LLMs face in assessing solutions, with the highest \u00b5-MATH F1-score being 76%, indicating room for improvement in LLMs\u2019 evaluation capabilities. The datasets and evaluation code are open-sourced to facilitate further research."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper demonstrates a high-quality collection of problems that are well-balanced across six core mathematical subjects. This ensures a comprehensive evaluation of LLMs across different areas of mathematics.\nThe problems sourced from actual teaching materials add a layer of authenticity and practical relevance to the benchmark, ensuring that the skills assessed are applicable to real-world academic standards.\nThe creation of \u00b5-MATH for meta-evaluation is an innovative approach to assessing the ability of LLMs to evaluate mathematical solutions. This adds another layer of complexity and originality to the benchmarking process, focusing not just on problem-solving but also on the assessment capabilities of the models."
            },
            "weaknesses": {
                "value": "While the inclusion of visual elements in 20% of the problems is a step forward, the remaining 80% are text-based. The paper could benefit from expanding the visual problem set to better assess and train LLMs in multimodal mathematical reasoning, which is increasingly important for real-world applications.\n\nThe paper focuses on university-level mathematics, but it is unclear how well the findings generalize to other levels or types of mathematical reasoning. Future work could explore the transferability of the models trained on U-MATH to other mathematical domains."
            },
            "questions": {
                "value": "Why are there no examples of problems that require visual input?\n\nThe accuracy when using LLM as a judge is not provided, especially for higher mathematics problems where answers may be in different forms but are actually equivalent, indicating that it is easier to make mistakes compared to comparing a single form of answer."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}