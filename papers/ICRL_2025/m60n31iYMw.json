{
    "id": "m60n31iYMw",
    "title": "The Implicit Bias of Structured State Space Models Can Be Poisoned With Clean Labels",
    "abstract": "Neural networks are powered by an implicit bias: a tendency of gradient descent to fit training data in a way that generalizes to unseen data. A recent class of neural network models gaining increasing popularity is structured state space models (SSMs), regarded as an efficient alternative to transformers. Prior work argued that the implicit bias of SSMs leads to generalization in a setting where data is generated by a low dimensional teacher. In this paper, we revisit the latter setting, and formally establish a phenomenon entirely undetected by prior work on the implicit bias of SSMs. Namely, we prove that while implicit bias leads to generalization under many choices of training data, there exist special examples whose inclusion in training completely distorts the implicit bias, to a point where generalization fails. This failure occurs despite the special training examples being labeled by the teacher, i.e. having clean labels! We empirically demonstrate the phenomenon, with SSMs trained independently and as part of non-linear neural networks. In the area of adversarial machine learning, disrupting generalization with cleanly labeled training examples is known as clean-label poisoning. Given the proliferation of SSMs, particularly in large language models, we believe significant efforts should be invested in further delineating their susceptibility to clean-label poisoning, and in developing methods for overcoming this susceptibility.",
    "keywords": [
        "Structured State Space Models",
        "Implicit Bias",
        "Implicit Regularization",
        "Clean Label Poisoning"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=m60n31iYMw",
    "pdf_link": "https://openreview.net/pdf?id=m60n31iYMw",
    "comments": [
        {
            "summary": {
                "value": "This paper presents a fundamental understanding of SSMs by investigating its implicit bias both theoretically and empirically. From the extended setting of the previous work [1], the authors address several theoretical findings which are corroborated with experimental evidence. First, the authors provide a dynamical characterization of gradient flow over SSM, which reveals that greedy learning can be implicitly induced under many choices of training sequences. Second, based on the first point, the authors prove the followings; (1) a collection of sequences labeled by a low dimensional teacher SSM leads to the implicit bias of the student SSM, (2) adding a certain single sequence labeled by the teacher to the training set ruins the implicit bias which fails the generalization. These analyses are supported by simple experiments that are well consistent with the theories.\n\n\n[1] Edo Cohen-Karlik, Itamar Menuhin-Gruman, Raja Giryes, Nadav Cohen, and Amir Globerson. Learning low dimensional state spaces with overparameterized recurrent neural nets. In International Conference on Learning Representations (ICLR), 2023"
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The theoretical analysis of the implicit bias of SSMs is a strength of this work. Providing a basic understanding of SSMs, which are often considered as an alternative to transformers, is indeed important for the community.\n\nThe paper is nicely written, while I'm not an expert in this area, I could easily follow the logical steps of the paper based on proper interpretation and the sketches of the mathematical details.\n\nThe experimental results that corroborates the theoretical analyses are a strength of this work. While the authors acknowledge the limitations of their work (to name a few, the experiments pertain near-zero initialization, the dimension of the teacher SSM is one in most settings), the identification of the phenomena and a theoretical modeling to explain such things is a meaningful contribution."
            },
            "weaknesses": {
                "value": "Again, I'm not an expert in this area, but I'd like to make a few points that might be helpful to the authors and also clarify my understanding.\n\n1. Is it possible to separately decompose and plot $\\gamma^{(0)}(t)$ along with Figure 2? If so, I think it could give a more concrete explanation to aid the Interpretation part of Section 3.1.\n2. Can the authors come up with a measure to qualitatively distinguish between the scenarios of the (leftmost) and (second) subplot of Figure 2? I'm aware that this might be an abrupt question, but it could be seen that the (second) subplot does not correspond to the case of 'not greedy', maybe close to 'less greedy' (especially for the case of Figure 4 in the supplementary material). \n3. (Slightly related to 2.) Adding results of more individual runs/seeds for Figure 2 could more strengthen the authors' explanation. Could be an indirect way to show this, but showing only one trial as confirmation to the theory seems less sufficient.\n4. Out of the scope of the teacher-student setting (i.e. in the real-world scenario), what could be the interpretation or example for $(\\mathbf{x}^{\\dagger}, y^{\\dagger})$ of Theorem 1 that harms the student from generalization?\n5. Adding a short discussion of the limitations with respect to the corresponding results from the supplementary materials would make the paper more complete. In addition, could the authors provide more experimental results beyond the limitation setting made in the supplementary materials (i.e., results for the teacher dimension greater than 2)? Providing these additional results is not necessary, and I do not want this point to be an extensive burden. But I believe that effectively showing such limitations is indeed important from a completeness perspective."
            },
            "questions": {
                "value": "Please refer to the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper examines the vulnerability of SSMs to clean-label poisoning. While SSMs generally possess an implicit bias that promotes generalization when trained on low-dimensional data from a teacher model, this study demonstrates that the inclusion of carefully chosen, correctly labeled training examples can disrupt this bias, resulting in a failure to generalize."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1). This paper discovered clean-label poisoning of SSMs, which is a vulnerability unrecognized by previous works, highlighting a potential risk in the safety, robustness, and reliability of SSMs.\n\n2). This paper provides a strong theoretical foundation and empirical validation to show that generalization can be ruined by introducing certain clean-labeled sequences."
            },
            "weaknesses": {
                "value": "This paper falls outside my current area of expertise."
            },
            "questions": {
                "value": "This paper falls outside my current area of expertise."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 1
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the implicit bias of Structured State Space Models (SSMs), an emerging and efficient alternative to transformers. Previous research suggested that the implicit bias of SSMs enables good generalization when data is generated by a low-dimensional teacher model. However, this study revisits that assumption and uncovers a previously undetected phenomenon: while implicit bias generally promotes generalization, certain special training examples can completely distort this bias, causing generalization to fail. Notably, this failure occurs even when these examples have clean labels provided by the teacher model. The authors empirically demonstrate this effect in SSMs trained both independently and within non-linear neural networks. In adversarial machine learning, disrupting generalization with cleanly labeled examples is known as clean-label poisoning. Given the growing use of SSMs, especially in large language models, the authors emphasize the need for further research into their vulnerability to clean-label poisoning and the development of strategies to mitigate this susceptibility."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper presents a solid theoretical work, extending the results on the gradient descent (GD) implicit bias of Structured State Space Models (SSMs) from the population risk setup, as discussed in [1], to the finite empirical risk setup. The authors demonstrate that training a student SSM on sequences labeled by a low-dimensional teacher SSM exhibits an implicit bias conducive to generalization. Their dynamical analysis also establishes a connection with greedy low-rank learning.\n\n2. Using an advanced tool from dynamical systems theory\u2014the non-resonance linearization theorem\u2014the authors prove that adding a single sequence to the training set, even if labeled by the teacher SSM (i.e., with a clean label), can completely distort the implicit bias to the point where generalization fails. This finding highlights the need for significant research efforts to better understand SSMs' vulnerability to clean-label poisoning and to develop effective methods to counteract this issue.\n\n3. Overall, the paper is very well written.\n\n**Reference**\n\n[1] Cohen-Karlik, E., Menuhin-Gruman, I., Giryes, R., Cohen, N., & Globerson, A. (2022). Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets. arXiv preprint arXiv:2210.14064."
            },
            "weaknesses": {
                "value": "1. The assumptions in Theorem 1 are overly restrictive, particularly as the structures of $ A^* $, $ B^* $, and $ C^* $ seem to be very simple, and there is a lack of detailed explanation as to why such a simplified setup is justified in this context.\n\n2. Regarding the special sequence data, Theorem 1 only provides an existence result without specifying a concrete construction method or offering a more detailed characterization, making it difficult for me to understand why the introduction of these clean data points leads to a failure in generalization.\n\n3. The experiments in the paper seem to be conducted solely on synthetic datasets, which greatly limits their persuasiveness. The authors should consider using more real-world datasets to demonstrate the generality of the phenomenon they describe."
            },
            "questions": {
                "value": "1. Could the authors provide a more formal statement regarding greedy low-rank learning and clarify whether the theoretical analysis can be observed in experiments on real-world datasets?\n\n2. For real-world datasets, can the theoretical analysis and conclusions of this paper help design an algorithm to identify the so-called special sequence for poisoning?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper shows that when learning certain linear state-space models, gradient flow converges to solutions that generalize well with certain training trajectories but to those that do not when some specific trajectory is appended."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "Data poisoning attacks are one of the practical concerns in training machine learning models, this paper studies some theoretical aspects of this issue for state-space models."
            },
            "weaknesses": {
                "value": "The result shown in this paper comes from an extremely contrived example and the reviewer has strong concerns regarding its practical significance.\n\n1. Theorem 1 states that the learned student SSM generalizes \"under various choices of training sequence\" (line 258), the truth is these training sequences are $\\left\\lbrace p_i\\mathbf{e_1},p_i\\right\\rbrace_{i=1}^n$, where $p_i\\in\\mathbb{R}$ (line 857, Definition 2 in Appendix), i.e., the input sequences to the teacher SSM are the same up to some scaling. Since the SSM to be learned is linear, these $n$ training sequences are effectively single sequences. It is not a big surprise if student SSM generalizes when this single training trajectory is crafted to guide the GF to the generalizing solutions, and then it is even less surprising if another crafted training trajectory is added to steer GF to other solutions. There are effectively only two training trajectories, after all.\n\n2. Another reason this example is contrived is that there is no persistence of excitation (Green&Moore, 1986) in the input sequence used for training, which is fundamentally needed to identify linear systems. \n\n3. The state matrix $A$ is assumed diagonal, the $B,C$ matrices are fixed during training. However, these are of less concern than the previous points.\n\n4. The reviewer does not appreciate the presentation in this paper where assumptions ($A$ being diagonal) are not stated as proper assumptions (line 112), and key definitions (precise definitions of the training trajectories) are written in the Appendix. \n\n**References**:\n\nM Green and J Moore, Persistence of excitation in linear systems, Systems & Control Letters, 1986"
            },
            "questions": {
                "value": "See \"Weakness\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}