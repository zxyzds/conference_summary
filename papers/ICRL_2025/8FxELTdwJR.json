{
    "id": "8FxELTdwJR",
    "title": "Hyperparameters in Continual Learning: A Reality Check",
    "abstract": "Continual learning (CL) aims to train a model on a sequence of tasks (i.e., a CL scenario) while balancing the trade-off between plasticity (learning new tasks effectively) and stability (retaining prior knowledge). The dominantly adopted conventional evaluation protocol for CL algorithms selects the best hyperparameters within a given scenario and then evaluates the algorithms\nusing these hyperparameters in the same scenario. However, this protocol has significant shortcomings: it overestimates the CL capacity of algorithms and relies on unrealistic hyperparameter tuning, which is not feasible for real-world applications. From the fundamental principles of evaluation in machine learning, we argue that the evaluation of CL algorithms should focus on assessing the generalizability of their CL capacity to unseen scenarios. Based on this, we propose a revised two-phase evaluation protocol consisting of a hyperparameter tuning phase and an evaluation phase. Both phases share the same scenario configuration (e.g., number of tasks) but are generated from different datasets. Hyperparameters of CL algorithms are tuned in the first phase and applied in the second phase to evaluate the algorithms. We apply this protocol to class-incremental learning, both with and without pretrained models. Across more than 8,000 experiments, our results show that most state-of-the-art algorithms fail to replicate their reported performance, highlighting that their CL capacity has been significantly overestimated in the conventional evaluation protocol.",
    "keywords": [
        "Continual Learning",
        "Class Incremental Learning",
        "Evaluation"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "We demonstrate that state-of-the-art continual learning algorithms have been overestimated in the conventional evaluation protocol.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=8FxELTdwJR",
    "pdf_link": "https://openreview.net/pdf?id=8FxELTdwJR",
    "comments": [
        {
            "title": {
                "value": "Complete to upload first responses"
            },
            "comment": {
                "value": "We would like to express our sincere gratitude to all the reviewers for their valuable feedback. We have provided detailed responses to all the weaknesses and questions raised by the reviewers, making a particular effort to clarify any misunderstandings about the content of the paper. We kindly ask the reviewers to review our responses and look forward to receiving their feedback. We hope to engage in an active discussion. \n\nAdditionally, we are currently in the process of updating the paper to reflect the reviewers' comments. Once the updates are complete, we will upload the revised version and share another public comment to notify everyone.  \n\nThank you."
            }
        },
        {
            "title": {
                "value": "Author response 2"
            },
            "comment": {
                "value": "**Q1) For BEEF, have you tried a different implementation or different seeds?**\n\nFirst, we would like to clarify that, consistent with all other experiments, **we reported the averaged results of five seeds for the experiments using BEEF**. As shown in Figure 5 in the manuscript, BEEF produced valid results on CIFAR-100; however, for ImageNet-100, we observed NaN errors arising from a specific seed (i.e., a particular task order). The table below presents the experimental results of BEEF with randomly selected hyperparameters across five different seeds.\n\nHyperparameters: ep_160_milestone_3_lr_0.05_lr_decay_0.1_batch_128_w_decay_0.005_scheduler_cosine_fusion_ep_120_energy_w_0.001_logits_align_1.7\n\n|Acc / AvgAcc| Seed 0 | Seed 1 | Seed 2 | Seed 3 | Seed 4\n| -------- | -------- | -------- | -------- |-------- |-------- |\n| BEEF     | NaN     | 52.52 / 64.43     | NaN     |NaN     |53.04 / 65.15     |\n\nNote that for each seed, the task order was applied consistently across experiments using different algorithms, and only BEEF displayed a similar trend\u2014namely, the occurrence of NaN values in specific seeds\u2014even under different hyperparameter settings. **These results again demonstrate that BEEF shows instability in learning with certain task orders**.\n\n**Q2) In Figure 4 (b), why do almost all methods perform better in the unseen scenario?**\n\nThis discrepancy may arise from differences in the datasets used across scenarios. Although ImageNet-100, used as $D^{HT}$, and ImageNet-100-2, used as $D^{E}$, contain the same number of classes, they consist of entirely different class labels. **Due to these dataset differences, even with identical hyperparameters, the final performance may vary across scenarios**. Nevertheless, we believe that comparing the performance rankings of algorithms on each dataset enables a meaningful comparison of each algorithm\u2019s CL capacity in both phases.\n\n**Q3) What criterium did you use to select the methods for evaluation? Is it their availability in PyCIL and PILOT?**\n\nOur rationale for selecting the algorithms used in our study is as follows. First, we chose class-incremental learning (CIL) as the primary category of continual learning (CL) for evaluation, as it is widely recognized as **a more challenging category** compared to task- and domain-incremental learning [1]. Additionally, CIL has recently become **the most actively researched area** in CL [1,2]. Within CIL, different algorithms have been introduced based on whether they utilize pretrained models, prompting us to include a range of algorithms from earlier to more recent ones in a unified framework. Consequently, as **PyCIL and PILOT have successfully reproduced many CL algorithms**, we applied our proposed revised evaluation protocol to these codebases to conduct our experiments.\n\n[1] Three scenarios for continual learning, NeurIPSW 2018\n\n[2] A Comprehensive Survey of Continual Learning: Theory, Method and Application, TPAMI 2024\n\nLastly, we have made every effort to thoroughly address all Weaknesses and Questions raised by the reviewer. We will upload the revised paper, reflecting these comments, as soon as the revisions are complete and notify you. We look forward to any further comments and an active discussion."
            }
        },
        {
            "title": {
                "value": "Author response 1"
            },
            "comment": {
                "value": "Thank you for recognizing the issues of the conventional evaluation protocol in continual learning (CL) research and for acknowledging the extensive experiments we conducted using a revised protocol to bring these issues to light. We sincerely hope that our work contributes to broader discussions on this topic and encourages future CL research to pursue meaningful achievements through more rigorous evaluation.\n\nBelow is our response to the Weaknesses raised by your review:\n\n1. **Poor presentation.**\n\nOur initial goal in drafting the paper was to convey the following points in sequence: (1) highlight the limitations of the conventional evaluation protocol and introduce the revised protocol, (2) provide details on experimental settings and algorithms in each scenario (e.g., number of hyperparameters), (3) demonstrate the issues through experimental results across various scenarios, and (4) conduct additional comparative analyses. To ensure clarity, we included figures strategically and used bar graphs in the experimental section to facilitate comparisons at a glance. However, based on your feedback, we could recognize the need for further improvements to make our key points even more clear. In future updates, we will revise the figures and tables to enhance clarity and emphasize our main findings more effectively in the paper.\n\n2. **Metrics to evaluate training cost.**\n\nThank you for your insightful comments. The paper you referenced ([1]) raises an important point: relying on a single cost indicator (e.g., FLOPs, number of parameters, or training time) to compare the efficiency of models with different architectures can lead to varying trends depending on the chosen indicator. We completely agree with this perspective. However, **since most CL research employs the same model (e.g., regularization-based methods) or starts from the same model (e.g., model expansion-based methods), we believe that the cost indicators used in this paper (i.e., number of parameters and training time) can serve as a somewhat reasonable basis for comparing the training costs of different algorithms.** We acknowledge, though, that these indicators are not fully ideal for assessing efficiency in CL research. In this regard, we also appreciate the reviewer\u2019s suggestion to use the Memory-Adjusted-FLOPs (MAFs) metric from [2] as a potential alternative for future efficiency evaluations. Should the paper be officially published and the code made available, we will consider incorporating MAFs in future work.\n\n\n[1] Dehghani et al. 2021 (The Efficiency Misnomer)\n\n[2] Roth et al. 2023 (A Practitioner's Guide to Continual Multimodal Pretraining)"
            }
        },
        {
            "comment": {
                "value": "**1. The rationality of the proposed evaluation protocol.**\n\n**We would like to strongly argue that the evaluation method described by the reviewer may be common sense in some machine learning settings, but it is not a universal rule that applies to all learning scenarios**. In fact, effective evaluation in machine learning should prioritize realistic methods tailored to each learning scenario, rather than rigidly adhering to assumptions (e.g., i.i.d.) for theoretical convenience. As one example in machine translation (MT), evaluation often focuses on the model's ability to generalize to unseen data, and one common approach is indeed to separate training and validation/test data based on their time of creation, which results in distinct distributions. This time-based split helps measure how well models perform on more recent language usage that might not appear in the training data, reflecting a more realistic scenario for deployment (please check more details in [1]).\n\nAs outlined in Section 3.1, **we argue that the proposed evaluation protocol that separates the hyperparameter tuning and evaluation phases across different datasets offers a more realistic reflection of real-world continual learning scenarios (note that this protocol is also supported by both Reviewer 3Qrs and irJH as a strength of our paper)**. We are eager to discuss this Weakness further and to engage in a constructive discussion.\n\n[1] ACL 2016 FIRST CONFERENCE ON MACHINE TRANSLATION (WMT16)\n\n**2. Considering other continual learning settings.**\n\nWe would like to emphasize that it is challenging to cover all CL domains; therefore, **we focused our experiments on class-incremental learning, the most actively studied area in CL research[1,2]**. While we chose a single domain, we conducted an extensive evaluation on 15 of the most representative algorithms, covering the progression from earlier to the most recent methods in the field. Our experiments reveal that in the conventional evaluation protocol, newer algorithms tend to have their CL capacity overestimated across various scenarios. As stated in the first paragraph of the Introduction, **the conventional evaluation protocol is widely adopted across most CL domains, making it reasonable to infer that similar issues likely exist in other domains as well**.\n\n[1] Three scenarios for continual learning, NeurIPSW 2018\n\n[2] A Comprehensive Survey of Continual Learning: Theory, Method and Application\n\n\n**3. Comparing with another paper.**\n\nThank you for introducing an interesting paper as relevant prior work. The primary contribution of this work is the proposal of CLEVA-Compass (Continual Learning EValuation Assessment Compass), a visual framework that enhances the evaluation and transparency of various methods in Continual Learning. However, we would like to point out that, despite the publication of that work two years ago, the conventional evaluation protocol remains dominantly used (see the first paragraph of our Introduction for further context). Also, we wish to highlight some key differences between that work and our own. Specifically: 1) we focus on constructing a specific revised evaluation protocol for accurate assessment, and 2) through extensive experimentation, we bring to light critical issues with the conventional evaluation protocol. Additionally, **we believe there is a significant distinction between discussing proper evaluation methods and presenting extensive experimental results that highlight the flaws in the conventional evaluation protocol**. In this regard, our paper falls into the latter category, offering a distinctly different contribution compared to the paper mentioned by the reviewer. We will cite this paper and incorporate the above discussion into the revised version of our manuscript.\n\n**4. New name of the proposed protocol.**\n\nWe will assign a name to the proposed protocol to reduce ambiguity in a future update.\n\n**5. Implementation issues of DER**\n\nThank you for your comments regarding the implementation of DER. As the reviewer mentioned, Both PyCIL and the official code of DER do not include the implementation details for masking and pruning. However, **we would like to emphasize that, in the manuscript, we did not solely praise DER for achieving excellent performance. As highlighted in Figure 6(b) and (c) and in Lines 381-382, we pointed out the inefficiency from a parameter perspective due to the lack of pruning implementation**. We were aware of this implementation issue, but unfortunately, we overlooked adding this detail to the manuscript. We will be sure to include it in the future update. Additionally, **we would like to argue that our paper's main focus is not merely on showing that a particular algorithm achieves strong performance, but rather on exposing the issues in the conventional evaluation protocol through extensive experiments**.\n\nWe hope that the reviewer will carefully consider these responses and engage in an active discussion."
            }
        },
        {
            "comment": {
                "value": "First, thank you for recognizing the issues with the conventional evaluation protocol and for appreciating our efforts in identifying these issues and conducting extensive experiments.\n\nOur responses to the weaknesses you mentioned are outlined below.\n\n**1. Concerns on the proposed two-phase protocol**\n\nFirst, while we applied our proposed two-phase protocol specifically to class-incremental learning, **we would like to emphasize that the high-level concept of this protocol can be broadly applied across various domains of continual learning**. The CL domains mentioned by the reviewer can indeed be accommodated within our framework, as illustrated in Figure 2, where different datasets are used to generate CL scenarios. For instance, to account for imbalanced classes per task, a CL scenario in both phases can be designed based on this factor. In conclusion, we believe that the high-level concept of our two-phase protocol allows for assessing the generalizability of each algorithm\u2019s CL capacity across other domains.\n\n**2. The experiments only consider class-incremental learning**\n\nAs mentioned in the first paragraph of the Introduction section, we would like to reiterate that a flawed conventional evaluation protocol is still widely used across most domains. Given this, **we chose class-incremental learning as the representative domain, as it is not only one of the most actively researched areas but also considered highly challenging than task- and domain-incremental learning[1,2]**. Also, we conducted extensive experiments on numerous scenarios using 15 of the most prominent class-incremental learning algorithms, revealing that newer algorithms tend to be overestimated in the conventional evaluation protocol. **This finding suggests that similar issues could likely arise in other continual learning domains that also rely on conventional, flawed evaluation protocols**.\n\n\n[1] Three scenarios for continual learning, NeurIPSW 2018\n\n[2] A Comprehensive Survey of Continual Learning: Theory, Method and Application, TPAMI 2024\n\n**3. Most of the algorithms have similar trends**\n\n**We would like to reiterate our claim that \u201cnewer algorithms tend to show lower generalizability in terms of CL capacity or face efficiency or instability issues even if they achieve high performance\u201d (refer to Lines 516\u2013525)**. In this context, please note once more the performance trends of the latest algorithms\u2014FOSTER, BEEF, and MEMO\u2014in Figure 4(b), where their performance differs markedly between $D^{HT}$ and $D^{E}$. Specifically, while model expansion-based methods like FOSTER and MEMO perform well on $D^{HT}$, they underperform on $D^{E}$ compared to methods like WA, PODNet, and BiC, which are order algorithms and maintain consistent model sizes. Additionally, although DER achieves high performance on both datasets, it has significant efficiency issues, as shown in Figure 6(b) and Lines 380\u2013382.\n\nFurthermore, Figure 8(b) experimentally demonstrates how performance rankings shift considerably from $D^{HT}$ to the two $D^{E}$ datasets. For instance, EASE, one of the latest representation-based methods, shows strong performance on $D^{HT}$, but its performance on $D^{E}$ is lower than that of relatively simpler prompt-based methods. Additionally, while Ranpac exhibits strong results across all datasets, our experiments reveal it suffers from serious instability in certain scenarios (see Figure 10 and graphs in Appendix C.2).\n\nAdditionally, note that Figures 5 and 9 again illustrate this trend, where recent algorithms perform significantly worse than earlier ones in many scenarios.\n\nBased on the above results, **we respectfully disagree with the reviewer\u2019s statement that 'the advanced methods exhibit consistent advantages'**. We are eager to discuss this weakness further and engage in a constructive conversation.\n\n**4. The training cost is independent of the hyperparameter issue.**\n\nWe would like to highlight the relationship between the proposed evaluation protocol and training costs. For instance, when utilizing cloud services with GPU resources, the cost is typically determined by usage time. The training duration for each algorithm (assuming the same model size and dataset) is primarily influenced by the number of computations required, including hyperparameter configurations\u2014especially the number of epochs. To accurately assess each algorithm\u2019s training cost (i.e., GPU usage time), it is crucial to compare training times using the optimal hyperparameters, which yield the best performance for each algorithm. **By employing our protocol to identify these optimal hyperparameters found from the hyperparameter tuning phase, we ensure a fair and effective comparison of each algorithm's efficiency in terms of training cost.**\n\nTo address the concerns, we have included references to the relevant sections and figures in the manuscript. We respectfully ask the reviewer to consider these responses and look forward to an active discussion."
            }
        },
        {
            "summary": {
                "value": "The paper proposes a more rigorous evaluation protocol for continual learning methods, emphasizing generalization to unseen scenarios. In contrast to the traditional approach, where hyperparameter tuning and performance measurement occur on the same sequential dataset, often without separation between test and validation sets, the authors propose separate hyperparameter tuning and evaluation phases. While the configuration of the continual learning scenario is identical for both stages, each uses a different dataset. The authors evaluate a number of class-incremental learning algorithms using this framework. Based on a range of experiments, they conclude that most modern class-incremental learning algorithms fail to achieve their reported performance under the new evaluation protocol."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The main strength of the paper is the extensive experimental evaluation conducted under a rigorous evaluation protocol, leading to an important insight\u2014the superior performance of some recent class-incremental methods may be due to meta-overfitting to the particular evaluation set through hyperparameter optimization. Challenging the dominant, flawed approach to evaluating continual learning algorithms is a valuable contribution that will hopefully help steer the community towards a more disciplined approach and help identify methods that have a good chance of generalizing to real-world applications."
            },
            "weaknesses": {
                "value": "Poor presentation and structure are the main weaknesses of the paper. Figure 4 (b) is perhaps the most important result, yet it is not given a prominent place. Figure 3 and Figure 7 could easily be short tables. Figure 1 and 2 should be simplified and would work together as a side-by-side comparison. Limiting the analysis to the 10-task and 20-task scenario, respectively, would allow to simplify Figures 5 and 9 and make them easier to parse. BEEF should be dropped from the figures (and, arguably, the analysis) if the authors were not able to run it. The hyperparameter sets in B.1 and B.2 would be easier to read as tables.\n\nAnother weakness is the use of the number of parameters and training time, which are not reliable proxies for efficiency, as explained in Dehghani et al. 2021 (The Efficiency Misnomer). For an efficiency metric in continual learning, see Roth et al. 2023 (A Practitioner's Guide to Continual Multimodal Pretraining)."
            },
            "questions": {
                "value": "For BEEF, have you tried a different implementation or different seeds?\n\nIn Figure 4 (b), why do almost all methods perform better on the unseen scenario?\n\nWhat criterium did you use to select the methods for evaluation? Is it their availability in PyCIL and PILOT?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to tackle the class-incremental learning problem, which is important to the machine learning field. The authors come up with a new evaluation protocol to investigate CIL methods of generalization. The authors have done extensive experiments to investigate the performance of different methods."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.\tThis paper aims to tackle the class-incremental learning problem, which is important to the machine learning field.\n2.\tThe topic of hyper-parameter robustness is interesting and has not been investigated in the CIL field\n3.\tThe authors have done extensive experiments to investigate the performance of different methods."
            },
            "weaknesses": {
                "value": "1.\tAlthough the authors have done extensive experiments in their new CIL setting, my major concern lies in the rationality of it. In typical machine learning scenarios, the training and testing data are i.i.d. sampled from the same training set. In other words, we train a model, evaluate it on the validation set, and utilize the best model to test on the test set (which has the same data distribution as the validation set). However, the authors advocate using the different data distributions for validation and testing, which is against common sense in typical machine learning. After reading the introduction, I would expect the authors to separate the original testing set into two disjoint sets for the current evaluation.\n2.\tAlthough the title is about continual learning, I find the experiments only focus on the class-incremental learning scenario. I would expect more interesting results in other continual learning settings like task-incremental learning, domain-incremental learning, learning with pre-trained vision-language models, etc.\n3.\tHow to holistically evaluate a CIL algorithm has been also explored in another ICLR paper, i.e., [1], which extensively discusses the capability of different continual learning algorithms. In this aspect, this paper seems to advocate a typical case of CLEVA-Compass, making the contribution limited.\n4.\tThe topic of this paper seems to be too narrow on the generalization ability, which is different from the typical CIL setting. I would suggest the authors name the protocol with some new name to avoid ambiguity.\n5.  Finally, I also noticed a critical fact that leads to wrong conclusions. As the authors figure out from the main paper, DER is the most robust class-incremental learning algorithm. However, as they are using the PyCIL package, the reproduced DER is also not the full version, which does not implement the masking and pruning process in DER. See https://github.com/G-U-N/PyCIL/blob/31f2372d374c3f9a6c86d82b3c3ea4e0a880db63/models/der.py#L1C104-L1C124 (PyCIL's implementation), https://github.com/Rhyssiyan/DER-ClassIL.pytorch (DER official repo),\nand \nhttps://arxiv.org/pdf/2103.16788 (Eq.8 to Eq. 10). The main reason, I assume, is that the masking and pruning functions are also not robust and cannot be reproducible. Hence, using such code for comparison obviously leads to unfair comparisons among different methods.\n\n[1] CLEVA-Compass: A Continual Learning EValuation Assessment Compass to Promote Research Transparency and Comparability. ICLR 2022"
            },
            "questions": {
                "value": "See Weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper argued that the commonly used protocol of selecting hyperparameters for continual learning, which often select the best hyperparameter values within a given scenario and then evaluate the continual learning methods with these hyperparameters within the same scenario, is impractical in real-world applications. The authors then proposed an evaluation protocol consisting of a hyperparameter tuning phase and an evaluation phase on different datasets. The authors reported the performance variance of representative methods with this new protocol."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. I appreciate the claim that the commonly used protocol of selecting hyperparameters for continual learning methods may not be optimal in applications, given that the old training samples are largely inaccessible.\n\n2. The authors perform extensive experiments with a variety of continual learning methods under the proposed evaluation protocol."
            },
            "weaknesses": {
                "value": "This paper is essentially based on intuitive ideas and the empirical results are not very clear. It fails to cover many critical considerations in real-world applications.\n\n1. The authors highlighted for many times that the two phases share the same scenario configuration (e.g., number of tasks) but are generated from different datasets. However, this consideration cannot fully reflect the possible differences across continual learning tasks, such as imbalanced classes per task, imbalanced training samples per class, blurred task boundaries, different task types, etc.\n\n2. The experiments only consider class-incremental learning, rather than other typical scenarios such as task-incremental learning and domain-incremental learning. \n\n3. Although continual learning methods show some performance differences between the two phases, most of them have similar trends (Figures 4 and 8). This reduces the significance of the proposed protocol, since the advanced methods exhibit consistent advantages.\n\n4. The authors further analyzed the training cost. I agree that the training cost is a critical issue for continual learning, but it is almost orthogonal to the hyperparameter issue and independent of the proposed evaluation protocol."
            },
            "questions": {
                "value": "My major concerns lie in the coverage of hyperparameter issues in real-world applications and its relevant to the training cost. Please refer to the Weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}