{
    "id": "KISgRiGCKS",
    "title": "Optimal Transport-Based Domain Alignment as a Preprocessing Step for Federated Learning",
    "abstract": "Federated learning is a subfield of machine learning that avoids sharing local data with a central server, which can enhance privacy and scalability. The inability to consolidate data in a central server leads to a unique problem called dataset imbalance, which can be challenging because learning typically requires iid datasets. In FL, fusing locally-trained models that are trained using non-iid datasets may deteriorate the performance of global model aggregation; this further reduces the quality of updated local models and the accuracy of the distributed agents' decisions. In this work, we introduce an Optimal Transport-based preprocessing algorithm that aligns the datasets in a privacy-preserving manner, in turn minimizing the distributional discrepancy of data along the edge devices. We accomplish this by leveraging Wasserstein barycenters when computing channel-wise averages. These barycenters are collected in a trusted central server where they collectively generate a target RGB space. By projecting our dataset towards this target space, we minimize the distributional discrepancy on a global level, which facilitates the learning process due to a minimization of variance across the samples in the analyzed network. We demonstrate the capabilities of the proposed approach over the CIFAR-10 dataset, where we show its capability of reaching higher degrees of generalization in fewer communication rounds.",
    "keywords": [
        "Federated Learning",
        "Optimal Transport",
        "Domain Alignment",
        "Data Preprocessing"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "To overcome the data heterogeneity problem in federated learning, we develop an optimal transport-based preprocessing algorithm that improves learning by projecting data closer to each other in space.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=KISgRiGCKS",
    "pdf_link": "https://openreview.net/pdf?id=KISgRiGCKS",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes a preprocessing step for federated learning aimed to reduce distributional discrepancy between clients via optimal transport. It computes the channel-wise Wasserstein barycenters on each client and sends them to a trusted server where global Wasserstein barycenters are then computed as the target space and broadcasted to clients. Each client projects their data on the target space. Projected client data are fed into any FL learning algorithm. Experiments demonstrate that this preprocessing step pairing with FedAvg outperforms a number of baseline algorithms."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The proposed preprocessing step is interesting, simple yet effective in boosting FL learning performance."
            },
            "weaknesses": {
                "value": "Some concerns are as follows:\n1) technical exposition in Section 3.2 is poor. For example, I have no idea what Lines 184-185 mean. There is no definition for $\\mathcal{L}\\_{d^P}(\\cdot,\\cdot)$ in Eq. (4), $\\Sigma\\_n$ in Eq.(5), $W\\_{reg}$ in Eq. (7). Why $\\lambda\\_{s}\\in\\Sigma\\_{n}$ in Line 215?\n2) I feel confusing in Lines 339-350 regarding the number of epochs when the number of clients is small. In Lines 339-341, it seems to use a large number of epochs when P is small. But in Line 345 it seems to use a small number of epochs when P is small and in Line 349 it says that more agents less data. \n3) In Line 469, could you explain why \"we do so with a model of fewer parameters\"?\n4) it would be better if more experiments are presented where the preprocessing step pairs with more FL algorithms other than FedAvg.\n5) The exposition of the proposed preprocessing step is instantiated with image data which has RGB channels. What if other data is given?\n6) This preprocessing step relies on a trusted central server for privacy, which doesn't seem like a rigorous privacy guarantee."
            },
            "questions": {
                "value": "see above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel preprocessing step for federated learning that uses the tools of Optimal Transport. To summarize, it computes the Wasserstein barycenter of each client and collects them to compute a global barycenter. The clients then project local data to the global one in order to align the data with others. Experiments show significant improvement after the alignment compared with non-aligned baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The author introduces a novel method using Optimal Transport to address the data heterogeneity in federated learning. \n2. The method is impressive overall and seems easy to apply to different FL methods."
            },
            "weaknesses": {
                "value": "1. Overall presentation is below average: Many of the notations in section 3.2 are not explained. Also, the authors should focus on the meaning and utility of OT, instead of the formula derivation. Section 4 is lack of description. The authors should explain the whole process in detail by bullet list or paragraphs with subtitles based on Figure 2,3.\n2. Weak Experiments: The comparisons are mostly with non-aligned FL methods but only one (CCVR) baseline. More baselines should be included to demonstrate OT to be a good preprocessing step."
            },
            "questions": {
                "value": "1. Please clarify the concepts and notations mentioned in Weakness 1 above. For example, section 3.2 introduced OT, but how WB was calculated is still unclear.\n2. As noted in Weakness 2, the author mainly chooses non-aligned FL methods as the baseline. Could other preprocessing methods, like simple regularization, be included in the comparison? If not, why are further comparisons unavailable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose an optimal transport-based preprocessing step in federated learning to align datasets of different clients. This alignment step is run on each RGB color space separately using Wasserstein barycenters. After that, the datasets are projected toward the  target (aggregated) space. The authors show improvements over baselines empirically."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is well motivated and clearly written.\n\n- As far as I know, use of optimal transport for domain alignment is novel in federated learning."
            },
            "weaknesses": {
                "value": "- The paper mentions several times that the proposed preprocessing algorithm would preserve privacy. However, I could not find a clear definition of privacy notion the paper is referring to. Could the authors clarify what privacy notion they're targeting and how they compute/measure the privacy guarantees?"
            },
            "questions": {
                "value": "See above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes an optimal transport preprocessing step for federated learning in order to align the data distributions between the clients. The main idea is to treat the alignment as a hierarchical Wasserstein barycenters problem, where each client first summarizes their local data into an \u201caverage image\u201d by computing a Wasserstein barycentre per image channel. These local barycenters are then communicated to the server, where new Wasserstein barycentres are computed across the local ones. These \u201cglobal\u201d barycenters are then communicated back to the clients and each one projects their data distribution onto them. After this preprocessing step, a standard federated learning algorithm such as FedAvg can be applied on top of these new input representations. The authors evaluate their method on CIFAR 10, showing improvements upon vanilla FedAvg and other baselines."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The main strength of this work is the simplicity of the proposed method. As it is a straightforward preprocessing step, it can be orthogonal to various other methods proposed for non-i.i.d. data in federated learning, potentially improving them even more."
            },
            "weaknesses": {
                "value": "I believe that this work has several axes for improvement. \n\n# Clarity\nThere are quite a few instances where the manuscript is unclear and could be written better. More specifically:\n- Important implementation details of the method are missing, i.e., what transport cost is used, what is the algorithm used for OT (e.g., is it Sinkhorn iterations?), what are its hyper parameters (if it is Sinkhorn iterations, what is the regularization parameter? This is important as it relates to the complexity analysis) and how is the projection to the global barycenters done. These need to be either in the main paper or in an appendix.\n- The authors do not mention the dataset details in the experimental section (there is just a single mention of CIFAR-10 in the abstract) and what optimization hyper parameters where used for FedAvg. These are important for reproducibility and need to also be either in the main paper or the appendix. \n- The notation in equations is a bit sloppy and needs to be improved. More specifically:\n    - In Eq. 2 there is a $\\sum_i$ but no index $i$ is present in the summand\n    - Above Eq. 4 the definition of $(D^p_{i,j})_{i,j} \\in R^{n\\times n}$ is unclear; does this mean that each entry of the distance / cost matrix is itself an $n\\times n$ matrix? Furthermore, there is a $diag(A) = 0$ introduced, but no mention of what $A$ is, and it is not clear what $\\forall (i,j,k) [[n]]^3$ means and why it is important. \n    - After Eq. 5 the authors optimise the Wasserstein barycentre $a \\in \\Sigma_n$ but there is no mention of what $\\Sigma_n$ is.  Also it seems that also $\\lambda_s \\in \\Sigma_n$ later on (after Eq. 7) which is a bit counterintuitive as one is a vector and the other is a scalar. \n    - After Eq. 7 the authors mention $WB(B)$ but there is no mention of what $B$ is. \n\n# Evaluation\nIn my opinion, there are quite a few ways that the evaluation can be improved:\n- There seems to be only one dataset (assuming CIFAR-10 from the abstract?) and architecture used. This is not enough to get a strong signal for the usefulness of this method. Other datasets (such as TinyImagenet and CIFAR 100) and architectures (such as ResNets and ViTs) are important to provide a broader picture. \n- It is not clear what the non-i.i.d.-ness is the data is. Uniform sampling without replacement should intuitively lead to more or less i.i.d. data across the clients, thus making OT-alignment not important. Prior works (such as McMahan et al. 2017) have more elaborate ways to generate non-i.i.d. data (by e.g., having each client only observe a subset of the classes). Furthermore, as the main premise of this work is domain alignment, I would also have expected a more elaborate evaluation of various non-i.i.d. settings, ranging from covariate shift (same $p(y)$ but different $p(x|y)$), to label skew (different $p(y)$ but same $p(x|y)$) or any mixture of the two. Intuitively, preprocessing the data should mainly help for the first setting as opposed to the second, so it would be beneficial if the authors update the manuscript with such cases. \n- The final performance improvement is a bit hard to believe. The authors argue that their OT alignment step leads to ~ +28% better accuracy on their dataset against vanilla FedAvg (which is a very big improvement). If we take this claim at face value, then this would imply that this small architecture gets SoTA on CIFAR-10 (if I compare it with results shown in https://paperswithcode.com/sota/image-classification-on-cifar-10 where a much larger/deeper ViT-H/14 model gets 99.5%), which is also a bit hard to believe. \n\n# Novelty\nWhile the authors do discuss quite a few related works in the related work section, they unfortunately missed what seems to be a crucial related work, FedOT [1]. There the authors do also discuss about projecting the data distributions of each of the clients to a common space and they perform this step with optimal transport. Therefore, this weakens the novelty of this work.\n\n[1] An Optimal Transport Approach to Personalized Federated Learning, Farnia et al., 2022, https://arxiv.org/abs/2206.02468"
            },
            "questions": {
                "value": "The questions stem from the prior discussion of the weaknesses of this work. To reiterate:\n\n# Clarity\n- The authors should elaborate and extensively discuss the implementation details of the their method. Questions such as what transport cost are important and relevant for reproduction.\n- The notation should be improved upon, with clear explanation of symbols.\n\n# Evaluation\n- The authors should increase the breadth and depth of the evaluation of their method, especially given its simplicity. Some recommendations are\n    - OT-alignment combined with other federated learning methods\n    - Evaluation on other datasets and architectures\n    - More involved client data distribution shifts and how OT alignment improves performance\n    - Perhaps a visual interpretation of what the Wasserstein barycenters look like\n    - Clarification of how such a big performance improvement is possible with OT alignment. Maybe a relevant baseline is FedAvg on i.i.d. data splits, as that would highlight what is the best possible performance of OT alignment. Furthermore, are the numbers of the other baselines discussed at Table 2 taken from the respective papers or reimplemented? This is important, as hyperparameter details (especially with how the data are partitioned between the clients) can have a big impact on model behaviour. \n    - The privacy claims are a bit vacuous, given that without formal privacy guarantees, FedAvg can break (e.g., see the work at [2]). I would encourage the authors to either substantiate the privacy claims (by e.g., an empirical evaluation) or removing them from the paper altogether. \n\n[2] Inverting Gradients \u2014 How easy is it to break privacy in federated learning? Geiping et al., 2020, https://arxiv.org/abs/2003.14053"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}