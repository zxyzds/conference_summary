{
    "id": "PklYedVFUW",
    "title": "DSR: Reinforcement Learning with Dynamical Skill Refinement",
    "abstract": "Reinforcement learning with skills (RL with skills) is an efficient paradigm for solving sparse-reward tasks by extracting skills from demonstration datasets and learning high-level policy which selects skills. Because each selected skill by high-level policy is executed for multiple consecutive timesteps, the high-level policy is essentially learned in a temporally abstract Markov decision process (TA-MDP) built on the skills, which shortens the task horizon and reduces the exploration cost. However, these skills are usually sub-optimal because of the potential low quality and low coverage of the datasets, which causes the sub-optimal performance in the downstream task. Refining skills is intuitive, but the change of skills will in turn lead to the non-stationarity of the transition dynamics of TA-MDP which we name temporal abstraction shift. To address the dilemma of sub-optimal skills and temporal abstraction shift, we unify the optimization objectives of the entire hierarchical policy consisting of the high-level policy and the low-level policy whose latent space embeds the skills. We theoretically prove that the unified optimization objective guarantees the performance improvement in TA-MDP, and that optimizing the performance in TA-MDP is equivalent to optimizing a lower bound of the performance of the entire hierarchical policy in original MDP. Furthermore, in order to overcome the phenomenon of skill space collapse, we propose the dynamical skill refinement (DSR) mechanism which names our method. The experiment results empirically validate the effectiveness of our method, and show the advantages over the state-of-the-art (SOTA) methods.",
    "keywords": [
        "Reinforcement Learning",
        "Reinforcement Learning with Skills",
        "Reinforcement Learning with Demonstrations"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "A reinforcement learning method of solving sparse-reward tasks using skills extracted from demonstration datasets.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=PklYedVFUW",
    "pdf_link": "https://openreview.net/pdf?id=PklYedVFUW",
    "comments": [
        {
            "title": {
                "value": "Change in my rating"
            },
            "comment": {
                "value": "Dear author,\nThanks for replying to my comments.\n\nI was uncertain about my score. After reading other reviewers' comments and noticing their confidence in their ratings, I decided to lower my score."
            }
        },
        {
            "title": {
                "value": "May I ask why you changed your rating?"
            },
            "comment": {
                "value": "Dear reviewer:\n\ufeff\nI am confused as to why you suddenly made significant changes to the rating? I plan to further update the content of the rebuttal and the paper. May I get your advice?\n\ufeff\nthank you."
            }
        },
        {
            "comment": {
                "value": "**About the Weakness**\n* **Weakness 1** The solution to temporal abstraction shift is to update both the high-level policy and the low-level policy in an on-policy RL manner, rather than the theorems mentioned in this paper. The reason why we propose these theorems is that previous works simply learn the hierarchical policy in the TA-MDP, without theoretically analyzing the effectiveness. Namely, our theorems answer the question why skill-based RL works. Combining the on-policy RL manner with our theorems can ensure the performance improvement in the original MDP while avoiding temporal abstraction shift.\n* **Weakness 2** In this paper, the input of RND is state-skill, instead of state only. Therefore, a smaller RND prediction error will let only a skill's behavior in the given state be refined to a greater extent. In the experiment, we found that only the parameters of the mapping function that maps RND errors to the weight of action increments require special tuning. We provide an operation example in Appendix D.4 to determine the appropriate hyper-parameters of the mapping function using a small number of online experiences.\n* **Weakness 3** We believe that the essential difference between navigation tasks and robotics is that in navigation tasks, skill space is usually represented by the state space. In this case, skill-based RL will be transformed into goal-conditional RL. Similarly, ReSkill has also been validated totally in robotics tasks. **We will try to add new experiments during the rebuttal period, and if time permits, we will resubmit a new version of the paper.**\n\n**Answers to Questions**\n* **Question 1** We repeatedly sample fixed-length state-action segments from the trajectories in the dataset. Subsequently, we use an LSTM to map the state-action segments into feature vectors. Encoder maps feature vectors to the embedding space. Finally, decoder conditioned on the embedding and the states attempts to reconstruct the actions in the segments. The reconstruction error and regularization term will be used for joint training of the entire VAE and LSTM.\n* **Question 2** ReSkill, like previous works (e.g. [1,2,3]), learns high-level policies in TA-MDP without explaining why this is effective. Because the original task of maximizing the expected return in the original MDP has been transformed into maximizing the expected return in TA-MDP. ReSkill even learns high-level policy in TA-MDP and low-level policy in Original MDP respectively, which exacerbates the interpretability problem of skill-based RL.\n  * **We novelly prove that optimizing the expected return in TA-MDP is equivalent to optimizing the expected return of the entire hierarchical policy in the original MDP, which is shown in Theorem 3. ($\\forall s\\in\\mathcal{S},\\tilde{\\gamma}\\cdot V^h_{\\pi^h,\\pi^l}(s)\\leq V_{\\pi^h,\\pi^l}(s)$)**\n  * We have presented a optimization objective of refining the low-level policy in Figure 3 and demonstrated in Theorem 1 that this objective can ensure the performance improvement in TA-MDP. ($\\forall s\\in\\mathcal{S},V^h_{\\pi^h_{\\phi'},\\pi^l_{\\theta'}}(s)\\geq V^h_{\\pi^h_{\\phi},\\pi^l_{\\theta}}(s)$)\n  * Combining Theorem 2 and Theorem 3, it is not difficult to find that under this unified optimization objective, it is equivalent to optimizing a lower bound of the performance of the entire hierarchical policy in the original MDP. This provides an unprecedented theoretical foundation for skill-based RL, and the key to all of this is that the discount factor $\\tilde{\\gamma}$ in TA-MDP and the discount factor $\\gamma$ in the original MDP satisfy the following condition: $\\tilde{\\gamma}=\\gamma^H$.\n  * The solid theoretical arguments mentioned above are not present in ReSkill and [1,2,3]. However, since the discount factor $\\gamma$ is usually set to a number close to 1, such as 0.99, its H-power $\\gamma^H$ is close to itself, which is why those works are also effective in the experiments. **We believe that in the field of skill-based RL, the theoretical foundation lags far behind experiments, and as increasingly complex environments are adopted, we need to first address the key issue why it is effective.**\n \n\n**References**\n\n[1]. Karl Pertsch, Youngwoon Lee, and Joseph Lim. Accelerating reinforcement learning with learned skill priors. In Conference on robot learning, pp. 188\u2013204. PMLR, 2021.\n\n[2]. Karl Pertsch, Youngwoon Lee, Yue Wu, and Joseph J Lim. Guided reinforcement learning with learned skills. In Conference on Robot Learning, pp. 729\u2013739. PMLR, 2022.\n\n[3]. Ce Hao, Catherine Weaver, Chen Tang, Kenta Kawamoto, Masayoshi Tomizuka, and Wei Zhan. Skill-critic: Refining learned skills for hierarchical reinforcement learning. IEEE Robotics and Automation Letters, 2024."
            },
            "title": {
                "value": "Dear reviewer, the following are responses to your questions"
            }
        },
        {
            "summary": {
                "value": "The paper introduces Dynamical Skill Refinement (DSR), an on-policy reinforcement learning method designed to optimize hierarchical policies in environments with sparse rewards. The paper refines the skills in an on-policy manner and proves that the temporal abstraction shift is circumvented by simultaneously updating the high-level policy and skills. The method prevents skill space collapse, which can lead to performance issues, by incorporating a dynamical mechanism that refines skills without disrupting the latent space. Empirical results demonstrate that DSR outperforms state-of-the-art methods in complex sparse-reward robotic manipulation tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1- The paper is well-written and easy to read.\n2- The proofs for theorems are solid.\n3- Good comparisons with SOTA methods."
            },
            "weaknesses": {
                "value": "1- The main difference between your approach and ReSKILL is that yours handles the temporal abstraction shift but it is not shown in your theorems.\n2- This method needs lots of extra parts such as an extra residual policy to avoid skill collapse. Also, using RND makes sure that a skill is refined enough but still you cannot make sure that modifying the skill does not impact the state of other skills in the same state. Moreover, RND is an approach that needs lots of tuning such as how you control the learning rate of the variable network and the threshold for being accurate enough for the refinement. This would impact the results and we expected to see an analysis of these parameters.\n\n3- Usually we have complex navigation robotic environments in the experiments with skills as the macro-action to reach the subgoals or landmarks. We do not see any navigation environment here."
            },
            "questions": {
                "value": "1- Can you explain again how skills are extracted using VAE?\n2- ReSKILL has a similar on-policy approach as your method. You said that ReSKILL cannot theoretically guarantee performance improvement, but you did not mention proof. Can you elaborate more on this?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper aims to address two issues in RL with skills: **(1)** sub-optimal pretrained skills due to low quality and low coverage of the datasets, and **(2)** temporabl abstraction shift due to refining skills. The authors propose to unify the optimization objectives of both high-level and low-level policies as the future return and use random network distillation to dynamically adjust the weight of action refinement. The paper demonstrates the effectiveness of the proposed method by providing theoretical analysis and empirical results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The proposed method is well-motivated, theoretically grounded, and demonstrates better performance against several state-of-the-art RL-with-skills methods.\n* The author uses multiple figures to improve the clarity of the paper."
            },
            "weaknesses": {
                "value": "While well-motivated, I have some questions about this work:\n\n1. In the introduction, the authors mention that previous works (e.g., SPiRL [1]) assumes near-optimal dataset to pretrain the skills. However, even if the skill contains some sub-optimal behavors, could the high-level policy still learns to avoid choosing the bad skills as it aims to maximize the reward?\n2. From the theoretical perspective, how is DSR different from the classic online hierarchical RL methods? For instance, [2] updates both high-level and low-level policies with RL objective with representation learning, and also provides the optimization objective bound. A similar bound is provided in [3] for offline settings. These two bounds actually quantify the exact form of sub-optimality, hence I think they are stronger than Theorem 3 in this work. I would appreciate it if the authors could discuss these methods in the paper.\n3. How well does DSR can transfer skills to new tasks? I think one promise of skill-based RL is that it can conbine the learned skills to novel tasks, as mentioned in [1]. I am wondering if the authors could show some results in their experimental setting, such as training on data from one task and evaluating on other tasks.\n\nOverall, while this paper presents some interesting ideas, I am unable to recommend acceptance at this stage given the questions mentioned above. However, I would consider raising my score if the authors could address my concerns.\n\n[1] Pertsch, Karl, Youngwoon Lee, and Joseph Lim. \"Accelerating reinforcement learning with learned skill priors.\" Conference on robot learning. PMLR, 2021.\n\n[2] Nachum, Ofir, et al. \"Near-optimal representation learning for hierarchical reinforcement learning.\" *arXiv preprint arXiv:1810.01257* (2018).\n\n[3] Yang, Yiqin, et al. \"Flow to control: Offline reinforcement learning with lossless primitive discovery.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 9. 2023."
            },
            "questions": {
                "value": "There are some questions and concerns, which I have outlined in the previous section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper addresses the problem of the sub-optimal dataset in the skill-based reinforcement learning settings. The authors propose a unified optimization objective to train a hierarchical skill structure, providing a mathematical proof within the TA-MDP. Additionally, the authors introduce a dynamic skill refinement approach to mitigate potential collapse in the skill space."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Learning skills from sub-optimal datasets and overcoming skill space collapse are crucial challenges in skill-based reinforcement learning scenarios."
            },
            "weaknesses": {
                "value": "- I am not fully convinced how the unified optimization objective addresses the issue of sub-optimality in the datasets. DSR learns the low-level policy, making it more likely to optimize the action sequence for the downstream policy. If this is the case, the authors should emphasize more on mitigating the collapse in the skill space. \n- The contribution on dynamic skill refinement (Section 5.2) seems minimal, and its benefits are not fully demonstrated in Ablation 6.2, as DirectRefinement also shows comparable performance. Otherwise, the authors should provide more ablation studies on the learning of TA-MDP.\n- In Section 6.1, the authors mention that DSR improves performance by avoiding skill space collapse. However, in Ablation 6.2, skill space collapse appears to occur only in the PyramidStack task. If the authors want to emphasize their contributions regarding skill space collapse, they should create more scenarios similar to PyramidStack and elaborate on why these tasks cause the collapse, and why it does not occur in other tasks.\n- The presentation could be further improved. Figure 4 is smaller compared to Figure 2, and Figure 3 could be replaced with an equation for better clarity. For clarity, it would be better to include the pseudocode in the main manuscript rather than in Appendix C."
            },
            "questions": {
                "value": "- As the paper addresses the problem arising from the sub-optimality (low quality and low coverage) of the datasets, the authors should elaborate more on their dataset collection regarding to its sub-optimality.\n-  In Figure 6, the y-axis denotes the episodic return. How is the episodic return computed? Additionally, can the authors elaborate more on their sparse reward setting? Is a reward given only upon task completion?\n- In Ablation 6.2, what do the authors mean by \"the extracted skills are not well initialized\"? In Appendix D.2, the authors mention using datasets collected from TableCleanup, SlipperyPush, and PyramidStack, but why does only the PyramidStack task show this performance gap in Ablation 6.2?\n- According to the original ReSkill paper, ReSkill achieves an average return of over 15 in the PyramidStack task. However, in Figure 6, ReSkill achieves only an average return of 10. The authors mentioned that the experimental settings are equivalent to those in ReSkill as described in Appendix D.2. Why is there a performance gap?\n- It would be benificial, if the authors provide more ablation studies on their learning mechanism of TA-MDP. (Section 4)\n- Can the authors provide additional experiments on other robotic manipulation environments, including long-horizon scenarios such as MetaWorld or Franka Kitchen?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}