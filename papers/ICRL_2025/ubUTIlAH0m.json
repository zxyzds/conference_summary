{
    "id": "ubUTIlAH0m",
    "title": "Multi-Physics Operator Network for In-context learning (m-PhOeNIX)",
    "abstract": "We propose a multi-physics operator network for simultaneous and sequential learning of solution operators of multiple heterogeneous parametric partial differential equations. Existing neural operators are adept at learning the solution operator of only a single physical system, and adapting to new physical equations requires training a new surrogate model from scratch with physics-specific intensive hyperparameter tuning. The proposed multi-physics neural operator leverages the recent advancements in wavelet-based kernel integral-induced neural operator modeling and instantiates a memory-based ensembling strategy for projecting heterogeneous physical systems into a common shared feature space. The local channel-level ensembling is supported by context gates, which not only utilize the shared features to embed the features of multiple heterogeneous physical systems into the network parameters but also allow the multi-physics operator to learn new solution operators by transferring knowledge sequentially; this allows the proposed model to continually learn without forgetting. We illustrate the efficacy of our algorithm by simultaneously and sequentially learning six complex time-dependent solution operators of six physical systems. The inference results on the simultaneous and sequentially trained models depict the ability to infer previously seen physical systems without fine-tuning and catastrophic forgetting, indicating the characteristics of a foundation model. The framework also demonstrates the super-resolution property and generalization to out-of-distribution input conditions.",
    "keywords": [
        "Multi-physics operator learning",
        "neural operator",
        "catastrophic forgetting",
        "continual learning",
        "wavelet"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "This framework does simultaneous and sequential learning of solution operators of multiple heterogeneous physical systems without catastrophic forgetting.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=ubUTIlAH0m",
    "pdf_link": "https://openreview.net/pdf?id=ubUTIlAH0m",
    "comments": [
        {
            "comment": {
                "value": "Dear authors and AC,\nWith the release of the reviews, I realized that I mistakenly swapped two of my reviews. The review I originally uploaded for this paper actually belonged to another paper. \nI apologize sincerely to the authors for this issue. \nI have uploaded the correct review.\nBest regards,"
            }
        },
        {
            "summary": {
                "value": "The paper presents m-PhOeNIX (Multi-Physics Operator Network for In-Context Learning), a model that combines local wavelet experts and context gates to enable multi-task and sequential learning for various physics-driven PDEs. The proposed framework aims to allow the model to learn new PDE systems without requiring extensive re-training, preventing catastrophic forgetting. However, significant limitations in theoretical rigor, computational efficiency, and a lack of sufficient validation experiments reduce the overall impact of the work."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The idea is interesting to use local wavelet experts and context gates to create a flexible framework for capturing multi-scale features across multiple physics systems."
            },
            "weaknesses": {
                "value": "1. The theoretical foundation of m-PhOeNIX is insufficiently developed. The authors need to provide a clearer theoretical rationale and motivation for combining the existing architectures. The paper also lacks a formal analysis of why Daubechies wavelets were chosen over other types of wavelets. \n\n2. Wavelet-based operations are typically more computationally expensive than FFT, especially for high-resolution data or real-time applications. However, the paper does not provide any benchmarks on runtime or memory usage. This information is essential to evaluate the model\u2019s practical viability in large-scale scientific applications.\n\n3. The use of multiple wavelet experts and context gates adds significant model complexity, which scales up with the number of experts and task diversity. This may introduce memory overheads, making m-PhOeNIX less scalable for high-dimensional PDEs.\n\n4. Boundary conditions significantly impact the solutions to PDEs, yet m-PhOeNIX does not explore how it would handle varying boundary conditions across tasks. A strategy for managing changing boundary conditions could be promising.\n\n5. The model is designed to handle cases where the underlying PDEs are unknown or complex, making it potentially valuable for real-world applications. However, the model has been tested only on synthetic data, limiting its demonstrated applicability. It is essential to test the model on real-world datasets and more complex, higher-dimensional systems. Such validation would provide stronger evidence for the model's claimed adaptability and robustness in handling diverse and realistic physical phenomena."
            },
            "questions": {
                "value": "See Weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a method for predicting spatio-temporal systems derived from various physical systems, based on neural operator while avoiding catastrophic forgetting. The model relies on wavelet neural operators experts and fine-tunes the mixing between these experts for each \"physics\" through context gates. The model is applied on various 1D and 2D PDEs including advection-diffusion, Nagumo-Burgers, Allen-Cahn, heat and wave equations."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper tackles a fundamental problem with neural operators, which is catastrophic forgetting\n- The paper provides a clear, concise and original integration of several non-trivial concepts\u2014sequential learning, wavelet neural operators, and local ensembling."
            },
            "weaknesses": {
                "value": "The main claim of the paper is that it presents an architecture designed to avoid catastrophic forgetting. This claim could be strengthened with more specific comparisons (see questions below)."
            },
            "questions": {
                "value": "- Fig.4, and Fig.5 indeed show that there is no degradation of performances when training on new datasets. However, I'm having a hard time assessing the risk of a catastrophic forgetting in that case. For example, if you have a large enough neural operator, that you train incrementally on the datasets you used in these figures, will you see this catastrophic forgetting? \n- on 1D data (Fig.4), when training on Allen-Cahn PDE dataset, it seems that the performances are slightly degraded on the Heat PDE: the performances are slightly worst on row 5 from column 2 to column 3, do you think it is significant? Isn't the model starting to forget about the Allen-Cahn PDE dataset? \n- l516: \"requires a small initial trajectory to learn the time-dependent solution operators\". Maybe I didn't understand what you meant, but how can we expect determining the solution operator without at least 2 time-frames? A single time frame doesn't give you the time operator. \n\nMinor comments / questions. \n- abstract, l028: \"indicating the characteristic of a foundation model\": it only indicates ONE characteristic that a foundation model should have. In particular, it seems to me that a foundation model should showcase that training on all these datasets is actually beneficial for each dataset, which your results don't necessarily show.\n- l327,328: I'm having a hard time convincing myself that this is a correct metric to measure the \"distance\" between two datasets, and in particular that this is a good notion of distance between operators as it is argued l330. For example, if I have the same advection-diffusion operators, but initial conditions that are completely different, e.g. non-overlapping, it seems that you can get distances that are quite small \n- l490-493. I am not certain that the number of parameters is the relevant measure here. In particular, in a transformer model, such as a Transformer like MPP, that you compare with, reducing the spatial downsampling should increase drastically the performances of the model, while not changing the number of parameters at all."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a neural operator designed to solve multiple physics problems simultaneously, thus removing the need to train distinct models for individual partial differential equations (PDEs). Additionally, the method employs an ensemble strategy to facilitate knowledge transfer when learning from new data, reducing the risk of overwriting pre-trained information during fine-tuning downstream tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The author conducts comparative experiments across a broad range of PDEs with varying dimensions in both single-task and joint training settings, which enhances the transparency of the proposed method\u2019s performance.\n\n- Including a demonstration showing similarities among each PDE dataset in the experiments section is commendable, as it highlights the model\u2019s ability to generalize across diverse problem sets."
            },
            "weaknesses": {
                "value": "- In the \u201cComparison against existing multiphysics operators\u201d section in Numerical Illustration, it is unclear whether the reported results for ICON and AVIT are from 1) their pre-trained models or 2) from versions fine-tuned for each downstream task. If based on the first, it would be more rigorous to include their fine-tuned results for a fair comparison. For instance, MPP outperforms FNO on certain PDEs in their paper [1].\n\n- To improve the clarity of the paper, consider including model parameters as a column in Table 1, along with such measurements for models in the problem-specific cases. For better readability, note that the term \u201cparameter\u201d is used in multiple contexts, leading to potential confusion, which could be mitigated by rephrasing in some cases. For instance, when describing the number of data. Additionally, consider adding punctuation to lengthy sentences, such as the second-to-last sentence in the \u201cProblem-specific comparison\u201d section.\n\n[1] McCabe, Michael, et al. \"Multiple physics pretraining for physical surrogate models.\" arXiv preprint arXiv:2310.02994 (2023)."
            },
            "questions": {
                "value": "- Is there an intuition for why most of the 2D data on the same PDE exhibits a lower relative L2 error compared to 1D cases?\n\n- How many local wavelet experts were used, and does changing this number affect performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}