{
    "id": "PDgZ3rvqHn",
    "title": "Select before Act: Spatially Decoupled Action Repetition for Continuous Control",
    "abstract": "Reinforcement Learning (RL) has achieved remarkable success in various continuous control tasks, such as robot manipulation and locomotion.\nDifferent to mainstream RL which makes decisions at individual steps, recent studies have incorporated action repetition into RL, achieving enhanced action persistence with improved sample efficiency and superior performance.\nHowever, existing methods treat all action dimensions as a whole during repetition, ignoring variations among them.\nThis constraint leads to inflexibility in decisions, which reduces policy agility with inferior effectiveness. \nIn this work, we propose a novel repetition framework called SDAR, which implements Spatially Decoupled Action Repetition through performing closed-loop act-or-repeat selection for each action dimension individually.\nSDAR achieves more flexible repetition strategies, leading to an improved balance between action persistence and diversity.\nCompared to existing repetition frameworks, SDAR is more sample efficient with higher policy performance and reduced action fluctuation.\nExperiments are conducted on various continuous control scenarios, \ndemonstrating the effectiveness and necessity of spatially decoupled repetition design proposed in this work.",
    "keywords": [
        "Reinforcement Learning",
        "Action Repetition"
    ],
    "primary_area": "reinforcement learning",
    "TLDR": "A novel reinforcement learning method introduces an act-or-repeat mechanism for individual action dimensions, enhancing sample efficiency and final performance with reduced action fluctuation.",
    "creation_date": "2024-09-25",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=PDgZ3rvqHn",
    "pdf_link": "https://openreview.net/pdf?id=PDgZ3rvqHn",
    "comments": [
        {
            "summary": {
                "value": "This paper introduces Spatially Decoupled Action Repetition (SDAR), a framework that allows reinforcement learning (RL) agents to make separate act-or-repeat decisions for each action dimension. The method consists of a two-stage process: first, a selection policy determines if each dimension should repeat, followed by an action policy that generates new actions for those dimensions that chose not to repeat. Experiments across several continuous control tasks demonstrates the framework's performance and sample efficiency over baseline methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The proposed method decouples the action dimensions and achieves more flexible and diverse repetition strategies, while maintaining high action persistence.\n* The paper provides a thorough comparison with multiple baseline methods, including SAC, TAAC, and UTE. This context clarifies the advantages of SDAR over both open-loop and closed-loop action repetition approaches."
            },
            "weaknesses": {
                "value": "* The method might be limited when the action dimension is high, which might cause computational overheads.\n* The novelty of this paper appears limited, as TAAC has employed a similar two-stage act-or-repeat mechanism; the primary contribution here seems to be the separation of action dimensions."
            },
            "questions": {
                "value": "* Can TAAC be regarded as an ablation of the proposed method by treating all action dimensions as a whole for repetition? How about its switch policy?\n* Would the proposed method be able to be applied to on-policy methods like PPO, where better sample efficiency would be more valuable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "A version of state-conditioned action-repeat is proposed that also allows\nfor arbitrary subsets of the action dimensions to be repeated, while the rest are\ndirectly controlled by the policy. The paper describes in detail how to learn the\naction-repeat-selection policy to be learned.  To cope with the combinatorial complexity\nissue that arises for high-D action spaces, an importance-based sampling approach is further proposed.\nThe method is evaluated on a reasonably-sized set of continuous action problems, \nranging from low-D to high-D action spaces. The method is shown to be more sample efficient\nthan a reasonably-chosen set of model-free alternatives, including SAC and a variety\nof other action-repeat-learning baselines.  Overall, the proposed method is shown to excel.\nThe learned patterns of action-subset-repeats are also visualized, to help provide\na qualitative understanding of their usage over time by the learned policies."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Strengths:\n- clean idea, good execution, clearly written\n- a \"relatively simple\" idea, entirely in the good sense, and so likely to have impact\n- will inspire follow-up work;  learning structure in action representations remains\n  very much an underexplored area, compared to learning structured or latent state representations"
            },
            "weaknesses": {
                "value": "Weaknesses:\n- I suspect that one could find an example where there is a price to be paid for using this approach,\n  as opposed to it always being more sample efficient\n- it is always hard to see if equal time has been spent tuning all the comparison RL algos;\n  with the right setup and high-replay ratios, some of the RL algos can possibly see further improved sample efficiencies\n- it's clear why we should care about learning rate / sample efficiency, but not so clear why\n  we should actually care about action persistence of action fluctuation."
            },
            "questions": {
                "value": "Questions:\n- how does experience replay sampling work when a- will be different?\n  Or is a- explicitly stored with each tuple in the replay buffer?\n- Could the learned \"synergies\" potentially be transferred when learning a new task?\n- Perhaps I missed it, but are the exploration rates for the repetition schema policy, and the regular policy, being annealed?  Might that make sense?\n- Ideally a method would also work when the \"repetition\" substructure could also be discovered even after the action-space was rotated, i.e., imagine all actions spanned all joints, to differing degrees, but the latent structure would still be there. Just a suggestion for something that would be fun (but arguably difficult) to pursue!\n\nMinor feedback:\n- At the end of the introduction, the second and third listed contributions probably belong together, as they\n  describe the same thing. I like the visualization as another minor contribution.\n- The notation might be easier to understand if the repetition schema is called a mask, m, rather than the current \"b\".\n- Is it possible to collapse the two policies to a single deterministic policy pi, after training? \n  This would be of important practical importance.\n- \"This demonstrates the necessity and effectiveness\":  necessity is perhaps too strong;\n  it is effective in terms of speeding learning, but arguably not \"necessary\"\n- Table 2: \"Avearage\"  (typo)\n- lines 479-480: \"rotors\" --> \"joints\"?\n- line 483:  \"Besides,\"  suggest to remove"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a novel reinforcement learning algorithm, \"Spatially Decoupled Action Repetition\", which decouples policy and repetition. This philosophy is achieved by learning two policies, selection and action policies. The selection policy outputs which action dimension should be masked and the action policy proposes a new action. The proposed algorithm was evaluated on several control domains, such as classical control, locomotion, manipulation."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The paper demonstrates a novel algorithm that decouples repetition and control, which is an interesting idea.\n* The proposed algorithm performs well on a wide range of control domains."
            },
            "weaknesses": {
                "value": "I don't think I am fully convinced by the paper.\n1. Overall, I don't think I understand why the proposed decoupling is important. It is an interesting idea that extends the existing action dimension. But why? In continuous control, I believe many challenging problems mandate new actions at every single time step. Even many discrete action problems may require the same. \n2. In other words, what will be the problem class that must require decoupled action repetition? \n3. As a result, I currently think there can be other reasons that the proposed method outperforms the baselines, such as better hyperparameters or larger network sizes. Please note that I am not questioning the integrity of the proposed authors; there can be other reasons that we may not understand."
            },
            "questions": {
                "value": "* Would you be able to give us the intuition as to why the proposed method \"should\" work better than the baselines? Examples or toy-problems would be appreciated.\n* The performance is only 10% better than SAC - many other ensemble-based approaches, such as RED-Q or Dro-Q, might perform much better than the proposed method."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}