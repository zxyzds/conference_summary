{
    "id": "4011PUI9vm",
    "title": "RankSHAP: Shapley Value Based Feature Attributions for Learning to Rank",
    "abstract": "Numerous works propose post-hoc, model-agnostic explanations for learning to rank, focusing on ordering entities by their relevance to a query through feature attribution methods. However, these attributions often weakly correlate or contradict each other, confusing end users.  We adopt an axiomatic game-theoretic approach, popular in the feature attribution community, to identify a set of fundamental axioms that every ranking-based feature attribution method should satisfy. We then introduce Rank-SHAP, extending classical Shapley values to ranking. We evaluate the RankSHAP framework through extensive experiments on two datasets, multiple ranking methods and evaluation metrics. Additionally, a user study confirms RankSHAP\u2019s alignment with human intuition. We also perform an axiomatic analysis of existing rank attribution algorithms to determine their compliance with our proposed axioms. Ultimately, our aim is to equip practitioners with a set of axiomatically backed feature attribution methods for studying IR ranking models, that ensure generality as well as consistency.",
    "keywords": [
        "Feature attributions",
        "Shapley values",
        "Information Retrieval",
        "Passage Reranking"
    ],
    "primary_area": "interpretability and explainable AI",
    "TLDR": "Shapley value based feature attributions for Ranking tasks",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4011PUI9vm",
    "pdf_link": "https://openreview.net/pdf?id=4011PUI9vm",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes RankSHAP as a framework for explaining how features contribute to a ranking model's output. The authors extend the classifcal Shapley value concept to the ranking domain by specifying two axioms that a ranking-based feature attribution must satisfy, in additon to the set of four fundamental axioms that Shapely values already satisfy. The authors argue that current methods for explaining ranking models often provide inconsistent or contradictory explanations, making it difficult for users to understand model behavior. These axioms, which are based on game theory and information retrieval principles, ensure the fairness, consistency, and reliability of the explanations. Through extensive experiments, the authors demonstrate that RankSHAP outperforms existing methods in terms of accuracy and alignment with human intuition."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Axiomatic Foundation: I appreciate that the authors propose a set of fundamental axioms specifically tailored for ranking feature attributions, drawing inspiration from Shapley values in coalitional game theory. These axioms, namely Rank-Efficiency, Rank-Missingness, Rank-Symmetry, and Rank-Monotonicity, ensure that the attributions are fair, consistent, and meaningful.\n- Generalized Ranking Effectiveness Metric (GREM): The authors introduce GREM, a generalized framework for evaluating the effectiveness of ordered lists. This framework encompasses widely used metrics like NDCG and provides a solid theoretical foundation for assessing the quality of rankings produced by considering feature subsets.\n- Computational Feasibility: Acknowledging the NP-completeness of exact Shapley value calculations, the authors propose an approximate algorithm to leverage a linear model between feature subsets and the ranking effectiveness metric, as well as Kernel-RankSHAP to induce non-linearity into this model. This makes RankSHAP practical for real-world applications.\n- Extensive Empirical Evaluation: The authors conduct comprehensive experiments on two datasets (MS MARCO and Robust04) using multiple ranking models, including BM25, BERT, T5, and LLAMA2. The results demonstrate RankSHAP's superior performance over competing methods like EXS, RankLIME, and RankingSHAP across various metrics like Fidelity and weighted Fidelity.\n- User Study Validation: The paper includes a user study to assess the alignment of RankSHAP with human intuition. This is an interesting study that tasks participants with re-ordering documents and inferring queries based on feature attributions. The results show that RankSHAP significantly improves user understanding of ranking decisions."
            },
            "weaknesses": {
                "value": "- User study caveats: (a) Preconceived Notions: The authors observed that randomly generated feature attributions achieved a higher concordance score in the re-ordering task than expected based on their metric evaluation. This suggests that participants might have relied on pre-existing assumptions or biases about the topics, potentially influencing their judgments throughout the experiment. Is that a drawback of the setup that also impacts rest of the observations?\n(b) Subjectivity: The authors noted significant variance in the queries estimated by participants for the same document set and feature attributions. This probably highlights the inherent subjectivity in interpreting feature attributions and formulating queries, which can lead to diverse responses and impact the evaluation's reliability. \nDespite limitations, it is indeed on interesting study to include in the paper since progress in the field of interpretability requires human subjects to be involved. \n- Dependence on Relevance Scores: The effectiveness of RankSHAP relies on the availability of accurate relevance scores for each query-document pair. Obtaining these scores often necessitates ground truth labels, which can be scarce or unavailable. The paper proposes using implicit measures like click-through rates to infer relevance when explicit labels are absent.\n-"
            },
            "questions": {
                "value": "- Since the model can be black box, what if the model is itself self-contradictory or inconsistent? For example, a listwise ranking models that ranks documents A and B as A>B>C in the presence of C but as D>B>A in the presence of a document D. Would the explanations provided by RankSHAP in such a scenario inconsistent or unfaithful? Similarly, what happens when the model handles uncertainly or chooses to use stochastic rankings (drawing a different sample from a distribution over permutations), e.g., Singh, Kempe, Joachims. Fairness in Ranking under Uncertainty (2021).\n- Can you explain why the fidelity scores are different for the two datasets? You mention that it is due to the difference in dataset sizes but I am not sure if it is clear if that is the reason.  \n- See weaknesses section too."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "No obvious concerns but the paper presents a user study which was IRB approval. I believe that is sufficient. However, I have not reviewed the approval."
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces a Shapley-value-based feature attribution method tailored specifically for ranking tasks. Traditional feature attribution methods, mostly developed for regression or classification, often produce conflicting results when adapted to ranking, which can lead to confusion among end-users. RankSHAP addresses this by adhering to a set of axioms tailored for ranking. Extensive experiments demonstrate that RankSHAP aligns well with human intuition and outperforms existing methods in fidelity and weighted fidelity. Additionally, a user study confirms its practical value in helping users understand model decisions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The introduction of axioms specific to ranking provides a robust framework, distinguishing RankSHAP from other feature attribution methods.\n\nThe authors incorporate a user study to validate that RankSHAP explanations align with human understanding, which adds practical value."
            },
            "weaknesses": {
                "value": "RankSHAP\u2019s reliance on relevance scores for accurate NDCG calculations could be a limitation in scenarios where relevance is difficult to quantify or subjective.\n\nAlthough RankSHAP was tested in a user study, the evaluation might have limited generalizability due to sample size."
            },
            "questions": {
                "value": "How does RankSHAP perform when features are highly interdependent? Are there adjustments made for such cases?\n\nHow does the RankSHAP framework handle scenarios where relevance scores are subjective or unavailable?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents RankSHAP, an extension of SHAP value tailored to interpret learning-to-rank models. The authors reinterpreted the Shapley value axioms (Rank-Efficiency, Rank-Missingness, Rank-Symmetry, and Rank-Monotonicity), and defined new properties (Relevance Sensitivity and Position Sensitivity) to better capture the unique requirements of ranking models in feature attribution. To demonstrate practicality, they integrated the NDCG metric into KernelSHAP and validated their method through experiments."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper introduces a thoughtful adaptation of the Shapley value for the ranking domain, defining new ranking-specific properties that enhance SHAP's applicability in ranking contexts.\n- The authors have conducted both performance evaluations and a user study to validate their method."
            },
            "weaknesses": {
                "value": "The proposed method, RankSHAP, heavily relies on the KernelSHAP method [1] with modifications to incorporate NDCG for ranking applications. Rather than introducing a fundamentally new method, the paper adapts an existing approach specifically for ranking tasks. While the axiomatic reformulation is valuable, the technical novelty beyond extending KernelSHAP with NDCG remains limited.\n\n[1] Scott M Lundberg and Su-In Lee, A Unified Approach to Interpreting Model Predictions, in NeurIPS, 2017."
            },
            "questions": {
                "value": "Are there experiments with GREM metrics other than NDCG? While the choice of NDCG is understandable (appendix C), experiments with other metrics are needed to demonstrate the validity of the proposed method across different metrics, which would strengthen its generality."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The RankSHAP method leverages Shapley values, considering all possible feature combinations and their interactions (or approximations thereof), to provide a detailed, interaction-aware explanation of feature contributions in ranking models. This approach aligns well with the feature importance analysis used in regression models, making the methodology intuitive."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper provides a good motivation for RankSHAP by discussing the limitations of simpler ranking explanation methods, like LIME, which lack consideration for full feature interaction and listwise ranking structure. By addressing these limitations, RankSHAP presents a more nuanced and comprehensive approach to ranking explainability.\n\n2. RankSHAP's performance is evaluated using fidelity metrics, which assess how accurately RankSHAP captures feature contributions in line with the model\u2019s scoring function. This fidelity-based evaluation aligns RankSHAP\u2019s explanations with the ranking model\u2019s actual logic, resulting in explanations that are reflective of the ranker's structure rather than arbitrary feature importance.\n\n3. The method is flexible since RankSHAP can work with a range of ranking models, from traditional learning-to-rank approaches to neural ranking models. This makes RankSHAP\u2019s applicable to many fields, including search, recommendation, and information retrieval."
            },
            "weaknesses": {
                "value": "1. **Correlation, Not Causation**: While RankSHAP provides insight into feature importance, the explanations are inherently correlational, not causal. Therefore, while RankSHAP can reveal which features push an item up in ranking, this does not imply that these features are directly causing higher ranks. I'm not saying the authors claim causality, but in terms of doing \"better\" at explaining ranking, they're explaining correlations between the rank and the features better, and not want causally drives a higher ranking. \n\n2. **Model Dependency and Bias**: The effectiveness of RankSHAP is fundamentally dependent on the quality and calibration of the ranker itself (correct?). If the scoring function is miscalibrated or biased (e.g., favoring higher-ranked items based solely on position), RankSHAP\u2019s explanations may reinforce these biases instead of offering corrective insights. The authors could discuss how potential biases (position, selection, algorithmic biases) within the ranker affect the usefulness of RankSHAP\u2019s explanations to be truly meaningful.\n\n3. **Limitations in Simple Averages for Shapley Values**: The reliance on simple averages in traditional Shapley values means that RankSHAP does not consider the number of interactions (N) or the variance in a feature\u2019s impact across different contexts. This averaging could dilute the significance of features with occasional strong influence, potentially overlooking context-dependent importance and limiting RankSHAP\u2019s insights.\n\n4. **Reinforcement of Potentially Polarized Content**: Because RankSHAP's explanations reflect the ranker\u2019s scoring function, any inherent polarization or bias in the model could be echoed in the explanations. This could inadvertently support polarized content if the ranking model favors certain types of data, raising concerns for applications in content moderation and recommendation.\n\n5. **Redundancy in Presentation and Scope of Rankers**: The paper does not clearly specify the types of ranking models used in its evaluation, which, I think could and maybe should matter? Or at least will help elucidate where it's particularly useful (I imagine the more non linear the ranker the more useful?)"
            },
            "questions": {
                "value": "I think RankSHAP is a nice contribution to ranking explainability, but improving the handling of bias, refining its reliance on averages, and clarifying its application scope could elevate its impact further."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}