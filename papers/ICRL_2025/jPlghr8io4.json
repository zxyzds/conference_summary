{
    "id": "jPlghr8io4",
    "title": "Learning with Analogical Reasoning for Robust Few-Shot Learning",
    "abstract": "Few-shot learning (FSL) is challenging due to limited support data for model training. The situation is much worse when the support data is contaminated with noise. To address this issue, in this work we propose a novel $\\textbf{T}$ransformer-based $\\textbf{A}$nalogical $\\textbf{R}$easoning model for $\\textbf{N}$oisy $\\textbf{F}$ew-$\\textbf{S}$hot learning (TarNFS), by mimicing the human's ability of learning by analogy. Concretely, we assume the existence of a large human cultivated knowledge base, and hypothesize that similar concepts in the knowledge base are visually similar in the latent space as well. Then we design a transformer-based analogical reasoning model to utilize inter-concept connections among these concepts, aiming to build robust and discriminative classification boundaries. In addition, we propose a task-level contrastive learning to analogically learn from negative tasks to facilitate model optimization with noisy tasks during training. Experiments demonstrate that our TarNFS enables more effective learning from limited and imperfect data. It not only improves the generalization ability of FSL in different noisy settings but also achieves competitive performance in the common clean FSL settings.",
    "keywords": [
        "few-shot learning",
        "analogical reasoning",
        "noisy label learning",
        "contrastive learning",
        "transformer"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "",
    "creation_date": "2024-09-20",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=jPlghr8io4",
    "pdf_link": "https://openreview.net/pdf?id=jPlghr8io4",
    "comments": [
        {
            "summary": {
                "value": "This paper addresses the challenge of Few-shot Learning with contaminated support data. To tackle this issue, the authors introduce a learning paradigm inspired by analogical reasoning. Specifically, the paper proposes a new approach to model inter-concept connections, which is then leveraged to improve inference in the presence of noisy support data. Additionally, the paper introduces a task-level contrastive learning strategy to mitigate the impact of noisy labels. To validate the effectiveness of the proposed method, the authors conduct experiments on several widely-used datasets, including MiniImageNet and TieredImageNet. The results demonstrate a significant enhancement in performance compared to existing methods."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The concept of employing semantic and analogical reasoning to improve the performance of few-shot learning with noisy labels appears promising. The writing and presentation of the methods in this paper are clear and well-articulated.\nThis paper conducts a thorough analysis of the method using two datasets to demonstrate the contributions of each component and to compare performance across different benchmarks.\nThe results obtained from the two datasets indicate a significant enhancement in performance of the proposed method compared to existing approaches."
            },
            "weaknesses": {
                "value": "While the author presents a detailed analysis using two datasets, I am concerned about their similarity. It would be beneficial if additional results could be demonstrated using other datasets, such as CUB and CIFAR-FS, to further validate the robustness of the method.\nThe methodology would be more convincing if it were also validated on a larger dataset, such as the Meta-Dataset. This would help in assessing its scalability and effectiveness across a broader range of learning scenarios.\nThe training procedure is multi-staged and could pose challenges in terms of reproducibility. Unfortunately, the author does not provide the code, which would be extremely helpful for those attempting to replicate the study's results."
            },
            "questions": {
                "value": "From Table 4, I noted that the proposed method outperforms the oracle under a 20% noise setting. Could the author provide examples of several pairs of support and query instances where the oracle fails to classify correctly, but the proposed TarNFS method succeeds? This would offer deeper insights into the strengths of the method.\nI observe that the TCL module exhibits a larger margin of improvement compared to AR. Could the author present an analysis that isolates the impact of using the TCL module alone? This comparison would help clarify the individual contribution of the TCL module to the overall performance."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes TarNFS, a Transformer-based analogical reasoning model to improve Few-Shot Learning (FSL) in noisy environments. It enhances class prototypes using a knowledge base with multi-head self-attention and introduces task-level contrastive learning to improve generalization under noisy conditions."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1.\tThe proposed analogical reasoning mechanism effectively incorporates contextual information from a knowledge base to overcome the limitations of noisy support data.\n\n2.\tThe combination of multi-head self-attention and a large-scale knowledge base results in efficient enhancement of class prototypes.\n\n3.\tThe inclusion of task-level contrastive learning provides additional regularization, significantly improving model generalization."
            },
            "weaknesses": {
                "value": "1.\tThe details of knowledge base construction and selection for analogical reasoning are insufficient, particularly regarding applicability in different tasks.\n\n2.\tWhile task-level contrastive learning enhances generalization, the computational complexity is high, with no discussion on training time and efficiency.\n\n3.\tScalability with larger or different types of knowledge bases is not discussed, which may limit the generalizability of the approach."
            },
            "questions": {
                "value": "1.\tDuring the construction of the query set using mixup, does the correlation between the sampled image and the combined augmented image affect the model's performance and stability? For instance, do cases where they belong to the same class or are completely unrelated impact the outcome?\n\n2.\tUnder time constraints, does the computational cost of this construction method lead to a significant performance improvement in the model?\n\n3.\tHow sensitive is the model's performance to the parameters used in the query set construction method?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a Transformer-based Analogical Reasoning model, TarNFS, designed for robust few-shot learning (FSL) in noisy environments. It leverages analogical reasoning to improve classification by building connections among categories within a semantic knowledge base, like WordNet, and introduces task-level contrastive learning to separate distinct tasks further. The proposed approach is evaluated on MiniImageNet and TieredImageNet, showing improvements over previous FSL methods, particularly under noisy conditions, and demonstrating competitive performance on standard clean data."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is well-organized, with a logical progression from problem formulation to methodology and experimental evaluation, making it accessible and easy to follow.\n\n- The experiments cover a variety of settings, including different noise levels, ablation studies, and comparisons with several FSL baselines, providing a solid foundation for evaluating the model's effectiveness.\n\n- TarNFS demonstrates advantages in handling noisy data, a challenging aspect of FSL, particularly through its strategy of refining noisy prototypes using analogical knowledge."
            },
            "weaknesses": {
                "value": "- While MiniImageNet and TieredImageNet are commonly used benchmarks, the model\u2019s real-world applicability could be tested on additional datasets, e.g. Meta-dataset.\n- The model relies on an existing knowledge base (WordNet), which may limit its applicability in domains lacking such resources or where these resources are incomplete.\n- The Transformer-based architecture and contrastive learning process, especially with Bi-LSTM in the task-level contrastive learning, may require considerable computational resources, which are not addressed in terms of efficiency or scalability.\n- The model is tailored to the structure of WordNet for analogy-based learning. Adapting this method for domains with different types of semantic structures could present challenges, and further explanation of this adaptation process is warranted."
            },
            "questions": {
                "value": "Can the proposed method outperforms the latest Noisy FSL method DETA [1]?\n\n[1] DETA: denoised task-adaptation for few-shot learning, ICCV 2023."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}