{
    "id": "dMj3SDNxn4",
    "title": "UNICORNN: Unimodal Calibrated Ordinal Regression Neural Network",
    "abstract": "Ordinal regression is a supervised machine learning technique aimed at predicting the value of a discrete dependent variable with an ordered set of possible outcomes. Many of the algorithms that have been developed to address this issue rely on maximum likelihood for training. However, the standard maximum likelihood approach often fails to adequately capture the inherent order of classes, even though it tends to produce well-calibrated probabilities. Alternatively, some methods use Optimal Transport (OT) divergence as their training objective. Unlike maximum likelihood, OT accounts for the ordering of classes; however, in this manuscript, we show that it doesn't always yield well-calibrated probabilities. To overcome these limitations, we introduce UNICORNN, an approach inspired by the well-known Proportional Odds Model, which offers three key guarantees: (i) it ensures unimodal output probabilities, a valuable feature for many real-world applications;\n(ii) it employs OT loss during training to accurately capture the natural order of classes;\n(iii) it provides well-calibrated probability estimates through a post-training accuracy-preserving calibration step.\nExperimental results on six real-world datasets \ndemonstrate that UNICORNN consistently either outperforms or performs as well as recently proposed deep learning approaches for ordinal regression. It excels in both accuracy and probability calibration, while also guaranteeing output unimodality. The code will be publicly available upon acceptance.",
    "keywords": [
        "unimodality",
        "ordinal regression",
        "probability calibration",
        "deep learning"
    ],
    "primary_area": "other topics in machine learning (i.e., none of the above)",
    "TLDR": "A deep ordinal regression model for unimodal and calibrated output probabilities.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=dMj3SDNxn4",
    "pdf_link": "https://openreview.net/pdf?id=dMj3SDNxn4",
    "comments": [
        {
            "summary": {
                "value": "In this paper, the authors propose an ordinal regression approach that enforces unimodularity in the output distribution. They use a truncated normal distribution to model the output probabilities.  They provide unimodular proof of the output distribution. They show experimental results to highlight the effectiveness of the proposed approach compared with baseline approaches on various metrics."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Authors propose a theoretically sound approach for ordinal regression, which can output unimodular outputs.\n2. Paper is very well written and easy to read."
            },
            "weaknesses": {
                "value": "1. Some references are missing. E.g.\n(a) Rank consistent ordinal regression for neural networks with application to age estimation. Wenzhi Caoa, Vahid Mirjalilib, Sebastian Raschkaa. Pattern Recognition Letters, (2020), 325-331.\n(b) Garg, B. &amp; Manwani, N.. (2020). Robust Deep Ordinal Regression under Label Noise. Proceedings of The 12th Asian Conference on Machine Learning. 129:782-796\n\n2. The baseline approaches used do not cover a variety of algorithms for ordinal regression. Please see the Questions section for suggested baselines.\n\n3. Code is not provided."
            },
            "questions": {
                "value": "1. Please comment on the rank consistency of the proposed approach. In think, unimodularity of the output distribution and rank consistency of the two contradictory properties. Please comment. It would be good to see the average number of rank violations made by the proposed approach.\n2. Please compare with a rank-consistent baseline (e.g. Shi et al., 2023)\n3. A simple approach for imposing unimodularity in the output vector is by posing the constraint $P(Y=1)\\leq P(Y=2)\\leq \\ldots \\leq P(Y=c)\\leq \\ldots P(Y=K)$, where $c$ be the target class and $K$ be the total number of classes. Minimizing loss with these constraints can be an alternate approach to achieving unimodularity. It can be a good baseline for the proposed approach. Can you show some results why such an is not good?\n4. Please explain the truncated normal distribution properly. What is the support set for this distribution?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "**Summary**:\n\nThe paper presents a technique for ordinal regression tasks, focusing on two aspects: (1) constructing uni-modal distribution, which is desirable in many cases; (2) generating calibrated predictive distribution that is comparable to the empirical distribution. For the uni-modal distribution construction, it generates a truncated normal distribution, and constructs evenly spaced thresholds to ensure the unimodality. For the calibration, the model performs retraining using Brier loss to adjust the parameter from the first stage. The authors then demonstrate the performance of the proposed model in the real experiments."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**Strengths**:\n\n1. The paper clearly describes the motivations and explanations of the proposed model.\n2. A rather simple and interesting approach for ensuring unimodality of the predictive distribution.\n3. Retraining procedure for producing calibrated distributions.\n4. The experiments show the benefit of the proposed approach on several datasets."
            },
            "weaknesses": {
                "value": "**Weaknesses**:\n\n1. The description of the proposed training algorithm in Sec. 4.4 is rather simple. I would suggest the authors to put a proper algorithm (using formal notation) for the model\n2. The way the authors ensure unimodal distribution can be limiting. Even though the proposed method always generates a unimodal distribution, not every unimodal distribution can be represented by the proposed construction. In fact, with just two degrees of freedom, it is rather limited.\n3. The novelty of the proposed models is rather limited, as similar unimodality construction and the calibrated distributions constructions have been proposed before."
            },
            "questions": {
                "value": "**Questions**:\n\nPlease address the weaknesses mentioned above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces UNICORNN, a deep-learning approach designed for ordinal regression tasks where class labels follow a natural order. The authors address three primary challenges in ordinal regression: (1) enforcing unimodal output probability distributions, (2) capturing the ordered relationships among classes effectively, and (3) providing well-calibrated probability estimates. Extensive experiments on six real-world datasets demonstrate that UNICORNN consistently matches or outperforms recent ordinal regression methods, excelling in accuracy, probability calibration, and ensuring output unimodality."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. UNICORNN addresses significant limitations in existing ordinal regression methods by providing an approach that ensures both unimodal and calibrated probabilities, which is critical for applications \n2. The paper is well-structured and Explanations are generally clear.\n3. The combination of Optimal Transport (OT) loss with accuracy-preserving calibration is original and significant for ensuring both order respect and reliable probability estimates"
            },
            "weaknesses": {
                "value": "1. The proposed method has a two-stage training process, significantly increasing the model's computational complexity. Can the author show the time complexity of training or compare the training time of the proposed method with other methods?\n2. The methods used in experiments are too old. The authors should include comparisons with methods from recent years such as [1,2]. Can the author show the results on some large-scale datasets, such as the datasets with more training samples or more classes? \n3. More experiments are needed in ABLATION STUDY. It only has two datasets. More experiments on different datasets are required.  \n\n\n[1] Shi X, Cao W, Raschka S. Deep neural networks for rank-consistent ordinal regression based on conditional probabilities[J]. Pattern Analysis and Applications, 2023, 26(3): 941-955.\n[2] Li Q, Wang J, Yao Z, et al. Unimodal-concentrated loss: Fully adaptive label distribution learning for ordinal regression[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 20513-20522.\n[3] Kim D, Chung H, Jang I. Calibration of ordinal regression networks[J]. arXiv preprint arXiv:2410.15658, 2024."
            },
            "questions": {
                "value": "See weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "None"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}