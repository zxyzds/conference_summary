{
    "id": "9Hxdixed7p",
    "title": "3D-Properties: Identifying Challenges in DPO and Charting a Path Forward",
    "abstract": "Aligning large language models (LLMs) with human preferences has recently garnered significant attention, with Proximal Policy Optimization (PPO) being a canonical yet computationally expensive method, and Direct Preference Optimization (DPO) offering a simpler and more efficient alternative. While prior studies have explored the trade-offs between PPO and DPO, DPO remains underutilized in state-of-the-art production-level LLMs, suggesting potential limitations. In this work, we revisit DPO with a comprehensive analysis of its theoretical foundations and empirical performance, aiming to chart a path forward and bridge this gap. We identify three critical properties\u2014termed the \\textbf{3D}-properties\u2014that arise from DPO\u2019s learning process: \\textbf{D}rastic drop in the likelihood of rejected responses, \\textbf{D}egradation into response suppression, and \\textbf{D}ispersion effect on unseen responses. We show that these phenomena stem from the inherent features of DPO's optimization objective, where the interaction between the gradients of chosen and rejected responses causes instability. These findings are supported by experiments on both a carefully constructed toy model and practical LLM tasks, including mathematical problem-solving and instruction following. Our work offers new insights, connecting these observations to related research while providing a theoretical explanation for the underlying mechanisms. To address the challenges posed by the \\textbf{3D}-properties, we propose straightforward regularization techniques that enhance training stability and final performance. Additionally, we investigate how the distribution of paired preference data affects DPO\u2019s efficacy, contributing to a broader understanding of how alignment models handle out-of-domain (OOD) data. We believe our findings will help guide future research toward closing the gap between reward-model-free preference learning and reward-model-based approaches.",
    "keywords": [
        "LLM",
        "DPO",
        "RLHF"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "understanding the degradation of reward-free alignment in LLMs",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-15",
    "forum_link": "https://openreview.net/forum?id=9Hxdixed7p",
    "pdf_link": "https://openreview.net/pdf?id=9Hxdixed7p",
    "comments": [
        {
            "comment": {
                "value": "**Q5: The detailed parameter adjustment strategy is only given in the toy experiment. What is the effect of different \u03b2 values \u200b\u200bin the real-world experiments?**\n\n**R5:** Thanks for the question. In Secion 4.3 we have analyze the effect of different $\\beta$ in the real-world experiments. From Figure 4, it can be seen that a smaller $\\beta^-$ is beneficial for improving the model's performance but the  trend is not monotonous. We recognize that tuning $\\beta^+$ and $\\beta^-$ is non-trivial, as the ultimate performance on real-world tasks depends on multiple factors, including the training and test dataset types and the choice of other hyperparameters. The thorough exploration is beyond the scope of this paper as an theory-driven work. Some recent works focus on exploring the optimal adjusting strategy such as [1], which we refer the reviewer to read.\n\n[1] Wu J, Xie Y, Yang Z, et al. $\\beta $-DPO: Direct Preference Optimization with Dynamic $\\beta$[J]. arXiv preprint arXiv:2407.08639, 2024."
            }
        },
        {},
        {
            "comment": {
                "value": "**Q1: The three observations have been widely studied by previous works and the proposed regularization method (incorporating an SFT loss) has been widely used in exsisting approaches.**\n\n**R1:** Thank you for pointing this out. We would like to emphasize that, although some researchers\u2014including ourselves\u2014have observed these three phenomena (as referenced in Section 2.1 of our paper), prior works have not conducted a more in-depth analysis of these observations. We have also provided a thorough comparison with related works in Section 2.2.\n\nThe primary aim of our paper is to examine the underlying reasons behind the emergence of these three observations and to summarize them as \"3D-properties,\" particularly from a theoretical perspective. The presentation of algorithms, such as iterative DPO, adding SFT loss, and Flex-DPO, is not our main focus. Furthermore, we propose that incorporating SFT loss or using other regularization strategy is effective because it ensures that the gradient for the chosen action is not zero when $\\pi^- \\rightarrow 0$, which partially addresses the 3D-properties\u2014a perspective that differs from prior work. Additionally, our analysis of the importance of on-policy data strengthens the theoretical foundation of the recent trend including iterative preference learning.\n\n**Q2: Some advanced preference learning algorithms, such as SimPO and DPOP, should also be tested.**\n\n**R2:** Thanks for the suggestion.  We discussed several variants of DPO, such as IPO and SLiC, in Section B.3 of the appendix. Regarding DPOP, while we acknowledge its potential, its effectiveness has not yet been widely validated compared to the other listed algorithms. Therefore, here we mainly discuss SimPO, which is a more recent and actively discussed algorithm. SimPO's optimization primarily involves length normalization and the introduction of a margin bias factor $\\gamma$, which was initially considered less relevant to the topic under discussion. ___Based on the reviewer's comments, we have added a theoretical analysis of SimPO in Appendix B.3.3 of the revised manuscript (highlighted in blue).___ The conclusion remains that the 3D-properties still hold.\n\nAs for experiments involving SimPO, to the best of our knowledge, its hyperparameters significantly affect the results and are highly sensitive compared to other variants. Conducting a thorough exploration would require considerable time to ensure solid conclusions. Therefore, we will consider including these experiments as part of our future work.  Besides, we have added these mentioned but missing works into our citation, including DPOP and SimPO.\n\n**Q3: More and more general LLMs should be included for evaluation, such as Meta-Llama3.**\n\n**R3:** Thank you for your suggestion. The limitation of vanilla DPO has been observed in many different series of models such as Pythia 2.8b [1] and some following works about the advantages of online DPO [2], which also focuses on other series of LLMs. These studies also consider different series of LLMs. In our work, we chose Baichuan as it is an in-house LLM series, allowing us full control over the model size and the data used for training. This control was crucial for managing variables in our comparison experiments. Nevertheless, we appreciate your suggestion and will incorporate additional experimental results with other open-source models, such as the LLaMa series, in future versions of our study.\n\n[1] https://wandb.ai/eric_anthony_mitchell/dpo-demos/runs/og8q3euz\n\n[2] Calandriello D, Guo D, Munos R, et al. Human alignment of large language models through online preference optimisation[J]. arXiv preprint arXiv:2403.08635, 2024.\n\n**Q4: How to ensure that the initialization assumptions of parameter distribution can be applied to, or related to LLMs?**\n\n**R4:** Thank you for your question. We have tested the effects of data and model distribution through real-world experiments, with the results presented in Table 1. We define that the distribution of the data aligns with that of the LLMs if the data is sampled directly from the LLMs. In Section 4.2, we detail our approach to constructing both on-policy and off-policy data, which helps us determine whether the data shares the same distribution as the LLMs. This approach ensures a consistent basis for distinguishing on-policy and off-policy distributions and validates the initialization assumptions under these different conditions."
            }
        },
        {
            "comment": {
                "value": "**Q1: Concern about the limited range of LLMs used in the study.**\n\n**R1:** \u00a0Thank you for your suggestion. The limitation of vanilla DPO has been observed in many different series of models such as Pythia 2.8b [1] and some following works about the advantages of online DPO [2], which also focuses on other series of LLMs. These studies also consider different series of LLMs. In our work, we chose Baichuan as it is an in-house LLM series, allowing us full control over the model size and the data used for training. This control was crucial for managing variables in our comparison experiments. Nevertheless, we appreciate your suggestion and will incorporate additional experimental results with other open-source models, such as the Llama series, in future versions of our study.\n\n[1] https://wandb.ai/eric_anthony_mitchell/dpo-demos/runs/og8q3euz\n\n[2] Calandriello D, Guo D, Munos R, et al. Human alignment of large language models through online preference optimisation[J]. arXiv preprint arXiv:2403.08635, 2024.\n\n**Q2: Concern about the paper's primary focus on math datasets rather than a broader range of tasks.**\n\n**R2:** Thank you for your valuable suggestion. In addition to the math datasets, we also utilized other datasets involving formatted text generation, such as poems and slogans, as detailed in Section 4 and the related appendices. Furthermore, we included the HH-RLHF dataset for comparing the RM and DPO. Our selection criteria for the datasets in this paper were based on the availability of standard and correct answers, which helps to minimize the impact of evaluation noise. We believe that this approach ensures a more reliable assessment of the model's performance.\n\n**Q3: The code is not open source, which may limit reproducibility.**\n\n**R3:** Actually we have provided the code in the supplementary material, which includes the toy model experiments, the main experiments, and most of the datasets used. We are currently organizing the GitHub repository for a public release. Although the motivation of this paper is primarily theoretical, we recognize the importance of reproducibility and are committed to open-sourcing the code along with all non-sensitive in-house datasets once the review process is complete, in accordance with anonymity requirements.\n\n**Q4: For the toy model setup, which specific model is used in the paper?**\n\n**R4:** As mentioned in line 289-291, Section 3.2.1, the toy model is implemented as a three-layer MLP that processes a one-hot vector and outputs a categorical distribution over the responses."
            }
        },
        {
            "summary": {
                "value": "The paper titled \"3D-Properties: Identifying Challenges in DPO and Charting a Path Forward\" presents a thorough analysis of the DPO method used for aligning LLMs with human preferences. The authors identify and term three critical properties of DPO's learning process the 3D-properties and propose regularization techniques to address the challenges these properties present. Theoretical analyses, toy model simulations and real-world experiments demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is well-structured, where toy example can support their claims.\nThe paper offers a balanced mix of theoretical analysis and empirical evidence, which strengthens the claims made about the 3D-properties and their impact on DPO's performance."
            },
            "weaknesses": {
                "value": "The three observations have been widely studied by previous works. Besides, one of the proposed regularization methods, incorporating an SFT loss into the objective, has been widely used in existing preference learning approaches [1]. This limits the novelty of the paper.\nConsidering that there are many existing methods to solve the DPO problem proposed in this paper, there is a lack of comparison with them, such as [2] and others.\nConsidering the generality of the proposed constraint algorithm, some advanced preference learning algorithms, such as SimPO [3], should also be tested.\nMore and more general LLMs should be included for evaluation, such as Meta-Llama3.\n\nReference: \n[1] Pang R Y, Yuan W, Cho K, et al. Iterative reasoning preference optimization[J]. arXiv preprint arXiv:2404.19733, 2024.\n[2] Pal A, Karkhanis D, Dooley S, et al. Smaug: Fixing failure modes of preference optimisation with dpo-positive[J]. arXiv preprint arXiv:2402.13228, 2024.\n[3] Meng Y, Xia M, Chen D. Simpo: Simple preference optimization with a reference-free reward. NeurIPS, 2024."
            },
            "questions": {
                "value": "How to ensure that the initialization assumptions of parameter distribution can be applied to, or related to LLMs?\n\nThe detailed parameter adjustment strategy is only given in the toy experiment. What is the effect of different \u03b2 values \u200b\u200bin the real-world experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper provides a comprehensive analysis of Direct Preference Optimization (DPO), examining its theoretical foundations and empirical performance to address current limitations. It identifies three perspectives\u2014(1) Drastic drop in the likelihood of rejected responses, (2) Degradation into response suppression, and (3) Dispersion effect on unseen responses. The paper connects these observations to related research and offers a theoretical explanation for the underlying mechanisms. To improve DPO\u2019s stability and performance, the authors propose regularization methods, including adaptive adjustment of gradient weights for chosen and rejected responses, as well as incorporating an SFT loss into the objective."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The topic is interesting for RLHF. \n\nThe paper introduces effective regularization methods, including adaptive gradient weighting for chosen and rejected responses.\n\nThe experiments are well-conducted and thorough."
            },
            "weaknesses": {
                "value": "The study could benefit from using a wider range of LLMs.\n\nThe experiments can use more datasets except for math. \n\nThe code is not open source, which may limit reproducibility."
            },
            "questions": {
                "value": "For the toy model setup, which specific model is used in the paper?\n\nWhy does the paper focus primarily on math datasets rather than exploring a wider range of tasks?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 1
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents an interesting theoretical and empirical analysis of Direct Preference Optimization (DPO) and identifies three main challenges in its optimization process, termed as \u201c3D-properties\u201d: Drastic drop in rejected response likelihood, Degradation into response suppression, and Dispersion effect on unseen responses. These limitations, which do not arise in RM-based approaches, impact the stability and effectiveness of DPO. To address these issues, the authors propose regularization techniques, including adaptive gradient weighting and SFT loss. They conduct experiments on toy examples as well as math reasoning and instruction-following tasks to validate the presence of the 3D-properties, the advantages of on-policy over off-policy DPO, the comparative superiority of RM-based methods, and the effectiveness of the proposed regularization technique."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Significance: The paper addresses a crucial and interesting gap by analyzing the limitations of DPO\n- Theoretical Analysis and Empirical Validation: The paper provides a theoretical framework alongside empirical results to validate the presence of the 3D-properties in DPO. This combined approach strengthens the findings, offering clear insights into the mechanisms driving DPO\u2019s limitations and supporting the proposed solutions."
            },
            "weaknesses": {
                "value": "- Presentation: The presentation could be improved to enhance readability. For example, the text size in Figures 2 and 3 is small, and the description of Scenarios 1-4, which is crucial for understanding the on-policy versus off-policy comparison, is currently only detailed in the appendix. Bringing this description to the main text would improve clarity.\n- Experimental Design for On-Policy vs. Off-Policy Comparison: The on-policy and off-policy experiments rely on different data sources, which introduces potential confounds in the comparison. Using a more direct on-policy and off-policy setup, such as comparing historical-only data with semi-on-policy DPO (e.g., iterative DPO), would make the findings more robust.\n- Parameter Tuning in Flex-DPO: Adjusting Flex-DPO requires tuning two parameters ( \\beta^+  and  \\beta^- ), and while Figure 4 provides some guidance, this approach may still present challenges for practical implementation due to a lack of clear tuning guidelines.\n\nIf the authors address these weaknesses, particularly by improving the clarity of presentation and by using a more controlled comparison between on-policy and off-policy data sources, I would raise my score. Addressing the Flex-DPO would also strengthen the work, though it is not essential for improving the overall contribution."
            },
            "questions": {
                "value": "- In Section 4.2, it is mentioned that for the MATH dataset, the best and worst responses were selected by GPT-4. Why did the authors choose this method instead of directly verifying the answers? Given that GPT-4\u2019s accuracy on MATH is only slightly above 50%, this approach seems potentially unreliable."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the limitations of DPO in aligning large language models with human preferences, identifying three critical properties that hinder its performance: drastic drops in rejected response likelihood, degradation into response suppression, and dispersion effects on unseen responses. The authors provide theoretical explanations for these properties and demonstrate how they arise from DPO's objective. To address these challenges, the paper proposes regularization techniques and validates their effectiveness through experiments on both toy models and real-world language model tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Studying DPO degradation phenomena is important due to its widespread use. This paper originally summarizes and theoretically analyzes several degradation phenomena of DPO discovered in previous work.\n\n- Novel comparative analysis between on-policy and off-policy DPO on toy models.\n\n- The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- I think the Explanation for Property 3 is inadequate. Firstly, compared to the previous two explanations, it lacks mathematical formulation and seems to merely restate empirical phenomena. Secondly, since the optimization process is conducted in mini-batches, while the model may ensure that overflow probability won't disperse to recently seen samples, I suspect it could also disperse to samples from the preference dataset that were encountered earlier, rather than necessarily dispersing to unseen samples outside the preference dataset.\n\n- Following the previous point, the toy model setup, as mentioned by the authors in lines 344-353, is closer to treating each input/output as a token rather than a complete prompt/response, which is not a good toy model approximation of the real situation. One possible improvement would be to maintain other settings unchanged while increasing the sample size to enable mini-batch optimization that better resembles real-world conditions, with fewer epoch repetitions.\n\n- While the authors used self-built Poem and Slogan datasets to evaluate the model's instruction following ability and acknowledged their limited scope, these datasets are insufficient to assess the model's general instruction following capabilities. The paper lacks evaluation on widely-used benchmarks in preference optimization work, such as AlpacaEval2, Arena-Hard, and MT-Bench, which are designed to test models' general instruction following ability.\n\n- The proposed regularization techniques lack substantial significance. The first technique, which independently adjusts beta for reject responses, shows effectiveness in the poem task, but the optimal reject beta is merely 0.02 lower than the chosen beta. Without showing gradient comparisons for this technique, it's unclear whether it actually improves performance by addressing the large gap demonstrated in Figure 2. Moreover, the second technique, SFT loss, is already a widely established regularization technique.\n\n- I am not quite convinced by the claims in section 3.4. Although existing works are cited to establish conceptual connections between RM and DPO, the subsequent gradient analysis focuses on r, creating a gap with the previous gradient analysis that focused on $\\pi$."
            },
            "questions": {
                "value": "- The probability distributions in the bottom-right figure don't seem to match with the leftmost figure in Figure 2. In Figure 2, the unseen probability at 500 epochs approaches 1, but in Figure 1 it's all zeros. The chosen probabilities also don't quite align."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}