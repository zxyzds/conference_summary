{
    "id": "0T49QbSOho",
    "title": "Regret-Optimal List Replicable Bandit Learning: Matching Upper and Lower Bounds",
    "abstract": "This paper investigates *list replicability* [Dixon et al., 2023] in the context of multi-armed (also linear) bandits (MAB). We define an algorithm $A$ for MAB to be $(\\ell,\\delta)$-list replicable if with probability at least $1-\\delta$, $A$ has at most $\\ell$ traces in independent executions even with different random bits, where a trace means sequence of arms played during an execution. For $k$-armed bandits, although the total number of traces can be $\\Omega(k^T)$ for a time horizon $T$, we present several surprising upper bounds that either independent of or logarithmic of $T$: (1) a $(2^{k},\\delta)$-list replicable algorithm with near-optimal regret, $\\widetilde{O}{\\sqrt{kT}}$, (2) a $(O(k/\\delta),\\delta)$-list replicable algorithm with regret $\\widetilde{O}\\left(\\frac{k}{\\delta}\\sqrt{kT}\\right)$, (3) a $((k+1)^{B-1}, \\delta)$-list replicable algorithm with regret $\\widetilde{O}(k^{\\frac{3}{2}}T^{{\\frac{1}{2}}+2^{-\\Omega(B)}})$ for any integer $B>1$. We show that result (3) is nearly tight by establishing there are no $(k-1,\\delta)$-list replicable algorithm with $o(T)$-regret, almost exactly matching $k$-list replicable upper bound for $B=2$. We further show that for linear bandits with $d$-dimensional features, there is a $\\widetilde{O}(d^2T^{1/2+2^{-\\Omega(B)}})$-regret algorithm with $((2d+1)^{B-1},\\delta)$-list replicability, for $B>1$, even when the number of possible arms can be infinite.",
    "keywords": [
        "Replicability",
        "Regret Bound",
        "Bandit"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=0T49QbSOho",
    "pdf_link": "https://openreview.net/pdf?id=0T49QbSOho",
    "comments": [
        {
            "summary": {
                "value": "This paper studies list replicability in multi-armed bandits (MAB), defining an algorithm as list replicable if it limits the distinct arm sequences (traces) across independent executions with high probability. Further, this paper proposes three algorithms with different parameters of list replicability. Finally, this paper investigates a lower bound of bandits with list replicability."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The problem setting proposed is both novel and intriguing, characterized by a rigorously defined concept of bandit replicability in Definition 2.2.\n2. The theoretical analysis provided is exhaustive, introducing three distinct algorithms tailored to various parameters of replicability."
            },
            "weaknesses": {
                "value": "1. Algorithms 1 and 2 exhibit considerable similarities. Could there be a method to consolidate these two algorithms into a unified framework?\n\n2. In Theorem 6.1, the designation \"lower bound\" appears misapplied as it does not seem to correspond to the lower bounds of any algorithms discussed previously. Notably, in Theorem 6.1 we have $l \\approx k$, whereas in prior algorithms $l \\gg k$ in most cases. In my humble opinion, a valid lower bound should be able to explain whether the proposed algorithms can be further optimized in general.\nFurthermore, why the authors said \"We show that result (3) is nearly tight for B=2\" in the abstract. What's the hidden constant behind $\\Omega(B) $ in (3). Do you mean the regret of (3) is $O(T)$ for $B=2$?\n\n3. Would it be more accurate to describe what is currently referred to as \"lower bounds\" in Theorem 6.1 as \"impossibility results\"? I think Theorem 6.1 is quite trivial because any pair of traces should share more than two arms if the total number of traces is less than $K$.\n\n4. The absence of experimental validation in this paper is notable. Including even preliminary numerical simulations or toy experiments could significantly enhance the validity and impact of the proposed algorithms."
            },
            "questions": {
                "value": "See weaknesses."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper studies list replicability in multi-armed bandits and linear bandits. It comes up with the notion of $(\\ell, \\delta)$-list replicability, and proved various trade-off between replicability and regret dependency on number of arms and on time horizon. Furthermore, the paper extends the results to linear bandits setting."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper proposes a definition of reproducibility in bandits problems.\n2. The paper proves tight trade-off between replicability and regret dependency on $T$. \n3. The proof to the lower bound is quite insightful."
            },
            "weaknesses": {
                "value": "1. The algorithms are generally based on successive elimination, so it contains less insight on more widely used bandits algorithms like UCB.\n2. The proofs to the upper bounds are quite simple and lack enough novelty given their similarity to successive elimination."
            },
            "questions": {
                "value": "1. Line 18, $\\widetilde O \\sqrt{kT}$ missing parentheses.\n2. The notion of $O(\\cdot)$ and $\\Omega(\\cdot)$ was a little abused. The paper contains regret bound like $\\widetilde O (k^{\\frac32} T^{\\frac12 + 2^{-\\Omega(B)}})$. Here, it's inappropriate to use $\\Omega(\\cdot)$ in $\\widetilde O(T^{2^{-\\Omega(B)}})$, because the constant before $B$ cannot be ignored, e.g., $T^{2^{-B}}$ and $T^{2^{-2B}}$ have very different order."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces replicability to the multi-armed bandit area through the concept of list replicability and proposes algorithms for both k-armed and linear bandits. Notably, for k-armed bandits, the authors provide a lower bound demonstrating that one proposed algorithm is nearly optimal."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper is well-written and structured, with a clear motivation. Tho short, it presents a comprehensive set of results for both k-armed and linear bandits, though the linear bandit results appear to be preliminary."
            },
            "weaknesses": {
                "value": "- It would be helpful to clarify which variables the hidden logarithmic factors depend on, and whether these factors are consistent throughout the paper.\n- No experiments are presented."
            },
            "questions": {
                "value": "- While it seems that replicability papers often omit experiments, bandit experiments are generally straightforward to conduct. Did the authors consider demonstrating some experimental results?\n- Most of the algorithms appear to be adaptations of standard elimination-based bandit algorithms for both k-armed and linear bandit problems. It would be valuable if the authors could reference each classical elimination algorithm and include a side-by-side comparison showing what aspects of these algorithms break replicability and how the new modifications enable it.\n- Given that the study addresses regret minimization\u2014typically dominated by UCB-type algorithms for stronger instance guarantees\u2014the authors\u2019 choice of elimination-based algorithms is interesting. Could you clarify the rationale behind this choice?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces the concept of list replicability to the multi-armed bandit model, where the sequence of arm pulls must lie in a small finite list with high probability. The authors design and analyze three algorithms, each providing different levels of guarantees on list replicability and high-probability regret. Additionally, a nearly matching lower bound is proved for any algorithm with sub-linear regret. The paper also extends the study to linear bandits."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Although the paper is highly theoretical, it is well-presented and clearly conveys the key ideas behind the algorithm designs and proofs.\n\n2. Three algorithms with varying levels of guarantees are introduced, each with its own significance. Notably, the first algorithm achieves near-optimal cumulative regret, and the total number of possible traces is independent of T. The last algorithm is based on a subroutine from Dixon et al. (2023) and is nearly optimal, given the lower bound in Section 6.\n\n3. The theoretical contributions are nontrivial, and the analysis of the phase-elimination algorithm is novel, which should be of interest to the bandit community. It is also interesting that the lower bound is proven using the Sperner/KKM lemma, a combinatorial result in coloring."
            },
            "weaknesses": {
                "value": "The main criticism of the paper might lie in its motivation. In the introduction, it is suggested that list replicability might be beneficial for safety-critical applications, as one could be prepared for the action sequence being played. However, although the proposed algorithms can ensure a small number of traces with high probability, these possible traces cannot be known without exact knowledge of the problem instance. Therefore, outside of the theoretical domain, the practical application of list replicability seems limited."
            },
            "questions": {
                "value": "1. Could you compare $\\rho$-replicability and list replicability with respect to their potential practical applications, such as in clinical trials?\n2. Why is $C$ referred to as the number of shifts? Do you mean the number of possible shift $r$?\n3. Minor typos: Line 207: Theorem 2.1 -> Assumption 2.1; Line 210: lemma -> lemmas; Line 346: the of -> the number of.\n4. Thomson sampling and UCB are two well-established algorithms in the bandit literature. Thomson sampling is randomized, making it tricky to provide strong list replicability guarantees. Could you discuss the potential challenges in adapting UCB? My intuition is that UCB might achieve good list replicability with appropriate early-stage modifications."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}