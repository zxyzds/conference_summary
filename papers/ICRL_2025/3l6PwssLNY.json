{
    "id": "3l6PwssLNY",
    "title": "CR2PQ: Continuous Relative Rotary Positional Query for Dense Visual Representation Learning",
    "abstract": "Dense visual contrastive learning (DRL) shows promise for learning localized information in dense prediction tasks, but struggles with establishing pixel/patch correspondence across different views (cross-contrasting). Existing methods primarily rely on self-contrasting the same view with variations, limiting input variance and hindering downstream performance. This paper delves into the mechanisms of self-contrasting and cross-contrasting, identifying the crux of the issue: transforming discrete positional embeddings to continuous representations. To address the correspondence problem, we propose a Continuous Relative Rotary Positional Query ({\\mname}), enabling patch-level representation learning. Our extensive experiments on standard datasets demonstrate state-of-the-art (SOTA) results. Compared to the previous SOTA method (PQCL), our approach achieves significant improvements on COCO: with 300 epochs of pretraining, {\\mname} obtains \\textbf{3.4\\%} mAP$^{bb}$ and \\textbf{2.1\\%} mAP$^{mk}$ improvements for detection and segmentation tasks, respectively. Furthermore, {\\mname} exhibits faster convergence, achieving \\textbf{10.4\\%} mAP$^{bb}$ and \\textbf{7.9\\%} mAP$^{mk}$ improvements over SOTA with just 40 epochs of pretraining.",
    "keywords": [
        "Self-supervised learning",
        "Distillation"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "",
    "creation_date": "2024-09-23",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=3l6PwssLNY",
    "pdf_link": "https://openreview.net/pdf?id=3l6PwssLNY",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces the Continuous Relative Rotary Positional Query to enhance dense visual contrastive learning by improving pixel/patch correspondence across different views. It addresses limitations in existing self-contrasting methods by transforming discrete positional embeddings into continuous representations. The proposed CR2PQ enables more effective patch-level representation learning, achieving state-of-the-art results and faster convergence in detection and segmentation tasks on the COCO dataset."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Writing quality is good. The paper is well-structured, and clearly written.\n2. SOTA performance. The paper demonstrates the state-of-the-art performance on mainstream detection and segmentation datasets, such as COCO and ADE20K, which is impressive.\n3. Versatility of the method. The paper shows the simplicity of CR2PQ, which can be easily integrated into a variety of popular representation learning frameworks, such as mask-based learning, contrastive learning, and distillation methods."
            },
            "weaknesses": {
                "value": "1. Reliance on random cropping. Although random cropping can increase the variability of the input, its results may still be limited by the randomness of the cropping. In extreme cases, it may result in almost no overlap between the generated views, affecting the learning effect of the model.\n2. Computational complexity. Complex matrix operations are required when calculating relative position embedding and rotating embedding, which increases the burden in scenarios with limited computing power.\n\nP.S. There is an error in Figure 1. [CLS] should be global information, while patch is local information."
            },
            "questions": {
                "value": "Please refer to the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a distillation technique where a student is densely trained to match teacher features. The novelty comes from using 2D RoPE in the network as well as a cross-attention module with relative positional information. They show good empirical results on detection and segmentation."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "-The empirical results are good and outperform previous SOTA.\n\n-I think this paper can be worthwhile to accept, I'm willing to improve my score based on the author's reply."
            },
            "weaknesses": {
                "value": "-L142: Relative positional encoding = RoPE?\n\n-L161: W_{pos} v.s. P_{pos} ?\n\n-The notation in equation 1 is confusing. It is as if the patches don\u2019t interact with each other. I would use a new variable to define a patch representation. Also if f_\\theta denotes the ViT, why does it take z as input, which already contains the linear layer on the left side of the equation but not on the right side. I think the notation should be made more precise.\n\n-Equation 2 has some n and m mixed.\n\n-L219: \u201cwe set each patch size of the view A as 1\u201d, but in L227 p_A (the patch size) is defined?\n\n-L228: There is a sentence \u201cSince we set each grid size of the anchor view as 1.\u201d What is that supposed to mean?\n\n-L297: If I\u2019m not mistaken, the definition of q doesn\u2019t make sense.\n\n-The first stated contribution is using 2D RoPE for SSL based methods. Then, in L358, shoud state \u201cWe also evaluate the detection and segmentation without pretraining i.e. directly using 2D RoPE\u201d. First, that entry is only in Table 1 and not Table 2. Second, I think you should also independently show empirical evidence of your 2 first contributions (2D RoPE and cross-attention module) and report results for that.\n\n-In general, I think the paper could be more explicitaly precise with how sizes/positions are encoded e.g. is it relative to the original image input grid or relative to the crop?\n\n\n\nMinor:\n\n-L082: \u201cas the downstream task only input\u201d\n\n-If I\u2019m not mistaken, there is a problem with sentence at L203 starting with i.e."
            },
            "questions": {
                "value": "-Why use a pretraining network for the teacher? You are comparing with other baselines which some of which learn everything from scratch. This seems like a logical thing to try, have you tried that?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "1. The paper introduces Continuous Relative Rotary Positional Query (CR2PQ), a novel method for dense visual representation learning.\nCR2PQ addresses the challenge of establishing pixel/patch correspondence across different views in dense contrastive learning (DRL) by transforming discrete positional embeddings to continuous representations.\n\n2. It utilizes a rotary positional embedding to represent the relative positions between two views and reconstructs the latent representations of one view from another through a rotary positional query.\n\n3. The method simplifies the dense contrastive learning paradigm by making it correspondence-free and integrates easily into various representation learning frameworks.\n\n4. Extensive experiments on standard datasets demonstrate state-of-the-art (SOTA) results, outperforming the previous SOTA method (PQCL) significantly in detection and segmentation tasks on COCO with improved mAP scores."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. CR2PQ introduces a pioneering method for dense visual representation learning by utilizing continuous relative rotary positional embeddings, which is a significant departure from traditional discrete embeddings.\n\n2. The method achieves state-of-the-art results across various benchmarks, including object detection and segmentation tasks on COCO and semantic segmentation on ADE20K, outperforming previous leading methods by a considerable margin.\n\n3. The introduction of a positional-aware cross attention module enhances the learning of semantic information without incurring significant additional computational costs. CR2PQ's use of rotary positional embeddings makes it robust to various view augmentations, including random cropping, which is a common challenge in contrastive learning methods.\n\n4. The paper supports the method's strengths through extensive experiments and ablation studies, providing a thorough analysis of CR2PQ's performance under different conditions and configurations."
            },
            "weaknesses": {
                "value": "1. Experiments. The author should provide more scales of backbone to validate the scalability of the method. Most experiments are conducted on ViT-S. The reviewer understands the efficiency of the experiments, however, there should be some experiments on larger backbones."
            },
            "questions": {
                "value": "1. What is the performance of the CR2PQ backbone performance on some strong detectors, such as DINO or Co-DETR?\n\n2. CR2PQ requires the teacher model to provide contrastive pairs, however, the performance does not improve as the model becomes larger (ViT-L vs ResNet50). The reviewer wonders about the performance of a larger model for the student. Does this approach work for a larger backbone as a student, such as ViT-L/ViT-G? The authors are suggested to validate the scalability of the method.\n\n3. Some small mistakes\n\n- The font of the paper is different from other papers. Should it be correct? \n\n- line 274, there is an overlap between the table and the caption."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper presents a novel self-supervised framework for dense visual representation learning, which avoids the need for explicit dense correspondences between local features across views. Instead, the framework reframes the task as predicting local representations from one view to another, guided by relative positional cues. It integrates rotary positional embeddings within the student model and distills knowledge from a pre-trained, frozen teacher model. This approach yields faster convergence and improved performance on standard benchmark evaluations."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The proposed self-supervised framework for dense visual representation learning is novel.\n- The method elegantly eliminates the need to establish explicit correspondence between local features across views by leveraging relative positional cues.\n- The performance on dense downstream tasks is thoroughly evaluated, showing faster convergence and achieving state-of-the-art results on standard benchmarks."
            },
            "weaknesses": {
                "value": "- The method differs from existing baselines in three key ways: (1) the use of rotary positional embeddings, (2) the use of a pre-trained, frozen teacher model, and (3) the proposed pretext task. This makes it challenging to assess the contribution of each component to the overall performance. Specifically, the fairness of the experimental setup is questionable, as other methods are trained from scratch while CR2PQ benefits from a pre-trained teacher. More ablation studies are needed to separate the impact of each element.\n\n- Overall, the writing is difficult to follow, with multiple notation inconsistencies, typos, and signs of negative vertical spacing used to fit within the page limit.\n\n- Equation 1 is misleading/incorrect as it suggests that the representation of a single patch is independent of its context.\n- Equation 2: The angle of the key seems incorrect.\n- Line 210: The image dimensions are inconsistent with line 157.\n- Line 214: Inconsistent use of $\\mathbf{p}{a}$ and $\\mathbf{p}{A}$.\n- Line 234: The notation is inconsistent with the left side of Equation 3.\n- Table 1: Framwork $\\rightarrow$ framework.\n- Figure 1: There seem to be inconsistencies in the notations used within the figure and also with respect to the method section.\n- \"pertaining\" $\\rightarrow$ \"pretraining\"/\"pre-training\" (11 occurrences).\n- Line 86: exhausted $\\rightarrow$ exhaustive.\n- Line 161: $\\mathbf{W}{pos}$ $\\rightarrow$ $\\mathbf{P}^{i}{pos}$."
            },
            "questions": {
                "value": "- In Table 1, what does the row \"RoPE\" exactly correspond to? A ViT-S/16 equipped with rotary positional embedding, randomly initialized and finetuned on the downstream task?\n\n- In Table 4, what does the row \"EMA update (Contrastive)\" exactly correspond to? Is the teacher randomly initialized?\n\n- At line 219. it is mentioned that the patch size of view A is set to 1, but then it is set to $p_{A}$. Can you clarify this?\n\n- At line 227: I suggest using another notation for $p_{A}$ as the patch size, as it is confusing."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}