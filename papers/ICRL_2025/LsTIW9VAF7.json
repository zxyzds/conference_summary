{
    "id": "LsTIW9VAF7",
    "title": "Less is More: Stealthy and Adaptive Clean-Image Backdoor Attacks with Few Poisoned",
    "abstract": "Deep neural networks are fundamental in security-critical applications such as facial recognition, autonomous driving, and medical diagnostics, yet they are vulnerable to backdoor attacks. Clean-image backdoor attack, a stealthy attack utilizing solely label manipulation to implant backdoors, renders models vulnerable to exploitation by malicious labelers. However, existing clean-image backdoor attacks likely lead to a noticeable drop in Clean Accuracy (CA), decreasing their stealthiness. In this paper, we show that clean-image backdoor attacks can achieve a negligible decrease in CA by poisoning only a few samples while still maintaining a high attack success rate. We introduce **G**enerative Adversarial **C**lean-Image **B**ackdoors (GCB), a novel attack method that minimizes the drop in CA to less than 1\\% by optimizing the trigger pattern for easier learning by the victim model. Leveraging a variant of InfoGAN, we ensure that the trigger pattern we used has already been contained in some training images and can be easily separated from those feature patterns used for benign tasks. Our experiments demonstrate that GCB can be adapted to 5 datasets\u2014including MNIST, CIFAR-10, CIFAR-100, GTSRB, and Tiny-ImageNet\u20145 different architectures, and 4 tasks, including classification, multi-label classification, regression, and segmentation. Furthermore, GCB demonstrates strong resistance to backdoor defenses, successfully evading all detection methods we know. Code: *anonymous.4open.science/r/GCB*.",
    "keywords": [
        "Backdoor Attack",
        "Generative Adversarial Networks",
        "Clean-Image Backdoor Attacks",
        "Deep Neural Networks",
        "InfoGAN",
        "Poisoning Attack",
        "Model Integrity"
    ],
    "primary_area": "applications to computer vision, audio, language, and other modalities",
    "TLDR": "We introduce GCB, a GAN-based method for clean-image backdoor attacks that achieves high success rates with minimal impact on model accuracy by optimizing triggers.",
    "creation_date": "2024-09-28",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=LsTIW9VAF7",
    "pdf_link": "https://openreview.net/pdf?id=LsTIW9VAF7",
    "comments": [
        {
            "summary": {
                "value": "This paper proposes a mutual information-constrained approach for backdoor pattern generation, to create backdoored samples with similar distribution to the target class. Therefore, the authors can enhance the stealthiness of backdoor samples. The authors demonstrate the strong correlation between the backdoor samples and the backdoor labels, showing that such samples can be easily learned by the model."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The authors propose to generate backdoor samples based on InfoGAN, enhancing stealthy of backdoor attacks. The proposed backdoor is proved to be more undetectable and easy to learn. This work is validated by theoretical analysis and supported by well-designed experiments."
            },
            "weaknesses": {
                "value": "1.To my best knowledge, other advanced clean image backdoor methods are strongly related to the proposed approach. Although they are not referred to as \u2018clean image backdoors,\u2019 they are also rather \u2018invisible backdoors\u2019. From this perspective, the innovation of the proposed approach seems limited. I suggest that the authors compare their method with these state-of-the-art techniques and clarify its advantages.\nReferences:\n[1]Li, Yuezun, et al. \"Invisible backdoor attack with sample-specific triggers.\"\u00a0Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n[2]S. Li, M. Xue, B. Z. H. Zhao, H. Zhu and X. Zhang, \"Invisible Backdoor Attacks on Deep Neural Networks Via Steganography and Regularization,\" in\u00a0IEEE Transactions on Dependable and Secure Computing, vol. 18, no. 5, pp. 2088-2105, 1 Sept.-Oct. 2021, doi: 10.1109/TDSC.2020.3021407.\n[3]R. Ning, J. Li, C. Xin and H. Wu, \"Invisible Poison: A Blackbox Clean Label Backdoor Attack to Deep Neural Networks,\"\u00a0IEEE INFOCOM 2021 - IEEE Conference on Computer Communications, Vancouver, BC, Canada, 2021, pp. 1-10, doi: 10.1109/INFOCOM42981.2021.9488902.\n2.The condition \u2018irrelevance\u2019 is only briefly explained in the ablation study, where this loss is removed to measure ASR. However, the experiments do not effectively demonstrate why the generated samples are irrelevant to the original-class samples."
            },
            "questions": {
                "value": "1.In Figure 4, the authors analyze the learning rate on backdoor data using the CIFAR-10 dataset. I find it interesting that the learning rate of GCB is higher than that of BadNet and clean image, as this contradicts previous findings suggesting that more \u2018out-of-distribution\u2019 backdoor samples are learned faster. To better validate this observation, I recommend the authors to test on additional datasets."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper proposes a clean-image backdoor attack that achieves low poisoning rates even in the all-to-one setting. The proposed method relies on learning an InfoGAN's generator network to generate images from real and fake classes (this is used to generate the triggered images) and a discriminator network to distinguish samples between real and fake classes (this is used as scoring function to select poisoned samples). This design of InfoGAN's networks ensures that the trigger patterns exist within the real images, while making the poisoned tuples separated from the clean tuples for easier backdoor training. The paper provides extensive empirical results to demonstrate the effectiveness of the proposed method on multiple datasets, architectures, and defenses."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The main strengths of the paper lie in its clever use of InfoGAN and the extensive empirical results (although I do have some concerns in these aspects as well)\n* The use of InfoGAN, while trivial, shows a cleverness in using it for clean-image backdoor attack. \n* The experiments include multiple benchmark datasets. The paper also evaluate against multiple networks and a large number of defenses."
            },
            "weaknesses": {
                "value": "While this paper is interesting, I also find several concerns, specifically on its rigorous analysis of why the method works so well:\n\n* The paper proposes to use Wasserstein loss, but the theoretical analysis instead shows the convergence on JS Divergence. This is a crucial mismatch. In fact, I don't even think the proof of convergence is necessary because it is quite well established for InfoGAN (and GAN in general), and the paper does not change anything in the base InfoGAN model, rather than changing the model's input. \n* I also find that the statement of converging, especially in the context of GANs, is quite strong. It's been known that GANs' theoretical convergence and what actually happens in practice are two very different things. I suggest that the paper focuses more on analyzing why the scoring function works so well instead, and provide a more rigorous analysis there. \n* In fact, I find that the design of the attack is based on several assumptions (such as convergence) that may or may not hold in practice. While the final results show favorable performance for GCB, the paper lacks rigorous connections between these assumptions and the performance, which is a bit disappointing. \n* For example, the statement that the backdoor can be learned even more easily than BadNets deserves more rigorous analysis. In general, in backdoor attacks, when the backdoor is learned really easily, it also causes several consequences; for example, ABL relies on the fact that the loss of poisoned samples drops abruptly during training, which urges the question of why GCB works so well. As there are so many backdoor attack papers in the last several years, I think that these analyses are much more important than demonstrating that the method \"just works very well\".\n* I also find that while several defenses have been tested (which is commendable), a new category of defenses (based on fine-tuning, such as FT-SAM) is not evaluated. I wonder whether the fact that the backdoor is learned very quickly could also mean that fine-tuning defenses could work very well against GCB.\n* Another weakness of the paper is that several experiments only include the evaluations on CIFAR10 and CIFAR100, which are essentially the same. I would suggest that the paper to include all datasets.\n* On line 214, the paper suddenly introduces *c*, a notation that is not explained until a bit later. I find that this part of the paper could be improved quite a lot. In addition, I struggled a bit to understand how the triggered images are created during inference. I think the paper assumes that the reader is very familiar with the backdoor domain, and I hope the paper can make this part clearer and more accessible to new readers.\n* Some minor grammatical errors/typos: - line 69: \"construct trigger\", several places the paper mention InforGAN."
            },
            "questions": {
                "value": "Please see the concerns in the weaknesses!"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes a GAN-based architecture that makes backdoor triggers easier to learn for the victim model and hence achieves high ASR and hinders non-trivial clean accuracy drop. The proposed GCB (Generative adversarial clean image backdoor) combines InfoGAN and Conditional GAN to ensure that the backdoor trigger meet requirements of existence, separability and irrelevancy. The authors perform comprehensive evaluations over various dataset across different model architectures and demonstrate that GCB achieves high ASR and CA with a low poisoning rate. They also conduct a extensive ablation study of their design choices and assess GCB against multiple existing defenses."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- This paper tackles the clean accuracy drop issue in clean-image backdoors, which further improves the effectiveness and stealthiness of such an attack.\n\n- The proposed GAN architecture is technically sounded and novel. The authors explicitly explain their motivations of using InfoGAN and conditional GAN, and the GCB design is well-aligned with the objectives of triggers properties (i.e., existence, separability and irrelevancy).\n\n- The authors also provide detailed theoretical proofs and mathematical analysis for the design.\n\n- The experiments are abundant to support the authors claims. They show that the GCB can generalize to different datasets and model architectures including TinyImageNet and ViT.  They also test it on multiple vision tasks such as image regression and segmentation. In addition, GCB is evaluated against prestigious defenses for backdoor attacks.\n\n- The paper is well-written and easy to follow."
            },
            "weaknesses": {
                "value": "- I don't see any major weaknesses/issues in this paper."
            },
            "questions": {
                "value": "- In the paper, you show figures of selected training samples and triggered test samples. Can you also provide the figure of generated triggers?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces Generative Adversarial Clean-Image Backdoors (GCB), a backdoor attack technique that minimizes detection in neural networks used for sensitive applications like facial recognition and autonomous driving. Unlike traditional backdoor attacks, GCB only uses clean images with manipulated labels, avoiding noticeable accuracy drops and making the attack stealthier. The technique employs a variant of InfoGAN, called C-InfoGAN, to embed backdoor triggers naturally by manipulating benign features in the dataset. The method maintains high attack success rates (ASR) across multiple tasks, datasets, and architectures, even with low poison rates. Extensive experiments show GCB\u2019s resilience against numerous backdoor defenses, including Neural Cleanse and STRIP, and strong adaptability across diverse visual tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "(1)  GCB achieves high attack success rates (ASR) with minimal impact on clean accuracy (CA), which is critical for stealthiness in security-sensitive applications.\n\n(2) With a poison rate as low as 0.1%, GCB still maintains high ASR, showcasing efficiency in terms of resource requirements."
            },
            "weaknesses": {
                "value": "(1) This paper employs GANs to generate the trigger image. However, GANs are known for their limited ability to fit complex data distributions, such as ImageNet, raising concerns about the method's applicability to more complex datasets.\n\n(2) Since the trigger in this study is GAN-generated, there may be a significant distributional gap between the generated trigger and real image data. Consequently, it is essential to analyze the method\u2019s resistance to defense strategies based on abnormal sample detection.\n\n(3) The theoretical justification in Section 3.2 closely aligns with prior work on InfoGAN, and this overlap should be clarified to better situate the contribution within the existing literature.\n\n(4) The paper uses InfoGAN to partition the benign training set into two subsets (A and B), selecting one (B) for poisoning. However, this approach may reduce benign accuracy on the original subset B. Additional experiments and discussion regarding the impact on benign accuracy would strengthen this section."
            },
            "questions": {
                "value": "(1) This paper employs GANs to generate the trigger image. However, GANs are known for their limited ability to fit complex data distributions, such as ImageNet, raising concerns about the method's applicability to more complex datasets.\n\n(2) Since the trigger in this study is GAN-generated, there may be a significant distributional gap between the generated trigger and real image data. Consequently, it is essential to analyze the method\u2019s resistance to defense strategies based on abnormal sample detection.\n\n(3) The theoretical justification in Section 3.2 closely aligns with prior work on InfoGAN, and this overlap should be clarified to better situate the contribution within the existing literature.\n\n(4) The paper uses InfoGAN to partition the benign training set into two subsets (A and B), selecting one (B) for poisoning. However, this approach may reduce benign accuracy on the original subset B. Additional experiments and discussion regarding the impact on benign accuracy would strengthen this section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Generative Adversarial Clean-Image Backdoors (GCB), a novel clean-image backdoor attack method that maintains high ASR with low poison rates and minimal drop in clean accuracy (CA). The key idea is optimizing the trigger pattern to make it easier for the victim model to learn, by using a variant of InfoGAN called C-InfoGAN. Experiments demonstrate GCB's effectiveness across 5 datasets, 5 model architectures, and 4 vision tasks. GCB also shows strong resistance to existing backdoor defenses."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Achieves outstanding stealthiness, with high ASR (>90%), low poison rate (<=1%), and minimal CA drop (<=1%) across all tested datasets. This significantly advances clean-image backdoor attack capabilities.\n- Shows strong adaptivity to 5 datasets, 5 architectures, and 4 vision tasks beyond just classification. Indicates the attack is widely applicable.\n- Introduces a novel C-InfoGAN method to optimize triggers for easy learning without interfering with clean task accuracy. The theoretical analysis supports why this works.\n- Demonstrates robustness to a wide range of backdoor defenses, revealing gaps in existing mitigation techniques that need to be addressed.\n- Extensive experiments and ablations provide good insight into the attack's behavior and validate the approach."
            },
            "weaknesses": {
                "value": "- Scalability concerns: The paper only evaluates on relatively small datasets. The required poison rate increases as dataset complexity grows (e.g. from CIFAR-10 to Tiny ImageNet), suggesting scalability issues. It's unclear if the method would still be effective on large-scale datasets like ImageNet-1K without requiring an impractically high poison rate. Testing on a wider range of dataset sizes would help assess the scalability limits.\n- Limited evaluation against newer defenses: Many of the backdoor defenses tested are relatively dated. The attack's effectiveness against more recent state-of-the-art defenses, particularly those developed in the past 1-2 years, is not demonstrated. Additionally, for the Label Cleaning experiments, only one technique is evaluated. There are several other advanced Label Cleaning approaches that may be more effective but are not considered, such as DivideMix, MentorMix, and Robust Meta-Learning.\n- Relabeling mitigation analysis: The authors' claim that a >95% relabeling rate is needed to keep ASR below 20% seems questionable given advancements in vision-language models. With the increasing popularity and capability of models like CLIP and BLIP, it may be feasible to automatically relabel large portions of the training set accurately and efficiently. This could significantly lower the cost of relabeling and make it a more viable mitigation strategy. The paper's analysis of relabeling as a defense does not sufficiently consider this."
            },
            "questions": {
                "value": "- Scalability: Have you considered evaluating GCB on larger, more complex datasets beyond Tiny ImageNet, such as ImageNet-1K? How do you expect the attack performance and required poison rate to scale on these large-scale datasets?\n- Hyperparameter sensitivity: How sensitive is the attack performance to the choice of hyperparameters, such as the learning rate and weight decay for the C-InfoGAN training? Did you find that careful tuning was necessary to achieve good results, or is the attack relatively robust to hyperparameter choices?\n- Transferability to other domains: While the paper demonstrates strong results on vision tasks, do you expect the GCB attack to generalize to other data modalities, such as audio, text, or graphs classification? What challenges might arise in adapting the method to these domains?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}