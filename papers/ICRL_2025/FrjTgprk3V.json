{
    "id": "FrjTgprk3V",
    "title": "Emergence in non-neural models: grokking modular arithmetic via average gradient outer product",
    "abstract": "Neural networks trained to solve modular arithmetic tasks exhibit grokking, the phenomenon where the test accuracy improves only long after the model achieves 100% training accuracy in the training process. It is often taken as an example of ``emergence'', where model ability manifests sharply through a phase transition. In this work, we show that the phenomenon of grokking is not specific to neural networks nor to gradient descent-based optimization. Specifically, we show that grokking occurs when learning modular arithmetic with Recursive Feature Machines (RFM), an iterative algorithm that uses the Average Gradient Outer Product (AGOP) to enable task-specific feature learning with kernel machines. We show that RFM and, furthermore, neural networks that solve modular arithmetic learn block-circulant features transformations which implement the previously proposed Fourier multiplication algorithm.",
    "keywords": [
        "Theory of deep learning",
        "grokking",
        "modular arithmetic",
        "feature learning",
        "kernel methods",
        "average gradient outer product (AGOP)",
        "emergence"
    ],
    "primary_area": "learning theory",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=FrjTgprk3V",
    "pdf_link": "https://openreview.net/pdf?id=FrjTgprk3V",
    "comments": [
        {
            "summary": {
                "value": "This paper studies the phenomenon of grokking in non-neural models, specifically Recursive Feature Machines (RFMs). In the setting they study (modular arithmetic), skills emerge purely from feature learning and a block-circulant pattern emerges in the learned square root of the AGOP matrices. Similar to previous work they identify progress measures related to the block circulant structure of the feature matrices which linearly improve during grokking. They also connect their findings to neural networks, showing that the NFM and neural network AGOP exhibit high correlation and contain a similar block circulant structure. Finally, they prove that kernel machines with these block circulant features implement the Fourier multiplication algorithm previously discovered to be implemented by neural networks to solve modular arithmetic."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- The paper is written and organized well, with the significance of the results in each section clear. \n- While it is a kernel setting, there are several interesting empirical results that cleanly exhibit grokking, the emergence of structured features, and linear progress measures despite training loss being 0 throughout.\n- The demonstration of grokking in their kernel setting challenges proposed explanations for grokking, such as grokking having a \u2018lazy\u2019 and \u2018rich\u2019 regime  with no feature learning and feature learning respectively.\n- This work suggests that there is a gap in our available theory to provide effective measures for generalization and explanations for mechanisms of emergence. I believe the work may be valuable to make a call to the community for a need of better theoretical tools and other directions (such as developing progress measures that are more general/applicable without having access to the training mechanism apriori, and are not tied to metrics like loss)."
            },
            "weaknesses": {
                "value": "- This work focuses on a particular task, providing evidence that there may be tasks in which skills can emerge purely as a result of feature learning and independent of loss, but the scope of the results extending to other task and the use case of this particular kernel machine setting is less clear.\n- There is still a gap between theory and experiment where there lacks a theoretical proof that grokking will occur, or identify what the mechanism is behind learning modular arithmetic from a finite set of samples.\n- I\u2019m not very familiar with the prior literature on RFMs and the use of AGOPs to explain feature learning for neural networks; I know there is context sprinkled throughout in section 2, but it might be useful to reframe Appendix A as a more consolidated review of its introduction and previous use cases."
            },
            "questions": {
                "value": "1. The empirical validity of emergence has been a popular and contentious point of discussion in the community. Even after Schaeffer\u2019s work, there still does remain tasks which seem emergent independent of metric choice. Do the authors have any thoughts about characteristics of tasks that will predict emergence, or what type of tasks will exhibit emergent behavior? \n2. How does the grokking behavior trend with the training fraction? \n3. The RFM and NN learn a similar mechanism for the tasks (i.e. implement the Fourier multiplication algorithm)-- in general, to what extent are these learned solutions for tasks exhibiting emergence \u2018universal\u2019? Could there be tasks where learned mechanism differs across algorithm or architecture, highlighting a difference in inductive bias? I'm curious if the authors tried training RFMs on other synthetic tasks that have been popular for studying grokking, such as general group operations; I'm wondering if similar structure is observed for eg. group operations on S_5 and S_6, where proposed circuits in neural networks seem to have differing perspectives (learning irreducible representations versus learning subgroup/coset structure [1]).\n\n[1] Stander, Dashiell, et al. \"Grokking Group Multiplication with Cosets.\" arXiv preprint arXiv:2312.06581 (2023)."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper attempts to isolate feature-learning in modular arithmetic tasks. They show that the known Fourier-features that are observed in neural networks grokked on modular arithmetic tasks (they call such features Fourier Multiplication Algorithm) can be viewed as \"circulant-matrix\" structure of input-output Jacobians (called AGOP) along with a global transformation via ridgeless kernel regression. They show that a non-parametric algorithm (RFM) that iteratively refines the kernel using AGOP can grok modular arithmetic tasks and find the aforementioned circulant features. They define progress measures to show the gradual learning of features as well as the agreement between the non-parametric vs neural-network-based training. They also show that initializing network weights using the circulant features accelerates training."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper is written in a clear, easy-to-follow manner.\n- The supporting experiments are well-presented.\n- Studying feature-learning in modular arithmetic tasks with non-parametric methods such as RFM is new. This analysis isolates feature-learning from the neural network training for this task.\n- Proofs supporting their claims are presented in the Appendix."
            },
            "weaknesses": {
                "value": "- Their method is essentially a different way of describing the already-known features in modular arithmetic [1,2]. Continuous progress measures  for this task also already exist in literature, and not new to this work [1,2]. Consequently, the main contribution of this work, in my opinion, is to show that such features can also be reached by using the non-parametric method, viz. RFM. Since the RFM method is also not new to this work [3], I am unable to identify the significant contribution of this paper. \nIn a barebones way, the message seems to be \"The known RFM method applied to modular arithmetic task leads to the known features, as it should.\"\n\n- The modular addition task $f(a,b) = a + b \\\\;mod\\\\; p$ (and other arithmetic tasks after the appropriate re-ordering) respect the modular version of \"translation symmetry\" in the inputs $a,b$ (i.e. $f(a+t \\\\;mod\\\\; p, b-t \\\\;mod\\\\; p) = f(a,b)$). Therefore, the feature-matrix of a network that solves this task must also respect this symmetry. This symmetry constraint readily gives the random circular matrix as the feature-matrix, making it trivial in some sense.\n(After applying this symmetry, the remaining task is to assign each symmetry class to its correct label, which can be readily achieved by kernel regression.)"
            },
            "questions": {
                "value": "- Do the authors understand why there is a difference in the progress measures between add/sub and mul/div tasks in Figure 5 (B)? Is it related to the fact that mul/sub operations have to deal with 0 separately? \n\n- Can the analysis presented in the work be used to predict the sample complexity of modular arithmetic tasks?\n\n- My understanding is that the circulant features are identical to the Fourier-features found in literature (up to a global transformation learned by kernel regression). Is this assertion correct?\n\n[1] Nanda et al., Progress measures for grokking via mechanistic interpretability (2023)\n\n[2] Gromov, Grokking Modular Arithmetic (2023)\n\n[3] Radhakrishnan et al., Mechanism for feature learning in neural networks and backpropagation-free machine learning models (2024)"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper examines the phenomenon of grokking in the context of modular arithmetic tasks. Using Recursive Feature Machines (RFM), which learn features through Average Gradient Outer Product (AGOP) updates rather than gradient descent. The authors show that grokking is not specific to neural networks or gradient descent optimization, but rather arises from feature learning itself. On top of that, the authors introduced different progressive measure that correlates with test accuracies. They also find block-circulant features that implement the Fourier multiplication algorithm."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Kernel method is rarely discussed in grokking settings\n2. The authors discuss their methods clearly\n3. Good connection with existing literatures"
            },
            "weaknesses": {
                "value": "1. More discussion on the importance of data is needed:\n    - Previous research (e.g., https://arxiv.org/abs/2201.02177) has established that there exists a minimum data threshold required for model grokking. For this kernel-based method, the behavior of this threshold remains unclear. The paper would be significantly strengthened if the authors investigated this threshold using their approach, as their method has fewer hyperparameters, which could provide compelling evidence for the essential role of data quantity in enabling grokking. \n    - At least, I would like to see a test-acc vs training data amount curve for the authors' method compared to say GD/Adam training, together with how the learned kernels change with the amount of data during training, especially how the kernels fail to form the correct feature when there is no enough training data.\n\n2. The insight that feature learning is central to grokking is not completely new, for instance, the work in https://arxiv.org/abs/2310.06110 demonstrates the train loss of a neural network decreases much earlier than its test loss can arise due to a neural network\ntransitioning from lazy training dynamics to a rich, feature learning regime.\n3. The mechanism behind the emergence of circular features during algorithm execution remains unclear, which seems crucial for understanding grokking, particularly from a dynamics perspective. From the curves I saw from the paper, I would expect some kind of lazy-to-rich transition happens here as well."
            },
            "questions": {
                "value": "1. Could the authors provide intuition about how RFM leads to this grokking behavior from the perspective of their iterative algorithm?\n2. Is the amount of data required for grokking comparable between this algorithm and other optimization methods like GD/Adam?\n3. Does the $p$ play a significant role? What are the implications of using a non-prime modulus?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}