{
    "id": "iVxxgZlXh6",
    "title": "LLaRA: Supercharging Robot Learning Data for Vision-Language Policy",
    "abstract": "LLMs with visual inputs, i.e., Vision Language Models (VLMs), have the capacity to process state information as visual-textual prompts and respond with policy decisions in text. We propose LLaRA: Large Language and Robotics Assistant, a framework that formulates robot action policy as conversations and provides improved action output when trained with auxiliary data that complements policy learning. We first introduce an automated pipeline to generate conversation-style instruction tuning data from existing behavior cloning data. Then we enrich the dataset in a self-supervised fashion by formulating six auxiliary tasks. A VLM finetuned with the resulting collection of datasets can generate meaningful robot action policy decisions. Our experiments across multiple simulated and real-world environments demonstrate the state-of-the-art performance of the proposed LLaRA framework. The code, datasets, and pretrained models will be made publicly available.",
    "keywords": [
        "Robot Learning",
        "VLM",
        "Behavior Cloning",
        "Instruction Tuning",
        "Self-supervised Learning"
    ],
    "primary_area": "applications to robotics, autonomy, planning",
    "TLDR": "Effortlessly transform a pretrained VLM into a robot policy and enhance its performance using auxiliary data generated from the same behavior cloning dataset.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=iVxxgZlXh6",
    "pdf_link": "https://openreview.net/pdf?id=iVxxgZlXh6",
    "comments": [
        {
            "comment": {
                "value": "I've been notified by the Area Chair that the ECoT work referenced in my review falls under concurrent work under ICLR's guidelines -- that's completely my bad, and I apologize!\n\nNevertheless, I am opting to maintain my score at this time. Many of the points I brought up still stand even without this comparison -- namely:\n1. That this work does not evaluate on sufficiently complex environments, only 2D pick-and-place. It is unclear if this approach would generalize to other manipulation domains, like Bridge, with actions like wiping being hard to express in \"conversational text\" as described in this paper.\n2. The tasks considered still seem to be solvable by copying values from either the task specification or from the object detection mechanism into the action space. It does not seem clear to me that one needs a complex LLM-based system for this.\n3. Given the above, if the policy does learn some kind of \"copying,\" it seems like LLaRA would be very susceptible to noisy labels (or noisy object detection generations), which seems like a major downside for real-world deployment.\n4. All the points in my \"Question\" section still stand.\n\nI will edit my original comment to make it clear which sections can now be ignored. I again apologize for any inconvenience, and would appreciate it if the authors could focus on the remaining points I made instead."
            }
        },
        {
            "summary": {
                "value": "The manuscript proposes a framework for performing auxiliary instruction-tuning of LLaVA, using a robotics behavior cloning dataset that has been converted into a conversation-style instruction-tuning dataset, with the goal of enabling the model to better understand spatiotemporal context."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The pursuit of robotics-oriented pretext tasks for training large-capacity open source models remains quite compelling."
            },
            "weaknesses": {
                "value": "Section 2, Section 6: The manuscript introduces relevant contemporary approaches \u2014 some with strong inductive biases \u2014 but the experimental comparisons with the method proposed by the manuscript remain shallow. I would like to see some direct comparisons with existing VLAs in the experiments section, in addition to ablations on, e.g., different action space representations (e.g., versus RoboPoint).\n\nSection 2 (L146-148): The manuscript states, \"Moreover, all the aforementioned studies lack of comprehensive investigation into the methods for generating auxiliary datasets from existing robot data, as well as the implications of integrating such datasets.\" OpenVLA uses mixtures of exisisting OXE component datasets, for a next action token-prediction pretext.\n\nSection 5: The dependency on high-quality behavior cloning data may limit LLaRA\u2019s performance in environments significantly different from the training data. Other methods cited by the manuscript do not share this limitation.\n\nSection 5: The pretext task proposed by this manuscript, which relies on object locations (in a tabletop coordinate system) and joint rotations will be less general than, e.g., the next-token prediction pretext and action tokenization proposed by OpenVLA. The manuscript should better motivate the reduced scope, whereas many of the models mentioned in Section 2 pursue general training and execution paradigms: across domains, embodiment, and task families.\n\n&nbsp;"
            },
            "questions": {
                "value": "No additional questions \u2014 please see above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "N/A"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper aims to transform behavior cloning data into a form that is more accessible for VLM training, enabling the VLM to leverage its pretraining for downstream tasks. The approach involves converting the BC dataset into image-text data and having the model output 2D coordinates rather than the 3D outputs. The study introduces curated datasets to assist in 2D grounding, action prediction, next observation prediction, and understanding spatial relationships between objects. The model is evaluated against RT-2 and VIMA, showing performance gains, particularly in low-data regimes."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The paper reformulates BC data into an image-text format compatible with VLMs. This is not a new idea (RT-2), but they do introduce a new output space and auxiliary objectives. \n- Restricting output to 2D coordinates sets the model apart from prior 3D-focused models like RT-2, aligning well with tasks where 3D positioning is unnecessary. Various curated datasets support 2D grounding and action prediction, aiding spatial and relational understanding. \n- The model outperforms versions of VIMA and RT-2, particularly in low-data settings.\n- They show gains on simulation and real tasks."
            },
            "weaknesses": {
                "value": "- It was not directly clear to me which components contribute to performance in the RT-2 and VIMA comparison. RT-2 also uses auxiliary tasks and internet data; it's unclear how the proposed auxiliary tasks compares. An ablation of the auxiliary tasks used in RT-2 versus LLaRA would help isolate the contribution of each component.\n- The contributions of each dataset are unclear; a more interpretable format is needed to highlight key influences on performance."
            },
            "questions": {
                "value": "- RT-2 also co-trains on auxillary tasks and internet data. How does this compare to the proposed approach?\n- For the comparison with VIMA, can additional insights be provided on which specific modeling choices (beyond single-view versus dual-view inputs) contribute to LLaRA\u2019s superior performance?\n- Can the comparisons with RT-2 and VIMA be unified to allow for a clearer, combined comparison?\n- More analysis on Figure 6 is needed to understand the dataset contribution. Which datasets contribute most? A visualization or table that clearly shows the impact of each dataset on different aspects of model performance would be helpful.\n- Is the primary difference between LLaRA and the RT-2 baseline solely the output token space?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors introduce a mechanism for (1) leveraging VLMs for robot task completion and (2) augmentation of trajectories with object/task auxiliary data."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "They present these results on both a simulated and physical benchmark. The paper also includes a substantial set of results in the appendix."
            },
            "weaknesses": {
                "value": "I'm concerned about the generality of the work, based in large part on the inconclusive trends presented.  I'll provide a series of questions below but my primary concern is that it's unclear on training.  It appears that training either has no effect or hurts performance in most cases.  There's some improvement when two epochs are run (in some conditions, but not all).  Similarly, how much data and when/where/why it helps are unclear.  This doesn't detract from the fact that a nice system was deployed but it does make it difficult to determine where this research leads in the future."
            },
            "questions": {
                "value": "1. See weakness discussion\n2. There's work on the limitations of numerical representations in transformers.  Presumably this means we should see failure cases due to misunderstanding in this work, can the authors provide those examples or justify why they wouldn't occur in this domain?\n3. Are the augmentations validated for correctness?\n4. Can Fig 5 be augmented with another plot that show how many training samples (not just trajectories) each approach has?\n5. Why was 12% chosen (e.g. Table 1), is that simply the first time the model outperforms VIMA? Do the results plateau afterwards? \n6. Fig 6, small confirmation that the X-axis means \"2x, 4x, ...\" ?\n7. L434: What's the coverage on 0-180 [sort of related to question #2]\n8. JT is worse than FT, why? -- similar confusion or missing justifications for Tables 18/19"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose to fine-tune an LLM into a robot policy by formatting robot decision-making as textual \u201cconversations.\u201d They provide an automated pipeline for converting existing demonstration data into their expected conversational text format. Additionally, they augment that dataset with additional helpful features, such as object bounding boxes extracted from some other vision model / simulation ground truth."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "- LLaRA seems well-motivated, given recent interest in fine-tuning (V)LMs for control\n- It seems to make intuitive sense that, if you\u2019re starting from an instruction-following \u201cconversational\u201d policy, you\u2019d want to fine-tune it to similarly respond to \u201cconversations.\u201d \n  - That said, the extent to which this is the case is unclear: see questions.\n- The authors communicate the format of their data and tasks extremely well, providing extensive examples."
            },
            "weaknesses": {
                "value": "**PLEASE READ MY FOLLOW-UP COMMENT.**\n\n**[IGNORE] This paper is extremely similar to [Embodied Chain-of-Thought Reasoning (ECoT)](https://embodied-cot.github.io/)**, which precedes this work (being accepted at CoRL 2024) and goes uncited in the paper. For reference:\n- **[IGNORE]** ECoT also proposes to format useful features for robot control into a string, then fine-tune a vision-language model into a robot policy that predicts said strings.\n  - The paper does not explicitly state they formulate robot control as \u201cchat,\u201d but as reasoning instead.\n  - However, this seems to be a somewhat superficial distinction; the only difference is the extent to which the \u201creasoning\u201d is dressed up as a conversation. It is unclear if there\u2019s a reason that one format would be better than another.\n- **[IGNORE]** As with this paper, the ECoT authors propose to extract e.g., bounding boxes, which serve as an intermediate feature which the model predicts.\n  - A minor difference is that ECoT always predicts the bounding boxes as part of generating its reasoning and action, whereas LLaRA requires a separate call of the model (if I am understanding Appendix A.3 correctly), which may introduce additional system complexity. In either case, both ECoT and LLaRA can accept detections from other models as well.\n- **[IGNORE]** Both papers use pretrained models to \"supercharge\" robot demo datasets with additional automatically-generated synthetic annotations. \n  - **[IGNORE]** While the reasoning steps employed by ECoT are not identical to the auxiliary datasets in LLaRA, a lot of the ones in the latter seem either simpler than the ones in ECoT (e.g., the future prediction step does not require reasoning by an external model, just formatting of existing data into a template / rephrased variations thereof) or are intractable or not robustly detectable in the real world with careful human annotation (action prediction, temporal prediction both are only easy if you have simulator state information).\n  - In contrast, all experiments in ECoT were done in the real world, and all features and reasoning steps were extracted via pretrained foundation models. While noisy, the authors of that paper nonetheless showed empirical improvement in having the policy predict those features.\n  - Additionally, ECoT demonstrates their pipeline on the Bridge dataset, which is much more representative of existing real-world robot demo datasets (with other datasets in collaborative efforts like Open-X-Embodiment having similar structure). As the BC datasets considered by LLaRA only do 2D pick-and-place high-level actions, it seems unclear if this will be practical for the more complex kinds of actions present in e.g., OXE again.\n\n**Likewise, most of the limitations mentioned in the appendix seem to be addressed by the ECoT paper:**\n> First, some concepts in the reference images still can not be easily and precisely described by language, while in many cases these features are critical for robot control. Due to the single image limit of the current VLM, we still rely on the object detector trained on a limited number of classes to describe the additional images in the task description, which limits the generalization ability of this method.\n\n- **[IGNORE the first part, but the second part is still applicable]** ECoT specifies that they use a VLM scene captioner and open-vocabulary bounding box generator to circumvent this. This does not completely remove the issue that some scenes are just naturally hard to describe in language, but at least removes the limitation of using object detectors trained on a limited number of classes.\n\n> Second, the information extracted from the dataset can be noisy, which may lead to model confusion. In Tab. 5, there are discrepancies such as the location of an object in a reference image differing from its actual location in the current image observation. \n\n- **[IGNORE]** ECoT seems like it would be much less susceptible to noisy labels, since it\u2019s just conditioning on them, rather than relying on them to exactly determine the action (parametrized by pixel coordinates that seem to be precisely dependent on the detected position of objects).\n- **[IGNORE]** All main experiments in ECoT were conducted in the real world, where one must expect there to always be more noise than in simulation. It thus seems like said approach seems robust to the noise.\n- Likewise, the real-world experiments for LLaRA seem very limited. In particular, from the examples shown in Table 13, it seems like all real world tasks can be completed by simply copying the values from the detector:\n  - \u201cRotate the <obj>.at <x, y> by <r> degrees\u201d is correctly solved by the command \u201cPick <obj> at <x, y>, rotate <r>\u201d\n  - \u201cPut <obj1> at <x1, y1> on top of <obj2> at <x2, y2>\u201d is correctly solved by the command \u201cPick <obj1> at <x1, y1>, rotate 0, place at <x2, y2>\u201d\n  - Given that no complex spatial reasoning is needed here, this seems like an insufficient demo of LLaRA\u2019s performance.\n  - It is likewise unclear why LLaRA pretrained in sim doesn\u2019t succeed at this, or why LLaRA trained on real-world data doesn\u2019t get 100% (intuitively the task seems exceedingly easy, since it just involves copying values?)\n\n> Finally, our method relies on 2D to 2D mapping to convert image coordinates into actual actions, which may face challenges when the complex 3D movement of the robot is required. We believe enhancements in the mentioned aspects could further improve performance and broader applicability in complex, real-world applications.\n\n- **[IGNORE]** ECoT states that they adopt the same tokenization scheme as OpenVLA / RT-2, which is significantly more expressive than the pick-and-place actions parametrized by start and end 2D location and rotations.\n- Some of the tasks they test on seem to be very hard to express in the simple 2D-to-2D action parameterization used in LLaRA (e.g., wiping); it thus seems clearly important to demonstrate approaches that work well with such action representations (the \u201cRT-2 style,\u201d as named in the paper), but there is limited evidence that LLaRA could do so, especially since so much of it seems inherently tied to the simplified 2D-to-2D case (e.g., expressing 3D rotations in language is much more unwieldy than the 2D rotation case presented in this paper).\n- Additionally, unless I am misunderstanding, LLaRA seems to only consider cases where tasks can be solved by a handful of 2D pick-and-place actions (with the real world experiments seemingly only to require one per episode). In the execution of that one action, the robot seems to be open-loop (that is, it cannot detect failure over the course of a single pick-and-place action, only before and after -- please correct me if I am misunderstanding). **[IGNORE]** In contrast, since ECoT uses \"lower-level\" actions, it can detect, reason about, and correct mistakes earlier.\n\n**Given all this, it is unclear if LLaRA makes significant contributions over ECoT.** As discussed above, there are certainly subtle differences between the two approaches, but many of those differences seem to either be design choices or in ECoT's favor."
            },
            "questions": {
                "value": "**[All questions should NOT be ignored, they still apply.]** \n- Why does sim LLaRA fail at the real-world tasks, if they seem to be so easy? Why doesn\u2019t fine-tuned LLaRA achieve near 100% success, except in the last row of the FT section of Table 3?\n- Does LLaRA ever experience e.g., failure to grasp, object slipping from gripper, or other similar failure cases in sim, and if so, is it able to recover?\n- Similarly, it looks like in Table 5, LLaRA is trained on text traces that say \u201cyou have completed __.\u201d Does it have to predict this itself, or is it just assumed that, after executing an earlier step, that step was successful, and so it can go in the action history?\n  - If that\u2019s the case, that seems to be another weakness compared to ECoT, as the latter approach has a reasoning step that determines which step of the plan it generated it should try to execute given the current observation. Thus, if it observes a failure case, it can return to and older step to try again\n- A big point of ECoT (and even non-reasoning works that train a VLM into a robot policy, like OpenVLA) is that the VLM provides good visuo-semantic representations that could be useful for robots, especially for generalization. Is this observed by LLaRA, and if so, how / to what extent?\n  - \"In general, LLaRA demonstrates a robust capacity for generalization. Our observations confirm that it takes much less effort and data to transfer LLaRA models pretrained on simulated VIMA data to a real-world domain\"\n- Did formatting things as conversations actually help in some way, rather than just formatting in general (potentially templated) language? It seems like the conversational responses are nonetheless very structured and templated, so it\u2019s unclear to me how much the VLM\u2019s conversational abilities are retained (or even matter for pretraining).\n- More broadly, did formatting things in natural language make a difference? What happens if the policy's response was always just in action tokens, akin to the RT-2-style?\n  - Related: Did RT-2-style have the same expressivity as your approach? Since you're outputting <x, y> coordinates as string representations of floating points, it seems like your approach needs more tokens than what RT-2-style policies are even allowed to output. Since you output the rotation in degrees (from -359 to 359), it seems unfair to give RT-2 only a single rotation token with 256 bins. That seems to artificially make RT-2-style policies less expressive than yours.\n- Similarly, what were the failure cases of RT-2 style? How did it achieve literally 0 across all real-world tests? Likewise, what were the empirical qualitative failures that made it work less well in simulation as well? As it stands, it seems to me that one such reason would be that, by expressing object positions and motions in the same coordinate system, it becomes artificially easier, since the VLM backbone may have a strong bias to being able to just learn to copy the object coordinates as the action parameters.\n- Related: it seems like having prompts like \"Place the object at <coordinates> in the bowl at <coordinates>\" makes things a lot easier (when you've got a good object detector), especially when your actions are specifically parameterized in that coordinate space. What happens when you remove the coordinates in the prompt? This is important to test, since 2D pick-and-place of this form is one of the few robotics settings where that mapping from object coordinates to action coordinates is extremely trivial; most realistic robot learning applications will not be like that."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}