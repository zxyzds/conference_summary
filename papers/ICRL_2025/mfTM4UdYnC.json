{
    "id": "mfTM4UdYnC",
    "title": "LogicJitter: Let LLMs play Logic Games and they will Detect Misinformation",
    "abstract": "In the face of the growing challenge of online information overload, the ability to accurately differentiate between genuine information and misinformation has become increasingly critical both from an individual and societal point of view. Current methodologies for misinformation detection predominantly rely on supervised approaches, which depend heavily on large labeled datasets. However, these datasets are not only costly and time-consuming to produce, but they are also susceptible to issues such as labeling bias, time leakage, the inherent subjectivity of the task, and domain-specific limitations. In this paper, we aim to overcome the aforementioned challenges by proposing a novel and cost-effective strategy to enhance the logical reasoning capabilities of Large Language Models (LLMs), thereby improving their ability to detect misinformation. Our approach, termed LogicJitter, employs a data augmentation technique during fine-tuning that generates both correct and incorrect statements within rule-based logic games. Moreover, these games are designed to counteract well-known human cognitive biases and logical fallacies. Hence, the primary contributions of this work include demonstrating the effectiveness of logical reasoning pre-training on LLMs and providing an open-source PyTorch package for the automatic generation of correct and incorrect logic-based training data.",
    "keywords": [
        "llm",
        "misinformation",
        "rule based AI",
        "toxicity"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=mfTM4UdYnC",
    "pdf_link": "https://openreview.net/pdf?id=mfTM4UdYnC",
    "comments": [
        {
            "summary": {
                "value": "The paper proposes generating synthetic data through logic games, targeted specifically at a large number of cognitive biases / logical fallacies. Then, an LLM is fine-tuned on this data (along with domain data, i.e., misinformation detection data). This improves performance on a misinformation detection dataset. Importantly, the augmenting data here also does not need human labeling, since it is generated from the rule-based logic games, potentially reducing the need for such labeled data that can be expensive and very difficult to obtain in large quantities in this domain."
            },
            "soundness": {
                "value": 1
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "Idea is very cool and could potentially be applied to many domains where reasoning is important. Perhaps even towards mitigating bias and making more fair LLMs.\n\nCovers many common biases/fallacies."
            },
            "weaknesses": {
                "value": "Mainly, the experimental results presented do not sufficiently prove this works. In particular:\n\nThe main evaluation is only done on one dataset, and the paper suggests that the dataset may not be high quality considering fine-tuning on any amount of it is worse than fine-tuning on any of the other datasets considered (Table 3, -100% row) - and by an extremely large margin in the case of VitaminC. This seems odd, and to indicate something might be unusual about this dataset. Why not test with VitaminC as target dataset, considering that one is already implemented? Even more datasets would be better. Testing on one dataset alone would already be cause for hesitation, not to mention when there seems to be something weird about it.\n\nThe PubHealth dataset has imbalanced class distribution, so evaluating with accuracy alone could potentially give misleading results (e.g., if some training procedure biases towards one class rather than actually improving reasoning). It seems important to add some other metrics, e.g., macro F1.\n\nWhat about performance without fine-tuning? That seems an important baseline.\n\nModel (GPT-2) is old and outdated. It seems important to assess if this can still improve modern models. For example, there are Llama-3.2 or Gemma 2 models now with comparable size to GPT-2, not to mention larger ones.\n\nThis could be a great idea. But it really needs more empirical evidence and thoroughness to back it up."
            },
            "questions": {
                "value": "How do you parse answers to get them into a form where they can be compared with the label? Could that be affecting the result in any way?\n\nAt end of intro it says \"pre-training\" but at beginning of methodology it says \"fine-tuning\", bit confusing. Should probably be fine-tuning throughout.\n\nThe style template appears to be incorrect, though it's below page limit by a substantial amount so it seems it would be fine if in the correct template."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes LogicJitter, a data augmentation technique that enhances the logical reasoning abilities of LLMs  for improved misinformation detection. The method generates both correct and incorrect statements within rule-based logic games to counteract human cognitive biases, offering a cost-effective alternative to large labeled datasets."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 1
            },
            "contribution": {
                "value": 1
            },
            "strengths": {
                "value": "1. simple and straight idea"
            },
            "weaknesses": {
                "value": "1. Providing a pytorch and huggingface package cannot be claimed as a research contribution, they are basically the engineering problem.\n2. There\u2019s no technical novelty in this paper, authors simply collect some logical tasks and finetune LLM to enhance misinformation detection. Moreover, the mechanism behind the generalization from logical tasks to misinformation detection task is not well studied.\n3. The GPT-2 small model has only 124M parameter size and cannot be classified as as an LLM. In LLM with billions of parameters, the model has seen tons of facts and reasoning data, I am concerned that the proposed training data paradigm might not generalize to that large LLMs.\n4. The table 3 is not clear, why the left row has a none value, and what does the 47.2\\% and 46.8\\% mean?"
            },
            "questions": {
                "value": "please see the weaknesses above"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 1
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work proposes LogicJitter, an approach to detect misinformation to augment training data and improve LLM reasoning. Experiments on various datasets demonstrate that this approach outperforms various baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ misinformation detection is an important research topic\n+ the connection from logic games to misinformation analysis is interesting"
            },
            "weaknesses": {
                "value": "- the experiments are underwhelming\n- the technical contribution is not clear"
            },
            "questions": {
                "value": "- I wonder if the authors have considered any valid baselines to misinformation detection. There are encoder-based LM-based approaches, LLM-based approaches, graph-based approaches, etc. For now the baseline selection seems largely inadequate.\n\n- I wonder if the authors could better highlight the technical contribution of this approach. To me it seems to mostly involve the generation of synthetic data at training time, which is somewhat trivial.\n\n- The design choices of the synthetic data generation could be better validated by ablation studies.\n\n- Overall the paper only has 8 pages of content, leaving much space for extensive experiments and analysis. I hope the authors could strengthen the experiments to better highlight the strength of this approach.\n\n- The font might be different from the template's default, which might be desk rejection likely."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}