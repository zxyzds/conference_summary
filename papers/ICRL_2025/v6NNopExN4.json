{
    "id": "v6NNopExN4",
    "title": "POST: A Framework for Privacy of Soft-prompt Transfer",
    "abstract": "Prompting has emerged as a dominant learning paradigm for adapting large language models (LLMs). While discrete (textual) prompts prepend tokens to the input for optimized outputs, soft (parameter) prompts are tuned in the embedding space via backpropagation, requiring less engineering effort. However, unlike semantically meaningful discrete prompts, soft prompts are tightly coupled to the LLM they were tuned on, hindering their generalization to other LLMs. This limitation is particularly problematic when efficiency and privacy are concerns, since (1) it requires tuning new prompts for each LLM which, due to the backpropagation, becomes increasingly computationally expensive as LLMs grow in size, and (2) when the LLM is centrally hosted, it requires sharing private data for soft prompt tuning with the LLM provider. To address these concerns, we propose a framework for Privacy Of Soft-prompt Transfer (POST), a novel method that enables private soft prompt tuning on a small language model and then transfers the prompt to the large LLM. Using knowledge distillation, we first derive the small language model directly from the LLM to facilitate prompt transferability. Then, we tune the soft prompt locally, if required with privacy guarantees, e.g., according to differential privacy. Finally, we use a small set of public data to transfer the prompt from the small model to the large LLM without additional privacy leakage. Our experimental results demonstrate that our method effectively transfers soft prompts, protecting local data privacy and reducing the computational complexity over soft prompt tuning on the large model.",
    "keywords": [
        "prompt transfer",
        "soft prompt",
        "privacy",
        "distillation",
        "confidentiality"
    ],
    "primary_area": "alignment, fairness, safety, privacy, and societal considerations",
    "TLDR": "We propose a method on how to transfer soft prompts tuned on a distilled small model to a larger model using public data.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=v6NNopExN4",
    "pdf_link": "https://openreview.net/pdf?id=v6NNopExN4",
    "comments": [
        {
            "summary": {
                "value": "Prompt tuning, the process of optimizing the prefix tokens (hard) or their embeddings (soft) to improve the utility of a downstream task has become one important way of adapting Large Language Models (LLMs). Traditional soft prompt tuning paradigms require backpropagation through the full model, which is computationally expensive to perform on-premise, or handing the private data to a central LLM provider, which could lead to privacy concerns. To reduce the computational overhead of local soft prompt tuning while preserving privacy, the authors propose Privacy Of Soft-prompt Transfer (POST), which performs soft prompt tuning on a small local LLM distilled from a large LLM and transfers the soft prompt to the large model with th help of public data. In addition, the proposed framework can also be coupled with differentially private prompt tuning techniques to achieve privacy guarantees. To validate their proposed method, the authors evaluate the transferability of soft prompts finetuned on small models distilled from Roberta-base, GPT2-XL, and Llama2-7b for classification tasks."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The setting of the paper is well-motivated. The introduction sections and the figures clearly explain the setting and the proposed methods."
            },
            "weaknesses": {
                "value": "1. While the paper presents an appealing middle ground by performing prompt finetuning on a distilled proxy model, the evaluations are performed with a fixed set of proxy models. As briefly mentioned by the authors, in the case of the centralized LLM provider, the utility of the distilled model should ideally be not strong enough to replace the original model but is still good enough to serve as a proxy for prompt finetuning. However, the authors do not further explore the trade-off between the quality of the proxy model and the effectiveness of the proposed pipeline.\n2. The evaluation of the proposed method mainly focuses on classification tasks, which have limited practicality given that modern autoregressive LLMs can perform open-ended generation tasks. While I recognize that prior works also mainly focus on classification tasks, I encourage the authors to discuss the applicability of the proposed method on open-ended tasks, or even better, demonstrate such ability through follow-up experiments."
            },
            "questions": {
                "value": "1. I appreciate the authors discussing the runtime of POST v.s. Full PT in Table 3, but I have some questions regarding the costs of the distillation step. While the distillation costs of a large LLM can be amortized as we finetune more soft prompts with the distilled LLM, could the authors quantitatively compare the computational overhead of finetuning one soft prompt on the large LLM to that of distilling the large LLM and finetuning one soft prompt on the distilled LLM?\n2. The authors use a fixed set of distilled models throughout the paper. How does the quality of the distilled model influence the transferability?\n3. Could other model compression techniques like pruning and quantization be used in this framework instead of model distillation?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The authors propose a framework for Privacy Soft-Prompt transfer to reduce the computation cost and potential privacy leakage. The framework mainly contains three steps as deriving SLM using knowledge distillation, locally prompt tuning, and prompt transferring using public dataset."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The author proposed a novel framework for transferring soft prompts between SLM and LLM, and achieved completive results on the test dataset\n2. The paper brought focus on the privacy and efficiency issue on prompt tuning, and tried to reduce the privacy leakage and computational cost."
            },
            "weaknesses": {
                "value": "1. The paper writing needs to be improved. For example, the author does not clearly represent whether POST needs the original input x during the inference. If so, this may lead to privacy concerns. I mean, in tasks like classification, the LM input contains privacy information.\n2. The paper needs to be clear about the setting. For black-box LMs, they cannot provide the service as giving a SLM through KD. If the author means for white-box LMs, they author should further explain why there is privacy concerns.\n3. For the experiment, I think other tasks in spite of classification help improve the credibility. For tasks as classification, the LM input contains privacy information which violates the setting in this paper. For other tasks as Q&A, the soft prompt may contains privacy information and the model inputs may have less privacy concerns, which is consistent with the setting in this paper."
            },
            "questions": {
                "value": "1. I wonder whether the framework needs the original input during evaluation. If so, the model input may contains privacy information.\n2. For the setting, could you please give further explanation about whether it is white or black box?\n3. Could you please provide more evidence (citations, or experiments) to confirm the claim\u201csoft prompts are highly coupled to the LLM they were tuned on, making them difficult to transfer\u201d?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "In this paper, the authors proposed a new framework that allows the users to perform private soft prompt tuning without accessing the original protected LLMs without leaking their private data. They proposed to  first obtained a distilled model from the original LLM, a step needed from the LLM provider, and then the users will utilize this distilled model to perform soft prompt tuning and transfer the resulting soft prompt to prompt the original LLM. They conducted a few experiments to demonstrate the performance of their proposed framework and compare it with several baselines."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper studies a very interesting problem and proposed a novel framework to allow users to \u201cperform soft prompt tuning\u201d without revealing their private data and LLM providers not revealing their protected LLMs. The most novel part comes from privacy-preserving prompt transfer through public data, which largely protect data privacy and maintain inference performance."
            },
            "weaknesses": {
                "value": "I listed a few weaknesses below:\n1. It is unclear how to select public dataset for prompt transfer, there is no guidance on how to select such a public dataset and how would the selection of public dataset affect the performance. From their experimental results in Table 1, it seems like different selection would make a large enough impact on the inference accuracy.\n2. No explanation on what would be the termination criterion for knowledge distillation step. The authors described that the distilled model should behave closely to the original model but not meeting the actual inference performance of the original model, which is too vague. I do not see either the ablation studies on how to select $\\alpha_{ce}$, $\\alpha_{lm}$ and $\\alpha_{cos}$.\n\nOverall, the main concern is that the proposed method contains many steps involving hyperparameters and public datasets need to be tuned and selected. More ablation studies and further theoretical analysis are needed to understand the general practical performance of the proposed method."
            },
            "questions": {
                "value": "1.\tNo ablation study is found for the selection of different value of $\\alpha$ in Eq. (3).\n2.\tHow to select a public dataset used for prompt transfer? From Table 1, it seems like sometimes selecting a different public dataset affects the final test accuracy a lot.\n3.\tOther than next word prediction task, does the proposed method perform well on other language task? For example, question answering and reading comprehension?\n4.\tWhy there is no result for llama2-7b in Table 2?\n5.\tWhen comparing to the status of the art baselines, it seems like larger models have better zero-shot performance. In this case, in Table 4, can you include more results on larger models like llama2?\n6.\tFrom Fig. 5, it seems like the attack success rate is low even without DP. In this case, would that be the MIA algorithm used in the paper is not good enough? It seems like there is no motivation to add DP to protect private data.  \n7.\tIf the original LLM is mixture of expert model, does the proposed framework apply to that case?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper studies the problem of differentially private soft prompt tranfer. They propose the POST framework which consists of 3 steps: 1) The LLM provider compresses their model into a smaller model using techniques in knowledge distillation and then sends the distilled model to the user. 2) The user performs private prompt tuning using PromptDPSGD (Duan et al., 2024) on the distilled model. 3) The user transfers this prompt on the large LLM."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The related previous work and key concepts (e.g., prompt tuning, differential privacy, knowledge distillation, prompt transfer) are clearly explained. The proposed algorithm is described in detail."
            },
            "weaknesses": {
                "value": "1, Motivation Unclear: This paper focuses on privacy-preserving soft prompt transfer, where the LLM provider is untrusted. Could you provide examples of closed-source LLM providers that support soft prompts? If, instead, the LLM provider is an open-source model that can be run locally (e.g., GPT-2 XL or Llama-2-7b, as mentioned in this paper), privacy concerns could potentially be mitigated by running the model locally. Could you clarify the motivation behind the need for privacy-preserving transfer in this context?\n\n\n\n\n2, Novelty Concerns: Given the existing work for knowledge distillation (Sanh et al., 2019) and PromptDPSGD (Duan et al., 2024), what is the technical novelty in this paper?\n\n\n\n\n3, Practical Concern: The proposed approach involves the LLM provider performing knowledge distillation to compress their LLM into a smaller model and then sending this distilled model to the user. Is this practical? It\u2019s unclear if an LLM provider would be willing to share a distilled model, given proprietary concerns, and risks of potential attacks. \n\n4, Missing DP Guarantee: I recommend including a formal theorem that states the DP guarantee of the proposed method, and a proof to ensure the privacy guarantee is rigorously supported.\n\n\n\n5, Missing Experimental Details: The value of $\\delta$ used in this paper should be mentioned in the main paper. What is the standard deviation of the results in Table 1, 2 and 4? \n\n\n6, Some Typos:\n\n(1) Line 117: fist $\\rightarrow$ first.\n\n(2) Line 119: There\u2019s an extra comma.\n\n\n\n(3) Line 132: from student to teacher $\\rightarrow$ from teacher to student.\n\n\n(4) Line 256: $\\mathcal{N}(0,\\sigma^2 c^2)\\rightarrow \\mathcal{N}(0,\\sigma^2 c^2 \\mathbf{I})$"
            },
            "questions": {
                "value": "1. I don't understand the statement in this paper ``KD has been shown effective to compress LLMs during the pre-training phase while maintaining\ntheir performance (Sanh et al., 2019)\". The cited paper discusses BERT, which is not typically recognized as an LLM. Could you clarify this point? Additionally, can all types of LLMs (e.g., GPT, Llama, Mistral) be effectively compressed using knowledge distillation?\n\n\n\n2. This paper considers two baselines, namely DP-OPT (Hong et al., 2023) and Zero-Shot Transfer (Wu et al., 2023b). Hong et al. (2023) use the TREC, MPQA, and Disaster datasets, and Wu et al. (2023b) consider the LAMA dataset. It is unclear why these datasets are not included in the experiments of this paper. Could the authors also include these datasets in the experiments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}