{
    "id": "lF9QXpfNHm",
    "title": "Efficient Open-world Test Time Adaptation of Vision Language Models",
    "abstract": "In dynamic real-world settings, models must adapt to changing data distributions, a challenge known as Test Time Adaptation (TTA). Open-world classification, where a model must distinguish between known and unknown classes, further complicates TTA. We introduce ROSITA, a novel method for Open World Single Image Test Time Adaptation using Vision-Language Models (VLMs). ROSITA leverages feature banks and a novel contrastive loss to improve the separation of known and unknown classes, enabling efficient adaptation to domain shifts while equipping the model to reject unknown classes. Our approach sets a new benchmark for this problem, validated through extensive experiments across diverse real-world test environments. Our code is anonymously released at \\url{https://github.com/anon-tta/ROSITA.git}",
    "keywords": [
        "Test Time Adaptation",
        "Vision Language Models",
        "Robust learning",
        "Domain Adaptation",
        "Open World learning"
    ],
    "primary_area": "transfer learning, meta learning, and lifelong learning",
    "TLDR": "An effective framework for open-world image classification using Vision Language Models",
    "creation_date": "2024-09-18",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=lF9QXpfNHm",
    "pdf_link": "https://openreview.net/pdf?id=lF9QXpfNHm",
    "comments": [
        {
            "summary": {
                "value": "This paper propose a novel method called ROSITA  for Open World Single Image Test Time Adaptation using VLMs.  \n\nROSITA employs feature banks and an innovative contrastive loss to enhance the distinction between known and unknown classes, allowing efficient adaptation to domain shifts while enabling the model to reject unfamiliar classes.\n\nThe extensive experiments demonstrate the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. The paper is well-written and easy to follow, with a clear motivation.\n2. The experimental results are impressive, and the ablation study is thorough.\n3. The method is novel, interesting, and effective in addressing the challenging problem of Open World Test Time Adaptation."
            },
            "weaknesses": {
                "value": "1. The title of this paper uses '...... vision-language models' , It seems that author only used CLIP for experiments. It would be better to also study the performance with CLIP-large and other VL backbones.\n2. The paper lacks a thorough discussion of the limitations of the current solution and does not provide suggestions for future work.\n3. The author doesn't report the error bar."
            },
            "questions": {
                "value": "Please see the weakness."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The article proposes ROSITA, a framework for test-time adaptation of vision language models in settings where, during inference, the model receives samples of both desired/target classes and undesired ones. The latter should be flagged by the model, as in standard open-set scenarios. ROSITA is based on two components: LDA-based separation of desired and undesired classes (following Li et al. (2023)), and a contrastive objective, updating LayerNorm parameters of the visual encoder. Experiments show that the approach outperforms common TTA approaches in this new, less-explored setting."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Although it is hard to define closed vs. open set in the context of VLMs, the case of test-time adaptation when the test samples comprise both desired and undesired classes is an understudied problem that this paper aims to tackle.\n\n2. The proposed method, ROSITA, is sound, exploiting a simple technique based on LDA to identify desired vs undesired samples (Li et al. (2023)) coupled with a contrastive objective to refine the feature representation. The various design choices are also motivated via ablation studies (e.g., Fig. 1, Tab. 4).\n\n3. The method consistently performs well across various settings and data streams."
            },
            "weaknesses": {
                "value": "1. As first statement for the focus of the work, lines 64-65 states this work adapts VLMs to work with single-image TTA. This sentence is misleading as multiple works already considered TTA with a single image: for instance, TPT (Shu et al. (2022)) and TDA (Karmanov et al. (2024)) are two examples. This sentence should be revised to avoid potential over claims and better contextualize the work, e.g., by focusing the claim on the differences in the specific setting considered and those of existing works on VLMs.\n\n2. While the definition of the setting as open-world follows previous work (i.e., Li et al. (2023)), it is inaccurate as it does not follow its original meaning. Specifically, open-world recognition implies that classes are added overtime to the model, adding semantic to samples of unknown (or undesired, as in this context) classes [a]. This is not the case in this work, as the set of target classes is static and it is not updated. A more precise definition would be following (Lee et al. (2023)) and use \"open-set TTA\", as that denotes the presence of a set of target classes and OOD samples to be recognized. It would be helpful to clarify the particular meaning of open world (comparing it with previous use, e.g., [a]) and/or consider adopting the term \"open-set TTA\" throughout the paper for consistency with existing literature.\n\n2. Currently, the set of baselines do not include any open-set TTA approach, beyond the adaptation of Li et al. (2023). In particular, there exist many potential techniques for detecting unknowns (e.g., MSP [b], max logic [c]) and previous works considered what happens when considering them as alternative OOD detection strategy (e.g., Lee et al. (2023), Tab. 5). As all the experiments and methods are held out with the same, LDA-based selection strategy (Appendix B.4, lines 809) it would be more thorough to include (i) alternatives to LDA's-based OOD detection (ii) simple combination of existing OOD identification algorithms and those for TTA. This would provide a more comprehensive evaluation of ROSITA's performance relative to a broader range of approaches.\n\n4. Technically, the article heavily relies on the TDA scores (proposed already in Li et al. (2023)) to recognize undesired classes. At the same time, there has been already efforts in constructing contrastive-based objectives for TTA [d,e]. Tuning only LayerNorm layers is an already known concept in the literature [f] as well, reducing the impact of Sec. 2.3). In this context, the article merges these two approaches for improved TTA in the \"open-world\" setting but the technical contribution and the differences with what has been already presented should be clarified and potentially analysed in the manuscript. I would be helpful to include a paragraph to explicitly discuss how the approach differs from and improves upon existing contrastive TTA methods and LayerNorm tuning techniques, potentially highlighting the aspects (and challenges) that are especially relevant for open-set/world TTA. \n\n6. TDA, one of the closest baseline, is compared with the proposed approach only on Table 3 and the same goes for the other main baseline, i.e., Li et al. (2023). Ii is not clear what is the criterion behind excluding part of the baselines between tables, especially two main ones which could further strengthen the contribution of the manuscript itself. TDA and Li et al. (2023) should be consistently reported across all relevant experiments, and/or it should be explained why these baselines have been excluded from certain comparisons. This would help readers better understand the relative performance of ROSITA across different scenarios.\n\n**Minors**:\n- From the abstract (i.e., lines 15-17) and the introduction (lines 73-75, 81-84) it is not clear how ROSITA work and what are its technical contribution. Clarifying them would allow the reader to better follow one of the key messages of the manuscript.\n\n- Lines 203-205, the meaning of \"text hypothesis transfer\" is not clear.\n\n- Line 481, typo \"identifierr\" \n\n**Rerefences**:\n\n[a] Bendale, Abhijit, and Terrance Boult. \"Towards open world recognition.\" In CVPR, 2015.\n\n[b] Hendrycks , Dan and Gimpel, Kevin. \"A baseline for detecting misclassified and out-of-distribution examples in neural networks\". In ICLR, 2017.\n\n[c] Sungha Choi, et al. \"Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes.\" ECCV, 2022\n\n[d] Chen, Dian, et al. \"Contrastive test-time adaptation.\" In CVPR, 2022.\n\n[e] Yamashita, Kota, and Kazuhiro Hotta. \"MixStyle-Based Contrastive Test-Time Adaptation: Pathway to Domain Generalization.\" In CVPR, 2024.\n\n[f] Zhao, Bingchen, et al. \"Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning.\" In ICLR, 2024."
            },
            "questions": {
                "value": "1. Is the article the first to perform TTA on single samples and with OOD/undesired classes? If not, clarifying the differences would help to better understand the technical contribution.\n\n2. How does the performance of the model change w.r.t. various design choices? (e.g., different OOD criterion, hyperparameters).\n\n3. Is there a motivation behind the choice of competitors per method? And would it be possible to add other baselines as competitors?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes ROSITA, a new approach for Open World Single Image Test Time Adaptation that utilizes Vision-Language Models (VLMs). This method enhances the differentiation between known and unknown classes through the use of feature banks and a novel contrastive loss function."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "**[New setup]** This work claims that it is the first to study open world single image test time adaptation using VLMs.\n\n**[Extensive experiments]** This work has conducted extensive experiments across a diverse array of domain adaptation benchmarks."
            },
            "weaknesses": {
                "value": "**[Incremental technical contribution]** The main contribution is the reduce loss, a specific contrastive loss, while the class identifier seems a reformulation of previous methods [1, 2] to this task. Given this fact, the technical contribution seems incremental.\n\n[1] Ronald A Fisher. The use of multiple measurements in taxonomic problems. Annals of eugenics, 7 (2):179\u2013188, 1936.\n\n[2] Yushu Li, Xun Xu, Yongyi Su, and Kui Jia. On the robustness of open-world test-time training:\nSelf-training with dynamic prototype expansion. In ICCV, 2023.\n\n**[Unconvincing updating parameter choices]** In section 2.3, the authors test using three different parameter groups for training. However, only these three groups are not comprehensive. Other parameters might be more suitable for updating at test time, e.g., the first layer or the last layer of the encoders. Moreover, additional parameters to keep the intact of the original model are also worth trying, e.g., adapter layers and LoRA.\n\n**[Unclear illustration]** In Figure 1, there might be some questions to be raised: 1) Which dataset is used for experiments? Is it representative enough? If not, better use the results on multiple datasets. 2) Is the learning rate only hyperparameter to tune? What about optimizer type, training epochs and learning rate schedular? 3) Is it possible to update both the prompts and the LN parameters?\n\n**[Missed baseline]** The more recent paper \u201cDiverse Data Augmentation with Diffusions for Effective  Test-time Prompt Tuning, ICCV 2023\u201d is not compared.\n\n**[Experiments]** (i) In Table 1 and 3, the proposed method often outperforms other methods significantly while in Table 2 the performance is close to that of (K+1)PC. Could the authors explain this? (ii) In Table 4, the experimental results with $L_{Re}$ and $L_D$ should be given. (iii) In Table 6, it would be nice to include other methods."
            },
            "questions": {
                "value": "What\u2019s the main difference between CNN based open world TTA and VLMs based one?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}