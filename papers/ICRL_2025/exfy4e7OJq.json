{
    "id": "exfy4e7OJq",
    "title": "Learning Towards Emergence: Paving the Way to Induce Emergence by Inhibiting Monosemantic Neurons on Pre-trained Models",
    "abstract": "Emergence, the phenomenon of a rapid performance increase once the model scale reaches a threshold, has achieved widespread attention recently. The literature has observed that monosemantic neurons in neural networks gradually diminish as the model scale increases. Subsequently, *Learning From Emergence* is proposed to actively inhibit monosemantic neurons in relatively small neural networks (e.g., BERT and Swin-Transformer) for promoting model performance with fine-tuning. However, to ultimately achieve emergence, it is demanding to support the monosemantic neuron inhibition in the pretraining phase of large-scale models. Thus, this work further pushes the boundary of this research direction to be *Learning Towards Emergence (L2E)* and enables the training and validating of the impact of inhibiting monosemantic neurons on larger pre-trained neural networks (e.g., Pythia-70M, 410M, and 2.8B). More specifically, to bridge the gap in current research, we first conduct experiments on models of various scales (up to 6.9B) to validate the monosemantic ideas. Then, we present a novel method L2E to address the inefficient monosemantic neuron retrieval and ineffective monosemantic neuron inhibition when existing methods are applied in the pretraining phase of large-scale models. It employs an adjustable thresholding technique for efficient neuron retrieval, incorporates a False Killing Rate metric to assess inhibition effects, and proposes a regularization-style inhibition approach, which addresses the limitations of previous approaches in both efficiency and effectiveness.",
    "keywords": [
        "Deep Learning",
        "Emergent Abilities",
        "Monosemanticity",
        "Large Language Model"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "",
    "creation_date": "2024-09-20",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-13",
    "forum_link": "https://openreview.net/forum?id=exfy4e7OJq",
    "pdf_link": "https://openreview.net/pdf?id=exfy4e7OJq",
    "comments": [
        {
            "comment": {
                "value": "We sincerely thank all the reviewers for their accurate and valuable comments. We highlight two common questions raised about this work:\n\n1. No experimental emergence, which commonly occurs in models with sizes \u226570B.\n2. Limited to Pythia models only.\n\nAfter working on this topic, we engaged with companies and learned how large models exhibiting emergence are developed: extensive manual, pipeline-style parallel trial and error (incrementally training on multiple small data subsets based on the current checkpoint, selecting the best for the next round). Even if we could support computational resources equivalent to 70B parameter models, without a very large team, it's almost impossible to train a model capable of emergence in a single attempt.\n\nSo we chose Pythia, which provides complete datasets and training data order, thus reducing the gap in resources and manpower during training. This also allows us to demonstrate, based on available computing power: 1) improvements in a widely used model under pre-training settings, and 2) a (possible) full-scale training comparison if more funding becomes available.\n\nThis work thoroughly demonstrates everything we can accomplish with our current resources: effective and necessary efficiency improvements for future (potential) complete pre-training; and validation experiments based on classic monosemantic analysis tools like probing. Many experiments and analyses serve as purely instructional guides\u2014helping users avoid pitfalls and even assisting potential competitors in accomplishing our ideas on emergence (if they have sufficient resources). \n\nWe acknowledge and clearly understand that researching emergence in ordinary research institutions is a fragile dead-end (and that\u2019s why we name the paper \u201ctowards emergence\u201d); we're simply contributing as much as we can before we burn out.\n\n&nbsp;\n\nWe've further added three pages of appendices after submission and now available above, particularly discussing two related directions (grokking and sparsity). More experiments in B.8 address the baseline concerns raised by Reviewers nf4t and fXs7. Hope this will be useful for those who are concerned."
            }
        },
        {
            "summary": {
                "value": "This authors conduct extensive analysis on monosemanticity. They further upgrade previous methods with more carefully determined number of inhibited neurons based on False Killing Rate, a moving threshold for global inhibition with better efficiency, and a regularization term for models in the early pre-training stage. The proposed method surpasses MEmeL and the backbone model in both efficiency and effectiveness."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper is well organized and is easy to follow.\n2. The paper includes a thorough overview of the background and development of prior arts.\n3. The authors provide interesting analysis on monosemantic neurons, and extend the experiments to relatively larger models. The insights can be useful to works in this direction."
            },
            "weaknesses": {
                "value": "1. The experiments are not strong enough to support the claims. \n(a) Since the title suggests this is a study towards \"emergence\", experiments should at least cover commonly used benchmarks (e.g. BIG-Bench, MMLU, etc.), at least the ones used in the original MEmel paper should be included for fair comparison. The motivation of benchmarks should be carefully stated in the paper.\n(b) The proposed methods do not use any assumption from natural language settings, hence they should work for tasks from other domains as well (e.g. vision). Benchmarks from other domains are not necessary but can be very helpful to support the effectiveness of the proposed methods.\n(c) The improvement in performance is somewhat marginal. Given the scale of improvement, it will be more convincing if signifcance tests can be provided.\n(d) The ablation study in the appendix should be moved to Section 5 with more serious discussion.\n\n2. I enjoy reading the analysis part, but unfortunately the technical novelty is rather limited in this work. The three major tricks may be useful, but are somehow heuristic and lack support from more rigorous experiments and ablation study (as stated in the first point). Without such analysis we can hardly determine which part of the ingredient works, and whether it is an ad-hoc improvement by accident or it can be generalized to other scenarios.\n\n3. I find the title \"learning towards emergence\" can be misleading. The previous work \"learning from emergence\" has clear motivation from the relationship between emergence and monosemantic neurons. However, one would expect \"learning towards emergence\" can promote models with relatively small scale to exhibit emergence similar to large models. It is a bit disappointing to see the work is some improvement on existing methods within the same framework."
            },
            "questions": {
                "value": "1. Can this method be generalized to other domains and other benchmarks?\n2. From Figure 6, the optimal percentage seems to be 1%. From Table 4, neither 1% nor 2% achieves consistently better performance (not to mention significance). Is there a more robust way for this percentage selection? \n3. Will the authors add more analysis on the relationship between the proposed method and \"emergence\"?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates the relationship between model scaling and monosemantic neurons, introducing a novel method called Learning Towards Emergence (L2E). The key observation driving this research is that larger language models tend to exhibit fewer monosemantic neurons - neurons that form one-to-one mappings with interpretable features. Building on this insight, the authors develop L2E to actively inhibit monosemantic neurons during the pre-training phase of large models. The method's effectiveness was validated on Pythia models ranging from 70M to 2.8B parameters. Through comprehensive experiments, they demonstrate that inhibiting monosemantic neurons can enhance model performance across multiple downstream tasks. A particularly significant contribution is their efficient implementation using adjustable thresholds, which addresses the computational challenges in scaling to larger models. The authors also introduce the False Killing Rate metric to quantify inhibition effects. While the study is currently limited to the Pythia model family, it provides valuable insights into neural network organization at scale and suggests a potential mechanism for understanding emergence phenomena in large language models. The findings open new avenues for improving pre-training strategies, though further research is needed to fully validate the connection to emergence."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.The study of emergence in large language models is a compelling and significant research direction. Understanding why and how models suddenly exhibit enhanced capabilities beyond simple scaling laws provides crucial insights into the nature of large language models. This kind of research is fundamental for both advancing our theoretical understanding and guiding practical model development.\n\n2.This paper provides fresh perspectives and introduces novel tools for understanding emergence, including the False Killing Rate metric and adjustable thresholding techniques. By linking monosemantic neurons to model performance, it offers a new angle for analyzing how neural networks organize and process information as they scale up, while also providing practical methods to enhance model capabilities.\n\n3.The experimental results are comprehensive and convincing, demonstrating consistent performance improvements across various model scales (70M to 2.8B) and downstream tasks. The proposed L2E method not only shows effectiveness in improving model performance but also addresses efficiency challenges in large-scale implementation, making it practically viable for real-world applications."
            },
            "weaknesses": {
                "value": "1.The paper's experimental scope falls significantly short of the scale where emergence typically occurs. Current observations suggest that dramatic emergence phenomena primarily manifest in models around 70B parameters or larger, where there's a clear performance gap compared to smaller models. By limiting their analysis to models up to 6.9B parameters, the study misses the most critical scale range where emergence actually happens. This substantially weakens their conclusions about emergence and raises questions about the practical applicability of their findings to truly large-scale models.\n\n2.The research is exclusively conducted on the Pythia model family, and the results are heavily dependent on the quality of feature datasets. For a study claiming to understand fundamental properties of large language models, this narrow focus is concerning. A more comprehensive analysis should include widely-used open-source models like Llama, BLOOM, or Falcon, which have different architectures and training approaches. This would help validate whether their findings about monosemantic neurons are truly general properties or just specific to Pythia models.\n\n3.Without analyzing mainstream models, the paper's conclusions remain largely theoretical and potentially disconnected from practical reality. While acknowledging the challenges of training large-scale models, the authors could have conducted analysis and interpretation on existing pre-trained models. This would have provided valuable validation of their hypotheses and strengthened their arguments about the relationship between monosemantic neurons and emergence. The lack of such analysis leaves a significant gap between their theoretical framework and real-world applications."
            },
            "questions": {
                "value": "1. How do you define and measure emergence more precisely? The paper suggests a connection between monosemantic neurons and emergence, but is this correlation or causation?\n\n2. What is the theoretical justification for why reducing monosemantic neurons would lead to better model performance?\n\n3. Why choose 2% as the optimal threshold for neuron inhibition? Could this vary with model size or architecture?\n\n4. How robust is the False Killing Rate metric across different types of models and tasks?\n\n5. Could the efficiency improvements in L2E potentially sacrifice some accuracy in identifying monosemantic neurons?\n\n6. Given that emergence typically occurs in models around 70B parameters, how relevant are the findings from much smaller models (up to 6.9B)?\n\n7. Why focus exclusively on the Pythia model family? How would the results generalize to other architectures?\n\n8. What specific characteristics of the feature datasets might affect the reliability of the results?\n\n9. Could this method be applied to existing pre-trained models without retraining?\n\n10. How would this approach scale to truly large models where emergence is actually observed?\n\n11. What are the computational costs and practical challenges of implementing L2E in production environments?\n\n12. How might this work extend to understanding other emergent properties in large language models?\n\n13. Could this approach be adapted for other types of neural networks beyond language models?\n\n14. What additional metrics or methods could help validate the connection between monosemantic neurons and model performance?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The manuscript presents an approach termed Learning Towards Emergence (L2E), which aims to enhance the performance of large pre-trained neural networks by inhibiting monosemantic neurons. The authors argue that as model scale increases, the presence of monosemantic neurons diminishes, potentially contributing to the phenomenon of emergence. L2E employs the FKR, an efficient retrieval method, and proposes a regularization-style inhibition approach. The experiments demonstrate the effectiveness of L2E in improving model performance during pre-training."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "* The introduction and the related works are clear.\n* The motivation is straightforward that previous methods for inhibiting monosemantic neurons lack efficiency and experimental evidence."
            },
            "weaknesses": {
                "value": "* The study is limited to Pythia models, and it is unclear how well L2E would perform on other architectures or datasets.\n* Table 1 lacks the comparison results of related works, such as MEmeL.\n* While the introduction highlights the potential of suppressing monosemantic neurons to promote emergence, the experiments presented in this paper fall short of validating this hypothesis."
            },
            "questions": {
                "value": "1. The y-axes in Table 1(a) and 1(b) are inconsistent. It is unclear why the minimum value in Table 1(a) is 2 rather than -2.\n2. The authors claim that the neurons in Table 1(b) are polysemantic, yet the data suggests that they are inactive to all inputs."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}