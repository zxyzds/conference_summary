{
    "id": "Lf5znhZmFu",
    "title": "Computational Limits of Low-Rank Adaptation for Transformer Models",
    "abstract": "We study the computational limits of Low-Rank Adaptation (LoRA) for finetuning transformer-based models using fine-grained complexity theory.\nOur key observation is that the existence of low-rank decompositions within the gradient computation of LoRA adaptation leads to possible algorithmic speedup.\nThis allows us to (i) identify a phase transition behavior of efficiency assuming the Strong Exponential Time Hypothesis (SETH), and (ii) prove the existence of nearly linear algorithms by controlling the LoRA update computation term by term. \nFor the former, we identify a sharp transition in the efficiency of all possible rank-$r$ LoRA update algorithms for transformers, based on specific norms resulting from the multiplications of the input sequence $X$, pretrained weights ${W^\\star}$, and adapter matrices $\\alpha BA/r$.\nSpecifically, we derive a shared upper bound threshold for such norms, and show that efficient (sub-quadratic) approximation algorithms of LoRA exist only below this threshold. \nFor the latter, we prove the existence of nearly linear approximation algorithms for LoRA adaptation by utilizing the hierarchical low-rank structures of LoRA gradients and approximating the gradients with a series of chained low-rank approximations. \nTo showcase our theory, we consider two practical scenarios: partial (e.g., only $W_V$ and $W_Q$) and full adaptations (e.g., $W_Q$, $W_V$, and $W_K$) of weights in attention heads.",
    "keywords": [
        "Parameter Efficient Finetuning",
        "Low-Rank Adaptation",
        "LoRA",
        "Transformer",
        "Foundation Models",
        "Large Language Models",
        "Fine-Grained Complexity",
        "Strong Exponential Time Hypothesis"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "We explore computational hardness of LoRA for transformers, showing a adaptor-weight-norm-based phase transitions in efficiency and the existence of nearly linear algorithms under strong exponential time hypothesis.",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-11-13",
    "forum_link": "https://openreview.net/forum?id=Lf5znhZmFu",
    "pdf_link": "https://openreview.net/pdf?id=Lf5znhZmFu",
    "comments": [
        {
            "title": {
                "value": "Global Response"
            },
            "comment": {
                "value": "Dear Reviewers,\n\nWe thank the reviewers for the insightful questions and reviews. We have answered all the questions and addressed all the problems in detail in rebuttal and revision.\n\nThe latest revision is readily available. Any changes or modifications made from the submitted version are highlighted in blue.\n\nIn response to the reviewers' suggestions, we have made revisions to enhance the overall readability of the paper. We conducted three additional rounds of proofreading to correct typos, and several sections, paragraphs, and statements have been updated for clarity and completeness. These updates include added explanations and informal versions of theoretical results to help readers build intuition. **Most importantly, we have added a new Section 6 on page 10, which provides numerical experiments to support our theoretical claims.**\nThe reproducible code has been uploaded to the supplementary materials.\n\n---\n## **Revision Details** (please also see the revised PDF)\n\n### Major revisions include:\n\n* **New Experimental Section:** [`Yw3J`, `FVCG`, `wZk2`, `D3KJ`]\n  - **Aim: Control the norms of attention head weights in LoRA fine-tuning 3 OPT models to achieve computational speedup:**\n  - **Model Setup**:\n    - Used Open Pretrained Transformer (OPT) models [1]: OPT-125M, OPT-350M, and OPT-1.3B.\n    - Compared two architectures: standard transformers [2] and outlier-free transformers [3].\n  - **Fine-Tuning Task**:\n    - Cross-modality fine-tuning of OPT models on speech and text data, creating a SpeechLM model.\n  - **LoRA Setup**:\n    - Rank $ r = 128 $, alpha value $ \\alpha = 256 $, fine-tuned with the LibriLight dataset [4] (12 million utterances from 60,000 speakers).\n  - **Efficiency Results**:\n    - Outlier-Free Transformers achieved significant speedups:\n      - **OPT-125M**: 5.5% faster\n      - **OPT-350M**: 13.1% faster\n      - **OPT-1.3B**: 33.3% faster\n  - **Conclusion**: Proper normalization of weights and inputs enhances LoRA training efficiency, with greater computational gains observed in larger models.\n\n### Minor revisions include:\n\n* Proofreading the manuscript and fixing all identified typos and grammatical errors by reviewers and authors.  [`FVCG`]\n\n* Add informal versions of our main results and combine them with highlevel discussions in the Intro section.  [`FVCG`]\n\n* Add intuitive, descriptive names to all Lemma, Def. [`FVCG`]\n\n* Highlight the practical implications of our theory in the Intro Sec.  [`Yw3J`, `FVCG`, `wZk2`, `D3KJ`]\n\n* Move the statement of SETH from Sec 2 (preliminary) to Sec4 as it's only used there  [`wZk2`, `D3KJ`]\n\n---\n\nWe hope these revisions address the reviewers' concerns and improve the overall quality of our paper.\n\nThank you again for your review!\n\n---\n\n[1] Zhang et al. \"Opt: Open pre-trained transformer language models.\" arXiv preprint arXiv:2205.01068 (2022).\n\n[2] Vaswani et al. \"Attention is all you need.\" NeurIPS 2017.\n\n[3] Hu et al. \"Outlier-efficient hopfield layers for large transformer-based models.\" ICML 2024\n\n[4] Kahn, et al. \"Libri-light: A benchmark for asr with limited or no supervision.\" ICASSP 2020"
            }
        },
        {
            "title": {
                "value": "Response"
            },
            "comment": {
                "value": "### Thank you for your detailed review. We have addressed all your comments and questions in this and the following responses. We have also modified the draft accordingly and marked the changes in blue in the latest revision. Please see the updated PDF for details.\n--\n\nDear Reviewer Yw3J,\n\nThank you for your feedback and the time you invested in reviewing our work.\n\nIn response to your request for numerical validation, we have added a new section of proof-of-concept experiments in Section 6 of the latest revision. These experiments involve LoRA fine-tuning on three different sizes of OPT models, and the numerical results align well with our theoretical predictions\u2014specifically, that controlling the suggested norm bounds enhances LoRA efficiency.\n\nRegarding the generalization of LoRA, we believe our results impose certain constraints on generalization theory in two key aspects:\n\n1. Since our results are **precision-guaranteed**, many of their implications may be transferable to generalization analysis.\n2. Generalization analysis sometimes depends on different optimizers, which may lead to varied gradient update patterns. It is important to clarify whether these patterns fall within or outside the scope defined by our Problem 1. If they do, our results may also be applicable to those analyses.\n\nWe acknowledge that more rigorous arguments for these generalization aspects are beyond the scope of this work and leave them for future research.\n\nLastly, to scale up our analysis beyond a single layer, we need to handle additional gradient chain-rule terms arising from deeper layers. This is an ongoing project of ours. The main technical challenge lies in managing the complex gradients produced when backpropagating through these layers. New techniques are required to address this complexity.\n\nWe hope these responses have addressed your questions. Thank you again for your review!"
            }
        },
        {
            "title": {
                "value": "Response 2"
            },
            "comment": {
                "value": "> `Q2. `Why do you put the analysis of full LoRA in the appendix instead of in the main body?\n\nThanks for the question. It\u2019s mainly for pedagogical purposes.\n\n1. The special case analyzed in Section 3 is easier to analyze because it involves fewer chain-rule terms. **While easier to handle, the proof structure can still be easily generalized to the more complex full LoRA case discussed in the appendix.** Readers only need to understand the simpler case to appreciate the techniques we have presented.\n\n2. The special case analyzed in Section 3 is a common choice suggested by the original LoRA paper and is representative for study.\n\n\n> `Q3.` What are SAT algorithms in line 142?\n\nThanks for the question. k-SAT Algorithms are algorithms solving k-SAT problems. k-SAT is NP-complete for $k \\geq 3$ and is a representative hard problem in Theoretical Computer Science community.\n\nSpecifically. k-SAT (k-Satisfiability) problems involve determining if there is an assignment of boolean variables that satisfies a Boolean formula in conjunctive normal form, where each clause has exactly $k$ literals. k-SAT is NP-complete for $k \\geq 3$.\n\nThe significance of k-SAT lies in its role within the Strong Exponential Time Hypothesis (SETH). SETH posits that no algorithm can solve k-SAT in time $O(2^{(1-\\varepsilon)n})$ for any $\\varepsilon > 0$ and large $k$. This connection is important because:\n\n* Complexity Bounds: SETH provides strong lower bounds for k-SAT time complexity, influencing assumptions for many other problems.\n\n* Reductions: SETH-based hardness results often use k-SAT reductions to show that problems cannot be solved faster.\n\n* Algorithmic Impact: Improved k-SAT algorithms could challenge SETH, impacting our understanding of computational complexity.\n\nThus, it is common in the literature to discuss SETH alongside k-SAT problems. \n\nAdditionally, as reviewer wZk2 suggested, placing SETH in the background section was indeed awkward since it is only used in Section 4. We have moved it there and updated the draft accordingly.\n\n> `Q4.` Is it $\\frac{\\partial L}{\\partial A_\\mu}$ rather than $\\frac{\\partial L}{\\partial A_Q}$ in line 97?\n\nYes, you are absolutely correct. Thank you for your careful proofreading. We have updated the draft accordingly.\n\n---\n\nThank you for your time and valuable feedback. Please do not hesitate to let us know if there are any other aspects of our work that you would like us to clarify."
            }
        },
        {
            "title": {
                "value": "Response 1"
            },
            "comment": {
                "value": "### Thank you for your detailed review. We have addressed all your comments and questions in this and the following responses. We have also modified the draft accordingly and marked the changes in **blue** in the latest revision. Please see the updated PDF for details.\n\n---\n\n> `W1.` There are two notations that may not be necessary. I suggest considering whether it is possible to remove or simplify some definitions to derive all the lemmas and theorems in the main body.\n\nWe believe you may have meant \u201ctoo many notations\u201d rather than \u201ctwo notations.\u201d If that is the case, here are some clarifications.\n\n1. We assure the reviewer that, **all definitions and technical lemmas in the main text are placed with careful consideration.** The main reason is **to be self-contained in the main text.** To maintain mathematical rigor, every concept or term in our paper is first clearly defined before it is used.\n\n    Here we further provide the detailed dependencies among Definitions, Lemmas, and Theorems for the reviewer's reference:\n\n    - $\\text{Def 2.1, 2.2, 2.3, 2.4}$ are essential for the entire paper.\n    - $\\text{Def 3.2, 3.3, 3.4, 3.5, 3.6}$ are essential for $\\text{Lemma 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7}$.\n    - $\\text{Lemma 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7}$ are essential for $\\text{Thm 3.1}$.\n    - $\\text{Def 3.2\u20133.8}$ are essential for $\\text{Lemma C.1\u2013C.9}$.\n    - $\\text{Lemma C.1\u2013C.9}$ are essential for $\\text{Thm C.1 (4.1)}$.\n\n\n2. To improve clarity, readability and accessibility, we have made the follow modifications:\n    - Add informal versions of our main results and combine them with highlevel discussions in the Intro section. `line114-125`\n    - Add intuitive, descriptive names to all Lemma, Def.\n    - Highlight the practical implications of our theory in the Intro Sec. `line130-131`\n    - Add a new section of numerical validations (fune-tuning 3 OPT models with LoRA with & without norm bound control). Sec 6\n\nWe have modified the draft accordingly. Please see the latest revision for details. \n\n\n> `W2.` The practical insights of this paper are not very clear. It is better to highlight the practical significance of the theoretical analysis in the paper.\n\nThanks for the comment. We would like to point out that **the practical implications of our results are discussed in the concluding remarks (Remark 5.2 of the submitted draft).** Specifically, we outlined some possible or existing design choices justified by our theory. \n\nWe understand that these discussions may not have been apparent or persuasive. In response, **we have expanded Remark 5.2 (\u201cInsights for Practitioners\u201d) into a new section of numerical experiments in Section 6 of the latest revision.**\n\n**Our numerical findings indicate that proper normalization of weights and inputs improves LoRA training efficiency**. Specifically, we show that Outlier-Free Transformers (as mentioned in Remark 5.2) significantly enhance LoRA training efficiency across various model sizes (OPT125M, OPT350M, and OPT1.3B) when fine-tuned with cross-modality data from LibriLight.\n\nBy controlling the norms of attention head weights, Outlier-Free Transformers achieve notable speedups: 5.5% for OPT125M, 13.1% for OPT350M, and 33.3% for OPT1.3B. **These results strongly support our theoretical claims, particularly for larger models.**\n\nPlease see Figure 1 and Table 1 of latest revision for details.\n\n\n> `Q1.` In lemmas 3.3, 3.4, and 3.5, the amount of time needed to construct the matrices is included. What is the algorithm used for construction here? Why is discussing the required amount of time meaningful since in practice these parameters are learned by gradient methods instead of any construction?\n\nThanks for the questions. \n\n* For your first question: The construction algorithm used in Lemma 3.3 is a straightforward application of Algorithm 1 from [A]. This algorithm approximates the attention matrix in almost linear time by constructing low-rank approximations. Consequently, the low-rank approximators (U_1\u200b and V_1\u200b in Lemma 3.3) are constructed efficiently, in almost linear time.\n\n* For your second question: The construction algorithms presented in Lemmas 3.3, 3.4, and 3.5 are designed to approximate a portion of the gradient matrix in almost linear time. It's important to note that in Lemmas 3.3 through 3.7, no parameters are learned. Instead, these lemmas demonstrate that it is possible to approximate one step of gradient descent in almost linear time.\n\n[A] Josh Alman and Zhao Song. Fast attention requires bounded entries.  NeurIPS, 2023."
            }
        },
        {
            "title": {
                "value": "Response 2"
            },
            "comment": {
                "value": "> `W5` An alternative approach might involve updating the feed-forward network (FFN) layer rather than the attention block, for example [1][2]. Could this adjustment also avoid/alleviate the $O(L^2)$ computational complexity issue in the paper?\n\n\n\nThat is a valid point. However, we want to clarify that exploring such an approach is beyond the scope of this work. By not adapting the attention heads, the method would no longer address Problem 1, as our focus is specifically on fine-tuning transformers using LoRA.\n\n   [1] AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition.  \n   [2] Parameter-Efficient Fine-Tuning with Controls.\n\n\n> `W6` The current title may not fully align with the paper\u2019s objectives, as it does not primarily address computational drawbacks but rather focuses on solving these computational limitations.\n\nAs discussed in the introduction, the objective of this paper is to formally study the limits of LoRA on transformer models in two aspects:\n- When can it be efficient?\n- How efficient can it be?\n\nWe believe these research questions justify our title, as they directly pertain to LoRA's computational efficiency. Additionally, we also discuss the computational drawbacks of LoRA on transformers in Appendix F for unfamiliar readers.\n\n---\n\nThank you again for your time and efforts. We hope that the revisions and clarifications provided in this response address the reviewer's concerns and make the value of our work clear. We look forward to further feedback and discussion."
            }
        },
        {
            "title": {
                "value": "Response 1"
            },
            "comment": {
                "value": "### Thank you for your detailed review. We have addressed all your comments and questions in this and the following responses. We have also modified the draft accordingly and marked the changes in **blue** in the latest revision. Please see the updated PDF for details.\n\n---\n\n> `W1.` Could the authors clarify why Equation 1.2 holds? The expression on the right-hand side appears to minimize the discrepancy between the attention output and the labels $Y$. Do we only consider 1-layer attention here?\n\nThanks for the question. As we explained in `line 46`, the attention layer is the computational bottleneck of a transformer block in both the forward and backward pass. Other components contribute only trivially to the quadratic complexity induced by the attention layer. Therefore, it suffices to focus solely on the abstract subroutine in Equation 1.2.\n\n> `W2` The Strong Exponential Time Hypothesis currently seems to serve only as a counterexample in the context of gradient approximation. Its relevance to the subsequent analysis is unclear in its present form. The reviewer suggests incorporating a more precise and directly relevant statement to clarify its role in the argument.\n\nWe believe there might be a misunderstanding. SETH is not a counterexample; it is a crucial hypothesis for the results in Section 4. Specifically, we use SETH to facilitate fine-grained reductions needed to establish the inefficiency threshold in Theorem 4.1.\n\nWe recognize that its placement in the background section may have seemed awkward, given that it is only used in Section 4. In response, we have moved the discussion of SETH to Section 4 and updated the draft accordingly.\n\n> `W3` While purely theoretical contributions may not always necessitate empirical validation, this paper's objective\u2014improving the efficiency of optimizing large language models (LLMs) with LoRA\u2014suggests that experimental results are essential for substantiating its claims. Specifically, an empirical evaluation could verify whether the bounded gradient approximation indeed holds in practice, as this is critical for the practical applicability of the proposed methods.\n\nThanks for the suggestion.\n\nIn the latest revision, we have included a section of proof-of-concept experiments. These experiments are extended from the discussion in our concluding remarks (Remark 5.2). Please refer to Sec 6 of the updated draft.\n\n**Our numerical results show that proper normalization of weights and inputs enhances LoRA training efficiency.** Specifically, we demonstrate that Outlier-Free Transformers [1] (as noted in Remark 5.2 of the submitted draft) improve LoRA training efficiency across various model sizes (OPT125M, OPT350M, and OPT1.3B) when fine-tuned with cross-modality data from LibriLight. \n\nBy controlling the norms of attention head weights, **the Outlier-Free Transformers achieve significant speedups: 5.5% for OPT125M, 13.1% for OPT350M, and 33.3% for OPT1.3B.** These results support our theoretical claims that proper weight and input normalization significantly enhance training efficiency, particularly for larger models.\nWe present Figure 1 and Table 1 to illustrate these results.\n\n[1] Hu et al. \"Outlier-efficient hopfield layers for large transformer-based models.\" ICML 2024\n\n> `W4` The authors assert that \"the existence of low-rank decompositions leads to potential algorithmic speedup.\" Could full parameter updating also yield similar benefits? Additionally, how is the speedup related to the rank $r$?\n\nThanks for the question. To clarify, full-parameter updates are a trivial case in our analysis. They are trivial because they do not require handling additional chain-rule terms arising from the multiplication of $AB$ matrices. We remind the reviewer that our paper specifically addresses this technical challenge (as discussed in the introduction, lines 66-73). In the full update case, the overall analysis becomes significantly simplified.\n\nRegarding the rank $r$, the rank $r$ rescales the norms of the $C$ matrices. Since this scaling does not depend on the length $L$, we omit it from the main results. However, it can be explicitly recovered from Theorem 3.1: for a fixed $\\Gamma$, a larger $r$ results in a looser norm bound condition on $X$. This implies that a higher LoRA rank requires less stringent input or weight normalization."
            }
        },
        {
            "title": {
                "value": "Response 2"
            },
            "comment": {
                "value": "> `W3.` In addition, a few of the lemmas lack clear statements, which can lead to confusion. For instance, in Lemma 1.1, the term (L) is introduced without any accompanying explanation. It would be helpful to review the formulations to ensure clarity, even if the technical details are correct. Improving the precision in how terms are introduced will aid readers in following the logical flow more naturally, without having to reread sections to understand each step.\n\n**Response:** Sorry for any confusion caused. The term $L$ is introduced in `line 51` of the submitted draft as sequence length of input $X$. We understand that we might not state this clearly enough. We rephrase it into a more precise manner (``line51` of latest revision):\n> Let $X$ be input of length $L$.\n\nIn response to **general clarity**, we have made revisions to enhance readability. Please see above **Response 1** and revised draft PDF for details.\n\n> `W4.` Given the extensive number of definitions, it would be beneficial to assign descriptive names to them. Naming each definition provides a quick reference, helping readers keep track of terms and concepts as they reappear later in the paper. Otherwise, it\u2019s easy to lose track of which definition corresponds to which concept, especially in a highly technical document.\n\n**Response:** Thanks for the suggestion. We absolutely agree. \n\nWe have added descriptive names to Def. 3.2-3.6 and Lemma 3.2.\nPlease see the latest revision for details.\n\n> `W5.` Sections 3 and 4, which seem intended to present the main results, read more like collections of lemmas, theorems, and technical details without adequate discussion or contextualization. This is a significant area for improvement. The results would be far more impactful if they were accompanied by clear explanations and discussions. Theoretical contributions are valuable only if they can be understood and appreciated, and in their current form, the key insights may be difficult for readers to discern.\n\n**Response:**  Thanks for the suggestion. We agree that the current Sec 3 and 4 are structured a bit technical.\n\nIn response, **we add informal versions of our main results and combine with relevant discussions in the introduction section.** Please see the latest revision for details.\n\nIn response to **practical impact**, we believe there may be a slight oversight. **Our concluding remarks already include in-depth discussions on several practical implications** of our main results, such as:\n- Insights for practitioners\n- Applicability to both self- and cross-attention transformers\n\nTo strengthen these implications, **we have expanded the discussion in Remark 5.2 (Insights for Practitioners) and added a new experimental section.** \n\nPlease refer to Section 6 in the latest revision.\n\n> `W6.` Finally, the paper would benefit from including plots, diagrams, or simple illustrations that clarify the results. While extensive experimental results may not be expected in a theoretical paper, even a few toy examples or visual aids could significantly enhance reader comprehension and provide concrete illustrations of the theoretical findings.\n\n**Response:** In the latest revision, we have included a section of proof-of-concept experiments. We summarize these numerical results in below.  Please refer to Sec 6 of the updated draft for details.\n\n**Our numerical results show that proper normalization of weights and inputs enhances LoRA training efficiency.**\n\nSpecifically, we demonstrate that Outlier-Free Transformers (as noted in Remark 5.2 of the submitted draft) improve LoRA training efficiency across various model sizes (OPT125M, OPT350M, and OPT1.3B) when fine-tuned with cross-modality data from LibriLight. \n\n**By controlling the norms of attention head weights, the Outlier-Free Transformers achieve significant speedups: 5.5% for OPT125M, 13.1% for OPT350M, and 33.3% for OPT1.3B.**\n\nThese results support our theoretical claims that proper weight and input normalization significantly enhance training efficiency, particularly for larger models.\n\nWe present Figure 1 and Table 1 to illustrate these results.\n\n---\n\n> `Q1.` Could you please provide additional explanation of the theoretical results?\n\n**Response:** As responded above, **please see the latest revision for informal versions of our main results**, accompanied by high-level discussions.\n\n\n> `Q2.` Additionally, would it be possible to include some toy or controlled experiments, or other illustrations to help clarify the findings?\n\n**Response:** As responded above, **please see the newly added Sec 6 for numerical experiments.** Our numerical findings align with our theory.\n\n---\nWe sincerely appreciate the time and effort that Reviewer FVCG has invested in reviewing our paper. We have taken all comments into careful consideration and have made corresponding revisions to address the concerns raised.\n\nWe're open to any further questions or clarifications you might have about our work."
            }
        },
        {
            "title": {
                "value": "Response 1"
            },
            "comment": {
                "value": "### Thank you for your detailed review. We have addressed all your comments and questions in this and the following responses. We have also modified the draft accordingly and marked the changes in **blue** in the latest revision. Please see the updated PDF for details.\n\n---\n\n> `W1.` This paper currently feels dense and challenging to navigate, as it primarily consists of a series of definitions, lemmas, and theorems, often presented without sufficient explanation, clarification, or intuitive context. For readers who are not already experts in this area, this can make it difficult to grasp the key concepts and results.\n\n**Response:** Thanks for the comment. Here are some clarifications.\n\n- **Why the sequence of Definitions & Lemmas (especially in Sec3)?**\n\n  Our main contribution is showing that LoRA gradient (3.5) can be computed in sub-quadratic time with low-rank approximation.\n\n  We do this by (stated in `line311: Remark 3.1` and `line336: Overview of Our Proof Strategy` of the submitted draft):\n  \n  1. Decomposing LoRA gradient (3.5) into carefully designed terms (Lemma 3.2).\n  2. Showing all these terms can be approximated in almost linear time with precision guarantees (Lemmas 3.3, 3.4, 3.5, 3.6, 3.7).\n  3. Most importantly, **our well-designed decomposition allows us to chain/combine/connect all low-rank approximations (of these terms) into one.** Hence, we obtained an almost linear time approximation of (3.5) with a precision guarantee.\n\n  While the process seems lengthy, we deem the inclusion of Lemma 3.2-3.7 necessary to ensure all terms can be approximated in almost linear time with a precision guarantee. To be precise, **they are necessary to keep the main text self-contained and mathematically rigorous.**\n\nYet, we agree that many intermediate results are overly formal and could benefit from intuitive explanations. \n\nIn response, **we had added informal versions of our main results and combine them into the intuitive discussions in the intro section.**\n\nWe have modified the draft accordingly. Please see the latest revision for details. \n\n> `W2.` There are several opportunities to improve accessibility and readability. Some of the definitions would be more appropriately placed in an appendix, as the main text is quite heavy with formal definitions, lemmas, and theorems. Moving certain definitions to an appendix could help streamline the main text, allowing readers to focus on the core arguments without getting overwhelmed by technical details.\n\n**Response:** We apologize for any confusion caused. Yet, we assure the reviewer that, all definitions and technical lemmas in the main text are placed with careful consideration. The main reason is to be self-contained in the main text. To maintain mathematical rigor, every concept or term in our paper is first clearly defined before it is used.\n\nFor example, **the definitions of vectorization and matrixization are necessary for mathematical correctness.** Otherwise, the LoRA gradient norm bound is ambiguous at best ($\\tilde{G}$ are matrices while $\\partial \\mathcal{L} / \\partial A_Q$ does not have a straightforward matrix expression). For this reason, we believe that the elements included in the current draft are minimally sufficient to ensure the paper is consistent and self-contained.\n\nTo be precise, here we further provide the dependencies among Definitions, Lemmas, and Theorems for the reviewer's reference:\n\n- $\\text{Def 2.1, 2.2, 2.3, 2.4}$ are essential for the entire paper.\n- $\\text{Def 3.2, 3.3, 3.4, 3.5, 3.6}$ are essential for $\\text{Lemma 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7}$.\n- $\\text{Lemma 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7}$ are essential for $\\text{Thm 3.1}$.\n- $\\text{Def 3.2\u20133.8}$ are essential for $\\text{Lemma C.1\u2013C.9}$.\n- $\\text{Lemma D.1\u2013D.9}$ are essential for $\\text{Thm D.1 (A.1)}$.\n- $\\text{Hypothesis 1}$ is essential for $\\text{Thm 4.1}$.\n\nIn response to **accessibility and readability**, we had made the following modifications in the latest revision:\n\n- Conduct 3 more rounds of proofread and fix all typos identified by reviewers and authors.\n- Add intuitive, descriptive names to all Lemma, Def.\n- Add informal versions of our main results in the intro Sec, combined with intuitive discussions. `line114-125`\n- Highlight the practical implications of our theory in the Intro Sec.  `line130-131`\n- Add a **new section of numerical validations** (fune-tuning 3 OPT models with LoRA with & without norm bound control). `Sec 6`\n\nWe hope these addresses your concerns."
            }
        },
        {
            "summary": {
                "value": "This paper studies how to make large Transformer models more computationally efficient during fine-tuning. The main contributions are:\n* Identifying a critical point of efficiency where models can be fine-tuned with less computation if they are below this threshold.\n* Proposing a new method that can complete model fine-tuning in almost linear time, which is much faster than traditional methods."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "The paper introduces a novel theoretical analysis on Low-Rank Adaptation (LoRA) for Transformer models, marking an innovative contribution to the fields of natural language processing and machine learning. It approaches the problem of LoRA adaptation from a fresh perspective, focusing on computational limits and efficiency, which is particularly novel in the context of large foundation models. Additionally, the paper presents an innovative method by proposing an almost linear-time algorithm for LoRA adaptation, which is a significant advancement over existing methods that typically have quadratic complexity."
            },
            "weaknesses": {
                "value": "* The paper's primary focus seems to be on theoretical analysis. To strengthen the claims, experimental validation with real-world datasets would be beneficial. Specifically, demonstrating the practical efficiency of the proposed algorithms on standard benchmarks could provide actionable insights into their performance.\n* It would be valuable to see how the proposed methods compare to current state-of-the-art techniques in terms of both efficiency and accuracy. This comparison could highlight the advantages and potential limitations of the new algorithms.models and datasets?"
            },
            "questions": {
                "value": "* Can the authors discuss the potential impact of LoRA adaptation on model generalization? \n* How does the efficiency of the proposed methods scale with larger models and datasets\uff1f"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work investigates the computational limits of Low-Rank Adaptation (LoRA) for finetuning transformer models through fine-grained complexity analysis. The central insight is that the low-rank structure within LoRA's gradient computations can enable algorithmic speedup. Two main contributions are highlighted:\n\n1. **Efficiency Phase Transition**: The study identifies a sharp efficiency threshold for rank-\\(r\\) LoRA update algorithms based on norms derived from the interactions between input sequences, pretrained weights, and adapter matrices. Efficient (sub-quadratic) algorithms are possible only when these norms fall below a specified threshold.\n\n2. **Nearly Linear Algorithms**: By leveraging hierarchical low-rank structures in LoRA gradients, the authors construct nearly linear approximation algorithms for LoRA adaptations, assuming the Strong Exponential Time Hypothesis (SETH).\n\nTo validate the theoretical findings, the authors explore partial and full weight adaptation scenarios within transformer attention heads, focusing on weights like $W_V$, $W_Q$, and $W_K$."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "This paper tackles a highly relevant and timely topic: Low-Rank Adaptation (LoRA). LoRA has gained widespread popularity in practice for its effectiveness in fine-tuning large models efficiently. Despite its practical success, there has been a notable gap in the theoretical understanding of LoRA, making this study\u2019s contributions especially valuable to the field. Developing a rigorous theoretical foundation for LoRA will not only solidify its current applications but also open doors for future research and refinement of the technique.\n\nThe abstract and introduction are clear, engaging, and well-crafted. They effectively set the stage for the paper, with a strong motivation that highlights both the practical importance of LoRA and the need for a deeper theoretical exploration. The authors have done a commendable job in outlining the primary contributions, making it easy for readers to understand the key takeaways from the study. The literature review is thorough and provides an excellent context for where this work fits within the broader landscape of model adaptation techniques.\n\nThe inclusion of a paragraph on paper organization is appreciated, as it offers a helpful roadmap for readers. It ensures that the structure of the paper is transparent from the outset, allowing readers to follow the flow of ideas with ease.\n\nThe authors have also set a high standard for notation and technical formalism. The clarity and consistency of notation enhance readability.\n\nWhile the technical results appear sound, I should note that, as someone not fully specialized in this type of analysis, I may not have caught every nuance. However, the reasoning seems robust, and the proofs are presented in a way that suggests a careful, thorough approach.\n\nFinally, the conclusion is well-done and reinforces the main points. It synthesizes the findings effectively and reflects on their implications, offering insights into how these results might shape future research in model adaptation. Overall, this paper makes a significant contribution to bridging the gap between LoRA\u2019s practical success and its theoretical understanding, providing a strong foundation for ongoing exploration in the field."
            },
            "weaknesses": {
                "value": "This paper currently feels dense and challenging to navigate, as it primarily consists of a series of definitions, lemmas, and theorems, often presented without sufficient explanation, clarification, or intuitive context. For readers who are not already experts in this area, this can make it difficult to grasp the key concepts and results. \n\nThere are several opportunities to improve accessibility and readability. Some of the definitions would be more appropriately placed in an appendix, as the main text is quite heavy with formal definitions, lemmas, and theorems. Moving certain definitions to an appendix could help streamline the main text, allowing readers to focus on the core arguments without getting overwhelmed by technical details.\n\nIn addition, a few of the lemmas lack clear statements, which can lead to confusion. For instance, in Lemma 1.1, the term \\(L\\) is introduced without any accompanying explanation. It would be helpful to review the formulations to ensure clarity, even if the technical details are correct. Improving the precision in how terms are introduced will aid readers in following the logical flow more naturally, without having to reread sections to understand each step.\n\nGiven the extensive number of definitions, it would be beneficial to assign descriptive names to them. Naming each definition provides a quick reference, helping readers keep track of terms and concepts as they reappear later in the paper. Otherwise, it\u2019s easy to lose track of which definition corresponds to which concept, especially in a highly technical document.\n\nSections 3 and 4, which seem intended to present the main results, read more like collections of lemmas, theorems, and technical details without adequate discussion or contextualization. This is a significant area for improvement. The results would be far more impactful if they were accompanied by clear explanations and discussions. Theoretical contributions are valuable only if they can be understood and appreciated, and in their current form, the key insights may be difficult for readers to discern.\n\nFinally, the paper would benefit from including plots, diagrams, or simple illustrations that clarify the results. While extensive experimental results may not be expected in a theoretical paper, even a few toy examples or visual aids could significantly enhance reader comprehension and provide concrete illustrations of the theoretical findings.\n\nOverall, with some adjustments to the structure, added explanations, and a few visual aids, this paper could become far more accessible and impactful, allowing a broader audience to appreciate the significance of the work.\n\nWhile the technical content appears sound, I am unable to recommend the paper for acceptance in its current form due to significant organizational issues outlined above. Addressing these concerns would greatly improve the paper\u2019s clarity and accessibility."
            },
            "questions": {
                "value": "Could you please provide additional explanation of the theoretical results? \n\nAdditionally, would it be possible to include some toy or controlled experiments, or other illustrations to help clarify the findings?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper examines the computational constraints inherent in current LoRA algorithms, focusing particularly on the O(L^2) computational complexity encountered when updating attention blocks. The authors aim to establish a unified upper-bound threshold for these norms, demonstrating that efficient approximation algorithms for LoRA can indeed operate below this threshold. Consequently, they provide proof of the existence of a nearly linear approximation algorithm, advancing the understanding of computational efficiency within the LoRA framework."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. Exploring the computational limits of parameter-efficient fine-tuning (PEFT) algorithms is a timely and relevant area of study.\n2. By utilizing the tensor vectorization tricks, the authors prove the existence of nearly linear approximation algorithms for LoRA adaptation.\nNotably, the authors also establish necessary conditions that could inspire the development of more efficient adaptation methods.These conditions are critical for future research aimed at accelerating the approximation process."
            },
            "weaknesses": {
                "value": "1.\tCould the authors clarify why Equation 1.2 holds? The expression on the right-hand side appears to minimize the discrepancy between the attention output and the labels Y. Do we only consider 1-layer attention here?\n2.\tThe Strong Exponential Time Hypothesis currently seems to serve only as a counterexample in the context of gradient approximation. Its relevance to the subsequent analysis is unclear in its present form. The reviewer suggests incorporating a more precise and directly relevant statement to clarify its role in the argument.\n3.\tWhile purely theoretical contributions may not always necessitate empirical validation, this paper's objective\u2014improving the efficiency of optimizing large language models (LLMs) with LoRA\u2014suggests that experimental results are essential for substantiating its claims. Specifically, an empirical evaluation could verify whether the bounded gradient approximation indeed holds in practice, as this is critical for the practical applicability of the proposed methods.\n4.\tThe authors assert that 'the existence of low-rank decompositions leads to potential algorithmic speedup.' Could full parameter updating also yield similar benefits? Additionally, how the speedup is related to the rank r?\n5.\tAn alternative approach might involve updating the feed-forward network (FFN) layer rather than the attention block, for example [1][2]. Could this adjustment also avoid/alleviate the O(L^2) computational complexity issue in the paper?\n6.\tThe current title may not fully align with the paper\u2019s objectives, as it does not primarily address computational drawbacks but rather focuses on **solving**  these computational limitations.\n\n[1] AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition. \n[2] Parameter-Efficient Fine-Tuning with Controls."
            },
            "questions": {
                "value": "See the questions in \"weakness\"."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This work shows the existence of almost linear approximation algorithms for LoRA on transformer-based models. This paper also proves a phase transition behavior in the efficiency of LoRA. A detailed proof sketch is provided in the paper to support the results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. As a theoretical work, this work is well-written and not difficult to follow.\n2. The proof idea makes sense and is solid."
            },
            "weaknesses": {
                "value": "1 There are two notations that may not be necessary. I suggest considering whether it is possible to remove or simplify some definitions to derive all the lemmas and theorems in the main body.\n2 The practical insights of this paper are not very clear. It is better to highlight the practical significance of the theoretical analysis in the paper."
            },
            "questions": {
                "value": "1 In lemmas 3.3, 3.4, and 3.5, the amount of time needed to construct the matrices is included. What is the algorithm used for construction here? Why is discussing the required amount of time meaningful since in practice these parameters are learned by gradient methods instead of any construction?\n2 Why do you put the analysis of full LoRA in the appendix instead of in the main body? \n3 What are SAT algorithms in line 142?\n4 Is it $\\frac{\\partial L}{\\partial A_\\mu}$ rather than $\\frac{\\partial L}{\\partial A_Q}$ in line 97?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}