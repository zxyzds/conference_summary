{
    "id": "774F8gF0UO",
    "title": "From Bulk to Budget: Best Practices To Compress Multimodal Large Language Models",
    "abstract": "Multimodal large language models (MLLMs) are increasingly developed to meet diverse deployment needs, varying in scale and computational demand. While recent research has focused on building MLLMs from Small Language Models (SLMs), these efforts remain limited in flexibility and are still data- and compute-intensive. In this paper, we present the first comprehensive study on flexibly compressing and recovering existing MLLMs in a data-efficient manner. Hence, we address a critical gap in the literature by empirically analyzing best practices for adapting to specific hardware or resource limitations. Our study investigates pruning and knowledge distillation techniques, examining their impact on downstream performance across various model compression strategies, including pruning paradigms, recovery training schemes, and data requirements. Key findings reveal that widthwise pruning is particularly effective in resource-constrained scenarios. For smaller compression ratios, finetuning the multimodal projector alone can restore most performance, while combining finetuning with hidden state knowledge distillation proves most effective across all compression levels. Notably, we demonstrate efficient model downsizing using as little as 5% of the original dataset for moderate compression. Our analysis suggests best practices for compressing MLLMs for resource-efficient deployment. With our best practices, Bunny-v1.0-3B retains over 95% of its original performance, while LLaVA-v1.5-7B maintains more than 97%, with compression ratios below 30%.",
    "keywords": [
        "Multimodal large language models",
        "model pruning",
        "knowledge distillation",
        "model compression"
    ],
    "primary_area": "foundation or frontier models, including LLMs",
    "TLDR": "Best practices for MLLM compression",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=774F8gF0UO",
    "pdf_link": "https://openreview.net/pdf?id=774F8gF0UO",
    "comments": [
        {
            "summary": {
                "value": "This paper addresses the challenge of compressing multimodal large language models in a data-efficient way, introducing structured pruning and recovery techniques for resource-constrained deployments. The paper explores layerwise and widthwise pruning strategies, with a focus on identifying effective recovery techniques, such as finetuning and knowledge distillation. Key findings include the effectiveness of widthwise pruning in low-resource scenarios and the combination of projector finetuning with hidden state knowledge distillation for optimal performance recovery. The study provides insights into best practices for compressing MLLMs while maintaining high performance across a variety of tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper presents a unique combination of widthwise and layerwise pruning for MLLMs, complemented by targeted recovery strategies like finetuning and knowledge distillation. This distinguishes the work from typical compression techniques that rely solely on one pruning or finetuning method, offering a more adaptable framework for diverse deployment needs.\n2. This ppaer emphasizes data-efficient model recovery, highlighting scenarios where only 5% of the original data suffices to restore a substantial portion of the model\u2019s performance, making the proposed method practical for environments where labeled data is scarce or costly, potentially expanding its applicability in low-data or real-time contexts.\n3. Extensive experimentation across two MLLMs with varying compression ratios, recovery techniques, and data requirements provides a detailed view of how different configurations affect model performance. The ablation studies also offer a deeper understanding of the benefits and limitations of each pruning and recovery method."
            },
            "weaknesses": {
                "value": "1. The proposed techniques are evaluated within the context of MLLMs; however, they lack comparisons with other prevalent compression methods. This absence makes it challenging to assess their effectiveness against existing solutions, especially as structured pruning and knowledge distillation are already well-established in the field.\n2. While the framework demonstrates potential at lower compression ratios, its performance gains are limited at higher compression ratios. This limitation may reduce the practical appeal of the method for applications that demand more aggressive compression while still preserving task performance.\n3. Although the proposed combination of multiple pruning and recovery strategies is thorough, it increases implementation complexity. The reliance on complex data structures and recovery steps may hinder deployment. An author should clarify this."
            },
            "questions": {
                "value": "1. What are the limitations of the chosen pruning strategies for different types of tasks within MLLMs?\n2. Can the proposed data-efficient recovery techniques be generalized to other model architectures or require specific adjustments?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a comprehensive study on compressing Multimodal Large Language Models (MLLMs) while maintaining performance. The authors investigate two pruning strategies (widthwise and layerwise) and various recovery methods (supervised finetuning and knowledge distillation) across different compression ratios. They conduct experiments on two MLLMs (LLaVA-v1.5-7B and Bunny-v1.0-3B) and provide best practices for MLLM compression based on their findings."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper provides extensive empirical evaluations across different compression ratios, recovery strategies, and data requirements.\n\nThe experiments are well-designed and cover multiple dimensions: pruning methods, and data efficiency."
            },
            "weaknesses": {
                "value": "1. The authors' claim of being the first to investigate general-purpose MLLM compression overlooks existing work, particularly the survey paper on efficient MLLMs [1], which contradicts the authors\u2019 claim. The literature review could be more comprehensive, especially in acknowledging related work in MLLM efficiency. \n\n2. The technical contributions largely adapt existing LLM compression techniques to MLLMs without introducing significant novel methods. \n\n3. The findings mostly confirm expected behaviors from LLM compression research. The paper primarily combines existing techniques rather than introducing new methodological advances. The exploration could better highlight MLLM-specific challenges and solutions that differentiate it from general LLM compression\n\n\n[1] Efficient Multimodal Large Language Models: A Survey"
            },
            "questions": {
                "value": "see weaknesses"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper provides a recipe to compress MLLM by evaluating the width-wise pruning and layerwise pruning, finding that widthwise pruning is better. Then, by recovery strategies like SFT and KD to restore the performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- This paper is easy to understand and follow. \n- This paper provides concrete best practices for practitioners"
            },
            "weaknesses": {
                "value": "- The experiments are not comprehensive with just limited model selection. Please include newer MLLM architectures like InternVL, CogVLM, MiniCPM-v; \n- The title of this paper claimed \"Best Practices To Compress MLLM\". However, this paper only focuses on pruning and then knowledge distillation. Additionally, the pruning only focuses on LLM and there is no experiment on pruning Vision encoder. It is a little bit overclaimed when you do not involve other compression methods like quantization or low-rank factorization. This paper should dive into compression and provide more detailed insight for readers. \n- Limited novelty: pruning+knowledge distillation are the common techniques on LLM, as presented in Sheared LLaMA and Minitron. This paper has limited novelty as there is no difference. \n- Limited comparison with other compression methods: this paper proposed several techniques like width/depth pruning and KD. This paper did not involve any compression method with other methods."
            },
            "questions": {
                "value": "See Weakness"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}