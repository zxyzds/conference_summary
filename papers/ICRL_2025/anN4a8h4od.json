{
    "id": "anN4a8h4od",
    "title": "Filtered Semantic Search via Vector Arithmetic",
    "abstract": "How can we retrieve search results that are both semantically relevant and satisfy certain filter criteria? Modern day semantic search engines are increasingly reliant on vector-based search, yet the ability to restrict vector search to a fixed set of filter criteria remains an interesting problem with no known satisfactory solution. In this note, we leverage the rich emergent structure of vector embeddings of pre-trained search transformers to offer a simple solution. Our method involves learning, for each filter, a vector direction in the space of vector embeddings, and adding it to the query vector at run-time to perform a search constrained by that filter criteria. Our technique is broadly applicable to any finite set of semantically meaningful filters, compute-efficient in that it does not require modifying or rebuilding an existing $k$-NN index over document vector embeddings, lightweight in that it adds negligible latency, and widely compatible in that it can be utilized with any transformer model and $k$-NN algorithm. We also establish, subject to mild assumptions, an upper bound on the probability that our method errantly retrieves irrelevant results, and reveal new empirical insights about the geometry of transformer embeddings. In experiments, we find that our method, on average, yields more than a 21% boost over the baseline (measured in terms of nDCG@10) across three different transformer models and datasets.",
    "keywords": [
        "passage retrieval",
        "dense retrieval",
        "feature representation"
    ],
    "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning",
    "TLDR": "We describe a lightweight modification to vector-based semantic search that efficiently restricts results to those matching a specified, semantically-meaningful filter.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=anN4a8h4od",
    "pdf_link": "https://openreview.net/pdf?id=anN4a8h4od",
    "comments": [
        {
            "summary": {
                "value": "The authors introduce the problem of retrieving items for queries with preference filters. They propose a simple yet effective solution of representing the resultant filtered query representation as weighted sum of the corresponding vectors. Additionally, they provide a theoretical analysis of the performance guarantees of their method under specific distributions of relevant and non-relevant items."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- Faceted search is an interesting and important problem, and the novelty of this work lies in its approach to addressing it.\n- The authors present a straightforward solution and set up an experimental framework to test their method's effectiveness in solving the filtered search problem."
            },
            "weaknesses": {
                "value": "- The evaluation domains are quite limited, and some of the curated data is synthetic in nature.\n- Using vector summation to produce a composite representation is a common approach. Given this, I feel the paper doesn\u2019t present a novel contribution in representation learning. However, with its unique problem formulation and with additional analysis, along with broader testing across domains, it could still be a fit for data mining and information retrieval conferences.\n- If the assumptions regarding the relative distribution of documents and query representations do not hold, the method may fail to perform effectively. Given the existing literature on the anisotropic behavior in embeddings, I am cautious about overlooking this concern.\n- The theoretical claims appear to be self-evident. The authors assume that relevant and irrelevant documents follow specific distributions, then design the representation as the distribution mean with a separability threshold based on variance. Naturally, this setup ensures separability; thus, the analysis seems to add little value. It would be more compelling if the authors showed that the method performs within an \u03f5-optimal range even when data distributions deviate from these assumptions."
            },
            "questions": {
                "value": "The weakness themselves can be considered as questions.\n\n**Additional Questions and Suggestions:**\n\n- Of the following three contributions, which do the authors consider most central to their work?\n    - Identifying the problem of filtered search and providing a benchmark for it\n    - Proposing a non-trivial method to solve the filtered search problem\n    - Offering theoretical guarantees for the proposed method\n    \n    The paper touches on aspects of each of these contributions, but none are explored in depth. I suggest enhancing one or more of these areas and restructuring the paper with a clearer, more focused motivation. While focusing on all of them is a good idea, but the paper reads very incomplete without going into depth in some of them.\n    \n- Do the authors believe similar ideas could extend to other domains? The current filters seem somewhat synthetic\u2014additional experiments on real-world data would strengthen the work.\n- Could you test alternative, simple baselines?\n    - For instance, multiply the scores of the query and filter for each document, such that\n        \n        p(q+f,d)=p(q,d)\u00d7p(f,d).\n        \n    - Another approach could involve converting  q+f into natural language queries using an LLM, then feeding the output text to the encoder to generate the query + filter representation"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper investigates how do efficient, vector-based nearest neighbor search given various filter criteria. The authors theoretically and empirically explore a linear probing-based approach. They show strong empirical gains using the proposed method compared to a particular baselines."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper explores a nuanced and import problem in nearest neighbor search, namely search for nearest neighbors meeting some filter set criteria. I believe that this is problem can have very good impact on both academic and industry research communities. \n\nThe strengths of the paper include:  \n* Well performing, simple, scalable approach to filtered-based nearest neighbor search\n* Well presented methodological approach, clearly demonstrated through intuitive examples\n* Grounding in theoretical statements"
            },
            "weaknesses": {
                "value": "I am an advocate for the setting of the paper as well as the simplicity of the approach. \n\nHowever, I feel that the paper falls short in a few key aspects:\n\n* First, given the simplicity of methodology of the approach (which I generally support), I would have expected more analysis in comparison to Filtered-DiskANN and other alternative approaches. Understanding why and when to use which approach is key for having impact, especially with practitioners. I think the authors could have more clearly outlined pros & cons between their approach and Filtered-DiskANN and further shown a more complete setting of experiments to demonstrate this. \n* Further, I am misguided, but I feel that the method is on the straightforward side, e.g., it is what many researchers may have wanted to try for this problem. In some ways that is a kudos for the authors for trying this, but in others, I would have expected much more explanation about the approach as it relates to classical methods like PCA, etc.\n* However, my major concern about the work is that, the datasets (Table 1) are much to small by modern standards for us to get a sense of how the method scales. \n\nMinor:\n* No conclusion\n* Various issues with citation parens\n* There is certainly work earlier than 2009 for IR (line 116)"
            },
            "questions": {
                "value": "* Can you say more about how you expect the method to scale w/ larger corpora?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper is on (vector) embedding-based retrieval. Suppose we have a pre-trained (text) embedding model. Suppose we have a document corpus and assume that each document has a label (such as the \\textit{color} or \\textit{brand} of a product). Given a (query, label), the problem is to fetch documents which are relevant to the query and with the specified label. \n\nThe authors work in the setting where they intend to use a generic embedding model to embed the document corpus. The key proposal is to learn $d$-dimensional representations for labels (where documents are also represented in $d$-dimensional Euclidean space). The authors assume that the set of labels is small \u2014 much smaller than the corpus size. Therefore, the number of additional learned parameters is small (compared to the number of parameters in the frozen embedding model).\n\nThis is how they learn the linear probe $\\nu_f$ for label f. Each document $d_i$ is assumed to have a label $y_i$. They learn a $d$-dimensional vector $\\nu_f$ for each label value $f$ such that $\\nu_f \\cdot v_{d_i}$ is close to $\\mathbf{1}\\{y_i = f\\}$. They minimize the squared loss $\\sum_{i=1}^T (\\nu_f \\cdot v_{d_i} - \\mathbf{1}\\{y_i = f\\})^2$ where $T$ is the training set size. \n\nNow given a query and label $(q, f)$, they perform nearest-neighbor search in the document corpus for the search vector\n$$\nv_{q+f} + \\lambda \\nu_f\n$$\nafter unit l2-normalization, where $\\lambda$ is tuned by cross-validation. Note that the document embeddings are not modified, it is only the search vector that is modified, by the procedure. So crucially, this procedure can be applied for multiple label sets (filter sets) with the same document embedding corpus.\n\nThe proposed method performs significantly better than natural baselines, with no consequential increase in serving cost."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "* The retrieval quality improvements in Table 2 are significant.\n\n* The method is straight-forward to implement. (This is a strength.)\n\n* Intuitively it makes sense to rank documents by the weighted sum of two scores: (i) $v_{q+f} \\cdot d$: a general query document dot-product (ii) $\\nu_f \\cdot d$: which is trained to be close to 1 if the document is likely to have label $f$."
            },
            "weaknesses": {
                "value": "* The datasets in experiments are small (Table 1). Unfortunately, I do not have pointers to larger datasets."
            },
            "questions": {
                "value": "* In Table 2, the performance of sgpt seems to be worse than the mpnet for colors and brands; even though from the model sizes (with similar training) one would expect the opposite.\n\n* Further, the baseline numbers are similar between the three base models. What explains this?\nHow is the hold-out set prepared for experiments documented in Table 3?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}