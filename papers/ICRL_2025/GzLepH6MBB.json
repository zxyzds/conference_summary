{
    "id": "GzLepH6MBB",
    "title": "MMTryon: Multi-Modal Multi-Reference Control for High-Quality Fashion Generation",
    "abstract": "This paper introduces MMTryon, a multi-modal multi-reference VIrtual Try-ON (VITON) framework, which can generate high-quality compositional try-on results by taking a text instruction and multiple garment images as inputs. Our MMTryon addresses three problems overlooked in prior literature: 1) Support of multiple try-on items. Existing methods are commonly designed for single-item try-on tasks (e.g., upper/lower garments, dresses). 2) Specification of dressing style. Existing methods are unable to customize dressing styles based on instructions (e.g., zipped/unzipped, tuck-in/tuck-out, etc.) 3) Segmentation Dependency. They further heavily rely on category-specific segmentation models to identify the replacement regions, with segmentation errors directly leading to significant artifacts in the try-on results. To address the first two issues, our MMTryon introduces a novel multi-modality and multi-reference attention mechanism to combine the garment information from reference images and dressing-style information from text instructions. Besides, to remove the segmentation dependency, MMTryon uses a parsing-free garment encoder and leverages a novel scalable data generation pipeline to convert existing VITON datasets to a form that allows MMTryon to be trained without requiring any explicit segmentation. Extensive experiments on high-resolution benchmarks and in-the-wild test sets demonstrate MMTryon's superiority over existing SOTA methods both qualitatively and quantitatively. MMTryon's impressive performance on multi-item and style-controllable virtual try-on scenarios and its ability to try on any outfit in a large variety of scenarios from any source image, opens up a new avenue for future investigation in the fashion community.",
    "keywords": [
        "Multi-modal Fashion Generation",
        "Compositional Virtual Try-on"
    ],
    "primary_area": "generative models",
    "TLDR": "A multi-modal multi-reference VIrtual Try-ON (VITON) framework, which can generate high-quality compositional try-on results",
    "creation_date": "2024-09-26",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=GzLepH6MBB",
    "pdf_link": "https://openreview.net/pdf?id=GzLepH6MBB",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces MMTryon, a model that integrates a multi-modality and multi-reference attention mechanism to enhance virtual try-on tasks by combining garment details from reference images with dressing styles from text guidance. The model is designed for multi-garment try-ons and does not require segmentation networks, thanks to a new parser-free garment encoder and a scalable data generation pipeline. MMTryon achieves superior performance in single and multi-garment try-on tasks, addressing limitations in previous models such as single-garment constraints, fixed dressing styles, and segmentation dependencies."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. The paper introduces innovative multi-modal and multi-reference attention mechanisms, allowing for multi-garment try-ons with customized, fine-grained text guidance.\n\n 2. Leveraging a pretrained garment encoder, MMTryon can capture detailed features of garments specified by text, improving model precision and contributing to advancements in the field.\n\n 3. Extensive qualitative and quantitative evaluations demonstrate MMTryon\u2019s superior performance compared to state-of-the-art methods, with high-quality visual results showcasing realistic try-on effects and robust editing capabilities.\n\n 4. The proposed data generation pipeline creates a comprehensive paired multi-garment dataset without segmentation, broadening the scope of try-on tasks and supporting complex garment compositions.The parsing-free approach in this article presents a promising direction, contributing to the advancement of try-on technology toward more practical applications.\n\n 5.This article is well-written"
            },
            "weaknesses": {
                "value": "1.some descriptions could be clearer to avoid reader confusion. For example, in Fig. 3, labeling the UNet as \u201cMMTryon UNet\u201d rather than specifying it as a pretrained SD UNet might mislead readers at this training stage. Similarly, terms like \u201cw/o TQL\u201d and \u201cw/o MRA\u201d in Fig. 8 are somewhat unclear;  \n\n2.the inference process requires more detailed explanation, especially given the model\u2019s support for multiple tasks. Currently, some detail of inference lack explaination, which may lead to unintended changes in specific garments, such as unexpected alterations in lower garments shown in the first image."
            },
            "questions": {
                "value": "1.As mentioned in the weaknesses, please explain why the pants in the last rows of the first image changed unexpectedly.\n\n2.the method is good\uff0cAre there any plans to release the code and model to support the  research community?\n\n3.What is the inference time and required GPU memory for MMTryon in different task?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces MMTryon, a multi-modal, multi-reference Virtual Try-On (VITON) framework capable of generating high-quality, compositional try-on results from text instructions and multiple garment images. MMTryon addresses three main limitations in existing methods: supporting multiple garments, specifying dressing styles, and reducing dependency on segmentation models. It achieves this through a novel multi-modality attention mechanism that combines garment details from images with style cues from text, and by using a parsing-free garment encoder alongside a scalable data pipeline to eliminate the need for segmentation. Extensive experiments demonstrate MMTryon\u2019s superior performance in multi-item and style-controllable try-on tasks, offering new potential for applications in the fashion field."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "1. The paper introduces a unique attention mechanism that enables multi-item try-on and customizable styling, addressing limitations in previous models.\n2. The paper adopts a novel approach by eliminating segmentation dependency using a parsing-free garment encoder and scalable data pipeline. This method presents a practical solution to common issues in virtual try-on tasks, improving model efficiency and minimizing artifacts.\n3. The paper is well-organized, and there are a lot of ablation study in main paper and appendix to demonstrate impact of each component in the proposed approach."
            },
            "weaknesses": {
                "value": "1. The Related Work section lacks recent studies on control and try-on methods,\n2. More detail information about dataset should be provided.\n3. in some case, the fine texture on the garment still cannot be recovered completely,"
            },
            "questions": {
                "value": "1. Could you provide more information about the e-commerce dataset used in the experiments?\n2. Do you have any plans to open-source the dataset and the data processing pipeline? Making these resources publicly available could greatly benefit research in this area and facilitate further advancements in virtual try-on methods."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper combines multi-modal implementation of VITON. \nThree problems are proposed and solved by the designed data set and multi-model instruction attention. \nThe experimental results show that the data in the wild well."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. MMTryon is able to combine multiple garments for fitting, while also allowing the fitting effect to be manipulated by text commands. \n2. A scalable data generation pipeline and a specially designed clothing encoder are introduced. \n3. Eliminates the need for any prior segmentation network during the training and reasoning phases."
            },
            "weaknesses": {
                "value": "1. The definition of the task is not very clear, and similar work may already exist, leading to unclear motivation. Related work needs to be strengthened.\n2. multi-modal multi-reference attention is just a simple extension of reference attention [1], and it is not clear how to decouple and align the details.\n3. The quantitative and qualitative results are not complete, especially the texture details are not so good.\n4. The data set is the highlight of this paper, but it doesn't seem to claim to be open data, so I don't think it's a contribution either.\n---\n[1] Hu L. Animate anyone: Consistent and controllable image-to-video synthesis for character animation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 8153-8163."
            },
            "questions": {
                "value": "1. The paper makes overstated claims, such as line 96 where it states, \"is the first model to support the multi-modal compositional try-on task.\" However, multi-modal compositional try-on is already supported by M&M VTO [1], AnyFit [2], and VITON-DiT [3]. I don't think the relevant work is sufficient. And I recommend the authors explicitly compare these previous works in terms of their multi-modal capabilities to clarify if the proposed approach indeed offers novel advantages or is simply an extension of existing techniques.\n\n2. The paper lacks clear motivation and has weak contributions. For example, Question 1 has already been explored, but the authors do not clearly articulate its relevance or contribution here. Additionally, texture consistency\u2014an essential issue in VTON\u2014seems neglected. I suggest the authors further clarify the connection between their research questions and contributions and add a discussion on texture consistency, as this factor is critical to the realism of virtual try-on. This improvement would enhance the clarity and relevance of the study.\n\n3. The novelty is limited. The garment encoder appears to use cross-attention, which raises the question of whether there is anything unique about it. I recommend the authors explain how their garment encoder differs from or improves upon standard cross-attention mechanisms, especially if there are any specific design or functional enhancements, to demonstrate genuine innovation.\n\n4. Serious concerns arise from fusing text and image features directly without decoupling, which may lead to unintended consequences, yet the paper does not address this. I suggest the authors provide a more thorough justification for this fusion strategy, as bypassing decoupling could compromise the model\u2019s interpretability or adaptability. Discussing the rationale behind this choice and evaluating potential pitfalls would strengthen the explanation.\n\n5. The paper lacks thorough comparison methods. In particular, quantitative results for single-garment cases are not provided for methods like Outfit Anyone, Kolors-Virtual-Try-On [4], Wear-Any-Way [5], and MV-VTON [6]. Additionally, Figure 6 does not display clear advantages. The omission of quantitative comparisons with key methods weakens the study\u2019s validity. I suggest including comparative performance metrics with these established models in Table 1 and providing more insightful analysis of Figure 6 to support the claimed improvements. Besides,  Under multi-modal conditions, Table 7 suggests that background is not controllable. However, Imagedressing [7] has achieved background control, yet it is not referenced or discussed. If background control is indeed relevant, I suggest that the authors provide a direct comparison with Imagedressing or explicitly state if background control is beyond the scope of their study to give readers a clearer understanding of the method\u2019s applicability.\n\n6. Texture details (such as Pants, logo changed) appear poorly rendered across multiple cases, including those in the appendix. I recommend identifying specific areas with poor texture rendering and offering suggestions to improve texture detail quality, as this would enhance the realism and appeal of the model\u2019s outputs.\n\n7. The implementation details concerning SDXL on line 295 and SD 1.5 on line 352 are confusing. I suggest that the authors clarify why they employ different Stable Diffusion versions in various pipeline segments and explain the implications for model outputs to improve method transparency.\n\n8. Adding results under similar conditions, such as challenging cases with letters, numbers, and diverse textures, is advised. The paper would benefit from showcasing the model\u2019s performance on complex garments, such as those with letters or intricate textures, as these are realistic challenges in virtual try-on systems.\n\n\nOther details:\n1. In Table 2,  '0.7' should be '0.70'\n2. Formulas 1 and 2 have commas at the end, while formulas 3 and 4 are missing.\n---\n[1] Zhu L, Li Y, Liu N, et al. M&M VTO: Multi-Garment Virtual Try-On and Editing[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 1346-1356.\n\n[2] Li Y, Zhou H, Shang W, et al. AnyFit: Controllable Virtual Try-on for Any Combination of Attire Across Any Scenario[J]. arXiv preprint arXiv:2405.18172, 2024.\n\n[3] Zheng J, Zhao F, Xu Y, et al. VITON-DiT: Learning In-the-Wild Video Try-On from Human Dance Videos via Diffusion Transformers[J]. arXiv preprint arXiv:2405.18326, 2024.\n\n[4] https://huggingface.co/spaces/Kwai-Kolors/Kolors-Virtual-Try-On.\n\n[5] Chen M, Chen X, Zhai Z, et al. Wear-any-way: Manipulable virtual try-on via sparse correspondence alignment[J]. arXiv preprint arXiv:2403.12965, 2024.\n\n[6] Wang H, Zhang Z, Di D, et al. MV-VTON: Multi-View Virtual Try-On with Diffusion Models[J]. arXiv preprint arXiv:2404.17364, 2024.\n\n[7] Shen F, Jiang X, He X, et al. Imagdressing-v1: Customizable virtual dressing[J]. arXiv preprint arXiv:2407.12705, 2024."
            },
            "flag_for_ethics_review": {
                "value": [
                    "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)",
                    "Yes, Potentially harmful insights, methodologies and applications",
                    "Yes, Responsible research practice (e.g., human subjects, data release)"
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}