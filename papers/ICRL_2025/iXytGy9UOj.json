{
    "id": "iXytGy9UOj",
    "title": "ARQ: A Mixed-Precision Quantization Framework for Accurate and Certifiably Robust DNNs",
    "abstract": "Mixed precision quantization has become an important technique for\nenabling the execution of deep neural networks (DNNs) on limited resource computing platforms.\nTraditional quantization methods have primarily concentrated on maintaining\nneural network accuracy, either ignoring the impact of quantization on the\nrobustness of the network, or using only empirical techniques for improving\nrobustness. In contrast, techniques for robustness certification, which can\nprovide strong guarantees about the robustness of DNNs have not been used\nduring quantization due to their high computation cost and/or scalability\nissues. \n\nThis paper introduces ARQ, an innovative mixed-precision quantization method that not only\npreserves the clean accuracy of the smoothed classifiers but also maintains\ntheir certified robustness. ARQ uses reinforcement learning to find accurate and robust\nDNN quantization, while efficiently leveraging randomized smoothing,\na popular class of statistical DNN verification algorithms, to guide the search process. \nWe compare ARQ with multiple state-of-the-art quantization techniques on\nseveral DNN architectures commonly used in quantization studies: ResNet-20 on\nCIFAR-10, ResNet-50 on ImageNet, and MobileNetV2 on ImageNet. \nWe demonstrate that ARQ consistently performs better than these baselines\nacross all the benchmarks and the input perturbation levels. In many cases, the performance of ARQ quantized networks can reach that of the original DNN with floating-point weights, but with only 1.5% instructions.",
    "keywords": [
        "Quantization",
        "Robustness",
        "Certification",
        "Formal Verification"
    ],
    "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=iXytGy9UOj",
    "pdf_link": "https://openreview.net/pdf?id=iXytGy9UOj",
    "comments": [
        {
            "summary": {
                "value": "The paper introduces ARQ (Accurate and Robust Quantization), a framework that addresses the critical challenge of deploying deep neural networks in resource-constrained environments while maintaining both accuracy and robustness. The framework's key innovation lies in its novel approach to mixed-precision quantization that directly optimizes for certified robustness, making it the first of its kind in the field. Through a sophisticated combination of reinforcement learning and randomized smoothing, ARQ achieves remarkable efficiency gains while preserving or even improving model performance compared to full-precision networks."
            },
            "soundness": {
                "value": 4
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 4
            },
            "strengths": {
                "value": "The use of average certified radius as an optimization objective is particularly novel, allowing simultaneous optimization of accuracy and robustness.\nThe quality of the research is evident in the comprehensive experimental validation across multiple architectures and datasets. The authors provide ablation studies and comparative analysis which gives valuable insights into the framework's behavior and advantages. The achievement of matching or exceeding full-precision performance with drastically reduced computational requirements is particularly impressive."
            },
            "weaknesses": {
                "value": "While the paper's contributions are substantial, there are areas that could benefit from further exploration. The current focus on convolutional neural networks and image classification, while well-executed, leaves open questions about generalization to other architectures and tasks. This limitation is acknowledged by the authors, but additional discussion of potential adaptation strategies would be valuable.\nThe paper would benefit from more detailed analysis of memory usage patterns during training, though the runtime analysis provided in Table 2 is helpful. Additionally, while the framework's theoretical foundations are strong, more discussion of practical hardware implementation considerations for mixed-precision inference would strengthen the work's immediate applicability."
            },
            "questions": {
                "value": "How might the ARQ framework be adapted for sequence models or transformers, particularly regarding the interaction between attention mechanisms and mixed-precision quantization?\nWhat modifications to the incremental randomized smoothing approach might be necessary to support other certification methods while maintaining computational efficiency?\nHow does the framework handle the transition between different precision levels during inference, and what are the implications for hardware implementation?\nCould the reinforcement learning approach be extended to dynamically adjust precision levels during inference based on input characteristics?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "none"
            },
            "rating": {
                "value": 8
            },
            "confidence": {
                "value": 2
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "The paper introduces ARQ, a novel mixed-precision quantization framework for executing deep neural networks (DNNs) on resource-constrained platforms while maintaining accuracy and certified robustness. ARQ employs reinforcement learning to find quantization policies that preserve a DNN's accuracy, enhance its robustness, and reduce computational costs, effectively utilizing random smoothing for guidance. The framework supports mixed-precision quantization, allowing different bit-widths for weights in each layer, providing fine-grained control over quantization policies. ARQ is the first to optimize for certified robustness in DNNs and includes random smoothing within its reinforcement learning loop. Experimental results on CIFAR-10 and ImageNet datasets demonstrate that ARQ outperforms state-of-the-art quantization techniques, often matching or exceeding the performance of original FP32 networks with significantly reduced operations. The paper acknowledges ARQ's limitations, such as dependence on the training of the original network and challenges in deploying mixed-precision inference, and suggests future work on applying ARQ to tasks beyond image classification."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 2
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The paper is easy to read."
            },
            "weaknesses": {
                "value": "1.\tThe performance of ARQ is heavily reliant on the quality of the training of the original DNN. If the original network is not properly trained or lacks Gaussian augmentation, the quantized network may not meet expected performance.\n2.\tARQ has only been evaluated on image classification tasks. Its performance and effectiveness on other types of tasks have not been validated.\n3.\tWhile ARQ performs well in the experiments, its generalizability across different network architectures and datasets remains an open question and requires further research for validation.\n4.\tThe performance of ARQ may be sensitive to hyperparameter choices, which could require additional tuning efforts to ensure optimal performance across different networks and datasets.\n5. The presentation of the paper is poor. For examples, the statement of robustness is not clear, adversarial inputs or what? If it is adversarial inputs, why not provide the details for adversarial methods? PGD or FGSM? \n6. The method is not novel, and too complex. The results are also not convincing.\n7. The main limitations are experiments, i.e., more datasets (COCO2017, VOC), more larger networks (e.g., VIT, GPT-2, Efficientnet), more tasks (detection, segmentation, VQA), and more validation for robustness.\n8. Why not compare with more competitors[1-3]? \n[1] SDQ: Stochastic Differentiable Quantization with Mixed Precision\n[2] EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization\n[3] OMPQ: Orthogonal Mixed Precision Quantization"
            },
            "questions": {
                "value": "1. What is the definition of robustness for MPQ?\n2. What is certified robustness?\n3. In your paper, you argue that traditional quantization methods have primarily concentrated on maintaining neural network accuracy, this is not accurate. The goal of MPQ is to study how to reduce the network's parameters, namely, obtaining the trade-off between accuracy and complexity. They have at least two objectives to learn.\n4. Why use reinforcement learning?\n5. In your paper, you state two objectives, i.e.,  accuracy, and robustness. Why not consider complexity? Maybe you don't understand the MPQ."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a mixed-precision quantization framework ARQ which optimizes both DNN\u2019s accuracy and certified robustness under computational resource constraint. ARQ employs reinforcement learning to optimize quantization policies and leverage incremental randomized smoothing (IRS) within the reinforcement learning loop, allowing it to efficiently guide the search for quantization policies that maximize the average certified radius (ACR) of the DNN."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "ARQ has better accuracy and robustness, i.e., ACR, than other compared methods."
            },
            "weaknesses": {
                "value": "1. The authors didn\u2019t compare ARQ with any robustness-aware quantization methods as listed in Table5.\n2. The authorsdidn\u2019t compare with other advanced fixed-precision or mixed-precision quantization algorithms, such as LSQ[1], LSQ+[2], or HAWQ[3][4].\n3. Although ARQ has better accuracy and robustness, it consumes much longer time than other methods as shown in Table2. What is the impact of search time on the final accuracy.\n4. The authors only use BitOps as the constraints. What about the model size?  \n5. The techniques proposed in the paper seem incremental. The authors need to provide more explanation on the novelty of the method.\n\n[1] Esser S K, McKinstry J L, Bablani D, et al. Learned step size quantization[J]. arXiv preprint arXiv:1902.08153, 2019.\n[2] Bhalgat Y, Lee J, Nagel M, et al. Lsq+: Improving low-bit quantization through learnable offsets and better initialization[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. 2020: 696-697.\n[3] Dong Z, Yao Z, Arfeen D, et al. Hawq-v2: Hessian aware trace-weighted quantization of neural networks[J]. Advances in neural information processing systems, 2020, 33: 18518-18529.\n[4] Yao Z, Dong Z, Zheng Z, et al. Hawq-v3: Dyadic neural network quantization[C]//International Conference on Machine Learning. PMLR, 2021: 11875-11886."
            },
            "questions": {
                "value": "Please refer to the weakness for questions."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces ARQ, a mixed-precision quantization framework for deep neural networks that focuses on robustness of quantization technique. ARQ incorporates robustness certification techniques directly into the quantization process, addressing the limitations of conventional methods that solely focus on accuracy. It leverages randomized smoothing to estimate the certified radius, which reflects the network's resilience to input perturbations. ARQ frames the quantization problem as a reinforcement learning task, where an agent searches for optimal bit-width assignments for each layer, maximizing the average certified radius while adhering to resource constraints."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. Strong motivation: Modern quantization techniques, even though they show less quality drop on specific models and tasks, show large quality drop on adversarial inputs which means reduced robustness of quantized network. This work targets the problem of making quantization technique which makes truly optimized network with high robustness that can widely adopted on the overall models.\n2. Well explained background.\n3. Novelty of methodology: Although ARQ adopts most of its robustness concept from existing works, its own optimization objective of mixed precision quantization is unique since many existing works haven\u2019t explored the robustness in terms of utilizing diverse bit-width in a model."
            },
            "weaknesses": {
                "value": "1. Weak baseline: Beyond robustness aware mixed precision quantization, there are several methods [1,2,3] that shows much less accuracy degradation on both ResNets and Mobilenets in Imagenet scale. What is the comparative advantage of ARQ compared to these methods even with accuracy degradation? For thorough comparison, shouldn't the authors include comparisons with various types of mixed precision quantization method?\n2. Comparison with existing robustness aware MPQ: Within robustness aware quantization, the current metrics cannot justify the benefit of ARQ in the aspect of robustness compared to existing robustness aware MPQ (in Table5).\n3. Algorithm: Lack of explanation of several terms in Algorithm1 makes bad readability, i.e., FullRobustCertify, IncrementalRobustCertify.\n\n[1] Huang, Xijie, et al. \"SDQ: Stochastic differentiable quantization with mixed precision.\" International Conference on Machine Learning. PMLR, 2022.\n\n[2] Kim, Han-Byul, et al. \"MetaMix: Meta-state Precision Searcher for Mixed-precision Activation Quantization.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 12. 2024.\n\n[3] Shin, Juncheol, et al. \"NIPQ: Noise proxy-based integrated pseudo-quantization.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
            },
            "questions": {
                "value": "See the weaknesses above."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}