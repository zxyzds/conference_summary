{
    "id": "wkp57p0uhm",
    "title": "WebCanvas: Benchmarking Web Agents in Online Environments",
    "abstract": "For web agents to be practically useful, they must adapt to the continuously evolving web environment characterized by frequent updates to user interfaces and content. However, most existing benchmarks only capture the static aspects of the web. To bridge this gap, we introduce WebCanvas, an innovative online evaluation framework for web agents that effectively addresses the dynamic nature of web interactions. WebCanvas contains three main components to facilitate realistic assessments: (1) A novel evaluation metric which reliably capture critical intermediate actions or states necessary for task completions while disregarding noise caused by insignificant events or changed web-elements. (2) A benchmark dataset called Mind2Web-Live, a refined version of original Mind2Web static dataset containing 542 tasks with 2439 intermediate evaluation states; (3) Lightweight and generalizable annotation tools and maintenance pipelines that enables the community to collect and maintain the high-quality, up-to-date dataset. Building on WebCanvas, we open-source a baseline agent framework with extensible modules for reasoning, providing a foundation for the community to conduct online inference and evaluations. Our best-performing agent achieves a task success rate of 23.1% and a task completion rate of 48.8% on the Mind2Web-Live test set. Additionally, we analyze the performance discrepancies across various websites, domains, and experimental environments. We encourage the community to contribute further insights on online agent evaluation, thereby advancing this field of research.",
    "keywords": [
        "web automation; benchmark; LLM; language-guided agents"
    ],
    "primary_area": "datasets and benchmarks",
    "TLDR": "We introduce WebCanvas, an online evaluation framework for web agents designed to address the dynamic nature of web interactions.",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=wkp57p0uhm",
    "pdf_link": "https://openreview.net/pdf?id=wkp57p0uhm",
    "comments": [
        {
            "summary": {
                "value": "The paper is devoted to the development of new benchmark for web agents, which should demonstrate flexibility and tolerance to (1) alternative (non-canonical) trajectories of task completion and (2) dynamic nature of the web, where sites and their features constantly evolve.\n \nThe key idea of the paper is to introduce \u201ckey nodes\u201d in the task completion process, which designate the inevitable intermediate states of requests and URLs."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "The motivation and the problem are very relevant"
            },
            "weaknesses": {
                "value": "The technical quality of the work is under concerns. The work relates to evaluation methodology, and the main contribution is the proposed benchmark based on key nodes. I expect an analysis of how the proposed metric for web agents correlates with the goal metrics such as success rate based on outcomes. We can annotate, for a number of agents, outcome results for a representative number of tasks, and compare the correlation between \u201ckey nodes-based success rate\u201d and outcome-based success rate against the same correlation for \u201cstep-based success rate\u201d proposed in Deng et al., 2024.\n\nThis is a common requirement for new metrics in methodology papers: to look at the directionality, see e.g. \u201cUsing the Delay in a Treatment Effect to Improve Sensitivity and Preserve Directionality of Engagement Metrics in A/B Experiments\u201d by Drutsa et al.\n\nTable 3: the result itself is expectable, because mindAct is based on direct finetuning to ground-truth actions, which are then used for evaluation of success rate in the offline setting. Such approach makes MindAct less generalizable to flexible metric and dynamic environment, unlike GPT-3,5/4, which are used with in-context learning, without finetuning."
            },
            "questions": {
                "value": "Why GPT-3,5/4 perform better in the more challenging online setting as compared to offline setting (Table 3)? It is not clear what was the protocol for online setting in this table. Authors only said that \u201cevaluation metrics \u2026 differ\u201d and, in online setting, \u201cwe evaluate the intermediate state, not the referenced action\u201d. Is this metric exactly \u201cTask Success Rate\u201d described in Section 3.2?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper proposes Webcanvas ,a benchmarking framework for evaluating web agents in dynamic online environments. In this framework, a new metric is proposed based on 'key nodes'. Then authors construct a online and dynamic dataset called Mind2Web-Live, which builds upon the existing Mind2set dataset. Mind2Web-Live includes extensive annotation data collected through human labor and will be regularly updated and maintained by authors. Finally, various models are evaluated on Mind2Web-Live, providing some insights according to the results."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "- Introduces a innovative evaluation framework WebCanvas for web agent. By focusing on \u201ckey nodes\u201d, this framework provides a more reliable and accurate assessment compared to traditional methods that only consider the final task success rate.\n- Constructs a online and dynamic benchmark Mind2Web-Live that is an enhanced version of the original Mind2Web static dataset. \n- The authors have developed a community-driven platform where users can report issues with the dataset, and regular updates are performed."
            },
            "weaknesses": {
                "value": "- When the data size was reduced from Mind2Web's original 2000+ tasks to 500 +, the authors did not analyze how many different domains the Mind2Web-Live can cover and whether there are enough tasks for each domain.\n- There is a problem of scalability in this dataset because updating data requires people to maintain it. When the scale of dataset increases, maintenance costs will increase."
            },
            "questions": {
                "value": "1. Can you provide more details about the data distribution in the benchmark. Such as how many domains this benchmark can cover?  and how many task in each domain?\n2. Whether the agent running in the real word environment will have a negative impact on related websites?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper presents a novel benchmark for web-based tasks designed for agent evaluation. The proposed benchmark introduces step-wise assessment, live sample testing, and a user-friendly community platform, facilitating the online evaluation of agents. The authors conduct experiments using several large language models (LLMs) on the proposed platform."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1. The paper contributes a significant new benchmark for web mining, which is expected to provide substantial value to the research community.\n    \n2. The benchmark incorporates several valuable features, including intermediate state evaluations, a user-friendly interface with plugin support, and access to live datasets.\n    \n3. The writing is clear and well-structured, with numerous case studies that aid in understanding the framework and its applications."
            },
            "weaknesses": {
                "value": "1. The experimental evaluation is limited to a comparison with Mind2Web. It would be beneficial to include comparisons with additional benchmarks, evaluating a wider range of models to yield deeper insights.\n    \n2. The paper lacks a detailed breakdown of the sample categories. Providing statistical information on the task categories would help demonstrate the scope and coverage of the benchmark.\n    \n3. The benchmark currently offers a relatively small set of tasks. Expanding the sample size in future iterations would improve the benchmark's applicability. Compared to benchmarks like Mind2Web and WEBLINX, the proposed benchmark\u2019s dataset remains limited.\n    \n4. Although the authors highlight their community-driven platform and a cost-effective bi-monthly data maintenance schedule, the benchmark project appears less active. Notably, the last update was two months ago, and several long-standing issues remain unresolved."
            },
            "questions": {
                "value": "1. Will the heatmap for evaluation function accuracy across annotation steps be generated automatically upon completion of the evaluation? This could provide users with useful insights into model performance. Additionally, could some textual analysis be offered through LLM integration?\n    \n2. Are all key nodes weighted equally in the evaluation process?\n    \n3. Could you clarify why there are no cases labeled as \"value include\" in Figure 5?\n    \n4. A minor note: The planning prompt specifies a requirement for \"JSON blob format,\" which seems somewhat contradictory, as JSON is typically string-based, while a blob refers to a binary object. Could you clarify this distinction?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "details_of_ethics_concerns": {
                "value": "NA"
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces a novel framework for assessing web agents in dynamic online environments. In contrast to conventional benchmarks that focus on static web conditions, WebCanvas proposes a novel key-node-based evaluation metric, an enhanced dataset named Mind2Web-Live, and efficient annotation tools. Additionally, the authors demonstrate their best-performing agent in the Mind2Web-Live dataset and provide the analysis of the performance discrepancies."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "+ This paper is mostly well-written and easy to follow.\n+ The paper is technically sound with most claims supported sufficiently by experimental results.\n+ The proposed evaluation metrics and datasets seem novel."
            },
            "weaknesses": {
                "value": "- The problem formulation is incomplete in Section 2. The authors should bring some contents in Section E.1 back to the main paper. Additionally, the final objective function is missing in Section 2 as well.\n- It is a bit odd that \u201cinclude match\u201d and \u201csemantic match\u201d share the same evaluation targets for step score. Not sure if it is better to introduce additional aspects to distinguish them.  \n- Some parts of the presentation could be improved, e.g., in Line 136, the notation of action history a_{1}^{t-1} is not clear. It is better to use a_{1:t-1} to represent history following POMDP literature."
            },
            "questions": {
                "value": "- The problem formulation is incomplete in Section 2. \n- The authors might consider bringing some contents in Section E.1 back to the main paper. \n- The final objective function seems to be missing in Section 2.\n- \u201cinclude match\u201d and \u201csemantic match\u201d share the same evaluation targets for step score. Consider introducing additional aspects to distinguish them.  \n- Some parts of the presentation could be improved, e.g., in Line 136, the notation of action history a_{1}^{t-1} is not clear. \n- It is better to use a_{1:t-1} to represent history following POMDP literature."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}