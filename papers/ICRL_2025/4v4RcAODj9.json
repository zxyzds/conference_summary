{
    "id": "4v4RcAODj9",
    "title": "DUALFormer: A Dual Graph Convolution and Attention Network for Node Classification",
    "abstract": "Graph Transformers (GTs), adept at capturing the locality and globality of graphs, have shown promising potential in node classification tasks. Most state-of-the-art GTs succeed through integrating local Graph Neural Networks (GNNs) with their global Self-Attention (SA) modules to enhance structural awareness. Nonetheless, this architecture faces limitations arising from scalability challenges and the trade-off between capturing local and global information. On the one hand, the quadratic complexity associated with the SA modules poses a significant challenge for many GTs, particularly when scaling them to large-scale graphs. Numerous GTs necessitated a compromise, relinquishing certain aspects of their expressivity to garner computational efficiency. On the other hand, GTs face challenges in maintaining detailed local structural information while capturing long-range dependencies. As a result, they typically require significant computational costs to balance the local and global expressivity. To address these limitations, this paper introduces a novel GT architecture, dubbed DUALFormer, featuring a dual-dimensional design of its GNN and SA modules. Leveraging approximation theory from Linearized Transformers and treating the query as the surrogate representation of node features, DUALFormer \\emph{efficiently} performs the computationally intensive global SA module on feature dimensions. Furthermore, by such a separation of local and global modules into dual dimensions, DUALFormer achieves a natural balance between local and global expressivity. In theory, DUALFormer can reduce intra-class variance, thereby enhancing the discriminability of node representations. Extensive experiments on eleven real-world datasets demonstrate its effectiveness and efficiency over existing state-of-the-art GTs.",
    "keywords": [
        "Graph Transformers",
        "Node Classification"
    ],
    "primary_area": "learning on graphs and other geometries & topologies",
    "TLDR": "",
    "creation_date": "2024-09-27",
    "original_date": "2024-10-04",
    "modification_date": "2024-10-13",
    "forum_link": "https://openreview.net/forum?id=4v4RcAODj9",
    "pdf_link": "https://openreview.net/pdf?id=4v4RcAODj9",
    "comments": [
        {
            "comment": {
                "value": "I appreciate the authors for their detailed rebuttal. However, based on the current version, I do not believe the work is ready for publication. My concerns are as follows:\n\nFirst, as the authors claimed, the proposed method actually only capture the relations between each feature. As the results, it is efficient since the dimension of the feature vector is much smaller than the number of nodes. Maybe the results reported in this paper show that the feature-feature attention can lead to better model performance than the node-node attention.\n\nSecondly, the experimental results presented by the authors remain unclear and potentially misleading. For instance, in their response to Question 4, they mention a maximum GPU cost of 146 MB. I strongly recommend that the authors carefully review and validate their experimental setup and results to ensure they are accurate and reproducible.\n\nLastly, the investigation of the hyperparameter $\\alpha$ is insufficient. In the original version of the manuscript, Table 5 indicates that the optimal value of $\\alpha$ for the proteins dataset is zero. This finding should be critically examined and explained. Furthermore, the authors state that the search space for $\\alpha$ includes 0.1, 0.3, and 0.5, yet they report 0.2 as the optimal value for the arXiv dataset. These inconsistencies raise questions about the reliability and thoroughness of the experimental results.\n\nIn light of these issues, I believe the current version of this work requires substantial revisions before it can be considered for acceptance. The current version lacks the necessary rigor and clarity to support the claims made, and a more meticulous examination of the experimental designs and results is warranted."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer GjDD (Part 2)"
            },
            "comment": {
                "value": ">Q4. The authors should report the overall training cost of each method for efficiency study, especially on large-scale graphs. Maybe authors can refer to the settings in NAGphormer. For instance, can the proposed method achieve more efficient and more powerful performance than NAGphormer on Aminer, Reddit, and Amazon2M? \n\nR4. According to your advice, we have compared the training cost in terms of total running time (s) and GPU memory (MB) of the proposed DUALFormer and NAGphormer. The batch size is uniformly set to 2000. The total number of training epochs is set to 100. All shared configurations are set to the same to ensure fairness. The result is shown in the table below. \n\n|  | AMiner-CS | Reddit  | Amazon2M |\n|:--------:|:--------:| :---------:|:--------:| \n|#Nodes| 593,486 | 232,965 | 2,449,029 | \n|#Edges| 6,217,004 | 11,606,919 | 61,859,140 | \n|#Attributes| 100 | 602 | 100 | \n|#Classes| 18 | 41 | 47 | \n| |Accuracy(%) / Time(s) / Memory(MB) | Accuracy(%) / Time(s) / Memory(MB) | Accuracy(%) / Time(s) / Memory(MB) |\n| NAGphormer | 56.21$_{\u00b1 0.42}$ / 38.51 / 84 | 93.58$_{\u00b1 0.05}$ / 30.88 / 140   | 83.97$_{\u00b1 0.43}$ / 568.91 / 146 | \n| DUALFormer |  58.56$_{\u00b1 0.50}$ / 2.32 / 30 | 94.71$_{\u00b1 0.07}$ / 6.82 / 64  |84.80$_{\u00b1 0.22}$ / 40.38 / 26 | \n\nFrom the table, two results can be observed: firstly, the proposed DUALFormer consistently outperforms NAGpormer across the three datasets, and secondly, the proposed DUALFormer has short running times across the three datasets compared to the baseline NAGpormer. The advantage of DUALFormer primarily stems from its elimination of the need for preprocessing to acquire structural encoding and storage, unlike NAGphormer, which requires such steps. This agrees with the conclusion of the complexity analysis.\n\n---\n>Q5. As shown in Section 4.2, DUALFormer relies on the sampling strategy to perform on large-scale graphs, just like advanced linear graph Transformers. Hence, I think the GPU memory comparison is questionable since it is largely related to the batch size. Do authors set the same batch for each method?\n\nR5. I understand your concern about fairness. The common hyper-parameters (including batch size) are the **same for each model**, as mentioned in Line 953. Specifically, all models are trained on ogbn-arxiv using full batch, whereas, for ogb-products and pokec, the batch size is set to 10K. We will explicitly mention this in the captions of the corresponding tables and figures in the revised manuscript.\n\n---\n>Q6. The analysis of the $\\alpha$ is missing. According to Table 5, the performance of DUALFormer could be sensitive to the value of $\\alpha$. So, the parameter analysis of $\\alpha$ should be added into the experiment section.\n\nR6. Thanks for your careful check. The impact of the hyper-parameter $\\alpha$ on model performance is shown below.\n\n|  | Cora | CiteSeer  | PubMed | Computers | Photo  | CS | Physics |\n|:--------:|:--------:| :---------:|:--------:|:--------:| :---------:|:--------:|:--------:|\n| 0.1 | 85.88$_{\u00b1 0.10}$| 74.45$_{\u00b10.39}$  |83.97 $_{\u00b1 0.43}$| 93.09$_{\u00b10.14}$ |96.74$_{\u00b10.09}$ |  95.62$_{\u00b10.05}$| 97.37$_{\u00b1 0.02}$|\n| 0.3 |  85.20$_{\u00b1 0.12}$| 73.69$_{\u00b1 0.03}$ |83.91$_{\u00b1 0.07}$ | 93.14$_{\u00b10.15}$ | 96.43$_{\u00b1 0.07}$| 95.38$_{\u00b1 0.04}$|  97.42$_{\u00b10.03}$|\n| 0.5 | 85.35$_{\u00b1 0.08}$| 74.06$_{\u00b10.06}$ | 83.89$_{\u00b1 0.52}$| 93.16$_{\u00b10.17}$ |96.39$_{\u00b10.09}$ | 95.52$_{\u00b1 0.05}$ |  97.39$_{\u00b1 0.02}$|\n| Margin | 0.68| 0.39| 0.08 | 0.07 | 0.35 | 0.24 | 0.05 | \n\nFrom the table, it can be observed that DUALFormer is not sensitive to the parameter $a$. Specifically, within the parameter selection range, the variation of classification accuracy does not exceed $0.7\\%$."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer GjDD (Part 1)"
            },
            "comment": {
                "value": ">Q1. The key contributions of the proposed method are not clear.  \n\nR1. The key contribution of the proposed DUALFormer is to introduce self-attention on the feature dimension. Although the design is simple, it possesses the following three excellent characteristics.\n\n1) **A scalable self-attention**. Due to the quadratic computational complexity of their self-attention to the node dimension, Vanilla GTs often encounter scalability issues. In contrast, self-attention in DUALFormer operates efficiently, with complexity linearly related to the size of the graph. It is designed to capture inter-feature correlations to approximate global inter-node dependencies. As a result, it is potentially scalable to large-scale graphs. \n\n2) **Improvement of discriminability**. Rigorous theoretical analysis demonstrates the rationality behind this design of self-attention on a novel dimension in improving the discriminability of node representations. \n\n3) **Comprehensive expressivity**. Due to the global self-attention operating on the feature dimension, it seamlessly integrates with the local GNN module without compromising their expressivity. Therefore, DUALFormer achieves an automatic trade-off between local and global expressivity. \n   \nFurthermore, the proposed DUALFormer has achieved **state-of-the-art** performances on many tasks, including node classification and node property prediction.  \n \n\n ---\n>Q2. As the authors claim in Eq. 13, the proposed method only captures the feature-to-feature correlations. In my opinion, it is not the global information on the graph since it is unable to capture the relations between nodes. Why do authors claim the proposed method can capture the global information on the graph?\n\nR2. The capability of capturing global information stems from the approximate equivalence between $\\operatorname{softmax}(\\mathbf{Q}\\mathbf{K}^T)\\mathbf{V}$ and $\\mathbf{Q}(\\mathbf{K}^T\\mathbf{V})$. The global information is captured by the attention between nodes, i.e., $\\operatorname{softmax}(\\mathbf{Q}\\mathbf{K}^T)$, in previous graph transformers. According to the combination law for matrix multiplication, it holds that $(\\mathbf{Q}\\mathbf{K}^T)\\mathbf{V} = \\mathbf{Q}(\\mathbf{K}^T\\mathbf{V})$ and $\\operatorname{softmax}(\\mathbf{Q}\\mathbf{K}^T)\\mathbf{V} \\approx \\mathbf{Q}(\\mathbf{K}^T\\mathbf{V})$. Thus, this paper tends to approximate the expensive node-node attention $(\\mathbf{Q}\\mathbf{K}^T)$ via efficient feature-feature attention $(\\mathbf{K}^T\\mathbf{V})$ since $\\operatorname{softmax}(\\mathbf{Q}\\mathbf{K}^T)\\mathbf{V} \\approx \\mathbf{Q}(\\mathbf{K}^T\\mathbf{V})$. Thus, the proposed DUALFormer can capture global information. \n\n\n---\n>Q3. The authors claim that the computational complexity of the proposed method is $O(n)$, which is obviously wrong. Based on Eq. 14, the calculation involves the adjacency matrix. Hence, the computational complexity of this part is $O(E)$, and it cannot be ignored since $|E| > |N|$(even $|E| \\gg |N|$ on some graphs). \n\nR3. Thanks for pointing out this error. In the previous version, only the complexities of self-attention in **ALL** GTs are considered. We will add the time complexity of GNNs to the time complexity of corresponding methods, including GraphTrans, SAT, GraphGPS, NodeFormer, NAGphormer, Exphormer, GOAT, SGFormer, Polynormer, GoBFormer, and the proposed DUALFormer. The adjusted time complexity is shown in the following table, where $e$ denotes the number of edges. \n\n|  |GraphTrans| SAT| GraphGPS | NodeFormer  | NAGphormer | Exphormer | GOAT | SGFormer | Polynormer | GoBFormer| DUALFormer\n|:--------:|:--------:|:--------:|:--------:| :---------:|:--------:|:--------:| :--------:|:--------:| :--------:| :---------:|:--------:|\n|Pre-processing | - | $O(n^3)$ | $O(n^3)$ | - | $O(n^3+e)$ | $O(n^3)$ | $O(nlogn)$ | - | - | $O(nlogn)$ | -|\n| Training | $O(n^2+e)$ | $O(n^2+e)$ | $O(n+e)$ | $O(n+e)$  | $O(n)$| $O(n+e)$ | $O(n+e)$| $O(n+e)$| $O(n+e)$| $O(n^{\\frac{4}{3}}+e)$ |$O(n+e)$| \n\nThe table reveals that the computational complexity of the proposed DUALFormer is linearly proportional to the number of nodes and edges, demonstrating its efficiency. Note that the complexity of DUALFormer aligns with those of the existing scalable graph transformers, such as NodeFormer, SGFormer, and Polynormer, while it does NOT require complicated preprocessing. This highlights the scalability of the proposed DUALFormer.  Therefore, the conclusion of the high scalability of the proposed DUALFormer is not changed."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer Q319"
            },
            "comment": {
                "value": "> Q1. The paper has some minor errors that need fixing. For example, Table 2 misses the mean value for the GraphGPS model on the Citeseer dataset. \n\nR1. Thanks for your careful checking. We will thoroughly check the manuscript to correct any omissions. \n\n---\n> Q2. To enhance readability, Equation 13 should be split into two or three equations. \n\nR2. Based on your suggestion, we will divide Equation 13 into three formulas by row. \n\n---\n> Q3. The model DUALFormer places the GNN layers, such as the SGC layers, after the attention layers. What is the rationale behind this design? Is it possible to reverse this order? \n\nR3. We would like to explain this design choice as follows. \n\nThis choice is primarily motivated by the desire to decouple local and global modules, thereby minimizing their mutual interference. The self-attention module generally relies on input representations to calculate attention coefficients, whereas the GNN module, typically GCN and GAT, utilizes fixed propagation coefficients that are input-independent. Therefore, placing the GNN module after the self-attention module can mitigate their mutual interference and ensure that comprehensive information is retained. Thus, it seems that this order cannot be reversed. \n\n---\n> Q4. Figure 4 shows that the model utilizing APPNP outperforms the one using SGC in the Cora and Pubmed datasets. What accounts for this performance difference?\n\nR4. This performance difference is primarily attributed to the difference in the localizing property of these two models. As can be seen in Figure 4, the original APPNP has a performance advantage over the original SGC on the Cora and PubMed datasets. This demonstrates the superiority of the former in terms of localizing property. By designing the global self-attention module in the pairwise dimension of the local GNN module, DUALFormer naturally obtains the global information with the guarantee that it does not interfere with each other. Thus, the DUALFormer based on APPNP with superior localizing property outperforms the DUALFormer based on SGC.\n\nThank you for the reminder. It underscores the compatibility of DUALFormer and suggests the potential for further enhancements by integrating it with more advanced GNNs.\n\n---\n> Q5. The effect of certain hyper-parameters, such as the parameter $\\alpha$ in Equation 13, on performance has yet to be unverified. \n\nR5. Thanks for your careful check. The impact of the hyper-parameter $\\alpha$ on model performance is shown below.  \n\n|  | Cora | CiteSeer  | PubMed | Computers | Photo  | CS | Physics |\n|:--------:|:--------:| :---------:|:--------:|:--------:| :---------:|:--------:|:--------:|\n| 0.1 | 85.88$_{\u00b1 0.10}$| 74.45$_{\u00b10.39}$  |83.97 $_{\u00b1 0.43}$| 93.09$_{\u00b1 0.14}$ |96.74$_{\u00b10.09}$ |  95.62$_{\u00b10.05}$| 97.37$_{\u00b1 0.02}$|\n| 0.3 |  85.20$_{\u00b1 0.12}$| 73.69$_{\u00b1 0.03}$ |83.91$_{\u00b1 0.07}$ | 93.14$_{\u00b1 0.15}$ | 96.43$_{\u00b1 0.07}$| 95.38$_{\u00b1 0.04}$|  97.42$_{\u00b10.03}$|\n| 0.5 | 85.35$_{\u00b1 0.08}$| 74.06$_{\u00b10.06}$ | 83.89$_{\u00b1 0.52}$| 93.16$_{\u00b1 0.17}$ |96.39$_{\u00b10.09}$ | 95.52$_{\u00b1 0.05}$ |  97.39$_{\u00b1 0.02}$|\n| Margin | 0.68| 0.39| 0.08 | 0.07 | 0.35 | 0.24 | 0.05 | \n\nFrom the table, it can be observed that DUALFormer is not sensitive to the parameter $a$. Specifically, within the parameter selection range, the variation of classification accuracy does not exceed $0.7\\%$.\n\n---\n> Q6. The paper does not mention any plans to open-source the code.\n\nR6. We promise to open-source the code and provide a GitHub link once the paper is accepted."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer CPJ4 (Part 2)"
            },
            "comment": {
                "value": ">Q4. The first question concerns the reasonableness of applying softmax to the global correlations between features. Could you clarify these differences and explain why it is reasonable to replace $\\phi(\\mathbf{Q})(\\phi(\\mathbf{K})^{\\top}\\mathbf{V})$ with $\\mathbf{Q}\\operatorname{softmax}(\\mathbf{K}^{\\top}\\mathbf{V})$?\n\nR4. The introducted softmax is just a implementation strategy, while the obvious equivalence $\\phi(\\mathbf{Q})(\\phi(\\mathbf{K})^{\\top}\\mathbf{V}) = (\\phi(\\mathbf{Q})\\phi(\\mathbf{K})^{\\top})\\mathbf{V}$ is the key point we want to emphasize. This equivalence motivates the additional transformer on feature dimension and the proposed DUALFormer. To demonstrate the ignorability of softmax, we conduct an ablation study on the impact of softmax with results shown in the following tables. It illustrate that we can employ $\\phi(\\mathbf{Q})(\\phi(\\mathbf{K})^{\\top}\\mathbf{V}) $, which is equivalent to $(\\phi(\\mathbf{Q})\\phi(\\mathbf{K})^{\\top})\\mathbf{V}$. We sincerely apologize for any confusion caused by the introduced softmax and will remove it in the final version. \n\n|  | Cora | CiteSeer  | PubMed | Computers | Photo  | CS | Physics |\n|:--------:|:--------:| :---------:|:--------:| :--------:| :---------:|:--------:|:--------:| \n| without softmax | 85.69 | 74.55  | 83.62 | 93.29 | 96.91 |   95.61 |  97.30 | \n| with softmax |  85.88 | 74.45 | 83.97 | 93.16 | 96.74 | 95.62 | 97.42 | \n\n---\n> Q5. The  interpretation of the proposed global attention on the special case of one-dimensional feature. \n\nR5.  For the case of a one-dimensional feature, **neither** previous approaches **nor** the proposed DUALFormer gather information. $\\phi(\\mathbf{Q})\\phi(\\mathbf{K})^T$ in previous methods reduces to a rank-1 matrix, whose rows only differ from each other by a factor since $\\phi(\\mathbf{K})^T$ is a row vector and $\\phi(\\mathbf{Q})$ is a column vector. Thus, the aggregation patterns/coefficients for different nodes, represented by the rows of $\\phi(\\mathbf{Q})\\phi(\\mathbf{K})^T$, only differ by this factor. As a result, $\\phi(\\mathbf{Q})\\phi(\\mathbf{K})^T\\mathbf{V}$ degrades to the same aggregation pattern/coefficient for different nodes. Since the essence of aggregation is the different aggregation patterns/coefficients for different nodes, previous approaches lose this characteristic for a one-dimensional feature. Therefore, they also do **NOT** gather information as the proposed DUALFormer in this special case. \n\n---\n> Q6. The motivation for the study is not fully convincing.\n\nR6. We hope the above two responses could clarify the rationality of our motivation.  Firstly, the key motivation is the obvious equivalence $\\phi(\\mathbf{Q})(\\phi(\\mathbf{K})^{\\top}\\mathbf{V}) = (\\phi(\\mathbf{Q})\\phi(\\mathbf{K})^{\\top})\\mathbf{V}$. Second,  **neither** previous approaches **nor** the proposed DUALFormer gather information for the case of a one-dimensional feature. Thanks for your special case. **It also demonstrates the importance of multiple features for the transformer.  Thus, the proposed DUALFormer is further justified by exploring the correlation among multiple features with an additional transformer.**"
            }
        },
        {
            "title": {
                "value": "Response to Reviewer CPJ4 (Part 1)"
            },
            "comment": {
                "value": "> Q1. Suggested datasets include Roman-Empire, Question[1], Wiki, and ogbn-papers100M. \n\nR1. According to your suggestion, we have conducted model comparisons on the Roman-Empire, Question, and ogbn-papers100M datasets. For the Roman-Empire and Questions datasets, the data partitioning follows the scheme from [1], specifically, a 50/25/25 split for training, validation, and testing. For the ogbn-papers100M, the split ratio is public split [2], namely 78/8/14. The statistics of the dataset and the experimental results are shown in the following table. \n\n|  | Roman-Empire | Question  | ogbn-papers100M |\n|:--------:|:--------:| :---------:|:--------:| \n|#Nodes| 22,662 | 48,921 | 111,059,956 | \n|#Edges| 32,927 | 153,540 | 1,615,685,872 | \n|#Attributes| 300 | 301 | 128 | \n|#Classes| 18 | 2 | 172 | \n| NAGphormer | 74.45$_{\u00b10.48}$ |   75.13$_{\u00b10.70}$ | - | \n| GOAT | 72.30$_{\u00b10.48}$ | 75.95$_{\u00b11.38}$  | - | \n| SGFormer |  73.91$_{\u00b10.79}$ | 77.06$_{\u00b11.20}$ | 66.01$_{\u00b10.37}$ | \n| DUALFormer |  77.31$_{\u00b10.17}$ | 78.62$_{\u00b10.56}$ | 67.59$_{\u00b10.28}$ | \n\nThe table reveals that, in comparison to the baselines, our proposed DUALFormer achieves consistently performance advantages on all three datasets, underscoring its superiority and scalability. \n\n[1] A critical look at the evaluation of GNNs under heterophily: Are we really making progress? ICLR 2023\n[2] Open Graph Benchmark: Datasets for Machine Learning on Graphs. NeurIPS 2020\n\n---\n>Q2. The statement, \u201cMost GTs consistently show superior performance over GNNs across all datasets\u201d (line 451), would be more convincing if compared with recent GNN baselines, such as ChebNetII[2] and OptBasis[3]. \n\nR2. Thank you for pointing out the imprecise description.  The correct description would be: \"Most GTs consistently show superior performance over **the backbone** GNNs, which typically are GCN and GAT, across all datasets.\u201d Based on your advice, we further compare the proposed DUALFormer with these two recent GNN baselines, namely ChebNetII and OptBasis, on five datasets. As can be seen from the following table on five datasets, the proposed DUALFormer consistently outperforms the baseline GNNs. This underscores the effectiveness of DUALFormer. \n\n|  | Roman-Empire | Question  | ogbn-papers100M | pokec| ogbn-arxiv\n|:--------:|:--------:| :---------:|:--------:| :--------:| :--------:| \n| ChebNetII | 74.64$_{\u00b10.39}$ |   74.41$_{\u00b10.58}$ | 67.18$_{\u00b10.32}$ | 82.33$_{\u00b1 0.28}$| 72.32$_{\u00b10.23}$|\n| OptBasisGNN | 76.91$_{\u00b10.37}$ |   73.82$_{\u00b10.83}$ | 67.22$_{\u00b10.15}$ | 82.83$_{\u00b10.04}$ | 72.27$_{\u00b1 0.15}$|\n| DUALFormer |  77.31$_{\u00b10.17}$ | 78.62$_{\u00b10.56}$ | 67.59$_{\u00b10.28}$ | 82.97$_{\u00b10.43}$| 73.71$_{\u00b10.22}$|\n\n---\n>Q3. There are a few typographical errors.\n\nR3. Thanks for your careful review. We will meticulously check the manuscript to ensure all errors are corrected."
            }
        },
        {
            "title": {
                "value": "Response to Reviewer YCph"
            },
            "comment": {
                "value": ">Q1. The proposed method can be interpreted as \"attention on attributes\". I wonder how is it different from the standard self attention. Especially why it can perform better on node classification? And when it is expected to perform better and when not?\n\nR1. Firstly, both the proposed self-attention on dimension regarding attributes and standard self-attention are to capture global information despite their different forms. The former approximately describes the global dependence between nodes, which is the main role of the latter, by characterizing the correlation between features. Secondly, the performance boost is not due to this alone but rather the dual design of local and global modules. This prevents the trade-off between local and global information and enables comprehensive information modeling. Finally, the proposed method improves the performance of GNNs by capturing the relationships between features. They perform better when there is a strong correlation among features, and their effectiveness may be less effective when such correlations are weak. \n\n---\n>Q2. Can you provide further analysis, such as case studies, to further explain the semantic meanings of the \"attention on attributes\"? \n\nR2. The semantic meaning of the proposed attribute (feature) attention is that it focuses on the correlation among features, allowing the model to capture the information that is most discriminative for the task. We would like to provide the following case to illustrate this point. \n\nSuppose there are five nodes with four features, where three of these nodes (the index are the first three) belong to one class, and the other two belong to the other class. When the feature matrix exhibits low-class discriminability, the matrix can be exemplified by $\\mathbf{H}=$\n\n[[ $\\frac{1}{3}$,  $\\frac{2}{3}$, 0, 0 ],\n\n[ $\\frac{2}{3}$, $\\frac{1}{3}$, 0 , 0 ],\n\n[ 0, 1, 0, 0 ], \n\n[ 0, 0, 1, 0 ], \n\n[ 0, 0, 0, 1 ]]$_{5\\times 4},$ where the rows correspond to nodes and columns to features. \n\nAssuming a clear feature correlation, e.g., the first two features signal the first class, while the last two features correspond to the second class. The attention score matrix can be expressed as $\\mathbf{S}=$\n\n[[ $\\frac{1}{2}$,  $\\frac{1}{2}$, 0, 0 ],\n\n[ $\\frac{1}{2}$, $\\frac{1}{2}$, 0 , 0 ],\n\n[ 0, 0, $\\frac{1}{2}$, $\\frac{1}{2}$ ], \n\n[ 0, 0, $\\frac{1}{2}$, $\\frac{1}{2}$ ]]$_{4 \\times 4}$. \n\nUsing feature attention, the updated features can be expressed as $\\hat{\\mathbf{H}}= \\mathbf{H}\\mathbf{S} =$ \n\n[[ $\\frac{1}{2}$,  $\\frac{1}{2}$, 0, 0 ],\n\n[ $\\frac{1}{2}$, $\\frac{1}{2}$, 0 , 0 ],\n\n[ $\\frac{1}{2}$, $\\frac{1}{2}$, 0, 0 ], \n\n[ 0, 0, $\\frac{1}{2}$, $\\frac{1}{2}$ ], \n\n[ 0, 0, $\\frac{1}{2}$, $\\frac{1}{2}$ ]]$_{5\\times 4}.$\n\nThe updated features exhibit more obvious class discriminability compared to the input features. \n\n---\n>Q3. Can you provide further analysis and empirical studies to show that the GNNs after the graph Transform can indeed learn the localities in graphs?\n\nR3. We would like to provide the following theoretical analysis to explain that the GNN module is able to learn the locality of the graph. \n\nFirstly, from the perspective of graph learning, many classical GNNs (e.g., GCN and SGC) can be induced by optimizing the objective function [1, 2], namely \n\n$tr(\\mathbf{H}^{\\top}\\tilde{\\mathbf{L}}\\mathbf{H})=\\frac{1}{2}\\sum_{v,u}\\tilde{a}_{v,u}\\Vert \\mathbf{h}_v-\\mathbf{h}_u\\Vert_2^2$, \n\nwhere $\\tilde{\\mathbf{L}}$ denotes the Laplacian matrix of the normalized adjacent matrix $\\tilde{\\mathbf{A}}$, and $\\mathbf{H}$ stands for the node features such as $\\mathbf{H}=\\mathbf{X}\\mathbf{W}$ in GCN and $\\mathbf{H}= \\mathbf{X}$ in SGC. This indicates that GNNs essentially learn local information through feature updates that are constrained by the graph topology.\n\nFrom the above perspective, the GNN module in the proposed DUALFormer is equivalent to solving the above objective function with $\\mathbf{H}=\\mathbf{Z}$, that is, $tr(\\mathbf{H}^{\\top}\\tilde{\\mathbf{L}}\\mathbf{H})$, where $\\mathbf{Z}$ denotes the node features obtained from the self-attention on the dimension regarding features. Thus, even as a post-processing technique, the GNN module can ensure localizing property by leveraging graph topology to constrain the feature updates. \n\n[1] Interpreting and Unifying Graph Neural Networks with An Optimization Framework. WWW 2021\n\n[2] Why Do Attributes Propagate in Graph Convolutional Neural Networks? AAAI 2021"
            }
        },
        {
            "summary": {
                "value": "This paper introduces DUALFormer, a novel Graph Transformer model designed to address scalability challenges and improve local-global information fusion. The approach is both simple and theoretically grounded. Extensive experiments demonstrate DUALFormer\u2019s effectiveness, scalability, and robustness."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1. This paper is well-motivated.\n2. The proposed method is simple and effective.\n3. The inclusion of theoretical analysis strengthens the work.\n4. Extensive experiments show the effectiveness, scalability and robustness.\n5. This paper is easy to follow."
            },
            "weaknesses": {
                "value": "1. The proposed method can be interpreted as \"attention on attributes\". I wonder how is it different from the standard self attention. Especially why it can perform better on node classification? And when it is expected to perform better and when not?\n2. Can you provide further analysis, such as case studies, to further explain the semantic meanings of the \"attention on attributes\"?\n3. Can you provide further analysis and empirical studies to show that the GNNs after the graph Transform can indeed learn the localities in graphs?\n\nI will raise my score if my concerns are properly addressed."
            },
            "questions": {
                "value": "N.A."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 4
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "To address the scalability limitations of graph transformers (GTs) and the challenge of balancing local and global information, this paper introduces DualFormer, a novel GT architecture. DualFormer calculates global attention along the feature dimension, enabling the model to perform effectively and efficiently on large graphs while maintaining strong performance."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "- The writing is generally clear and accessible, making the paper readable and easy to follow.\n- The proposed method is both understandable and implementable, yet effective. It performs well on several datasets.\n- The paper includes diverse experimental analyses, such as node classification, node property prediction, ablation studies, and parameter sensitivity analyses. Furthermore, the authors offer theoretical guarantees to support the method."
            },
            "weaknesses": {
                "value": "- The motivation for the study is not fully convincing. Further details are provided in the questions below.\n- Since the paper emphasizes the method\u2019s scalability, additional experiments on larger graphs would reinforce this claim. Suggested datasets include *Roman-Empire*, *Question[1]*, *Wiki*, and *ogbn-papers100M*. Moreover, the GNN baselines in Tables 2 and 3 are outdated, which may reduce the persuasiveness of the results. For instance, the statement, \u201cMost GTs consistently show superior performance over GNNs across all datasets\u201d (line 451), would be more convincing if compared with recent GNN baselines, such as *ChebNetII[2]* and *OptBasis[3]*, to present a more comprehensive evaluation.\n- Minor Issues: There are a few typographical errors, such as \"abov\" (line 182). Consistent notation throughout the paper is also preferable. For instance, in line 168, there is a \"$\\times$\" symbol between a scalar and a matrix, but not in line 216. Additionally, line 191 includes a \"$\\cdot$\" between matrices, whereas line 167 does not.\n\n[1] A critical look at the evaluation of GNNs under heterophily: Are we really making progress? In ICLR 2023.\n\n[2] Convolutional Neural Networks on Graphs with Chebyshev Approximation, Revisited. In NeurIPS 2022.\n\n[3] Graph Neural Networks with Learnable and Optimal Polynomial Bases. In ICML 2023."
            },
            "questions": {
                "value": "- The first question concerns the reasonableness of applying softmax to the global correlations between features.\n\n  - In standard self-attention, $ \\mathbf{O} = \\exp(\\text{sim}(\\mathbf{Q}, \\mathbf{K}))\\mathbf{V} $ (Eq. 6).\n  - Through linearized attention, $ \\mathbf{O} = \\phi(\\mathbf{Q}) \\phi(\\mathbf{K})^\\top \\mathbf{V} $ (Eq. 11), where each element in $ \\phi(\\mathbf{Q}) \\phi(\\mathbf{K})^\\top $ is non-negative, representing attention weights (global dependencies between nodes).\n  - By the commutative property of matrix multiplication, $ \\mathbf{O} = \\phi(\\mathbf{Q}) (\\phi(\\mathbf{K})^\\top \\mathbf{V}) $, so we can interpret $ (\\phi(\\mathbf{K})^\\top \\mathbf{V}) $ as a correlation matrix (with elements that can be positive or negative).\n\n  However, in Eq. 13, $ \\mathbf{V} \\text{softmax}(\\mathbf{Q}^\\top \\mathbf{K}) $, i.e., $ \\mathbf{Q} \\text{softmax}(\\mathbf{K}^\\top \\mathbf{V}) $, differs from $ \\phi(\\mathbf{Q}) (\\phi(\\mathbf{K})^\\top \\mathbf{V}) $ because elements in $\\text{softmax}(\\mathbf{K}^\\top \\mathbf{V}) $ are all non-negative, unlike those in $ (\\phi(\\mathbf{K})^\\top \\mathbf{V})$. Could you clarify these differences and explain why it is reasonable to replace $ \\phi(\\mathbf{Q}) (\\phi(\\mathbf{K})^\\top \\mathbf{V}) $ with $ \\mathbf{Q} \\text{softmax}(\\mathbf{K}^\\top \\mathbf{V}) $?\n\n- The second question pertains to the interpretation of the proposed global attention. The method appears to aggregate information along the feature dimension, unlike previous approaches that gather global information across all or most nodes in a graph. For a one-dimensional feature, $ \\mathbf{V}  \\text{softmax}(\\mathbf{Q} \\mathbf{K}^T) $ in Eq. 13 reduces to $ \\mathbf{V} \\cdot \\alpha $, where $ \\alpha $ is a scalar and $ \\mathbf{V} \\in \\mathbb{R}^{n} $. How can this be understood as gathering information from a global perspective?"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 5
            },
            "confidence": {
                "value": 3
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper introduces DUALFormer, a graph transformer that tackles the challenges of the scalability and trade-off between local and global expressivity faced by current models. The motivation is to model the global dependencies among nodes by approximately characterizing the correlations between features. DUALFormer adopts a simple, intuitive design that includes local graph convolutional networks operating on the node dimension and a global self-attention mechanism operating on the feature dimension. The effectiveness and efficiency of the proposed DUALFormer are demonstrated in experimental evaluations across node classification and node property prediction tasks."
            },
            "soundness": {
                "value": 3
            },
            "presentation": {
                "value": 4
            },
            "contribution": {
                "value": 3
            },
            "strengths": {
                "value": "1) The motivation for the dual design of local and global modules in this paper is clear and interesting.\n2) The model DUALFormer is simple and efficient with a solid theoretical foundation. \n3) The paper offers extensive experimental validation across various datasets. \n4) The paper is well-organized and easy to read."
            },
            "weaknesses": {
                "value": "1) The paper has some minor errors that need fixing. For example, Table 2 misses the mean value for the GraphGPS model on the Citeseer dataset. \n2) To enhance readability, Equation 13 should be split into two or three equations. \n3) The model DUALFormer places the GNN layers, such as the SGC layers, after the attention layers. What is the rationale behind this design? Is it possible to reverse this order? \n4) Figure 4 shows that the model utilizing APPNP outperforms the one using SGC in the Cora and Pubmed datasets. What accounts for this performance difference?\n5) The effect of certain hyper-parameters, such as the parameter $\\alpha$ in Equation 13, on performance has yet to be unverified. \n6) The paper does not mention any plans to open-source the code."
            },
            "questions": {
                "value": "See the above 'Weaknesses'"
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 6
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        },
        {
            "summary": {
                "value": "This paper develop a new architecture based on GNNs and modified Transformers. The authors conduct expensive experiments as well as theoretical analysis to show the effectiveness of the proposed method."
            },
            "soundness": {
                "value": 2
            },
            "presentation": {
                "value": 3
            },
            "contribution": {
                "value": 2
            },
            "strengths": {
                "value": "1.  This paper is easy to follow.\n2.  The authors provide the theoretical analysis.\n3.  The results on various datasets seem to be promising."
            },
            "weaknesses": {
                "value": "1.  The comparison of efficiency study seems to be not reasonable.\n2.  The key contributions of the proposed method are not clear.\n3.  The complexity analysis of the proposed method seems to be wrong.&#x20;"
            },
            "questions": {
                "value": "I have the following questions:\n1.  As the authors claim in Eq. 13, the proposed method only captures the feature-to-feature correlations. In my opinion, it is not the global information on the graph since it is unable to capture the relations between nodes. Why do authors claim the proposed method can capture the global information on the graph?\n2.  According to the paper, the efficiency is the most important contribution of the proposed method. I think the authors express this point in a wrong way. Firstly, the authors claim that the computational complexity of the proposed method is $O(n)$ which is obviously wrong. Based on Eq. 14, the calculation involves the adjacency matrix. Hence, the computational complexity of this part is $O(E)$ and it is cannot be ignored since $|E|>|N|$ \uff08even $|E|>>|N|$ on some graphs). Then, the authors only compare the time cost of each epoch to demonstrate the efficiency which is not reasonable. I think the total training time cost is the most important metric to demonstrate the efficiency of a method. So, the authors should report the overall training cost of each method for efficiency study, especially on large-scale graphs.  Maybe authors can refer to the settings in NAGphormer. For instance, can the proposed method achieve more efficient and more powerful performance than NAGphormer on Aminer, Reddit and Amazon2M?\n3.  As shown in Section 4.2,  DUALFormer relies on the sampling strategy to perform on large-scale graphs, just like advanced linear graph Transformers. Hence, I think the GPU memory comparison is questionable since it is largely related to the batchsize. Do authors set the same batch for each method?\n4.  The analysis of the $\\alpha$ is missing. According to Table 5, the performance of DUALFormer could be sensitive to the value of $\\alpha$. So, the parameter analysis of $\\alpha$ should be added into the experiment section."
            },
            "flag_for_ethics_review": {
                "value": [
                    "No ethics review needed."
                ]
            },
            "rating": {
                "value": 3
            },
            "confidence": {
                "value": 5
            },
            "code_of_conduct": {
                "value": "Yes"
            }
        }
    ]
}